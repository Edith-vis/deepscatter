id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
36bccb0b293b8c84ce6844d516731e231437b17e	average-case performance analysis of online non-clairvoyant scheduling of parallel tasks with precedence constraints	parallel task;average case performance ratio;approximation algorithm;online scheduling;simulation;scheduling algorithm;probabilistic analysis;performance analysis;worst case performance ratio;non clairvoyant scheduling;precedence constraint;performance bounds;task graphs	We evaluate the average-case performance of three approximation algorithms for online non-clairvoyant scheduling of parallel tasks with precedence constraints. We show that for a class of wide task graphs, when task sizes are uniformly distributed in the range [1₀C], the online non-clairvoyant scheduling algorithm LL-SIMPLE has an asymptotic average-case performance bound of M/(M-(3-(1 + 1/C)C+1)C-1), where M is the number of processors. For uniform probability distributions of task sizes, we present numerical and simulation data to demonstrate the accuracy of our general asymptotic average-case performance bound. We also report extensive experimental results on the average-case performance of online non-clairvoyant scheduling algorithms LL-GREEDY and LS. Algorithm LL-GREEDY has better performance than LL-SIMPLE using an improved algorithm to schedule independent tasks in the same level. Algorithm LS produces even better schedules due to a break of boundaries among levels.		Keqin Li	2008	Comput. J.	10.1093/comjnl/bxm059	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;probabilistic analysis of algorithms;real-time computing;dynamic priority scheduling;computer science;rate-monotonic scheduling;distributed computing;scheduling;approximation algorithm	HPC	-13.022032324724998	60.208017826569304	42200
06b5ce2fc7e86f2b4ee61dee4ff564dfa576d2d9	accurate application progress analysis for large-scale parallel debugging	mathematics;high performance computing;computing;parallel debugging;and information science;mpi;dynamic analysis	Debugging large-scale parallel applications is challenging. In most HPC applications, parallel tasks progress in a coordinated fashion, and thus a fault in one task can quickly propagate to other tasks, making it difficult to debug. Finding the least-progressed tasks can significantly reduce the effort to identify the task where the fault originated. However, existing approaches for detecting them suffer low accuracy and large overheads; either they use imprecise static analysis or are unable to infer progress dependence inside loops. We present a loop-aware progress-dependence analysis tool, Prodometer, which determines relative progress among parallel tasks via dynamic analysis. Our fault-injection experiments suggest that its accuracy and precision are over 90% for most cases and that it scales well up to 16,384 MPI tasks. Further, our case study shows that it significantly helped diagnosing a perplexing error in MPI, which only manifested at large scale.	analysis of algorithms;debugging;dependence analysis;experiment;fault injection;full scale;markov chain;markov model;message passing interface;nas parallel benchmarks;scalability;sensor;simulation;static program analysis	Subrata Mitra;Ignacio Laguna;Dong H. Ahn;Saurabh Bagchi;Martin Schulz;Todd Gamblin	2014		10.1145/2594291.2594336	computing;supercomputer;parallel computing;real-time computing;computer science;distributed computing	PL	-17.292150707061193	48.259843899563116	42278
26840f08dcc74c6c15946eca7716a477be48daed	guest editors' introduction: evaluating servers with commercial workloads	analytical models;application software;semiconductor device measurement;decision support systems operating systems hardware counting circuits analytical models switches transaction databases delay application software semiconductor device measurement;counting circuits;transaction databases;decision support systems;switches;operating systems;hardware	Commercial workloads, which differ markedly from scientific and technical workloads, can more accurately evaluate new server designs.		Kimberly Keeton;Russell M. Clapp;Ashwini K. Nanda	2003	IEEE Computer	10.1109/MC.2003.1178043	embedded system;application software;real-time computing;decision support system;network switch;computer science;operating system	Arch	-13.812989285260423	50.826473784412634	42311
20de6e5f9b15c6d6de94b4ebb3b603e1d8471adf	synchronisation for dynamic load balancing of decentralised conservative distributed simulation	conference publication;dynamic load balancing;info eu repo semantics conferenceobject;synchronisation;distributed simulation	Synchronisation mechanisms are essential in distributed simulation. Some systems rely on central units to control the simulation but central units are known to be bottlenecks. If we want to avoid using a central unit to optimise the simulation speed, we lose the capacity to act on the simulation at a global scale. Being able to act on the entire simulation is an important feature which allows to dynamically load-balance a distributed simulation. While some local partitioning algorithms exist, their lack of global view reduces their efficiency. Running a global partitioning algorithm without central unit requires a synchronisation of all logical processes (LPs) at the same step. The first algorithm requires the knowledge of some topological properties of the network while the second algorithm works without any requirement. The algorithms are detailed and compared against each other. An evaluation shows the benefits of using a global dynamic load-balancing for distributed simulations.	algorithm;bottleneck (software);load balancing (computing);logical volume management;simulation	Quentin Bragard;Anthony Ventresque;Liam Murphy	2014		10.1145/2601381.2601386	real-time computing;simulation;computer science;distributed computing	HPC	-16.418733909330335	58.80702864964822	42315
afe93e6544df0924b19fd3ee318dab48b02faea5	using queue structures to improve job reliability	reliability;probability of failure;scaling up;high performance computer;mathematical model;failure rate;performance ratio;cluster design and architecture	Many high performance computing systems today exploit the availability and remarkable performance characteristics of stand alone server systems and the impressive price / performance ratio of commodity components. Small scale HPC systems, in the range from 16 to 64 processors, have enjoyed significant popularity and are an indispensable tool for the research community. Scaling up to hundreds and thousands of processors, however, has exposed operational issues, which include system availability and reliability. In this paper, we explore the impact of individual component reliability rates on the overall reliability of an HPC system. We derive a mathematical model for determining the failure rate of the system, the probability of failure of a job running on a subset of the system, and show how to design a reasonable queue structure to provide a reliable system over abroad job mix. We also explore the impact of reliability and queue structure on checkpoint intervals and recovery. Our results demonstrate that it is possible to design a reliable high performance computing system with very good operational reliability characteristics from a collection of moderately reliable components.	central processing unit;failure rate;mathematical model;scalability;server (computing);supercomputer;transaction processing system	Thomas J. Hacker;Zdzislaw Meglicki	2007		10.1145/1272366.1272373	reliability engineering;availability;parallel computing;real-time computing;simulation;reliability theory;operating system;failure rate;mathematical model;reliability;distributed computing	HPC	-17.47416937296171	50.33002926495739	42380
4f8b76259970a8dc4be051daa927dd2d9cadca88	exploiting i/o reordering and i/o interleaving to improve application launch performance	prefetching;application launch performance;i o reordering;i o interleaving	Application prefetchers improve application launch performance through either I/O reordering or I/O interleaving. However, there has been no proposal to combine the two techniques together, missing the opportunity for further optimization. We present a new application prefetching technique to take advantage of both the approaches. We evaluated our method with a set of applications to demonstrate that it reduces cold start application launch time by 50%, which is an improvement of 22% from the I/O reordering technique.	article 8 of the european convention on human rights;cpu cache;cold start;cooley–tukey fft algorithm;forward error correction;hard disk drive;input/output;launch time;mathematical optimization;on the fly	Yongsoo Joo;Sangsoo Park;Hyokyung Bahn	2017	TOS	10.1145/3024094	real-time computing;computer science;theoretical computer science;distributed computing	HPC	-7.695255586745266	50.19963967569005	42423
1e998d68dc840bf12c2feed71d8d892a3556a92f	multi-stage cpi stacks		CPI stacks are an intuitive way to visualize processor core performance bottlenecks. However, they often do not provide a full view on all bottlenecks, because stall events can occur concurrently. Typically one of the events is selected, which means information about the non-chosen stall events is lost. Furthermore, we show that there is no single correct CPI stack: stall penalties can be hidden, can overlap or can cause second-order effects, making total CPI more complex than just a sum of components. Instead of showing a single CPI stack, we propose to measure multiple CPI stacks during program execution: a CPI stack at each stage of the processor pipeline. This representation reveals all performance bottlenecks and provides a more complete view on the performance of an application. Multi-stage CPI stacks are easy to collect, which means that they can be included in a simulator with negligible slowdown, and that they can be included in the core hardware with limited overhead.	bottleneck (software);dynamic dispatch;electronic circuit simulation;experiment;multi-core processor;multistage amplifier;out-of-order execution;overhead (computing);superscalar processor	Stijn Eyerman;Wim Heirman;Kristof Du Bois;Ibrahim Hur	2018	IEEE Computer Architecture Letters	10.1109/LCA.2017.2761751	parallel computing;real-time computing;instruction pipeline;stack (abstract data type);computer science;multi-core processor;slowdown	Arch	-7.988943373080231	50.979570468853055	42584
c2465a0e16f4b4977ef37457be2efca6bcde9f9e	speedup for multi-level parallel computing	optimisation;e gustafson s law;e gustafsons law;parallel processing computational modeling abstracts optimization hardware programming resource management;resource management;multi level parallel computing;drntu engineering computer science and engineering;conference paper;computational modeling;abstracts;e amdahl s law;parallel processing optimisation;optimization;multi level parallel computing e amdahl s law e gustafsons law;programming;parallel processing;performance optimization multilevel parallel computing speedup fixed size speedup fixed time speedup communication latency high level abstract case e amdahl law e gustafson law;hardware	This paper studies the speedup for multi-level parallel computing. Two models of parallel speedup are considered, namely, fixed-size speedup and fixed-time speedup. Based on these two models, we start with the speedup formulation that takes into account uneven allocation and communication latency, and gives an accurate estimation. Next, we propose a high-level abstract case with providing a global view of possible performance enhancement, namely E-Amdahl's Law for fixed-size speedup and E-Gustafson's Law for fixed-time speedup. These two laws demonstrate seemingly opposing views about the speedup of multi-level parallel computing. Our study illustrates that they are not contradictory but unified and complementary. The results lead to a better understanding in the performance and scalability of multi-level parallel computing. The experimental results show that E-Amdahl's Law can be applied as a prediction model as well as a guide for the performance optimization in multi-level parallel computing.	amdahl's law;gustafson's law;high- and low-level;mathematical optimization;parallel computing;performance tuning;scalability;speedup	Shanjiang Tang;Bu-Sung Lee;Beixin Julie He	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.72	parallel processing;programming;amdahl's law;parallel computing;speedup;computer science;resource management;theoretical computer science;distributed computing;karp–flatt metric;computational model;algorithm	HPC	-6.321540381345205	48.04427101056834	42609
0b150686ffa4353704c1f5465bb1b0507a456f26	automatic on-chip memory minimization for data reuse	chip;parallel processing;logic;concurrent computing;engines;computer applications;field programmable gate arrays;acceleration	FPGA-based computing engines have become a promising option for the implementation of computationally intensive applications due to high flexibility and parallelism. However, one of the main obstacles to overcome when trying to accelerate an application on an FPGA is the bottleneck in off-chip communication, typically to large memories. Often it is known at compile-time that the same data item is accessed many times, and as a result can be loaded once from large off-chip RAM onto scarce on-chip RAM, alleviating this bottleneck. This paper addresses how to automatically derive an address mapping that reduces the size of the required on-chip memory for a given memory access pattern. Experimental results demonstrate that, in practice, our approach reduces on-chip storage requirements to the minimum, corresponding to a reduction in on-chip memory size of up to 40times (average 10times) for some benchmarks compared to a naive approach. At the same time, no clock period penalty or increase in control logic area compared to this approach is observed for these benchmarks.	array data structure;clock rate;compile time;compiler;control flow;data item;field-programmable gate array;memory access pattern;parallel computing;random-access memory;requirement	Qiang Liu;George A. Constantinides;Kostas Masselos;Peter Y. K. Cheung	2007	15th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM 2007)	10.1109/FCCM.2007.18	chip;uniform memory access;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;reading;real-time computing;sense amplifier;memory refresh;computer hardware;computer science;physical address;operating system;computer data storage;computer memory;non-volatile random-access memory;registered memory;sequential access memory;field-programmable gate array;computing with memory;memory map;memory management	EDA	-7.119247996717534	52.9078260604113	42646
7767608865c6b9ff42d1351ad060dc0d19f8e83a	workload-driven database optimization for cloud applications		The performance of modern data-intensive applications is closely related to the speed of data access. However, a physical database optimization by design is often infeasible, due to the presence of large databases and time-varying workloads. In this paper we introduce a novel methodology for physical database optimization which allows for a quick and dynamic selection of indexes through the analysis of database logs. The application of the technique to cloud applications, which use a pay-per-use model, results in immediate cost savings, due to the presence of elastic resources. In order to demonstrate the effectiveness of the approach, we present the case study Nuvola, a SaaS multitenant application for schools that is characterized by heavy workloads. Experimental results show that the proposed technique leads to a 52.1% reduction of query execution time for a given workload. A comparative analysis of database performance before and after the optimization is also performed through a M/M/1 queue model and the results are discussed.	b+ tree;b-tree;cloud computing;column (database);data access;data-intensive computing;database;database theory;hash table;materialized view;mathematical optimization;multitenancy;performance;preprocessor;qualitative comparative analysis;range query (data structures);relational database;run time (program lifecycle phase);sql;semiconductor industry;software as a service;trapezoidal rule	Claudia Diamantini;Alex Mircoli;Domenico Potena;Valentina Tempera;Matteo Moretti	2017	2017 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCS.2017.94	database tuning;physical data model;workload;database;cloud computing;data access;software as a service;multitenancy;queue;computer science	DB	-15.975758419529807	55.157837439032534	42720
5986d3bae8df7cf3c693042da8120a283419c477	register estimation in unscheduled dataflow graphs	register estimation;area estimation;estimation method;high level synthesis;scheduling;analytical method;probabilities	A method for register number estimation in unscheduled or partially scheduled dataflow graphs is presented. The strategy consists of studying the probability that an edge between two nodes crosses the boundary between two control steps, and its is based on a model that associates probabilities with the different scheduling alternatives of each node. These probabilities are computed by means of an analytic method that takes into account the distribution of operations in the dataflow graph and the hardware modules available in the library. The results highlight that the estimation method is very accurate becaused the error between the estimated value and the real value is always within a narrow margin.	data-flow analysis;dataflow;scheduling (computing)	R. Moreno;Román Hermida;Milagros Fernández	1996	ACM Trans. Design Autom. Electr. Syst.	10.1145/234860.234866	parallel computing;real-time computing;computer science;theoretical computer science;operating system;probability;high-level synthesis;scheduling	EDA	-4.670284102576662	51.81764972803569	42732
ddb26ba7f1b2abbed43321433438f2f28369e9c1	a case for scoped persist barriers in gpus		Two key trends in computing are evident --- emergence of GPU as a first-class compute element and emergence of byte-addressable nonvolatile memory technologies (NVRAM) as DRAM-supplement. GPUs and NVRAMs are likely to coexist in future systems. However, previous works have either focused on GPUs or on NVRAMs, in isolation. In this work, we investigate the enhancements necessary for a GPU to efficiently and correctly manipulate NVRAM-resident persistent data structures.  Specifically, we find that previously proposed CPU-centric persist barriers fall short for GPUs. We thus introduce the concept of scoped persist barriers that aligns with the hierarchical programming framework of GPUs. Scoped persist barriers enable GPU programmers to express which execution group (a.k.a., scope) a given persist barrier applies to. We demonstrate that: 1 use of narrower scope than algorithmically-required can lead to inconsistency of persistent data structure, and 2 use of wider scope than necessary leads to significant performance loss (e.g., 25% or more). Therefore, a future GPU can benefit from persist barriers with different scopes.	algorithm;byte addressing;central processing unit;coexist (image);dynamic random-access memory;emergence;graphics processing unit;memory barrier;non-volatile memory;non-volatile random-access memory;nonvolatile bios memory;persistent data structure;programmer	Dibakar Gope;Arkaprava Basu;Sooraj Puthoor;Mitesh Meswani	2018		10.1145/3180270.3180275	parallel computing;computer science;software framework;distributed computing;persistent data structure;non-volatile memory;non-volatile random-access memory	Arch	-13.657996096360778	49.216909230919484	42761
78b2e1a8941a09be11d8416a9d77cd719805ed41	off-loading application controlled data prefetching in numerical codes for multi-core processors	high performance computing;chip multiprocessing;data prefetching;cache optimisation;numerical codes;article;multi core processors	An important issue when designing numerical code in High Performance Computing is cache optimization in order to exploit the performance potential of a given target architecture. This includes techniques to improve memory access locality as well as prefetching. Inherent algorithm constrains often limit the first approach, which typically uses a blocking technique. While there exist automatic prefetching mechanisms in hardware and/or compilers, they can not complement blocking with additional prefetching. We provide an infrastructure for off-loading application controlled prefetching on a chip multiprocessor, allowing to further improve numerical code already optimized by standard cache optimization. Clear benefits are shown for real workloads on existing	algorithm;assembly language;blocking (computing);cpu cache;central processing unit;code;compiler;die (integrated circuit);dual independent map encoding;existential quantification;itanium;locality of reference;mathematical optimization;memory bound function;multi-core processor;multiprocessing;numerical analysis;parallel computing;programmer;solver;static program analysis	Josef Weidendorfer;Carsten Trinitis	2008	IJCSE	10.1504/IJCSE.2008.021109	multi-core processor;computer architecture;supercomputer;parallel computing;real-time computing;computer science;operating system	HPC	-7.918065965601976	48.2828068499912	42777
38b3e80c6bcf6f6a598cb23e2df5494b198c807b	a heterogeneous multiprocessor design and the distributed scheduling of its task group workload	processor suitability value;optimal schedule;heterogeneous multiprocessor design;processor deal;different group;processor suitability measure;dissimilar processor;task group workload;task group load;task group;different category;priority-based task management scheduling	A multiprocessor architecture is proposed which is based on the Multics concept of having all on-line information processor-addressible. All memory management is done by an intelligent paged virtual memory system, and each processor deals only with those segments relevant to its single executing program. The processors are chosen to have different implementations of a single system-wide instruction set and the problem is to effectively schedule different categories of programs, called task groups, on the dissimilar processors.  Average weighted instruction times for each task group on every processor are defined as task/processor suitability measures, and typical values are given for different groups of programs running on IBM 370 models. Through the use of linear programming techniques, an optimal schedule for any such multiprocessor is then defined for the static case where task group loads and task/processor suitability values are known in advance. A priority-based task management scheduling algorithm is then defined which uses the optimal schedule of the formal model as a parameter, and its performance is simulated.	algorithm;central processing unit;formal language;ibm system/370;information processor;linear programming;memory management;multics;multiprocessing;online and offline;openvms;scheduling (computing);simulation	Leslie Jill Miller	1982		10.1145/800048.801737	computer architecture;parallel computing;real-time computing;distributed memory;computer science;operating system;distributed computing;multiprocessor scheduling	Embedded	-13.815680345001791	60.273185121047526	42817
962ee2a9bbe2d523252a02349379ab4687889b18	quantitative analysis and systematic parametrization of a two-level real-time scheduler	utilization bounds quantitative analysis systematic parametrization two level real time scheduler embedded systems memory size computational power virtualization virtual machine monitors global scheduler temporal resources;working group;virtual machine monitor;global scheduler;virtual machines embedded systems scheduling;virtualization;real time;computational power;data mining;real time systems processor scheduling application software software maintenance embedded computing embedded system computer applications operating systems application virtualization virtual machine monitors;embedded system;virtual machine monitors;embedded systems;temporal resources;time factors;operating system;virtual machines;two level real time scheduler;periodic tasks;scheduling;real time scheduling;aerospace electronics;systematic parametrization;quantitative analysis;utilization bounds;rate monotonic;memory size;real time systems	The computational power of embedded systems have increased steadily during the recent years. In contrast to former approaches which allowed at least one application per computational node the memory size and computational power of today allows to host more than one application per node. Often applications are delivered by suppliers as a whole including the operating systems where the application tasks run on top. In this case virtualization is a common software approach to maintain isolation between different applications on the same computation system. Virtual machine monitors are able to divide the resources of a physical system into several logical subsystems. However, those monitors which are available today do not focus on the preservation of real-time properties. Consequently, our working group develops and investigates a two-level hierarchy of real-time schedulers, where a global scheduler assigns temporal resources to guest systems, while each subsystem has its own local scheduler for its application tasks. In this contribution, we focus on a formal investigation of the real-time properties of the two-level scheduling hierarchy. The starting points are independent applications building subsystems, each containing a set of tasks and a local scheduler, which have to be integrated and configured at the global scheduling level. Utilization bounds are derived unfolding the overhead of such an approach. Furthermore we propose systematic process for the computation of the task parameters for both levels of scheduling. Representatively the whole approach is applied to the rate monotonic assignment of priorities to tasks at the low scheduling level. For reasons of abstraction all these tasks are mapped into a single task proxy. This enables the global scheduler to treat all of its subsystems as periodic tasks allowing again for the application of the rate monotonic assignment of priorities to tasks.	central processing unit;coexist (image);computation;earliest deadline first scheduling;embedded system;microsoft outlook for mac;network switch;overhead (computing);proxy server;rate-monotonic scheduling;real-time clock;real-time operating system;real-time transcription;scheduling (computing);stepwise regression;two-level scheduling;unfolding (dsp implementation);virtual machine manager	Robert Kaiser;Dieter Zöbel	2009	2009 IEEE Conference on Emerging Technologies & Factory Automation	10.1109/ETFA.2009.5347110	fixed-priority pre-emptive scheduling;embedded system;real-time computing;virtualization;working group;computer science;quantitative analysis;virtual machine;operating system;distributed computing;hypervisor;scheduling	Embedded	-9.590091190022862	58.506264619706684	42887
b0e479c5dbc05b9eb4ed63747a5271c3e10ade1c	optimizing the datapath for key-value middleware with nvme ssds over rdma interconnects		In-memory key-value store is a crucial building block of large-scale web architecture. Given the growth of the data volume and the need for low-latency responses, cost-effective storage expansion and fast large-message processing are the major challenges. In this paper, we explore the design of key-value middleware that takes advantage of modern NVMe SSDs and RDMA interconnects to achieve high performance without excessive DRAM deployment. We propose an all-in-userland approach to improve the data plane efficiency. Both NVMe and RDMA are interfaced directly from the user-space for effective data access and tailored data management. We present a low-latency storage extension framework based on NVMe and a new design of JVM-aware Memcache protocol based on RDMA. To further accelerate large-message transfer, we provide a hybrid communication protocol fusing Eager and Rendezvous schemas, and a united I/O staging approach to achieve maximum latency hiding through pipelining. As the benchmarking results indicate, with the non-negligible JVM overhead taken into account, our solution obtains comparable communication performance with the RDMA-Memcached released by the OSU. For SSD-involved operations, the latency decreases by up to 31% compared to the kernel-based I/O processing.	attribute–value pair;communications protocol;data access;datapath;disk staging;dynamic random-access memory;electrical connection;forwarding plane;input/output;key-value database;memcached;middleware;optimizing compiler;overhead (computing);pipeline (computing);remote direct memory access;software deployment;solid-state drive;usb flash drive;user space	Zhongqi An;Zhengyu Zhang;Qiang Li;Jing Xing;Hao Du;Zhan Wang;Zhigang Huo;Jie Ma	2017	2017 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2017.69	real-time computing;parallel computing;communications protocol;computer science;nvm express;latency (engineering);forwarding plane;pipeline (computing);middleware;server;remote direct memory access	HPC	-14.365123569432274	53.368394685287356	42899
7b87cc874289289ac0360b0de4a0383dccf28516	efficient logging of metadata using nvram for nand flash based file system	linux os;nand circuits;circuit reliability;metadata garbage reduction;random-access storage;nand flash based file system;nvram;nonvolatile memory;metadata synchronously logging modification;linux;metadata synchronous writing;log scanning;reliability;meta data;flash memories;writing;system monitoring;non volatile memory	The synchronous writing of metadata for flash file system generates excessive garbage. We propose the scheme for merging the writing of metadata so as to reduce the garbage of the NAND flash while ensuring file system consistency. The proposed scheme uses the non-volatile memory for synchronously logging modifications of the metadata. The last modified metadata can be recovered from a crash, after scanning logs in the non-volatile memory. The evaluation results show that the proposed scheme greatly reduced the overall application time and the number of written pages across various benchmarks, compared to the conventional flash file system.	adobe flash;benchmark (computing);computer data storage;flash file system;flash memory;non-volatile memory;non-volatile random-access memory;overhead (computing);volatile memory	Chul Lee;Seung-Ho Lim	2012	IEEE Transactions on Consumer Electronics	10.1109/ICCE.2012.6161940	flash file system;parallel computing;non-volatile memory;computer hardware;computer science;operating system;journaling file system;database	OS	-12.084360098856457	54.004050783800096	43058
6d2db3d64131cd4a0f5bb1de46581f36046385f2	execution migration in a heterogeneous-isa chip multiprocessor	phase change;heterogeneous cmp;energy efficient;chip multiprocessor;thread migration;binary translation	Prior research has shown that single-ISA heterogeneous chip multiprocessors have the potential for greater performance and energy efficiency than homogeneous CMPs. However, restricting the cores to a single ISA removes an important opportunity for greater heterogeneity. To take full advantage of a heterogeneous-ISA CMP, however, we must be able to migrate execution among heterogeneous cores in order to adapt to program phase changes and changing external conditions (e.g., system power state).  This paper explores migration on heterogeneous-ISA CMPs. This is non-trivial because program state is kept in an architecture-specific form; therefore, state transformation is necessary for migration. To keep migration cost low, the amount of state that requires transformation must be minimized. This work identifies large portions of program state whose form is not critical for performance; the compiler is modified to produce programs that keep most of their state in an architecture-neutral form so that only a small number of data items must be repositioned and no pointers need to be changed. The result is low migration cost with minimal sacrifice of non-migration performance.  Additionally, this work leverages binary translation to enable instantaneous migration. When migration is requested, the program is immediately migrated to a different core where binary translation runs for a short time until a function call is reached, at which point program state is transformed and execution continues natively on the new core.  This system can tolerate migrations as often as every 100 ms and still retain 95% of the performance of a system that does not do, or support, migration.	arm architecture;binary translation;compiler;ibm notes;industry standard architecture;maxima and minima;multi-core processor;multiprocessing;overhead (computing);pointer (computer programming);state (computer science);system migration;total loss	Matthew DeVuyst;Ashish Venkat;Dean M. Tullsen	2012		10.1145/2150976.2151004	computer architecture;parallel computing;real-time computing;computer hardware;computer science;operating system;efficient energy use;phase change	Arch	-5.433920302424343	53.35954322121883	43131
ec875f6c63c855e54a7efb9ed05fe9003308068a	a fine-grain time-sharing time warp system	optimistic synchronization	Several techniques have been proposed to improve the performance of Parallel Discrete Event Simulation platforms relying on the Time Warp (optimistic) synchronization protocol. Among them we can mention optimized approaches for state restore, as well as techniques for load balancing or (dynamically) controlling the speculation degree, the latter being specifically targeted at reducing the incidence of causality errors leading to waste of computation. However, in state-of-the-art Time Warp systems, events’ processing is not preemptable, which may prevent the possibility to promptly react to the injection of higher priority (say, lower timestamp) events. Delaying the processing of these events may, in turn, give rise to higher incidence of incorrect speculation. In this article, we present the design and realization of a fine-grain time-sharing Time Warp system, to be run on multi-core Linux machines, which makes systematic use of event preemption in order to dynamically reassign the CPU to higher priority events/tasks. Our proposal is based on a truly dual mode execution, application versus platform, which includes a timer-interrupt-based support for bringing control back to platform mode for possible CPU reassignment according to very fine grain periods. The latter facility is offered by an ad-hoc timer-interrupt management module for Linux, which we release, together with the overall time-sharing support, within the open source ROOT-Sim platform. An experimental assessment based on the classical PHOLD benchmark and two real-world models is presented, which shows how our proposal effectively leads to the reduction of the incidence of causality errors, especially when running with higher degrees of parallelism.	benchmark (computing);block matrix pseudoinverse;causality;central processing unit;computation;computer simulation;dynamic time warping;experiment;hoc (programming language);incidence matrix;linux;load balancing (computing);multi-core processor;open-source software;parallel computing;preemption (computing);root;rollback (data management);root-finding algorithm;speculative execution;time-sharing;timer;x86;x86-64	Alessandro Pellegrini;Francesco Quaglia	2017	ACM Trans. Model. Comput. Simul.	10.1145/3013528	parallel computing;real-time computing;simulation;computer science;mathematics;distributed computing	OS	-15.073801486490398	48.128650707508726	43136
a6004f0928cd51b5a6999cc2287b61dd7336298f	a mini-computer network for support of real time research	networks;shared resources;real time;distributed computing;satisfiability;computer network;real time computers;operating system;real time computing;mini computers	A star-shaped network of mini-computers provides all remote computers access of commonly used peripherals and the University's CDC 6400. The key hardware feature, independent of computer type, is asynchronous serial communication at 153,000 baud with character rate dictated by the receiving computer. Interconnection hardware and software overhead is minimal. The key software feature is the loose connection of near homogeneous machines. Real-time constraints are satisfied by remote computers with idle machines potentially available as a distributed intelligence resource to support additional computer types. Minimal computers may be supported to run unmodified, paper-tape oriented programs using a transparent monitor. Computers with adequate memory may be supported to run mass memory operating systems using Network Central resources. The cost per computer is less than that for paper-tape equipment, while only under heavy load conditions does performance degrade to paper-tape speeds.	asynchronous serial communication;cdc 6400;classes of computers;computer;interconnection;minicomputer;operating system;overhead (computing);peripheral;real-time clock;real-time computing;software feature	William J. Lennon	1974		10.1145/1408800.1408861	computing;parallel computing;real-time computing;computer science;distributed computing	Arch	-12.886334719509303	49.86367258915642	43139
b59c6e2eca208e99a18d2778d99cccd1792835fc	simd-scan: ultra fast in-memory table scan using on-chip vector processing units	processor architecture;database system;multi core processor;query processing;scientific data;data management;data mining;hardware architecture;chip;superscalar processor;streaming simd extensions;power consumption;data warehouse;high performance;data structure	The availability of huge system memory, even on standard servers, generated a lot of interest in main memory database engines. In data warehouse systems, highly compressed columnoriented data structures are quite prominent. In order to scale with the data volume and the system load, many of these systems are highly distributed with a shared-nothing approach. The fundamental principle of all systems is a full table scan over one or multiple compressed columns. Recent research proposed different techniques to speedup table scans like intelligent compression or using an additional hardware such as graphic cards or FPGAs. In this paper, we show that utilizing the embedded Vector Processing Units (VPUs) found in standard superscalar processors can speed up the performance of mainmemory full table scan by factors. This is achieved without changing the hardware architecture and thereby without additional power consumption. Moreover, as on-chip VPUs directly access the system’s RAM, no additional costly copy operations are needed for using the new SIMD-scan approach in standard main memory database engines. Therefore, we propose this scan approach to be used as the standard scan operator for compressed column-oriented main memory storage. We then discuss how well our solution scales with the number of processor cores; consequently, to what degree it can be applied in multi-threaded environments. To verify the feasibility of our approach, we implemented the proposed techniques on a modern Intel multicore processor using Intel® Streaming SIMD Extensions (Intel® SSE). In addition, we integrated the new SIMD-scan approach into SAP® Netweaver® Business Warehouse Accelerator. We conclude with describing the performance benefits of using our approach for processing and scanning compressed data using VPUs in column-oriented main memory database systems. 1. I TRODUCTIO Computer technology is continually developing, with abiding rapid improvement in processor architecture, disk storage, and main memory capacity. At the same time, the massive increase in data volumes has created a demand for high performance data management capabilities. This is reflected by the data-intensive query processing tasks like OLAP, data mining, and scientific data analysis. These tasks rely on powerful hardware resources and require optimized software solutions for processing the huge amount of data with high performance. As the system memory gets larger and cheaper, database systems started therefore to evolve from disk-based to memorybased operation (and storage). As a result, main memory is becoming a critical resource. As in disk-based database engines, data compression techniques are considered as one way to handle this new main memory bottleneck. Previous research showed that the performance of relational disk-based database system can be increased by extending the storage manager, the query execution engine, and the query optimizer to handle compressed data [1]. In main memory column-store database systems like SAP® Netweaver® Business Warehouse Accelerator (BWA), relational tables are completely loaded into memory and are stored columnwise. In order to save RAM and to improve access rates, the inmemory data structures are highly compressed. This is achieved by using different variants of Light Weight Compression (LWC) techniques like run-length encoding or multiple version of fixedlength encoding. In SAP® Netweaver® BWA, the default compression mechanism is dictionary compression with bitwise encoding of columns. Here, all distinct values of a column are extracted, sorted, and inserted into a dictionary. The column itself keeps only references to the dictionary, where each reference value is stored using as many bits as needed for fixed-length encoding. While most access functions work directly on compressed data by implicit decompression, operations like projection have to materialize the data, and therefore, explicitly decompress it. Here, high performance is achieved by making optimal use of the CPU local cache. This is accomplished by processing small data chunks in one execution step. Hence, data decompression is becoming a significant part of the query execution. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Database Endowment. To copy otherwise, or to republish, to post on servers or to redistribute to lists, requires a fee and/or special permissions from the publisher, ACM. VLDB ’09, August 24-28, 2009, Lyon, France. Copyright 2009 VLDB Endowment, ACM 000-0-00000-000-0/00/00. In database engines similar to SAP® Netweaver® BWA, the central low-level access function is the main memory full table scan over highly compressed data (table columns). This operation requires lots of calculations and is CPU-intensive. The need for decompression increases the CPU-bound behavior, whereas a scan over uncompressed data is more memory-bus-bound. With the arrival of multi-core processors, operating on compressed data is continuously becoming cheaper as CPU processing rates are increasing faster than data-access bandwidth rates [2]. As a result, more sophisticated compression algorithms are being used while full table scan operations are shifting from being IO-bound to CPU-bound. In addition, the data in the system’s memory is being compressed as much as possible. This created a new exciting research field focusing on optimizing table scan operations for multi-core processors using different parallelization techniques. However, even though multi-core processors offer rich Simultaneous Multi-Processing (SMP) experience, their fast Vector Processing Units (VPUs) have not been fully exploited to vectorize and streamline table scan operations. In this paper, we introduce a novel SIMD approach for inmemory fast table scan operations working on compressed table columns. We utilize the latest SIMD capabilities of each core in superscalar multi-core processors to efficiently decompress inmemory table columns and search for a scan value with a considerably lower latency. With this approach, we extend the parallelization of table scan operations that has been limited to multithreading on the task and data level. SIMD, firstly classified by Flynn [3][4], represents a vector processing model providing instruction level parallelism. SIMD forms an important extension to modern processors architectures and provides the ability for a single instruction stream to process multiple data streams simultaneously. Figure 1 shows the SIMD execution model. Figure 1. SIMD execution model: In (a) scalar mode: one operation produces one result. In (b) SIMD mode: one operation produces multiple results	adobe streamline;algorithm;bitwise operation;cpu cache;central processing unit;column (database);computer data storage;data compression;data mining;data structure;data-intensive computing;dictionary coder;disk storage;embedded system;emoticon;field-programmable gate array;full table scan;graphics processing unit;high- and low-level;i/o bound;in-memory database;instruction-level parallelism;list of sequence alignment software;load (computing);mathematical optimization;multi-core processor;multiprocessing;multithreading (computer architecture);non-volatile random-access memory;online analytical processing;parallel computing;query optimization;run-length encoding;sap hana;sap netweaver;sap r/3;shared nothing architecture;speedup;streaming simd extensions;superscalar processor;thread (computing);vldb;vector processor;video card;von neumann architecture	Thomas Willhalm;Nicolae Popovici;Yazan Boshmaf;Hasso Plattner;Alexander Zeier;Jan Schaffner	2009	PVLDB	10.14778/1687627.1687671	chip;multi-core processor;parallel computing;real-time computing;data structure;computer hardware;microarchitecture;data management;computer science;data warehouse;data mining;hardware architecture;database;data	DB	-11.152299473674425	52.464602665274064	43298
7a12cc2cf5582406505fc063713a281bf773fe29	survey of energy-cognizant scheduling techniques	energy conservation;cooperative resource sharing survey shared resource contention thread level scheduling power aware scheduling thermal effects asymmetric multicore processors;power aware scheduling;processor scheduling;cost reduction;thread level scheduling;shared resource contention;power aware computing;energy consumption;heuristic algorithms;battery management systems;cooperative resource sharing;multiprocessing systems;asymmetric multicore processors;survey;thermal management;operating systems computers;thermal management energy consumption heuristic algorithms processor scheduling hardware instruction sets;survey energy cognizant scheduling technique energy saving server farm cooling cost minimisation mobile device prolong battery life hardware designer os scheduler energy efficient computation energy minimization asymmetric multicore architecture;instruction sets;hardware;thermal effects;processor scheduling battery management systems cost reduction energy conservation multiprocessing systems operating systems computers power aware computing	Execution time is no longer the only metric by which computational systems are judged. In fact, explicitly sacrificing raw performance in exchange for energy savings is becoming a common trend in environments ranging from large server farms attempting to minimize cooling costs to mobile devices trying to prolong battery life. Hardware designers, well aware of these trends, include capabilities like DVFS (to throttle core frequency) into almost all modern systems. However, hardware capabilities on their own are insufficient and must be paired with other logic to decide if, when, and by how much to apply energy-minimizing techniques while still meeting performance goals. One obvious choice is to place this logic into the OS scheduler. This choice is particularly attractive due to the relative simplicity, low cost, and low risk associated with modifying only the scheduler part of the OS. Herein we survey the vast field of research on energy-cognizant schedulers. We discuss scheduling techniques to perform energy-efficient computation. We further explore how the energy-cognizant scheduler's role has been extended beyond simple energy minimization to also include related issues like the avoidance of negative thermal effects as well as addressing asymmetric multicore architectures.	approximation algorithm;central processing unit;clock rate;compiler;computation;computer cooling;dynamic voltage scaling;energy minimization;information flow;interaction;mobile device;multi-core processor;operating system;scheduling (computing);server (computing);server farm;synergy;time-sharing	Sergey Zhuravlev;Juan Carlos Saez;Sergey Blagodurov;Alexandra Fedorova;Manuel Prieto	2013	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2012.20	embedded system;parallel computing;real-time computing;thermal management of electronic devices and systems;energy conservation;computer science;operating system;instruction set;database;distributed computing;computer network	Arch	-5.560143221599905	56.243976316293534	43409
6bfb5449a5827d5725902d0680850e68b1e96db8	optimizing dynamic resource allocation	high performance computing;stochastic dynamic programming;polyhedral model;automatic parallelization	We present a formulation, solution method, and program acceleration techniques for two dynamic control scenarios, both with the common goal of optimizing resource allocations. These approaches allocate resources in a non-myopic way, accounting for long-term impacts of current control decisions via nominal belief-state optimization (NBO). In both scenarios, the solution techniques are parallelized for reduced execution time. A novel aspect is included in the second scenario: dynamically allocating the computational resources in an online fashion which is made possible through constant aspect ratio tiling (CART).	algorithm;automatic programming;cpu cache;code generation (compiler);computation;computational resource;endianness;mathematical optimization;optimizing compiler;parallel computing;partially observable markov decision process;run time (program lifecycle phase);scheduling (computing);tiling window manager	Lucas W. Krakow;Louis Rabiet;Yun Zou;Guillaume Iooss;Edwin K. P. Chong;Sanjay V. Rajopadhye	2014		10.1016/j.procs.2014.05.115	mathematical optimization;real-time computing;simulation;computer science	HPC	-13.738587916020123	58.25404935495068	43439
215bdcdb0d8ebc8837d5ef37104378a0fe676c17	parallelism-aware request scheduling for mems-based storage devices	micromechanical devices flash memory costs scheduling algorithm processor scheduling computer science parallel processing aging delay microelectromechanical systems;flash memory;microelectromechanical systems;mobile device;processor scheduling;aging;physical characteristic;scheduling algorithm;micromechanical devices;simulation study;computer science;low power consumption;parallel processing	MEMS-based storage is being developed as a new storage media. Due to its attractive features such as high-bandwidth, low-power consumption, and low cost, MEMS storage is anticipated to be used for a wide range of applications. However, MEMS storage has vastly different physical characteristics compared to a traditional disk. First, MEMS storage has thousands of heads that can be activated simultaneously. Second, the media of MEMS storage is a square structure which is different from the platter structure of disks. This paper presents a new request scheduling algorithm for MEMS storage that makes use of the aforementioned characteristics. This new algorithm considers the parallelism of MEMS storage as well as the seek time on the two dimensional square structure. We then extend this algorithm to consider the aging factor so that starvation resistance is improved. Simulation studies show that the proposed algorithms improve the performance of MEMS storage by up to 39.2% in terms of the average response time and 62.4% in terms of starvation resistance compared to the widely acknowledged SPTF (Shortest Positioning Time First) algorithm.	algorithm;hard disk drive performance characteristics;low-power broadcasting;microelectromechanical systems;parallel computing;response time (technology);scheduling (computing);simulation	Soyoon Lee;Hyokyung Bahn;Sam H. Noh	2006	14th IEEE International Symposium on Modeling, Analysis, and Simulation	10.1109/MASCOTS.2006.37	fair-share scheduling;embedded system;parallel processing;real-time computing;computer hardware;telecommunications;computer science;operating system;mobile device;microelectromechanical systems;scheduling	Embedded	-10.97936164715063	56.599967700526264	43443
d67bdce1d52133d7902302edd4c5b343b54254d2	cache interference phenomena	modeling technique;performance evaluation;optimization technique;data locality;analytical method;numerical codes;memory hierarchy;modeling;cache interferences or conflicts	The impact of cache interferences on program performance (particularly numerical codes, which heavily use the memory hierarchy) remains unknown. The general knowledge is that cache interferences are highly irregular, in terms of occurrence and intensity. In this paper, the different types of cache interferences that can occur in numerical loop nests are identified. An analytical method is developed for detecting the occurrence of interferences and, more important, for computing the number of cache misses due to interferences. Simulations and experiments on real machines show that the model is generally accurate and that most interference phenomena are captured. Experiments also show that cache interferences can be intense and frequent. Certain parameters such as array base addresses or dimensions can have a strong impact on the occurrence of interferences. Modifying these parameters only can induce global execution time variations of 30% and more. Applications of these modeling techniques are numerous and range from performance evaluation and prediction to enhancement of data locality optimizations techniques.	acm/ieee supercomputing conference;algorithm;base address;cpu cache;cache (computing);cobham's thesis;code;compiler;computer performance;computer simulation;concrete mathematics: a foundation for computer science;denial-of-service attack;experiment;graham scan;input/output base address;interference (communication);locality of reference;mathematical optimization;memory hierarchy;numerical analysis;performance evaluation;refinement (computing);run time (program lifecycle phase);sensor	Olivier Temam;Christine Fricker;William Jalby	1994		10.1145/183018.183047	parallel computing;real-time computing;systems modeling;computer science;theoretical computer science	HPC	-9.769672315611013	49.3126720389581	43812
a70d1337045979737089718c9710adca69ab0853	senfis: a sensor node file system for increasing the scalability and reliability of wireless sensor networks applications	estensibilidad;distributed memory;distributed system;flash memory;red sin hilo;memoire flash;reseau capteur;gestion memoire;fiabilidad;reliability;systeme reparti;distributed database;execution time;nivellement;reseau sans fil;integration information;gestion archivos;memoria compartida;levelling;storage management;wireless network;interrogation base donnee;base repartida dato;interrogacion base datos;gestion fichier;stockage donnee;file management;wireless sensor network;captador medida;data storage;gestion memoria;information integration;measurement sensor;red sensores;sistema repartido;capteur mesure;base de donnees repartie;energy consumption;file system;fiabilite;integracion informacion;sensor array;consommation energie;sensor nodes;almacenamiento datos;temps execution;extensibilite;scalability;memoire repartie;tiempo ejecucion;nivelacion;memoria flash;database query;wireless sensor networks;file systems;consumo energia;distributed query processor	In the last years the Wireless Sensor Networks’ (WSN) technology has been increasingly employed in various application domains. The extensive use of WSN posed new challenges in terms of both scalability and reliability. This paper proposes Sensor Node File System (SENFIS), a novel file system for sensor nodes, which addresses both scalability and reliability concerns. SENFIS can be mainly used in two broad scenarios. First, it can transparently be employed as a permanent storage for distributed TinyDB queries, in order to increase the reliability and scalability. Second, it can be directly used by a WSN application for permanent storage of data on the WSN nodes. The experimental section shows that SENFIS implementation makes an efficient use of resources in terms of energy consumption, memory footprint, flash wear levelling, while achieving execution times similarly with existing WSN file systems.	application domain;data access;eeprom;memory footprint;persistence (computer science);random-access memory;scalability;sensor node;tinyos;wear leveling	Soledad Escolar;Florin Isaila;Alejandro Calderón;Luis Miguel Sánchez;David E. Singh	2009	The Journal of Supercomputing	10.1007/s11227-009-0275-8	embedded system;wireless sensor network;telecommunications;computer science;operating system;key distribution in wireless sensor networks;distributed database	Embedded	-9.564457597068339	55.90573491199611	43912
d4f8d2b9f7fad337d9ad22a92555e159c9543836	tucana: design and implementation of a fast and efficient scale-up key-value store		Given current technology trends towards fast storage devices and the need for increasing data processing density, it is important to examine key-value store designs that reduce CPU overhead. However, current key-value stores are still designed mostly for hard disk drives (HDDs) that exhibit a large difference between sequential and random access performance, and they incur high CPU overheads. In this paper we present Tucana, a feature-rich keyvalue store that achieves low CPU overhead. Our design starts from a Bε –tree approach to maintain asymptotic properties for inserts and uses three techniques to reduce overheads: copy-on-write, private allocation, and direct device management. In our design we favor choices that reduce overheads compared to sequential device accesses and large I/Os. We evaluate our approach against RocksDB, a stateof-the-art key-value store, and show that our approach improves CPU efficiency by up to 9.2× and an average of 6× across all workloads we examine. In addition, Tucana improves throughput compared to RocksDB by up to 7×. Then, we use Tucana to replace the storage engine of HBase and compare it to native HBase and Cassandra two of the most popular NoSQL stores. Our results show that Tucana outperforms HBase by up to 8× in CPU efficiency and by up to 10× in throughput. Tucana’s improvements are even higher when compared to Cassandra.	apache cassandra;apache hbase;attribute–value pair;central processing unit;concurrency (computer science);copy-on-write;database engine;hard disk drive;in-memory database;input/output;key-value database;nosql;overhead (computing);persistence (computer science);random access;rocksdb;software feature;throughput	Anastasios Papagiannis;Giorgos Saloustros;Pilar González-Férez;Angelos Bilas	2016			computer science;throughput;direct device;nosql;real-time computing;operating system;embedded system;associative array;overhead (business);data processing;minimum efficient scale;random access	OS	-14.051355636569912	53.47071840611849	43938
151da1c6862b86433b89afe0318b7227658ca5fc	exploit dynamic voltage and frequency scaling for soc test scheduling under thermal constraints	job shop scheduling;thermal management packaging integer programming integrated circuit testing linear programming scheduling system on chip;system on chip;thermal aware;soc test scheduling dvfs thermal aware;power density itc 02 benchmarks milp model thermal constraints scaling factor thermal aware test scheduling modern ic devices dvfs dynamic voltage and frequency scaling tat test application time soc system on chip integrated circuits thermal hotspots;soc test scheduling;temperature;dvfs;benchmark testing;temperature system on chip job shop scheduling benchmark testing cooling;cooling	Increasing power density and thermal hotspots has become a major problem for integrated circuits. The problem is exacerbated when applying tests to a System-on-Chip (SoC). Running a test individually may exceed the given temperature threshold. So scheduling tests to reduce the test application time (TAT) while keep the cores thermally safe has become a key issue. Dynamic Voltage and Frequency Scaling (DVFS) has been widely used in modern IC devices to control power and temperature. We exploit such features for thermal-aware test scheduling and propose a method to efficiently determine the scaling factor which leads to optimized TAT without violating the thermal constraints. The proposed method can also be used to efficiently calculate the maximum temperature certain tests can achieve when DVFS is applied. After formulating the problem into an MILP model, experimental results on ITC'02 benchmarks showed that DVFS can be used to deal with power intensive tests efficiently and exploiting such features can achieve up to 16.37% reduction of TAT.	algorithm;dynamic frequency scaling;dynamic voltage scaling;heuristic;image scaling;integrated circuit;overhead (computing);prospective search;scheduling (computing);system on a chip;thermal grease	Li Ling;Jianhui Jiang	2014	2014 IEEE 23rd Asian Test Symposium	10.1109/ATS.2014.36	system on a chip;fair-share scheduling;embedded system;job shop scheduling;benchmark;parallel computing;real-time computing;temperature;computer science;engineering;operating system	EDA	-4.996182382852635	57.78414532533785	43984
49d5943c2b437fcbaf10d6783d45cdf551d18a59	implicit vs. explicit resource allocation in smt processors	resource allocation	In a simultaneous multithreaded (SMT) architecture, the front end of a superscalar is adapted in order to be able to fetch from several threads while the back end is shared among the threads. In this paper, we describe different resource sharing models in SMT processors. We show that explicit resource allocation can improve SMT performance. In addition, it enables SMTs to solve other QoS requirements, not realizable before.	central processing unit;operating system;quality of service;requirement;simultaneous multithreading;superscalar processor	Francisco J. Cazorla;Peter M. W. Knijnenburg;Rizos Sakellariou;Enrique Fernández;Alex Ramírez;Mateo Valero	2004	Euromicro Symposium on Digital System Design, 2004. DSD 2004.	10.1109/DSD.2004.1333257	resource allocation;computer science;static memory allocation	Embedded	-8.526599559780419	57.79794340828221	43995
3a97dd1759121e4b11ea6d47a1972dff5b90a47d	portable parallel test generation for sequential circuits	sequential circuit;portable parallel test generation;fault coverage;message passing;sequential circuits;parallel processing;shared memory	We report a new parallel test generation algorithm, ProperTEST, for sequential circuits that is portable across a range ojMIMD parallel architectures. It uses prioritized execution to ensure consistent speedups as the number of processors is increased. This consistency is achieved without loss in fault coverage with increase in number of processors. This also enables the use of parallel processing to improve the fault coverage when the execution time is bounded. Results on ISCAS 89 benchmark programs are provided on a shared memory machine, a message passin.g machine and a net work of Sun workstations. ProperTEST was run unchanged 011 these different architectures.	algorithm;benchmark (computing);central processing unit;fault coverage;parallel computing;run time (program lifecycle phase);shared memory;workstation	Balkrishna Ramkumar;Prithviraj Banerjee	1992		10.1145/304032.304094	shared memory;parallel processing;computer architecture;parallel computing;message passing;real-time computing;fault coverage;computer science;operating system;sequential logic	EDA	-12.578954981908284	47.44969190069283	44114
9fee77a57846ab2bb659e99407ade977bf31bf84	dataflow in matlab: algorithm acceleration through concurrency	concurrent computing;concurrency computers data flow computing electronic data interchange mathematics computing microprocessor chips parallel architectures pipeline processing radio receivers software radio telecommunication computing wireless lan;matlab parallel processing instruction sets throughput concurrent computing computer architecture real time systems;computer architecture;data flow architecture thread level pipelining matlab functions concurrency support matlab implementations data streaming data dependent operations cpu core counts performance analysis high level throughput gains 802 11a receiver software defined radio hardware user perspective matlab language data transfer;matlab;parallel processing;instruction sets;throughput;real time systems;software defined radio dataflow hpc	In this paper, we present a novel Data-Flow architecture for MATLAB. This architecture provides thread-level pipelining of MATLAB functions as well as general concurrency support. The proposed approach yields a significant speedup of current MATLAB implementations that rely on streaming data or employ data-dependent operations. Following the development of increased CPU core counts, this proposed framework will provide additional benefit only as this trend continues. A performance analysis of the proposed framework is performed, and we are able to demonstrate high-level throughput gains of specific applications. Discussions on implementation guidelines, as well as limitations of the framework, are proposed in this paper. Through the use of this tool, we have demonstrated a 802.11a receiver employing Software-Defined Radio hardware running in real time. From the user’s perspective, this tool requires interaction only from the MATLAB language, handling all threading and data transfer without user intervention.	algorithm;central processing unit;concurrency (computer science);data dependency;dataflow architecture;high- and low-level;matlab;pipeline (computing);profiling (computer programming);speedup;stream (computing);thread (computing);throughput	Travis F. Collins;Alexander M. Wyglinski	2017	IEEE Access	10.1109/ACCESS.2017.2672200	parallel processing;throughput;computer architecture;parallel computing;real-time computing;concurrent computing;computer science;operating system;instruction set	Visualization	-6.724679226169907	47.528587239175714	44120
7c7d816fb76a96004c0af66f28e060479fbaedf9	cregs: a new kind of memory for referencing arrays and pointers	computer architecture;data structures;cregs;compiler analysis;memory;optimization techniques;pointers;referencing arrays	Often, pointer and subscripted array references touch memory locations for which there are several possible aliases, hence these references cannot be made from registers. Although conventional caches can increase performance somewhat, they do not provide many of the benefits of registers, and do not permit the compiler to perform many optimizations associated with register references. The CReg (pronounced “C-Reg”) mechanism combines the hardware structures of cache and registers to create a new kind of memory structure, which can be used either as processor registers or as a replacement for conventional cache memory. By permitting aliased names to be grouped together, CRegs resolve ambiguous alias problems in hardware, resulting in more efficient execution than even the combination of conventional registers and cache can provide. This paper discusses both the conceptual CReg hardware structure and the compiler analysis and optimization techniques to manage that structure.	cpu cache;hardware random number generator;mathematical optimization;optimizing compiler;pointer (computer programming);processor register;register file;touch memory	Henry G. Dietz;Chi-Hung Chi	1988			failure analysis;computer architecture;parallel computing;real-time computing;memory type range register;pointer;cpu cache;data structure;computer science;operating system;optimizing compiler;pipeline transport;processor register;memory;programming language;register allocation;bandwidth	Arch	-7.857119380735902	50.404045382976264	44233
8a85cae110b5a4026f3f8cd541e4a9166e6700df	remote reference counting	distributed system;comptage;reference counting;gestion memoire;systeme reparti;memory management;garbage collectors;storage management;ramasse miettes;distributed programs;langage java;contaje;garbage collection;gestion memoria;sistema repartido;network traffic;counting;communication cost;automatic memory management;distributed shared memory;java language	Automatic storage management is important in distributed programming environments because programmer controlled reclamation is highly prone to errors. The main disadvantage of existing memory reclamation schemes is their high communication cost, a cost that is proportional to the number of pointer operations. We present a novel, efficient distributed garbage collection (GC) algorithm called Remote Reference Counting (RRC). This algorithm has small network traffic and computation overheads, essentially eliminating all communication when there is no active collection. The efficiency is provided by locality of RRC computations. RRC sends an average of only two to three messages from every node in order to reach global consensus on reclamation of any object. Moreover, no GC messages are sent as a result of a pointer operation. Messages for different objects can be combined and piggybacked on non-GC messages, trading promptness of garbage reclamation for additional communication efficiency. The low cost of local garbage identification is provided by a reference counting strategy which is independent of the heap size. RRC works correctly for asynchronous environments where messages may experience arbitrary delays on the way to their destinations. In addition, when applied in granularity of pages, the communication and memory overhead is not inflated when the average allocation size is small, and the memory reorganization required due to the GC operations is simplified. RRC was implemented as a WIN32 library on Windows-NT on top of a distributed shared memory system called millipage. We confirm RRC efficiency by providing performance results measured on a set of widely accepted benchmarks.		Dmitry Kogan;Assaf Schuster	2000	J. Parallel Distrib. Comput.	10.1006/jpdc.2000.1653	distributed shared memory;reference counting;parallel computing;real-time computing;telecommunications;computer science;operating system;distributed computing;garbage collection;programming language;counting;algorithm;computer network;memory management	HPC	-17.054279990935893	46.542296108127786	44253
44ecee1b36a03ae16f5d59dc2d3a7586120eb8fd	on the energy and performance of commodity hardware transactional memory	energy;performance evaluation;transactional memory	The advent of multi-core architectures has brought concurrent programming to the forefront of software development. In this context, Transactional Memory (TM) has gained increasing popularity as a simpler, attractive alternative to traditional lock-based synchronization. The recent integration of Hardware TM (HTM) in the last generation of Intel commodity processors turned TM into a mainstream technology, raising a number of questions on its future and that of concurrent programming.  To evaluate the potential impact of Intel's HTM, we conducted the largest study on TM to date, comparing different locking techniques, hardware and software TMs, as well as different combinations of these mechanisms, from the dual perspective of performance and power consumption. As a result we perform a workload characterization, to help programmers better exploit the currently available TM facilities, and identify important research directions.	central processing unit;commodity computing;concurrent computing;html;lock (computer science);microsoft forefront;multi-core processor;programmer;software development;transactional memory	Nuno Diegues;Paolo Romano;Luís E. T. Rodrigues	2014		10.1145/2591971.2592030	embedded system;transactional memory;parallel computing;real-time computing;energy;computer science;operating system	Arch	-8.023183141820606	47.07755282134973	44450
b583f558fff605c9d7092205f6bcf67f34ff216a	rainbow: efficient memory dependence recording with high replay parallelism for relaxed memory model	multi threading;europe abstracts;software fault tolerance;software fault tolerance benchmark testing multi threading parallel architectures program debugging;rainbow splash 2 benchmarks nonconfiict memory operations expandable spectrum replay parallelism near precise happens before relations memory operations sc sequential consistency machines snoopy protocols strata fault tolerance intrusion analysis program debugging multithreaded applications r r record and replay architectures relaxed memory model high replay parallelism memory dependence recording;parallel architectures;program debugging;benchmark testing	Architectures for record-and-replay (R&R) of multithreaded applications ease program debugging, intrusion analysis and fault-tolerance. Among the large body of previous works, Strata enables efficient memory dependence recording with little hardware overhead and can be applied smoothly to snoopy protocols. However, Strata records imprecise happens-before relations and assumes Sequential Consistency (SC) machines that execute memory operations in order. This paper proposes Rainbow, which is based on Strata but records near-precise happens-before relations, reducing the number of logs and increasing the replay parallelism. More importantly, it is the first R&R scheme that supports any relaxed memory consistency model. These improvements are achieved by two key techniques: (1) To compact logs, we propose expandable spectrum (the region between two logs). It allows younger non-conflict memory operations to be moved into older spectrum, increasing the chance of reusing existing logs. (2) To identify the overlapped and incompatible spectra due to reordered memory operations, we propose an SC violation detection mechanism based on the existing logs and the extra information can be recorded to reproduce the violations when they occur. Our simulation results with 10 SPLASH-2 benchmarks show that Rainbow reduces the log size by 26.6% and improves replay speed by 26.8% compared to Strata. The SC violations are few but do exist in the applications evaluated.	consistency model;debugging;fault tolerance;memory model (programming);overhead (computing);parallel computing;sequential consistency;simulation;smoothing;thread (computing)	Xuehai Qian;He Huang;Benjamin Sahelices;Depei Qian	2013	2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2013.6522349	benchmark;interleaved memory;parallel computing;real-time computing;multithreading;computer science;theoretical computer science;operating system;programming language;software fault tolerance	Arch	-8.727952065930552	51.215246882694956	44465
2c027d474346b601192c51ee29c2001bb39f7b30	load balancing for mapreduce-based entity resolution	erbium;load management indexes erbium scalability computational modeling memory management image color analysis;cluster computing;memory management;search space;computer model;data integration cloud computing;data distribution;indexes;computational modeling;complex data;image color analysis;indexation;load management;entity resolution;load balance;scalability;real cloud infrastructure load balancing mapreduce entity resolution complex data intensive task data redistribution skewed data skew handling blocking technique search space;data integration;cloud computing	The effectiveness and scalability of MapReduce-based implementations of complex data-intensive tasks depend on an even redistribution of data between map and reduce tasks. In the presence of skewed data, sophisticated redistribution approaches thus become necessary to achieve load balancing among all reduce tasks to be executed in parallel. For the complex problem of entity resolution, we propose and evaluate two approaches for such skew handling and load balancing. The approaches support blocking techniques to reduce the search space of entity resolution, utilize a preprocessing MapReduce job to analyze the data distribution, and distribute the entities of large blocks among multiple reduce tasks. The evaluation on a real cloud infrastructure shows the value and effectiveness of the proposed load balancing approaches.	blocking (computing);cloud computing;data-intensive computing;entity;load balancing (computing);mapreduce;preprocessor;scalability	Lars Kolb;Andreas Thor;Erhard Rahm	2012	2012 IEEE 28th International Conference on Data Engineering	10.1109/ICDE.2012.22	computer simulation;database index;parallel computing;real-time computing;scalability;erbium;cloud computing;name resolution;computer cluster;computer science;load balancing;data integration;operating system;database;distributed computing;programming language;computational model;complex data type;memory management	DB	-16.72829445074639	55.80164252546587	44784
b7d67a9ab469868950698a1efdcb5a02887050ff	beyond cpu: considering memory power consumption of software		ICTs (Information and Communication Technologies) are responsible around 2% of worldwide greenhouse gas emissions (Gartner, 2007). And according to the Intergovernmental Panel on Climate Change (IPPC) recent reports, CO2 emissions due to ICTs are increasing widely. For this reason, many works tried to propose various tools to estimate the energy consumption due to software in order to reduce carbon footprint. However, these studies, in the majority of cases, takes into account only the CPU and neglects all others components. Whereas, the trend towards high-density packaging and raised memory involve a great increased of power consumption caused by memory and maybe memory can become the largest power consumer in servers. In this paper, we model and then estimate the power consumed by CPU and memory due to the execution of a software. Thus, we perform several experiments in order to observe the behavior of each component.		Hayri Acar;Gülfem Isiklar Alptekin;Jean-Patrick Gelas;Parisa Ghodous	2016	2016 5th International Conference on Smart Cities and Green ICT Systems (SMARTGREENS)		cpu core voltage;bogomips;computer architecture;parallel computing;computer hardware;cpu modes;cpu shielding	SE	-7.684209655758875	55.500837735910665	45090
b0a40164f80d7bf328aaade5190874806d275601	evaluation of multithreaded uniprocessors for commercial application environments	2-level adaptive prediction;branch prediction;correlation;system traces;object oriented programming;multithreading;switches;computer architecture;operating system;parallel processing;computer networks	As memory speeds grow at a considerably slower rate than processor speeds, memory accesses are starting to dominate the execution time of processors, and this will likely continue into the future. This trend will be exacerbated by growing miss rates due to commercial applications, object-oriented programming and micro-kernel based operating systems. We examine the use of coarse-grained multithreading to address this important problem in uniprocessor on-line transaction processing environments where there is a natural, coarse-grained parallelism between the tasks resulting from transactions being executed concurrently, with no application software modifications required. Our results suggest that multithreading can provide significant performance improvements for uniprocessor commercial computing environments.	thread (computing);uniprocessor system	Richard J. Eickemeyer;Ross E. Johnson;Steven R. Kunkel;Mark S. Squillante;Shiafun Liu	1996		10.1109/ISCA.1996.10023	embedded system;parallel processing;computer architecture;parallel computing;real-time computing;multithreading;network switch;computer science;operating system;temporal multithreading;programming language;object-oriented programming;correlation;branch predictor	HCI	-8.826931471845203	50.928709510398754	45109
8f4d710f7da013241f179c9bb7d9383bd51b334b	data grid performance analysis through study of replication and storage infrastructure parameters	resource scheduling;communication networks;availability;digital storage grid computing;resource management;high energy;network performance;data replication;packaging;physics computing;computer networks;network servers;community networks;remote tier 1 node data grid data replication storage infrastructure parameters latency reduction communication networks optimisation solution storage parameter settings local tier2 nodes;performance analysis;model development;performance analysis grid computing fabrics delay resource management availability packaging network servers computer networks physics computing;fabrics;optimisation solution;digital storage;local tier2 nodes;grid computing;remote tier 1 node;storage infrastructure parameters;latency reduction;wide area network;storage parameter settings;data grid	Running data grid applications such as high energy nuclear physics (HENP) and weather modelling experiments involves working with huge data sets possibly of hundreds of Terabytes to Petabytes in size often kept over wide area networks. Data replication is a useful technique for reducing latency across communication networks over which the source data are accessed. As a starting point towards developing a multifaceted optimisation solution for data grids, this paper considers the effect of replication and storage parameter settings on data grid performance. The simulation results we obtained suggest that replication at local (Tier2) nodes has significant impact on data grid performance while cache settings at remote (Tier 1) node result in minimal performance improvement.	experiment;mathematical optimization;mechatronics;multitier architecture;numerical weather prediction;petabyte;profiling (computer programming);replication (computing);simulation;source data;telecommunications network;terabyte;tier 1 network	Ernest Sithole;Gerard P. Parr;Sally I. McClean	2005	CCGrid 2005. IEEE International Symposium on Cluster Computing and the Grid, 2005.	10.1109/CCGRID.2005.1558568	availability;packaging and labeling;real-time computing;computer science;resource management;operating system;data grid;database;distributed computing;network performance;grid computing;replication;computer network	HPC	-18.715028034840447	58.12925020692536	45218
8f38951da084559c932e39ec74015008d8f05f7f	a mirroring-assisted channel-raid5 ssd for mobile applications		Simply applying an existing redundant array of independent disks (RAID) technique to enhance data reliability within a single solid-state drive for safety-critical mobile applications significantly degrades performance. In this article, we first propose a new RAID5 architecture called channel-RAID5 with mirroring (CR5M) to alleviate the performance degradation problem. Next, an associated data reconstruction strategy called mirroring-assisted channel-level reconstruction (MCR) is developed to further shrink the window of vulnerability. Experimental results demonstrate that compared with channel-RAID5 (CR5), CR5M improves performance up to 40.2%. Compared with disk-oriented reconstruction, a traditional data reconstruction scheme, MCR on average improves data recovery speed by 7.5% while delivering a similar performance during reconstruction.	data recovery;data redundancy;disk mirroring;elegant degradation;memory card reader;mobile app;replication (computing);requirement;solid-state drive;standard raid levels	Wen Pan;Tao Xie	2018	ACM Trans. Embedded Comput. Syst.	10.1145/3209625	parallel computing;computer science;mirroring;communication channel	Arch	-11.646849875870036	55.315473734766435	45484
c5b8a7f73f97c54fb16ac0eee7dd8a4efb616e6b	parity data de-duplication in all flash array-based openstack cloud block storage	openstack;raid;solid state drives;parity data deduplication;cloud storage			Huiseong Heo;Cheong-Jin Ahn;Deok-Hwan Kim	2016	IEICE Transactions	10.1587/transinf.2016EDL8006	computer science;operating system;database;computer security;raid	OS	-14.365077203252444	55.23893135830645	45487
f48262681e0c66b71009d6fb58633c51f423d513	shadowgc: cooperative garbage collection with multi-level buffer for performance improvement in nand flash-based ssds		Garbage collection, an essential background activity in NAND flash based SSDs, often introduces large runtime overhead. Recent studies showed that it is beneficial to separate the flash pages that have dirty copies in the write buffers from those that do not. However, the existing schemes exploring this observation have limitations, which prevent them from maximizing the performance improvement. In this paper, we address the above challenge through ShadowGC, a novel GC design that exploits the pages in both host-side and device-side write buffers and adopts different read and write strategies to minimize the GC overhead. When garbage collecting flash pages that have dirty copies in the device-side write buffer, ShadowGC reads data from the write buffer. When garbage collecting flash pages that have dirty copies in the host-side write buffer, ShadowGC moves them to dedicated blocks and speeds up the movement with fast-write operations. Our experimental results show that, on average, ShadowGC reduces the write amplification by 16.2% and the GC latency by 20.5% over the state-of-the-art.	adobe flash;flash memory;garbage collection (computer science);overhead (computing);write buffer	Jinhua Cui;Youtao Zhang;Jianhang Huang;Weiguo Wu;Jun Yang	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342206	latency (engineering);computer science;real-time computing;parallel computing;nand gate;write buffer;garbage;garbage collection;exploit;server;write amplification	EDA	-11.867399043122244	54.30904053978088	45532
5fff6219bc66df34ef8dbdf00ae6848f69583883	reducing tlb power requirements	cache storage;energy consumption delay proposals permission computer architecture power engineering computing power engineering and energy process design delta modulation particle measurements;content addressable storage cache storage virtual storage file organisation delays;content addressable storage;virtual storage;delays;cycle time tlb power requirements translation look aside buffers caches virtual memory processors power consumption fully associative tlbs set associative tlbs direct mapped tlbs miss rates spec92 benchmark delay;file organisation	Translation look-aside buffers (TLBs) are small caches to speed-up address translation in processors with virtual memory. This paper considers two issues: (1) a comparison of the power consumption of fully-associative, set-associative, and direct mapped TLBs for the same miss rate and (2) the proposal of modifications of the basic cells and of the structure of set-associative TLBs to reduce the power. The power evaluation is done using a model and the miss rates are obtained from simulations of the SPEC92 benchmark. With respect to (1) we conclude that for small TLBs (high miss rates) fully-associative TLBs consume less power but for larger TLBs (low miss rates) set-associative TLBs are better. Moreover, the proposed modifications produce significant reductions in power consumption. Our evaluations show a reduction of 40 to 60% compared to the best traditional TLB. The proposed TLB implementation produces an increase in delay and in area. However, these increases are tolerable because the cycle time is determined by the slower cache and because the TLB area corresponds to only a small portion of the chip area.	benchmark (computing);cpu cache;central processing unit;requirement;simulation;translation lookaside buffer	Toni Juan;Tomás Lang;Juan J. Navarro	1997		10.1145/263272.263332	embedded system;parallel computing;real-time computing;computer hardware;computer science;operating system	Arch	-7.388442220064802	53.281718568592716	45538
49dfd51f81566664c7f85a209bdeb51d8c5419c2	performance analysis in code-x partitioning for structural programmable accelerators	high level synthesis;logic partitioning;parallelising compilers;software performance evaluation;structured programming;transputer systems;code-x partitioning;accelerator source code;application development methodology;application time estimation;candidate task performance analysis;data dependencies;hardware/software codesign strategies;parallel task execution;parallelizing compilation environment;performance analysis;performance optimization;procedural programmable host processor;profiling-driven host/accelerator partitioning;reconfigurable resource utilization;resource-driven sequential/structural partitioning;sequential programmable host processor;structural programmable data-driven accelerator processor;target hardware platform;transputer codesign;transputer-based accelerators	The paper presents the performance analysis process within the parallelizing compilation environment CoDe-X for simultaneous programming of Xputer-based accelerators and their host. The paper introduces briefly its hardware/software co-design strategies at two levels of partitioning. CoDe-X performs both, at first level a profiling-driven host/accelerator partitioning for performance optimization, and at second level a resource-driven sequential/structural partitioning of the accelerator source code to optimize the utilization of its reconfigurable resources. The analysis of candidate (task) performances in CoDe-X has to be done for both, a procedural (sequential) programmable host processor, and the structural programmable data-driven accelerator processor. In complete application time estimation data-dependencies for parallel task execution (host/accelerators) are considered. To stress the significance of this application development methodology, the paper first gives an introduction to the target hardware platform.	automatic parallelization;compiler;data dependency;mathematical optimization;performance;profiling (computer programming);xputer	Reiner W. Hartenstein;Jürgen Becker	1997		10.1145/792768.793509	computer architecture;parallel computing;real-time computing;computer science	EDA	-7.359440933684666	47.345280881976926	45563
dd47223fc26a4bed7b2829f6569e0bcd317c9d24	a dedicated message matching mechanism for collective communications		The Message Passing Interface (MPI) libraries use message queues to guarantee correct message ordering between communicating processes. Message queues are in the critical path of MPI communications and thus, the performance of message queue operations can have significant impact on the performance of applications. Collective communications are widely used in MPI applications and they can have considerable impact on generating long message queues. In this paper, we propose a message matching mechanism that improves the message queue search time by distinguishing messages coming from point-to-point and collective communications and allocating separate queues for them. Moreover, it dynamically profiles the impact of each collective call on message queues during the application runtime and uses this information to adapt the message queue data structure for each collective operation dynamically. The proposed approach can successfully reduce the queue search time while maintaining scalable memory consumption. The evaluation results show that we can obtain up to 5.5x runtime speedup for applications with long list traversals. Moreover, we can gain up to 15% and 45% queue search time improvement for applications with short and medium list traversals, respectively.	collective operation;critical path method;data structure;library (computing);message passing interface;message queue;open mpi;point-to-point protocol;point-to-point (telecommunications);requirement;scalability;speedup;thread (computing);tree traversal	S. Mahdieh Ghazimirsaeed;Ryan E. Grant;Ahmad Afsahi	2018		10.1145/3229710.3229712	critical path method;message queue;distributed computing;speedup;scalability;message passing interface;data structure;computer science;queue	HPC	-13.096447403299774	47.89979924315606	45708
b620d886b1e3ca0270d397a0a3b7c14524a84355	automatic generation of parallel programs with dynamic load balancing	performance measure;dynamic load balancing;performance evaluation;perforation;resource allocation;high speed networks;parallelizing compilers;parallel programming;automatic programming;automatic generation;run time system;load management workstations program processors parallel languages dynamic scheduling parallel architectures application software high speed networks power generation marine vehicles;compiler functionality parallel program generation dynamic load balancing parallelizing compilers parallel architectures high performance workstations high speed networks time application workload run time system automatically generated programs performance measurements;parallel systems;software tools program compilers parallel programming automatic programming resource allocation performance evaluation;software tools;parallel architecture;program compilers;parallel programs	Because of their high availability and relatively low cost, networks of workstations are now often considered as platforms for applications that used to be relegated to de dicated multiprocessors. Parallelizing compilers have simplified the programming of shared and distributed me mory multiprocessors. However, with networks of workstations, which are more loosely coupled, a dditional problems of heterogeneity, varying resource availability, and higher communication costs mus t be addressed in order to maximize utilization of system resources. Computational capabilities may vary wit h time due to other applications competing for resources, so dynamic load balancing is very important. Our research explores issues in retargeting a parallelizin g compiler for a network of workstations. In this dissertation, we describe a system that supports dynam ic load balancing of distributed applications consisting of parallelized DOALL and DOACROSS loops. We out line the added compiler functionality needed to generate parallel programs with dynamic load bala ncing and demonstrate how parameters for dynamic load balancing can be selected and controlled autom atically at run time with cooperation between the compiler and runtime system. We have implemented a proto type runtime system on the Nectar system at Carnegie Mellon University and have evaluated its perfor mance using hand-parallelized applications running in various environments. Key performance parameters under our control include the gr ain size of the application, the frequency of load balancing, and the amount and frequency of work movem ent. The optimal grain size is selected based on computation and communication costs of the applica tion on the particular system on which it is run. Selecting an appropriate load balancing frequency req ui s information about communication costs and process scheduling by the operating system. The frequen cy must be adjusted as loads on the processors change, and controlling the frequency requires the coopera tion of the compiler. Making correct decisions regarding work movement is a difficult problem because of hig h work movement costs and the unpredictable	automatic parallelization;central processing unit;compiler;computation;computer cluster;cylinder-head-sector;distributed computing;high availability;load balancing (computing);loose coupling;operating system;parallel computing;retargeting;run time (program lifecycle phase);runtime system;scheduling (computing);workstation	Bruce S. Siegell;Peter Steenkiste	1994		10.1109/HPDC.1994.340247	computer architecture;parallel computing;real-time computing;resource allocation;computer science;operating system;programming language	HPC	-15.258553917999842	59.08552140132063	45728
b73a68883f7f151bb33fde576e7a8eb024c6570f	divide-and-conquer: a bubble replacement for low level caches	high performance computing;circuit design;cache replacement;cache replacement policy;high performance computer;divide and conquer;replacement policy	The widely used LRU replacement policy suffers from the following problems. First, LRU does not exploit fre-quency information of cache accesses. Second, LRU may experience cache thrashing when access to cache exhibits cyclic patterns and the cache capacity is less than the working set. Finally, LRU is expensive to implement in hardware. We propose a bubble replacement for low-level caches, where cache blocks in one set are arranged in a queue for replacement determination. An incoming block enters the queue from the bottom and exchanges its posi-tion with the block above when the block hits, therefore, both recency and frequency information of a program are exploited. A victim block can be chosen from either the bottom or the top block of the queue, which is controlled by a single-bit set-hit flag per set. Choosing the bottom block as the victim makes the bubble replacement resistant to less frequently used blocks from polluting the cache while choosing the top block as the victim makes the bub-ble replacement adaptable to changes in the working set. We also propose to divide the blocks in a cache set into groups where each group implements the bubble replace-ment (we name it the DC-Bubble) to resolve the problems of the bubble replacement. The victim block is chosen ran-domly from the bottom block of each group. The proposed DC-Bubble reduces the average MPKI of the baseline 1MB 16-way L2 cache by 14%, bridges 47% of the gap between LRU and the OPT, reduces the storage require-ment by 61% and simplifies the circuit design compared to LRU.	baseline (configuration management);cpu cache;cache pollution;circuit design;high- and low-level;thrashing (computer science);working set	Chuanjun Zhang;Bing Xue	2009		10.1145/1542275.1542291	least frequently used;cache-oblivious algorithm;supercomputer;parallel computing;real-time computing;divide and conquer algorithms;cache coloring;page cache;cpu cache;computer hardware;cache;computer science;cache invalidation;operating system;circuit design;adaptive replacement cache;smart cache;programming language;cache algorithms;cache pollution	HPC	-12.8889008295021	51.13641006905261	45729
0fc486313ac490f51d29c33938bc8a935438fe5e	challenges in building a flat-bandwidth memory hierarchy for a large-scale computer with proximity communication	network on chip;large scale systems bandwidth computer aided instruction physics computing communications technology paper technology network on a chip optical fiber communication computer networks optical computing;network on chip optical interconnections optical communication;dense optical communication memory system large scale computer instruction execution memory capacity proximity communication low power on chip network;chip;large scale;low power;memory systems;optical communication;memory hierarchy;optical interconnections	"""Memory systems for conventional large-scale computers provide only limited bytes/s of data bandwidth when compared to their flop/s of instruction execution rate. The resulting bottleneck limits the bytes/flop that a processor may access from the full memory footprint of a machine and can hinder overall performance. This paper discusses physical and functional views of memory hierarchies and examines existing ratios of bandwidth to execution rate versus memory capacity (or bytes/flop versus capacity) found in a number of large-scale computers. The paper then explores a set of technologies, proximity communication, low-power on-chip networks, dense optical communication, and sea-of-any thing interconnect, that can flatten this bandwidth hierarchy to relieve the memory bottleneck in a large-scale computer that we call """"Hero""""."""	byte;compiler;computer;deadlock;driver circuit;flops;glossary of computer graphics;input/output;low-power broadcasting;memory footprint;memory hierarchy;optical fiber;proximity communication;software architect;switched fabric;von neumann architecture;wavelength-division multiplexing	Robert J. Drost;Craig Forrest;Bruce Guenin;Ron Ho;Ashok V. Krishnamoorthy;Danny Cohen;John E. Cunningham;Bernard Tourancheau;Arthur Zingher;Alex Chow;Gary Lauterbach;Ivan E. Sutherland	2005	13th Symposium on High Performance Interconnects (HOTI'05)	10.1109/CONECT.2005.12	chip;uniform memory access;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;sense amplifier;memory refresh;telecommunications;memory geometry;computer science;operating system;computer data storage;computer memory;overlay;conventional memory;network on a chip;flat memory model;registered memory;memory bandwidth;computing with memory;memory map;optical communication;computer network;non-uniform memory access;memory management	Arch	-7.0093111348493995	52.37069308516318	45736
511e85be79f279ed9430c720915658ca3f1b01cd	near-dram acceleration with single-isa heterogeneous processing in standard memory modules	random access memory;die stacking;data processing;accelerator;instruction sets dram chips energy consumption;die stacking dram accelerator near data processing;computer architecture;random access memory through silicon vias computer architecture program processors bandwidth energy consumption data transfer data processing;nda architecture near dram acceleration dynamic random access memory single isa heterogeneous processing instruction set architecture standard memory modules energy consumption processor memory hierarchy technology scaling;energy consumption;bandwidth;dram;program processors;near data processing;data transfer;through silicon vias	Energy consumed for transferring data across the processor memory hierarchy constitutes a large fraction of total system energy consumption, and this fraction has steadily increased with technology scaling. This article presents a near-DRAM acceleration (NDA) architecture wherein lightweight processors (LWPs) with the same ISA as their host processor are 3D-stacked atop commodity DRAM devices in a standard memory module to efficiently process data. In contrast to previous architectures, the authors' NDA architecture requires negligible changes to commodity DRAM device and standard memory module architectures. This allows the NDA to be more easily adopted in both existing and emerging systems. Experiments demonstrate that, on average, the authors' NDA-based system consumes almost 65 percent less energy at nearly two times higher performance than the baseline system.	baseline (configuration management);central processing unit;dimm;dynamic random-access memory;experiment;image scaling;memory hierarchy;memory module	Hadi Asghari Moghaddam;Amin Farmahini Farahani;Katherine Morrow;Jung Ho Ahn;Nam Sung Kim	2016	IEEE Micro	10.1109/MM.2016.8	embedded system;computer architecture;parallel computing;data processing;computer hardware;computer science;operating system;dram;bandwidth	Arch	-8.220669619642846	52.991068716696375	45803
9b5f36c9065827ae164d40b838f0e949391aab05	designing a hybrid scale-up/out hadoop architecture based on performance measurements for high application performance	scheduling data handling network operating systems parallel architectures performance evaluation;remote file system hadoop scale up scale out;job completion time hybrid scale up hadoop architecture hybrid scale out hadoop architecture application performance scale up machines scale out machines workload data storage small data size jobs data transmission automatic job scheduling scale out cluster scale up cluster performance measurement hadoop distributed file system hdfs remote file system ofs;computer architecture facebook measurement random access memory data communication production	Since scale-up machines perform better for jobs with small and median (KB, MB) data sizes while scale-out machines perform better for jobs with large (GB, TB) data size, and a workload usually consists of jobs with different data size levels, we propose building a hybrid Hadoop architecture that includes both scale-up and scale-out machines, which however is not trivial. The first challenge is workload data storage. Thousands of small data size jobs in a workload may overload the limited local disks of scale-up machines. Jobs from scale-up and scale-out machines may both request the same set of data, which leads to data transmission between the machines. The second challenge is to automatically schedule jobs to either scale-up or scale-out cluster to achieve the best performance. We conduct a thorough performance measurement of different applications on scale-up and scale-out clusters, configured with Hadoop Distributed File System (HDFS) and a remote file system (i.e., OFS), respectively. We find that using OFS rather than HDFS can solve the data storage challenge. Also, we identify the factors that determine the performance differences on the scale-up and scale-out clusters and their cross points to make the choice. Accordingly, we design and implement the hybrid scale-up/out Hadoop architecture. Our trace-driven experimental results show that our hybrid architecture outperforms both the traditional Hadoop architecture with HDFS and with OFS in terms of job completion time.	apache hadoop;computation;computer data storage;dce distributed file system;expect;experiment;ibm notes;job stream;load balancing (computing);matthew cook;microsoft research;remote file sharing;scalability;scheduling (computing);stack machine;terabyte	Zhuozhao Li;Haiying Shen	2015	2015 44th International Conference on Parallel Processing	10.1109/ICPP.2015.11	parallel computing;real-time computing;computer science;operating system	HPC	-16.957833717098215	57.01055591388681	45814
e5d667061ef1aeba3196124afb5481dd7f4632d4	exploiting cache locality to speedup register clustering		Physical design tools must handle huge amounts of data in order to solve problems for circuits with millions of cells. Traditionally, Electronic Design Automation tools are implemented using Object-Oriented Design. However, using this paradigm may lead to overly complex objects that result in waste of cache memory space. This memory wasting harms cache locality exploration and, consequently, degrades software runtime. This work proposes applying Data-Oriented Design on the register clustering problem. Differently from the traditional Object-Oriented design, the Data-Oriented Design programming model focus on how the data is organized in the memory. As consequence, this programming model may better explore cache spatial locality. In order to evaluate the impact of using the Data-Oriented Design programming model for register clustering, we implemented two software prototypes (a sequential and a parallel implementation) of the K-means clustering algorithm for each programming model. Experimental results showed that the sequential Data-Oriented Design implementation is on average 7.5% faster when compared to the Object-Oriented Design implementation, while its parallel version is 15% faster when compared to the Object-Oriented one.	algorithm;block size (cryptography);cpu cache;central processing unit;cluster analysis;dspace;data-oriented design;electronic design automation;field-programmable gate array;graphics processing unit;iteration;k-means clustering;locality of reference;openmp;parallel computing;principle of locality;programming model;software prototyping;sorting algorithm;speedup;thread (computing)	Tiago Augusto Fontana;Sheiny Almeida;Renan Netto;Vinicius S. Livramento;Chrystian Guth;Laércio Lima Pilla;José Luís Almada Güntzel	2017	2017 30th Symposium on Integrated Circuits and Systems Design (SBCCI)	10.1145/3109984.3110005	real-time computing;parallel computing;cpu cache;programming paradigm;physical design;computer science;cluster analysis;speedup;cache;electronic design automation;cache algorithms	EDA	-5.512460512178266	49.299724534978104	45835
65f539498d552a64eb10002a0da668e0ef262d46	frem: a fast restart mechanism for general checkpoint/restart	protocols;kernel;software fault tolerance checkpointing fault tolerant computing linux;complexity theory;computed tomography;fault tolerant;high performance computing;application restart latency;failure rate fast restart mechanism checkpoint operation restart operation application restart latency system dependability system performance checkpoint restart protocols linux fault tolerance technique;checkpoint restart protocols;fast restart;software fault tolerance;restart operation;system performance;checkpointing;fault tolerant computing;operating system;fault tolerance;high performance computer;data access;checkpoint operation;high performance computing fast restart operating system linux fault tolerance;failure rate;linux;optimization;system dependability;fault tolerance technique;fast restart mechanism;hardware;protocols checkpointing optimization hardware kernel computed tomography complexity theory	As failure rate keeps on increasing in large systems, applications running atop restart more frequently than ever. Existing research on checkpoint/restart mainly focuses on optimizing checkpoint operation, without paying much attention to the restart operation. As a result, application restart latency maybe substantial, which greatly threatens system dependability and performance. To attack the restart latency problem, in this paper, we present FREM, a fast restart mechanism for general checkpoint/restart protocols. By dynamically tracking the process data accesses after each checkpoint, FREM masks restart latency by overlapping application recovery with the retrieval of its checkpoint image. We have implemented FREM as a prototype system and tested it under Linux environments. Extensive experiments with real applications demonstrate that it can effectively reduce restart latency by over 50 percent on average, as compared to the conventional restart mechanisms.	application checkpointing;dependability;experiment;failure rate;linux;prototype;transaction processing system	Yawei Li;Zhiling Lan	2011	IEEE Transactions on Computers	10.1109/TC.2010.129	fault tolerance;parallel computing;real-time computing;computer science;operating system;distributed computing;computer performance;computed tomography	DB	-17.839930397957207	49.968474303944184	45975
3869bac212b18c2d7b73435fe6d30c8891a62adf	speculative parallelization of sequential loops on multicores	multicores;software engineering programming and operating systems;speculative parallelization;profile guided parallelization;theory of computation;state separation;multicore processors;is success;computer science;processor architectures	The advent of multicores presents a promising opportunity for speeding up the execution of sequential programs through their parallelization. In this paper we present a novel solution for efficiently supporting software-based speculative parallelization of sequential loops on multicore processors. The execution model we employ is based upon state separation, an approach for separately maintaining the speculative state of parallel threads and non-speculative state of the computation. If speculation is successful, the results produced by parallel threads in speculative state are committed by copying them into the computation’s non-speculative state. If misspeculation is detected, no costly state recovery mechanisms are needed as the speculative state can be simply discarded. Techniques are proposed to reduce the cost of data copying between non-speculative and speculative state and efficiently carrying out misspeculation detection. We apply the above approach to speculative parallelization of loops in several sequential programs which results in significant speedups on a Dell PowerEdge 1900 server with two Intel Xeon quad-core processors.	algorithm;automatic parallelization;central processing unit;computation;dell poweredge;experiment;multi-core processor;overhead (computing);parallel computing;sequential consistency;server (computing);speculative execution	Chen Tian;Min Feng;Vijay Nagarajan;Rajiv Gupta	2009	International Journal of Parallel Programming	10.1007/s10766-009-0111-z	multi-core processor;computer architecture;parallel computing;real-time computing;theory of computation;computer science;operating system;speculative multithreading;algorithm;automatic parallelization	PL	-13.275792027793491	48.081664696120384	46001
6a1901afb80955b6e33e66d5bbf13727726f6547	nbti-aware data allocation strategies for scratchpad based embedded systems	scratchpad;nbti;aging;low power;data allocation;sram	While performance and power continue to be important metrics for embedded systems, as CMOS technologies continue to shrink, new metrics such as variability and reliability have emerged as limiting factors in the design of modern embedded systems. In particular, the reliability impact ofpMOS negative bias temperature instability (NBTI) has become a s erious concern. Recent works have shown how conventional leakage optimization techniques can help mitigate NBTI-induced aging effects on cache memories. In this paper we focus specificall y on scratchpad memory (SPM) and present novel software approaches as a means of alleviating the NBTI-induced aging effects. In particular, we demonstrate how intelligent sof tware directed data allocation strategies can extend the lifetime of partitioned SPMs by means of distributing the idleness acro ss the memory sub-banks. I. I NTRODUCTION AND BACKGROUND Memory subsystems have long been known to be a critical component in the overall performance for embedded platforms. However, more recently, these memory systems have also been demonstrated to be the major contributor to the tot al power budget. The problem of power dissipation in memories is exacerbated in ultra-deep sub-micron CMOS technologies , where static power due to leakage currents (and sub-thresho ld current in particular) is coupled with high dynamic power dissipation [1]. The leakage power is extremely critical for memories, where the high density of integration translates into high power density. The latter is the main source of heat generate d across the substrate, which, if not quickly removed through packaging and/or cooling architectures, may induce drasti c increases in temperature. Working at high on-chip temperatures affects both performance (MOS transistors and global wires get slower at high temperatures [2], [3]) and static power consumption (leakage current increases exponential ly with temperature [1]). For these reasons, several leakageaware memories solutions, ranging from software and system level hierarchy optimization to architecture and circu it level structures, have been proposed over the past few years (e.g., [4], [5]). While performance and power continue to be important metrics for embedded systems, as CMOS technologies continue to shrink down below 65nm, new metrics such as variability and reliability have emerged as limiting factors in the design of modern embedded systems. This is especially true when considering memory architectures; access to/fro m memory is involved in every executed instruction (e.g., dat a and instruction fetch), and once the memory system becomes unreliable, the reliability is compromised for the system a s a whole. Although process variation is the most evident source of variability, and thus, unreliability [6], [7], time-depen dent deviations in the operating characteristics of nanoscale M OS devices [8] have recently been shown to be one of the more critical issues in determining the lifetime of CMOS circuit s. In particular, the reliability impact of pMOS negative bias te mperature instability (NBTI) has become a serious concern [9 ]. In CMOS logic, NBTI effects occur on pMOS transistors when they are negatively biased (i.e., a 0-logic is applied t o the gate of the pMOS, resulting in Vgs = VDD), and manifest themselves as an increase in the threshold voltage Vth over time [9]. Such aVth variation has a direct impact on the longterm stability of traditional 6T-SRAM cells, whose static n oise margin (SNM) degrades over time, thus altering the capabili ty of a cell to reliably store a correct logic value [10]–[12]. Experimental data report variation of Vth of about 10-15% per year, which translates into more than a 10% SNM degradation (after 3 years) depending on the target technology and the operating conditions [11]. The push to embed reliable andlow-power memories archictures into modern systems-on-chip is driving the EDA com munity to develop new design techniques and circuit solutio ns hat can address these critical issues. Very recent works [1 3], [14] have shown how conventional leakage optimization tech niques [4], [5] can offer a valuable solution to mitigate NBTI-induced aging effects on memories while still obtaini ng reductions in power dissipation. More specifically, they ha ve quantified the beneficial effects of popular power managemen t schemes such as power-gating and dynamic voltage scaling (DVS) on aging. Power gating, when implemented using sleep transistors, has the effect of completely nullifying the ag ing effects [15]. Similarly, but with a smaller impact, voltage scaling improves NBTI-induced aging because a reduced VDD corresponds to a smaller bias voltage. As reported in [13], reduction in SNM as a function of Vdd is roughly linear; the degradation of SNM under a “drowsy” voltage Vdd,drowsy is about 60% of the degradation at the nominal Vdd. Although the majority of these works focus on cache memories, there exist other types of SRAM memory structures whose efficiency also impacts the total power-performance tradeoff, as well as on the lifetime of the embedded systems. Among them, scratch-pad memory (SPM) is the most significant example. SPMs are widely used in embedded systems for image and video processing applications that make heavy use of multi-dimensional arrays of signals and nested loops. Fo r these classes of applications, the flexibility of caches in t erms of workload adaptability is often not needed; instead, perf ormance predictability, power consumption, and implementat ion costs play a much more critical role. Even though caches and SPMs perform the same function (namely, temporarily holding small chunks of data for highspeed, frequent retrievals), their implementation and man agement is completely different. The main differentiating fac tor is that, while caches guarantee full transparency at the cos t of more hardware resources and non-deterministic latency, SPMs, which do not need any caching logic, are faster, more predictable and less power consuming. Note that for SPMs, it is the designer that decides the mapping of addresses to locations into the SPM, not the hardware. This new degree of freedom in the design space opens a completely unexplored area in the field of concurrent poweraging memory optimization. Namely, it allows for the use of innovative software approaches as a means of alleviating th e NBTI-induced aging effects. In contrast to previous works t hat focus on pure circuit/architectural cache solutions (e.g. , [14], [16]), in this paper we investigate new software controlled NBTI-aware data managing solutions for low-power SPMs. Building off of previous findings that SRAM cells in a low-leakage state (i.e., idle state) are less affected by NB TI stress [13], we demonstrate how intelligent data allocatio n strategies can extend the lifetime of partitioned SPMs by means of distributing the idleness across the memory subbanks. The basic reasoning behind this approach is that, whi le from a leakage viewpoint it is the total number of idle blocks that matters, for aging, it is the distribution of idleness across the memory banks that can help maximize the lifetime of the entire SPM, and thus the system as a whole. To support our claims we developed a dedicated library of C-functions that implement NBTI-aware data allocation through a dedicated malloc, calledSPM malloc. This function is aware of the current aging of each memory bank and maps the heap of each task such that all the banks age at the same rate. More specifically, the data are allocated such that all the banks can spend the same amount of time in the idle state (low-VDD state). Dedicated lookup tables containing precharacterized NBTI-induced SNM degradation of a standard 6T-SRAM cell mapped into an industrial 45nm technology are used to estimate the aging of the SPM’s sub-banks. We demonstrate through a motivational example the effectiven ess of our approach and show that overall idle times across all banks of the SPM can be more evenly distributed, thereby preventing some banks from aging faster than others and increasing the reliability of the memory system overall. II. A RCHITECTURAL OVERVIEW Our target architecture is shown in Figure 1. The baseline configuration consists of an ARM7 CPU, coupled with a L1 cache and a block of scratch-pad memory. A Direct-Memory Access engine (DMA) is also present, and it is in charge of accelerating the data movements between the on-chip and off chip memories. This type of architecture has many applicati ons (e.g., smartphones, cameras, game consoles etc.), and it is widely adopted in the embedded domain. Next, we describe the proposed hardware and software extensions. Fig. 1. Architectural overview. A. Hardware Extensions Similarly to [17], we propose a multi-banked type of scratch-pad memory, with independently powered banks. Thi s kind of structure has been studied also in caches (e.g., [18] ), mainly for power concerns. Fig. 2. Scratchpad memory configuration. As shown in Figure 2, a special control unit sets the power state (i.e.,active, or drowsy) for each bank. We deliberately didn’t consider a shutdown state, since that configuration would complicate the design of the scratch-pad memory. The insertion of extra sleep transistors in the memory cells, in fact, would not only impact on the complexity but also on the performance of such critical component. The control unit is memory mapped, and can be programmed with regular write operations. In order to simplify the programming process, we implemented some interfacing func tions (described in Section II-C). We estimate that reactiv ting a bank from theDrowsy state incurs a realistic performance TABLE I FUNCTIONS VISIBLE TO THE PROGRAMMER. Name Description Example of Utilization void* SPM malloc(int size, Allocates data in SPM. int *A= unsigned int** requestor); Two input parameters: (int*)SPM malloc(sizeof(int),&A) 1) Size 2) Reference to the pointer of t	arm7;baseline (configuration management);biasing;c dynamic memory allocation;cmos;cpu cache;cpu power dissipation;cell (microprocessor);central processing unit;computer cooling;control unit;direct memory access;dynamic voltage scaling;elegant degradation;embedded system;existential quantification;fo (complexity);heart rate variability;image scaling;instruction cycle;integer (computer science);lookup table;low-power broadcasting;map;mathematical optimization;memory bank;naive bayes classifier;negative-bias temperature instability;operating system;perf (linux);plug-in (computing);pointer (computer programming);power gating;power supply;program optimization;scratchpad memory;shutdown (computing);smartphone;spatial variability;spectral leakage;static random-access memory;super paper mario;system on a chip;time complexity;tor messenger;transistor;value-driven design;video processing;xfig	Cesare Ferri;Dimitra Papagiannopoulou;R. Iris Bahar;Andrea Calimera	2012	J. Electronic Testing	10.1007/s10836-012-5295-2	embedded system;negative-bias temperature instability;electronic engineering;parallel computing;real-time computing;static random-access memory;computer science	EDA	-4.731349026703605	55.97618624852014	46059
0d6787f19c7a521784a38d31420dd8da7bd490ef	mos: an architecture for extreme-scale operating systems		Linux®, or more specifically, the Linux API, plays a key role in HPC computing. Even for extreme-scale computing, a known and familiar API is required for production machines. However, an off-the-shelf Linux distribution faces challenges at extreme scale. To date, two approaches have been used to address the challenges of providing an operating system (OS) at extreme scale. In the Full-Weight Kernel (FWK) approach, an OS, typically Linux, forms the starting point, and work is undertaken to remove features from the environment so that it will scale up across more cores and out across a large cluster. A Light-Weight Kernel (LWK) approach often starts with a new kernel and work is undertaken to add functionality to provide a familiar API, typically Linux. Either approach however, results in an execution environment that is not fully Linux compatible.  mOS (multi Operating System) runs both an FWK (Linux), and an LWK, simultaneously as kernels on the same compute node. mOS thereby achieves the scalability and reliability of LWKs, while providing the full Linux functionality of an FWK. Further, mOS works in concert with Operating System Nodes (OSNs) to offload system calls, e.g., I/O, that are too invasive to run on the compute nodes at extreme-scale. Beyond providing full Linux capability with LWK performance, other advantages of mOS include the ability to effectively manage different types of compute and memory resources, interface easily with proposed asynchronous and fine-grained runtimes, and nimbly manage new technologies.  This paper is an architectural description of mOS. As a prototype is not yet finished, the contributions of this work are a description of mOS's architecture, an exploration of the tradeoffs and value of this approach for the purposes listed above, and a detailed architecture description of each of the six components of mOS, including the tradeoffs we considered. The uptick of OS research work indicates that many view this as an important area for getting to extreme scale. Thus, most importantly, the goal of the paper is to generate discussion in this area at the workshop.	application programming interface;ibm websphere extreme scale;input/output;kernel (operating system);linux;operating system;prototype;runtime system;scalability;system call	Robert W. Wisniewski;Todd Inglett;Pardo Keppel;Ravi Murty;Rolf Riesen	2014		10.1145/2612262.2612263	embedded system;real-time computing;htree;computer science;operating system	OS	-16.187450163243057	51.27538188351576	46136
dfd125482399ac7525ebd5e623b6f90a28ddc747	afluentes concurrent i/o made easy with lazy evaluation	afluentes callback based approach asynchronous functions java framework lazy evaluation concurrent i o intensive systems;java programming runtime contracts context libraries data transfer;lazy evaluation;i o;concurrency;lazy evaluation asynchronous callback concurrency function composition i o;function composition;callback;asynchronous;java	I/O intensive systems can significantly reduce their total execution time by performing these operations concurrently. Despite this enormous potential, most systems perform I/O operations sequentially. One of the reasons behind this is that the most widespread mechanisms for concurrent I/O are callback-based, resulting in code hard to write and maintain. In this context, we propose Afluentes, a Java framework that allows asynchronous functions to be composed just as easily as synchronous ones, facilitating the development of concurrent I/O intensive systems. We performed an experimental evaluation whose results showed that Afluentes leads to significant performance gains over sequential I/O. Compared to callback-based approaches, Afluentes provides similar increases in performance, but with less programming effort.	asynchronous i/o;callback (computer programming);central processing unit;database;experiment;input/output;java;lazy evaluation;mathematical optimization;object graph;programmer;requirement;run time (program lifecycle phase);web application	Saulo Medeiros de Araujo;Kiev Gama;Nelson Souto Rosa;Silvio Romero de Lemos Meira	2014	2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing	10.1109/PDP.2014.75	callback;input/output;function composition;parallel computing;real-time computing;concurrency;computer science;operating system;asynchronous communication;lazy evaluation;distributed computing;programming language;java	HPC	-14.71299733975725	47.40742616314406	46184
6f2e24def1b1e1a55a92089ff2b7521fe7fdb965	integrating flash-based ssds into the storage stack	cache storage;disc drives;storage system;performance evaluation;high density;radiation detectors ash performance evaluation semantics systematics physical layer indexes;systematics;physical layer;radiation detectors;semantics;radiation detector;indexes;solid state disk flash based ssd hybrid storage architecture high performance ssd high density hdd capital cost operating cost hdd based storage stack caching dynamic storage tiering systematic side by side analysis virtualized modern storage installation loris storage stack hybrid storage system dst based hybrid system;memory architecture;virtual storage cache storage disc drives flash memories hard discs memory architecture;indexation;ash;hybrid system;hybrid architecture;high performance;hard discs;virtual storage;flash memories	Over the past few years, hybrid storage architectures that use high-performance SSDs in concert with high-density HDDs have received significant interest from both industry and academia, due to their capability to improve performance while reducing capital and operating costs. These hybrid architectures differ in their approach to integrating SSDs into the traditional HDD-based storage stack. Of several such possible integrations, two have seen widespread adoption: Caching and Dynamic Storage Tiering. Although the effectiveness of these architectures under certain workloads is well understood, a systematic side-by-side analysis of these approaches remains difficult due to the range of design alternatives and configuration parameters involved. Such a study is required now more than ever to be able to design effective hybrid storage solutions for deployment in increasingly virtualized modern storage installations that blend several workloads into a single stream. In this paper, we first present our extensions to the Loris storage stack that transform it into a framework for designing hybrid storage systems. We then illustrate the flexibility of the framework by designing several Caching and DST-based hybrid systems. Following this, we present a systematic side-by-side analysis of these systems under a range of individual workload types and offer insights into the advantages and disadvantages of each architecture. Finally, we discuss the ramifications of our findings on the design of future hybrid storage systems in the light of recent changes in hardware landscape and application workloads.	cache (computing);discrete sine transform;hard disk drive;hierarchical storage management;hybrid system;operating-system-level virtualization;software deployment;wiki	Raja Appuswamy;David C. van Moolenbroek;Andrew S. Tanenbaum	2012	012 IEEE 28th Symposium on Mass Storage Systems and Technologies (MSST)	10.1109/MSST.2012.6232365	embedded system;real-time computing;converged storage;computer hardware;engineering	Arch	-15.020373296685824	53.05505436474243	46203
6a6081fb8dc23e05f67b74108f6881bb4174d3a0	consistent, interactive steering of distributed computations: algorithms and implementation	termination;distributed computation;transaction;message logging;causality;consistency;checkpoint;rollback.;index words: interactive steering;vector time;distributed algorithms	Interactive computational steering provides users with the opportunity to tackle new problems in a way that helps them to learn about the computation in a highly engaging, interactive, visual environment. Causal consistency is an important feature of interactive steering of distributed computations, as it is often required to maintain the correctness of the computation. However, due to the asynchronous nature of distributed computations, it is difficult to coordinate steering changes across processes to guarantee that the changes are applied consistently at all processes. This thesis introduces a transaction-based computation model for distributed computation. This abstract model not only gives users a simple and high-level view of distributed computation, but also simplifies reasoning consistency problem by reducing the amount of information to be handled. Furthermore, this work investigates two approaches for achieving consistent steering: conservative steering and optimistic steering. The performance of conservative and optimistic steering approaches is evaluated in term of perturbation and lag. Our experiments show that when the percentages of consistency on the first attempt are large enough and the size of checkpoint is not too large, the optimistic approach will achieve better performance; otherwise, the conservative approach will be better.	algorithm;causal consistency;computational steering;correctness (computer science);distributed computing;experiment;high- and low-level;model of computation;optimistic replication;transaction processing system	Jinhua Guo	2002				HPC	-18.757069635263974	48.20771349212434	46251
3788a06d010cacdc6a8cad6fb5995532407a2a88	experiments with program locality	computer storage devices;technology assessment;performance tests;probability distribution functions;computer programs;error analysis;sequential control;computerized simulation;algorithms;optimization;curve fitting;allocations	"""For many years, there has been interest in """"program locality"""" as a phenomenon to be considered in storage allocation. This notion arises from the empirical observation that it is possible to run a program efficiently with only some fraction of its total instruction and data code in main storage at any given time. That virtual memory systems can be made to run at all demonstrates that program locality can be used to advantage; and though it is certainly possible to write a program which violates the principles of locality, it seems one must go out of one's way to do so."""	computer data storage;locality of reference	Jeffrey R. Spirn;Peter J. Denning	1972		10.1145/1479992.1480078	simulation;computer science;theoretical computer science;algorithm	PL	-12.146235620606065	50.39730566521556	46613
2767bbdae1ae61e51ee0f0f52a6b96bcdcda9e12	strome: energy-aware data-stream processing		Handling workloads generated by a large number of users, data-stream–processing systems also require large amounts of energy. To reduce their energy footprint, such systems typically rely on the operating systems of their servers to adjust processor speeds depending on the current workload by performing dynamic voltage and frequency scaling (DVFS). In this paper, we show that, although effective, this approach still leaves room for significant energy savings due to DVFS making conservative assumptions regarding its impact on application performance. To leverage the unused potential we present Strome, an energyaware technique to minimize energy demand in data-stream–processing systems by dynamically adapting upper limits for the power demand of hardware components. In contrast to DVFS, Strome exploits information on application performance and is therefore able to achieve energy savings while minimizing its effects on throughput and latency. Our evaluation shows that Strome is particularly effective in the face of varying workloads, reducing power demand by up to 25% compared with the state-of-the-art data-stream–processing system Heron relying on DVFS.	central processing unit;dynamic frequency scaling;dynamic voltage scaling;experiment;frequency capping;heron;image scaling;interference (communication);java caps;operating system;server (computing);stream processing;throughput	Christopher Eibel;Christian Gulden;Wolfgang Schröder-Preikschat;Tobias Distler	2018		10.1007/978-3-319-93767-0_4	workload;latency (engineering);throughput;data stream;leverage (finance);real-time computing;frequency scaling;exploit;computer science;server	Arch	-5.845975099410111	55.981486394582376	46716
cbb9cd9c2c3b72e4f4f9b191fc411625571ba60a	performance evaluation of a remote memory system with commodity hardware for large-memory data processing	remote memory system;large memory data processing;main memory databases	The explosion of data and transactions demands a creative approach for data processing in a variety of applications. Research on remote memory systems (RMSs), so as to exploit the superior characteristics of dynamic random access memory (DRAM), has been performed for many decades, and today’s information explosion galvanizes researchers into shedding new light on the technology. Prior studies have mainly focused on architectural suggestions for such systems, highlighting different design rationale. These studies have shown that choosing the appropriate applications to run on an RMS is important in fully utilizing the advantages of remote memory. This article provides an extensive performance evaluation for various types of data processing applications so as to address the efficacy of an RMS by means of a prototype RMS with reliability functionality. The prototype RMS used is a practical kernel-level RMS that renders large memory data processing feasible. The abstract concept of remote memory was materialized by borrowing unused local memory in commodity PCs via a high speed network capable of Remote Direct Memory Access (RDMA) operations. The prototype RMS uses remote memory without any part of its computation power coming from remote computers. Our experimental results suggest that an RMS can be practical in supporting the rigorous demands of commercial in memory database systems that have high data access locality. Our evaluation also convinces us of the possibility that a reliable RMS can satisfy both the high degree of reliability and efficiency for large memory data processing applications whose data access pattern has high locality.	commodity computing;computation;computer memory;data access;design rationale;dynamic random-access memory;hard disk drive;ibm tivoli storage productivity center;in-memory database;information explosion;locality of reference;meltwater entrepreneurial school of technology;operating system;performance evaluation;prototype;random access;randomness;record management services;remote computer;remote direct memory access;rendering (computer graphics);solid-state drive;throughput	Hyuck Han;Hyungsoo Jung;Sooyong Kang;Heon Young Yeom	2011	Cluster Computing	10.1007/s10586-011-0164-9	uniform memory access;shared memory;parallel computing;real-time computing;distributed memory;remote direct memory access;computer hardware;computer science;operating system;overlay;flat memory model;computer network;non-uniform memory access;memory management	Arch	-11.77889364757453	51.95688798599282	47182
289013bfd42e27c9864cd374a462c8f9a4603783	investigating the effects of using different nursery sizing policies on performance	garbage collection;multithreaded applications;fixed ratio;generational garbage collection	In this paper, we investigate the effects of using three different nursery sizing policies on overall and garbage collection performances. As part of our investigation, we modify the parallel generational collector in HotSpot to support a fixed ratio policy and heap availability policy (similar to that used in Appel collectors), in addition to its GC Ergonomics policy. We then compare the performances of 16 large and small multithreaded Java benchmarks; each is given a reasonably sized heap and utilizes all three policies. The result of our investigation indicates that many benchmarks are sensitive to heap sizing policies, resulting in overall performance differences that can range from 1 percent to 36 percents. We also find that in our server application benchmarks, more than one policy may be needed. As a preliminary study, we introduce a hybrid policy that uses one policy when the heap space is plentiful to yield optimal performance and then switches to a different policy to improve survivability and yield more graceful performance degradation under heavy memory pressure.	benchmark (computing);elegant degradation;garbage collection (computer science);human factors and ergonomics;java hotspot virtual machine;memory management;network switch;performance;server (computing);thread (computing)	Xiaohua Guan;Witawas Srisa-an;ChengHuan Jia	2009		10.1145/1542431.1542441	parallel computing;real-time computing;simulation;computer science;operating system;garbage collection;programming language	Metrics	-13.029256825648401	51.01687808552808	47213
b902a487378b19716a61dfbf669cec3ea75e001d	dynamic task assignment in server farms: better performance by task grouping	task subset allocation;remaining processing time;optimisation;dynamic load balancing;load management delay network servers distributed computing computer science information technology australia interference system performance throughput;performance evaluation;process capability;bepress selected works;processor scheduling;resource allocation;information technology;performance;distributed computing;mean waiting time;task grouping;mean slowdown time;interference;computer networks performance evaluation resource allocation processor scheduling optimisation;very large tasks;system performance;computer networks;network servers;distributed server farm systems;waiting time;dynamic task assignment;load management;load balancing algorithm algorithm server farm system servers;heavy tailed distribution;load balance;task assignment;farming system;computer science;mean slowdown time dynamic task assignment performance task grouping dynamic load balancing distributed server farm systems heavy tailed distribution task subset allocation remaining processing time priority system response time optimisation very large tasks mean waiting time;priority;australia;system response time optimisation;educational assessment;throughput	This paper describes a dynamic load balancing approach to distributed server farm systems. This approach overcomes the interference caused by non-negligible verylarge tasks in the heavy-tailed distribution. First, a subset of tasks is allocated proportionally to the processing capability of participating servers by taking into account their remaining processing time. Later, tasks in the servers are processed in order of priority to optimise the system response time. The proposed load balancing algorithm also takes into account the information on server loads to avoid load imbalance caused by very large tasks. The experiments show that the mean waiting time and the mean slow down time are reduced at the server farm system.	algorithm;downtime;experiment;interference (communication);load balancing (computing);overhead (computing);quality of service;response time (technology);server (computing);server farm;task computing	Ling Tan;Zahir Tari	2002		10.1109/ISCC.2002.1021675	round-robin dns;throughput;parallel computing;real-time computing;process capability;performance;heavy-tailed distribution;resource allocation;computer science;load balancing;operating system;distributed computing;interference;information technology;educational assessment;statistics;computer network	Metrics	-15.997644348219836	60.14928393244673	47327
bd82324d5f88897fd4c23b6b64d9a6cab97e0651	research on real-time performance testing methods for robot operating system	operating system	To test real-time performance of RGMP-ROS, a robot operating system, this paper takes an in-depth study of the basic functions of real-time operating system kernel. By analyzing the main factors that affect the real-time performance of an operating system, we propose a set of real-time performance testing methods based on mixed load. This paper introduces the concept of the calibration procedure program. Takes the average execution time of the program as the standard time unit to normalize the test results of each test indicator, so as to shield the effect of the hardware test environment on the test results to a certain extent. While testing task preemption time and interrupt response time, we use background tasks that can trigger banning preemption or disabling interrupts to simulate the load, which makes the test results more real and reliable.	deployment environment;interrupt;kernel (operating system);preemption (computing);real-time clock;real-time operating system;real-time transcription;response time (technology);robot operating system;run time (program lifecycle phase);simulation;software performance testing	Ming Huang;Shujie Guo;Xu Liang;Xudong Song	2014	JSW		embedded system;real-time computing;load testing;software performance testing;computer science;operating system	Embedded	-11.552957443554261	57.37039829449099	47673
9c19732cea945a70d190a4efabf8e76cf5f5c5d4	pipors: a parallel input parallel output register switching system	central shared memory switching system;communication system;switched system;shared memory;time division multiplex;performance comparison;parallel input parallel output register switching system;data exchange;statistical time division multiplexing technique;input output;distributed shared memory;loss probability;high speed;data transfer	In order to make data exchange speed fast enough for supporting the current communication systems or networks, a high speed switching system with low transmission delay and low data loss is required. Many researchers used statistical time division multiplexing techniques to design the switching system for achieving a higher throughput. In such switching systems with n input/output ports, the internal execution speed must be n times faster than the speed of the system with single input/output port. This designing philosophy is really not an appropriate way as the demand trend for higher speed system in the future. For improving the drawbacks of the switching system mentioned above, a novel, revolutionary architecture of a Parallel Input Parallel Output Register Switching System (PIPORS) is proposed in this paper. The PIPORS is based on the interconnection of the small distributed Shared Memory Modules (SMM) and the Shift Register Switch Array (SRSA). This construction will accelerate the switching speed. In addition, the number of input/output ports of the system can easily be extended for providing a higher capacity to respond to the trend of fast increasing amount of data transferred in the system. Three simple methods to extend the input/output ports and the capacity of the internal memory are presented. For evaluating the performance of the proposed system, we made some performance comparisons among our PIPORS and Central Shared Memory Switching System (CSMS) with respect to the amount 0045-7906/$ see front matter 2004 Elsevier Ltd. All rights reserved. doi:10.1016/j.compeleceng.2004.02.002 q This material is based upon work supported by Tatung University under Grant no. B90-1600-05. * Corresponding author. Tel.: +886 2 2592 5252x3263. E-mail addresses: ltlee@cse.ttu.edu.tw (L.-T. Lee), tdf@ms8.hinet.net (D.-F. Tao). 428 D.-F. Tao, L.-T. Lee / Computers and Electrical Engineering 30 (2004) 427–440 of total memory required, data loss probability, transmission delay and switching performance. It shows that a better performance can be achieved in our PIPORS. 2004 Elsevier Ltd. All rights reserved.	computer data storage;dimm;distributed shared memory;electrical engineering;electronic switching system;input/output;interconnection;memory module;multiplexing;parallel computing;shift register;throughput	Der-Fu Tao;Liang-Teh Lee	2004	Computers & Electrical Engineering	10.1016/j.compeleceng.2004.02.002	data exchange;distributed shared memory;shared memory;input/output;parallel computing;real-time computing;computer hardware;computer science;cut-through switching;operating system;communications system	HPC	-11.32570902486994	52.1309679125093	47762
1f5c67abf4525bb151846a64fa4a10141e1a07f8	tightening the bounds on cache-related preemption delay in fixed preemption point scheduling		Limited Preemptive Fixed Preemption Point scheduling (LP-FPP) has the ability to decrease and control the preemption-related overheads in the real-time task systems, compared to other limited or fully preemptive scheduling approaches. However, existing methods for computing the preemption overheads in LP-FPP systems rely on over-approximation of the evicting cache blocks (ECB) calculations, potentially leading to pessimistic schedulability analysis. In this paper, we propose a novel method for preemption cost calculation that exploits the benefits of the LP-FPP task model both at the scheduling and cache analysis level. The method identifies certain infeasible preemption combinations, based on analysis on the scheduling level, and combines it with cache analysis information into a constraint problem from which less pessimistic upper bounds on cache-related preemption delays (CRPD) can be derived. The evaluation results indicate that our proposed method has the potential to significantly reduce the upper bound on CRPD, by up to 50% in our experiments, compared to the existing over-approximating calculations of the eviction scenarios. 1998 ACM Subject Classification C.3 Real-Time and Embedded Systems	approximation;cpu cache;embedded system;experiment;fixed-priority pre-emptive scheduling;lp-type problem;preemption (computing);real-time clock;real-time transcription;scheduling (computing);scheduling analysis real-time systems	Filip Markovic;Jan Carlson;Radu Dobrin	2017		10.4230/OASIcs.WCET.2017.4	cache;real-time computing;scheduling (computing);computer science;preemption	Embedded	-8.84535469481189	60.29037264582197	47895
62461a66ab0176b1054871b0c59c545baf4a4754	worst-case energy consumption minimization based on interference analysis and bank mapping in multicore systems	real-time systems;worst-case execution time;worst-case energy consumption	Energy is a scarce resource in real-time embedded systems due to the fact that most of them run on batteries. Hence, the designers should ensure that the energy constraints are satisfied in addition to the deadline constraints. This necessitates the consideration of the impact of the interference due to shared, low-level hardware resources such as the cache on the worst-case energy consumption of the tasks. Toward this aim, this article proposes a fine-grained approach to analyze the bank-level interference (bank conflict and bus access interference) on real-time multicore systems, which can reasonably estimate runtime interferences in shared cache and yield tighter worst-case energy consumption. In addition, we develop a bank-to-core mapping algorithm for reducing bank-level interference and improving the worst-case energy consumption. The experimental results demonstrate that our approach can improve the tightness of worst-case energy consumption by 14.25% on average compared to upper-bound delay approach. The bank-to-core mapping provides significant benefits in worst-case energy consumption reduction with 7.23%.	algorithm;best, worst and average case;cpu cache;computer memory;declaration (computer programming);embedded system;filter bank;high- and low-level;interference (communication);multi-core processor;real-time clock;real-time computing;real-time transcription	Zhihua Gan;Zhimin Gu;Hai Tan;Mingquan Zhang;Jizan Zhang	2017	IJDSN	10.1177/1550147716686969	telecommunications	Embedded	-4.984843617611601	58.13824836433905	47935
10818733bad4a86f77da645accc6c49d0e8db7c0	distributed logging for transaction processing	distributed transactions;low latency;communication protocol;transaction processing;high performance;data structure	Increased interest in using workstations and small processors for distributed transaction processing raises the question of how to implement the logs needed for transaction recovery. Although logs can be implemented with data written to duplexed disks on each processing node, this paper argues there are advantages if log data is written to multiple log server nodes. A simple analysis of expected logging loads leads to the conclusion that a high performance, microprocessor based processing node can support a log server if it uses efficient communication protocols and low latency, non volatile storage to buffer log data. The buffer is needed to reduce the processing time per log record and to increase throughput to the logging disk. An interface to the log servers using simple, robust, and efficient protocols is presented. Also described are the disk data structures that the log servers use. This paper concludes with a brief discussion of remaining design issues, the status of a prototype implementation, and plans for its completion.	central processing unit;data structure;distributed transaction;microprocessor;prototype;server (computing);throughput;transaction processing;volatile memory;workstation	Dean S. Daniels;Alfred Z. Spector;Dean S. Thompson	1987		10.1145/38713.38728	communications protocol;log shipping;real-time computing;two-phase commit protocol;data structure;transaction processing;distributed transaction;computer science;transaction log;web log analysis software;database;distributed computing;online transaction processing;programming language;transaction processing system;low latency	OS	-13.991456680343655	51.55275941678359	48043
b30a20af3302a1f2cc97602c1001ccb49bc0aabe	on the performance of magnetic bubble memories in computer systems	queueing theory;computer architecture magnetic bubbles major minor loop memory organization queueing theory;major minor loop;magnetic bubbles;chip;computer architecture;performance improvement;queueing model;memory organization	This correspondence describes simple queueing models for a magnetic bubble memory chip organization. Some of the unique features of bubble memories are outlined. The performance improvement due to bidirectional shifting is modeled as a function of program locality.	bubble bobble;bubble memory;locality of reference;queueing theory	Dileep Bhandarkar	1975	IEEE Transactions on Computers	10.1109/T-C.1975.224145	chip;parallel computing;real-time computing;computer hardware;telecommunications;computer science;memory organisation;queueing theory	Visualization	-11.102641187382215	49.11030575188232	48396
76fc6a0cd4caf08e279b3b5d1a94de42abac9a71	avss: an adaptable virtual storage system	storage allocation;resource management application virtualization resource virtualization performance analysis interference technology management grid computing computers fluctuations costs;layout reorganization;storage system;data access frequency statistics;resource allocation;disc storage;resource management;allocate on demand;heating;layout;virtual storage disc storage resource allocation storage allocation;yfq algorithm;media;virtual disk;adaptable virtual storage system;storage layout reorganization;design and implementation;bandwidth resource allocation;data access;bandwidth;storage layout reorganization adaptable virtual storage system storage system capacity virtualization storage system performance virtualization yfq algorithm bandwidth resource allocation virtual disk hierarchy structure dynamic mapping mechanism heterogeneous storage resource management data access frequency statistics;performance isolation;dynamic address mapping;hierarchy structure;layout reorganization virtual storage performance isolation hierarchy structure dynamic address mapping allocate on demand behavior analysis;behavior analysis;heterogeneous storage resource management;virtual storage;storage system performance virtualization;file systems;dynamic mapping mechanism;dynamic scheduling;storage system capacity virtualization;dynamic behavior	This paper presents the design and implementation of the Adaptable Virtual Storage System (AVSS) and introduces the capacity virtualization and performance virtualization for storage systems. AVSS has the following characteristics: 1) adoption of extended YFQ algorithm to control the allocation of bandwidth resources, realization of the performance isolation and guarantees of virtual disks; 2) adoption of hierarchy structure and dynamic mapping mechanism to manage heterogeneous storage resources flexibly and effectively, which lays a foundation for allocating storage resources on demand; 3) application of data-access frequency statistics and dynamic behavior analysis to supervise storage layout reorganization. The experimental results proved the correctness of our design. AVSS can isolate different applications and avoid performance interference. It can adjust storage layout according to the behavior of applications and improve the utilization of storage resources while improving the performance of the storage system.	algorithm;computer data storage;correctness (computer science);interference (communication);x86 virtualization	Jian Ke;Xudong Zhu;Wenwu Na;Lu Xu	2009	2009 9th IEEE/ACM International Symposium on Cluster Computing and the Grid	10.1109/CCGRID.2009.42	layout;data access;parallel computing;real-time computing;media;converged storage;dynamic priority scheduling;resource allocation;computer science;resource management;operating system;bandwidth	Arch	-14.757349984024687	55.96779911599544	48408
50bea4bcef2cb12cedcf72d2b62c3d413f9df7ae	less watts, more performance: an intelligent storage engine for data appliances	energy;ibex;fpga;intelligent;data appliance;storage	In this demonstration, we present Ibex, a novel storage engine featuring hybrid, FPGA-accelerated query processing. In Ibex, an FPGA is inserted along the path between the storage devices and the database engine. The FPGA acts as an intelligent storage engine supporting query off-loading from the query engine. Apart from significant performance improvements for many common SQL queries, the demo will show how Ibex reduces data movement, CPU usage, and overall energy consumption in database appliances.	central processing unit;database engine;field-programmable gate array;sql;watts humphrey	Louis Woods;Jens Teubner;Gustavo Alonso	2013		10.1145/2463676.2463685	embedded system;intelligence;energy;computer hardware;computer science;world wide web;field-programmable gate array	DB	-15.106285698369847	54.98095553857848	48426
74dfbbea4fa8c0137b1fecdd9e825934bddce7c7	dynamic streamization model execution for simd engines on multicore architectures	streamization dynamic technique multicore performance single instruction multiple data;computer architecture;parallel processing computer architecture multiprocessing systems;dynamic streamization model execution compiler extension gp processor design dynamic placement single instruction multiple data dynamic vectorization technique dsme multicore architecture simd engines;engines multicore processing hardware benchmark testing programming kernel;multiprocessing systems;parallel processing	This paper proposes dynamic streamization model execution (DSME), a dynamic vectorization technique for single instruction multiple data (SIMD) engines on multicore architectures. The technique uses stream model as intermediate representation for programs to optimize the combination of computation and memory accesses of SIMD engines in general-purpose (GP) designs. DSME allows the dynamic placement of computations on different cores when they are not in use to utilize multiple SIMD engines. This study also discusses hardware extensions to existing GP processor designs as well as related compiler extensions that use the special hardware components. Our extensive experiments demonstrate that performance gains of DSME can be achieved.	automatic vectorization;central processing unit;compiler;computation;experiment;general-purpose markup language;general-purpose programming language;intermediate representation;memory bandwidth;multi-core processor;simd;stream processing	Libo Huang;Zhiying Wang;Nong Xiao;Yongwen Wang;Qiang Dou	2013	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2013.2272537	parallel processing;computer architecture;parallel computing;real-time computing;computer science;operating system	EDA	-6.005993953499755	46.73933029712937	48538
ef96d2212d8660f4c7fd6636f1998de7f76007e6	a general-purpose counting filter: making every bit count	network monitoring;bloom filters and hashing;computational biology;sketching and sampling	Approximate Membership Query (AMQ) data structures, such as the Bloom filter, quotient filter, and cuckoo filter, have found numerous applications in databases, storage systems, networks, computational biology, and other domains. However, many applications must work around limitations in the capabilities or performance of current AMQs, making these applications more complex and less performant. For example, many current AMQs cannot delete or count the number of occurrences of each input item, take up large amounts of space, are slow, cannot be resized or merged, or have poor locality of reference and hence perform poorly when stored on SSD or disk. This paper proposes a new general-purpose AMQ, the counting quotient filter (CQF). The CQF supports approximate membership testing and counting the occurrences of items in a data set. This general-purpose AMQ is small and fast, has good locality of reference, scales out of RAM to SSD, and supports deletions, counting (even on skewed data sets), resizing, merging, and highly concurrent access. The paper reports on the structure's performance on both manufactured and application-generated data sets.  In our experiments, the CQF performs in-memory inserts and queries up to an order-of magnitude faster than the original quotient filter, several times faster than a Bloom filter, and similarly to the cuckoo filter, even though none of these other data structures support counting. On SSD, the CQF outperforms all structures by a factor of at least 2 because the CQF has good data locality.  The CQF achieves these performance gains by restructuring the metadata bits of the quotient filter to obtain fast lookups at high load factors (i.e., even when the data structure is almost full). As a result, the CQF offers good lookup performance even up to a load factor of 95%. Counting is essentially free in the CQF in the sense that the structure is comparable or more space efficient even than non-counting data structures (e.g., Bloom, quotient, and cuckoo filters).  The paper also shows how to speed up CQF operations by using new x86 bit-manipulation instructions introduced in Intel's Haswell line of processors. The restructured metadata transforms many quotient filter metadata operations into rank-and-select bit-vector operations. Thus, our efficient implementations of rank and select may be useful for other rank-and-select-based data structures.	64-bit computing;approximation algorithm;bit manipulation;bloom filter;central processing unit;computation;computational biology;concurrency control;data structure;difference quotient;experiment;general-purpose markup language;general-purpose modeling;hash table;haswell (microarchitecture);in-memory database;kalman filter;locality of reference;lookup table;parallel computing;quotient filter;random-access memory;solid-state drive;the cuckoo's egg;x86	Prashant Pandey;Michael A. Bender;Rob Johnson;Robert Patro	2017		10.1145/3035918.3035963	real-time computing;computer hardware;computer science;theoretical computer science;network monitoring	DB	-13.212256480733116	53.278365288372505	48704
7d12976d3d7180a18cba28ee265ad515c38f120b	spmt wavecache: exploiting thread-level parallelism in wavescalar	yarn parallel processing multithreading hardware computer science computer architecture instruction sets out of order program processors protocols;cache storage;speculative multithreading;thread level parallelism;multi threading;yarn;superscalar architecture;wavecache architecture;wavescalar isa;hazards;transactional memory system;spmt wavecache;out of order;thread context table;computer architecture;thread memory history;memory architecture;transactional wavecache;mibench;thread level transaction;speculative execution;multi threading cache storage memory architecture;transactional memory;context;spec;transactional wavecache spmt wavecache thread level parallelism wavescalar isa speculative multithreading transactional memory system wavecache architecture thread context table thread level transaction thread memory history spec mediabench mibench superscalar architecture;multithreading;mediabench	Speculative Multithreading (SpMT) increases the performance by means of executing multiple threads speculatively to exploit thread-level parallelism. By combining software and hardware approaches, we have improved the capabilities of previous WaveScalar ISA on the basis of Transactional Memory system for the WaveCache Architecture. Threads are extracted at the course of static compiling, and speculatively executed as a thread-level transaction that is supported by extra hardware components, such as Thread-Context-Table (TCT) and Thread-Memory-History (TMH). We have evaluated the SpMT WaveCache with 6 real benchmarks from SPEC, Mediabench and Mibench. On the whole, the SpMT WaveCache outperforms superscalar architecture ranging from 2X to 3X, and great performance gains are achieved over original WaveCache and Transactional WaveCache as well.	cache coherence;compiler;deployment environment;elias delta coding;instruction-level parallelism;mathematical optimization;parallel computing;putnam model;run time (program lifecycle phase);serial port memory technology;simultaneous multithreading;software transactional memory;speculative execution;speculative multithreading;superscalar processor;task parallelism;the coroner's toolkit;transactional memory	Songwen Pei;Baifeng Wu;Min Du;Gang Chen;Leandro A. J. Marzulo;Felipe Maia Galvão França	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.35	computer architecture;parallel computing;real-time computing;multithreading;computer science	Arch	-7.6573097591258446	50.103719365273356	48914
caa30c85c618c89cf34ffcc656c15ee11a866a9b	an online self-adaptive system management technique for multi-core systems		Abstract   This paper presents a light-weight online system adaptation technique for multi-core embedded systems running multiple applications. Thus far, online system adaptation techniques are restricted to reconfiguring resource management schemes such as operating frequency scaling or task-to-processor assignment. Additionally, in this paper, we enable to adapt the algorithm at runtime. That is, we selectively apply a suitable algorithm considering the system status when more than one algorithm candidates exist in the target application. We propose a generic and extensible self-adaptive framework with multiple applications in mind by providing generic programming interface that requires minimal changes in the legacy software code. It is shown that the proposed framework adaptively optimizes both resource management and algorithm selection with negligible performance overhead. The effectiveness of the proposed framework is experimentally proven with real-life examples.	adaptive system;multi-core processor	Hyunwoo Kim;Hoeseok Yang	2016		10.1016/j.procs.2016.04.204	real-time computing;computer science;artificial intelligence;theoretical computer science;operating system;data mining;distributed computing;algorithm	EDA	-13.3031899463024	57.98031981912709	48936
1ae2d637edb6742f68c2b5dc4f560a8bc87e78fe	multi-program benchmark definition	measurement;multiprogramming;runtime;servers;multicore processing;multiprogram benchmark sampling multiprogram benchmark definition program runtime program interaction;benchmark testing measurement runtime servers program processors planning multicore processing;planning;program processors;benchmark testing;conference proceeding	Although definition of single-program benchmarks is relatively straight-forward-a benchmark is a program plus a specific input-definition of multi-program benchmarks is more complex. Each program may have a different runtime and they may have different interactions depending on how they align with each other. While prior work has focused on sampling multiprogram benchmarks, little attention has been paid to defining the benchmarks in their entirety. In this work, we propose a four-tuple that formally defines multi-program benchmarks in a well-defined way. We then examine how four different classes of benchmarks created by varying the elements of this tuple align with real-world use-cases. We evaluate the impact of these variations on real hardware, and see drastic variations in results between different benchmarks constructed from the same programs. Notable differences include significant speedups versus slowdowns (e.g., +57% vs -5% or +26% vs -18%), and large differences in magnitude even when the results are in the same direction (e.g., 67% versus 11%).	align (company);benchmark (computing);calculus of variations;computer multitasking;interaction;sampling (signal processing)	Adam N. Jacobvitz;Andrew D. Hilton;Daniel J. Sorin	2015	2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)	10.1109/ISPASS.2015.7095786	planning;multi-core processor;benchmark;computer architecture;parallel computing;real-time computing;computer multitasking;computer science;operating system;server;measurement	Arch	-4.665327016156031	49.03843305880483	49346
0a5e20769fc8306a24d4426278493a999438dd42	aptstore: dynamic storage management for hadoop	storage management;distributed processing;software fault tolerance;storage management distributed processing public domain software software fault tolerance;public domain software;hadoop distributed file system aptstore dynamic storage management direct attached storage das hadoop i o throughput fault tolerance dynamic data management system storage cost reduction tiered storage design network attached enterprise filers file popularity prediction algorithm ppa file system audit log analysis transparent data movement application execution time trace driven simulations;throughput bandwidth prediction algorithms standards algorithm design and analysis fault tolerance fault tolerant systems	Typical Hadoop setups employ Direct Attached Storage (DAS) with compute nodes and uniform replication of data to sustain high I/O throughput and fault tolerance. However, not all data is accessed at the same time or rate. Thus, if a large replication factor is used to support higher throughput for popular data, it wastes storage by unnecessarily replicating unpopular data as well. Conversely, if less replication is used to conserve storage for the unpopular data, it means fewer replicas for even popular data and thus lower I/O throughput. We present AptStore, a dynamic data management system for Hadoop, which aims to improve overall I/O throughput while reducing storage cost. We design a tiered storage that uses the standard DAS for popular data to sustain high I/O throughput, and network-attached enterprise filers for cost-effective, fault-tolerant, but lower-throughput storage for unpopular data. We design a file Popularity Predictor (PP) that analyzes file system audit logs and predicts the appropriate storage policy of each file, as well as use the information for transparent data movement between tiers. Our evaluation of AptStore on a real cluster shows 21.3% improvement in application execution time over standard Hadoop, while trace driven simulations show 23.7% increase in read throughput and 43.4% reduction in the storage capacity requirement of the system.	apache hadoop;database;direct-attached storage;dynamic data;fault tolerance;hierarchical storage management;input/output;memory hierarchy;ppa (complexity);run time (program lifecycle phase);simulation;software deployment;throughput	R. K. KrishK.;Aleksandr Khasymski;Ali Raza Butt;Sameer Tiwari;Milind Bhandarkar	2013	2013 IEEE 5th International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2013.12	real-time computing;storage area network;converged storage;object storage;computer science;operating system;database;information repository;public domain software;software fault tolerance	HPC	-14.588125255349672	54.148952711570495	49606
4f1ef379673ba60f79672eebd3d02ab0d13fca1a	a fill-unit approach to multiple instruction issue	instruction level parallel;cache storage;multiplication operator;superscalar;branch prediction;fill unit multiple instruction superscalar vliw machines code compatibility shadow cache scalar instruction stream mips architecture;vliw;parallel architectures;multiple operation issue;vliw decoding hardware parallel processing permission out of order clocks logic design delay distributed computing;machine design;data dependence;speculative execution;functional unit;cache storage parallel architectures;instruction level parallelism	Multiple issue of instructions occurs in superscalar and VLIW machines. This paper investigates a third type of machine design, which combines the advantages of code compatibility as in superscalars and the absence of complex dependency-checking logic from the decoder as in VLIW. In this design, a stream of scalar instructions is executed by the hardware and is simultaneously compacted into VLIW-type instructions, which are then stored in a structure called a shadow cache. When a shadow cache line contains the instructions requested by the fetch unit, the scalar instruction stream is preempted and all operations in the shadow cache line are simultaneously issued and executed. The mechanism that compacts instructions is called a fill unit, and was first proposed for dynamically compacting microoperations into large executable units by Melvin, Shebanow, and Patt in 1988. We have extended their approach to directly handle data dependencies, delayed branches, and speculative execution (using branch prediction). This approach is evaluated using the MIPS architecture, and a six-functional-unit machine is found to be 52 to 108% faster than a single-issue processor for unrecompiled SPECint92 benchmarks.	branch predictor;cpu cache;data dependency;executable;micro-operation;speculative execution;superscalar processor;very long instruction word	Manoj Franklin;Mark Smotherman	1994		10.1145/192724.192748	multiplication operator;computer architecture;parallel computing;real-time computing;computer science;very long instruction word;superscalar;operating system;programming language;instruction-level parallelism;branch predictor;instructions per cycle;speculative execution	Arch	-7.728002706227651	50.81007954268294	49620
bd22da0a31d444aa314675dab4084fa67ea55ce0	web services for mpi-based parallel applications on a rocks cluster	libraries;scientific application;cluster computing;software tool;parallel programming;web service;web services computer applications middleware simple object access protocol concurrent computing high performance computing writing application software programming profession software tools;numerical calculation;servers;scientific applications web services mpi rocks cluster cluster computing;time factors;design and implementation;web services;xml;message passing;access protocols;middleware;scientific applications;scientific communication;mpi;xml access protocols message passing middleware open systems parallel programming web services workstation clusters;workstation clusters;simple object access protocol;open systems;rocks cluster;experience base;parallel applications;xml mpi based parallel application rocks cluster scientific application mpi sv middleware gsoap web service interface mpich parallel software tool function call soap standard specification interoperability	MPI-based parallel applications including scientific applications are now widely executed on clusters and grids, and great benefits have been brought to scientific community. However, writing parallel applications would not be easy even for experienced programmers. In this paper, we propose the design and implementation of the MPI-SV middleware that connects gSOAP, the Web service interface, and MPICH, the parallel software tool running on a Rocks cluster. With MPI-SV, users can automatically call parallel functions executed on a cluster as if they were regular functions. Hence, users do not have to write MPI-based parallel functions themselves. MPI-SV middleware is implemented on a Rocks cluster and complies with the SOAP standard specification so that it could interoperable with other Web services. Three experiments based on numerical calculations are conducted, and the results show that the response time is increased when the data size increases. Even though sending data in MPI-SV via XML in SOAP incurs high communication overhead, our experimental results show that, for a parallel application requiring high computation, the overhead time would be low when compared with computation time. Thus, MPI-SV would give good performance for applications that the computation time dominates the communication time, and fortunately most scientific applications have high computation.	computation;computer cluster;experiment;grid computing;interoperability;mpich;message passing interface;middleware;numerical analysis;overhead (computing);programmer;programming tool;response time (technology);rocks cluster distribution;soap;systemverilog;time complexity;web service;world wide web;xml;gsoap	Pitch Sajjipanon;Sudsanguan Ngamsuriyaroj	2008	2008 IEEE Asia-Pacific Services Computing Conference	10.1109/APSCC.2008.43	web service;computer science;operating system;database;distributed computing;programming language;law	HPC	-17.809201880169173	50.131367314376035	49972
51098280164dcc12b1ef69632430a8a362b70452	an in-memory object caching framework with adaptive load balancing		The extreme latency and throughput requirements of modern web applications are driving the use of distributed in-memory object caches such as Memcached. While extant caching systems scale-out seamlessly, their use in the cloud --- with its unique cost and multi-tenancy dynamics --- presents unique opportunities and design challenges.  In this paper, we propose MBal, a high-performance in-memory object caching framework with adaptive <u>M</u>ultiphase load <u>B</u>alancing, which supports not only horizontal (scale-out) but vertical (scale-up) scalability as well. MBal is able to make efficient use of available resources in the cloud through its fine-grained, partitioned, lockless design. This design also lends itself naturally to provide adaptive load balancing both within a server and across the cache cluster through an event-driven, multi-phased load balancer. While individual load balancing approaches are being lever-aged in in-memory caches, MBal goes beyond the extant systems and offers a holistic solution wherein the load balancing model tracks hotspots and applies different strategies based on imbalance severity -- key replication, server-local or cross-server coordinated data migration. Performance evaluation on an 8-core commodity server shows that compared to a state-of-the-art approach, MBal scales with number of cores and executes 2.3x and 12x more queries/second for GET and SET operations, respectively.	cache (computing);cloud computing;commodity computing;event-driven programming;holism;in-memory database;load balancing (computing);memcached;multitenancy;performance evaluation;requirement;sxal/mbal;scalability;server (computing);throughput;web application;web cache	Yue Cheng;Aayush Gupta;Ali Raza Butt	2015		10.1145/2741948.2741967	parallel computing;real-time computing;computer science;operating system;distributed computing	OS	-15.72486067606047	54.125402700270485	50022
460fa8140dd4319f82c6939bc1139d04a37853d0	evaluating replication for parallel jobs: an efficient approach	software reliability cloud computing concurrent engineering grid computing parallel processing quality of service;absorption;reliability;parallel job processing parallel cluster response time percentile slo service level objective replica failures subtask reliability job level replication stochastic model grid infrastructures cloud infrastructures resource pools;parallel job processing;quality of service parallel job processing performance analysis;time factors reliability correlation program processors computational modeling absorption servers;servers;computational modeling;time factors;performance analysis;correlation;quality of service;program processors	Many modern software applications rely on parallel job processing to exploit large resource pools available in cloud and grid infrastructures. The response time of a parallel job, made of many subtasks, is determined by the last subtask that finishes. Thus, a single laggard subtask or a failure, requiring re-processing, may increase the response time substantially. To overcome these issues, we explore concurrent replication with canceling. This mechanism executes two job replicas concurrently, and retrieves the result of the first replica that completes, immediately canceling the other one. To analyze this mechanism we propose a stochastic model that considers replication at both job-level and task-level. We find that task-level replication achieves a much higher reliability and shorter response times than job-level replication. We also observe that the impact of replication depends on the system utilization, the subtask reliability, and the correlation among replica failures. Based on the model, we propose a resource-provisioning strategy that determines the minimum number of computing nodes needed to achieve a service-level objective (SLO) defined as a response-time percentile. This strategy is evaluated by considering realistic traffic patterns from a parallel cluster, where task-level replication shows the potential to reduce the resource requirements for tight response-time SLOs.	concurrent computing;provisioning;reliability engineering;requirement;response time (technology)	Zhan Qiu;Juan F. Pérez	2016	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2015.2496593	absorption;parallel computing;real-time computing;quality of service;computer science;operating system;reliability;database;distributed computing;computational model;computer security;correlation;server;computer network	HPC	-18.976290976252795	60.21617061909472	50088
7e26a8f03db2d1bf1308f7dcc8fe03e1eff05f6b	resolving classical concurrency problems using adaptive conflictless scheduling		Classical concurrency problems define environments for task processing with high contention of shared resources. The adaptive conflictless scheduling is an alternative to existing synchronization mechanisms used in known solutions of concurrency problem to provide parallel tasks processing without resource conflicts. Used in this paper scheduling concept eliminates deadlock between tasks that are caused by the wrong allocation of shared resources, regardless of the specification of task environment. The adaptive conflictless scheduling, in opposition to other solutions dedicated to specific problem, is a universal approach and can be used to solve different concurrency problems. This article presents versatility of applications of adaptive conflictless scheduling by its usage in two classical concurrency problems: readers and writers and dining philosophers. Analysis of scheduling rules, when conflictless scheduling is applied, shows lack of task starvation in both classical concurrency problems. If conflictless scheduling is used, there is no need to use any other synchronization mechanisms for control tasks access to groups of shared resources. In presented approach only the programmer has to separate tasks and for each of them has to establish and report all required shared resources. Then tasks synchronization in accessing resource groups will be performed according to the adaptive conflictless schedule, which is the sequence of conflictless schedules. The concept of conflictless scheduling bases on efficient resource conflict detection between tasks by its resource identifiers and delayed execution of conflicted tasks using many FIFO queues assigned individually to each group of shared resources. Task queues are emptied based on the conflictless schedule, which is calculated every time according to specific state of the task processing environment.	concurrency (computer science);deadlock;dining philosophers problem;fifo (computing and electronics);file synchronization;identifier;programmer;scheduling (computing)	Mateusz Smolinski	2017	2017 IEEE International Conference on INnovations in Intelligent SysTems and Applications (INISTA)	10.1109/INISTA.2017.8001193	deadline-monotonic scheduling;fair-share scheduling;round-robin scheduling;dynamic priority scheduling;schedule;fixed-priority pre-emptive scheduling;two-level scheduling;computer science;distributed computing;rate-monotonic scheduling	Embedded	-10.831819555273373	59.89028523103136	50181
a0e902dad2943f08c4fc0860e8e9fd3cc0d2a286	blockwatch: leveraging similarity in parallel programs for error detection	software reliability error detection multiprocessing systems parallel programming program diagnostics software performance evaluation;program diagnostics;control data;software performance evaluation;parallel programming;spmd;multicore revolution blockwatch similarity leverage parallel program protection hardware error detection silicon devices computer systems high level programming models splash 2 benchmarks static analysis;multiprocessing systems;runtime hardware instruction sets multicore processing computers;error detection;runtime checks parallel programs control data spmd static analysis;static analysis;parallel programs;software reliability;runtime checks	The scaling of Silicon devices has exacerbated the unreliability of modern computer systems, and power constraints have necessitated the involvement of software in hardware error detection. Simultaneously, the multi-core revolution has impelled software to become parallel. Therefore, there is a compelling need to protect parallel programs from hardware errors. Parallel programs' tasks have significant similarity in control data due to the use of high-level programming models. In this study, we propose BLOCKWATCH to leverage the similarity in parallel program's control data for detecting hardware errors. BLOCKWATCH statically extracts the similarity among different threads of a parallel program and checks the similarity at runtime. We evaluate BLOCKWATCH on seven SPLASH-2 benchmarks to measure its performance overhead and error detection coverage. We find that BLOCKWATCH incurs an average overhead of 16% across all programs, and provides an average SDC coverage of 97% for faults in the control data.	central processing unit;columbia (supercomputer);computer;conformance testing;error detection and correction;exception handling;farid essebar;farid f. abraham;high- and low-level;high-level programming language;image scaling;multi-core processor;overhead (computing);posix threads;parallel computing;run time (program lifecycle phase);spmd;sensor;smart data compression	Jiesheng Wei;Karthik Pattabiraman	2012	IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2012)	10.1109/DSN.2012.6263959	computer architecture;parallel computing;real-time computing;error detection and correction;computer science;operating system;distributed computing;programming language;spmd;static analysis;software quality	Arch	-17.435942258376084	48.799921496235186	50246
3d2cc8b941c74e2b79bc41491c2946061989cb6c	up by their bootstraps: online learning in artificial neural networks for cmp uncore power management	supervised learning;training;artificial neural networks;multicore processing;mathematical model;tiles;neurons;artificial neural networks training neurons tiles mathematical model multicore processing supervised learning	With increasing core counts in Chip Multi-Processor (CMP) designs, the size of the on-chip communication fabric and shared Last-Level Caches (LLC), which we term uncore here, is also growing, consuming as much as 30% of die area and a significant portion of chip power budget. In this work, we focus on improving the uncore energy-efficiency using dynamic voltage and frequency scaling. Previous approaches are mostly restricted to reactive techniques, which may respond poorly to abrupt workload and uncore utility changes. We find, however, there are predictable patterns in uncore utility which point towards the potential of a proactive approach to uncore power management. In this work, we utilize artificial intelligence principles to proactively leverage uncore utility pattern prediction via an Artificial Neural Network (ANN). ANNs, however, require training to produce accurate predictions. Architecting an efficient training mechanism without a priori knowledge of the workload is a major challenge. We propose a novel technique in which a simple Proportional Integral (PI) controller is used as a secondary classifier during ANN training, dynamically pulling the ANN up by its bootstraps to achieve accurate predictions. Both the ANN and the PI controller, then, work in tandem once the ANN training phase is complete. The advantage of using a PI controller to initially train the ANN is a dramatic acceleration of the ANN's initial learning phase. Thus, in a real system, this scenario allows quick power-control adaptation to rapid application phase changes and context switches during execution. We show that the proposed technique produces results comparable to those of pure offline training without a need for prerecorded training sets. Full system simulations using the PARSEC benchmark suite show that the bootstrapped ANN improves the energy-delay product of the uncore system by 27% versus existing state-of-the-art methodologies.	artificial intelligence;artificial neural network;benchmark (computing);context switch;dynamic frequency scaling;dynamic voltage scaling;image scaling;multiprocessing;network switch;online and offline;parsec benchmark suite;power management;simulation;uncore	Jae-Yeon Won;Xi Chen;Paul V. Gratz;Jiang Hu;Vassos Soteriou	2014	2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2014.6835941	multi-core processor;parallel computing;simulation;computer science;artificial intelligence;operating system;machine learning;mathematical model;supervised learning	Arch	-5.694059944188359	55.80512816722325	50269
b33b795533155637ceaa2c89da8bd20794a34d51	convgpu: gpu management middleware in container based virtualized environment		Nowadays, Graphics Processing Unit (GPU) is essential for general-purpose high-performance computing, because of its dominant performance in parallel computing compare to that of CPU. There have been many successful trials on the use of GPU in virtualized environment. Especially, NVIDIA Docker obtained a most practical way to bring GPU into the container-based virtualized environment. However, most of these trials did not consider sharing GPU among multiple containers. Without the above consideration, a system will experience a program failure or a deadlock situation in the worst case. In this paper, we propose ConVGPU, a solution to share the GPU in multiple containers. With ConVGPU, the system can guarantee the required GPU memory which the container needs to execute. To achieve it, we introduce four scheduling algorithms that manage the GPU memory to be taken by the containers. These algorithms can prevent the system from falling into deadlock situations between containers during execution.	algorithm;best, worst and average case;central processing unit;cluster analysis;deadlock;docker;general-purpose markup language;graphics processing unit;hardware-assisted virtualization;middleware;parallel computing;sandbox (computer security);scheduling (computing);supercomputer;swarm	Daeyoun Kang;Tae Joon Jun;Dohyeun Kim;Jin Woo Kim;Daeyoung Kim	2017	2017 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2017.17	computer science;distributed computing;graphics processing unit;virtualization;deadlock;parallel computing;scheduling (computing);memory management;middleware;parallel processing	HPC	-9.130775974702308	50.13317310084621	50350
b2ac6855c3afb8b2396e66418467f0be50bfcc7c	gbrad: a general framework to evaluate design strategies for hybrid race detection		Data race detection is a method in testing multithreaded programs to ensure their reliability against concurrency errors. In this paper, we present the GBRAD framework to support the initialization of various hybrid race detection techniques, which also supports the evaluation of these strategies at two decision points based on two major design factors of hybrid race detectors. In the GBRAD frame-work, one decision point consists of six skipping strategies and another decision point consists of eight reduction strategies. By combining these strategies, 48 hybrid detection techniques are initialized. We report a controlled experiment on the PARSEC benchmark suite as well as four real-world applications to evaluate these 48 techniques and their strategies in terms of runtime slowdown, memory overhead, and race detection effectiveness. The experiment identified 9 previously unknown techniques that are comparable to the state-of-the-art hybrid race detection technique.	benchmark (computing);concurrency (computer science);overhead (computing);parsec benchmark suite;race condition;sensor;thread (computing)	Jialin Yang;Wing Kwong Chan;Yuen-Tak Yu;Jacky W. Keung	2018	2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2018.00024	memory management;real-time computing;synchronization;initialization;benchmark (computing);computer science;instruction set;concurrency	SE	-13.656359492662835	48.24285652130468	50375
a3ab68efb8ae2bb584c7f7e37ea99f306927dd74	self-aggregation techniques for load balancing in distributed systems	distributed systems load balancing self organization clustering autonomic computing;distributed system;topology;convergence;interconnected nodes;routing;resource allocation;distributed computing systems;software engineering distributed programming resource allocation;software engineering;self aggregation techniques;simulation experiment;autonomic self aggregation techniques self aggregation techniques load balancing software engineering distributed computing systems interconnected nodes;distributed computing system;clustering;aggregates;heuristic algorithms;distributed programming;load management;load balancing;load management clustering algorithms distributed computing aggregates convergence software engineering inference algorithms computer networks network topology throughput;clustering algorithms;self organization;load balance;distributed systems;autonomic computing;autonomic self aggregation techniques	One of the today issues in software engineering is to find new effective ways to deal intelligently with the increasing complexity of distributed computing systems. In this context a crucial role is played by the balancing of the work load among all nodes in a system composed of interconnected nodes that enter and exit the system without following any rule. To address this issue, we are experimenting with the usage of autonomic self-aggregation techniques that rewire the system in groups of homogeneous nodes that are then able to balance the load among each others using classical techniques. We present our approach together with some simulation experiments that show how the application of self- aggregation algorithms makes it possible to balance the load also in these extreme situations. Besides, our experiments show that the introduction of self-aggregation does not introduce a significant overhead in terms of execution time, even if it requires the exchange of a higher number of messages between nodes.	algorithm;autonomic computing;distributed computing;experiment;load balancing (computing);overhead (computing);review aggregator;run time (program lifecycle phase);simulation;software engineering	Elisabetta Di Nitto;Daniel J. Dubois;Raffaela Mirandola;Fabrice Saffre;Richard Tateson	2008	2008 Second IEEE International Conference on Self-Adaptive and Self-Organizing Systems	10.1109/SASO.2008.38	real-time computing;computer science;load balancing;theoretical computer science;distributed computing;cluster analysis;autonomic computing	DB	-16.711021607498505	58.957337580097345	50415
3c74d26a4a884419f92ed518b42c616435d19096	qos management on heterogeneous architecture for parallel applications	parallel application heterogeneous quality of service;quality of service multiprocessing systems parallel processing;quality of service system performance instruction sets multicore processing degradation benchmark testing;heterogeneous;quality of service management qos policies quickia real heterogeneous hardware task to core mapping policies parallelization pattern data sharing thread synchronization thread level parallelism parallel applications processor cores heterogeneous architecture cmp platforms chip multiprocessor platforms qos management;quality of service;parallel application	Quality of service (QoS) management is widely employed to provide differentiable performance to programs with distinctive priorities on conventional chip multi-processor (CMP) platforms. Recently, heterogeneous architecture integrating diverse processor cores on the same silicon has been proposed to better serve various application domains and it is expected to be an important design paradigm of future processors. Therefore, the QoS management on emerging heterogeneous systems will be of great significance. On the other hand, parallel applications are becoming increasingly important in modern computing community in order to explore the benefit of thread-level parallelism on CMPs. However, considering the diverse characteristics of thread synchronization, data sharing, and parallelization pattern, governing the execution of multiple parallel programs with different performance requirements becomes a complicated yet significant problem. In this paper, we study QoS management for parallel applications running on heterogeneous CMP systems. We comprehensively assess a series of task-to-core mapping policies on a real heterogeneous hardware (QuickIA) by characterizing their impacts on performance of individual applications. Our evaluation results show that the proposed QoS policies are effective to improve the performance of programs with highest priority while striking good tradeoff with system fairness.	central processing unit;concurrency (computer science);fairness measure;memory bandwidth;multiprocessing;parallel computing;programming paradigm;prototype;quality of service;requirement;synchronization (computer science);system of linear equations;task parallelism;throughput	Ying Zhang;Lu Zhao;Ramesh Illikkal;Ravi Iyer;Andrew Herdrich;Lu Peng	2014	2014 IEEE 32nd International Conference on Computer Design (ICCD)	10.1109/ICCD.2014.6974702	embedded system;parallel computing;real-time computing;quality of service;computer science;operating system;distributed computing;computer network	HPC	-5.608436019413302	51.758135640752485	50429
5f1dcb4ac655d5ef178230e050c46b098078641e	two-tier storage dbms for high-performance query processing	query processing;storage management;system performance;design and implementation;present day;dbms;high performance	This paper describes the design and implementation of a two-tier DBMS for handling massive data and providing faster response time. In the present day, the main requirements of DBMS are figured out using two aspects. The first is handling large amounts of data. And the second is providing fast response time. But in fact, Traditional DBMS cannot fulfill both the requirements. The disk-oriented DBMS can handle massive data but the response time is relatively slower than the memory-resident DBMS. On the other hand, the memory-resident DBMS can provide fast response time but they have original restrictions of database size. In this paper, to meet the requirements of handling large volumes of data and providing fast response time, a two-tier DBMS is proposed. The cold-data which does not require fast response times are managed by disk storage manager, and the hot-data which require fast response time among the large volumes of data are handled by memory storage manager as snapshots. As a result, the proposed system performs significantly better than diskoriented DBMS with an added advantage to manage massive data at the same time.	disk storage;multitier architecture;requirement;response time (technology)	Sang Hun Eo;Yan Li;Ho Seok Kim;Hae-Young Bae	2008	JIPS	10.3745/JIPS.2008.4.1.009	sargable;real-time computing;computer science;operating system;database;computer performance	DB	-18.068808081916025	53.51146055251381	50439
0160321dd366f7fb8686385fe43872cba31f28a6	maximizing speedup through self-tuning of processor allocation	compiler parallelized sequential programs;run time processor number self selection;optimisation;time dependent;performance evaluation;application speedup maximization;application software;processor scheduling;resource allocation;runtime velocity measurement program processors;dynamic job efficiency measurement;self adjusting systems;best possible static allocation application speedup maximization processor allocation self tuning run time processor number self selection data dependent peak speedups time dependent peak speedups dynamic job efficiency measurement hand coded parallel programs compiler parallelized sequential programs dynamic allocations;parallel programming;runtime;hand coded parallel programs;data dependent peak speedups;time dependent peak speedups;tuning;life testing;processor allocation self tuning;dynamic allocations;best possible static allocation;runtime system;computer science;velocity measurement;parallel programs;program processors;parallel applications;operating systems;hardware;optimization methods;self adjusting systems processor scheduling resource allocation optimisation parallel programming tuning	We address the problem of maximizing application speedup through runtime, self-selection of an appropriate number of processors on which to run. Automatic, runtime selection of processor allocations is important because many parallel applications exhibit peak speedups at allocations that are data or time dependent. We propose the use of a runtime system that: (a) dynamically measures job efficiencies at different allocations, (b) uses these measurements to calculate speedups, and (c) automatically adjusts a job’s processor allocation to maximize its speedup. Using a set of 10 applications that includes both hand-coded parallel programs and compiler-parallelized sequential programs, we show that our runtime system can reliably determine dynamic allocations that match the best possible static allocation, and that it has the potential to find dynamic allocations that outperform any static allocation.	central processing unit;compiler;critical section;emoticon;memory management;parallel computing;runtime system;self-tuning;speedup	Thu D. Nguyen;Raj Vaswani;John Zahorjan	1996		10.1109/IPPS.1996.508096	application software;parallel computing;real-time computing;resource allocation;computer science;operating system;distributed computing	Arch	-14.67526607739516	58.92939703280739	50522
2be113b082e77d934b54163141e6f8169d8bc159	pcow: pipelining-based cow snapshot method to decrease first write penalty	data protection	While Copy-On-Write (COW) snapshot is the popular technique for online data protection, its first write request suffers from severe performance penalty because 3 I/Os are needed for each data block after creating snapshot. This paper proposes Pipelining-based COW (PCOW) method to minimize the snapshot impact on the first write request. When origin data is read from origin volume to a buffer queue, pending host write request can be serviced immediately. Origin data will be deferred to write to the snapshot volume in a background asynchronous thread. In order to demonstrate PCOW feasibility and efficiency, we have implemented both COW and PCOW in a standard iSCSI target as independent modules. We use popular benchmarks to quantitatively compare PCOW and COW techniques. Numerical results show that PCOW can effectively decrease first write penalty and improve performance.	pipeline (computing);snapshot isolation	Zhikun Wang;Dan Feng;Ke Zhou;Fang Wang	2008		10.1007/978-3-540-68083-3_27	parallel computing;real-time computing;computer hardware;computer science;operating system;data protection act 1998	EDA	-12.570729547542419	53.48976569918584	50527
0c77c33ad84467ecaa722049156d213fa7b4d6c5	optimizing for parallelism and data locality	algorithms;cache line reuse;experimentation;parallel programming;data locality;utilizing parallelism;super computers;memory model;memory hierarchy;optimization;loop parallelization algorithm;memory optimizing;consecutive memory access;memory location;performance;parallel processing;parallel algorithm	Previous research has used program transformation to introduce parallelism and to exploit data locality. Unfortunately, these two objectives have usually been considered independently. This work explores the trade-offs between effectively utilizing parallelism and memory hierarchy on shared-memory multiprocessors. We present a simple, but surprisingly accurate, memory model to determine cache line reuse from both multiple accesses to the same memory location and from consecutive memory access. The model is used in memory optimizing and loop parallelization algorithms that effectively exploit data locality and parallelism in concert. We demonstrate the efficacy of this approach with very encouraging experimental results.	algorithm;locality of reference;memory address;memory hierarchy;optimizing compiler;parallel computing;program transformation;shared memory	Ken Kennedy;Kathryn S. McKinley	1992		10.1145/143369.143427	locality of reference;uniform memory access;memory model;parallel processing;computer architecture;parallel computing;computer science;memory-level parallelism;theoretical computer science;parallel algorithm;data parallelism;instruction-level parallelism;scalable parallelism;implicit parallelism;cache-only memory architecture;task parallelism;non-uniform memory access	PL	-6.081268956359625	49.163899680495675	50635
3592a9f29395e10ef751510d3a109c6718ae4118	cost, capacity, and performance analyses for hybrid scm/nand flash ssd	ssd mlc nand flash scm;ash performance evaluation phase change random access memory energy consumption memory management;nand circuits flash memories;read latency performance analysis storage class memories solid state drive hybrid scm mlc nand flash ssd next generation storage solution scm chip latency parameters scm chip design write latency	Storage class memories (SCMs) are fast and energy-efficient solid-state memories with high endurance. However, the bit cost of SCM is higher than that of NAND flash memory due to its relative production immaturity. As a cost-effective alternative to replace the conventional NAND flash-only solid-state drive (SSD), the hybrid SCM/MLC NAND SSD is a promising next generation storage solution. It is already understood that SCM capacity requirement depends on the workload characteristics. However, the optimum SCM capacity is also determined by the latency parameters of the SCM chips themselves. Therefore, in this paper, the dependencies of SCM capacity on SCM chip latency parameters are analyzed for the hybrid SCM/MLC NAND flash SSD. From the experimental results, increasing SCM capacity to accelerate the SSD performance is feasible when the write and read latencies of the SCM are below 1 μs. Furthermore, SSD energy consumption is more dependent on write rather than read latency. Finally, optimistic and pessimistic models of SCM area cost are considered in order to optimize the cost-efficiency of the SCM chip design in the hybrid SCM/MLC NAND SSD. According to the SCM area cost models, a SCM:NAND capacity ratio of at least 1.5% with a SCM read latency of less than 630 ns is recommended for the Financial1 workload, assuming that the SCM write/read latency ratio is 5.	flash memory;multi-level cell;nand gate;solid-state drive	Chao Sun;Tomoko Ogura Iwasaki;Takahiro Onagi;Koh Johguchi;Ken Takeuchi	2014	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2014.2309780	flash file system;parallel computing;real-time computing;computer hardware;computer science	Metrics	-10.060105008975931	54.80974754210649	50787
1a34288a2a19e378125d6baec93c95e129d3d35f	unifying thread-level speculation and transactional memory	base tm;transactional memory;common program;existing tm;hybrid approach;prominent concurrency;thread-level speculation;tls capability;multi-threaded application;long transaction	The motivation of this work is to ask whether Transactional Memory (TM) and Thread-Level Speculation (TLS), two prominent concurrency paradigms usually considered separately, can be combined into a hybrid approach that extracts untapped parallelism and speed-up from common programs. We show that the answer is positive by describing an algorithm, called TLSTM, that leverages an existing TM with TLS capabilities. We also show that our approach is able to achieve up to a 48% increase in throughput over the base TM, on read dominated workloads of long transactions in a multi-threaded application.	algorithm;concurrency (computer science);parallel computing;speculative multithreading;thread (computing);throughput;transactional memory	João Pedro Barreto;Aleksandar Dragojevic;Paulo Ferreira;Ricardo Filipe;Rachid Guerraoui	2012		10.1007/978-3-642-35170-9_10	parallel computing;real-time computing;computer science;distributed computing	Arch	-13.907786627766354	48.94747298336391	50820
9dc5e391b5bfb826c14fbc39a0d0806a8320e321	operating system impact on trace-driven simulation	operating system impact;performance evaluation;computer architecture;operating system;branch predictors;cache memories operating system impact trace driven simulation computer architecture benchmark execution branch predictors;performance evaluation computer architecture discrete event simulation operating systems computers;benchmark execution;trace driven simulation;operating systems computers;cache memories;operating systems;discrete event simulation	Trace-driven simulation is commonly used by the computer architecture research community to pursue answers to a wide variety of architectural design issues. Traces taken from benchmark execution have been extensively studied to optimize the design of pipelines, branch predictors, and especially cache memories. Today's computer designs have been optimized based on the characteristics of these benchmarks. One important aspect that has been ignored in a majority of these trace-driven studies is the e ect of the operating system interacting with the benchmark. It has been acknowledged that operating system overhead can introduce a level of interference that can limit the bene ts of new designs. The reason why the operating system has been, for the most part, ignored in these studies is the lack of readily available tools that can generate kernel-laden traces. In this paper we describe two tracing systems that allow the capture of operating system and application traces. We have captured traces of both benchmark and commercial applications using these tools. We show the e ects of including operating system activity while studying various architectural design tradeo s. We have found that the operating system can increase the number of instructions captured in the trace (for a given time sample) by as much as 100X, can signi cantly modify the instruction mix, and produce different ranges of reference locality. The results of this study show that the inherent characteristics of the applications will dictate the amount of overhead introduced by the operating system.	benchmark (computing);branch predictor;cpu cache;computer architecture;interaction;interference (communication);locality of reference;multiprocessing;operating system;overhead (computing);pipeline (software);simulation;tracing (software);x86	Jason P. Casmira;John Fraser;David R. Kaeli;Waleed Meleis	1998		10.1109/SIMSYM.1998.668444	embedded operating system;parallel computing;real-time computing;simulation;real-time operating system;computer science;discrete event simulation;operating system	Arch	-8.947602289442582	49.17515026079661	50954
95c3659ddf6c7f28dfb4710b69f09f501678441e	cache partitioning for energy-efficient and interference-free embedded multitasking	configurable cache;energy efficient;real time;cache memory;embedded system;chip;wcet;data cache;low power;leakage power;embedded multitasking;memory bandwidth	We propose a technique that leverages configurable data caches to address the problem of energy inefficiency and intertask interference in multitasking embedded systems. Data caches are often necessary to provide the required memory bandwidth. However, caches introduce two important problems for embedded systems. Caches contribute to a significant amount of power as they typically occupy a large part of the chip and are accessed frequently. In nanometer technologies, such large structures contribute significantly to the total leakage power as well. Additionally, cache outcomes in multitasking environments are notoriously difficult to predict, if not impossible, thus resulting in poor real-time guarantees. We study the effect of multiprogramming workloads on the data cache in a preemptive multitasking environment, and propose a technique which leverages configurable cache architectures to not only eliminate intertask cache interference, but also to significantly reduce both dynamic and leakage power. By mapping tasks to different cache partitions, interference is completely eliminated. Dynamic and leakage power are significantly reduced as only a subset of the cache is active at any moment. We introduce a profile-based, off-line algorithm, which identifies a beneficial cache partitioning. The OS configures the data cache during context-switch by activating the corresponding partition. Our experiments on a large set of multitasking benchmarks demonstrate that our technique not only efficiently eliminates intertask interference, but also significantly reduces both dynamic and leakage power.	cpu cache;cache (computing);catastrophic interference;computer multitasking;context switch;embedded system;experiment;interference (communication);memory bandwidth;online algorithm;online and offline;operating system;real-time clock;spectral leakage	Rakesh Reddy;Peter Petrov	2010	ACM Trans. Embedded Comput. Syst.	10.1145/1698772.1698774	chip;bus sniffing;embedded system;least frequently used;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;operating system;efficient energy use;smart cache;mesi protocol;cache algorithms;cache pollution;memory bandwidth;mesif protocol	Arch	-5.9322510896832465	55.4585841570639	51276
fa3f6610f91a47e989826211910cc7db502b628f	cache organizations	computer architecture;paging;memory organization;buffer memories;cache memories	The cache is a general buffer for addressable main memory. The incentives for using a cache are discussed along with the objectives of the cache designer. Three cache organizations; direct mapping, fully associative, and set associative are described using a single notation, and two caches are examined in detail. Interactions between the replacement algorithm and the degree of associativity are discussed as well as other possible approaches to cache design.	cpu cache;computer data storage;interaction;page replacement algorithm	Jeffrey J. Rothschild	1979		10.1145/503506.503524	bus sniffing;pipeline burst cache;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;cache coloring;page cache;cpu cache;tag ram;cache;computer science;write-once;cache invalidation;distributed computing;write buffer;smart cache;memory organisation;mesi protocol;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture	Arch	-11.456958985132417	48.45122118436672	51328
0b8060041bb38ec8d685dfabdca2be2fbc39d241	scheduling multiple multithreaded applications on asymmetric and symmetric chip multiprocessors	asymmetric multiprocessors;multi threading;instruction sets benchmark testing jitter throughput runtime measurement linux;measurement;processor scheduling;chip multiprocessor;thread assignment techniques multiple multithreaded applications scheduling asymmetric chip multiprocessors symmetric chip multiprocessors;runtime;operating system;scheduling;linux;scheduling asymmetric multiprocessors operating systems;experimental evaluation;jitter;processor scheduling multi threading;benchmark testing;operating systems;instruction sets;throughput	This paper evaluates new techniques to improve performance, fairness and jitter of workloads consisting of multiple multithreaded applications running on Chip MultiProcessors (CMP). Current thread assignment techniques which are tailored for single-thread applications result in sub-optimal usage of the multiprocessor resources, unfairness between applications and jitter in execution runtimes when dealing with multiple multithreaded applications running in parallel. Multithreaded applications contain serial phases (single thread) and parallel phases (many threads). In this paper, we propose a new thread assignment mechanism that takes into account the different requirements of each phase, granting higher priority to applications during their critical-serial phases. Analytic and experimental evaluation of the proposed thread assignment mechanism on both symmetric and asymmetric multiprocessors show throughput improvements by as much as 16%, improved fairness by as much as 26% and reduced jitter by as much as 88%.	asymmetric multiprocessing;fairness measure;multithreading (computer architecture);requirement;scheduling (computing);thread (computing);throughput	Tomer Y. Morad;Avinoam Kolodny;Uri C. Weiser	2010	2010 3rd International Symposium on Parallel Architectures, Algorithms and Programming	10.1109/PAAP.2010.50	computer architecture;parallel computing;real-time computing;computer science	Arch	-9.020777622507032	57.8168103033197	51659
e3d68ea6abcd23d3af3c00e9c807dd61762a9e8a	fast cycle-approximate mpsoc simulation based on synchronization time-point prediction	prediction method;os model;transaction level model;cycle approximate simulation;multiprocessor system on chip;static analysis;dynamic scheduling;dynamic behavior	In this paper, we propose techniques for fast cycle-approximate multi-processor SoC simulation with timed transaction level models and OS models. Cycle-approximate simulation with an abstract model is widely used for fast validation of a multi-processor SoC in early design stages. However, the performance gain of abstract-level simulation is limited by the overhead of synchronizing multiple concurrent processor/module simulators, which is inevitable in timed simulation. To reduce the synchronization overhead, we adopt the synchronization time-point prediction method, which consists of two phases: static code analysis and dynamic scheduling of synchronizations. In the static analysis phase before simulation, it estimates minimum execution time from every point in the code to the nearest synchronization point. Then, during simulation, it pessimistically predicts the synchronization time-points based on the estimates. The proposed approach targets fast cycle-approximate simulation of a system with delay annotated SW code and transaction level models of HW with dynamic behavior. We present, in this paper, techniques to analyze such abstract models of SW and HW and schedule minimal number synchronizations during cycle-approximate simulation of the models. Experiments show that the approach achieves orders of magnitude higher performance in cycle-approximate multi-processor SoC simulation.	approximation algorithm;mpsoc;simulation	Jinyong Jung;Sungjoo Yoo;Kiyoung Choi	2007	Design Autom. for Emb. Sys.	10.1007/s10617-007-9010-y	embedded system;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;static analysis	EDA	-7.508854108329431	59.768971711437814	51684
119773e02a6cf8f707c5e3c956daeaf5a0758066	author retrospective for bloom filtering cache misses for accurate data speculation and prefetching	cache hit miss prediction;bloom filter;prefetching;data speculation;instruction scheduling	"""In this paper, we provide the authors? retrospective analysis of the paper """"Bloom Filtering Cache Misses for Accurate Data Speculative and Prefetching"""" which was published in the proceedings of 2002 International Conference on Supercomputing.  DOI: http://dx.doi.org/10.1145/514191.514219"""	acm/ieee supercomputing conference;cpu cache;link prefetching;speculative execution	Jih-Kwon Peir;Shih-Chang Lai;Shih-Lien Lu;Jared Stark;Konrad Lai	2002		10.1145/2591635.2591664	parallel computing;real-time computing;computer science;bloom filter;database;instruction scheduling	HPC	-10.444848889578417	48.28100618807874	51693
1ae7be5d55833e6aa53d24f620be5df9006a3558	pipp: promotion/insertion pseudo-partitioning of multi-core shared caches	insertion;contention;multi core processor;cache;core competencies;last level cache;memory access;sharing;promotion;cache management;multi core	Many multi-core processors employ a large last-level cache (LLC) shared among the multiple cores. Past research has demonstrated that sharing-oblivious cache management policies (e.g., LRU) can lead to poor performance and fairness when the multiple cores compete for the limited LLC capacity. Different memory access patterns can cause cache contention in different ways, and various techniques have been proposed to target some of these behaviors. In this work, we propose a new cache management approach that combines dynamic insertion and promotion policies to provide the benefits of cache partitioning, adaptive insertion, and capacity stealing all with a single mechanism. By handling multiple types of memory behaviors, our proposed technique outperforms techniques that target only either capacity partitioning or adaptive insertion.	cpu cache;cache (computing);central processing unit;fairness measure;multi-core processor	Yuejian Xie;Gabriel H. Loh	2009		10.1145/1555754.1555778	bus sniffing;multi-core processor;pipeline burst cache;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;operating system;distributed computing;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol;non-uniform memory access;global assembly cache	HPC	-9.660489750322425	52.28450314720912	52037
8445dd076e8356bdc78ec47d00692e641e4733c2	gather-arrange-scatter: node-level request reordering for parallel file systems on multi-core clusters	gather arrange scatter architecture;buffer storage;file systems servers electronics packaging computer architecture benchmark testing bandwidth program processors;i o request;computer architecture;servers;performance improvement;node level request reordering architecture;buffer storage node level request reordering architecture parallel file system multicore cluster concurrent process i o request gather arrange scatter architecture;parallel file system;concurrency control;parallel processing buffer storage concurrency control multiprocessing systems;bandwidth;multiprocessing systems;electronics packaging;concurrent process;program processors;parallel processing;benchmark testing;multicore cluster;file systems	Multiple processors or multi-core CPUs are now in common, and the number of processes running concurrently is increasing in a cluster. Each process issues contiguous I/O requests individually, but they can be interrupted by the requests of other processes if all the processes enter the I/O phase together. Then, I/O nodes handle these requests as non-contiguous. This increases the disk seek time, and causes performance degradation. To overcome this problem, a node-level request reordering architecture, called gather-arrange-scatter (GAS) architecture, is proposed. In GAS, the I/O requests in the same node are gathered and buffered locally. Then, those are arranged and combined to reduce the I/O cost at I/O nodes, and finally they are scattered to the remote I/O nodes in parallel. A prototype is implemented and evaluated using the BTIO benchmark. This system reduces up to 84.3% of the lseekO calls and reduces up to 93.6% of the number of requests at I/O nodes. This results in up to a 12.7% performance improvement compared to the non-arranged case.	benchmark (computing);central processing unit;computer cluster;elegant degradation;hard disk drive performance characteristics;input/output;interrupt;multi-core processor;performance tuning;prototype	Kazuki Ohta;Hiroya Matsuba;Yutaka Ishikawa	2008	2008 IEEE International Conference on Cluster Computing	10.1109/CLUSTR.2008.4663792	parallel processing;benchmark;parallel computing;real-time computing;computer science;operating system;concurrency control;distributed computing;electronic packaging;bandwidth;i/o scheduling;server;computer network	HPC	-13.395524387031253	51.14126810683901	52272
eea090c7c67d1bbf6ad1ce92d8749e7106523921	a new processor architecture with a new program driving method	dependency.;program;instruction;processor;parallelism;processor architecture	This paper proposes a new type of processor architecture using a new program driving method which makes it possible for more programs to run in a single kernel processor concurrently without using interrupt technique. The main idea of the method is to collect all the program/instruction driving elements/factors to form a driving vector, called Program Driver (PD) for short, so that more than one PD can drive its own program to run in a single kernel processor concurrently.	interrupt;kernel (operating system);microarchitecture	Xiaobo Li;Ke Luo;Xiangdong Cui;Lalin Jiang;Xiaoqiang Ni;Chiyuan Ma;Jingfei Jiang;Huiping Zhou;Zhou Zhou	2006			computer architecture;parallel computing;trips architecture;dataflow architecture;computer science;processor register;pipeline burst cache;media processor;cellular architecture;microarchitecture	Arch	-6.417773247294307	48.263540117417065	52284
1b4226f0b0b259218285cf7f76e4dfa541354f7d	parallel implementation of irregular terrain model on ibm cell broadband engine	processing element;microprocessors;terrain profiles;propagation losses;multicore processor technology;computation kernels granularity irregular terrain model ibm cell broadband engine architecture radio coverage prediction radio signal median attenuation land mobile systems multicore processor technology hardware resources terrain profiles memory management overhead;design engineering;radio signal median attenuation;surveillance;cellular radio;real time;hardware resources;attenuation;cell broadband engine;computation kernels granularity;radio transmitters;computer architecture;distance measurement;computational modeling;engines;parallel architectures;system design;land mobile systems;multicore processing;ibm cell broadband engine architecture;memory management overhead;predictive models;terrain modeling;multicore processors;parallel implementation;radiowave propagation;radio coverage prediction;mobile systems;irregular terrain model;broadband communication;radio propagation;engines radio propagation propagation losses predictive models attenuation design engineering surveillance radio transmitters multicore processing partitioning algorithms;radiowave propagation cellular radio parallel architectures radio links;partitioning algorithms;radio links	Prediction of radio coverage, also known as radio “hear-ability” requires the prediction of radio propagation loss. The Irregular Terrain Model (ITM) predicts the median attenuation of a radio signal as a function of distance and the variability of the signal in time and in space. Algorithm can be applied to a large amount of engineering problems to make area predictions for applications such as preliminary estimates for system design, surveillance, and land mobile systems. When the radio transmitters are mobile, the radio coverage changes dynamically, taking on a real-time aspect that requires thousands of calculations per second, which can be achieved through the use of recent advances in multicore processor technology. In this study, we evaluate the performance of ITM on IBM Cell Broadband Engine (BE). We first give a brief introduction to the algorithm of ITM and present both the serial and parallel execution manner of its implementation. Then we exploit how to map out the program on the target processor in detail. We choose message queues on Cell BE which offer the simplest possible expression of the algorithm while being able to fully utilize the hardware resources. Full code segment and a complete set of terrain profiles fit into each processing element without the need for further partitioning. Communications and memory management overhead is minimal and we achieve 90.2% processor utilization with 7.9× speed up compared to serial version. Through our experimental studies, we show that the program is scalable and suits very well for implementing on the CELL BE architecture based on the granularity of computation kernels and memory footprint of the algorithm.	algorithm;cell (microprocessor);code segment;computation;heart rate variability;irish transverse mercator;jason;memory footprint;memory management;mercury;message queue;michael creutz;multi-core processor;overhead (computing);parallel computing;processor technology;radio wave;real-time transcription;scalability;software propagation;speedup;systems design;transmitter;workstation	Yang Song;Jeffrey A. Rudin;Ali Akoglu	2009	2009 IEEE International Symposium on Parallel & Distributed Processing	10.1109/IPDPS.2009.5161051	multi-core processor;embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing;computer network	Arch	-8.52773904874284	48.33260163563752	52421
8291741a4d38581d9f30ccb4b2f1828eef43efc0	the dash prototype: logic overhead and performance	atomic tests;coherent caches;shared memory;hardware performance monitor;performance evaluation;hardware performance monitors;storage management;shared memory multi processor;buffer storage;parallel programming;prototypes logic large scale systems hardware parallel architectures software prototyping costs protocols parallel programming scalability;index termsdash project;indexing terms;large scale shared memory multiprocessors;atomic tests dash project large scale shared memory multiprocessors directory based cache coherence coherent caches hardware performance monitor reference behavior dash protocol;large scale;reference behavior;shared memory systems;cache coherence;directory based cachecoherence;memory systems;parallel architecture;dashprotocol;parallel applications;storage management buffer storage parallel programming performance evaluation shared memory systems	The fundamental premise behind the DASH project is that it is feasible to build large-scale shared-memory multiprocessors with hardware cache coherence. While paper studies and software simulators are useful for understanding many high-level design trade-offs, prototypes are essential to ensure that no critical details are overlooked. A prototype provides convincing evidence of the feasibility of the design, allows one to accurately estimate both the hardware and the complexity cost of various features, and provides a platform for studying real workloads. A 48processor prototype of the DASH multiprocessor is now operational. In this paper, we first examine the hardware overhead of directory-based cache coherence in the prototype. The data show that the overhead is only about 10-15%, which appears to be a small cost for the ease of programming offered by coherent caches and the potential for higher performance. We then discuss the performance of the system and show the speedups obtained by a variety of parallel applications running on the prototype. Using a sophisticated hardware performance monitor, we also characterize the effectiveness of coherent caches and the relationship between an application’s reference behavior and its speedups. Finally, we present an evaluation of the optimizations incorporated in the DASH protocol in terms of their effectiveness on parallel applications and on atomic tests that stress the memory system. 1	address space;barrier (computer science);cpu cache;cache (computing);cache coherence;central processing unit;clock rate;coherence (physics);complexity;computation;dash (cryptocurrency);data structure;directory (computing);distributed shared memory;dynamic random-access memory;electrical connection;experimental system;flops;high- and low-level;interconnection;level design;locality of reference;multiprocessing;octal;overhead (computing);parallel computing;performance;performance engineering;prototype;scalability;serialization;shared memory;simulation;speedup;static random-access memory;uniprocessor system	Daniel Lenoski;James Laudon;Truman Joe;David Nakahira;Luis Stevens;Anoop Gupta;John L. Hennessy	1993	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.205652	shared memory;cache coherence;computer architecture;parallel computing;real-time computing;index term;computer science;operating system;database;distributed computing	Arch	-10.951902558759407	49.110999508923534	52820
b1776762a9c281a3be41e6e770d0e2f723e41218	a threshold scheduling strategy for sisal on distributed memory machines	parallelisme;distributed system;systeme reparti;general and miscellaneous mathematics computing and information science;dependence analysis;memory management;seuil;probleme np complet;performance;distributed memory machine;data processing;threshold;longest path;estrategia;mathematical logic;parallel computation;algorithme;strategy;algorithm;parallelism;calculo paralelo;sistema repartido;paralelismo;scheduling;polynomial algorithm;array processors;processing 990200 mathematics computers;benchmarks;memoire reparti;algorithms;ordonamiento;problema np completo;umbral;rendimiento;task scheduling;strategie;calcul parallele;logical process;ordonnancement;heuristic algorithm;np complete problem;algoritmo	The problem of scheduling tasks on distributed memory machines is known to be NP-complete in the strong sense, ruling out the possibility of a pseudo-polynomial algorithm. This paper introduces a new heuristic algorithm for scheduling Sisal (Streams and Iterations In a Single Assignment Language) programs on a distributed memory machine, Intel Touchstone i860. Our compile time scheduling method works on IF-2, an intermediate form based on the dataflow parallelism in the program. We initially carry out a dependence analysis, to bind the implicit dependencies across IF-2 graph boundaries, followed by a cost assignment based on Intel Touchstone i860 timings. The scheduler works in two phases. The first phase of the scheduler finds the earliest and latest completion times of each task given by the shortest and longest paths from root task to the given task respectively. A threshold defined as the difference between the latest and the earliest start times of the task, is found. The scheduler varies the value of the allowable threshold, and determines the best value for minimal schedule length. In the second phase of the scheduler, we merge the processors to generate schedule to match the available number of processors. Schedule results for several benchmark programs have been included to demonstrate the effectiveness of our approach.	distributed memory	Santosh Pande;Dharma P. Agrawal;Jon Mauney	1994	J. Parallel Distrib. Comput.	10.1006/jpdc.1994.1054	heuristic;fixed-priority pre-emptive scheduling;mathematical optimization;combinatorics;mathematical logic;parallel computing;np-complete;data processing;longest path problem;performance;strategy;computer science;operating system;mathematics;distributed computing;programming language;scheduling;algorithm;dependence analysis;memory management	HPC	-11.227425055978436	59.07927024267119	52846
a1a2635d6c8995ac2fd8e37d4b36b98c404d50a8	enabling massive multi-threading with fast hashing		The next generation of high-performance computers is expected to execute threads in orders of magnitude higher than today’s systems. Improper management of such huge amount of threads can create resource contention, leading to overall degraded system performance. By leveraging more practical approaches to distribute threads on the available resources, execution models and manycore chips are expected to overcome limitations of current systems. Here, we present DELTA—a Data-Enabled muLti-Threaded Architecture, where a producer-consumer scheme is used to execute threads via complete distributed thread management mechanism. We consider a manycore tiled-chip architecture where Network-on-Chip (NoC) routers are extended to support our execution model. The proposed extension is analysed, while simulation results confirm that DELTA can manage a large number of simultaneous threads, relying on a simple hardware structure.	computer;hash function;manycore processor;multi-core processor;network on a chip;parallel computing;pixelmator;producer–consumer problem;resource contention;scheduling (computing);simulation;supercomputer;thread (computing)	Alberto Scionti;Somnath Mazumdar;Stephanie Zuckerman	2018	IEEE Computer Architecture Letters	10.1109/LCA.2017.2697863	parallel computing;scheduling (computing);real-time computing;instruction set;execution model;computer science;green threads;multithreading;architecture;thread (computing);network on a chip;distributed computing	Arch	-9.743660866943017	50.7891851173265	52924
8c770254292bc3e855c1b08577e9f637f14b7dd5	a partition scheduler model for dynamic dataflow programs		Abstract   The definition of an efficient scheduling policy is an important, difficult and open design problem for the implementation of applications based on dynamic dataflow programs for which optimal closed-form solutions do not exist. This paper describes an approach based on the study of the execution of a dynamic dataflow program on a target architecture with different scheduling policies. The method is based on a representation of the execution of a dataflow program with the associated dependencies, and on the cost of using scheduling policy, expressed as a number of conditions that need to be verified to have a successful execution within each partition. The relation between the potential gain of the overall execution satisfying intrinsic data dependencies and the runtime cost of finding an admissible schedule is a key issue to find close-to-optimal solutions for the scheduling problem of dynamic dataflow applications.	dataflow programming;scheduling (computing)	Malgorzata Michalska;Endri Bezati;Simone Casale Brunet;Marco Mattavelli	2016		10.1016/j.procs.2016.05.415	dataflow architecture;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;signal programming;distributed computing;programming language	Arch	-8.211368089662157	60.124986283196606	52968
c80a3c2faf8465c00358fabdfb68b36680c47e8e	low-power scheduling algorithms for sporadic task with shared resources in hard real-time systems		Dynamic voltage scaling (DVS), which adjusts the clock frequency and supply voltage based on the workload of systems, has been adopted in many computing systems to reduce the energy consumption of the processor. In this paper, we address the problem of minimizing overall energy consumption of a sporadic task in real-time system with a shared set of serially reusable, single unit resources, considering a generalized power model. We first present a static scheduling algorithm, called SSTSASR, which minimizes the system energy consumption of a given set of sporadic tasks and takes the off-chip workload into consideration, assuming that each task releases with the worstcase workload. In addition, we present dynamic slack reclamation techniques, called DSTSASR, which reclaims the slack time from the early completion task to reduce the energy consumption and takes the speed transition overhead into consideration. The simulation results show that the proposed DSTSASR algorithm could achieve a significant amount of energy savings compared with other DVS algorithms. It can reduce the energy consumption by 7.37–31.99% over the DVSSR and by 0.1–5.90% over the SSTSASR.	algorithm;best, worst and average case;clock rate;dynamic voltage scaling;early completion;image scaling;overhead (computing);randomness;real-time clock;real-time computing;real-time operating system;requirement;slub (software);sst (menter’s shear stress transport);scheduling (computing);simulation;slack variable;the computer journal	Yiwen Zhang;Rui-feng Guo	2015	Comput. J.	10.1093/comjnl/bxu103	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;dynamic priority scheduling;rate-monotonic scheduling;distributed computing	Embedded	-5.813944766939093	58.539547824177994	52985
d0681661813662bfa8b9f17f7661c668174e2360	evaluation of high-frequency financial transaction processing in distributed memory systems	ethernet;distributed memory system;infiniband;remote direct memory access;low latency;high performance interconnection technology;financial transaction processing;high frequency;throughput	A financial system requires processing a large-scale data stream. These data are considerably large for realtime processing. Using a distributed memory system is the most efficient means to process and store this data. In addition, using a high-performance interconnection technology such as InfiniBand is necessary because existing Ethernet technology that uses low bandwidth and possesses high latency can have detrimental effects on the financial system. In particular, in a system that processes large-scale financial data, high latency can cause financial loss. Therefore, high-performance interconnection technology is critical for processing large-scale financial data. We propose a scheme that uses high-performance interconnection technology in a distributed memory system for processing a large-scale data stream. In experiments that test the performance of our system, we use Redis, which is an open source data structure server, and messages of the size of the actual financial data in existence. Experimental results show that our distributed memory system using InfiniBand can process financial transactions with high throughput and low latency.	data structure;distributed memory;experiment;infiniband;interconnection;open-source software;redis;server (computing);source data;throughput;transaction processing	Woochur Kim;Jungmee Yun;Hyedong Jung	2014		10.1145/2663761.2664234	embedded system;parallel computing;real-time computing;computer science	HPC	-13.81779651724013	51.92159805127685	53009
80344d934475e062b20db0c8cfb773174d983534	brief announcement: on scheduling best-effort htm transactions	hardware transactional memory;synchronization;scheduling	This paper shows the issues to face while designing contention management policies that involve best-effort hardware transactions. Also, in this paper we present Octonauts, a solution for scheduling HTM transactions without relying on on-the-fly information. Octonauts learns the objects accessed by a hardware transaction while running and it uses them in case of conflict. It also proposes an innovative scheme for optimizing the communication between transactions running in hardware and software.	best-effort delivery;computer hardware;html;schedule (computer science);scheduling (computing)	Mohamed Mohamedin;Roberto Palmieri;Binoy Ravindran	2015		10.1145/2755573.2755612	synchronization;transactional memory;parallel computing;real-time computing;computer science;operating system;distributed computing;scheduling	OS	-13.256427714239555	50.37337935412734	53187
4b4088635fe9e0d8621e22bbf1f65fe165bcbccf	power-aware scheduling and dynamic voltage setting for tasks running on a hard real-time system	processor scheduling real time systems integer programming;multiprocessors soc architecture;video encoder;architecture exploration;processor scheduling;customization;dynamic scheduling real time systems voltage control energy consumption processor scheduling scheduling algorithm dynamic voltage scaling energy management power system management power system reliability;system performance;optimization problem;hard real time system;integer programming;energy consumption;power management;precedence constraint;system wide energy saving power aware scheduling dynamic voltage setting hard real time system precedence constraint dynamic power management voltage scaling technique energy consumption integer programming three phase solution framework power management scheduling task voltage assignment;integer program;dynamic power management;voltage scaling;mpeg4;energy saving;hard real time;real time systems	This paper addresses the problem of minimizing energy consumption of a computer system performing periodic hard real-time tasks with precedence constraints. In the proposed approach, dynamic power management and voltage scaling techniques are combined to reduce the energy consumption of the CPU and devices. The optimization problem is first formulated as an integer programming problem. Next, a three-phase solution framework, which integrates power management scheduling and task voltage assignment, is proposed. Experimental results show that the proposed approach outperforms existing methods by an average of 18% in terms of the systemwide energy savings.	central processing unit;computer;dynamic voltage scaling;image scaling;integer programming;mathematical optimization;optimization problem;power management;real-time clock;real-time computing;scheduling (computing)	Peng Rong;Massoud Pedram	2006	Asia and South Pacific Conference on Design Automation, 2006.	10.1145/1118299.1118416	optimization problem;embedded system;mathematical optimization;encoder;parallel computing;real-time computing;integer programming;computer science;mpeg-4	EDA	-5.39520565643502	59.00927078893057	53190
e3be21815fbe90a06cdb32a1a9b273a3c8b956f2	applying distributed shared memory techniques for implementing distributed objects	object oriented language;distributed objects;distributed shared memory;object model;memory model	In this paper we study how the potential advantages of distributed shared memory (DSM) techniques can be applied to concurrent object-oriented languages. We assume a DSM scheme based on an entry consistency memory model and propose an object model that can incorporate that DSM scheme. The object model is characterized by the requirement of explicitly enclosing object invocations between acquire and release operations, and the distinction between command and query operations. Details of a thread-based implementation are discussed, and results show that significant speed-ups can be obtained. We also conclude that using kernel-level threads can lead to better performance, and the overhead versus userlevel threads is negligible.	computer performance;distributed computing;distributed object;distributed shared memory;exclusive or;fits;heterogeneous computing;high- and low-level;kernel (operating system);matrix multiplication;message passing;overhead (computing);posix threads;parallel virtual machine;programming model;requirement;software portability;user space	Antonio J. Nebro;Ernesto Pimentel;José M. Troya	1997		10.1007/3-540-69687-3_90	distributed shared memory;shared memory;memory model;parallel computing;real-time computing;distributed memory;object model;computer science;distributed computing;distributed object;programming language;object-oriented programming;data diffusion machine	DB	-14.495197989663064	47.081843447932975	53281
5843e36547498373da1f64039d7145ac6d4853ee	modeling the implications of dram failures and protection techniques on datacenter tco	datacenters;random access memory;reliability;error correction codes;measurement;online and offline services;total cost of ownership;servers;dram;co running services;benchmark testing;hardware	Total Cost of Ownership (TCO) is a key optimization metric for the design of a datacenter. This paper proposes, for the first time, a framework for modeling the implications of DRAM failures and DRAM error protection techniques on the TCO of a datacenter. The framework captures the Effects and interactions of several key parameters including: the choice of DRAM protection technique (e.g. single vs dual channel Chipkill), device width (x4 or x8), memory size, power, FITs for various failure modes, the performance, power and temperature overheads of a protection technique for a given service and mixes of collocated services. The usefulness of the proposed framework is demonstrated through several case studies that identify the best DRAM protection technique in each case, in terms of TCO. Interestingly, our analysis reveals that among the three DRAM protection techniques considered, there is no one that is always superior to all the others. Moreover, each technique is better than the others for some cases. This underlines the importance and the need of the proposed framework for making optimal memory protection datacenter design decisions. As part of this work, we analyze and report the performance and power with single channel and dual channel Chipkill on real hardware when running a web search benchmark alone and collocated with benchmarks of varying memory intensity. This analysis reveals that the choice of memory protection can have serious performance and TCO ramifications depending on the memory characteristics of collocated services. Other analysis reveals that, for the datacenter and services assumed in this study, when using Chipkill protection it can be beneficial for TCO to use DRAM with 100x the failure rate of a baseline DRAM as long as the cost per DIMM is at least a dollar less compared to the baseline.	baseline (configuration management);benchmark (computing);chipkill;dimm;data center;dynamic random-access memory;failure rate;interaction;mathematical optimization;memory protection;multi-channel memory architecture;total cost of ownership;web search engine	Panagiota Nikolaou;Yiannakis Sazeides;Lorena Ndreu;Marios Kleanthous	2015	2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)	10.1145/2830772.2830804	embedded system;benchmark;data center;parallel computing;real-time computing;computer science;operating system;reliability;dram;server;measurement	Arch	-8.466242577626794	55.96493197244418	53283
ca62867bdaf6fa096f600b2dd03dc55272caa4e3	mixed criticality systems with varying context switch costs		In mixed criticality systems, it is vital to ensure that there is sufficient separation between tasks of LO- and HI-criticality applications, so that the behavior or mis-behavior of the former cannot affect the functional or timing correctness of the latter. To ensure appropriate spatial isolation, the memory address spaces and cache use by LO- and HI-criticality tasks must be distinct. A consequence of this separation is that the cost of switching between tasks of the same criticality can be small, whereas the cost of context switching between tasks of different criticality levels can be much larger. In this paper, we focus on integrating the differing context switch costs into fixed priority preemptive scheduling, and the two mixed criticality scheduling schemes based on it: SMC and AMC. We derive simple, refined, and multi-set analyses for each scheme. Further, we show that the refined and multi-set analyses are not compatible with Audsley's Optimal Priority Assignment algorithm, we therefore propose a heuristic priority assignment policy aimed at reducing the number of high cost context switches. Our evaluation is grounded in measurements of context switch times (save and restore costs) from a prototype implementation of an explicitly managed cache on an FPGA. The evaluation shows the effectiveness of the derived analyses and the proposed priority assignment policy.	address space;advanced mezzanine card;algorithm;cpu cache;cache (computing);complexity;context switch;correctness (computer science);criticality matrix;deferred procedure call;expectation propagation;field-programmable gate array;headroom (audio signal processing);heuristic;kilobyte;mathematical optimization;memory address;mixed criticality;network switch;opa;preemption (computing);program optimization;prototype;real-time clock;real-time computing;real-time operating system;run time (program lifecycle phase);scheduling (computing);scheduling analysis real-time systems;self-organized criticality;static timing analysis	Robert I. Davis;Sebastian Altmeyer;Alan Burns	2018	2018 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)	10.1109/RTAS.2018.00024	real-time computing;context switch;memory address;mixed criticality;scheduling (computing);correctness;cache;criticality;distributed computing;preemption;computer science	Embedded	-8.467517494420887	58.597524009015956	53326
580835ee30534043f53b1cb03fe1f27ce85bcdcf	gputerasort: high performance graphics co-processor sorting for large database management	data parallel;nvidia geforce 7800 gt;paper;continuous queries;sorting;spatial join;resource manager;duality;database management;data communication;data streams;nvidia geforce 6800;performance improvement;graphics processors;nvidia;graphic processing unit;nvidia geforce 6800 ultra;algorithms;computer science;batch processing;sorting algorithm;high performance;memory bandwidth;large data	We present a novel external sorting algorithm using graphics processors (GPUs) on large databases composed of billions of records and wide keys. Our algorithm uses the data parallelism within a GPU along with task parallelism by scheduling some of the memory-intensive and compute-intensive threads on the GPU. Our new sorting architecture provides multiple memory interfaces on the same PC -- a fast and dedicated memory interface on the GPU along with the main memory interface for CPU computations. As a result, we achieve higher memory bandwidth as compared to CPU-based algorithms running on commodity PCs. Our approach takes into account the limited communication bandwidth between the CPU and the GPU, and reduces the data communication between the two processors. Our algorithm also improves the performance of disk transfers and achieves close to peak I/O performance. We have tested the performance of our algorithm on the SortBenchmark and applied it to large databases composed of a few hundred Gigabytes of data. Our results on a 3 GHz Pentium IV PC with $300 NVIDIA 7800 GT GPU indicate a significant performance improvement over optimized CPU-based algorithms on high-end PCs with 3.6 GHz Dual Xeon processors. Our implementation is able to outperform the current high-end PennySort benchmark and results in a higher performance to price ratio. Overall, our results indicate that using a GPU as a co-processor can significantly improve the performance of sorting algorithms on large databases.	benchmark (computing);central processing unit;computation;computer data storage;coprocessor;data parallelism;database;external sorting;geforce 7 series;gigabyte;graphics processing unit;input/output;memory bandwidth;p5 (microarchitecture);parallel computing;scheduling (computing);sorting algorithm;task parallelism	Naga K. Govindaraju;Jim Gray;Ritesh Kumar;Dinesh Manocha	2006		10.1145/1142473.1142511	parallel computing;duality;computer hardware;computer science;sorting;resource management;operating system;sorting algorithm;database;data stream mining;memory bandwidth;batch processing	DB	-5.819790736554718	48.17712778989997	53387
3c16161113391e09aeb16cfa192bcb44de8fd8a0	hybrid mpi+openmp reactive work stealing in distributed memory in the pde framework sam(oa)^2		"""""""Equal work results in equal execution time"""" is an assumption that has fundamentally driven design and implementation of parallel applications for decades. However, increasing hardware variability on current architectures (e.g., through Turbo Boost, dynamic voltage and frequency scaling or thermal effects) necessitate a revision of this assumption. Expecting an increase of these effects on future (exascale-)systems, in this paper, we present reactive work stealing across nodes on distributed memory machines using only MPI and OpenMP. We develop a novel distributed work stealing concept that – based on on-line performance monitoring – selectively steals and remotely executes tasks across MPI boundaries. This concept has been implemented in the parallel adaptive mesh refinement (AMR) framework sam(oa)^2 for OpenMP tasks of traversing a grid section. Corresponding performance measurements in the presence of enforced CPU clock frequency imbalances demonstrate that a state-of-the-art cost-based (chains-on-chains partitioning) load balancing mechanism is insufficient and can even degrade performance while distributed work stealing successfully mitigates the frequency-induced imbalances. Furthermore, our results indicate that our approach is also suitable for load balancing work-induced imbalances in a realistic AMR test case."""	adaptive multi-rate audio codec;adaptive mesh refinement;central processing unit;clock rate;clock signal;distributed memory;dynamic frequency scaling;dynamic voltage scaling;image scaling;intel turbo boost;load balancing (computing);message passing interface;online and offline;openmp;refinement (computing);run time (program lifecycle phase);spatial variability;test case;work stealing	Philipp Samfass;Jannis Klinkenberg;Michael Bader	2018	2018 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2018.00051	real-time computing;parallel computing;computer science;adaptive mesh refinement;distributed memory;work stealing;frequency scaling;instruction set;clock rate;load balancing (computing);load management	HPC	-7.763115640734101	49.328138590100295	53404
e526e335acba67ba22a121e096bd382c09280ea9	adaptive cache coherence mechanisms with producer–consumer sharing optimization for chip multiprocessors	producer consumer;chip multiprocessors cmps;protocols;radiation detectors;multi core single chip multiprocessors;system architectures;adaptable architectures cache coherence producer consumer chip multiprocessors cmps;performance evaluation cache storage multiprocessing systems parallel architectures;multicore processing;protocols coherence bandwidth optimization radiation detectors multicore processing;cache coherence;bandwidth;coherence;optimization;memory hierarchy;integration and modeling;adaptable architectures;processor architectures;64 core tiled cmp adaptive cache coherence mechanisms producer consumer sharing optimization chip multiprocessors cache to cache misses write invalidate based protocols dynamic write update mechanism producer consumer sharing pattern bandwidth adaptive mechanism performance degradation base adaptive protocol latency based optimizations splash 2 suites nas parallel benchmark suites proximity aware extension;cache memories	In chip multiprocessors (CMPs), maintaining cache coherence can account for a major performance overhead. Write-invalidate protocols adapted by most CMPs generate high cache-to-cache misses under producer-consumer sharing patterns. Accordingly, this paper presents three cache coherence mechanisms optimized for CMPs. First, to reduce coherence misses observed in write-invalidate-based protocols, we propose a dynamic write-update mechanism augmented on top of a write-invalidate protocol. This mechanism is specifically triggered at the detection of a producer-consumer sharing pattern. Second, we extend this adaptive protocol with a bandwidth-adaptive mechanism to eliminate performance degradation from write-updates under limited bandwidth. Finally, proximity-aware mechanism is proposed to extend the base adaptive protocol with latency-based optimizations. Experimental analysis is conducted on a set of scientific applications from the SPLASH-2 and NAS parallel benchmark suites. The proposed mechanisms were shown to reduce coherence misses by up to 48% and in return speed up application performance up to 30%. Bandwidth-adaptive mechanism is proven to perform well under varying levels of available bandwidth. Results from our proposed proximity-aware extension demonstrated up to 6% performance gain over the base adaptive protocol for 64-core tiled CMP runs. In addition, the analytical model provided good estimates for performance gains from our adaptive protocols.	bandwidth (signal processing);benchmark (computing);cpu cache;cache coherence;computer architecture simulator;elegant degradation;interconnection;manycore processor;network-attached storage;overhead (computing);producer–consumer problem;simulation	Abdullah Kayi;Olivier Serres;Tarek A. El-Ghazawi	2015	IEEE Transactions on Computers	10.1109/TC.2013.217	multi-core processor;embedded system;communications protocol;cache coherence;computer architecture;parallel computing;real-time computing;coherence;computer science;operating system;producer–consumer problem;particle detector;mesi protocol;bandwidth	HPC	-9.220794390251404	51.202729529705685	53420
29ce59fe27f9421de7cfb386fcf98ca598016cac	characterizing scalar opportunities in gpgpu applications	parallel architectures general purpose computers graphics processing units;niobium;computer architecture;computational modeling;parallel architectures;vectors;general purpose computers;registers;graphics processing units;heterogeneous scalar vector gpu architecture characterizing scalar opportunity gpgpu applications general purpose computing graphics processing units general purpose community gpu computation single instruction multiple data model simd model gpu execution simd computation redundant computations multiple outputs scalar units input data operands data parallel simd units non scalar operations cuda programs nvidia sdk scalar execution static simd instructions compiler dynamic occurences dynamic instruction execution;graphics processing units computer architecture instruction sets vectors registers computational modeling niobium;instruction sets	General Purpose computing with Graphics Processing Units (GPGPU) has gained widespread adoption in both the high performance and general purpose communities. In most GPU computation, execution exploits a Single Instruction Multiple Data (SIMD) model. However, GPU execution typically pays little attention to whether the data operated upon by the SIMD units is the same or different. When SIMD computation operates on multiple copies of the same data, redundant computations are generated. It provides an opportunity to improve efficiency by just broadcasting the results of a single computation to multiple outputs. To better serve those operations, modern GPUs are armed with scalar units. Then SIMD instructions that are operating on the same input data operands will be directed to execute upon scalar units, requiring only a single copy of the data, and leaving the data-parallel SIMD units available to execute non-scalar operations. In this paper, we first characterize a number of CUDA programs taken from the NVIDIA SDK to quantify the potential for scalar execution. We observe that 38% of static SIMD instructions are recognized to operate on the same data by the compiler, and their dynamic occurences account for 34% of the total dynamic instruction execution. We then evaluate the impact of scalar units on a heterogeneous scalar-vector GPU architecture. Our results show that scalar units are utilized 51% of the time during execution, though their use places additional pressure on the interconnect and memory, as shown in the results of our study.	benchmark (computing);cuda;compiler;computation;general-purpose computing on graphics processing units;graphics processing unit;interconnection;multiprocessing;operand;pipeline (computing);simd;software development kit;static program analysis	Zhongliang Chen;David R. Kaeli;Norman Rubin	2013	2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)	10.1109/ISPASS.2013.6557173	niobium;computer architecture;parallel computing;computer science;theoretical computer science;operating system;instruction set;processor register;computational model	Arch	-5.420001791669592	46.75176721130827	53465
a406c6b255c0c61291b8ffd2f8d1f2104f5ab0b2	cassim: a top-level-simulator for grid scheduling and applications	grid scheduling;large applications;software testing;code coverage;experimental validation	Effective and efficient use of grid resources is achieved by using scheduling heuristics. Many such heuristics have been developed over the last decade. However, as none of the heuristics performs well in all conditions, how can we dynamically determine the best one to choose? In this paper, we describe CasSim (Centre for Advanced Studies Simulator), and use it to dynamically select the best heuristic. In contrast to other grid simulators, CasSim has a simple abstract model, and can easily and quickly compare different scheduling heuristics. Experimental validation of CasSim shows that CasSim selects heuristics accurately and it can be used in the real system. In addition, we extend CasSim to support a wider range of applications by enabling users to define costs.		Edward Xia;Igor Jurisica;Julie Waterhouse	2006		10.1145/1188966.1189008	parallel computing;real-time computing;computer science;software engineering;distributed computing;software testing;code coverage	HPC	-17.547261547031287	60.31388941651873	53474
2b1415ed444d67e78439e63baa73c05d7246f8f2	denovo: rethinking the memory hierarchy for disciplined parallelism	software;cache storage;protocols;shared memory;protocols hardware coherence software transient analysis arrays programming;design complexity memory hierarchy disciplined parallelism mass programmer shared memory language arbitrary data race software evolution hardware designer many core scalability software oblivious hardware evolution hardware architecture parallel programming model cache architecture denovo coherence protocol mesi protocol flexible communication granularity direct cache to cache transfer cache hit rate network traffic shared memory programming model message passing;parallel programming;disciplined parallelism;computer science denovo rethinking the memory hierarchy for disciplined parallelism university of illinois at urbana champaign marc snir sung;memory consistency;transient analysis;hardware architecture;programming model;arrays;shared memory systems;thesis;software evolution;model checking;global address space;network traffic;data races;cache coherence;message passing;hyojin;coherence;hardware design;ubiquitous computing;memory hierarchy;parallel programming model;programming;parallel memories;ubiquitous computing cache storage message passing parallel memories parallel programming protocols shared memory systems;hardware	"""For parallelism to become tractable for mass programmers, shared-memory languages and environments must evolve to enforce disciplined practices that ban """"wild shared-memory behaviors;'' e.g., unstructured parallelism, arbitrary data races, and ubiquitous non-determinism. This software evolution is a rare opportunity for hardware designers to rethink hardware from the ground up to exploit opportunities exposed by such disciplined software models. Such a co-designed effort is more likely to achieve many-core scalability than a software-oblivious hardware evolution. This paper presents DeNovo, a hardware architecture motivated by these observations. We show how a disciplined parallel programming model greatly simplifies cache coherence and consistency, while enabling a more efficient communication and cache architecture. The DeNovo coherence protocol is simple because it eliminates transient states -- verification using model checking shows 15X fewer reachable states than a state-of-the-art implementation of the conventional MESI protocol. The DeNovo protocol is also more extensible. Adding two sophisticated optimizations, flexible communication granularity and direct cache-to-cache transfers, did not introduce additional protocol states (unlike MESI). Finally, DeNovo shows better cache hit rates and network traffic, translating to better performance and energy. Overall, a disciplined shared-memory programming model allows DeNovo to seamlessly integrate message passing-like interactions within a global address space for improved design complexity, performance, and efficiency."""	cache (computing);cache coherence;cobham's thesis;code;compiler;computer data storage;interaction;mesi protocol;manycore processor;memory hierarchy;message passing;model checking;multi-core processor;network packet;nondeterministic algorithm;parallel computing;parallel programming model;partitioned global address space;programmer;scalability;shared memory;software evolution;synchronization (computer science)	Byn Choi;Rakesh Komuravelli;Hyojin Sung;Robert Smolinski;Nima Honarmand;Sarita V. Adve;Vikram S. Adve;Nicholas P. Carter;Ching-Tsun Chou	2011	2011 International Conference on Parallel Architectures and Compilation Techniques	10.1109/PACT.2011.21	model checking;shared memory;communications protocol;programming;cache coherence;computer architecture;parallel computing;message passing;coherence;computer science;software evolution;operating system;hardware architecture;distributed computing;programming paradigm;programming language;ubiquitous computing;parallel programming model	Arch	-10.641145562046393	47.279781328690575	53611
aa6e990396c28adf691818af7d34578e129776b7	design and modeling of a non-blocking checkpointing system	software fault tolerance;markov model;fault tolerance	As the capability and component count of systems increase, the MTBF decreases. Typically, applications tolerate failures with checkpoint/restart to a parallel file system (PFS). While simple, this approach can suffer from contention for PFS resources. Multi-level checkpointing is a promising solution. However, while multi-level checkpointing is successful on today's machines, it is not expected to be sufficient for exascale class machines, which are predicted to have orders of magnitude larger memory sizes and failure rates. Our solution combines the benefits of non-blocking and multi-level checkpointing. In this paper, we present the design of our system and model its performance. Our experiments show that our system can improve efficiency by 1.1 to 2.0x on future machines. Additionally, applications using our checkpointing system can achieve high efficiency even when using a PFS with lower bandwidth.	application checkpointing;blocking (computing);clustered file system;experiment;fault tolerance;forward secrecy;mean time between failures;non-blocking algorithm	Kento Sato;Naoya Maruyama;Kathryn Mohror;Adam Moody;Todd Gamblin;Bronis R. de Supinski;Satoshi Matsuoka	2012	2012 International Conference for High Performance Computing, Networking, Storage and Analysis		fault tolerance;computing;parallel computing;real-time computing;computer science;operating system;distributed computing;markov model;software fault tolerance	HPC	-18.284539682796083	49.81870919686406	53628
eb7350ba85975a0932f13bb34478e6adc2673a8e	effects of buffered memory requests in multiprocessor systems	simulation model;input output;cpu;contention	A simulation model is developed and used to study the effect of buffering of memory requests on the performance of multiprocessor systems. A multiprocessor system is generalized as a parallel-pipelined processor of order (s,p), which consists of p parallel processors each of which is a pipelined processor with s degrees of multiprogramming, there can be up to s*p memory requests in each instruction cycle. The memory, which consists of N(&equil;2n) identical memory modules, is organized such that there are l(&equil;2i) lines and m(&equil;2n−i) identical memory modules, where each module is characterized by the address cycle (address hold time) and memory cycle of a and c time units respectively. Too large an l is undesirable in a multiprocessor system because of the cost of the processor-memory interconnection network. Hence, we will show how effective buffering can be used to reduce the system cost while effectively maintaining a high level of performance.	multiprocessing;registered memory	Faye A. Briggs	1979	SIGMETRICS Performance Evaluation Review	10.1145/1009373.805450	uniform memory access;distributed shared memory;shared memory;input/output;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;memory refresh;computer science;physical address;operating system;central processing unit;simulation modeling;overlay;conventional memory;memory coherence;extended memory;flat memory model;contention ratio;registered memory;symmetric multiprocessor system;computing with memory;memory map;memory management	HPC	-11.850849996595086	48.09075471295831	53656
50624eceb4f17c6ce35ffbdc7c2fc495c2dfbd31	energy-efficient deadline scheduling for heterogeneous systems	energy efficiency;high performance computing;pricing scheme;deadline scheduling	Energy efficiency is a major concern in modern high performance computing (HPC) systems and power-aware scheduling approach is a promising way to achieve that. While there are a number of studies in power-aware scheduling by means of dynamic power management (DPM) and/or dynamic voltage and frequency scaling (DVFS) techniques, most of them only consider scheduling at a steady state. However, HPC applications like scientific visualization often need deadline constraints to guarantee timely completions. In this paper we present power-aware scheduling algorithms with deadline constraints for heterogeneous systems. We formulate the problem by extending the traditional multiprocessor scheduling and design approximation algorithms with analysis on the worst-case performance. We also present a pricing scheme for tasks in the way that the price of a task varies as its energy usage as well as largely depends on the tightness of its deadline. Last we extend the proposed algorithm to control dependence graph and the online case which is more realistic. Through the extensive experiments, we demonstrate that the proposed algorithm achieves near-optimal energy efficiency, on average 16.4% better for synthetic workload and 12.9% better for realistic workload than EDD (Earliest Due Date)-based algorithm; The extended online algorithm also outperforms EDF (Earliest Deadline First)-based algorithm on average up to 26% of energy saving and 22% of deadline satisfaction. It is as well experimentally shown that the pricing scheme provides a flexible trade-off between deadline tightness and price.	approximation algorithm;best, worst and average case;computation;dhrystone;dynamic frequency scaling;dynamic voltage scaling;earliest deadline first scheduling;embedded entertainment system;energy minimization;experiment;image scaling;multiprocessing;multiprocessor scheduling;np-hardness;online algorithm;power management;scheduling (computing);scientific visualization;steady state;supercomputer	Yan Ma;Bin Gong;Ryo Sugihara;Rajesh Gupta	2012	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2012.07.006	priority inversion;fair-share scheduling;supercomputer;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;distributed computing;efficient energy use;least slack time scheduling	HPC	-5.7963520718412695	59.21168129407148	53714
abb3ae0b13f496ff4ea27d1fff62ad23f6ab9576	time synchronization on sp1 and sp2 parallel systems	performance measure;multiprocessor interconnection networks;parallel program tracing;parallel processes;clocks;parallel program tuning;parallel processes sp2 parallel system sp1 parallel system experimental time utility time synchronization operating system clocks node clocks interconnection network synchronous feature parallel program performance measurement parallel program tuning parallel program tracing parallel program debugging gang scheduling;processor scheduling;sp2 parallel system;gang scheduling;reduced instruction set computing;node clocks;synchronous feature;software performance evaluation;reduced instruction set computing synchronisation synchronisation clocks clocks operating systems computers operating systems computers program debugging program debugging processor scheduling processor scheduling multiprocessor interconnection networks multiprocessor interconnection networks software performance evaluation software performance evaluation parallel machines parallel machines reduced instruction set computing;time synchronization;interconnection network;synchronisation;parallel program debugging;operating system;sp1 parallel system;parallel systems;clocks frequency synchronization switches workstations counting circuits protocols local area networks network servers delay measurement;parallel machines;operating system clocks;parallel program performance measurement;program debugging;parallel programs;operating systems computers;parallel processing;experimental time utility	We descnbe an ezpertmental time uttlrty for synchronizrng the operatrng system clocks on the SP1 and SP2 parallel system nodes. It synchronzzes the node clocks typicully wrthrn 5 mrcroseconds of each other utilazang the synchronous feature of the SP1 and SP2 mterconnectton netwc4rk. Thrs rs 2 to 3 orders of magnttvde better than what can be achaeved by previous methods Synchronized clocks arc useful for parallel program performance measurement and tunmg, parallel proyram tracang and debuggang, and gung scheduling of parallel processes, to name a few We also measure the performance of a wrdely u ~ e d time synchronrzataon utrlaty ustng the SPl and SP2 interconnectron	scheduling (computing);system time	Bülent Abali;Craig B. Stunkel	1995		10.1109/IPPS.1995.395877	parallel computing;real-time computing;computer science;distributed computing	Metrics	-16.335700383516585	48.39929697709929	53729
e0c3c211ab84807943fe539937983d3d37f3f6ac	a simple latency tolerant processor	cache storage;multi core processor;scarce pin bandwidth;logic design;out of order superscalar architecture execution memory latency tolerant multicore processor design parallel application die size chip cache miss latency scarce pin bandwidth bus bandwidth memory wall problem single thread performance;latency tolerance;toxicology;out of order;chip;computer architecture;delay out of order bandwidth multicore processing process design throughput memory architecture hardware dynamic scheduling buffer storage;out of order execution;single thread performance;registers;memory latency tolerant multicore processor design;pipelines;out of order superscalar architecture execution;multiprocessing systems cache storage logic design microprocessor chips;memory wall problem;chip cache miss latency;bus bandwidth;multiprocessing systems;high throughput;magnetic cores;program processors;parallel applications;die size;parallel application;memory latency;dynamic scheduling;microprocessor chips	The advent of multi-core processors and the emergence of new parallel applications that take advantage of such processors pose difficult challenges to designers. With relatively constant die sizes, limited on chip cache, and scarce pin bandwidth, more cores on chip reduces the amount of available cache and bus bandwidth per core, therefore exacerbating the memory wall problem. How can a designer build a processor that provides a core with good single-thread performance in the presence of long latency cache misses, while enabling as many of these cores to be placed on the same die for high throughput. Conventional latency tolerant architectures that use out-of-order superscalar execution have become too complex and power hungry for the multi-core era. Instead, we present a simple, non-blocking architecture that achieves memory latency tolerance without requiring complex out-of-order execution hardware or large, cycle-critical and power hungry structures, such as dynamic schedulers, fully associative load and store queues, and reorder buffers. The non-blocking property of this architecture provides tolerance to hundreds of cycles of cache miss latency on a simple in-order issue core, thus allowing many more such cores to be integrated on the same die than is possible with conventional out-of-order superscalar architecture.	access time;blocking (computing);cas latency;cpu cache;central processing unit;die (integrated circuit);emergence;interrupt latency;manycore processor;multi-core processor;non-blocking algorithm;out-of-order execution;random-access memory;simulation;superscalar processor;thread (computing);throughput	Satyanarayana Nekkalapu;Haitham Akkary;Komal Jothi;Renjith Retnamma;Xiaoyu Song	2008	2008 IEEE International Conference on Computer Design	10.1109/ICCD.2008.4751889	embedded system;pipeline burst cache;computer architecture;parallel computing;real-time computing;computer science;out-of-order execution;operating system	Arch	-8.169895181490523	51.55332890012307	53785
2e68189f848afd146ad30ca8ee8dadb53953aa00	an efficient parallel join algorithm based on hypercube-partitioning	size ratio;performance evaluation;multiprocessor systems;hypercube multicomputers;conference;storage management;performance comparison;disk i o;relational database;join processing performance;disk input output;analytic cost model;efficient parallel join algorithm;relational database efficient parallel join algorithm hypercube partitioning disk i o cpu costs communication cost join processing performance multiprocessor systems parallel join algorithm disk input output hypercube multicomputers size ratio analytic cost model cube robust join algorithm;distributed databases;communication cost;hypercubes;robustness;costs broadcasting multiprocessing systems central processing unit robustness computer science hypercubes algorithm design and analysis relational databases parallel processing;cpu costs;relational databases;multiprocessing systems;hypercube partitioning;computer science;broadcasting;cube robust join algorithm;storage management distributed databases relational databases hypercube networks database theory performance evaluation;parallel join algorithm;database theory;algorithm design and analysis;parallel processing;cost model;central processing unit;hypercube networks	Many parallel join algorithms have been proposed so far but most of which are developed focused on minimizing the disk I t 0 and CPU costs. The communication cost, however, is also an important factor that can significantly affect the join processing performance in multiprocessor system. In this paper we propose an eficient parallel join algorithm, called CubeRobust, for hypercube multicomputers. The proposed algorithm is developed based on the observation that the size ratio of two relations to be joined is the dominant factor in the communication cost. We develop the analytic cost model for the proposed join algorithm. The performance comparisons show that the Cube-Robust join algorithm works better than others proposed earlier in a wide range of size ratios.	analysis of algorithms;central processing unit;cube;join (sql);multiprocessing;parallel algorithm	Hwan-Ik Choi;Byoung Mo Im;Myoung-Ho Kim;Yoon-Joon Lee	1994		10.1109/PDIS.1994.331733	parallel computing;computer science;theoretical computer science;distributed computing	DB	-11.302543959932922	50.07772553294772	53794
93a8ab87cfb44bbde31c7dd33a4df446cd4e1c1e	scheduling divisible loads on bus networks with arbitrary processor release time and start-up costs: xrmi	system buses processor scheduling recursive estimation;recursive estimation;concurrent computing;task initialization overhead;processor scheduling;efficient algorithm;distributed computing;bus networks;start up costs;system buses;scheduling algorithm;task initialization overhead scheduling divisible loads bus networks arbitrary processor release time start up costs xrmi extended recursive multi round installment load distribution;processor scheduling costs scheduling algorithm distributed computing grid computing algorithm design and analysis clustering algorithms load modeling concurrent computing communication channels;arbitrary processor release time;scheduling divisible loads;clustering algorithms;xrmi;extended recursive multi round installment load distribution;load distribution;communication channels;load modeling;grid computing;algorithm design and analysis	We present a novel algorithm to schedule divisible loads using multi-round installment load distribution. This algorithm considers additional real-world factors, such as task initialization overhead and arbitrary processor release times. We analyze this algorithm's properties and demonstrate its behaviors through multiple illustrative examples. The analysis provides both a deeper understanding of the divisible load scheduling difficulties when considering more real-world factors and allows us to design more efficient algorithms. We show that this algorithm generate better solutions for practical applications.	algorithm;central processing unit;computation;genetic algorithm;job shop scheduling;load balancing (computing);makespan;overhead (computing);parallel computing;schedule (project management);scheduling (computing)	Jie Hu;Raymond Klefstad	2007	2007 IEEE International Performance, Computing, and Communications Conference	10.1109/PCCC.2007.358914	algorithm design;parallel computing;real-time computing;concurrent computing;computer science;weight distribution;distributed computing;cluster analysis;scheduling;grid computing;channel	HPC	-13.679006977930067	60.39584449117633	53854
12722ec1f4aadcd415409952d4a4d82e5e2cbb03	power-aware scheduling of compositional real-time frameworks	power aware scheduling;periodic resource model;compositional and hierarchical real time scheduling;periodic task model	The energy consumption problem has become a great challenge in all computing areas from modern handheld devices to large data centers. Dynamic voltage scaling (DVS) is widely used as mean to reduce the energy consumption of computer systems by lowering whenever possible the voltage and operating frequency of processors. Unfortunately, existing compositional real-time scheduling frameworks have been focusing only on efficient scheduling of tasks inside their components given a resource model, providing no interest on power/energy consumption. In this paper, we define the real-time DVS problem for a compositional scheduling framework. Considering the periodic resource model, we propose optimal static DVS schemes at system, component, and task levels. We also introduce component and task level dynamic DVS schemes that take advantage of runtime unused slack times and resource availability to provide even better energy savings. Finally, we provide power-aware schedulability conditions to guarantee the feasibility of each component under DVS for the Earliest Deadline First and the Rate Monotonic scheduling algorithms. Through simulations, we showed that our schemes can reduce the energy consumption of a component by up to 96%. © 2015 Elsevier Inc. All rights reserved.	algorithm;central processing unit;clock rate;data center;dynamic voltage scaling;earliest deadline first scheduling;image scaling;mobile device;rate-monotonic scheduling;real-time clock;real-time operating system;scheduling (computing);simulation;slack variable	Guy Martin Tchamgoue;Kyong Hoon Kim;Yong-Kee Jun	2015	Journal of Systems and Software	10.1016/j.jss.2014.12.031	embedded system;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system	Embedded	-5.0865309181385125	58.7174826588746	53862
06cb514157c1a97be0a6672a92be0fa4a8ad9602	low overhead task migration mechanism in noc-based mpsoc	hardware message passing software system on chip nickel resource management parallel programming	Task migration, an effective resource management approach, contributes to an increase of on-chip communication overhead. We propose an MMPI-based task migration mechanism to lower task migration overhead in NoC-based MPSoC. This task migration mechanism depends on MPSoC message passing interface (MMPI), which defines a parallel programming pattern that program is dependent of task mapping. In the migration mechanism, task state information is transferred to another PE. The migration overhead is lowered since the task migration is based on MMPI and task code is not transferred. Furthermore, the task migration mechanism does not require checkpoints to detect migration request. Experimental results show that the migration delay decreases around 28% without transferring task code.	mpsoc;message passing interface;network on a chip;overhead (computing);parallel computing;real-time clock;requirement;software design pattern;speculative execution	Fangfa Fu;Liang Wang;Yu Lu;Jinxiang Wang	2013	2013 IEEE 10th International Conference on ASIC	10.1109/ASICON.2013.6811839	parallel computing;real-time computing;computer science;distributed computing	HPC	-15.104868676666214	48.87045105384357	53911
3a27bff3dbfc0c44b165730855ab5a1dffd043f9	thermal management of a many-core processor under fine-grained parallelism	run-time impact;various dtm technique;fine-grained parallelism;1024-core xmt chip;thermal management;many-core processor;high level insight;shared memory many-core system;general global scheme;broader family;dedicated interconnection controller;single task performance	In this paper, we present the work in progress that studies the run-time impact of various DTM techniques on a proposed 1024-core XMT chip. XMT aims to improve single task performance using finegrained parallelism. Via simulations, we show that relative to a general global scheme, speedups of up to 46% with a dedicated interconnection controller and 22% with distributed control of computing clusters are possible. Our findings lead to several high level insights that can impact the design of a broader family of shared memory many-core systems.	algorithm;benchmark (computing);central processing unit;cray xmt;distributed control system;high- and low-level;high-level programming language;intel core (microarchitecture);interconnection;manycore processor;mathematical optimization;multi-core processor;optimization problem;parallel computing;run time (program lifecycle phase);shared memory;simulation;task parallelism;uniprocessor system	Fuat Keceli;Tali Moreshet;Uzi Vishkin	2010		10.1007/978-3-642-29737-3_29	computer architecture;parallel computing;real-time computing;computer science;operating system;data parallelism	HPC	-8.04799183093958	47.74186381017245	54014
b757c80be0cf285cd7fa20392b0efcfd5fdf7c9a	realistic evaluation of interconnection networks using synthetic traffic	libraries;multiprocessor interconnection networks;focusing;performance evaluation;point to point;probability density function;simulation;traffic characterization interconnection networks performance evaluation simulation;k ary n cubes interconnection networks high performance parallel systems synthetic workload application driven workloads synthetic communication micro kernels synthetic traffic point to point causality interconnection architecture;data mining;interconnection network;traffic characterization;parallel architectures multiprocessor interconnection networks;computational modeling;parallel architectures;parallel systems;synchronization;multiprocessor interconnection networks telecommunication traffic traffic control intelligent networks stress concurrent computing application software high performance computing performance evaluation computational modeling;interconnection networks;high performance;benchmark testing	Evaluation of high performance parallel systems is a delicate issue, due to the difficulty of generating workloads that represent, those that will run on actual systems. We overview the most usual workloads for performance evaluation purposes, in the scope of interconnection networks simulation. Aiming to fill the gap between purely synthetic and application-driven workloads, we present a set of synthetic communication micro-kernels that enhance regular synthetic traffic by adding point-to-point causality. They are conceived to stress the interconnection architecture. As an example of the proposed method-ology, we use these micro-kernels to evaluate a topological improvement of k-ary n-cubes.	causality;dhrystone;interconnection;olap cube;performance evaluation;point-to-point protocol;simulation;synthetic data;synthetic intelligence	Javier Navaridas;José Miguel-Alonso	2009	2009 Eighth International Symposium on Parallel and Distributed Computing	10.1109/ISPDC.2009.20	synchronization;benchmark;probability density function;parallel computing;real-time computing;point-to-point;computer science;operating system;distributed computing;computational model;computer network	Arch	-9.166715706073713	47.72681889453462	54048
6a8392825e40becdb3c3f0f459ca244bffb2815e	dvm: scaling out virtual memory in userspace		One of the most challenging problems in modern distributed big data systems lies in their memory management: these systems preallocate a fixed amount of memory before applications start. In the best case where more memory can be acquired, users have to reconfigure the deployment and re-compute many intermediate results. If no more memory is available, users are then forced to manually partition the job into smaller tasks, incurring both development and performance overhead. This paper presents a user-level utility for scaling the memory in a distributed setup---the Distributed Virtual Memory (DVM). DVM enables to efficiently swap data between memory and disk between arbitrary nodes without users' intervention or applications' awareness.	best, worst and average case;big data;data system;high- and low-level;memory management;middleware;operating system;overhead (computing);paging;run time (program lifecycle phase);software deployment;user space	Abdullah Al Mamun;Ke Wang;Jialin Liu;Dongfang Zhao	2018		10.1145/3229710.3229737	memory management;software deployment;parallel computing;swap (finance);big data;distributed computing;scaling;computer science;general-purpose computing on graphics processing units;virtual memory;preemption	OS	-17.492050828955893	55.019227488841345	54060
17c0fad541cb6b71b368ed20d73964d20740cf4a	achieving energy efficiency during collective communications	cpu throttling;dynamic voltage and frequency scaling dvfs;energy aware collective communications;materials science energy aware collective communications;message passing interface mpi	Energy consumption has become a major design constraint in modern computing systems. With the advent of petaflops architectures, power-efficient software stacks have become imperative for scalability. Techniques such as dynamic voltage and frequency scaling (called DVFS) and CPU clock modulation (called throttling) are often used to reduce the power consumption of the compute nodes. To avoid significant performance losses, these techniques should be used judiciously during parallel application execution. For example, its communication phases may be good candidates to apply the DVFS and CPU throttling without incurring a considerable performance loss. They are often considered as indivisible operations although little attention is being devoted to the energy saving potential of their algorithmic steps. In this work, two important collective communication operations, all-to-all and allgather, are investigated as to their augmentation with energy saving strategies on the per-call basis. The experiments prove the viability of such a fine-grain approach. They also validate a theoretical power consumption estimate for multicore nodes proposed here. While keeping the performance loss low, the obtained energy savings were always significantly higher than those achieved when DVFS or throttling were switched on across the entire application run. Copyright © 2012 John Wiley & Sons, Ltd.	algorithm;benchmark (computing);car–parrinello molecular dynamics;central processing unit;computation;concurrency control;design closure;dynamic frequency scaling;dynamic voltage scaling;elegant degradation;elemental;experiment;flops;hpcc;ibm notes;imperative programming;indivisible;john d. wiley;lapack;linear algebra;modulation;multi-core processor;nas parallel benchmarks;network-attached storage;point-to-point (telecommunications);reduction (complexity);scalability	Vaibhav Sundriyal;Masha Sosonkina;Zhao Zhang	2013	Concurrency and Computation: Practice and Experience	10.1002/cpe.2911	parallel computing;real-time computing;computer science;operating system;distributed computing	HPC	-5.432011104576634	49.62246752746926	54125
9ce702cdc87fed9ba23f21eeba384331967ea1a5	a novel cache-utilization-based dynamic voltage-frequency scaling mechanism for reliability enhancements	voltage control;reliability 7t 14t sram cache dynamic voltage frequency scaling dvfs;reliability;error correction codes;7t 14t sram cache utilization based dynamic voltage frequency scaling mechanism dvfs methods cache architecture;computer architecture;sram cells;sram chips cache storage integrated circuit design integrated circuit reliability low power electronics;reliability sram cells voltage control computer architecture switches error correction codes;switches	We propose a cache architecture using a 7T/14T SRAM (Fujiwara et al., 2009) and a control mechanism for reliability enhancements. Our control mechanism differs from conventional dynamic voltage-frequency scaling (DVFS) methods in that it considers not only the cycles per instruction behaviors but also the cache utilization. To measure cache utilization, a novel metric is proposed. The experimental results show that our proposed method achieves 1000 times less bit-error occurrences compared with conventional DVFS methods under the ultralow-voltage operation. Moreover, the results indicate that our proposed method surprisingly not only incurs no performance and energy overheads but also achieves on average a 2.10% performance improvement and a 6.66% energy reduction compared with conventional DVFS methods.	cpu cache;cache (computing);cycles per instruction;dynamic voltage scaling;experiment;frequency scaling;image scaling;iterative method;monte carlo method;static random-access memory	Yen-Hao Chen;Yi-Lun Tang;Yi-Yu Liu;Allen C.-H. Wu;TingTing Hwang	2016	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2016.2614993	computer architecture;parallel computing;real-time computing;network switch;computer science;reliability	EDA	-4.781701466624559	55.87299903901255	54240
c3ce58b41ef5d94f471ef155b885692092464798	fmi: fault tolerant messaging interface for fast and transparent recovery	dynamic node allocation fault tolerant messaging interface transparent recovery fast recovery supercomputers higher fidelity simulations failure rates checkpoint restart parallel file system failure mitigation extreme scale systems low latency recovery survivable communication runtime fast in memory c r check pointing application state fmi runtime software failure free performance mpi;software fault tolerance checkpointing failure analysis parallel processing;checkpoint restart;fault tolerance fault tolerant systems overlay networks runtime peer to peer computing resource management hardware;checkpoint restart fault tolerance mpi;fault tolerance;mpi	Future supercomputers built with more components will enable larger, higher-fidelity simulations, but at the cost of higher failure rates. Traditional approaches to mitigating failures, such as checkpoint/restart (C/R) to a parallel file system incur large overheads. On future, extreme-scale systems, it is unlikely that traditional C/R will recover a failed application before the next failure occurs. To address this problem, we present the Fault Tolerant Messaging Interface (FMI), which enables extremely low-latency recovery. FMI accomplishes this using a survivable communication runtime coupled with fast, in-memory C/R, and dynamic node allocation. FMI provides message-passing semantics similar to MPI, but applications written using FMI can run through failures. The FMI runtime software handles fault tolerance, including check pointing application state, restarting failed processes, and allocating additional nodes when needed. Our tests show that FMI runs with similar failure-free performance as MPI, but FMI incurs only a 28% overhead with a very high mean time between failures of 1 minute.	application checkpointing;clustered file system;experiment;failure cause;fault tolerance;fault-tolerant computer system;fault-tolerant messaging;finnish meteorological institute;ibm websphere extreme scale;in-memory database;mean time between failures;message passing interface;overhead (computing);programming model;requirement;scalability;simulation;solver;state (computer science);supercomputer;transaction processing system	Kento Sato;Adam Moody;Kathryn Mohror;Todd Gamblin;Bronis R. de Supinski;Naoya Maruyama;Satoshi Matsuoka	2014	2014 IEEE 28th International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2014.126	fault tolerance;parallel computing;real-time computing;computer science;message passing interface;operating system;distributed computing	HPC	-17.600677191758727	49.741052819099785	54243
f93a64f4a7d54cc2654bd7717a273dd4d70e3fde	exploiting replication to improve performances of nuca-based cmp systems	cache memory;block replication;multicore;nuca	Improvements in semiconductor nanotechnology made chip multiprocessors the reference architecture for high-performance microprocessors. CMPs usually adopt large Last-Level Caches (LLC) shared among cores and private L1 caches, whose performances depend on the wire-delay dominated response time of LLC. NUCA (NonUniform Cache Architecture) caches represent a viable solution for tolerating wire-delay effects. In this article, we present Re-NUCA, a NUCA cache that exploits replication of blocks inside the LLC to avoid performance limitations of D-NUCA caches due to conflicting access to shared data. Results show that a Re-NUCA LLC permits to improve performances of more than 5% on average, and up to 15% for applications that strongly suffer from conflicting access to shared data, while reducing network traffic and power consumption with respect to D-NUCA caches. Besides, it outperforms different S-NUCA schemes optimized with victim replication.	cpu cache;lunar lander challenge;microprocessor;network packet;performance;reference architecture;replication (computing);response time (technology);semiconductor	Pierfrancesco Foglia;Marco Solinas	2014	ACM Trans. Embedded Comput. Syst.	10.1145/2566568	multi-core processor;cache coherence;computer architecture;parallel computing;real-time computing;cpu cache;computer science;operating system	Arch	-7.831214600130597	54.17532444681787	54420
d36f0cf5375345732339abe77255f024d5a9d05a	improving per-node efficiency in the datacenter with new os abstractions	datacenter;large scale;operating system;akaros;custom os;high performance;parallel applications;fault recovery	We believe datacenters can benefit from more focus on per-node efficiency, performance, and predictability, versus the more common focus so far on scalability to a large number of nodes. Improving per-node efficiency decreases costs and fault recovery because fewer nodes are required for the same amount of work. We believe that the use of complex, general-purpose operating systems is a key contributing factor to these inefficiencies.  Traditional operating system abstractions are ill-suited for high performance and parallel applications, especially on large-scale SMP and many-core architectures. We propose four key ideas that help to overcome these limitations. These ideas are built on a philosophy of exposing as much information to applications as possible and giving them the tools necessary to take advantage of that information to run more efficiently. In short, high-performance applications need to be able to peer through layers of virtualization in the software stack to optimize their behavior. We explore abstractions based on these ideas and discuss how we build them in the context of a new operating system called Akaros.	data center;general-purpose modeling;manycore processor;operating system;scalability;symmetric multiprocessing	Barret Rhoden;Kevin Klues;David X. Zhu;Eric A. Brewer	2011		10.1145/2038916.2038941	embedded system;data center;parallel computing;real-time computing;computer science;operating system;computer network	HPC	-15.854682924006857	51.143385470050895	54433
06902cb95ede2c305db4000852014f276b25c082	active pages: a computation model for intelligent memory	performance evaluation;reconfigurable architectures memory architecture multiprocessing systems performance evaluation;reconfigurable architectures;computer model;reconfigurable logic;memory technologies active pages computation model intelligent memory data intensive computations rxdram reconfigurable architecture dram memory system simplescalar simulator radram system;reconfigurable architecture;memory architecture;computational modeling logic microprocessors computer architecture random access memory fabrication integrated circuit technology computer applications programming profession computer science;memory systems;multiprocessing systems;data intensive computing	Microprocessors and memory systems suffer from a growing gap in performance. We introduce Active Pages, a computation model which addresses this gap by shifting data-intensive computations to the memory system. An Active Page consists of a page of data and a set of associated functions which can operate upon that data. We describe an implementation of Active Pages on RADram (Reconfigurable Architecture DRAM), a memory system based upon the integration of DRAM and reconfigurable logic. Results from the SimpleScalar simulator [BA97] demonstrate up to 1000X speedups on several applications using the RADram system versus conventional memory systems. We also explore the sensitivity of our results to implementations in other memory technologies.	data-intensive computing;dynamic random-access memory;microprocessor;model of computation;reconfigurable computing	Mark Oskin;Frederic T. Chong;Timothy Sherwood	1998		10.1145/279358.279387	computer simulation;memory address;uniform memory access;shared memory;embedded system;interleaved memory;computer architecture;semiconductor memory;parallel computing;distributed memory;memory refresh;computer science;operating system;data-intensive computing;memory protection;computer memory;overlay;conventional memory;flat memory model;computing with memory;cache-only memory architecture;memory map;non-uniform memory access	Arch	-8.698178767389109	47.36295971178999	54532
5918cbd17727819092039cf252e3eb26a0c16430	in-dram data initialization		Initializing memory with zero data is essential for safe memory management. However, initializing a large memory area slows down the system significantly. The most likely cause for initialization to slow down the system is the limited DRAM initialization method. At present, the only way to initialize DRAM area is to execute multiple WRITE commands. However, the WRITE command slows the initialization because of its small granularity and data bus occupancy. In this brief, we propose an efficient in-DRAM initialization method inspired by the internal structure and operation of DRAM. The proposed method, called row reset, uses a DRAM row buffer to zero out a single DRAM row at a time. Row Reset allows for parallel initialization on multiple DRAM banks without using off-chip data transfer, thus reducing initialization time by up to 63 times. Row reset is a practical approach, because it can be implemented with existing circuitry in DRAM without additional area overhead.	dynamic random-access memory;electronic circuit;memory management;overhead (computing)	Hoseok Seol;Wongyu Shin;Jaemin Jang;Jungwhan Choi;Jinwoong Suh;Lee-Sup Kim	2017	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2017.2737646	cas latency;very-large-scale integration;real-time computing;memory management;computer science;parallel computing;initialization;dram;memory rank;granularity;system bus	Visualization	-8.673406826500193	54.09259125566632	54700
bc524e690856a71c50d170f7fc93532b09b4085f	limited contiguous processor allocation mechanism in the mesh-connected multiprocessors using compaction	allocation;mapping;migration;multiprocessor;submesh	In this paper, several efficient migration and allocation strategies have been compared on the mesh-based multiprocessor systems. The traditional non-preemptive submesh allocation strategies consist of two row boundary (TRB) and two column boundary (TCB). The existing migration mechanisms are online dynamic compaction-four corner (ODC-FC), limited top-down compaction (LTDC), TCB, and the combination of TCB and ODC-FC algorithms. Indeed, the new allocation method is presented in this paper. This mechanism has the benefits of two efficient traditional allocation algorithms. It is the combination of the TCB and TRB allocation methods. Also, in this process the impact of four key metrics on online mapping is considered. The parameters are average task execution time (ATET), average task system utilization (ATSU), average task waiting time (ATWT), and average task response time (ATRT). Using TCB and TRB mechanism with the migration strategies is shown that the new algorithm has better ATET, ATRT, ATWT, and ATSU. It has, respectively, 23.5494, 97.1216, 39.1291, and 4.142% improvements in comparison with the previous mechanisms.	3d printing;central processing unit;data compaction;experiment;memory management;molecular dynamics;multiprocessing;orthogonal defect classification;response time (technology);run time (program lifecycle phase);selection algorithm;simulation;terminating reliable broadcast;top-down and bottom-up design;trusted computing base;web mapping	Akram Reza;Mahnaz Rafie	2017	The Journal of Supercomputing	10.1007/s11227-017-2031-9	computer science;distributed computing;parallel computing;contiguity (probability theory);multiprocessing;response time	HPC	-14.989551364818613	59.780676012305314	54707
864fc6401c7d393a361dd5faa810b4f13f81d9aa	power-aware computing for multi-agent systems	multi threading;multi agent system;power aware computing multi agent systems multiprocessing systems multi threading;computational modeling clocks instruction sets throughput runtime load modeling java;power aware computing;multi agent systems;biological computation power aware computing multiagent system agent based modelling multicore processing concurrent programming thermal capacity energy availability sustainability game of life event driven computation dynamic frequency scaling multithreaded rate limited application energy efficiency;multiprocessing systems	Agent-based modelling is becoming a common technique for studying complex phenomenon in diverse fields including sociology, economics and biology. This technique is assisted by the continuing exponential and pervasive growth in computing power. In recent years, engineering limits on processor speed have spurred focus on multi-core processing as a means of continuing this growth. In order to harness this computing power, however, careful concurrent programming is required to develop multi-threaded applications. Furthermore, the power consumed by systems has come under increasing scrutiny from the standpoints of thermal capacity, energy availability, and sustainability. We investigated two schemes for reducing simulation power demands using The Game of Life as a representative simulation: a) event-driven computation and b) dynamic frequency scaling in multi-threaded rate-limited applications. Both schemes were found to significantly improve energy efficiency. Both event-driven and parallelized computation are central to the low power usage of biological computation compared to silicon.	agent-based model;algorithm;artificial neural network;biological computation;clock rate;complex systems;concurrent computing;conway's game of life;dynamic frequency scaling;emoticon;event-driven finite-state machine;event-driven programming;heuristic;image scaling;multi-agent system;multi-core processor;parallel computing;performance per watt;pervasive informatics;scalability;shutdown (computing);simulation;sparse matrix;spiking neural network;thread (computing);time complexity;underclocking;vii	Vaenthan Thiruvarudchelvan;Terry Bossomaier	2011	2011 IEEE Symposium on Artificial Life (ALIFE)	10.1109/ALIFE.2011.5954668	real-time computing;multithreading;computer science;artificial intelligence;theoretical computer science;multi-agent system;distributed computing	Arch	-4.689664394808988	48.56699115156509	54816
550d51485e3a7e7deb41f12bb8c66c7474a416c3	compiler support for software-based cache partitioning	software-based cache partitioning;data reference;execution time;cache memory;unpredictable execution time behavior;hard real-time system;cached real-time system;instruction partitioning;data partitioning;compiler support;real-time task;charts;real time systems;real time;non linear control	Cache memories have become an essential part of modern processors to bridge the increasing gap between fast processors and slower main memory. Until recently, cache memories were thought to impose unpredictable execution time behavior for hard real-time systems. But recent results show that the speedup of caches can be exploited without a significant sacrifice of predictability. These results were obtained under the assumption that real-time tasks be scheduled non-preemptively.This paper introduces a method to maintain predictability of execution time within preemptive, cached real-time systems and discusses the impact on compilation support for such a system. Preemptive systems with caches are made predictable via software-based cache partitioning. With this approach, the cache is divided into distinct portions associated with a real-time task, such that a task may only use its portion. The compiler has to support instruction and data partitioning for each task. Instruction partitioning involves non-linear control-flow transformations, while data partitioning involves code transformations of data references. The impact on execution time of these transformations is also discussed.	algorithm;best, worst and average case;cpu cache;cache (computing);central processing unit;compiler;computer data storage;control flow;control system;nonlinear system;preemption (computing);real-time clock;real-time computing;real-time locating system;real-time transcription;run time (program lifecycle phase);scheduling analysis real-time systems;simulation;speedup;worst-case execution time	Frank Mueller	1995		10.1145/216636.216677	computer architecture;parallel computing;real-time computing;cache coloring;page cache;nonlinear control;cpu cache;cache;computer science;cache invalidation;chart;smart cache;programming language;cache algorithms;cache pollution	Embedded	-10.568889057539469	50.5706310147387	54924
1da14776bed9636ee26e8cb0715392ad801f2e62	a process migration subsystem for a workstation-based distributed system	workload;distributed system;reliability;personal computers;process migration subsystem;memory management;fault tolerant;service needs;processor scheduling;resource allocation;distributed processing;resource management;distributed computing;computer networks;computer reliability;fault tolerant computing;design and implementation;real time scheduling deadlines;dynamic process relocation;networking technology;scheduling;real time scheduling;workstations;fault tolerance;reliability distributed processing workstations processor scheduling resource allocation fault tolerant computing;algorithms;distributed control workstations distributed computing costs fault tolerant systems hardware minerals fluctuations real time systems dynamic scheduling;workstation based environment process migration subsystem workstation based distributed computing environments cost performance ratio networking technology workload preemptive process migration facility dynamic process relocation dynamic fluctuations service needs fault tolerance real time scheduling deadlines node failures;distributed computing environment;preemptive process migration facility;cost performance ratio;workstation based distributed computing environments;distributed systems;performance ratio;distributed data processing;process migration;node failures;fault tolerant computers;unix;dynamic fluctuations;workstation based environment;mathematics computers information science management law miscellaneous	Workstation based distributed computing environments are getting popular in both academic and commercial communities, due to the continuing trend of decreasing cost/performance ratio and rapid development of networking technology. However, the workload on these workstations is usually much lower than their computing capacity, especially with the ever increasing computing power of new hardware. As a result, the resources of such workstations are often under utilized and many of them are frequently idle. A preemptive process migration facility can be provided, in such a distributed system, to dynamically relocate running processes among the component machines. Such relocation can help cope with dynamic fluctuations in loads and service needs, improve the system's fault tolerance, meet real time scheduling deadlines, or bring a process to a special device. The paper presents a process migration subsystem for tolerating process and node failures on a workstation based environment. The design and implementation of the subsystem are also discussed.	distributed computing;process migration;workstation	Khalid M. Al-Tawil;Muslim Bozyigit;Syed Khwaja Naseer	1996		10.1109/HPDC.1996.546222	fault tolerance;parallel computing;real-time computing;computer science;resource management;operating system;distributed computing;computer network	HPC	-16.834881431674827	59.67845749045653	55003
f36425ef37d0f2aabf4993101c9b2c75727dce2e	an evaluation of asynchronous software events on modern hardware		Runtimes and applications that rely heavily on asynchronous event notifications suffer when such notifications must traverse several layers of processing in software. Many of these layers necessarily exist in order to support a general-purpose, portable kernel architecture, but they introduce considerable overheads for demanding, high-performance parallel runtimes and applications. Other overheads can arise from a mismatched event programming or system call interface. Whatever the case, the average latency and variance in latency of commonly used software mechanisms for event notifications is abysmal compared to the capabilities of the hardware, which can exhibit orders of magnitude lower latency. We leverage the flexibility and freedom of the previously proposed Hybrid Runtime (HRT) model to explore the construction of low-latency, asynchronous software events uninhibited by interfaces and execution models commonly imposed by general-purpose OSes. We propose several mechanisms in a system we call Nemo which employs kernel mode-only features to accelerate event notifications by up to 4,000 times and we provide a detailed evaluation of our implementation using extensive microbenchmarks. We carry out our evaluation both on a modern x64 server and the Intel Xeon Phi. Finally, we propose a small addition to existing interrupt controllers (APICs) that could push the limit of asynchronous events closer to the latency of the hardware cache coherence network.	cache (computing);cache coherence;event (computing);general-purpose markup language;linux mint;protection ring;runtime system;server (computing);system call;traverse;user space;x86-64;xeon phi	Kyle C. Hale;Peter A. Dinda	2018	2018 IEEE 26th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)	10.1109/MASCOTS.2018.00041	latency (engineering);real-time computing;interrupt;computer hardware;architecture;asynchronous communication;system call;xeon phi;computer science;software;event-driven programming	Arch	-10.74394638547168	47.5058021976099	55113
3590ffc986bc973be1db95ac3ea0eeb020b77f92	dynamic tolerance region computing for multimedia	application development;time trade off;energy aware systems;special purpose hardware;form factor;multimedia application;satisfiability;multimedia computing;media;performance improvement;mobile applications;energy delay savings dynamic tolerance region computing multimedia computing portable devices computational requirements power consumption energy consumption region reuse schema tolerant region reuse media algorithms less consuming hardware structures media contents signal to noise ratio;registers;energy consumption;application profile;signal processing;multimedia applications and multimedia signal processing;multimedia communication;output error;mobile applications special purpose hardware energy aware systems multimedia applications and multimedia signal processing;signal to noise ratio;program processors media hardware registers benchmark testing multimedia communication energy consumption;program processors;mobile application;benchmark testing;multimedia computing energy consumption;hardware	Multimedia computation has clearly become a primary and demanding application segment for new architectures targeted at portable devices. The main challenge for such architectures is to keep pace with the computational requirements of ever evolving media standards and applications while satisfying the power and energy consumption required to leverage smaller form-factors and longer battery lifetimes. One technique aimed at reducing both the energy consumption and the execution time of an application is Reuse. This technique memorizes the outcome of an instruction or set of instructions so that we can reuse it the next time we perform the same operation with the same inputs. In this paper, we analyze a region reuse schema specially focused on multimedia applications. While the technique appears to be, in theory, a promising vehicle to improve both timing and energy for low-end media applications, we will show that the extra hardware cost required becomes a severe shortcoming as we find the undesirable situation where we have to consume more energy in order to reduce the execution time (hence becoming a poor power-oriented solution). To mitigate the overhead of the reuse hardware, we advocate for exploiting a third variable in the power-time trade-off and we evaluate tolerant region reuse, a technique that relies in the tolerance in the output precision of media algorithms to improve reuse. With this technique, we afford to use less consuming hardware structures that drives benefits in both energy and timing. As a trade-off, tolerant region reuse introduces non-noticeable errors in the output data. The main drawback of tolerant region reuse is the strong reliance on application profiling, the need for careful tuning from the application developer, and the inability of the technique to adapt to the variability of the media contents being used as inputs. To address that inflexibility, we introduce dynamic tolerant region reuse. This novel technique overcomes the drawbacks of tolerant region reuse by allowing the hardware to study the precision quality of the region reuse output. Our mechanism allows the programmer to grant a minimum threshold on signal-to-noise ratio (SNR) while letting the technique adapt to the characteristics of the specific application and workload to minimize time and energy consumption. This leads to greater energy-delay savings while keeps output error below noticeable levels, avoiding at the same time the need of profiling. We studied our mechanism applied to a set of three different processors, from low to high end. As we will show our technique leads to consistent performance improvements in all of our benchmark programs while reducing energy consumption. We can report savings up to 30 percent in the energy*delay factor for all three processors.		Carlos Álvarez;Jesús Corbal;Mateo Valero	2012	IEEE Trans. Computers	10.1109/TC.2011.79	embedded system;benchmark;parallel computing;real-time computing;simulation;media;form factor;telecommunications;computer science;electrical engineering;operating system;signal processing;processor register;signal-to-noise ratio;rapid application development;computer network;satisfiability	Visualization	-5.244031525525383	53.861861454688764	55522
c7ac478c0f155260d17d238b116954b41b02c468	integrating mpi with docker for hpc	libraries;kernel;standards;linux;tools;containers	Container technology has the potential to considerably simplify the management of the software stack of High Performance Computing (HPC) clusters. However, poor integration with established HPC technologies is still preventing users and administrators to reap the benefits of containers. Message Passing Interface (MPI) is a pervasive technology used to run scientific software, often written in Fortran and C/C++, that presents challenges for effective integration with containers. This work shows how an existing MPI implementation can be extended to improve this integration.	c++;docker;fortran;message passing interface	Maximilien de Bayser;Renato F. G. Cerqueira	2017	2017 IEEE International Conference on Cloud Engineering (IC2E)	10.1109/IC2E.2017.40	embedded system;parallel computing;computer science;operating system	HPC	-15.668192156750901	50.22521719250084	55547
8576a98639bb48310a317b324458bc1795e14207	solo: a lightweight virtual machine	lightweight;software;virtual machine;virtual machine monitor;high performance computing application requirements;protocols;virtualization;memory protocols;memory management;performance evaluation;virtual machines memory protocols parallel processing supervisory programs;supervisory programs;virtual machine monitor solo lightweight virtual machine virtualization overhead reduction vm performance isolation high performance computing application requirements i o operation traditional os hardware support centralized memory sharing protocol vmm design;vmm design;solo lightweight virtual machine;virtualization overhead reduction;vm performance isolation;hpc;i o operation;virtual machines;hardware support;centralized memory sharing protocol;high performance computer;linux;traditional os;hpc lightweight virtualization;security;high performance;parallel processing;hardware;virtual machining virtual manufacturing hardware voice mail application virtualization operating systems application software isolation technology data security linux	The overhead caused by virtualization makes it difficult to apply VM in the applications which require high degrees of both performance isolation and efficiency, such as the high performance computing. In this paper, we present a lightweight virtual machine, named Solo. It simplifies the design of VMM greatly by making most privileged instructions bypass the VMM, except the I/O operations. Solo allows VM running directly on hardware with the highest privileges, therefore greatly reduces the overhead caused by virtualization. Our evaluation shows that Solo not only guarantees the VM performance isolation, but also improves VM performance to the level of traditional OS, and thus meets the requirements of the high performance applications without special hardware support.	centralized computing;hardware virtualization;input/output;operating system;overhead (computing);requirement;solo;supercomputer;virtual machine manager;x86 virtualization;z/vm	Xiang Zhang;Jie Ma;Yanchao Miao;Qingwei Meng;Dan Meng	2009	2009 IEEE International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2009.7	embedded system;parallel processing;parallel computing;real-time computing;computer science;virtual machine;operating system;database;distributed computing;computer network	Arch	-17.621593544135692	51.84544609518996	55630
0304ea50f51505138668cf5699c7ada4f33e23f2	fine-grain data management directory for openmp 4.0 and openacc	accelerator memory coherence;openacc;openmp 4 0;heterogeneous directory;automatic tranfers	Today's trend to use accelerators in heterogeneous systems forces a paradigm shift in programming models. The use of low-level APIs for accelerator programming is tedious and not intuitive for casual programmers. To tackle this problem, recent approaches focused on high-level directive-based models, with a standardization effort made with OpenACC and the directives for accelerator in the latest OpenMP 4.0 release. The pragmas for data management automatically handle data exchange between the host and the device. To keep the runtime simple and efficient, severe restrictions hinder the use of these pragmas. To address this issue, we propose the design for a directory, along with a reduced runtime application binary interface, to handle correctly data management in these standards. A few improvements to our directory allow a more flexible use of data management pragmas, with negligible overhead. Our design fits a multi-accelerator system. Copyright © 2014 John Wiley & Sons, Ltd.	openacc;openmp	Julien Jaeger;Patrick Carribault;Marc Pérache	2015	Concurrency and Computation: Practice and Experience	10.1002/cpe.3352	parallel computing;computer science;operating system;database;distributed computing;programming language	DB	-15.71982498028897	50.56509226433949	55682
a7a569d87773cf261c5848227435a7c8b3fb8840	a simulation analysis of reliability in primary storage deduplication	analytical models;measurement;metadata;storage management raid;maintenance engineering;layout;maintenance engineering metadata redundancy measurement analytical models layout;redundancy;physical capacity simulation analysis primary storage deduplication storage efficiency improvement storage system reliability primary workloads file system snapshots trace driven deduplication aware simulation framework data loss analyis chunk level file level whole disk failures sector error damage reduction intrafile redundancy elimination whole disk failure damage copy technique physical area	Deduplication has been widely used to improve storage efficiency in modern primary and secondary storage systems, yet how deduplication fundamentally affects storage system reliability remains debatable. This paper aims to analyze and compare storage system reliability with and without deduplication in primary workloads using real-world file system snapshots. Specifically, we propose a trace-driven, deduplication-aware simulation framework that analyzes data loss in both chunk and file levels due to sector errors and whole-disk failures. Compared to without deduplication, our analysis shows that deduplication consistently reduces the damage of sector errors due to intra-file redundancy elimination, but potentially increases the damages of whole-disk failures if the highly referenced chunks are not carefully placed on disk. To improve reliability, we examine a deliberate copy technique that stores and repairs first the most referenced chunks in a small dedicated physical area (e.g., 1% of the physical capacity), and demonstrate its effectiveness through our simulation framework.	auxiliary memory;computer data storage;data deduplication;fragmentation (computing);raid;real life;simulation;storage efficiency;universal disk format;world file	Min Fu;Patrick P. C. Lee;Dan Feng;Zuoning Chen;Yu Xiao	2016	2016 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2016.7581280	maintenance engineering;layout;parallel computing;real-time computing;data deduplication;computer science;operating system;database;redundancy;metadata;measurement	OS	-12.614509362549283	55.29355420595019	55819
e10c83abeea3a5260c3b0b8c9dfb75f582b2a720	further exploit the potential of i/o forwarding by employing file striping		I/O forwarding has become a standard I/O layer in today's top HPC systems to alleviate the mismatch of computing power and I/O bandwidth. Each I/O node is exclusively utilized by a small group of compute nodes to forward their I/O requests to storage servers. Since I/O workloads on production HPC systems are bursty both in time and space, the load on I/O nodes are highly unbalanced. Rank 0 I/O pattern that commonly exists in scientific applications aggravates the load imbalance as well. A small number of I/O nodes can easily become hot spots, which hinders the potential of I/O forwarding layer to be fully realized. In addition, the lack of coordination between I/O forwarding layer and storage system layer causes serious I/O interference and impacts the performance significantly. In this paper, we propose to employ file striping on I/O forwarding layer to apportion the heavy workloads to multiple idle I/O nodes. We also coordinate the file striping on I/O forwarding layer and storage system layer to minimize the I/O interference on storage servers. Our ideas are implemented and evaluated on an open-source I/O forwarding software IOFSL, and the results show that the I/O performance of applications can be greatly accelerated by our approach.	computer data storage;data striping;hotspot (wi-fi);input/output;interference (communication);open-source software;unbalanced circuit	Jie Yu;Guang Ming Liu;Wenrui Dong;Xiaoyong Li;Yusheng Liu;Bin Xu;Yuqi Li;Qingzhen Ma	2017	2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC)	10.1109/ISPA/IUCC.2017.00053	multimedia;computer network;concurrent computing;input/output;data striping;exploit;computer science;small number;computer data storage;server;idle	HPC	-15.802395152060292	52.921923829198306	55833
8ff24b26e23ced3f93893d58687c74282f18d73a	streamcloud: an elastic and scalable data streaming system	informatica;elasticity;resource allocation cloud computing data handling parallel processing;resource allocation;semantics;streaming media;load management;elasticity data streaming scalability;scalability;data handling;peer to peer computing;peer to peer computing semantics streaming media scalability load management cloud computing elasticity;dynamic load balancing streamcloud elastic data streaming system scalable data streaming system continuous data flows stream processing engines parallelization technique elastic protocols;parallel processing;data streaming;cloud computing	Many applications in several domains such as telecommunications, network security, large-scale sensor networks, require online processing of continuous data flows. They produce very high loads that requires aggregating the processing capacity of many nodes. Current Stream Processing Engines do not scale with the input load due to single-node bottlenecks. Additionally, they are based on static configurations that lead to either under or overprovisioning. In this paper, we present StreamCloud, a scalable and elastic stream processing engine for processing large data stream volumes. StreamCloud uses a novel parallelization technique that splits queries into subqueries that are allocated to independent sets of nodes in a way that minimizes the distribution overhead. Its elastic protocols exhibit low intrusiveness, enabling effective adjustment of resources to the incoming load. Elasticity is combined with dynamic load balancing to minimize the computational resources used. The paper presents the system design, implementation, and a thorough evaluation of the scalability and elasticity of the fully implemented system.	bottleneck (software);computation;computational resource;elasticity (cloud computing);elasticity (data store);load balancing (computing);network security;overhead (computing);parallel computing;sql;scalability;stream processing;systems design;telecommunications network	Vincenzo Gulisano;Ricardo Jiménez-Peris;Marta Patiño-Martínez;Claudio Soriente;Patrick Valduriez	2012	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2012.24	parallel processing;parallel computing;real-time computing;scalability;cloud computing;resource allocation;computer science;operating system;group method of data handling;distributed computing;semantics;elasticity	OS	-18.822616138700514	56.19499561197569	55842
05dbb6f58d8459d26e93f0b50421f25558533af5	an improved biased random sampling algorithm for load balancing in cloud based systems	random sampling;queue length;random sampling method;load balancing;load balance;cloud computing	In cloud computing, load balancing is an important issue. We propose an improvement on random sampling algorithm for load balancing in cloud based systems. We have redesigned the Biased Random Sampling algorithm by making use of queue length and processing time as parameters for arriving at load balancing among competing nodes. Our experimental results show that the proposed algorithms perform better than the current Biased Random Sampling algorithm.	algorithm;cloud computing;gibbs sampling;load balancing (computing);monte carlo method;sampling (signal processing)	Sheeja S. Manakattu;S. D. Madhu Kumar	2012		10.1145/2345396.2345472	parallel computing;real-time computing;computer science;load balancing;distributed computing	Metrics	-16.714170640152446	60.08189139286188	55863
18c9aaafbe51da1485d2aac7c6b998a5b0f2db12	pstore: an efficient storage framework for managing scientific data	floating point data;range query;two level chunking;array data;compression;byte wise partitioning	In this paper, we present the design, implementation, and evaluation of PStore, a no-overwrite storage framework for managing large volumes of array data generated by scientific simulations. PStore consists of two modules, a data ingestion module and a query processing module, that respectively address two of the key challenges in scientific simulation data management. The data ingestion module is geared toward handling the high volumes of simulation data generated at a very rapid rate, which often makes it impossible to offload the data onto storage devices; the module is responsible for selecting an appropriate compression scheme for the data at hand, chunking the data, and then compressing it before sending it to the storage nodes. On the other hand, the query processing module is in charge of efficiently executing different types of queries over the stored data; in this paper, we specifically focus on dicing (also called range) queries. PStore provides a suite of compression schemes that leverage, and in some cases extend, existing techniques to provide support for diverse scientific simulation data. To efficiently execute queries over such compressed data, PStore adopts and extends a two-level chunking scheme by incorporating the effect of compression, and hides expensive disk latencies for long running range queries by exploiting chunk prefetching. In addition, we also parallelize the query processing module to further speed up execution. We evaluate PStore on a 140 GB dataset obtained from real-world simulations using the regional climate model CWRF [5]. In this paper, we use both 3D and 4D datasets and demonstrate high performance through extensive experiments.	cpu cache;chunking (computing);climate model;dicing;data compression;database;experiment;multiple buffering;overhead (computing);range query (data structures);shallow parsing;simulation;speedup	Souvik Bhattacherjee;Amol Deshpande;Alan Sussman	2014		10.1145/2618243.2618268	range query;computer science;theoretical computer science;data mining;database;programming language;compression	DB	-14.518453734399678	53.857137442170625	55900
788799f2335f955164f7c6278b7f38443f8de9d5	scheduling in data intensive and network aware (diana) grid environments	grid scheduling;data processing;active network;scheduling algorithm;peer to peer;job scheduling	In Grids scheduling decisions are often made on the basis of jobs being either data or computation intensive: in data intensive situations jobs may be pushed to the data and in computation intensive situations data may be pulled to the jobs. This kind of scheduling, in which there is no consideration of network characteristics, can lead to performance degradation in a Grid environment and may result in large processing queues and job execution delays due to site overloads. In this paper we describe a Data Intensive and Network Aware (DIANA) meta-scheduling approach, which takes into account data, processing power and network characteristics when making scheduling decisions across multiple sites. Through a practical implementation on a Grid testbed, we demonstrate that queue and execution times of data-intensive jobs can be significantly improved when we introduce our proposed DIANA scheduler. The basic scheduling decisions are dictated by a weighting factor for each potential target location which is a calculated function of network characteristics, processing cycles and data location and size. The job scheduler provides a global ranking of the computing resources and then selects an optimal one on the basis of this overall access and execution cost. The DIANA approach considers the Grid as a combination of active network elements and takes network characteristics as a first class criterion in the scheduling decision matrix along with computation and data. The scheduler can then make informed decisions by taking into account the changing state of the network, locality and size of the data and the pool of available processing cycles.	algorithm;as-easy-as;central processing unit;computation;crew scheduling;data-intensive computing;elegant degradation;emulator;fault tolerance;first-class function;grid computing;job scheduler;job stream;large hadron collider;locality of reference;mathematical optimization;meta-scheduling;peer-to-peer;queue (abstract data type);run time (program lifecycle phase);scalability;scheduling (computing);simulation;testbed;worldwide lhc computing grid	Richard McClatchey;Ashiq Anjum;Heinz Stockinger;Arshad Ali;Ian Willers;Michael Thomas	2007	CoRR		fair-share scheduling;fixed-priority pre-emptive scheduling;active networking;parallel computing;real-time computing;earliest deadline first scheduling;flow shop scheduling;data processing;dynamic priority scheduling;computer science;rate-monotonic scheduling;job scheduler;operating system;two-level scheduling;distributed computing;round-robin scheduling;scheduling	HPC	-17.330928841886116	60.05213061657933	55935
10dbdd726404cd6b19428a0f5278026f5a4a9383	scalable parallel i/o alternatives for massively parallel partitioned solver systems	libraries;finite element methods;scalable parallel i o alternatives;mathematics computing;reduced blocking strategy;concurrent computing;poor mans parallel i o massively parallel partitioned solver systems high performance computing 1 posix file per processor reduced blocking strategy cfd solver data phasta navier stokes solver token passing scalable parallel i o alternatives synchronized parallel i o library;performance test;high performance computing;unix input output programs mathematics computing parallel processing;input output programs;navier stokes;testing;finite element;1 posix file per processor;computational fluid dynamics;unstructured grid;massively parallel partitioned solver systems;phasta;high performance computer;token passing;writing;synchronized parallel i o library;bandwidth;poor mans parallel i o;cfd solver data;parallel i o;computer science;program processors;unix;parallel applications;concurrent computing writing computational fluid dynamics equations economic indicators finite element methods bandwidth computer science libraries testing;parallel processing;economic indicators;navier stokes solver	With the development of high-performance computing, I/O issues have become the bottleneck for many massively parallel applications. This paper investigates scalable parallel I/O alternatives for massively parallel partitioned solver systems. Typically such systems have synchronized “loops” and will write data in a well defined block I/O format consisting of a header and data portion. Our target use for such an parallel I/O subsystem is checkpoint-restart where writing is by far the most common operation and reading typically only happens during either initialization or during a restart operation because of a system failure. We compare four parallel I/O strategies: 1 POSIX File Per Processor (1PFPP), a synchronized parallel IO library (syncIO), “Poor-Man's” Parallel I/O (PMPIO) and a new “reduced blocking” strategy (rbIO). Performance tests using real CFD solver data from PHASTA (an unstructured grid finite element Navier-Stokes solver [1]) show that the syncIO strategy can achieve a read bandwidth of 6.6GB/Sec on Blue Gene/L using 16K processors which is significantly faster than 1PFPP or PMPIO approaches. The serial “token-passing” approach of PMPIO yields a 900MB/sec write bandwidth on 16K processors using 1024 files and 1PFPP achieves 600 MB/sec on 8K processors while the “reduced-blocked” rbIO strategy achieves an actual writing performance of 2.3GB/sec and perceived/latency hiding writing performance of more than 21,000 GB/sec (i.e., 21TB/sec) on a 32,768 processor Blue Gene/L.	8k resolution;application checkpointing;blocking (computing);blue gene;central processing unit;finite element method;input/output;megabyte;navier–stokes equations;posix;parallel i/o;scalability;solver;supercomputer;time formatting and storage bugs;transaction processing system;unstructured grid	Jing Fu;Ning Liu;Onkar Sahni;Kenneth E. Jansen;Mark S. Shephard;Christopher D. Carothers	2010	2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)	10.1109/IPDPSW.2010.5470887	parallel computing;computer science;theoretical computer science;massively parallel;distributed computing	HPC	-16.144695948342555	49.72481555008291	56161
5e8045347e39748456d163d7726cb3598be8e207	in-situ feature-based objects tracking for large-scale scientific simulations	pattern clustering;shared memory;scientific data;chip;shared memory systems data analysis feature extraction microprocessor chips object tracking pattern clustering;data analysis;large scale;large scale simulation;shared memory systems;feature extraction;object tracking;feature based object tracking;experimental evaluation;data movement in situ feature based object tracking scientific simulation leadership class system computation speed disk input ouptut speed data analytics pipeline data staging data analysis operation doc algorithm decentralized and online clustering algorithm cluster tracking algorithm on chip shared memory;scientific data analysis;feature based object tracking scientific data analysis scalable in situ data analytics;scalable in situ data analytics;microprocessor chips	Emerging scientific simulations on leadership class systems are generating huge amounts of data. However, the increasing gap between computation and disk I/O speeds makes traditional data analytics pipelines based on post-processing cost prohibitive and often infeasible. In this paper, we investigate an alternate approach that aims to bring the analytics closer to the data using data staging and the in-situ execution of data analysis operations. Specifically, we present the design, implementation and evaluation of a framework that can support in-situ feature-based object tracking on distributed scientific datasets. Central to this framework is the scalable decentralized and online clustering (DOC) and cluster tracking algorithm, which executes in-situ (on different cores) and in parallel with the simulation processes, and retrieves data from the simulations directly via on-chip shared memory. The results from our experimental evaluation demonstrate that the in-situ approach significantly reduces the cost of data movement, that the presented framework can support scalable feature-based object tracking, and that it can be effectively used for in-situ analytics for large scale simulations.	algorithm;anomaly detection;application programming interface;autonomic computing;cluster analysis;common access card;computation;computer simulation;data center;dataspaces;disk staging;end-to-end principle;experiment;ibm notes;input/output;microsoft word for mac;non-volatile random-access memory;pipeline (computing);scalability;shared memory;solid-state drive;tracking system;ut-vpn;video post-processing	Fan Zhang;Solomon Lasluisa;Tong Jin;Ivan Rodero;Hoang Bui;Manish Parashar	2012	2012 SC Companion: High Performance Computing, Networking Storage and Analysis	10.1109/SC.Companion.2012.100	chip;shared memory;parallel computing;real-time computing;feature extraction;computer science;operating system;video tracking;data mining;database;distributed computing;data analysis;data	HPC	-17.719826321416804	54.81378859044213	56175
79ba6510698178486dbd5ee03cbeaac258298744	using data compression in an mpsoc architecture for improving performance	data compression;system on a chip;chip;memory access;compiler optimization;data access;power consumption;compression;mpsoc	"""Multiprocessor-System-on-a-Chip (MPSoC) performance and power consumption are greatly affected by the application data access characteristics. While the way the application is written is critical in shaping the data access pattern, the compiler optimizations employed can also make a significant difference. Considering that cost of off-chip memory accesses is continuously rising (in terms of CPU cycles), minimizing the number and volume of off-chip data traffic in MPSoCs can be very important. This paper addresses this problem by proposing data compression for increasing the effective on-chip storage space in an MPSoC-based environment. A critical issue is to schedule compressions and decompressions intelligently such that they do not conflict with application execution. In particular, one needs to decide which processors should participate in the compression (and decompression) activity at any given point during the course of execution. We propose both """"static"""" and """"dynamic"""" algorithms for this purpose. In the static scheme, the processors are divided into two groups (those performing compression/decompression and those executing the application), and this grouping is maintained throughout the execution of the application. In the dynamic scheme, on the other hand, the execution starts with some grouping but this grouping can change during the course of execution, depending on the dynamic variations in the data access pattern."""	algorithm;central processing unit;computer memory;data access;data compression;mpsoc;multiprocessing;noise shaping;optimizing compiler;system on a chip	Ozcan Ozturk;Mahmut T. Kandemir;Mary Jane Irwin	2005		10.1145/1057661.1057746	data compression;chip;system on a chip;data access;embedded system;parallel computing;real-time computing;telecommunications;computer science;operating system;optimizing compiler;compression	HPC	-5.86376959528956	54.358333645292724	56204
321dfd477ad176b2995a843e38583f4d3375fb0c	light-weight remote communication for high-performance cloud networks	heterogeneous cloud workloads light weight remote communication library high performance cloud networks libripc high performance computing;software libraries;parallel processing;hardware protocols sockets computer architecture libraries semantics ip networks;software libraries cloud computing parallel processing;cloud computing	In this paper, we present early experiences with libRIPC, a light-weight communication library for high-performance cloud networks. Coming cloud networks are expected to be tightly interconnected and to show capabilities formerly reserved to high-performance computing. LibRIPC aims to bring the benefits of such architectures to heterogeneous cloud workloads. LibRIPC was designed for low footprint and easy integration; it supports reconfiguration and mutually untrusted communication partners. LibRIPC offers short and long transmit primitives, which are optimized for control messages and bulk data transfer respectively. Early experiments with a Java-based web server indicate that libRIPC integrates well into typical cloud workloads and brings substantial speedup of at least a factor of three for larger data transfers compared to socket-based TCP/IP communication.	cloud computing;experiment;internet protocol suite;java;server (computing);speedup;supercomputer;web server	Jens Kehne;Marius Hillenbrand;Jan Stoess;Frank Bellosa	2012	2012 IEEE 1st International Conference on Cloud Networking (CLOUDNET)	10.1109/CloudNet.2012.6483669	parallel computing;real-time computing;single-chip cloud computer;cloud computing;computer science;cloud testing;distributed computing	HPC	-10.833025591990353	46.43736058022728	56215
cab37e4ab84312f8428ac2ec183ca3a8dbaa7d37	understanding memory access patterns using the bsc performance tools		Abstract The growing gap between processor and memory speeds has lead to complex memory hierarchies as processors evolve to mitigate such divergence by exploiting the locality of reference. In this direction, the BSC performance analysis tools have been recently extended to provide insight into the application memory accesses by depicting their temporal and spatial characteristics, correlating with the source-code and the achieved performance simultaneously. These extensions rely on the Precise Event-Based Sampling (PEBS) mechanism available in recent Intel processors to capture information regarding the application memory accesses. The sampled information is later combined with the Folding technique to represent a detailed temporal evolution of the memory accesses and in conjunction with the achieved performance and the source-code counterpart. The reports generated by the latter tool help not only application developers but also processor architects to understand better how the application behaves and how the system performs. In this paper, we describe a tighter integration of the sampling mechanism into the monitoring package. We also demonstrate the value of the complete workflow by exploring already optimized state–of–the–art benchmarks, providing detailed insight of their memory access behavior. We have taken advantage of this insight to apply small modifications that improve the applications’ performance.	binary symmetric channel	Harald Servat;Jesús Labarta;Hans-Christian Hoppe;Judit Giménez;Antonio J. Peña	2018	Parallel Computing	10.1016/j.parco.2018.06.007	locality of reference;computer science;theoretical computer science;sampling (statistics);workflow;instrumentation	HPC	-5.630617869408525	46.76149415804259	56227
21c9597f30ca8c8ce1f31528258a9c2d683b393d	path-based next trace prediction	performance loss;path-based prediction;basic predictor;prediction accuracy;predictor yield;multiple branch;trace cache;next trace predictor;path-based next trace prediction;trace sequence;return history stack;predictor performance;next trace prediction;basic unit;multiple branch prediction;accuracy;computer architecture;bandwidth;parallel processing;computer science;history;aliasing;assembly;instruction sets;branch prediction	The trace cache has been proposed as a mechanism for providing increased fetch bandwidth by allowing the processor to fetch across multiple branches in a single cycle. But to date predicting multiple branches per cycle has meant paying a penalty in prediction accuracy. We propose a next trace predictor that treats the traces as basic units and explicitly predicts sequences of traces. The predictor collects histories of trace sequences (paths) and makes predictions based on these histories. The basic predictor is enhanced to a hybrid configuration that reduces performance losses due to cold starts and aliasing in the prediction table. The Return History Stack is introduced to increase predictor performance by saving path history information across procedure call/returns. Overall, the predictor yields about a 26% reduction in misprediction rates when compared with the most aggressive previously proposed, multiple-branch-prediction methods.	aliasing;branch misprediction;branch predictor;cpu cache;interference (communication);kerrison predictor;subroutine;tracing (software)	Quinn Jacobson;Eric Rotenberg;James E. Smith	1997			parallel computing;real-time computing;computer science;theoretical computer science;operating system;instruction set;branch predictor	Arch	-7.25292650444124	51.083580202464915	56235
a9ba6aaba1299fbe0ca8a8239885ea2919fa24da	integrating memory perspective into the bsc performance tools		The growing gap between processor and memory speeds results in complex memory hierarchies as processors evolve to mitigate such differences by taking advantage of locality of reference. In this direction, the BSC performance analysis tools have been recently extended to provide insight relative the application memory accesses depicting their temporal and spatial characteristics, correlating with the source-code and the achieved performance simultaneously. These extensions rely on the Precise Event-Based Sampling (PEBS) mechanism available in recent Intel processors to capture information relative to the application memory accesses. The sampled information is processed with the Folding mechanism to provide a detailed temporal evolution of the memory accesses and in conjunction with the achieved performance and the source-code counterpart. The results obtained from the combination of these tools help application developers to understand better how the application behaves and how the system performs. We demonstrate the value of the complete work-flow by exploring an already optimized state-of-the-art benchmark, providing detailed insight of their memory access behavior.	benchmark (computing);binary symmetric channel;central processing unit;locality of reference;memory hierarchy	Harald Servat;Jesús Labarta;Hans-Christian Hoppe;Judit Giménez;Antonio J. Peña	2017	2017 46th International Conference on Parallel Processing Workshops (ICPPW)	10.1109/ICPPW.2017.42	distributed computing;parallel computing;theoretical computer science;computer science;real-time computing;benchmark (computing);uniform memory access;parallel processing;computing with memory;locality of reference	HPC	-5.675493806944794	46.78011348339057	56334
bccd869e600a79bdedd92d84a86ea85f5aad3f6e	smt-aware instantaneous footprint optimization	instantaneous footprint;locality;performance tools;smt aware optimization;smt;memory hierarchy	Modern architectures employ simultaneous multithreading (SMT) to increase thread-level parallelism. SMT threads share many functional units and the entire memory hierarchy of a physical core. Without a careful code design, SMT threads can easily contend with each other for these shared resources, causing severe performance degradation. Minimizing SMT thread contention for HPC applications running on dedicated platforms is very challenging because they typically spawn threads within Single Program Multiple Data (SPMD) models. Since these threads have similar resource requirements, their contention cannot be easily mitigated through simple thread scheduling. To address this important issue, we first vigorously conduct a systematic performance evaluation on a wide-range of representative HPC and CMP applications on three mainstream SMT architectures, and quantify their performance sensitivity to SMT effects. Then we introduce a simple scheme for SMT-aware code optimization which aims to reduce the memory contention across SMT threads. Finally, we develop a lightweight performance tool, named SMTAnalyzer, to effectively identify the optimization opportunities in the source code of multithreaded programs. Experiments on three SMT architectures (i.e., Intel Xeon, IBM POWER7, and Intel Xeon Phi) demonstrate that our proposed SMT-aware optimization scheme can significantly improve the performance for general HPC applications.	elegant degradation;mathematical optimization;memory hierarchy;multithreading (computer architecture);parallel computing;performance evaluation;program optimization;requirement;spmd;scheduling (computing);simultaneous multithreading;spawn (computing);task parallelism;thread (computing);thread pool;xeon phi	Probir Roy;Xu Liu;Shuaiwen Song	2016		10.1145/2907294.2907308	parallel computing;real-time computing;computer science;operating system	HPC	-7.5284496599154895	48.58805577392615	56514
da11198b1c6e497b0dd71f4e9fff1cc7e4559342	energy efficient scheduling for hard real-time systems with fixed-priority assignment	voltage control;processor shutdown strategy;energy efficient;real time;voltage scaling energy efficient scheduling hard real time systems fixed priority assignment energy consumption reduction processor shutdown strategy;fixed priority assignment;scheduling power aware computing real time systems;fixed priority;energy consumption reduction;power aware computing;realtime system;voltage control energy consumption schedules power demand real time systems equations heuristic algorithms;hard real time system;energy consumption;scheduling;heuristic algorithms;energy efficient scheduling;schedules;voltage scaling;power demand;lower bound;hard real time systems;real time systems	In this paper, we study the problem of reducing the energy consumption for hard real-time systems based on fixed-priority (FP) scheme. To balance the static and dynamic energy consumption, the concept of critical speed was proposed. Moreover, when combined with the processor shutdown strategy, the critical speed was widely used as the lower bound for voltage scaling in literature. In this paper, we show that the critical speed strategy might not always be more energy efficient than the traditional DVS strategy and there is a tradeoff between these two strategies. Based on it, we propose an off-line approach to set up the energy efficient static speed schedule for real-time tasks. The simulation results demonstrate that our proposed techniques can effectively reduce the energy consumption for hard realtime systems.	algorithm;cmos;central processing unit;dynamic voltage scaling;energy minimization;image scaling;online and offline;p (complexity);real-time clock;real-time computing;real-time transcription;scheduling (computing);shutdown (computing);simulation;spectral leakage	Linwei Niu	2010	International Performance Computing and Communications Conference	10.1109/PCCC.2010.5682315	embedded system;parallel computing;real-time computing;schedule;computer science;efficient energy use;upper and lower bounds;scheduling	Embedded	-5.3680201187959335	58.65368156398941	56593
3eca563549a9ee1c802e888e8a143a706a8afe5d	thermal-aware dynamic page allocation policy by future access patterns for hybrid memory cube (hmc)		The Hybrid Memory Cube (HMC) is a promising solution to overcome memory wall by stacking DRAM chips on top of a logic die and connecting them with dense and fast Through Silicon Vias (TSVs). However, 3D stacking technique brings another problem: high temperature and temperature variations between the DRAM dies. The thermal problem may lead to chip failure of 3D stacked DRAMs since the temperature may exceed the maximum operating temperature. Dynamic thermal management (DTM) scheme such as bandwidth throttling can effectively decrease the temperature. However, it results in the loss of the performance. To maximize the performance of the system with HMC, the appropriate memory mapping should consider the thermal characteristics of HMC, memory interference and bandwidth variations among processes, and current temperature conditions of each memory channel. This paper proposes a thermal-aware dynamic OS page allocation using future access pattern to find a best performance-oriented setting of the above factors. An analytical model has been proposed to estimate the system performance considering the memory interference, the bandwidth variation, and the throttling impact. Our method can improve the system performance by 12.7% compared to best performance-oriented allocation method (MCP) [1]. The average error rate of our analytical model to predict the trend of performance variations is only 0.86%.	dynamic random-access memory;hybrid memory cube;interference (communication);memory management;memory-mapped i/o;norm (social);operating system;run time (program lifecycle phase);stacking;thermal management of high-power leds;three-dimensional integrated circuit	Wei-Hen Lo;Kai-zen Liang;TingTing Hwang	2016	2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;interleaved memory;electronic engineering;semiconductor memory;parallel computing;real-time computing;memory rank;memory refresh;dynamic priority scheduling;computer science;engineering;resource management;operating system;static memory allocation;interference;memory controller;registered memory;bandwidth	EDA	-7.2784108377932535	55.226032868235116	56717
69b8c7f168fdb610471c473b9c2f20daaffe052c	energy- and endurance-aware design of phase change memory caches	pcm cache;cache storage;sram cache;high immunity;pcm cache simulator;phase change memories;sram chips;energy saving;endurance-aware design;endurance-aware pcm cache;baseline pcm cache;phase change memory caches;high density;pcm cache design;energy-aware design;phase change memory cache;nonvolatile random access memory technologies;cache memory;power optimization;crystallization;computer architecture;nonvolatile memory;energy efficient;cloud computing;datacenter;phase change memory	Phase change memory (PCM) is one of the most promising technology among emerging non-volatile random access memory technologies. Implementing a cache memory using PCM provides many benefits such as high density, non-volatility, low leakage power, and high immunity to soft error. However, its disadvantages such as high write latency, high write energy, and limited write endurance prevent it from being used as a drop-in replacement of an SRAM cache. In this paper, we study a set of techniques to design an energy- and endurance-aware PCM cache. We also modeled the timing, energy, endurance, and area of PCM caches and integrated them into a PCM cache simulator to evaluate the techniques. Experiments show that our PCM cache design can achieve 8% of energy saving and 3.8 years of lifetime compared with a baseline PCM cache having less than a hour of lifetime.	baseline (configuration management);cpu cache;drop-in replacement;non-volatile memory;non-volatile random-access memory;overhead (computing);phase-change memory;random access;soft error;spectral leakage;static random-access memory;volatility	Yongsoo Joo;Dimin Niu;Xiangyu Dong;Guangyu Sun;Naehyuck Chang;Yuan Xie	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)		data center;parallel computing;real-time computing;cache coloring;non-volatile memory;cpu cache;phase-change memory;cloud computing;computer hardware;computer science;operating system;efficient energy use;crystallization;cache algorithms;cache pollution;power optimization	Arch	-7.465842798763594	55.16856763646395	56748
8322a4b96423e9f2c966f6476dbb9d1a53d588e8	reconciling the tension between hardware isolation and data sharing in mixed-criticality, multicore systems		Recent work involving a mixed-criticality framework called MC2 has shown that, by combining hardware-management techniques and criticality-aware task provisioning, capacity loss can be significantly reduced when supporting real-time workloads on multicore platforms. However, as in most other prior research on multicore hardware management, tasks were assumed in that work to not share data. Data sharing is problematic in the context of hardware management because it can violate the isolation properties hardware-management techniques seek to ensure. Clearly, for research on such techniques to have any practical impact, data sharing must be permitted. Towards this goal, this paper presents a new version of MC2 that permits tasks to share data within and across criticality levels through shared memory. Several techniques are presented for mitigating capacity loss due to data sharing. The effectiveness of these techniques is demonstrated by means of a large-scale, overhead-aware schedulability study driven by micro-benchmark data.	benchmark (computing);best, worst and average case;capacity loss;computer cluster;criticality matrix;emoticon;library (computing);mixed criticality;multi-core processor;optical burst switching;overhead (computing);provisioning;real-time clock;sbp;scheduling (computing);self-organized criticality;shared memory;xfig	Micaiah Chisholm;Namhoon Kim;Bryan C. Ward;Nathan Otterness;James H. Anderson;F. Donelson Smith	2016	2016 IEEE Real-Time Systems Symposium (RTSS)	10.1109/RTSS.2016.015	embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing	Embedded	-9.658369404933781	56.59332262208692	56854
066f37e61624a168b075fcd9ad437b76d7f393dd	scheduling instructions on hierarchical machines	graph theory;distributed system;random graph;processor scheduling clustering algorithms scheduling algorithm approximation algorithms algorithm design and analysis delay systems partitioning algorithms iterative algorithms warranties convergence;scheduling instructions;approximate algorithm;scheduling graph theory multiprocessing systems;approximation algorithms;processor scheduling;performance of systems;scheduling fine grain task graphs;communication delay scheduling instructions hierarchical machines scheduling fine grain task graphs hierarchical distributed systems random graphs structured graphs;random graphs;structured graphs;scheduling;number of clusters;schedules;clustering algorithms;list scheduling;communication delay;hierarchical distributed systems;optimization;approximation methods;multiprocessing systems;task graphs;hierarchical machines	The aim of this work is to study the problem of scheduling fine grain task graphs on hierarchical distributed systems with communication delay. We consider as a case study how to schedule the instructions on a processor that implements incomplete bypass ( ST200). We show first how this problem can be expressed as scheduling unitary tasks on a hierarchical architecture with heavy communications between clustered units. The proposed analysis is generic and can be extended to other challenging problems like scheduling in clusters of multi-cores. Our main result is an approximation algorithm based on list scheduling whose approximation ratio is the minimum of two expressions, the first one depends on the number of clusters while the second one depends on the communication delay. Experiments run on random graphs and on structured graphs demonstrate the effectiveness of the proposed approach.	approximation algorithm;central processing unit;distributed computing;experiment;list scheduling;locality of reference;multi-core processor;multiprocessing;parallel computing;random graph;st200 family;schedule (project management);scheduling (computing)	Florent Blachot;Guillaume Huard;Johnatan E. Pecero;Erik Saule;Denis Trystram	2010	2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)	10.1109/IPDPSW.2010.5470711	fair-share scheduling;fixed-priority pre-emptive scheduling;open-shop scheduling;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;two-level scheduling;distributed computing;round-robin scheduling	Arch	-13.576132688767718	60.37206450757906	56875
957ae212c16ea9a70a53d1143e0f8a908a496648	performance of greedy garbage collection in flash-based solid-state drives	markov chain model;performance guarantee;negative affect;stochastic modeling;theoretical model;write amplification;log structured systems;garbage collection;upper bound;solid state drive;log structured file system;ssd;stochastic model;lower bound	In flash-based solid-state drives (SSD) and log-structured file systems, new data is written out-of-place, which over time exhausts the available free space. New free space is created by the garbage-collection process, which reclaims the space occupied by invalidated data. The write amplification, incurred because of the additional write operations performed by the garbage-collectionmechanism is a critical factor that negatively affects the lifetime and endurance of SSDs. We develop two complementary theoretical models of the SSD operation for uniformly-distributed random small user writes: a Markov chain model, which is useful to explore the performance characteristics of small and medium-sized systems, and a secondmodel that captures the behavior of large systems. The combination of bothmodels allows us to comprehensively characterize the system operation and behavior. Results of theoretical and practical importance are analytically derived and confirmed bymeans of simulation. Our results demonstrate that (i) as the system occupancy increases, the write amplification increases; (ii) as the number of blocks increases, the write amplification decreases and approaches a lower bound; and (iii) as the number of pages contained in a block increases, the write amplification increases and approaches an upper bound. They also show that, for large systems, the number of free pages reclaimed by the greedy garbagecollection mechanism after each block recycling takes one of two successive values, which provides a quasi-deterministic performance guarantee. © 2010 Elsevier B.V. All rights reserved.	amplifier;computer data storage;dependability;garbage collection (computer science);greedy algorithm;log-structured file system;markov chain;next-generation network;simulation;solid-state drive	Werner Bux;Ilias Iliadis	2010	Perform. Eval.	10.1016/j.peva.2010.07.003	real-time computing;simulation;computer hardware;computer science;mathematics;upper and lower bounds;statistics	OS	-11.716303148701401	56.22418804173741	56955
07c6f0736588c419e10ba9242a1e1e7970dd6137	relaxreplay: record and replay for relaxed-consistency multiprocessors	memory race recording;relaxed consistency;record and deterministic replay	Record and Deterministic Replay (RnR) of multithreaded programs on relaxed-consistency multiprocessors has been a long-standing problem. While there are designs that work for Total Store Ordering (TSO), finding a general solution that is able to record the access reordering allowed by any relaxed-consistency model has proved challenging. This paper presents the first complete solution for hard-ware-assisted memory race recording that works for any relaxed-consistency model of current processors. With the scheme, called RelaxReplay, we can build an RnR system for any relaxed-consistency model and coherence protocol. RelaxReplay's core innovation is a new way of capturing memory access reordering. Each memory instruction goes through a post-completion in-order counting step that detects any reordering, and efficiently records it. We evaluate RelaxReplay with simulations of an 8-core release-consistent multicore running SPLASH-2 programs. We observe that RelaxReplay induces negligible overhead during recording. In addition, the average size of the log produced is comparable to the log sizes reported for existing solutions, and still very small compared to the memory bandwidth of modern machines. Finally, deterministic replay is efficient and needs minimal hardware support.	cache coherence;central processing unit;consistency model;memory bandwidth;multi-core processor;overhead (computing);simulation;thread (computing);warez	Nima Honarmand;Josep Torrellas	2014		10.1145/2541940.2541979	computer architecture;parallel computing;real-time computing;computer science;theoretical computer science;operating system;programming language	Arch	-10.53635778794093	50.15351749485305	57004
f05c0b8a62bf24386be9cfb84767baab2f34c0dd	hub: hugepage ballooning in kernel-based virtual machines		Modern applications running on cloud data centers often consume a large amount of memory and their memory demands can vary during execution. Dynamic memory allocation is a necessity for high memory utilization. For a large dataset application, using hugepages instead of regular 4KB pages can efficiently reduce memory access and memory management overhead and improve overall performance. Virtualization, which is widely applied in data centers for server consolidation, brings new challenges to manage memory dynamically and effectively, especially for hugepages. In a virtualized system, ballooning is a popular mechanism used to dynamically adjust memory allocations for co-located virtual machines. We observe that the current Linux Kernel-Based Virtual Machine (KVM) does not support huge page ballooning. An application that can benefit from hugepages often loses its performance advantage when the guest OS experiences memory ballooning. This paper presents design and implementation of HUB, a HUgepage Ballooning mechanism in KVM which can dispatch memory in the granularity of hugepages. The experimental results show that our approach significantly reduces TLB misses and improves the overall performance for those applications with large memory demand.		Jingyuan Hu;Xiaokuang Bai;Sai Sha;Yingwei Luo;Xiaolin Wang;Zhenlin Wang	2018		10.1145/3240302.3240420	virtualization;memory management;operating system;cloud computing;high memory;translation lookaside buffer;linux kernel;c dynamic memory allocation;virtual machine;computer science	OS	-15.767787319614758	55.351569562857236	57135
282292a7f5c6763f4228b27358346a12cd47c0c0	adaptive fair resource allocation for energy and qos trade-off management	tecnologia electronica telecomunicaciones;optimal resource allocation;time complexity;resource allocation;dynamic voltage scaling;real time embedded system;energy consumption;tecnologias;quality of service;grupo a;nonlinear optimization;real time systems	In real-time embedded systems, there is requirement for adapting both energy consumption and Quality of Services (QoS) of tasks according to their importance. This paper proposes an adaptive power-aware resource allocation method to resolve a trade-off between the energy consumption and QoS levels according to their importance with guaranteeing fairness. The proposed resource allocator consists of two components: the total resource optimizer to search for the optimal total resource and QoS-fairness-based allocator to allocate resource to tasks guaranteeing the fairness. These components adaptively achieve the optimal resource allocation formulated by a nonlinear optimization problem with the time complexity O(n) for the number of tasks n even if tasks' characteristics cannot be identified precisely. The simulation result shows that the rapidness of the convergence of the resource allocation to the optimal one is suitable for real-time systems with large number of tasks.	quality of service	Fumiko Harada;Toshimitsu Ushio;Yukikazu Nakamoto	2008	IEICE Transactions	10.1093/ietfec/e91-a.11.3245	time complexity;real-time computing;quality of service;nonlinear programming;resource allocation;computer science;distributed computing;algorithm	Networks	-5.877042968483	59.71069689411493	57148
929f0e9d161389d9e28d465e5a9359d46017fd1c	anticipatory access pipeline design for phased cache	cache storage;one access cycle cache anticipatory access pipeline design phased cache embedded processor power consumption external memory access set associative cache embedded pipelining processor;external memory access;phased cache;embedded pipelining processor;cache memory;set associative cache;pipeline processing cache storage embedded systems;chip;embedded systems;anticipatory access pipeline design;energy consumption pipeline processing process design microprocessors control engineering computer science cache memory phase detection filters costs;one access cycle cache;low power;low power design;external memory;power consumption;high performance;embedded processor;article;pipeline processing	For an embedded processor, the cache design almost occupies half chip area and power consumption. According to Amdahl's law, if the power consumption of cache memories is reduced, the embedded processor can significantly save much power. However, the cache misses result in the penalty of thousands of cycles waiting and power consumption due to increasing the number of external memory access. Based on the above reason, the phased cache design is proposed and can largely improve the power consumption which wastes a set-associative cache. In this paper, the embedded pipelining processor without stalling and low-power phase cache is practiced with high-level simulation to achieve high-performance and low-power design. As experimental results, the proposed phase cache can reduce 44% power consumption compared with traditional one-access-cycle cache and eliminate pipeline stalls incurred by phased cache with only 6% gate count overhead.	amdahl's law;benchmark (computing);cpu cache;embedded system;gate count;high- and low-level;instruction pipelining;low-power broadcasting;overhead (computing);pipeline (computing);simulation	Chih-Wen Hsueh;Jen-Feng Chung;Lan-Da Van;Chin-Teng Lin	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4541924	chip;bus sniffing;embedded system;least frequently used;pipeline burst cache;computer architecture;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;uncore;smart cache;cache algorithms;cache pollution;non-uniform memory access	EDA	-7.20396335925476	53.44911709206708	57370
17810349765c08963af130efe28b6a6b77b7ec51	a top-down method for performance analysis and counters architecture	software architecture performance evaluation;pipelines radiation detectors out of order electric breakdown measurement bandwidth microarchitecture;super scalar cores top down method performance analysis microarchitecture complexity workload diversity data volume performance counters architecture out of order processor performance events	Optimizing an application's performance for a given microarchitecture has become painfully difficult. Increasing microarchitecture complexity, workload diversity, and the unmanageable volume of data produced by performance tools increase the optimization challenges. At the same time resource and time constraints get tougher with recently emerged segments. This further calls for accurate and prompt analysis methods. The insights from this method guide a proposal for a novel performance counters architecture that can determine the true bottlenecks of a general out-of-order processor. Unlike other approaches, our analysis method is low-cost and already featured in in-production systems - it requires just eight simple new performance events to be added to a traditional PMU. It is comprehensive - no restriction to predefined set of performance issues. It accounts for granular bottlenecks in super-scalar cores, missed by earlier approaches.	bottleneck (engineering);bottleneck (software);central processing unit;enterprise software;general-purpose markup language;hyper-threading;long tail;mathematical optimization;microarchitecture;multi-core processor;optimizing compiler;power management unit;profiling (computer programming);sandy bridge;scalability;server (computing);software developer;speedup;thread (computing);vish (game)	Ahmad Yasin	2014	2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)	10.1109/ISPASS.2014.6844459	computer architecture;parallel computing;real-time computing;computer hardware;computer science;operating system	Arch	-5.187590933673127	54.216133867272625	57471
e75047604a0fd016784944f459c214e52bec344a	method of adaptive controllable parallel execution of operations in object database management systems	database management system;system design;legacy software;management system	The design of parallel database management systems (DBMSs) normally implies using special-purpose multiprocessor computing systems. Most often, a DBMS is supposed to work in an exclusive mode of operation. However, in the class of x86-based multiprocessor computing systems designed for mass usage, the exclusive mode of DBMS operation with respect to other software is often not secured. In addition, the legacy software for this class of computing systems is often not designed for mass parallel usage. When the exclusive mode requirement is ignored and the resources of the computing system are not used in low-load DBMS regimes, the efficiency of using resources of the computing system as a whole reduces. This paper considers a method of program organization of controlled parallelism at the level of internal DBMS operations, allowing for their controlled execution based on the state of the entire computing system. This method made it possible to significantly reduce the time of response for low densities of query arrival in the ODB-Jupiter commercial object DBMS developed in the Inteltec Plus scientific manufacturing center. The method of controlled parallel execution can be used in a wide class of program systems.	database	R. S. Samarev	2008	Programming and Computer Software	10.1007/s11086-008-1002-6	sargable;real-time computing;computer science;management system;database;distributed computing;programming language;legacy system;systems design	DB	-17.943060189442605	53.26815917837218	57553
5c30b11ae6eb7f5ec233021f088685c9c266f362	"""is automatic """"folding"""" of programs efficient enough to displace manual?"""	folding;measurement;performance;storage hierarchies;paging;memory hierarchies;automatic folding;automatic paging;memory hierarchy;replacement algorithms;demand paging	The operation of “folding” a program into the available memory is discussed. Measurements by Brawn et al. and by Nelson on an automatic folding mechanism of simple design, a demand paging unit built at the IBM Research Center by Belady, Nelson, O'Neill, and others, permitting its quality to be compared with that of manual folding, are discussed, and it is shown that given some care in use the unit performs satisfactorily under the conditions tested, even though it is operating across a memory-to-storage interface with a very large speed difference. The disadvantages of prefolding, which is required when the folding is manual, are examined, and a number of the important troubles which beset computing today are shown to arise from, or be aggravated by, this source. It is concluded that a folding mechanism will probably become a normal part of most computing systems.	compiler;ibm research;lászló bélády;overhead (computing);paging	D. Sayre	1969	Commun. ACM	10.1145/363626.363629	demand paging;parallel computing;real-time computing;performance;computer science;theoretical computer science;folding;operating system;algorithm;measurement;paging	HPC	-16.77086686005298	49.71812984512779	57620
75d2ac1093ce33b2f6329f8a198671c52224341e	smcv: a methodology for detecting transient faults in multicore clusters		The challenge of improving the performance of curre nt processors is achieved by increasing the integration scale. This carries a growing vulnerabi lity to transient faults, which increase their impact on multicore clusters running large scientif ic parallel applications. The requirement for enhancing the reliability of these systems, coupled with the high cost of rerunning the application from the beginning, create the motivation for havin g specific software strategies for the target systems. This paper introduces SMCV, which is a ful ly distributed technique that provides fault detection for message-passing parallel applications , by validating the contents of the messages to be sent, preventing the transmission of errors to o ther processes and leveraging the intrinsic hardware redundancy of the multicore. SMCV achieves a wide robustness against transient faults with a reduced overhead, and accomplishes a trade-o ff between moderate detection latency and low additional workload.	central processing unit;computer program;fault detection and isolation;high-level programming language;message passing;multi-core processor;overhead (computing);redundancy (engineering);run time (program lifecycle phase)	Diego Montezanti;Fernando Emmanuel Frati;Dolores Rexachs;Emilio Luque;Marcelo R. Naiouf;Armando De Giusti	2012	CLEI Electron. J.		simulation;computer science;theoretical computer science;software engineering;computer engineering	HPC	-17.42790049391713	48.825703804975824	57735
2ed7c61ec0ec862b8315d77b48442757b6a68d87	job communication characterization and its impact on meta-scheduling co-allocated jobs in a mini-grid	inter cluster network utilization;time varying;degradation;concurrent computing;processor scheduling;resource allocation;dynamic model;distributed computing;meta scheduling mini grid bandwidth centric parallel job communication model inter cluster network utilization co allocated jobs;co allocated jobs;mini grid;workstation clusters resource allocation parallel processing grid computing processor scheduling;communication model;processor scheduling workstation clusters resource allocation parallel processing grid computing;grid computing bandwidth concurrent computing predictive models laboratories processor scheduling distributed computing parallel architectures computer science education degradation;computer science education;parallel architectures;meta scheduling;co allocated jobs meta scheduling mini grid bandwidth centric parallel job communication model inter cluster network utilization;number of clusters;bandwidth centric parallel job communication model;bandwidth;predictive models;workstation clusters;grid computing;parallel processing	Summary form only given. We present a bandwidth-centric parallel job communication model that takes into account inter-cluster network utilization as a means by which to capture the interaction and impact of simultaneously co-allocated jobs in a mini-grid. Our model captures the time-varying utilization of shared inter-cluster network resources in the grid. We compare our dynamic model with previous research that utilizes a fixed execution time penalty for co-allocated jobs. We have found that the fixed penalty model is more generous in its prediction of job turnaround time than our dynamic communication model. Additionally, we see that the penalty co-allocated jobs may experience without causing a severe performance degradation decreases as the number of clusters increases.	algorithm;elegant degradation;job scheduler;job stream;mathematical model;meta-scheduling;overhead (computing);run time (program lifecycle phase);scheduling (computing);time-varying network	William M. Jones;Louis W. Pang;Daniel C. Stanzione;Walter B. Ligon	2004	18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.	10.1109/IPDPS.2004.1303317	parallel processing;parallel computing;real-time computing;models of communication;degradation;concurrent computing;resource allocation;computer science;operating system;distributed computing;predictive modelling;bandwidth;grid computing	HPC	-16.451037352641958	58.86899757360089	57795
a75a713d73f045fa831bbec62abd1888681b54b4	helix: the architecture of a distributed file system	distributed file system		clustered file system;dce distributed file system	Marek Fridrich;William J. Older	1984			distributed file system;device file;replication (computing);operating system;network file system;architecture;unix file types;computer science;file area network;self-certifying file system	OS	-19.001007591825363	51.89813629959739	57812
16fab9bb8442d3d675cd6fe239c9ed834c60a4ed	exploring void search for fault detection on extreme scale systems	computers;reliability;lead;environmental data;lead computers;fault detection;environmental data reliability void search fault detection blue gene q;mira blue gene q supercomputer fault detection extreme scale systems mean time between failures mtbf resilience technologies hardware component software component fault tolerance technologies checkpointing failure prediction void search algorithm vs algorithm astrophysics galaxies;search problems checkpointing fault tolerant computing;blue gene q;void search	Mean Time Between Failures (MTBF), now calculated in days or hours, is expected to drop to minutes on exascale machines. The advancement of resilience technologies greatly depends on a deeper understanding of faults arising from hardware and software components. This understanding has the potential to help us build better fault tolerance technologies. For instance, it has been proved that combining checkpointing and failure prediction leads to longer checkpoint intervals, which in turn leads to fewer total checkpoints. In this paper we present a new approach for fault detection based on the Void Search (VS) algorithm. VS is used primarily in astrophysics for finding areas of space that have a very low density of galaxies. We evaluate our algorithm using real environmental logs from Mira Blue Gene/Q supercomputer at Argonne National Laboratory. Our experiments show that our approach can detect almost all faults (i.e., sensitivity close to 1) with a low false positive rate (i.e., specificity values above 0.7). We also compare our algorithm with a number of existing detection algorithms, and find that ours outperforms all of them.	algorithm;anomaly detection;application checkpointing;blue gene;component-based software engineering;computer hardware;experiment;fault detection and isolation;fault tolerance;graphics processing unit;hardware acceleration;ibm websphere extreme scale;independent computing architecture;mean time between failures;parallel computing;petascale computing;sensitivity and specificity;supercomputer	Eduardo Berrocal;Li Yu;Sean Wallace;Michael E. Papka;Zhiling Lan	2014	2014 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2014.6968757	lead;parallel computing;real-time computing;computer science;theoretical computer science;operating system;reliability;fault detection and isolation;statistics	HPC	-4.86504194586297	48.17114672489219	57884
e206a258d32d36a8ab43d630b820460be74c2277	a utility-maximizing tasks assignment method for rendering cluster system	random access memory;resource management;render farm;tasks assignment;utility maximizing;resource allocation rendering computer graphics;load management;load balancing;fixed thread method utility maximizing task assignment method cluster system rendering task centered assignment method naive load balancing strategy resource usage fine grain computing units frame to frame coherence static load balancing strategy naive method;coherence;rendering computer graphics;rendering computer graphics instruction sets load management load modeling resource management random access memory coherence;load modeling;cluster rendering;instruction sets;utility maximizing tasks assignment cluster rendering render farm load balancing	A utility-maximizing tasks assignment method for rendering cluster system based on feedback is proposed to solve the problem that traditional task-centered assignment method and naïve load balancing strategy cannot make full use of resources. The method uses feedback on resource usage to choose an appropriate number of threads for the renderer and then divides computing nodes of the rendering cluster system into fine grain computing units. After that, the method takes advantage of frame-to-frame coherence to assign tasks to computing units with a new static load balancing strategy. Experiments on two scene models with different complexity and comparisons with the naïve method and the fixed-threads method show that the utility-maximizing method not only reduces the rendering time of a rendering job, but also balances the load between computing units. The scalability of the proposed method is also verified on different number of computing nodes.	algorithm;earliest deadline first scheduling;feedback;load balancing (computing);naivety;requirement;scalability;scheduling (computing);structural load;vii	Jianhang Huang;Weiguo Wu;Qian Li	2014	2014 IEEE International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2014.19	parallel computing;real-time computing;render farm;coherence;computer hardware;computer science;load balancing;resource management;operating system;instruction set;distributed computing;computer security	HPC	-16.15024441394678	58.26101704109884	58039
f8acd6f6b4aa82302d284cf9b0abd5ee386ec97a	write buffer-oriented energy reduction in the l1 data cache of two-level caches for the embedded system	cache memory;embedded system;chip;write buffer;data cache;low power;energy consumption;point of view	In resource-constrained embedded systems, on-chip cache memories play an important role in both performance and energy consumption points of view. In contrast with read operations, little effort has been made to write operations though write energy consumption in the data cache constitutes a large portion of total energy consumption. To this end, this paper proposes write buffer-oriented energy reduction schemes in the L1 data cache of two-level cache architecture. First, the write operations update only the write buffer but not the L1 data cache which is updated later by the write buffer after the write operations are merged. Write merging significantly reduces write accesses to the data cache and, consequently, energy consumption. Second, many write operations from the write buffer to the L1 data cache do not require tag matching by recording way numbers of the L1 data cache in the write buffer. The two optimizations in combination are shown to reduce write energy consumption by 77% in the L1 data cache and total L1 data cache energy consumption by 27%.	cpu cache;embedded system;write buffer	Soontae Kim;Jongmin Lee	2010		10.1145/1785481.1785542	chip;bus sniffing;embedded system;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;computer hardware;telecommunications;cache;computer science;write-once;cache invalidation;write buffer;smart cache;write combining;mesi protocol;cache algorithms;cache pollution	Embedded	-9.300874731286427	54.73487411661187	58069
65883bf4e5267467a5db1547976153f820b71fb4	globally homogeneous, locally adaptive sparse matrix-vector multiplication on the gpu		The rising popularity of the graphics processing unit (GPU) across various numerical computing applications triggered a breakneck race to optimize key numerical kernels and in particular, the sparse matrix-vector product (SpMV). Despite great strides, most existing GPU-SpMV approaches trade off one aspect of performance against another. They either require preprocessing, exhibit inconsistent behavior, lead to execution divergence, suffer load imbalance or induce detrimental memory access patterns. In this paper, we present an uncompromising approach for SpMV on the GPU. Our approach requires no separate preprocessing or knowledge of the matrix structure and works directly on the standard compressed sparse rows (CSR) data format. From a global perspective, it exhibits a homogeneous behavior reflected in efficient memory access patterns and steady per-thread workload. From a local perspective, it avoids heterogeneous execution paths by adapting its behavior to the work load at hand, it uses an efficient encoding to keep temporary data requirements for on-chip memory low, and leads to divergence-free execution. We evaluate our approach on more than 2500 matrices comparing to vendor provided, and state-of-the-art SpMV implementations. Our approach not only significantly outperforms approaches directly operating on the CSR format ( 20% average performance increase), but also outperforms approaches that preprocess the matrix even when preprocessing time is discarded. Additionally, the same strategies lead to significant performance increase when adapted for transpose SpMV.	best, worst and average case;computer graphics;graphics processing unit;matrix multiplication;numerical analysis;preprocessor;requirement;sparse matrix;the matrix	Markus Steinberger;Rhaleb Zayer;Hans-Peter Seidel	2017		10.1145/3079079.3079086	workload;transpose;parallel computing;real-time computing;computer science;matrix (mathematics);graphics processing unit;sparse matrix;trade-off;preprocessor;sparse matrix-vector multiplication	HPC	-5.446443947897734	49.71137971006531	58159
62b70fb5d21414d202f3e4415052bbe9222f9d87	adopting system call based address translation into user-level communication	cache storage;cache based approach;network interface controller;kernel application software network interfaces delay costs hardware protocols electronic mail communication system software control systems;software overhead;network interfaces;low latency;cluster system;parallel computer;cache based approach system call based address translation user level communication software overhead network interface controller;operating system kernels cache storage network interfaces;communication protocol;user level communication;operating system kernels;network interface;system call based address translation;high performance;cache memories	User-level communication alleviates the software overhead of the communication subsystem by allowing applications to access the network interface directly. For that purpose, efficient address translation of virtual address to physical address is critical. In this study, we propose a system call based address translation scheme where every translation is done by the kernel instead of a translation cache on a network interface controller as in the previous cache based address translation. According to our experiments, our scheme achieves up to 4.5 % reduction in application execution time compared to the previous cache based approach	cpu cache;cache (computing);experiment;hit (internet);infiniband;information;kernel (operating system);microprocessor;network interface controller;overhead (computing);physical address;run time (program lifecycle phase);syntax-directed translation;system call;user space	Moon-Sang Lee;Sang-Kwon Lee;Joonwon Lee;Seung Ryoul Maeng	2006	IEEE Computer Architecture Letters	10.1109/L-CA.2006.2	computer architecture;parallel computing;real-time computing;computer science;network interface;operating system;translation lookaside buffer;logical address;network address translation;link-local address;address space;computer network	Arch	-12.543249145210142	49.22663852939338	58219
6fb66f38ec6226c10fcc0f432905f5fcf7244ac2	reducing the “tax” of reliability: a hardware-aware method for agile data persistence in mobile devices		Nowadays, mobile devices are pervasively used by almost everyone. The majority of mobile devices use embedded-Multi Media Cards (eMMC) as storage. However, the crash-proof mechanism of existing I/O stack has not fully exploited the features of eMMC. In some real usage scenarios, the legacy data persistence procedure may dramatically degrade performance of the system. In response to this, this paper exploits the hardware features of eMMC to improve the efficiency of data persistence while preserving the reliability of current mobile systems. We characterize the existing data persistence scheme and observe that the hardware-agnostic design generates excessive non-critical data and adds expensive barriers in data persistence paths. We alleviate these overheads by leveraging eMMC features. Based on evaluations on real systems, our optimizations achieve 5%-31% performance improvement across a wide range of mobile apps.	agile software development;disk buffer;embedded system;input/output;mathematical optimization;mobile app;mobile device;persistence (computer science);persistent data structure	Meng Wang;Huixiang Chen;Tao Li	2017	2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)	10.1109/DSN.2017.46	persistent data structure;real-time computing;agile software development;performance improvement;computer science;distributed computing;computer hardware;overhead (business);exploit;mobile device	Embedded	-12.042738243159837	55.5076038134273	58286
397d2ec83d365011df4757a9fdd558048cb1a473	delft-java link translation buffer	object oriented methods;programming language;perforation;application program interfaces buffer storage instruction sets object oriented methods object oriented programming object oriented languages software performance evaluation digital storage;software performance evaluation;buffer storage;instruction set architecture;object oriented programming;dynamic linking;application program interfaces;c programming language;digital storage;object oriented languages;java computer languages joining processes program processors runtime delay virtual machining hardware libraries;instruction sets;frequently utilized methods link translation buffer delft java processor hardware support dynamic linking java programming language method invocation dynamically linked classes instruction set architecture architecturally transparent buffer performance c programming language caching	We describe the hardware support in the DELFT-JAVA processor which enables efficient dynamic linking of JAVA programs. The proposed mechanism supports method invocation of dynamically linked classes through the use of a Link Translation Buffer (LTB). Since our Instruction Set Architecture directly supports dynamically linked method invocation, the Link Translation Buffer is architecturally transparent to the executing program. The operation of the LTB is described and preliminary performance results are reported. Method invocation differences between the C++ programming language and the JAVA programming language are outlined. Preliminary performance results for the Link Translation Buffer suggest that program performance may improve from 1.1x to 1.5x when a suitable LTB is used to cache frequently utilized methods.	16-bit;compiler;dispatch table;dynamic dispatch;dynamic linker;encode;elegant degradation;high- and low-level;java processor;late binding;microarchitecture;multithreading (computer architecture);opcode;subroutine;the c++ programming language;thread (computing)	C. John Glossner;Stamatis Vassiliadis	1998		10.1109/EURMIC.1998.711804	parallel computing;real-time computing;computer science;programming language	Arch	-6.9985240179164405	49.85227181065256	58413
648817b9ff83d2be1c2ebcc45ac70f34b929972f	locality issues in a fine-grained parallel machine	locality mechanisms;global address space;role of simulation;functional languages;parallel machines;load balance;static analysis;functional language;high performance	Abstract   This paper discusses mechanisms to exploit static information to increase locality in the Flagship fine-grained parallel machine. The machine supports a global address space across its distributed physical store. Load balancing is entirely dynamic and global-to-local address mapping to achieve high performance is carried out in a number of ways. A program is compiled into a graph of packets. This paper considers how static information is used to enable the dynamic mapping of the graph of packets to the distributed stores to improve performance and increase locality. Simulation figures suggest utilising static information results in some access taking less than half the time.		John A. Keane;Ian Watson;Xinfeng Ye	1996	Simul. Pr. Theory	10.1016/0928-4869(95)00037-2	locality of reference;parallel computing;simulation;computer science;artificial intelligence;load balancing;theoretical computer science;operating system;distributed computing;programming language;functional programming;static analysis	ECom	-13.74935630377437	46.483893996392624	58718
36e1b02a66ed928ef13e3a2ba6852e90a8713036	traffic management: a holistic approach to memory placement on numa systems	multicore;scheduling;numa;operating systems	NUMA systems are characterized by Non-Uniform Memory Access times, where accessing data in a remote node takes longer than a local access. NUMA hardware has been built since the late 80's, and the operating systems designed for it were optimized for access locality. They co-located memory pages with the threads that accessed them, so as to avoid the cost of remote accesses. Contrary to older systems, modern NUMA hardware has much smaller remote wire delays, and so remote access costs per se are not the main concern for performance, as we discovered in this work. Instead, congestion on memory controllers and interconnects, caused by memory traffic from data-intensive applications, hurts performance a lot more. Because of that, memory placement algorithms must be redesigned to target traffic congestion. This requires an arsenal of techniques that go beyond optimizing locality. In this paper we describe Carrefour, an algorithm that addresses this goal. We implemented Carrefour in Linux and obtained performance improvements of up to 3.6 relative to the default kernel, as well as significant improvements compared to NUMA-aware patchsets available for Linux. Carrefour never hurts performance by more than 4% when memory placement cannot be improved. We present the design of Carrefour, the challenges of implementing it on modern hardware, and draw insights about hardware support that would help optimize system software on future NUMA systems.	algorithm;data-intensive computing;electrical connection;holism;linux;locality of reference;memory bandwidth;memory management;network congestion;non-uniform memory access;operating system;page (computer memory);uniform memory access;wire wrap	Mohammad Dashti;Alexandra Fedorova;Justin R. Funston;Fabien Gaud;Renaud Lachaize;Baptiste Lepers;Vivien Quéma;Mark Roth	2013		10.1145/2451116.2451157	multi-core processor;embedded system;parallel computing;real-time computing;computer science;operating system;scheduling;cache-only memory architecture;non-uniform memory access	Arch	-9.489336337738607	50.60215566644013	58806
adc467f91c012e201811b56b5df3738820556bf6	greedy page replacement algorithm for flash-aware swap system	flash memory;electronic mail;clean aware victim page selection method flash aware swap system flash memory magnetic disk swap storage all page write operations garbage collection energy consumption flash page read operation greedy page replacement algorithm flash page write operations;flash memory algorithm design and analysis energy consumption consumer electronics educational institutions linux electronic mail;storage management;page replacement algorithm;greedy algorithms;consumer electronics;swap storage;energy consumption;linux;swap storage flash memory page replacement algorithm;algorithm design and analysis;flash memories;storage management flash memories greedy algorithms	Because of the attractive features, flash memory replaces magnetic disk as swap storage. All page write operations to flash-memory-based swap storage are requested during the page replacement algorithm in terms of swapping out dirty pages to obtain free page frames. Due to out-of-place update scheme, intensive write operations could result in using up the flash-memory-based swap storage quickly and incurring frequent garbage collection operations with high energy consumption. Moreover, the cost of flash page write operation is much higher than that of flash page read operation. Therefore, in this paper, we propose a greedy page replacement algorithm, called GDLRU, for flash-aware swap system. In order to reduce the number of flash page write operations, GDLRU introduces a clean-aware victim page selection method called CPS which evicts clean page preferentially. If there is no clean page, CPS evicts the dirty page with the least dirty data preferentially. To further reduce the number of flash page write operations, GDLRU also introduces a clean-aware victim page update scheme called CPU which only writes back the dirty flash pages within the victim dirty page. The simulation results indicate that our proposed algorithm outperforms other existing page replacement algorithms in terms of replacement cost.	central processing unit;computer data storage;dirty data;experiment;flash memory;garbage collection (computer science);greedy algorithm;magnetic storage;page replacement algorithm;paging;simulation	Mingwei Lin;Shuyu Chen;Guiping Wang	2012	IEEE Transactions on Consumer Electronics	10.1109/TCE.2012.6227444	zero page;algorithm design;greedy algorithm;parallel computing;page fault;page replacement algorithm;computer hardware;computer science;operating system;page;page attribute table;linux kernel	DB	-10.940292217207075	54.754195604184524	58864
3af216f371069b57c0dca5448384d052fb490fb4	rewind: recovery write-ahead system for in-memory non-volatile data-structures		Recent non-volatile memory (NVM) technologies, such as PCM, STT-MRAM and ReRAM, can act as both main memory and storage. This has led to research into NVM programming models, where persistent data structures remain in memory and are accessed directly through CPU loads and stores. Existing mechanisms for transactional updates are not appropriate in such a setting as they are optimized for block-based storage. We present REWIND, a usermode library approach to managing transactional updates directly from user code written in an imperative generalpurpose language. REWIND relies on a custom persistent in-memory data structure for the log that supports recoverable operations on itself. The scheme also employs a combination of non-temporal updates, persistent memory fences, and lightweight logging. Experimental results on synthetic transactional workloads and TPC-C show the overhead of REWIND compared to its non-recoverable equivalent to be within a factor of only 1.5 and 1.39 respectively. Moreover, REWIND outperforms state-of-the-art approaches for data structure recoverability as well as general purpose and NVM-aware DBMS-based recovery schemes by up to two orders of magnitude.	application programming interface;central processing unit;compiler;computer data storage;ibm tivoli storage productivity center;imperative programming;in-memory database;input/output;linear algebra;magnetoresistive random-access memory;non-blocking algorithm;non-volatile memory;overhead (computing);persistent data structure;persistent memory;programmer;protection ring;rendering (computer graphics);resistive random-access memory;schedule (computer science);serializability;software transactional memory;synthetic intelligence;user space;volatile memory	Andreas Chatzistergiou;Marcelo Cintra;Stratis Viglas	2015	PVLDB	10.14778/2735479.2735483	parallel computing;real-time computing;computer science;database	DB	-12.588415035862045	53.4999290032224	58889
6310042a644fd1027557aa51a4287f12f24c437a	fully distributed on-chip instruction memory design for stream architecture based on field-divided vliw compression	kernel;distributed memory systems;vliw kernel system on a chip memory management registers streaming media;field divided vliw compression;memory management;storage management;storage management distributed memory systems instruction sets memory architecture parallel architectures;fully distributed instruction memory stream architecture code characteristics analysis field divided vliw compression;system on a chip;vliw;fully distributed instruction memory;instruction memory energy consumption fully distributed on chip instruction memory design field divided vliw compression huge code size poor code density vliw processor stream architecture stream application domains stream program code characteristics instruction code fdim off chip instruction code on chip instruction memory space demand program performance masa stream processor area reduction;parallel architectures;streaming media;registers;memory architecture;stream architecture;code characteristics analysis;instruction sets	Huge code size and poor code density have always been a serious problem in VLIW processor. In order to deal with the problem and its influence on the instruction memory in stream architecture, this paper proposes a novel method called field-divided VLIW compression through analyzing the code characteristics of stream program across a wide range of typical stream application domains and dividing the instruction code unrelated to each other into different subfields. Based on the field-divided VLIW compression, this paper designs a fully distributed on-chip instruction memory (FDIM) for stream architecture. The experiment on MASA stream processor demonstrates that the field-divided VLIW compression can reduce about 38% of off-chip instruction code and about 66% of on-chip instruction memory space demand in the case of having little influence on the program performance; FDIM reduces the area of on-chip instruction memory by about 37%, thus reduces the area of the MASA stream processor by about 8.92%. Besides, the energy consumption of instruction memory is decreased by about 61%.	dspace;opcode;stream processing;very long instruction word	Yi He;Maolin Guan;Chunyuan Zhang;Tian Tian;Qianming Yang	2012	2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems	10.1109/HPCC.2012.14	system on a chip;computer architecture;parallel computing;kernel;computer hardware;computer science;very long instruction word;operating system;central processing unit;instruction set;processor register;memory management	EDA	-5.675512437779437	50.60466591395642	59116
e6955d629cb48ecad53223e3182bef96c788bf34	a communication staging technique for network cache interconnected clusters	cache storage;broadcasting libraries network interfaces message passing hardware computer applications computer networks concurrent computing distributed computing;collective communication;point to point;multistage interconnection networks;mpi messaging libraries communication staging technique network cache interconnected clusters parallel applications network cache architecture broadcast data shared buffer nic shared buffers workstations network cache large staging area collective communications synchronization messages application performance matrix vector multiply result vector traditional mpi messaging libraries completion time network cache staging point to point implementation;shared memory systems;message passing;matrix multiplication;workstation clusters;multistage interconnection networks workstation clusters cache storage shared memory systems matrix multiplication message passing;parallel applications	This work is an experimental investigation of a technique for staging communication between tasks of parallel applications in clusters that are interconnected with network cache architecture. With network cache a process can write broadcast data to a shared buffer on the NIC. This data is immediately broadcast on the network and available in shared buffers on the NIC’s of all workstations. By using network cache as a large staging area for collective communications, the required number of data and synchronization messages is reduced and application performance is improved. A version of matrix-vector multiply in which the result vector is available to all processes at the end of the computation is implemented both using network cache and using traditional MPI messaging libraries. The implementations are compared experimentally, and results show that completion time is improved by several factors with network cache staging rather than the point-to-point implementation of MPI messaging libraries.	computation;disk staging;experiment;library (computing);message passing interface;network interface controller;parallel computing;point-to-point protocol;workstation	Amy W. Apon;Hsiang Ann Chen;Charlotte F. Fischer;Larry Wilbur	1999		10.1109/IWCC.1999.810809	bus sniffing;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cache;computer science;write-once;cache invalidation;distributed computing;smart cache;cache algorithms;cache pollution;mesif protocol	HPC	-11.251603073651257	46.76850452771605	59282
49931fd010128708a5a1b8fc42e29b6d934186e2	data allocation in a heterogeneous disk array - hda with multiple raid levels for database applications		We consider the allocation of Virtual Arrays (VAs) in a Heter ogeneous Disk Array (HDA). Each VA holds groups of related objects and datasets such as files, relational tables, which has similar performance and availability characteristics. We evaluate single-pass data allocation methods for HDA using a synthetic stream of allocation requests, where each VA is chara cte ized by its RAID level, disk loads and space requirements. Th e goal is to maximize the number of allocated VAs and maintain high disk bandwidth and capacity utilization, while balanc ing disk loads. Although only RAID1 (basic mirroring) and RAID5 (rotated parity arrays) are considered in the experimental study, we develop the analysis required to estimate disk loads for o ther RAID levels. Since VA loads vary significantly over time, the VA allocation is carried out at the peak load period, while en suring that disk bandwidth is not exceeded at other high load per iods. Experimental results with a synthetic stream of alloca ti n requests show that allocation methods minimizing the maxim um disk bandwidth and capacity utilization or their variance a ross all disks yield the maximum number of allocated VAs. HDA saves disk bandwidth, since a single RAID level accommodati ng the most stringent availability requirements for a small su bset of objects would incur an unnecessarily high overhead for upda ting check blocks or data replicas for all objects. The number of a llocated VAs can be increased by adopting the clustered RAID5 paradigm, which exploits the tradeoff between redundancy a d bandwidth utilization. Since rebuild can be carried out at t he level of individual VAs, prioritizing rebuild of VAs with hi gher access rates can improve overall performance.	disk array;disk mirroring;experiment;load profile;overhead (computing);programming paradigm;requirement;standard raid levels;synthetic intelligence;titanium nitride;unified model	Alexander Thomasian;Jun Xu	2016	Comput. Syst. Sci. Eng.		embedded system;real-time computing;computer science;operating system;database;distributed computing	DB	-13.876993665971856	56.536169436142714	59361
a3609cdba057cabb49725c02a6c087a5f92289b3	design optimization for high-speed per-address two-level branch predictors	optimal configurations design optimization high speed per address two level branch predictors microprocessors direct mapped designs set associative designs branch target buffer performance sensitivity instruction benchmark suite spec cint95;microprocessors;optimal configurations;design optimization counting circuits history accuracy cost function taxonomy microprocessors;history;performance evaluation;performance evaluation microprocessor chips computer architecture instruction sets;cost function;perforation;direct mapped designs;instruction benchmark suite;design optimization;design space;branch target buffer;computer architecture;counting circuits;accuracy;set associative designs;high speed per address two level branch predictors;taxonomy;spec cint95;high speed;microprocessor chips;performance sensitivity;instruction sets	Per-address two-level branch predictors have been shown to be among the best predictors and have been implemented in current microprocessors. However, as cle time of modern microprocessors-continues to de the implementation of set-associa er-address two-level branch predictors will become ificult. Instead, direct-mapped designs may be more attractive. In this paper, we investigate an alternative implementation of the per-address two-level predictor referred to as the tagless, directmapped predictor, which is simpler and has faster access time. The tagless predictor can offer comparable performance to current s iative designs since removal of ctor ). Removal of tag deedictors from the BTB, thus allowing the two components to be optimized individually. Furthermore, our results show that this tagless implementation is more accurate because it handles conflict mis the branch history table better. Finally, we examine the system cost-benefit for tagictors across a wide design space usurs. We study the sensitivity of e to the workloads by comparing m ion Benchmark Suite (IBS) and SP 5. Our work provides principles and quantitative parameters for optimal configurations of such predictors.	access time;benchmark (computing);branch predictor;constructor (object-oriented programming);kerrison predictor;microprocessor	I-Cheng K. Chen;Chih-Chieh Lee;Matt Postiff;Trevor N. Mudge	1997		10.1109/ICCD.1997.628854	embedded system;electronic engineering;parallel computing;real-time computing;multidisciplinary design optimization;computer hardware;branch target predictor;computer science;operating system;instruction set;accuracy and precision;taxonomy	Arch	-5.230870256070661	50.359132685519185	59828
b823846d56f7b5511327a74d1ac6bdcc21aa03e9	dual direction load balancing and partial replication storage of cloud daas	databases;cloud servers dual direction load balancing partial replication storage cloud daas distributed as a service cloud storage issues file partitioning dual direction file download data reliability data availability;storage management cloud computing resource allocation;servers;load management;optimization;daas cloud computing load balancing storage optimization partial replication cloud data download data as a service;conferences;cloud computing;servers cloud computing optimization partitioning algorithms load management databases conferences;partitioning algorithms	In this paper, we present a novel approach to solve the cloud storage issues and provide a fast load balancing algorithm. Our approach is based on partitioning and dual direction download of the files by multiple cloud nodes. Partitions of the files are also saved into the cloud rather than the full files, which provide a good optimization to the cloud storage usage. Only partial replication is used in this algorithm to ensure the reliability and availability of the data. Our focus is to improve the performance and optimize the storage usage in providing the DaaS service of the cloud. This algorithm solves the problem of having to fully replicate very large data sets, which uses up a lot of precious space on the cloud nodes. Reducing the space needed will help in reducing the cost of providing such space. Moreover, performance is also increased since multiple cloud servers will collaborate to provide the data to cloud clients in a faster manner.	algorithm;backup;big data;cloud computing;cloud load balancing;cloud storage;data as a service;download;fault tolerance;load balancing (computing);mathematical optimization;replication (computing);requirement;self-replicating machine;server (computing);virtual private server	Klaithem Al Nuaimi;Nader Mohamed;Mariam Al Nuaimi;Jameela Al-Jaroodi	2014	2014 IEEE 3rd International Conference on Cloud Networking (CloudNet)	10.1109/CloudNet.2014.6969033	real-time computing;cloud computing;computer science;cloud testing;database;distributed computing	HPC	-15.965686312860246	57.420236532659054	59873
eb72fd6a7935263c0db3c7f2d277061943cdf5d2	near optimal work-stealing tree scheduler for highly irregular data-parallel workloads		We present a work-stealing algorithm for runtime scheduling of dataparallel operations in the context of shared-memory architectures on data sets with highly-irregular workloads that are not known a priori to the scheduler. This scheduler can parallelize loops and operations expressible with a parallel reduce or a parallel scan. The scheduler is based on the work-stealing tree data structure, which allows workers to decide on the work division in a lock-free, workloaddriven manner and attempts to minimize the amount of communication between them. A significant effort is given to showing that the algorithm has the least possible amount of overhead. We provide an extensive experimental evaluation, comparing the advantages and shortcomings of different data-parallel schedulers in order to combine their strengths. We show specific workload distribution patterns appearing in practice for which different schedulers yield suboptimal speedup, explaining their drawbacks and demonstrating how the work-stealing tree scheduler overcomes them. We thus justify our design decisions experimentally, but also provide a theoretical background for our claims.	data parallelism;data structure;experiment;non-blocking algorithm;overhead (computing);scheduling (computing);shared memory;speedup;tree (data structure);work stealing	Aleksandar Prokopec;Martin Odersky	2013		10.1007/978-3-319-09967-5_4	parallel computing;real-time computing;computer science;distributed computing;scheduling	HPC	-13.375549780589045	47.85829791104431	59930
4148b6709c18f02fafd5b7a42c348fcb92cff905	a framework to analyze the performance of load balancing schemes for ensembles of stochastic simulations	budding yeast cell cycle;parallel computation;stochastic simulation algorithm ssa;ensemble simulations;probabilistic framework analysis;dynamic load balancing dlb;high performance computing hpc	Ensembles of simulations are employed to estimate the statistics of possible future states of a system, and are widely used in important applications such as climate change and biological modeling. Ensembles of runs can naturally be executed in parallel. However, when the CPU times of individual simulations vary considerably, a simple strategy of assigning an equal number of tasks per processor can lead to serious work imbalances and low parallel efficiency. This paper presents a new probabilistic framework to analyze the performance of dynamic load balancing algorithms for ensembles of simulations where many tasks are mapped onto each processor, and where the individual compute times vary considerably among tasks. Four load balancing strategies are discussed: most-dividing, all-redistribution, random-polling, and neighbor-redistribution. Simulation results with a stochastic budding yeast cell cycle model are consistent with the theoretical analysis. It is especially significant that there is a provable global decrease in load imbalance for the local rebalancing algorithms due to scalability concerns for the global rebalancing algorithms. The overall simulation time is reduced by up to 25 %, and the total processor idle time by 85 %.	central processing unit;centralized computing;color gradient;computation;computer simulation;ensemble forecasting;gillespie algorithm;initial condition;load balancing (computing);numerical method;parallel computing;provable security;scalability;speedup	Tae-Hyuk Ahn;Adrian Sandu;Layne T. Watson;Clifford A. Shaffer;Yang Cao;William T. Baumann	2014	International Journal of Parallel Programming	10.1007/s10766-014-0309-6	parallel computing;real-time computing;computer science;theoretical computer science;distributed computing	HPC	-16.416493793831584	57.672090294592735	59969
2cdbb0a5b67f89eb42aad76361eb5a6a86ae0459	an analytical model for trace cache instruction fetch performance	analytical models;cache storage;benchmark programs;microarchitecture;electronic mail;decoding;tulip;software performance evaluation;cache memory;software performance evaluation instruction sets cache storage;accuracy;analytical models microarchitecture accuracy electronic mail hardware cache memory delay decoding performance analysis;trace cache instruction fetch performance;performance analysis;benchmark programs analytical model trace cache instruction fetch performance microarchitecture tulip;analytical model;instruction sets;hardware	This paper presents an analytical model of instruction fetch performance of a trace cache. This paper also presents an analytical model of miss rate of a trace cache. These models can be used to analyze performance and behavior of a microarchitecture of a processor. These models are implemented in a new microarchitecture tool Tulip. Performances of several benchmark programs based on Tulip are also presented in this paper.	cpu cache;instruction cycle	Afzal Hossain;Daniel J. Pease	2001		10.1109/ICCD.2001.955069	computer architecture;parallel computing;cpu cache;computer hardware;microarchitecture;computer science;operating system;instruction set;accuracy and precision;cache algorithms	Arch	-7.082160873648731	50.76077387726659	60085
cf848539159f4b9ddd50f0f6ebc0704b5aeda91f	task profiling model for load profile prediction	division of load;cluster computing;load average;free load profile;timing optimization;task profiling model;prediction accuracy;software framework;performance prediction;prediction model;grid computing;historical data	The accurate prediction of load profiles of future job tasks on the nodes of a cluster or grid supplies vital information for the users to make CPU/Disk resource usage decisions. At present, the Unix five-second host load is collected and used to predict the host loads, but forecasting can be improved if CPU and Disk load data are collected separately for each user on each host. The Free Load Profile or footprint of a job task on a load free node is a necessary input to the proposed Performance Prediction Model. To this end, the Task Profiling Model for Load Profile Prediction is proposed, which forecasts the load profiles of job tasks of individual machines based on current and historical data. The data is collected by agents running on the nodes of the cluster/grid. The data so obtained aids in choosing the most suitable set of computers for the deployment of the tasks in time optimal manner. Also, accurately predicted load profiles are useful inputs to the cost prediction models. The Task Profiling Model has been implemented in a software framework and evaluated for its prediction accuracy.	load profile	Sena Seneviratne;David C. Levy	2011	Future Generation Comp. Syst.	10.1016/j.future.2010.09.004	parallel computing;real-time computing;computer cluster;computer science;software framework;operating system;predictive modelling;programming language;grid computing	Arch	-18.99067659161883	58.58473931569579	60150
f0be03b0466fb7bdf3be4baa03c7d7343d74cb50	write mode aware loop tiling for high performance low power volatile pcm in embedded systems	phase change materials;energy efficient;embedded systems mlc pcm energy efficient loop tiling write mode;resistance;law;phase change memories embedded systems;arrays;embedded systems;shape;mlc pcm;phase change materials resistance law shape arrays hardware;loop tiling;write mode;cddw scheme write mode aware loop tiling approach high performance low power volatile pcm embedded systems phase change memory mlc pcm dram deployment dynamic random access memory compiler directed dual write scheme;hardware	Architecting PCM, especially MLC PCM, as main memory for MCUs is a promising technique to replace conventional DRAM deployment. However, PCM/MLC PCM suffers from long write latency and large write energy. Recent work has proposed a compiler directed dual-write (CDDW) scheme to combat the drawbacks of PCM by adopting fast or slow mode for different write operations. For large-scale loops, we observe that write instances' lifetime is very long and can only be written by the expensive slow mode. This paper proposes a write mode aware loop tiling approach to effectively reduce the lifetime of write instances and maximize the number of efficient fast writes in loops. The experimental results show that the proposed approach improves performance by 50.8 percent and reduces dynamic energy by 32.0 percent across a set of benchmarks compared to the CDDW approach on average.	embedded system;tiling window manager	Keni Qiu;Qing'an Li;Jingtong Hu;Weigong Zhang;Chun Jason Xue	2016	IEEE Trans. Computers	10.1109/TC.2015.2479605	loop tiling;parallel computing;real-time computing;computer hardware;shape;computer science;efficient energy use;resistance	EDA	-7.293251483506939	55.323354894175814	60259
22311f7db08d32152e0c4bb30344967439fd75cb	user-level dynamic page migration for multiprogrammed shared-memory multiprocessors	shared memory;parallel programming performance evaluation multiprogramming shared memory systems;performance evaluation;perforation;parallel programming;multiprogramming;thread migration;shared memory systems;operating system;yarn job shop scheduling operating systems costs concurrent computing iterative algorithms throughput delay memory management informatics;parallel programs;migration schemes user level dynamic page migration multiprogrammed shared memory multiprocessors performance improvement parallel programs numa multiprocessors dynamic page migration page migration engine operating system kernel scheduler generic codes performance evaluation sgi origin2000 irix 6 5 5 page placement;shared memory multiprocessor	This paper presents algorithms for improving the performance of parallel programs on multiprogrammed sharedmemory NUMA multiprocessors, via the use of user-level dynamic page migration. The idea that drives the algorithms is that a page migration engine can perform accurate and timely page migrations in a multiprogrammed system if it can correlate page reference information with scheduling information obtained from the operating system. The necessary page migrations can be performed as a response to scheduling events that break the implicit association between threads and their memory affinity sets. We present two algorithms that use feedback from the kernel scheduler to aggressively migrate pages upon thread migrations. The first algorithm exploits the iterative nature of parallel programs, while the second targets generic codes without making assumptions on their structure. Performance evaluation on an SGI Origin2000 shows that our page migration algorithms provide substantial improvements in throughput of up to 264% compared to the native IRIX 6.5.5 page placement and migration schemes.	algorithm;code;computer multitasking;experiment;generic programming;home page;irix;iterative method;openmp;operating system;parallel programming model;performance evaluation;processor affinity;reference counting;scheduling (computing);selectivity (electronic);shared memory;throughput;triple modular redundancy;url redirection;user space	Dimitrios S. Nikolopoulos;Theodore S. Papatheodorou;Constantine D. Polychronopoulos;Jesús Labarta;Eduard Ayguadé	2000		10.1109/ICPP.2000.876083	zero page;shared memory;demand paging;parallel computing;real-time computing;page fault;computer multitasking;page replacement algorithm;computer science;operating system	Arch	-12.661862489667898	49.08450522010147	60289
04bffc7c4b7e6e40815621c8981f94ba5a3fad8a	improving restore speed for backup systems that use inline chunk-based deduplication	ram budget;inline chunk-based deduplication;recent backup;new restore-time caching;forward assembly area;improving restore speed;restore time;chunk fragmentation;deduplication system;future chunk;cache size;backup system;larger cache	Slow restoration due to chunk fragmentation is a serious problem facing inline chunk-based data deduplication systems: restore speeds for the most recent backup can drop orders of magnitude over the lifetime of a system. We study three techniques—increasing cache size, container capping, and using a forward assembly area— for alleviating this problem. Container capping is an ingest-time operation that reduces chunk fragmentation at the cost of forfeiting some deduplication, while using a forward assembly area is a new restore-time caching and prefetching technique that exploits the perfect knowledge of future chunk accesses available when restoring a backup to reduce the amount of RAM required for a given level of caching at restore time. We show that using a larger cache per stream—we see continuing benefits even up to 8 GB—can produce up to a 5–16X improvement, that giving up as little as 8% deduplication with capping can yield a 2–6X improvement, and that using a forward assembly area is strictly superior to LRU, able to yield a 2–4X improvement while holding the RAM budget constant.	backup;cpu cache;cache (computing);chunking (computing);circuit restoration;data deduplication;fragmentation (computing);frequency capping;locality of reference;operating system;random-access memory	Mark Lillibridge;Kave Eshghi;Deepavali Bhagwat	2013			parallel computing;real-time computing;data deduplication;computer science;operating system	OS	-10.057441833377961	54.55212792830075	60304
57f4550fbee717f3fe0881af7dd77dff4e7a0c78	design trade-offs in high-throughput coherence controllers	cache storage;reconfigurable architectures;node hardware high throughput coherence controllers scalable shared memory multiprocessors microarchitectural enhancements superpipelining nonblocking execution coherence transactions directory cache tag cache splash 2 applications parallelized spec95 applications simulation models smp;load imbalance;clustered processors;carbon capture and storage throughput microprocessors bandwidth engines microarchitecture hardware pipeline processing protocols computer science;system performance;shared memory systems;inter pe communication;instruction replication;parallel architectures;parallel architectures shared memory systems reconfigurable architectures pipeline processing cache storage;instruction distribution;high throughput;simulation model;pipeline processing;instructions per cycle;scalable shared memory multiprocessors	Recent research shows that the high occupancy of Coherence Controllers (CCs) is a major performance bottleneck in scalable shared-memory multiprocessors. In this paper, we propose to take microarchitectural enhancements used for microprocessors and apply them to improve the throughput of hardwired CCs. These enhancements are CC support for nonblocking execution, early fetches of directory and L3 information, and superpipelining. Nonblocking execution in the CC reduces stalls by processing subsequent coherence transactions in the presence of misses in the directory cache and tag cache. Early fetching in the CC hides misses in the directory and tag caches and, therefore, also removes stalls. Finally, superpipelining in the CC increases its processing bandwidth. These supports all serve to increase the overall throughput of CCs and improve overall system performance.Using both SPLASH-2 and parallelized SPEC95 applications on detailed simulation models, we show that CCs that support nonblocking execution and superpipelining boost the performance of machines substantially. With these CCs, a 64-processor machine with four nodes of four SMPs per node runs on average 3.56 times faster than if it used conventional CCs. In addition, the machine runs about as fast as a more costly 64-processor machine with sixteen nodes of one SMP per node and the same advanced CCs. This is despite using much less network, chassis, and node hardware. Consequently, with our proposed advanced CCs, we can reduce the system cost signi.cantly without affecting performance.	chassis;directory (computing);disk controller;microarchitecture;microprocessor;parallel computing;scalability;shared memory;simulation;switched-mode power supply;symmetric multiprocessing;throughput	Anthony-Trung Nguyen;Josep Torrellas	2003		10.1109/PACT.2003.1238015	high-throughput screening;computer architecture;parallel computing;real-time computing;computer science;operating system;simulation modeling;computer performance;instructions per cycle	Arch	-10.124295377628824	51.33759958985939	60423
ada2d842174f32604edf6ce72d914beb35d51c53	a group-based load balance scheme for software distributed shared memory systems	thread migration;software distributed shared memory;load balancing;dsm;load balance;data consistency;minimizing communication	Load balance is an important issue for the performance of software distributed shared memory (DSM) systems. One solution of addressing this issue is exploiting dynamic thread migration. In order to reduce the data consistency communication increased by thread migration, an effective load balance scheme must carefully choose threads and destination nodes for workload migration. In this paper, a group-based load balance scheme is proposed to resolve this problem. The main characteristic of this scheme is to classify the overloaded nodes and the lightly loaded nodes into a sender group and a receiver group, and then consider all the threads of the sender group and all the nodes of the receiver group for each decision. The experimental results show that the group-based scheme reduces more communication than the previous schemes. Besides, this paper also resolves the problem of the high costs caused by group-based schemes. Therefore, the performance of the test programs is effectively enhanced after minimizing the communication increased by thread migration.	algorithm;cpu cache;central processing unit;computational resource;distributed shared memory;load balancing (computing);overhead (computing);process migration;requirement;thread (computing);vector quantization	Yi-Chang Zhuang;Tyng-Yue Liang;Ce-Kuen Shieh;Jun-Qi Lee;Laurence Tianruo Yang	2004	The Journal of Supercomputing	10.1023/B:SUPE.0000022101.41799.cc	parallel computing;real-time computing;computer science;load balancing;operating system;distributed computing;computer security	HPC	-13.230195937666046	51.375769486810334	60436
9974a3fd6d09ea575f23f9108e8987813a64d15f	multiprocessor cache analysis using atum	storage allocation;multiprocessor systems;storage management;interference;computer architecture;side effect;operating system;process affinity multiprocessor cache analysis tracing facility atum 2 address tracing using microcode memory references vax 8350 multiprocessor process identifier hashing cache interference;memory systems;multiprocessing systems;parallel programs;high performance;analytical model;storage management computer architecture multiprocessing systems storage allocation	The design of high-performance multiprocessor systems necessitates a careful analysis of the memory system performance of parallel programs. Lacking multiprocessor address traces, previous multiprocessor performance studies using analytical models had to make an inordinate number of assumptions about the underlying memory reference patterns. We previously developed a scheme called ATUM - Address Tracing Using Microcode - to get reliable operating system and multiprogramming traces on single processors. This paper briefly describes the multiprocessor extension of ATUM and its implementation on a VAX 8350 multiprocessor. We also report on our use of the resulting traces to analyze physical versus virtual addressing of large caches, process-identifier hashing in virtual caches, cache interference between multiple processes, cache interference between multiple CPUs, process affinity, and semaphore usage in writeback caches.	cpu cache;cache (computing);central processing unit;computer multitasking;interference (communication);microcode;multiprocessing;operating system;process identifier;processor affinity;tracing (software);vax	Richard L. Sites;Anant Agarwal	1988		10.1145/633625.52422	computer architecture;parallel computing;real-time computing;computer science;operating system;interference;programming language;side effect;symmetric multiprocessor system	Arch	-10.441832429232056	48.8784959192513	60517
43040292c9b17fcdcbb42ae5998ba67091172205	compositional real-time scheduling framework	resource partitioning;resource allocation;real time;multi level scheduling;real time composition;delays real time systems scheduling resource allocation;timing real time systems independent component analysis assembly systems processor scheduling information science information analysis design methodology;scheduling;real time scheduling;bounded delay tasks compositional real time scheduling system level timing properties component level timing properties liu layland periodic model bounded delay resource partition model;component based design;delays;real time systems	Our goal is to develop a compositional real-time scheduling framework so that global (system-level) timing properties can be established by composing independently (specified and) analyzed local (component-level) timing properties. The two essential problems in developing such a framework are: (1) to abstract the collective real-time requirements of a component as a single real-time requirement and (2) to compose the component demand abstraction results into the system-level real-time requirement. In our earlier work, we addressed the problems using the Liu and Layland periodic model. In this paper, we address the problems using another well-known model, a bounded-delay resource partition model, as a solution model to the problems. To extend our framework to this model, we develop an exact feasibility condition for a set of bounded-delay tasks over a bounded-delay resource partition. In addition, we present simulation results to evaluate the overheads that the component demand abstraction results incur in terms of utilization increase. We also present utilization bound results on a bounded-delay resource model.	real-time clock;real-time computing;real-time locating system;real-time transcription;requirement;scheduling (computing);simulation	Insik Shin;Insup Lee	2004	25th IEEE International Real-Time Systems Symposium	10.1109/REAL.2004.15	fair-share scheduling;fixed-priority pre-emptive scheduling;niche differentiation;real-time computing;simulation;dynamic priority scheduling;resource allocation;computer science;rate-monotonic scheduling;component-based software engineering;operating system;two-level scheduling;distributed computing;scheduling	Embedded	-8.4169722983951	60.36515876359407	60685
645b8ec695a0b7020cbdf1843a4dc74dd5f5253c	a two-phase optimization approach for condition codes in a machine adaptable dynamic binary translator	program interpreters binary codes instruction sets optimisation;instruction set two phase optimization approach condition codes machine adaptable dynamic binary translator crossbit redundant flag computing code lazy evaluation technique;optimisation;crossbit;machine adaptable dynamic binary translator;lazy evaluation;program interpreters;instruction set;binary codes;emulation;adaptive dynamics;data mining;instruction sets carbon capture and storage emulation optimization methods computer science chaos performance gain sun delay computational modeling;computer architecture;redundancy;binary translation;two phase optimization approach;sun;code size;optimization;condition codes;lazy evaluation technique;redundant flag computing code;instruction sets	Condition codes (CCs) are special architected bits that characterize instruction results. Reducing the overhead of emulating condition codes is a critical performance issue in binary translation. In this paper, we propose a two-phase optimization approach for condition codes, which has been implemented in Crossbit—a machine adaptable dynamic binary translator (DBT). First, redundant flag computing code in a basic block is reduced based on the information collected by Crossbit when the block is identified. Then, lazy evaluation technique is used inter basic blocks, which make the condition codes emulation more efficient. Experimental results show that this method works much better than the straightforward emulation method—it eliminate the code size more than 20% and have a performance gain nearly 40% in Crossbit. The data proves that the approach is effective to remove the redundant code and to increase the performance of the translated code.	binary translation;status register	Chao Chu;Yuyu Zheng;Haibing Guan;Alei Liang	2009		10.1109/CSIE.2009.275	block code;parallel computing;real-time computing;computer science;theoretical computer science;operating system;instruction set;linear code;database;programming language;algorithm	Theory	-6.1294103247317855	50.15207368123209	60903
9d901d2bd8808ca7da361f3fad52a0328d24a79b	internet nuggets	repetition probability;parallel computing;control speculation;energy efficiency;mesh of trees;program behavior;compiler flag tuning;lut look up table;hardware data prefeching;simd;program instrumentation;microprocessor;look up table;multiple index dbns;beagleboard;pram;memory access time;power saving;upper bound reliability;workload characterization;microarchitecture;point multiplication;last address predictor;high radix multiplication;parallel algorithm;stack coloring;chip multi processors;multimedia;linux kernel;performance evaluation;cache memory profiling;hybrid predictor;access and execute mechanisms;neural networks;stride predictor;optially reconfigurable gate arrays;context predictor;reconfigurable computing;cpi analysis;queuing system;dual path execution;superscalar;high performance computing;convolution;interconnect;stream computation;scalar cache;partial product generator ppg;ecc;memory wall;performance;prefetching;stream buffer;crossover;emulation;mpp;dynamic compilation;video processing;conversion processing element;cache memory;fft;four tree network;sorting by ranking;android;shared cache;partitioning level;multistage interconnection network;fpga;hardware accelerators;hybrid cache;nft;memory overhead;constant time sorting;inherent parallelism;data distribution;shared memories;private cache;traceconstruction;input output;reconfigurable computer;spec cpu2006 benchmarks;wallace tree algorithm;computer architecture;chi square;embedded systems;array cache;parallelism;concurrency;booth algorithm;dbns;parallel architectures;speculation;deep submicron technology;precomputation;low power multiplication;avs;binary search tree;open multimedia application processor;binary translation;load address prediction;critical path;tightly coupled reconfigurable processor;false sharing;multiprocessor architecture;fault tolerance;modulii;enumeration sort;directory based cache coherence protocol;register transfer level rtl;power management;fpgas;soc multiprocessors	This column consists of selected traffic from the comp.arch newsgroup, a forum for discussion of computer architecture on the Internet-an international computer network.As always, the opinions expressed in this column are the personal views of the authors, and do not necessarily represent the institutions to which they are affiliated.Text which sets the context of a message appears underlined or in italics; this is usually text the author has quoted from earlier messages. The codelike expressions below the authors' names are their addresses on Internet.	computer architecture;internet;regular expression	Mark Thorson	2004	SIGARCH Computer Architecture News	10.1145/1024295.1024307	computer architecture;parallel computing;real-time computing;computer science;operating system;programming language;artificial neural network;field-programmable gate array	Arch	-6.60357326050241	51.68205046356469	61249
0e9f2c4a4f153dabfdb7e858af38d3426b30cb2d	holistic run-time parallelism management for time and energy efficiency	run time optimization;parallel programming;autotuning;performance portability;performance tuning	The ubiquity of parallel machines will necessitate time- and energy-efficient parallel execution of a program in a wide range of hardware and software environments. Prevalent parallel execution models can fail to be efficient. Unable to account for dynamic changes in operating conditions, they may create non-optimum parallelism, leading to underutilization or contention of resources. We propose ParallelismDial (PD), a model to dynamically, continuously and judiciously adapt a program's degree of parallelism to a given dynamic operating environment. PD uses a holistic metric to measure system-efficiency. The metric is used to systematically optimize the program's execution.  We apply PD to two diverse parallel programming models: Intel TBB, an industry standard, and Prometheus, a recent research effort. Two prototypes of PD have been implemented. The prototypes are evaluated on two stock multicore workstations. Dedicated and multiprogrammed environments were considered. Experimental results show that the prototypes outperform the state-of-the-art approaches, on average, by 15% on time and 31% on energy efficiency, in the dedicated environment. In the multiprogrammed environment, the savings are to the tune of 19% and 21% in time and energy, respectively.	degree of parallelism;holism;multi-core processor;operating environment;parallel computing;prometheus;technical standard;threading building blocks;workstation	Srinath Sridharan;Gagan Gupta;Gurindar S. Sohi	2013		10.1145/2464996.2465016	parallel computing;real-time computing;computer science;operating system;distributed computing;programming language	HPC	-5.0772516087154935	49.99244074283541	61267
bb668f76a474d3547290b17116b21dc5bc89967f	an enhanced scheduling approach in a distributed parallel environment using mobile agents	lightweight workstations;java message passing software agents workstation clusters resource allocation parallel programming;cluster computing;application software;processor scheduling;resource allocation;mobile agents;enhanced scheduling approach;cluster configuration;mobile agent technology;parallel programming;mobile agents application software workstations packaging processor scheduling software packages load management programming profession microcomputers throughput;packaging;software agents;message passing interface;message passing interface mpi and clustercomputing;cluster computing enhanced scheduling approach distributed parallel environment mobile agents mpi cluster configuration distributed cluster environment load balancing jota lightweight workstations scheduling message passing interface;programming profession;scheduling;workstations;load management;load balancing;message passing;distributed parallel environment;mpi;load balance;workstation clusters;mobile agent;microcomputers;parallel applications;parallel processing;jota;distributed cluster environment;software packages;throughput;java	Our goal is to apply mobile agent technology to provide a better scheduling for MPI applications executing in a cluster configuration. This approach could represent in a distributed cluster environment an enhancement on the load balancing of the parallel processes. MPI in a cluster of heterogeneous machines could lead parallel programmers to obtain frustrated results, mainly because of the lack of an even distribution of the workload in the cluster. As a result, before submitting a MPI application to a cluster, we use our JOTA mobile agent approach to acquire a more precise information of machine's workload. Therefore, with a more precise knowledge of the load and characteristics in each machine, we are ready to gather lightweight workstations to form a cluster. Our empirical results indicate that it is possible to spend less elapsed time when considering the execution of a parallel application using the agent approach in comparison to an ordinary MPI environment.	mobile agent;scheduling (computing)	Mario A. R. Dantas;J. G. R. C. Lopes;T. G. Ramos	2002		10.1109/HPCSA.2002.1019152	parallel processing;parallel computing;real-time computing;computer science;message passing interface;load balancing;operating system;distributed computing	AI	-15.733137387969517	59.30258363483565	61269
40136c8d1505a15b88e5f7fa95699496ffbb450a	an enhancement for a scheduling logic pipelined over two cycles	dynamic scheduler;pipelined scheduling logic back to back execution dynamic scheduler;indexing terms;out of order;dynamic scheduling logic;instruction level parallelism dynamic scheduling logic pipelined scheduling logic scheduling loop latency;pipeline processing dynamic scheduling instruction sets;logic processor scheduling pipeline processing degradation out of order dynamic scheduling parallel processing computer aided instruction delay proposals;pipelined scheduling logic;scheduling loop latency;instruction level parallelism;back to back execution;pipeline processing;dynamic scheduling;instruction sets	Out of order processors use the dynamic scheduling logic both to expose and to exploit parallelism. Pipelining this logic may sacrifice the ability to execute dependent instructions in consecutive cycles. Several previous studies have shown that pipelining the scheduling logic over two cycles degrades performance; our evaluations, in a 4-way machine, on SPEC-2000 integer benchmarks show a performance degradation about 11% compared to an unpipelined scheduling logic. In this work, we present two non-speculative enhancements for a scheduling logic pipelined over two cycles. The idea is computing in advance which instructions will be woken-up by all instructions that are currently competing for selection. Once all of them have been selected, the pre-computed group of instructions can compete for selection in next cycle. The enhancement goal is to tolerate the scheduling-loop latency when not enough ILP is available through the scheduling of dependent instructions in consecutive cycles. Our results in a 4-way machine show that our two proposed enhancements perform, on average, slightly better than two previously proposed speculative schedulers. The performance of our proposals is within a 2.6% and 2% of an unpipelined ideal scheduler.	baseline (configuration management);central processing unit;elegant degradation;parallel computing;pipeline (computing);precomputation;scheduling (computing);speculative execution	Ruben Gran Tejero;Enric Morancho;Àngel Olivé;José María Llabería	2006	2006 International Conference on Computer Design	10.1109/ICCD.2006.4380818	fair-share scheduling;fixed-priority pre-emptive scheduling;computer architecture;parallel computing;real-time computing;earliest deadline first scheduling;index term;gang scheduling;flow shop scheduling;dynamic priority scheduling;computer science;out-of-order execution;rate-monotonic scheduling;operating system;two-level scheduling;stride scheduling;instruction set;instruction scheduling;lottery scheduling;round-robin scheduling;instruction-level parallelism;i/o scheduling;trace scheduling	Arch	-8.64286546418937	51.149076027197644	61503
4c86ca38192aea7ffce25aa359e7999c6ed14f01	brief announcement: locality-aware load balancing for speculatively-parallelized irregular applications	data parallel;speculative parallelization;data partitioning;load balancing;irregular programs;load balance	Load balancing is an important consideration when running data-parallel programs. While traditional techniques trade off the cost of load imbalance with the overhead of mitigating that imbalance, when speculatively parallelizing amorphous data-parallel applications, we must also consider the effects of load balancing decisions on locality and speculation accuracy. We present two data centric load balancing strategies which account for the intricacies of amorphous data-parallel execution. We implement these strategies as schedulers in the Galois system and demonstrate that they outperform traditional load balancing schedulers, as well as a data-centric, non-load-balancing scheduler.	amorphous computing;load balancing (computing);locality of reference;overhead (computing);parallel computing;scheduling (computing)	Youngjoon Jo;Milind Kulkarni	2010		10.1145/1810479.1810516	network load balancing services;parallel computing;real-time computing;computer science;load balancing;distributed computing	OS	-13.079964783566409	48.17024074925167	61544
5da6600662b086435d4415753e3c9abf47c010d1	measurement-based timing analysis of the aurix caches	caches;automotive;embedded systems;wcet;conference report;aurix;real time systems	Cache memories are one of the hardware resources with higher potential to reduce worst-case execution time (WCET) costs for software programs with tight real-time constraints. Yet, the complexity of cache analysis has caused a large fraction of real-time systems industry to avoid using them, especially in the automotive sector. For measurement-based timing analysis (MBTA) – the dominant technique in domains such as automotive – cache challenges the definition of test scenarios stressful enough to produce (cache) layouts that causing high contention. In this paper, we present our experience in enabling the use of caches for a real automotive application running on an AURIX multiprocessor, using software randomization and measurement-based probabilistic timing analysis (MBPTA). Our results show that software randomization successfully exposes – in the experiments performed for timing analysis – cache related variability, in a manner that can be effectively captured by MBPTA. 1998 ACM Subject Classification [D4.7] Real-time Systems and Embedded Systems	best, worst and average case;cpu cache;embedded system;experiment;infineon aurix;multiprocessing;real-time clock;real-time computing;run time (program lifecycle phase);spatial variability;static timing analysis;worst-case execution time	Leonidas Kosmidis;Davide Compagnin;David Morales;Enrico Mezzetti;Eduardo Quiñones;Jaume Abella;Tullio Vardanega;Francisco J. Cazorla	2016		10.4230/OASIcs.WCET.2016.9	embedded system;parallel computing;real-time computing;computer science;automotive industry;operating system;cache algorithms	Embedded	-7.350893222511478	58.19275068271104	61552
342a1b8bfdc86461ae9048cc43539198aabbe853	an experimental study on memory allocators in multicore and multithreaded applications	computer program;multi threading;memory management;multithreading memory allocators performance;storage management;performance;resource manager;resource management;statistical significance;indexes;statistical analysis;storage management multiprocessing systems multi threading statistical analysis;data structures;instruction sets memory management resource management middleware indexes multicore processing data structures;multicore processing;indexation;memory allocators;middleware;multiprocessing systems;memory allocation;software design;data structure;anova method memory allocators multicore machine multithreaded applications computer programs memory allocation operations software design response time memory consumption memory fragmentation statistical significance;instruction sets;multithreading	Memory allocations are one of the most omnipresent operations in computer programs. The performance of memory allocation operations is a very important aspect to be considered in software design, however it is frequently neglected. This paper presents an experimental comparative study of seven largely adopted memory allocators. Unlike other related works, we assess the selected memory allocators using real-world multithreaded applications. We consider the applications' response time, memory consumption, and memory fragmentation, in order to compare the performance of the investigated memory allocators running on a multicore machine. All test results are evaluated with respect to their statistical significance throughout the ANOVA method.	computer program;fragmentation (computing);memory management;multi-core processor;response time (technology);software design;thread (computing)	Taís Borges Ferreira;Rivalino Matias;Autran Macedo;Lucio Borges de Araujo	2011	2011 12th International Conference on Parallel and Distributed Computing, Applications and Technologies	10.1109/PDCAT.2011.18	uniform memory access;computer architecture;parallel computing;real-time computing;multithreading;distributed memory;data structure;computer science;resource management;operating system;static memory allocation;database;distributed computing;programming language;cache-only memory architecture;non-uniform memory access;memory management	HPC	-8.815397090388267	49.63199380687786	61650
9f3ecc8c7d4c9db0f64d9a10e30b362172f1e901	parallelism in dynamic well-spaced point sets	dynamic change;parallel algorithm;self adjusting computation;computational geometry;well spaced point sets;design technique;mesh refinement;time use;empirical evaluation;parallel batch dynamic updates;voronoi diagrams;voronoi diagram;multithreading	Parallel algorithms and dynamic algorithms possess an interesting duality property: compared to sequential algorithms, parallel algorithms improve run-time while preserving work, while dynamic algorithms improve work but typically offer no parallelism. Although they are often considered separately, parallel and dynamic algorithms employ similar design techniques. They both identify parts of the computation that are independent of each other. This suggests that dynamic algorithms could be parallelized to improve work efficiency while preserving fast parallel run-time.  In this paper, we parallelize a dynamic algorithm for well-spaced point sets, an important problem related to mesh refinement in computational geometry. Our parallel dynamic algorithm computes a well-spaced superset of a dynamically changing set of points, allowing arbitrary dynamic modifications to the input set. On an EREW PRAM, our algorithm processes batches of k modifications such as insertions and deletions in O(k log Δ) total work and in O(log Δ) parallel time using k processors, where Δ is the geometric spread of the data, while ensuring that the output is always within a constant factor of the optimal size. EREW PRAM model is quite different from actual hardware such as modern multiprocessors. We therefore describe techniques for implementing our algorithm on modern multi-core computers and provide a prototype implementation. Our empirical evaluation shows that our algorithm can be practical, yielding a large degree of parallelism and good speedups.	adaptive mesh refinement;armadillo;central processing unit;column (database);computation;computational geometry;computer;degree of parallelism;dynamic problem (algorithms);multi-core processor;parallel algorithm;parallel computing;parallel random-access machine;prototype;refinement (computing);stanford bunny;thread (computing)	Umut A. Acar;Andrew Cotter;Benoît Hudson;Duru Türkoglu	2011		10.1145/1989493.1989498	mathematical optimization;combinatorics;parallel computing;voronoi diagram;computational geometry;computer science;theoretical computer science;analysis of parallel algorithms;distributed computing;algorithm	DB	-15.347903289234878	46.462907726501896	61660
9c92996f6bff58954d03190d0f3f8e6fe5cff293	a novel rename register architecture and performance analysis	evaluation performance;diseno circuito;storage access;performance evaluation;circuit commande;evaluacion prestacion;circuit design;low complexity;tiempo acceso;computer architecture;architecture ordinateur;data dependence;acces memoire;performance analysis;control circuit;traitement exception;procesador superescalar;acceso memoria;temps acces;exception handling;superscalar processor;conception circuit;circuito control;arquitectura ordenador;processeur superscalaire;access time	In today's superscalar processors, the register renaming scheme is widely used to resolve data dependence constraints. The drawback of the conventional design is that the bit-line load of the storage cell is so heavy that the access time to these storage elements is more than one cycle, impacting the IPC adversely. Moreover, in order to implement precise exception handling, the conventional allocation and recovery strategy is very complex. A novel Rename Register architecture is presented in this paper to overcome these problems. This Rename Register has such features: 1) each storage cell has just one write port, which reduces the bit line load and simplifies the circuit design, so the access time of this Rename Register could be greatly improved; 2) the allocation and recovery strategy of this Rename Register is low-complex. This feature not only simplifies the Rename Register control circuit, but also improves the exception handling speed.	profiling (computer programming);rename (relational algebra)	Zhenyu Liu;Jiayue Qi	2004		10.1007/978-3-540-30102-8_42	exception handling;embedded system;parallel computing;real-time computing;access time;computer science;operating system;circuit design;stack register;programming language	Arch	-9.618968885130727	53.88728650871401	61732
e8ce82c11cc9e54ef80cd704ff5b1d9d5fc6f5b0	mlc pcm main memory with accelerated read	phase change materials random access memory acceleration buffer storage phase change memory memory architecture;phase change materials;random access memory;buffer storage;phase change memories cache storage;acceleration;memory architecture;parsec 2 benchmark mlc pcm main memory accelerated read multilevel cell phase change memory most significant bit least significant bit half line pcm cache line cmp model;phase change memory	This paper alleviates the problem of slow reads in the Multi-Level Cell Phase Change Memory (MLC PCM) by exploiting a the fact that the Most-Significant Bit (MSB) of MLCs is read fast, while reading the Least-Significant Bits (LSBs) is slower. We propose Half-Line PCM (HL-PCM), a memory architecture that leverages this property to send half of a cache line to the processor ahead of the other half, so that processor continues its execution if the missed data element is in the first half. Our evaluation shows that HL-PCM improves program execution time by 23%, on average, in a 16-core CMP model for workloads from PARSEC-2 benchmark.	benchmark (computing);computer data storage;data element;horseland;most significant bit;multi-level cell;phase-change memory;run time (program lifecycle phase)	Mohammad Arjomand;Amin Jadidi;Mahmut T. Kandemir;Anand Sivasubramaniam;Chita R. Das	2016	2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)	10.1109/ISPASS.2016.7482082	acceleration;auxiliary memory;uniform memory access;interleaved memory;semiconductor memory;parallel computing;real-time computing;dynamic random-access memory;sense amplifier;phase-change memory;memory refresh;computer hardware;computer science;bubble memory;computer memory;conventional memory;extended memory;flat memory model;registered memory;cache-only memory architecture;non-uniform memory access;memory management	Arch	-9.569886923248525	53.79879722921451	61741
3a65796bd1d75170fb02beea80599e35cd52d5ab	allocation of computations with dynamic structures on hypercube based distributed systems	distributed system;divide conquer algorithms;concurrent computing;performance evaluation;allocation strategy;processor scheduling;resource allocation;distributed processing;distributed computing;computations allocation;resource allocation hypercube networks performance evaluation;computer architecture;computational modeling;recursive algorithms;parallel architectures;dynamic partitioning;hypercube based distributed systems;load management;distributed computing hypercubes load management processor scheduling concurrent computing computational modeling parallel architectures distribution strategy computer architecture distributed processing;parallel computer;hypercubes;simulation study;load distribution;task graphs;distribution strategy;allocation strategy computations allocation recursive algorithms dynamic structures hypercube based distributed systems dual level dynamic load distribution strategy divide conquer algorithms dynamic partitioning median processors simulation study;dynamic structures;median processors;hypercube networks;dual level dynamic load distribution strategy;dynamic loading	A dual-level dynamic load distribution strategy is proposed for allocating parallel computations with unpredictable structures to hypercube based distributed systems. Computations with dynamic structures represent a wide range of recursive and divide/conquer algorithms. The allocation strategy supports dynamic partitioning of these computations into communicating sub-tasks. Using the topological characteristics of hypercube networks, the system is divided into multiple regions of processors. The first level allocation is done by the central computer that spreads out the initial computations into these regions to reduce processor contention. The second level allocation is done by the median processors of these regions which enable the processors of their regions to optimally balance the dynamically created load and to communicate with each other with reduced overhead. The results of a simulation study are presented illustrating numerous examples that exhibit the performance of the proposed strategy under different loading conditions, varying degrees of depth and parallelism in the task graphs. The proposed allocation strategy is shown to outperform distributed load distribution. >		Ishfaq Ahmad;Arif Ghafoor;Geoffrey C. Fox	1992		10.1109/HPDC.1992.246470	parallel computing;concurrent computing;resource allocation;computer science;weight distribution;theoretical computer science;distributed computing;computational model;hypercube	HPC	-14.23408007755588	59.639033960105074	61897
864c9ba4b359b495afc2b39be0828f7ef8d7ff05	maxpb: accelerating pcm write by maximizing the power budget utilization	power budget;write unit;write scheme;pcm	Phase Change Memory (PCM) is one of the promising memory technologies but suffers from some critical problems such as poor write performance and high write energy consumption. Due to the high write energy consumption and limited power supply, the size of concurrent bit-write is restricted inside one PCM chip. Typically, the size of concurrent bit-write is much less than the cache line size and it is normal that many serially executed write units are consumed to write down the data block to PCM when using it as the main memory. Existing state-of-the-art PCM write schemes, such as FNW (Flip-N-Write) and two-stage-write, address the problem of poor performance by improving the write parallelism under the power constraints. The parallelism is obtained via reducing the data amount and leveraging power as well as time asymmetries, respectively. However, due to the extremely pessimistic assumptions of current utilization (FNW) and optimistic assumptions of asymmetries (two-stage-write), these schemes fail to maximize the power supply utilization and hence improve the write parallelism.  In this article, we propose a novel PCM write scheme, called MaxPB (Maximize the Power Budget utilization) to maximize the power budget utilization with minimum changes about the circuits design. MaxPB is a “think before acting” method. The main idea of MaxPB is to monitor the actual power needs of all data units first and then effectively package them into the least number of write units under the power constraints. Experimental results show the efficiency and performance improvements on MaxPB. For example, four-core PARSEC and SPEC experimental results show that MaxPB gets 32.0% and 20.3% more read latency reduction, 26.5% and 16.1% more write latency reduction, 24.3% and 15.6% more running time decrease, 1.32× and 0.92× more speedup, as well as 30.6% and 18.4% more energy consumption reduction on average compared with the state-of-the-art FNW and two-stage-write write schemes, respectively.	computer data storage;parsec;parallel computing;phase-change memory;power supply;speedup;time complexity	Zheng Li;Fang Wang;Dan Feng;Yu Hua;Jingning Liu;Sheng Zhong	2016	TACO	10.1145/3012007	pulse-code modulation;power budget;parallel computing;real-time computing;computer hardware;computer science	Arch	-8.49228616297879	53.99783956848496	61908
28e8eab778dd635a86c0b6e8eed0ab52b762f8bf	performance evaluation of dynamic speculative multithreading with the cascadia architecture	instruction level parallel;speculative multithreading;thread level parallelism;multithreading hardware parallel processing multicore processing computer architecture program processors programming profession computer society application software;multi threading;computer society;yarn;performance evaluation;high performance superscalar processor;application software;sequential program;loop nesting level;nested loops;simulation;multithreading processors;speculative multithreading multithreading processors multicore processors simulation;computer architecture;instruction set extension;programming profession;multicore processing;dynamic speculative multithreading;loop nesting level dynamic speculative multithreading cascadia architecture thread level parallelism instruction level parallelism high performance superscalar processor sequential program d spmt multicore architecture loop tree;superscalar processor;multicore processors;d spmt multicore architecture;multiprocessing systems;loop tree;instruction level parallelism;cascadia architecture;high performance;program processors;parallel processing;multi threading instruction sets multiprocessing systems;instruction sets;hardware;multithreading	Thread-level parallelism (TLP) has been extensively studied in order to overcome the limitations of exploiting instruction-level parallelism (ILP) on high-performance superscalar processors. One promising method of exploiting TLP is dynamic speculative multithreading (D-SpMT), which extracts multiple threads from a sequential program without compiler support or instruction set extensions. This paper introduces Cascadia, a D-SpMT multicore architecture that provides multigrain thread-level support and is used to evaluate the performance of several benchmarks. Cascadia applies a unique sustainable IPC (sIPC) metric on a comprehensive loop tree to select the best performing nested loop level to multithread. This paper also discusses the relationships that loops have on one another, in particular, how loop nesting levels can be extended through procedures. In addition, a detailed study is provided on the effects that thread granularity and interthread dependencies have on the entire system.	benchmark (computing);central processing unit;compiler;control flow;instruction-level parallelism;multi-core processor;multithreading (computer architecture);parallel computing;performance evaluation;specfp;simultaneous multithreading;speculative execution;speculative multithreading;speedup;subroutine;superscalar processor;task parallelism;thread (computing)	David A. Zier;Ben Lee	2010	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2009.47	multi-core processor;parallel processing;computer architecture;parallel computing;real-time computing;multithreading;computer science;operating system	Arch	-6.511712179128745	47.66350787761332	61957
7d8e19a7c96135fe679bad9b61b3e61a74519dc1	tplcr: time-bound, pre-copy live checkpointing and parallel restart of virtual machines using distributed memory servers	virtualization;checkpointing servers virtual machine monitors computers instruction sets virtual machining random access memory;checkpoint restart;fault tolerance checkpoint restart virtualization;virtual machines checkpointing distributed memory systems fault tolerant computing network servers;fault tolerance;memory intensive class d nas parallel benchmark kernels tplcr parallel restart virtual machines time bound pre copy live checkpointing and parallel re start mechanism multiple distributed memory servers in memory checkpointing cpu	Live checkpointing of virtual machines is the ability to save the state of a virtual machine to storage while the machine is running. This paper presents a novel Time-bound, Pre-Copy Live Checkpointing and parallel Re-start mechanism (TPLCR) that implements live checkpointing based on a time-bounded, pre-copy live migration algorithm. The performance improvements of TPLCR rely on the use of multiple Distributed Memory Servers to allow fast, in-memory checkpointing and parallel restart. Along with the new TPLCR protocol, we introduce the Checkpoint-Restart Service to manage the checkpoint and restart operations in a datacenter. This paper describes a prototype implementation of TPLCR based on KVM. A series of checkpointing experiments were conducted using four CPU and memory intensive Class D NAS Parallel Benchmark kernels. Experimental results show that TPLCR checkpoint-restart performance is significantly better than traditional approaches.	algorithm;application checkpointing;benchmark (computing);central processing unit;computation;data center;distributed memory;downtime;experiment;hypervisor;in-memory database;parallel computing;parallel programming model;prototype;transaction processing system;virtual machine;z/vm	Kasidit Chanchio	2015	2015 Third International Symposium on Computing and Networking (CANDAR)	10.1109/CANDAR.2015.108	parallel computing;real-time computing;computer science;distributed computing	HPC	-16.383628197027573	51.29330322142001	62012
0c8ea46a26494c0a65668b7d2ed555ab621aaaf9	power management for real-time embedded systems on block-partitioned multicore platforms	power supplies;energy efficiency;power supply circuits;complexity theory;energy efficient;power efficiency;chip multiprocessor;real time embedded system;real time embedded systems;embedded system;power supply;chip;power supply circuits embedded systems microprocessor chips multiprocessing systems power aware computing;embedded systems;power aware computing;computational modeling;block configuration power management real time embedded systems block partitioned multicore platforms energy efficient architecture chip multiprocessor real time systems power supply voltage energy efficiency;magnetic cores energy efficiency computational modeling power supplies real time systems complexity theory time frequency analysis;power system management;multicore processing;voltage;power management;manufacturing;block configuration;energy efficient architecture;block partitioned multicore platforms;multiprocessing systems;magnetic cores;frequency;power supply voltage;time frequency analysis;energy management;microprocessor chips;real time systems	Power management has become a very important research area and various approaches have been proposed. As an energy-efficient architecture, chip multiprocessor (CMP) has been widely adopted by chip manufacturers. In this paper, we study power management schemes for real-time systems on block-partitioned multicore platforms, where the processing cores are grouped into different blocks and cores on one block share the same power supply voltage (thus have the same frequency). We evaluate the energy efficiency of different block configurations. Simulation results show that block-partitioned CMP has its inherent advantages to fulfill the goal of power efficiency and low design complexity for future CMPs.	embedded system;multi-core processor;multiprocessing;performance per watt;power management;power supply;real-time clock;real-time computing;simulation	Xuan Qi;Dakai Zhu	2008	2008 International Conference on Embedded Software and Systems	10.1109/ICESS.2008.43	embedded system;parallel computing;real-time computing;computer science;efficient energy use	EDA	-4.549137057080394	57.2854739522247	62049
c218730430763d91e78ac819b1479b00c75a2e41	hybrid mpi: efficient message passing for multi-core systems	distributed load balancer;parallel processing application program interfaces distributed shared memory systems memory architecture message passing multiprocessing systems;epidemic algorithm;load balancing;reduced cache footprint hybrid mpi process multicore shared memory architectures multicore systems high performance computing hpc commodity systems distributed memory os level process separations shared memory nodes thread based mpi libraries process based approach shared memory communication shared memory message passing;message systems libraries receivers protocols random access memory message passing memory management	Multi-core shared memory architectures are ubiquitous in both High-Performance Computing (HPC) and commodity systems because they provide an excellent trade-off between performance and programmability. MPI's abstraction of explicit communication across distributed memory is very popular for programming scientific applications. Unfortunately, OS-level process separations force MPI to perform unnecessary copying of messages within shared memory nodes. This paper presents a novel approach that transparently shares memory across MPI processes executing on the same node, allowing them to communicate like threaded applications. While prior work explored thread-based MPI libraries, we demonstrate that this approach is impractical and performs poorly in practice. We instead propose a novel process-based approach that enables shared memory communication and integrates with existing MPI libraries and applications without modifications. Our protocols for shared memory message passing exhibit better performance and reduced cache footprint. Communication speedups of more than 26% are demonstrated for two applications.	distributed memory;library (computing);message passing interface;multi-core processor;operating system;shared memory	Andrew Friedley;Greg Bronevetsky;Torsten Hoefler;Andrew Lumsdaine	2013	2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)	10.1145/2503210.2503294	uniform memory access;distributed shared memory;memory footprint;shared memory;interleaved memory;computer architecture;parallel computing;distributed memory;computer science;load balancing;operating system;distributed computing;overlay;conventional memory;extended memory;flat memory model;spmd;data diffusion machine;computing with memory;cache-only memory architecture;memory map;non-uniform memory access;memory management	HPC	-10.530249791416177	47.05998634605105	62102
44ff6a8aff2e50409419b865d56b7b5c9b0f78bf	a novel network storage scheme: intelligent network disk storage cluster	resource utilization;network storage scheme;intelligent network disk storage cluster;resource allocation;resource management;intelligent networks network servers runtime resource management load management storage area networks system performance algorithm design and analysis throughput adaptive arrays;runtime;system performance;divide and conquer methods;storage area networks;file arp;network servers;load balancing scheme;farp;intelligent network;adaptive arrays;load management;load balance;intelligent networks;workstation clusters;workstation clusters divide and conquer methods file organisation resource allocation storage area networks;two rounds divide and conquer;divide and conquer;algorithm design and analysis;throughput;file organisation;load balancing scheme network storage scheme intelligent network disk storage cluster file arp farp two rounds divide and conquer	"""Single point of failure widely exists in current storage clusters. In this paper, a novel network storage scheme called intelligent network disk storage cluster (INDSC) is presented. Firstly, it proposes a model of alternate service based on system load, not only effectively solving the aforementioned problem, but also greatly improving system performance; Secondly, a new concept FARP (File ARP) that maps the file inode number to the corresponding storage node is introduced, and an algorithm named """";two rounds divide and conquer""""; is designed to quickly locate a file; Thirdly, it supports both single layout and stripe layout strategies simultaneously, which can achieve the maximum performance and throughput; Finally, it provides a runtime and idle-time load balancing scheme to balance I/O load and storage resource utilization. Experimental results indicate that for small request sizes single layout compares favorably to stripe layout. While for large request sizes, the latter outperforms the former."""	algorithm;data striping;disk storage;input/output;intelligent network;load (computing);load balancing (computing);reliability engineering;single point of failure;throughput	Wenfeng Wang;Yuelong Zhao	2008	2008 IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2008.4525199	embedded system;intelligent network;parallel computing;real-time computing;telecommunications;computer science;resource management;operating system;distributed computing;computer network	HPC	-16.15161066305684	57.1747026932265	62239
6dfcc6656fda915792a711d026bb96fdafa1db30	happy: hybrid address-based page policy in drams	page closure policy;memory systems;dram	Memory controllers have used static page closure policies to decide whether a row should be left open, open-page policy, or closed immediately, close-page policy, after the row has been accessed. The appropriate choice for a particular access can reduce the average memory latency. However, since application access patterns change at run time, static page policies cannot guarantee to deliver optimum execution time. Hybrid page policies have been investigated as a means of covering these dynamic scenarios and are now implemented in state-of-the-art processors. Hybrid page policies switch between open-page and close-page policies while the application is running, by monitoring the access pattern of row hits/conflicts and predicting future behavior. Unfortunately, as the size of DRAM memory increases, fine-grain tracking and analysis of memory access patterns does not remain practical.  We propose a compact memory address-based encoding technique which can improve or maintain the performance of DRAMs page closure predictors while reducing the hardware overhead in comparison with state-of-the-art techniques. As a case study, we integrate our technique, HAPPY, with a state-of-the-art Intel-adaptive monitor (e.g. part of the Intel Xeon X5650) and a traditional Hybrid page policy. We evaluate them across 70 memory intensive workload mixes consisting of single-thread and multi-thread applications. The experimental results show that using the HAPPY encoding applied to the Intel-adaptive page closure policy can reduce the hardware overhead by 5x for the evaluated 64 GB memory (up to 40× for a 512 GB memory) while maintaining the prediction accuracy.	cas latency;central processing unit;dynamic random-access memory;memory address;overhead (computing);run time (program lifecycle phase);thread (computing)	Mohsen Ghasempour;Aamer Jaleel;Jim D. Garside;Mikel Luján	2016		10.1145/2989081.2989101	zero page;uniform memory access;demand paging;embedded system;b-heap;interleaved memory;parallel computing;real-time computing;page fault;memory management unit;memory-mapped file;page replacement algorithm;computer hardware;telecommunications;computer science;physical address;virtual memory;operating system;conventional memory;flat memory model;registered memory;dram;algorithm;memory map;computer network	Arch	-9.412083370873832	53.10924263478834	62250
27a35b13625a3f767c178d0a8443a1002a35842b	exploiting data deduplication to accelerate live virtual machine migration	virtual machine;banking;virtualization;duplicated memory image data;memory management;fault tolerant;instruction sets encoding fingerprint recognition bandwidth memory management acceleration banking;resource allocation;hash based fingerprint;storage management;self similarity;redundant memory data elimination;data deduplication virtualization live migration;acceleration;storage area networks;data center;run time memory image;virtual machines;fingerprint recognition;live virtual machine migration;fault tolerance;power management;run length encoding;load balancing;bandwidth;data deduplication;load balance;workstation clusters;run length encode;encoding;system maintenance;redundant memory data elimination data deduplication live virtual machine migration load balancing power management fault tolerance system maintenance data centers duplicated memory image data self similarity run time memory image hash based fingerprint run length encode;data transfer;workstation clusters resource allocation storage area networks storage management virtual machines;instruction sets;live migration;data centers	As one of the key characteristics of virtualization, live virtual machine (VM) migration provides great benefits for load balancing, power management, fault tolerance and other system maintenance issues in modern clusters and data centers. Although Pre-Copy is a widespread used migration algorithm, it does transfer a lot of duplicated memory image data from source to destination, which results in longer migration time and downtime. This paper proposes a novel VM migration approach, named Migration with Data Deduplication (MDD), which introduces data deduplication into migration. MDD utilizes the self-similarity of run-time memory image, uses hash based fingerprints to find identical and similar memory pages, and employs Run Length Encode (RLE) to eliminate redundant memory data during migration. Experiment demonstrates that compared with Xen's default Pre-Copy migration algorithm, MDD can reduce 56.60% of total data transferred during migration, 34.93% of total migration time, and 26.16% of downtime on average.	algorithm;data center;data deduplication;data rate units;disk storage;downtime;encode;fault tolerance;fingerprint;gigabyte;load balancing (computing);megabit;model-driven engineering;power management;run time (program lifecycle phase);run-length encoding;self-similarity;system migration;virtual machine	Xiang Zhang;Zhigang Huo;Jie Ma;Dan Meng	2010	2010 IEEE International Conference on Cluster Computing	10.1109/CLUSTER.2010.17	data center;fault tolerance;parallel computing;real-time computing;computer science;virtual machine;load balancing;operating system;computer network	HPC	-15.32198173631778	52.46237755548657	62362
a99f70fb09e8224a0e9e7fe2b84f85d5fd7d8ae8	achieving high instruction cache performance with an optimizing compiler	fault tolerance;system performance;majority voting;compiler optimization;hardware;bandwidth;cache memory;aerospace materials;optimizing compiler;encoding	Increasing the execution power requires a high instruction issue bandwidth, and decreasing instruction encoding and applying some code improving techniques cause code expansion. Therefore, the instruction memory hierarchy performance has become an important factor of the system performance. An instruction placement algorithm has been implemented in the IMPACT-I (Illinois Microarchitecture Project using Advanced Compiler Technology - Stage I) C compiler to maximize the sequential and spatial localities, and to minimize mapping conflicts. This approach achieves low cache miss ratios and low memory traffic ratios for small, fast instruction caches with little hardware overhead. For ten realistic UNIX* programs, we report low miss ratios (average 0.5%) and low memory traffic ratios (average 8%) for a 2048-byte, direct-mapped instruction cache using 64-byte blocks. This result compares favorably with the fully associative cache results reported by other researchers. We also present the effect of cache size, block size, block sectoring, and partial loading on the cache performance. The code performance with instruction placement optimization is shown to be stable across architectures with different instruction encoding density.	cpu cache;optimizing compiler	Wen-mei W. Hwu;Pohua P. Chang	1989		10.1109/ISCA.1989.714559	computer architecture;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;cache invalidation;operating system;instruction set;optimizing compiler;computer performance;smart cache;instruction scheduling;programming language;cache algorithms;cache pollution	Arch	-8.246265380859843	51.72591009924334	62427
8d8339f6e0adaa556643c4c9e721e0a17df41348	towards scheduling hard real-time image processing tasks on a single gpu		Graphics Processing Units (GPU) are becoming the key hardware accelerators in the emerging image processing applications such as self-driving cars and mobile augmented reality systems. As GPUs execute launched workloads non-preemptively, their usage in safety-critical systems with hard real-time constraints is impeded. The existing solutions for scheduling real-time tasks on a single GPU focus on soft real-time systems. In this paper, we consider real-time systems with a single dedicated GPU handling sporadic tasks with hard deadlines and propose a scheduling approach based on time division multiplexing called the GPU-TDMh — a lightweight middleware framework located between the application and the GPU driver layers. We evaluate the proposed approach on a matrix multiplication benchmark on a heterogeneous platform. The experiments demonstrate the effectiveness of our method as well as superiority over the non-preemptive online scheduling policies.	augmented reality;autonomous car;benchmark (computing);experiment;graphics processing unit;hardware acceleration;image processing;matrix multiplication;middleware;multiplexing;real-time clock;real-time computing;real-time locating system;real-time transcription;scheduling (computing)	Vladislav Golyanik;Mitra Nasri;Didier Stricker	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8297110	image processing;computer vision;parallel computing;task analysis;matrix multiplication;time-division multiplexing;artificial intelligence;scheduling (computing);augmented reality;computer science;middleware;server	Embedded	-8.224256755163251	57.98022455895092	62433
42a75596280f5fa9f5e9e3a285a953a514e39637	program analysis and transformations for fast data sharing	data sharing;eager dsm systems fast data sharing distributed shared memory systems message passing systems shared values local memory system performance compile time analysis model parallelized loop structures;distributed memory systems;multiprocessing programs;computer science delay prefetching hardware scalability system performance parallel programming parallel processing superconducting logic circuits contracts;parallel programming;shared memory systems;message passing;program analysis;message passing distributed memory systems shared memory systems program compilers parallel programming multiprocessing programs;program compilers;distributed shared memory	Distributed Shared Memory (DSM) systems have been proposed to combine the programmability of traditional shared memory and the scalability of message-passing systems. Eager DSM systems can greatly reduce access latencies for remote data by keeping copies of shared values in local memory and updating them immediately whenever a shared datum changes. However , sharing all changes globally can limit the system performance. It is usually possible to transform a program into an equivalent form that generates much less traac, and therefore executes much more eeciently. This paper describes a compile-time analysis model for transforming simple shared memory programs with parallelized loop structures into programs that are optimized for eecient execution on eager DSM systems.	access time;compile time;compiler;distributed shared memory;geodetic datum;message passing;parallel computing;program analysis;scalability	Ai Li;Gudjon Hermannsson;Larry D. Wittie	1994		10.1109/HPDC.1994.340251	program analysis;shared disk architecture;uniform memory access;distributed shared memory;shared memory;memory model;interleaved memory;computer architecture;parallel computing;message passing;distributed memory;computer science;operating system;distributed computing;overlay;conventional memory;extended memory;flat memory model;programming language;registered memory;data diffusion machine;memory map;non-uniform memory access;memory management;supercomputer architecture	HPC	-10.786868343751955	50.144531196337155	62530
dd7fed3b6cf4a59d224130e3e818f35e0de6cd1d	perasure: a parallel cauchy reed-solomon coding library for gpus	error recovery;paper;encoding graphics processing units decoding kernel instruction sets bandwidth libraries;quad core modern cpu gpu parallel cauchy reed solomon coding library perasure erasure coding large scale cloud storage systems graphics processing units crs coding library nvidia gtx780 card multithreaded jerasure;reed solomon codes cloud computing graphics processing units multiprocessing systems;cuda;nvidia;computer science;nvidia geforce gtx 780	In recent years, erasure coding has been adopted by large-scale cloud storage systems to replace data replication. With the increase of disk I/O throughput and network bandwidth, the speed of erasure coding becomes one of the key system bottlenecks. In this paper, we propose to offload the task of erasure coding to Graphics Processing Units (GPUs). Specifically, we have designed and implemented PErasure, a parallel Cauchy Reed-Solomon (CRS) coding library. We compare the performance of PErasure with that of two state-of-the-art libraries: Jerasure (for CPUs) and Gibraltar (for GPUs). Our experiments show that the raw coding speed of PErasure on a $500 Nvidia GTX780 card is about 10 times faster than that of multithreaded Jerasure on a quad-core modern CPU, and 2-4 times faster than Gibraltar on the same GPU. PErasure can achieve up to 10GB/s of overall encoding speed using just a single GPU for a large storage system that can withstand up to 8 disk failures.	central processing unit;cloud storage;computer data storage;erasure code;experiment;graphics processing unit;input/output;key;library (computing);multi-core processor;pci express;reed–solomon error correction;replication (computing);thread (computing);throughput	Xiaowen Chu;Chengjian Liu;Kai Ouyang;Ling Sing Yung;Yiu-Wing Leung	2015	2015 IEEE International Conference on Communications (ICC)	10.1109/ICC.2015.7248360	parallel computing;computer hardware;computer science;operating system;general-purpose computing on graphics processing units;computer graphics (images)	HPC	-15.140771497922795	52.27943473751029	62554
4b1eec831aef55921b4c9c7618f8fc8e67e0829a	sharp hash: a high-performing distributed hash for extreme-scale systems		A high-performing distributed hash is critical for achieving performance in many applications and system software using extreme-scale systems. It is also a central part of many Big-Data frameworks including Memcached, file systems, and job schedulers. However, there is a lack of high-performing distributed hash implementations. In this work, we propose, design, and implement, SharP Hash, a high-performing, RDMA-based distributed hash for extreme-scale systems. SharP Hash's high performance is obtained through the use of high-performing networks and one-sided semantics. We perform an evaluation of SharP Hash and demonstrate its performance characteristics with a synthetic micro-benchmark and implementation of a Key Value (KV) store, Memcached.	benchmark (computing);big data;dhrystone;ibm websphere extreme scale;memcached;remote direct memory access	Zachary W. Parchman;Ferrol Aderholdt;Manjunath Gorentla Venkata	2017	2017 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2017.104	parallel computing;distributed computing;sha-2;computer science;secure hash standard;double hashing;hash chain;fowler–noll–vo hash function;mdc-2;hash function;hash list	HPC	-18.760345254208715	52.662756328689575	62587
a49a972402c105eaac8e582082161d845675d8bd	a fully persistent and consistent read/write cache using flash-based general ssds for desktop workloads	secondary storage	The flash-based SSD is used as a tiered cache between RAM and HDD. Conventional schemes do not utilize the nonvolatile feature of SSD and cannot cache write requests. Writes are a significant, or often dominant, fraction of storage workloads. To cache write requests, the SSD cache should persistently and consistently manage its data and metadata, and guarantee no data loss even after a crash. Persistent cache management may require frequent metadata changes and causes high overhead. Some researchers insist that a nonvolatile persistent cache requires new additional primitives that are not supported by general SSDs in the market. We proposed a fully persistent read/write cache, which improves both read and write performance, does not require any special primitive, has a low overhead, guarantees the integrity of the cache metadata and the consistency of the cached data, even during a crash or power failure, and is able to recover the flash cache quickly without any data loss. We implemented the persistent read/write cache as a block device driver in Linux. Our scheme aims at virtual desktop infra servers. So the evaluation was performed with massive, real desktop traces of five users for ten days. The evaluation shows that our scheme outperforms an LRU version of SSD cache by 50% and the read-only version of our scheme by 37%, on average, for all experiments. This paper describes most of the parts of our scheme in detail. Detailed pseudo-codes are included in the Appendix.	desktop computer	Sung Hoon Baek;Ki-Woong Park	2016	Inf. Syst.	10.1016/j.is.2016.02.002	bus sniffing;auxiliary memory;least frequently used;pipeline burst cache;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cache;computer science;write-once;cache invalidation;operating system;database;adaptive replacement cache;write buffer;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol	OS	-12.683933613378018	53.58918498940145	62631
a22080f1a7f54317b24d4176b8810446665dd8d1	architectural support for software transactional memory	transactional semantics;conflict detection;nested transaction;storage management;hardware transactional memory;hardware accelerated software transactional memory;hardware accelerator;instruction set architecture;synchronisation;stock processor;hardware accelerated software transactional memory concurrency control lock based synchronization hardware transactional memory transactional semantics stock processor instruction set architecture;hardware acceleration instruction sets proposals concurrency control runtime yarn programming profession optimizing compilers object detection;software transactional memory;data structures;concurrency control;transaction processing concurrency control data structures instruction sets storage management synchronisation;lock based synchronization;transaction processing;transactional memory;high performance;instruction sets	Transactional memory provides a concurrency control mechanism that avoids many of the pitfalls of lock-based synchronization. Researchers have proposed several different implementations of transactional memory, broadly classified into software transactional memory (STM) and hardware transactional memory (HTM). Both approaches have their pros and cons: STMs provide rich and flexible transactional semantics on stock processors but incur significant overheads. HTMs, on the other hand, provide high performance but implement restricted semantics or add significant hardware complexity. This paper is the first to propose architectural support for accelerating transactions executed entirely in software. We propose instruction set architecture (ISA) extensions and novel hardware mechanisms that improve STM performance. We adapt a high-performance STM algorithm supporting rich transactional semantics to our ISA extensions (called hardware accelerated software transactional memory or HASTM). HASTM accelerates fully virtualized nested transactions, supports language integration, and provides both object-based and cache-line based conflict detection. We have implemented HASTM in an accurate multi-core IA32 simulator. Our simulation results show that (1) HASTM single-thread performance is comparable to a conventional HTM implementation; (2) HASTM scaling is comparable to a STM implementation; and (3) HASTM is resilient to spurious aborts and can scale better than HTM in a multi-core setting. Thus, HASTM provides the flexibility and rich semantics of STM, while giving the performance of HTM.	algorithm;cpu cache;central processing unit;concurrency (computer science);concurrency control;file synchronization;html;ia-32;image scaling;multi-core processor;object-based language;simulation;software transactional memory;thread (computing)	Bratin Saha;Ali-Reza Adl-Tabatabai;Quinn Jacobson	2006	2006 39th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO'06)	10.1109/MICRO.2006.9	computer architecture;transactional memory;parallel computing;real-time computing;data structure;computer science;operating system;software transactional memory;instruction set;programming language	Arch	-14.2309729604537	49.013714688114895	62637
8cb27ef68ffcbae823306a8a5221ecc7554e3280	bounding completion times of jobs with arbitrary release times and variable execution times	workload;priority driven basis;arbitrary release times;scheduling real time systems software performance evaluation resource allocation computational complexity;iterative algorithms;processor scheduling;resource allocation;upper bounds;performance;simulation;software performance evaluation;complexity;linear precedence constraints;upper bound;scheduling algorithm;simulation job completion time bounding arbitrary release times variable execution times real time systems workload linear precedence constraints upper bounds scheduling priority driven basis performance complexity;job completion time bounding;iterative algorithms scheduling algorithm upper bound timing real time systems algorithm design and analysis processor scheduling delay sun computer science;computational complexity;scheduling;sun;precedence constraint;variable execution times;computer science;job scheduling;algorithm design and analysis;real time systems;timing	In many real-time systems, the workload can be characterized as a set of jobs with linear precedence constraints among them. Jobs often have variable execution times and arbitrary release times. We describe here three algorithms that can be used to compute upper bounds on completion times of such jobs scheduled on a priority-driven basis. The algorithmshave different performance and complexity. Simulation was performed to compare their performance.	algorithm;job (computing);job stream;real-time clock;real-time computing;simulation	Jun Sun;Jane W.-S. Liu	1996		10.1109/REAL.1996.563692	parallel computing;real-time computing;computer science;operating system;distributed computing;scheduling	Embedded	-12.03862332205205	60.422086076080795	62790
57f2fa996940a36a3282be7f094c75e6e20e14fc	properties of dynamically dead instructions for contemporary architectures	sequential program performance dead instructions contemporary architectures processor frequency scaling sequential program speed single threaded program speed hardware and software system developer sequential software performance dynamically dead instruction executed instruction dynamic instruction stream contemporary programs solution mechanism robust compiler portable compiler gcc based framework contemporary x86 based machine arm based machines architectural feature static program instruction instruction window static context information ddi detection ddi elimination techniques;compiler;computer architecture;registers;software performance evaluation program compilers reduced instruction set computing;optimization;compiler dynamically dead instructions architecture;architecture;benchmark testing context program processors registers hardware optimization computer architecture;program processors;dynamically dead instructions;context;benchmark testing;hardware	Processor frequency scaling has greatly stagnated over the last few years, making it difficult to continue improving sequential or single-threaded program speed. Hardware and software system developers now need to devise innovative and aggressive schemes to grow sequential software performance. The goal of this work is to assess the potential and feasibility of eliminating dynamically dead instructions (DDI) -- where the results of executed instructions are not used by the program -- to benefit program speed. Specifically, we quantify the ratio of DDI in the dynamic instruction stream for different classes of contemporary programs (general-purpose vs. embedded) and architectures (CISC vs. RISC), and explore characteristics of DDI to assist the design of effective solution mechanisms. To achieve our goal, we develop a robust and portable compiler (GCC) based framework for DDI research, and target this investigation at contemporary x86 and ARM based machines. We find that while a substantial fraction of instructions executed by all classes of programs are dynamically dead, architectural features show a visible impact. Our experiments reveal that a handful of static program instructions contribute a majority of DDI. We further find that DDI are often highly predictable, can be detected within small instruction windows, and a small amount of static context information can significantly benefit DDI detection at run-time. Thus, our research can induce the development and adoption of practical DDI elimination techniques to scale sequential program performance in future processors.	central processing unit;compiler;embedded system;experiment;frequency scaling;general-purpose macro processor;image scaling;microsoft windows;software performance testing;software system;thread (computing);x86	Marianne J. Jantz;Katherine Wu;Prasad A. Kulkarni	2014	2014 17th Euromicro Conference on Digital System Design	10.1109/DSD.2014.108	embedded system;computer architecture;compiler;parallel computing;real-time computing;computer science;architecture;operating system	Arch	-4.688831455759569	49.62349930985177	62936
1a13966d219eed1129fcd9a27dec6eb2c3816077	competitive dynamic multiprocessor allocation for parallel applications	competitive dynamic multiprocessor allocation;memory management;execution time;application software;reallocation;processor scheduling;resource allocation;lower bounds;upper bounds;processor scheduling parallel processing dynamic scheduling information analysis application software scheduling algorithm computer science costs algorithm design and analysis memory management;scheduling algorithm;makespan minimisation;parallel jobs;upper bounds competitive dynamic multiprocessor allocation parallel applications parallel jobs execution time makespan minimisation parallel job reallocation competitive ratio lower bounds;scheduling;competitive analysis;upper and lower bounds;multiprocessing systems;computer science;parallel job;information analysis;resource allocation multiprocessing systems scheduling processor scheduling;algorithm design and analysis;parallel applications;parallel processing;dynamic scheduling;competitive ratio	In this paper we use competitive analysis to study preemptive multiprocessor allocation policies for parallel jobs whose execution time is not known to the scheduler at the time of scheduling. The objective is to minimize the makespan (i.e., the completion time of the last job to finish executing). We characterize a parallel job, Ji , by two parameters: its execution time, li , and its parallelism, Pi , which may vary over time. The preemption and reallocation of processors can take place at any time. We devise a preemptive policy which achieves the best possible competitive ratio and then derive upper and lower bounds for scheduling N parallel jobs on P processors.	central processing unit;competitive analysis (online algorithm);job stream;makespan;multiprocessing;parallel computing;preemption (computing);run time (program lifecycle phase);scheduling (computing)	Tim Brecht;Xiaotie Deng;Nian Gu	1995		10.1109/SPDP.1995.530717	competitive analysis;parallel processing;parallel computing;real-time computing;computer science;operating system;distributed computing;scheduling;multiprocessor scheduling	Theory	-12.957499772314364	60.331010664790924	62948
6b6033d761558ad3867a91821f73c6a0deb66309	reducing biased lock revocation by learning	virtual machine;concurrent language;learning;lock protocol;java virtual machine;lock reservation;system performance;rebias;atomicity;lock policy;synchronization;biased locking;high performance;rogers lock;monitor;entry and exit	For languages supporting concurrency the implementation of synchronization primitives is important for achieving high-performance. Many concurrent languages use object based locks to control access to critical regions. When lock ownership doesn't change for most of its lifetime, lock biasing allows a thread to take ownership of an object so that atomic operations aren't necessary on lock entry and exit. Revoking ownership of locks biased to a thread is an expensive operation compared to the atomic operation, as the thread that holds the lock must be suspended.  When lock revocation occurs it is common for the object being locked to be modified so that future lock attempts use atomic operations. When repeated revocations occur the locking policy can reduce the amount of lock biasing that the system performs. Factors that can drive this include the type of the object being revoked and how recently it was allocated. The system must achieve a balance between being pessimistic about biased lock use and avoiding revocations.  This work introduces a new locking protocol where revocations can be sampled by the locker without having to bias. The mechanism provides locking information specific to a particular instance that can be used to avoid unprofitable bias lock speculation and create a better locking policy. We demonstrate a new instance specific locking policy implemented in the Zing Virtual Machine, an extension of the HotSpot Java Virtual Machine. We present results on how the sampling window effects the number of atomic lock operations and revocations for the SPECjvm2008 and DaCapo Bach benchmark suites.	benchmark (computing);biasing;concurrency (computer science);java hotspot virtual machine;java virtual machine;linearizability;lock (computer science);object-based language;sampling (signal processing);synchronization (computer science);vendor lock-in	Ian Rogers;Balaji Iyengar	2011		10.1145/2069172.2069179	double-checked locking;giant lock;lock;lock;embedded system;real-time computing;spinlock;ticket lock;computer science;readers–writer lock;distributed computing;lock convoy	PL	-15.532476824684336	48.3496461286563	62978
5cdd7ad068f825ba69f3653509e7617d58ea8d52	design-to-criteria scheduling: real-time agent control	resource constraint;real time;soft constraints;temporal constraints;problem solving	Design-to-Criteria builds custom schedules for agents tha t meet hard temporal constraints, hard resource constraints, and soft constraints stemming from soft task interactions or soft co mmitments made with other agents. Design-to-Criteria is design d specifically for online application, it copes with exponent ial combinatorics to produce these custom schedules in a resour ce bounded fashion. This enables agents to respond to changes i problem solving or the environment as they arise.	common criteria;interaction;problem solving;real-time transcription;stemming;web application	Thomas Wagner;Victor R. Lesser	2000		10.1007/3-540-47772-1_12	mathematical optimization;real-time computing;computer science	AI	-9.830511021426556	59.37435549312407	63292
a80a98fb037b763d825ec9a01dd44c592735f9cf	an evaluation of user-level failure mitigation support in mpi	fault tolerance;mpi	As the scale of computing platforms becomes increasingly extreme, the requirements for application fault tolerance are increasing as well. Techniques to address this problem by improving the resilience of algorithms have been developed, but they currently receive no support from the programming model, and without such support, they are bound to fail. This paper discusses the failure-free overhead and recovery impact aspects of the User-Level Failure Mitigation proposal presented in the MPI Forum. Experiments demonstrate that fault-aware MPI has little or no impact on performance for a range of applications, and produces satisfactory recovery times when there are failures.	message passing interface	Wesley Bland;Aurelien Bouteiller;Thomas Hérault;Joshua Hursey;George Bosilca;Jack J. Dongarra	2012		10.1007/978-3-642-33518-1_24	real-time computing;simulation;engineering;distributed computing	HPC	-17.418100583451885	49.00366521397174	63389
80da6fd7638b8084a0638d40718ad0d0b61061bd	a lightweight, gpu-based software raid system	arrays encoding linux graphics processing unit hardware kernel;storage management computer graphic equipment coprocessors linux raid;graphics processing unit gpu based software raid system reliable secondary storage infrastructure linux operating system;error recovery;kernel;general and miscellaneous mathematics computing and information science;paper;reliable secondary storage infrastructure;fault tolerant;fault simulation;raid tp;gpu based software raid system;storage management;raid;computer graphic equipment;coprocessors;tesla c1060;cuda;arrays;error correction code;operating system;fault tolerance;raid tp raid graphics processing unit fault tolerance reed solomon code raid 6;high performance computer;computer codes;nvidia;graphic processing unit;linux;computer science;linux operating system;high throughput;graphics processing unit;encoding;programming;reed solomon code;parallel processing;software implementation;hardware;raid 6	While RAID is the prevailing method of creating reliable secondary storage infrastructure, many users desire more flexibility than offered by current implementations. Traditionally, RAID capabilities have been implemented largely in hardware in order to achieve the best performance possible, but hardware RAID has rigid designs that are costly to change. Software implementations are much more flexible, but software RAID has historically been viewed as much less capable of high throughput than hardware RAID controllers. This work presents a system, Gibraltar RAID, that attains high RAID performance by offloading the calculations related to error correcting codes to GPUs. This paper describes the architecture, performance, and qualities of the system. A comparison to a well-known software RAID implementation, the md driver included with the Linux operating system, is presented. While this work is presented in the context of high performance computing, these findings also apply to a general RAID market.	auxiliary memory;code;computer data storage;forward error correction;graphics processing unit;linux;operating system;raid;supercomputer;throughput	Matthew L. Curry;Lee Ward;Anthony Skjellum;Ron Brightwell	2010	2010 39th International Conference on Parallel Processing	10.1109/ICPP.2010.64	disk data format;parallel processing;fault tolerance;parallel computing;computer hardware;computer science;disk array controller;operating system;non-standard raid levels;non-raid drive architectures;parity drive;standard raid levels;raid;raid processing unit	HPC	-15.192646445765615	51.79162204643485	63670
e4e26822589aa524c79b529e43cbb10da8dba46c	improving the memory efficiency of in-memory mapreduce based hpc systems		In-memory cluster computing systems based MapReduce, such as Spark, have made a great impact in addressing all kinds of big data problems. Given the overuse of memory speed, which stems from avoiding the latency caused by disk I/O operations, some process designs may cause resource inefficiency in traditional high performance computing (HPC) systems. Hash-based shuffle, particularly large-scale shuffle, can significantly affect job performance through excessive file operations and unreasonable use of memory. Some intermediate data unnecessarily overflow to the disk when memory usage is unevenly distributed or when memory runs out. Thus, in this study, Write Handle Reusing is proposed to fully utilize memory in shuffle file writing and reading. Load Balancing Optimizer is introduced to ensure the even distribution of data processing across all worker nodes, and Memory-Aware Task Scheduler that coordinates concurrency level and memory usage is also developed to prevent memory spilling. Experimental results on representative workloads demonstrate that the proposed approaches can decrease the overall job execution time and improve memory efficiency.	mapreduce	Cheng Pei;Xuanhua Shi;Hai Jin	2015		10.1007/978-3-319-27119-4_12	parallel computing	HPC	-11.97705569256098	53.040949897052435	63831
d7f87ccc3b9bfca2cef9c41fcfecc95be58ca486	ntb branch predictor: dynamic branch predictor for high-performance embedded processors	branch prediction;saturating counter;pht;gbh;perceptron	Branch prediction accuracy becomes more crucial in high-performance embedded processors. The importance of branch prediction in embedded processors continues to grow in the future. Many branch predictors have been proposed to alleviate the performance penalty due to branch mispredictions. However, recent embedded processors still have problems in increasing the branch prediction accuracy. This paper proposes number of taken branch instructions (NTB) branch predictor, a new dynamic branch predictor for high-performance embedded processors. The NTB branch predictor utilizes two-bit saturating counters in the pattern history table based on the information about the number of taken-branches in the global branch history. The proposed NTB branch predictor achieves improved accuracy by making use of longer branch history with no hardware overhead, because hardware resources for the proposed NTB branch predictor are independent of the history length. By contrast, existing dynamic branch prediction schemes require more hardware resources as the history length increases. According to our experiments with a 4 KB branch predictor which suits embedded processors, the NTB branch predictor improves the prediction accuracy by 7.11 and 43.41 % on average over the perceptron predictor and the two-level adaptive branch predictor, respectively.	aliasing;branch (computer science);branch misprediction;branch predictor;central processing unit;embedded system;experiment;overhead (computing);perceptron;processor design	Cong Thuan Do;Hong Jun Choi;Dong Oh Son;Jong-Myon Kim;Cheol Hong Kim	2014	The Journal of Supercomputing	10.1007/s11227-014-1280-0	embedded system;parallel computing;real-time computing;branch target predictor;computer science;perceptron;branch predictor	Arch	-7.1976147756409645	51.759808917112	64128
afd3b824eedb30633a8ad16951b5c53b5e721b40	improving energy efficiency of gpus through data compression and compressed execution	energy efficiency;registers graphics processing units instruction sets message systems data compression context indexes;data compression;energy efficiency gpu architectures register file data compression;indexes;energy efficiency improvement data compression compressed execution register file size thread level parallelism gpu chip power register file data compression power efficiency improvement register file read operation register file write operation dynamic power reduction arithmetic differences thread register values value similarity data redundancy removal operand values data movement minimization processing minimization main execution unit total register file energy consumption total execution unit energy consumption;registers;message systems;gpu architectures;graphics processing units;register file;gpu architectures register file data compression energy efficiency;data compression graphics processing units multi threading power aware computing;context;instruction sets	GPU design trends show that the register file size will continue to increase to enable even more thread level parallelism. As a result register file consumes a large fraction of the total GPU chip power. This paper explores register file data compression for GPUs to improve power efficiency. Compression reduces the width of the register file read and write operations, which in turn reduces dynamic power. This work is motivated by the observation that the register values of threads within the same warp are similar, namely the arithmetic differences between two successive thread registers is small. Compression exploits the value similarity by removing data redundancy of register values. Without decompressing operand values some instructions can be processed inside register file, which enables to further save energy by minimizing data movement and processing in power hungry main execution unit. Evaluation results show that the proposed techniques save 25 percent of the total register file energy consumption and 21 percent of the total execution unit energy consumption with negligible performance impact.	data compression;data redundancy;execution unit;graphics processing unit;operand;parallel computing;performance per watt;register file;task parallelism	Sangpil Lee;Keunsoo Kim;Gunjae Koo;Hyeran Jeon;Murali Annavaram;Won Woo Ro	2017	IEEE Transactions on Computers	10.1109/TC.2016.2619348	data compression;database index;computer architecture;parallel computing;register window;computer hardware;control register;computer science;memory buffer register;operating system;register renaming;stack register;instruction set;index register;efficient energy use;processor register;register file;status register;memory data register;memory address register	Arch	-6.861410154193636	54.329468925815064	64134
a5362493ec1711645e7438cde8a0aa9bd8c58c40	cmg-t: windows pa: windows system performance measurement and analysis, parts 1-3			microsoft windows	Jeffry A. Schwartz	2009			performance measurement;embedded system;computer science	Metrics	-18.09388591439536	47.410719194799	64199
0d8c897d4577fb6223b8ea0924cf35bda5f0f1fb	fault-tolerant dynamic task mapping and scheduling for network-on-chip-based multicore platform		In Network-on-Chip (NoC)-based multicore systems, task allocation and scheduling are known to be important problems, as they affect the performance of applications in terms of energy consumption and timing. Advancement of deep submicron technology has made it possible to scale the transistor feature size to the nanometer range, which has enabled multiple processing elements to be integrated onto a single chip. On the flipside, it has made the integrated entities on the chip more susceptible to different faults. Although a significant amount of work has been done in the domain of fault-tolerant mapping and scheduling, existing algorithms either precompute reconfigured mapping solutions at design time while anticipating fault(s) scenarios or adopt a hybrid approach wherein a part of the fault mitigation strategy relies on the design-time solution. The complexity of the problem rises further for real-time dynamic systems where new applications can arrive in the multicore platform at any time instant. For real-time systems, the validity of computation depends both on the correctness of results and on temporal constraint satisfaction. This article presents an improved fault-tolerant dynamic solution to the integrated problem of application mapping and scheduling for NoC-based multicore platforms. The developed algorithm provides a unified mapping and scheduling method for real-time systems focusing on meeting application deadlines and minimizing communication energy. A predictive model has been used to determine the failure-prone cores in the system for which a fault-tolerant resource allocation with task redundancy has been performed. By selectively using a task replication policy, the reliability of the application, executing on a given NoC platform, is improved. A detailed evaluation of the performance of the proposed algorithm has been conducted for both real and synthetic applications. When compared with other fault-tolerant algorithms reported in the literature, performance of the proposed algorithm shows an average reduction of 56.95% in task re-execution time overhead and an average improvement of 31% in communication energy. Further, for time-constrained tasks, deadline satisfaction has also been achieved for most of the test cases by the developed algorithm, whereas the techniques reported in the literature failed to meet deadline in about 45% test cases.	algorithm;benchmark (computing);computation;constraint satisfaction;correctness (computer science);dhrystone;dynamical system;earliest deadline first scheduling;entity;fault tolerance;multi-core processor;network on a chip;overhead (computing);parallel computing;predictive modelling;real-time clock;real-time computing;replication (computing);run time (program lifecycle phase);scheduling (computing);simulation;slack variable;test case;transistor;very-large-scale integration	Navonil Chatterjee;Suraj Paul;Santanu Chattopadhyay	2017	ACM Trans. Embedded Comput. Syst.	10.1145/3055512	real-time computing;parallel computing;fault tolerance;scheduling (computing);earliest deadline first scheduling;multi-core processor;dynamic priority scheduling;network on a chip;computer science;resource allocation;constraint satisfaction;distributed computing	Embedded	-5.964064410621498	57.865237297465285	64335
b4a5f1fe2dfbf01353ff3b51f3b2e07653eaab6f	asymmetric memory extension for openshmem	hybrid programming;openshmem;threading	Memory allocation in Openshmem is a global operation that requires in-step calls from all Processing Elements (PEs). Although this approach works with applications that split the work evenly, it prevents using Openshmem in cases where the workload and the memory it uses are allocated dynamically and can change significantly while the application is running. To broaden the cases where Openshmem can be used, we propose an extension -- asymmetric memory support. The extension allows PEs to allocate memory independently, and make this memory available for remote access from other PEs.	memory management;microprocessor	Latchesar Ionkov;Ginger Young	2014		10.1145/2676870.2676887	parallel computing;real-time computing;simulation;computer science	OS	-13.451814092382719	47.01037379382459	64452
39e365c2de1881cccb969d12864b904bdfcd2136	predictable communication protocol processing in real-time mach	operating systems computers protocols real time systems unix network operating systems scheduling;protocols;network operating systems;real time;multimedia systems;operating system;scheduling;communication protocol;fixed priority scheduling;end to end delay;protocols processor scheduling real time systems timing bandwidth steel ethernet networks kernel computer science multimedia systems;operating systems computers;unix;end to end delays predictable communication protocol processing real time mach scheduling distributed real time systems multimedia systems real time systems unpredictable priority inversion timing behavior rt mach operating system application control rt mach scheduling mechanisms fixed priority scheduling processor reservation protocol architecture synthetic workloads realistic distributed videoconferencing system;real time systems	Scheduling of many different kinds of activities takes place in distributed real-time and multimedia systems. It includes scheduling of computations, window services, filesystem management, I/O services and communication protocol processing. In this paper, we investigate the problem of scheduling communication protocol processing in real-time systems. Communication protocol processing takes a relatively substantial amount of time and if not structured correctly, unpredictable priority inversion and undesirable timing behavior can result to applications communicating with other processors but are otherwise scheduled correctly. We describe the protocol processing architecture in the RTMach operating system, which allows the timing of protocol processing to be under strict application control. An added benefit is also obtained in the form of higher performance. This scheduling architecture is consistent with the other RT-Mach scheduling mechanisms including fixed priority scheduling and processor reservation. The benefits of this protocol architecture are demonstrated both under synthetic workloads and in a realistic distributed videoconferencing system we have implemented in RTMach. End-to-end delays for both audio and video are as predicted even with other threads competing for the CPU and the network.1	aspectj;bandwidth management;central processing unit;communications protocol;computation;end-to-end principle;fixed-priority pre-emptive scheduling;input/output;interaction;network interface;operating system;priority inversion;real-time clock;real-time computing;real-time transcription;requirement;scheduling (computing);synthetic intelligence;windows rt	Chen Lee;Katsuhiko Yoshida;Clifford W. Mercer;Ragunathan Rajkumar	1996		10.1109/RTTAS.1996.509539	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;communications protocol;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;distributed computing;gain scheduling;least slack time scheduling;round-robin scheduling;priority ceiling protocol	Embedded	-10.477786715004592	59.97151006556917	64992
fce292b76d9bd425711f7195e4a1234777db2cff	evaluation of performance unfairness in numa system architecture	bandwidth sockets servers multiprocessor interconnection system on chip micromechanical devices virtual machining	NUMA (Non-uniform memory access) system architectures are commonly used in high-performance computing and datacenters. Within each architecture, a processor-interconnect is used for communication between the different sockets and examples of such interconnect include Intel QPI and AMD HyperTransport. In this work, we explore the impact of the processor-interconnect on overall performance—in particular, we explore the impact on performance fairness from the processor-interconnect arbitration. It is well known that locally-fair arbitration does not guarantee globally-fair bandwidth sharing as closer nodes receive more bandwidth in a multi-hop network. However, this paper is the first to demonstrate the opposite can occur in a commodity NUMA servers where remote nodes receive higher bandwidth (and perform better). This problem occurs because router micro-architectures for processor-interconnects commonly employ external concentration. While accessing remote memory can occur in any NUMA system, performance unfairness (or performance variation) is more critical in cloud computing and virtual machines with shared resources. We demonstrate how this unfairness creates significant performance variation when executing workload on the Xen virtualization platform. We then provide analysis using synthetic workloads to better understand the source of unfairness.	cloud computing;data center;dhrystone;electrical connection;experiment;fairness measure;high- and low-level;hypertransport;intel quickpath interconnect;non-uniform memory access;router (computing);scheduling (computing);simulation;supercomputer;uniform memory access;verification and validation;virtual machine	WonJun Song;Hyungjoon Jung;Jung Ho Ahn;Jae W. Lee;John Kim	2017	IEEE Computer Architecture Letters	10.1109/LCA.2016.2602876	embedded system;real-time computing;computer hardware;computer science	Arch	-9.499622400911125	56.69175400005235	65020
93686a65ab84630b09a3f891db64fe543f215265	penalty scheduling policy applying user estimates and aging for supercomputing centers		In this article we address the problem of scheduling on realistic high performance computing facilities using incomplete information about tasks execution times. We introduce a variation of our previous Penalty Scheduling Policy, including an aging scheme that increases the priority of jobs over time. User-provided runtime estimates are applied as in the original Penalty Scheduling Policy, but a realistic priority schema is proposed to avoid starvation. The experimental evaluation of the proposed scheduler is performed using real workload logs, and validated using a job scheduler simulator. We study different realistic workload scenarios to evaluate the performance of the Penalty Scheduling Policy with aging. The main results suggest that using the proposed scheduler with the aging scheme, the waiting time of jobs in the high performance computing facility is significantly reduced (up to 50% in average).	acm/ieee supercomputing conference;job scheduler;job stream;schedule (project management);scheduling (computing);supercomputer	Nestor Rocchetti;Miguel Da Silva;Sergio Nesmachnow;Andrei Tchernykh	2016		10.1007/978-3-319-57972-6_4	job scheduler;workload;complete information;scheduling (computing);computer science;real-time computing;distributed computing;supercomputer	HPC	-16.22383264612638	60.37048551679268	65061
25bf7adaceaebd86c45f5d5a24b6c0b92a4ec452	efficient service allocation in hardware using credit-controlled static-priority arbitration	static priority;hardware resource management regulators delay real time systems costs computer applications embedded computing memory management system on a chip;implementation;system on chip scheduling;real time;indexing terms;ccsp rate regulator efficient service allocation credit controlled static priority arbitration systems on chip shared resources hardware implementation heavily loaded resources credit controlled static priority arbiter static priority scheduler;arbitration;system on chip;scheduling;success rate;soc;ccsp;over allocation real time arbitration soc implementation ccsp;use case;high speed;hardware implementation;over allocation;real time systems	Resources in contemporary systems-on-chip (SoC) are shared between applications to reduce cost. Access to shared resources is provided by arbiters that require a small hardware implementation and must run at high speed. To manage heavily loaded resources, such as memory channels, it is also important that the arbiter minimizes over allocation. A Credit-Controlled Static-Priority (CCSP) arbiter comprised of a rate regulator and a static-priority scheduler has been proposed for scheduling access to SoC resources. The proposed rate regulator, however, is not straight-forward to implement in hardware, and assumes that service is allocated with infinite precision. In this paper, we introduce a fast and small hardware implementation of the CCSP rate regulator and formally prove its correctness. We also show an efficient way of representing the allocated service in hardware with finite precision. Based on this representation, we define and evaluate two allocation strategies, and derive tight bounds on their respective over allocations. We show that increasing the precision of the implementation results in an exponential reduction in maximum over allocation at the cost of a linear increase in area. We demonstrate that the allocation strategy has a large impact on the allocation success rate for use cases with high load. Finally, we compare CCSP to traditional frame-based approaches and conclude that having a fine allocation granularity that is decoupled from latency is essential to manage highly loaded resources in real-time systems.	approximation algorithm;arbiter (electronics);correctness (computer science);credit bureau;real-time clock;real-time computing;requirement;scheduling (computing);system on a chip;time complexity	Benny Akesson;Elisabeth F. M. Steffens;Kees G. W. Goossens	2009	2009 15th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications	10.1109/RTCSA.2009.13	system on a chip;embedded system;parallel computing;real-time computing;computer science;operating system	Embedded	-8.891323431706878	59.00806751199065	65179
24ab28ccd3df6b289eb265c1c0fc00559ca48b12	resource-aware task scheduling	resource contention;dependency aware;task parallel;software engineering;conference report;taskparallel;programvaruteknik;dynamic scheduling	Dependency-aware task-based parallel programming models have proven to be successful for developing efficient application software for multicore-based computer architectures. The programming model is amenable to programmers, thereby supporting productivity, whereas hardware performance is achieved through a runtime system that dynamically schedules tasks onto cores in such a way that all dependencies are respected. However, even if the scheduling is completely successful with respect to load balancing, the scaling with the number of cores may be suboptimal due to resource contention. Here we consider the problem of scheduling tasks not only with respect to their interdependencies but also with respect to their usage of resources, such as memory and bandwidth. At the software level, this is achieved by user annotations of the task resource consumption. In the runtime system, the annotations are translated into scheduling constraints. Experimental results for different hardware, demonstrating performance gains both for model examples and real applications, are presented. Furthermore, we provide a set of tools to detect resource sensitivity and predict the performance improvements that can be achieved by resource-aware scheduling. These tools are solely based on parallel execution traces and require no instrumentation or modification of the application code.	computer architecture;image scaling;interdependence;load balancing (computing);multi-core processor;parallel computing;programmer;programming model;resource contention;runtime system;schedule (project management);scheduling (computing);tracing (software)	Martin Tillenius;Elisabeth Larsson;Rosa M. Badia;Xavier Martorell	2015	ACM Trans. Embedded Comput. Syst.	10.1145/2638554	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;parallel computing;real-time computing;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;distributed computing;programming language	HPC	-8.12670857647722	49.274721015076004	65211
2e132e2fffe6a1b3e0a2ecaa5d9f5436bfc7c8dc	the influence of parallel decomposition strategies on the performance of multiprocessor systems	partial differential equation;data access;iterative algorithm;parallel processing;vlsi design	We present a model for predicting multiprocessor performance on iterative algorithms, where each iteration consists of some amount of access to global data and some amount of local processing. The application cycles may be synchronous or asynchronous, arid the processors may or may not incur waiting time, depending on the relationship between the access time and processing time, The amount of processing time and global data accesses incurred by the parallel processes depends upon characteristics of the algorithm and its decomposition. We study the decompositions.of several sample algorithms, and identify several decomposition groups. Finally, using the Poisson partial-differential equation algorithm as an example, we investigate how its decomposition affects its performance. 1 In t roduct ion The value of a computer system is measured by the work it can perform. Parallel processing opens a new horizon for handling targe workloads, but its promise can only be realized if multiprocessots can be built and programmed to take full advantage of the potential parallelism. The computer-system designer must choose from a wide range of design alternatives--far too many to test empirically. Good performance models are needed to provide a'starting place for empirical investigation. In an earlier paper, 1 we presented a model which measures the cyclic processing power that a multiprocessor can apply to a v.,orldoad with given characteristics. Unlike other models. 2' 3, 4, s it is not based on predicting statistical mean values for performance over some time interval. Rather, it is based on the fact that most algorithm are iterative in nature, and the iterations tend to perform approximately the same amount of processing and data access. Let us assume that a given algorithm consumes Tp units of processing time each iteration, and spends T a time units accessing global data. The ratio of processing.to-access time depends upon the algorithm, and will be denoted by T X= P T a When an algorithm is decomposed into parallel processes, an iteration by c. subprocess u.~ually takes less time than an iteration in its uniprocessor counterpmt. Let tp and t a be the processing and access times within a subprocess iteration. If we let N > 1 denote the number of processors engaged in a parallel decomposition, usually tp < Tp ~.nd t a < T a, but often it is not true that N tp = Tp or N t a = T a. ]'he change in tp a~d t,~ as processors are added is also characteristic of the algorithm. Let us define the decomposition functions fp and fa as T T f = p f = a P t a t p a The time for a single iteration of a uniprocessor algorithm is simply T¢ = Tp + T a. However, due to contention for global data, the iteration time for a subprocess in a parallel implementation also depends on a nonnegative waiting time t w. Thus, the cycle time is t c = tp + t= + t w. Both the decomposition functions and the waiting time influence the speed of a multiprocessor implementation, usually preventing an N-processor decomposition from finishing in 1/Nth the time needed by the uniprocessor version. Wi~ shall define the speedup SP as a ratio of cycle times:	access time;algorithm;c date and time functions;central processing unit;child process;computer;data access;iteration;multiprocessing;parallel computing;parallel processing (dsp implementation);speedup;systems design;triangular decomposition;uniprocessor system	Dalibor F. Vrsalovic;Edward F. Gehringer;Zary Segall;Daniel P. Siewiorek	1985		10.1145/327010.327372	data access;parallel processing;computer architecture;parallel computing;real-time computing;computer science;iterative method;very-large-scale integration;partial differential equation;symmetric multiprocessor system	Arch	-7.703840828595525	48.87421516028573	65249
48963a992ab1f20d91e0fc33c3af9ae6c16eace1	handling branches in tls systems with multi-path execution	thread level speculation;parallel processing multi threading;protocols;multi threading;microarchitecture;squashed threads reexecution;code partitioning;branch prediction;hard to predict conditional branches;spec2000 int benchmark suite;parallel thread extraction;multipath execution;spec2000 int benchmark suite multipath execution thread level speculation parallel thread extraction sequential applications microarchitectural feature branch prediction code partitioning squashed threads reexecution hard to predict conditional branches;sequential applications;programming profession;informatics;microarchitectural feature;program processors protocols programming profession informatics microarchitecture;program processors;path following;parallel processing	Thread-Level Speculation (TLS) has been proposed to facilitate the extraction of parallel threads from sequential applications. Most prior work on TLS has focused on architectural features directly related to supporting the main TLS operations. In this work we, instead, investigate how a common microarchitectural feature, namely branch prediction, interacts with TLS. We show that branch prediction for TLS is even more important than it is for sequential execution. Unfortunately, branch prediction for TLS systems is also inherently harder. Code partitioning and re-executions of squashed threads pollute the branch history making it harder for predictors to be accurate. We thus propose to augment the hardware, so as to accommodate Multi-Path Execution (MP) within the existing TLS protocol. Under the MP execution model, all paths following a number of hard-to-predict conditional branches are followed simultaneously. MP execution thus removes branches that would have been otherwise mispredicted, helping in this way the core to exploit more ILP. We show that, with only minimal hardware support, one can combine these two execution models into a unified one. Experimental results show that our combined execution model achieves speedups of up to 23.2%, with an average of 9.2%, over an existing state-of-the-art TLS system and speedups of up to 138 %, with an average of 28.2%, when compared with MP execution for a subset of the SPEC2000 Int benchmark suite.	benchmark (computing);branch predictor;int (x86 instruction);microarchitecture;multithreading (computer architecture);speculative execution;speculative multithreading;transport layer security	Polychronis Xekalakis;Marcelo Cintra	2010	HPCA - 16 2010 The Sixteenth International Symposium on High-Performance Computer Architecture	10.1109/HPCA.2010.5416632	communications protocol;parallel processing;computer architecture;parallel computing;real-time computing;multithreading;microarchitecture;computer science;operating system;programming language;informatics;branch predictor	Arch	-7.757574914587474	49.54373367976675	65253
eb674977062db778d0c37506ec858f82f1b88d92	parallel simulation: practice: contrasting distributed simulation with parallel replication: a case study of a queuing simulation on a network of transputers	analytical models;microprocessors;queuing system;runtime;virtual prototyping;computer aided software engineering;computational modeling;technical report;computer science;velocity measurement;distributed simulation;computer aided software engineering computational modeling computer simulation discrete event simulation computer science microprocessors analytical models virtual prototyping runtime velocity measurement;computer simulation;parallel simulation;discrete event simulation	As discrete event simulation programs become larger and more complex, the amount of computing power required for their execution is rapidly increasing. One way to achieve this power is by a employing a multiple processor network to run the simulation programs. Two approaches to the problem of assigning tasks to processors are described--environment partitioning distributed simulation, in which the tasks required to perform a simulation are assigned to processors in the network; and parallel replication, in which copies of the simulation program are assigned to processors and the results of their execution aggregated. A simulation of an M/M/c queuing system has been implemented on networks of two and three transputers, using each approach. Heidelberger's statistical efficiency and the stabilization time of the system are used as metrics. The parallel replications tended to stabilize faster, but the statistical efficiencies were not significantly different.	simulation	R. Rajagopal;John Craig Comfort	1989		10.1109/WSC.1989.718750	computer simulation;computer architecture;real-time computing;simulation;simulation software;computer science;technical report;theoretical computer science;discrete event simulation;power system simulation;network simulation;queue management system;computational model;computer-aided software engineering;simulation language;network traffic simulation	HPC	-15.851992420836494	58.53479470794337	65297
05203478f860cea30bd8b4d42a1d0312d566aff5	data buffer performance for sequential prolog architectures	prolog;buffer storage;computer architecture;performance evaluation;warren abstract machine;benchmark programs;choice point buffers;copyback caches;data buffer performance;data memory performance results;local data buffers;multiple-register sets;reduced instruction set;sequential prolog architectures;smart caches;split-stack buffers;stack buffers	Several local data buffers are proposed and measurements are presented for variations of the Warren Abstract Machine (WAM) architecture for Prolog. Choice point buffers, stack buffers, split-stack buffers, multiple register sets, copyback caches, and “smart” caches are examined. Statistics collected from four benchmark programs indicate that small conventional local memories perform quite well because of the WAM's high locality. The data memory performance results are equally valid for native code and reduced instruction set implementations of Prolog.	benchmark (computing);cpu cache;data buffer;locality of reference;machine code;prolog;warren abstract machine	Evan Tick	1988			computer architecture;parallel computing;real-time computing;computer science;operating system;programming language;prolog	Arch	-6.971182946138601	48.96649031884549	65329
968ad4eec870e110e5d52ad76640bb31ede2b50d	providing cuda acceleration to kvm virtual machines in infiniband clusters with rcuda	virtualization;infiniband;cuda;hpc;kvm	There is a trend towards using graphics processing units GPUs not only for graphics visualization, but also for accelerating scientific applications. But their use for this purpose is not without disadvantages: GPUs increase costs and energy consumption. Furthermore, GPUs are generally underutilized. Using virtual machines could be a possible solution to address these problems, however, current solutions for providing GPU acceleration to virtual machines environments, such as KVM or Xen, present some issues. In this paper we propose the use of remote GPUs to accelerate scientific applications running inside KVM virtual machines. Our analysis shows that this approach could be a possible solution, with low overhead when used over InfiniBand networks.	cuda;infiniband;rcuda	Ferran Perez;Carlos Reaño;Federico Silla	2016		10.1007/978-3-319-39577-7_7	supercomputer;parallel computing;virtualization;computer hardware;computer science;operating system	HPC	-9.81406351832953	46.791581662312495	65489
ac8055808fafed176eb3599bf00e3071f2c735a4	an efficient method to schedule tandem of real-time tasks in cluster computing with possible processor failures	cluster computing;real time;search algorithm;tasks grouping computer system real time tasks cluster computing processor failures scheduling goals objective function search algorithm reliability deadline;processor scheduling scheduling algorithm redundancy hardware clustering algorithms real time systems fault tolerance computer science environmental factors software design;objective function;fault tolerant computing;system recovery;scheduling;group process;workstation clusters;software design;fault tolerant computing real time systems workstation clusters system recovery scheduling;environmental factor;real time systems	Each computer system in operation today is certain to experience faults in its operational lifetime. There is more than one source, which can cause faults, for example environmental factors, communication, interaction with users, or hardware or software design flaws randomly build into the system. A reliable system is the one that would continue operation albeit in a degraded mode despite the presence of faults. In this paper, we introduce an efficient method to schedule a tandem of real-time tasks in cluster computing with possible processor failures. The method is based on developing an objective function that combines different scheduling goals. This objective function is used to guide the search algorithm to find an efficient solution. The proposed algorithm consists of three terms, reliability, deadline and tasks grouping to minimize remote communication among tasks. The proposed method creates a scheduling table based on the objective function then searches this table for a feasible solution. The method is efficient because it reduces communication between different tasks by grouping them together. This grouping process reduces application de-fragmentation as well.	computer cluster;real-time transcription	Alaa Amin;Reda A. Ammar;Swapna S. Gokhale	2003		10.1109/ISCC.2003.1214277	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer cluster;computer science;software design;operating system;distributed computing;least slack time scheduling;scheduling;group dynamics;search algorithm	HPC	-15.187245385287724	60.12758955523586	65799
712c27fd1343b5992392ad0921dffce86df9af2e	p-scheduler: adaptive hierarchical scheduling in apache storm	apache storm;graph partitioning;big data;scheduling;stream processing	With ever-accelerating data creation rates in Big Data applications, there is a need for efficient stream processing engines. Apache Storm has been of interest in both academia and industry because of its real-time, distributed, scalable and reliable framework for stream processing. In this paper, we propose an adaptive hierarchical scheduler for the Storm framework, to allocate the resources more efficiently and improve performance. In our method, we consider the data transfer rate and traffic pattern between Storm's tasks and assign highly-communicating task pairs to the same computing node by dynamically employing two phases of graph partitioning. We also calculate the number of required computing nodes in the cluster based on the overall load of the application and use this information to reduce inter-node traffic. Our performance evaluation shows a significant improvement compared to the default scheduler provided by Storm, which evenly distributes the tasks across the cluster, ignoring the communication patterns between them.	apache storm;big data;graph partition;performance evaluation;real-time clock;scalability;scheduling (computing);stream processing	Leila Eskandari;Zhiyi Huang;David M. Eyers	2016		10.1145/2843043.2843056	parallel computing;real-time computing;computer science;distributed computing	HPC	-17.514314621428984	57.5853948379572	65821
62c1bda395fdc4b2f94df92090d8001873d47b95	a comparative evaluation of hierarchical network architecture of the hp-convex exemplar	scaling capability;hot spot contention;distributed memory systems;coherent toroidal interconnect;cc numa;performance evaluation;hewlett packard computers;spp1000;performance;distributed shared memory architecture;hierarchical networks;hot spot;synchronization primitives;shared memory systems;hp convex exemplar;parallel architectures;memory architecture;cache coherence;memory systems;coherent shared memory;network structure;network latency;shared memory system;synchronization primitives hp convex exemplar spp1000 spp2000 distributed shared memory architecture performance network latency hot spot contention cache coherence scaling capability cc numa coherent shared memory hierarchical interconnection networks coherent toroidal interconnect;hierarchical interconnection networks;parallel architectures distributed memory systems shared memory systems hewlett packard computers performance evaluation;spp2000;large scale systems computer architecture scalability multiprocessor interconnection networks educational institutions military computing parallel processing kernel process design microprocessors	The Convex Exemplar (SPP1000 and SPP2000 series) is a new commercial distributed shared-memory architecture. Using a set of system kernels and two application programs, we examine performance eeects on network latency, hot spot contention, cache coherence and overall scaling capability, which result both from the choice of the network structure as well as from its CC-NUMA memory system feature. Since the KSR-1 was also targeted at scalable cache coherent shared-memory systems by using hierarchical interconnection networks, we compared the architecture and performance results with the KSR-1. Our experiments indicate that the memory access latency of the Exemplar is comparatively low due to its fast processor node and the unique network/system structure. In addition, the Coherent Torodial Interconnect (CTI) rings are eecient in handling cache coherence activities on the Exemplar. However, the Ex-emplar synchronization primitives need further exploit its hierarchical architecture in a high 1 2 contention environment. As we expect with extremely fast processors in a hierarchical memory structure, the high-speed cluster-based Exemplar system is more sensitive to data locality.	cache coherence;central processing unit;coherent;experiment;image scaling;interconnection;locality of reference;network architecture;network topology;scalability;shared memory;tree network	Robert Castañeda;Xiaodong Zhang;James M. Hoover	1997		10.1109/ICCD.1997.628877	embedded system;cache coherence;computer architecture;latency;parallel computing;real-time computing;performance;computer science;operating system;hot spot	Arch	-11.289422222817365	47.419526612837075	66071
4b1512baab46672beebb8ec63e880c9934e6055c	feasibility and performance analysis of rdma transfer through pci express		The PCI Express is a widely used system bus technology that connects the processor and the peripheral I/O devices. The PCI Express is nowadays regarded as a de facto standard in system area interconnection network. It has good characteristics in terms of high-speed, low power. In addition, PCI Express is becoming popular interconnection network technology as like Gigabit Ethernet, InfiniBand, and Myrinet which are extensively used in high-performance computing. In this paper, we designed and implemented a evaluation platform for interconnect network using PCI Express between two computing nodes. We make use of the non-transparent bridge (NTB) technology of PCI Express in order to isolate between the two subsystems. We constructed a testbed system and evaluated the performance on the testbed.	bridging (networking);data recovery;gigabit;infiniband;input/output;interconnection;pci express;peripheral;profiling (computer programming);remote direct memory access;supercomputer;system bus;testbed	Min Choi;Jong Hyuk Park	2017	JIPS	10.3745/JIPS.01.0013	parallel computing;remote direct memory access;computer science;pci express	HPC	-10.353565975423917	46.45759820982923	66080
e4cf5fe7c47d39ee7d3685b8db400577192fbe8e	using sdrams for two-dimensional accesses of long 2n × 2m-point ffts and transposing	sdram indexes pipelines bandwidth field programmable gate arrays throughput clocks;sdram memories;bank interleaving method;field programmable gate array;2d fft processing;double buffering;random access storage fast fourier transforms;clocks;indexes;two dimensional index space;radar application two dimensional access transposing sdram memories address mapping scheme two dimensional index space bank interleaving method double buffering 2d fft processing;radar application;pipelines;indexation;fast fourier transforms;bandwidth;random access storage;address mapping scheme;two dimensional access;transposing;field programmable gate arrays;sdram;throughput	When transposing large matrices using SDRAM memories, typically a control overhead significantly reduces the data throughput. In this paper, a new address mapping scheme is introduced, taking advantage of multiple banks and burst capabilities of modern SDRAMs. Other address mapping strategies minimize the total number of SDRAM page-opens while traversing the two-dimensional index-space in row or column direction. The new approach uses bank interleaving methods to hide wait cycles, caused by page-opens. In this way, data bus wait cycles do not depend on the overall number of page-opens directly. It is shown, that the data bus utilization can be increased significantly, in particular, if memories are accessed with parallel samples. Therefore, double buffering can be omitted. As a special operation, 2D-FFT processing for radar applications is considered. Depending on SDRAM parameters and dimensions, a continuous bandwidth utilization of 96 to 98% is achieved for accesses in both matrix directions, including all refresh commands.	bus (computing);column (database);fast fourier transform;forward error correction;multiple buffering;overhead (computing);random access;throughput	Stefan Langemeyer;Peter Pirsch;Holger Blume	2011	2011 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation	10.1109/SAMOS.2011.6045467	electronic engineering;parallel computing;real-time computing;computer science	EDA	-8.505591434011954	54.85841301420826	66327
ac4e946f1c4cf34975a38de9586bd95f645b7b9d	considering all starting points for simultaneous multithreading simulation	cophase matrix phase analysis;multi threading;multiprocessing programs;phase behavior;multithreading surface mount technology microarchitecture computational modeling yarn throughput analytical models phase estimation computer architecture computer simulation;simultaneous multithreading simulation;matrix algebra;smt architecture optimization;cpi estimate;simulation technique;phase analysis;architecture behavior;simultaneous multithreading;smt simulation;smt processor;microprocessor chips digital simulation multi threading multiprocessing programs matrix algebra;digital simulation;cpi estimate simultaneous multithreading simulation architecture behavior smt architecture optimization smt simulation smt processor cophase matrix phase analysis;microprocessor chips	Commercial processors have support for simultaneous multithreading (SMT), yet little work has been done to provide representative simulation results for SMT. Given a workload, current simulation techniques typically run one combination of those programs from a specific starting offset, or just run one combination of samples across the benchmarks. We have found that the architecture behavior and overall throughput seen can vary drastically based upon the starting points of the different benchmarks. Therefore, to completely evaluate the effect of an SMT architecture optimization on a workload, one would need to simulate many or all of the program combinations from different starting offsets. But exhaustively running all program combinations from many starting offsets is infeasible - even running single programs to completion is often infeasible with modern benchmarks. In this paper we propose an SMT simulation methodology that estimates the average performance over all possible starting points when running multiple programs concurrently on an SMT processor. This is based on our prior co-phase matrix phase analysis and simulation infrastructure. This approach samples all of the unique phase combinations for a set of benchmarks to be run together. Once these phase combinations are sampled, our approach uses these samples, along with a trace of the phase behavior for each program, to provide a CPI estimate of all starting points. This all starting point CPI estimate is precisely calculated in just minutes.	benchmark (computing);best, worst and average case;central processing unit;mathematical optimization;monte carlo method;multithreading (computer architecture);population;quantum entanglement;randomness;sampling (signal processing);simulation;simultaneous multithreading;stratified sampling;throughput	Michael Van Biesbrouck;Lieven Eeckhout;Brad Calder	2006	2006 IEEE International Symposium on Performance Analysis of Systems and Software	10.1109/ISPASS.2006.1620799	computer architecture;parallel computing;real-time computing;multithreading;computer science;operating system;simultaneous multithreading	Arch	-5.504001008055717	51.29921222409998	66477
358a02bbd428f2614e4959b675babc0d408b3d4a	harvesting-aware adaptive energy management in solar-powered embedded systems	quality of service supercapacitors real time systems energy harvesting energy consumption embedded systems energy management;solar power energy consumption energy harvesting energy management systems middleware quality of service;solar powered smart camera system harvesting aware adaptive energy management solar powered embedded system energy consumption energy sustainability solar powered real time system adaptive qos model holistic middleware framework dvfs	Solar-powered embedded systems are challenged by variations and uncertainties in energy availability due to seasonal changes and weather conditions. Such systems need to deploy various schemes to adapt their energy consumption in order to achieve energy sustainability. We target solar-powered real-time systems with adaptive QoS model. A holistic middleware framework for energy management that orchestrates DVFS and application QoS in solar-powered real-time systems is proposed. Extensive experiments are carried out to show the effectiveness and the robustness of our technique on two case studies on a solar-powered smart camera system. Simulation results show an improvement of 19%-50% in terms of total QoS compared to a DVFS framework with fixed QoS model for real time embedded systems.	dynamic voltage scaling;embedded system;experiment;heuristic;holism;microsoft windows;middleware;multi-core processor;online and offline;quality of service;real-time clock;real-time computing;scheduling (computing);simulation;smart camera;unified framework	Nga Dang;Zana Ghaderi;Moonju Park;Elaheh Bozorgzadeh	2016	2016 17th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2016.7479223	embedded system;real-time computing;simulation;engineering	Embedded	-4.679363117621195	58.65495673061075	66633
000c82d2682b1117359db3db4e738b600b211c78	fast database restarts at facebook	shared memory;database;recovery;rollover;restart	Facebook engineers query multiple databases to monitor and analyze Facebook products and services. The fastest of these databases is Scuba, which achieves subsecond query response time by storing all of its data in memory across hundreds of servers. We are continually improving the code for Scuba and would like to push new software releases at least once a week. However, restarting a Scuba machine clears its memory. Recovering all of its data from disk --- about 120 GB per machine --- takes 2.5-3 hours to read and format the data per machine. Even 10 minutes is a long downtime for the critical applications that rely on Scuba, such as detecting user-facing errors. Restarting only 2% of the servers at a time mitigates the amount of unavailable data, but prolongs the restart duration to about 12 hours, during which users see only partial query results and one engineer needs to monitor the servers carefully. We need a faster, less engineer intensive, solution to enable frequent software upgrades.  In this paper, we show that using shared memory provides a simple, effective, fast, solution to upgrading servers. Our key observation is that we can decouple the memory lifetime from the process lifetime. When we shutdown a server for a planned upgrade, we know that the memory state is valid (unlike when a server shuts down unexpectedly). We can therefore use shared memory to preserve memory state from the old server process to the new process. Our solution does not increase the server memory footprint and allows recovery at memory speeds, about 2-3 minutes per server. This solution maximizes uptime and availability, which has led to much faster and more frequent rollouts of new features and improvements. Furthermore, this technique can be applied to the in-memory state of any database, even if the memory contains a cache of a much larger disk-resident data set, as in most databases.	cpu cache;coupling (computer programming);downtime;fastest;gigabyte;in-memory database;memory footprint;response time (technology);sensor;server (computing);shared memory;shutdown (computing);uptime	Aakash Goel;Bhuwan Chopra;Ciprian Gerea;Dhruv Mátáni;Josh Metzler;Fahim Ul Haq;Janet L. Wiener	2014		10.1145/2588555.2595642	uniform memory access;rollover;shared memory;real-time computing;recovery;distributed memory;computer science;operating system;database;memory management	OS	-17.35454561813961	52.939061032832264	66713
b47b3b47dc6269d81c323912115f26dfb5350332	dynamic allocation of spm based on time-slotted cache conflict graph for system optimization	energy optimization		cpu cache;cache (computing);memory management;program optimization;super paper mario	Jianping Wu;Ming Ling;Yonghui Zhang;Chen Mei;Huan Wang	2012	IEICE Transactions		parallel computing;real-time computing;computer science;distributed computing;energy minimization	Embedded	-6.356701362266778	54.727084129478236	66942
b349f8d6c7660ca37f81279f80cc0d6f3bac2b14	achieving reliable system performance by fast recovery of branch miss prediction	system reliability;fault tolerant system;branch misprediction recovery	Today’s technology evolution provides users inexpensive and powerful computer systems. However, there are argues that system reliability and fault tolerance is necessary in the systems as well. A proper design for the reliable and fault-tolerant computer system requires a trade-off among cost, reliability, and availability. In this paper, we propose a low-cost recovery scheme for reliable system performance. With this approach, it completely eliminates the roll-back overhead on branch misprediction. Thus, the instruction fetcher does not stop and it fetches instructions from the correct path immediately after the misprediction detected. So, this approach prevents a processor from flushing the pipeline, even under branch misprediction by allowing the instruction fetcher to work continuously. Our approach reduces the branch misprediction penalty for achieving reliable system performance. It instantly reconstructs the map table to any mispredicted branch and it outperforms the conventional RMT by an average of 10.93%. & 2011 Elsevier Ltd. All rights reserved.	apache nutch;associative entity;branch misprediction;fault tolerance;fault-tolerant computer system;overhead (computing);virtual economy	Min Choi;Jong Hyuk Park;Seungho Lim;Young-Sik Jeong	2012	J. Network and Computer Applications	10.1016/j.jnca.2011.03.015	embedded system;fault tolerance;parallel computing;real-time computing;computer science;branch misprediction	Arch	-19.086546812037053	49.03184102448364	67043
df1b685313821490e383c4a2de5fb77426a33f3a	fault site pruning for practical reliability analysis of gpgpu applications		Graphics Processing Units (GPUs) have rapidly evolved to enable energy-efficient data-parallel computing for a broad range of scientific areas. While GPUs achieve exascale performance at a stringent power budget, they are also susceptible to soft errors, often caused by high-energy particle strikes, that can significantly affect the application output quality. Understanding the resilience of general purpose GPU applications is the purpose of this study. To this end, it is imperative to explore the range of application output by injecting faults at all the potential fault sites. This problem is especially challenging because unlike CPU applications, which are mostly single-threaded, GPGPU applications can contain hundreds to thousands of threads, resulting in a tremendously large fault site space – in the order of billions even for some simple applications. In this paper, we present a systematic way to progressively prune the fault site space aiming to dramatically reduce the number of fault injections such that assessment for GPGPU application error resilience can be practical. The key insight behind our proposed methodology stems from the fact that GPGPU applications spawn a lot of threads, however, many of them execute the same set of instructions. Therefore, several fault sites are redundant and can be pruned by a careful analysis of faults across threads and instructions. We identify important features across a set of 10 applications (16 kernels) from Rodinia and Polybench suites and conclude that threads can be first classified based on the number of the dynamic instructions they execute. We achieve significant fault site reduction by analyzing only a small subset of threads that are representative of the dynamic instruction behavior (and therefore error resilience behavior) of the GPGPU applications. Further pruning is achieved by identifying and analyzing: a) the dynamic instruction commonalities (and differences) across code blocks within this representative set of threads, b) a subset of loop iterations within the representative threads, and c) a subset of destination register bit positions. The above steps result in a tremendous reduction of fault sites by up to seven orders of magnitude. Yet, this reduced fault site space accurately captures the error resilience profile of GPGPU applications.		Bin Nie;Lishan Yang;Adwait Jog;Evgenia Smirni	2018	2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)	10.1109/MICRO.2018.00066	psychological resilience;parallel computing;application error;pruning;power budget;computer science;fault injection;thread (computing);general-purpose computing on graphics processing units;graphics	Arch	-4.859721500745808	47.55516052891832	67080
1bed30d161683d279780aee34619f94a860fa973	efficient virtual memory for big memory servers	virtual memory;tanslation lookaside buffer	"""Our analysis shows that many """"big-memory"""" server workloads, such as databases, in-memory caches, and graph analytics, pay a high cost for page-based virtual memory. They consume as much as 10% of execution cycles on TLB misses, even using large pages. On the other hand, we find that these workloads use read-write permission on most pages, are provisioned not to swap, and rarely benefit from the full flexibility of page-based virtual memory.  To remove the TLB miss overhead for big-memory workloads, we propose mapping part of a process's linear virtual address space with a direct segment, while page mapping the rest of the virtual address space. Direct segments use minimal hardware---base, limit and offset registers per core---to map contiguous virtual memory regions directly to contiguous physical memory. They eliminate the possibility of TLB misses for key data structures such as database buffer pools and in-memory key-value stores. Memory mapped by a direct segment may be converted back to paging when needed.  We prototype direct-segment software support for x86-64 in Linux and emulate direct-segment hardware. For our workloads, direct segments eliminate almost all TLB misses and reduce the execution time wasted on TLB misses to less than 0.5%."""	address space;attribute–value pair;big memory;computer data storage;data structure;in-memory database;linux;memory-mapped i/o;overhead (computing);page (computer memory);paging;prototype;read-write memory;run time (program lifecycle phase);server (computing);translation lookaside buffer;usb flash drive;x86;x86-64	Arkaprava Basu;Jayneel Gandhi;Jichuan Chang;Mark D. Hill;Michael M. Swift	2013		10.1145/2485922.2485943	demand paging;parallel computing;real-time computing;thrashing;computer science;physical address;virtual memory;operating system;page table;translation lookaside buffer;page;overlay;flat memory model;memory segmentation;memory map;memory management	Arch	-13.503746539821678	53.5605616477165	67144
18c34d0af511b553723fd4e9a55ebafae5240682	space-time scheduling of instruction-level parallelism on a raw machine	instruction level parallel;branch point;space time;register file;functional unit;instruction scheduling	Increasing demand for both greater parallelism and faster clocks dictate that future generation architectures will need to decentralize their resources and eliminate primitives that require single cycle global communication. A Raw microprocessor distributes all of its resources, including instruction streams, register files, memory ports, and ALUs, over a pipelined two-dimensional mesh interconnect, and exposes them fully to the compiler. Because communication in Raw machines is distributed, compiling for instruction-level parallelism (ILP) requires both spatial instruction partitioning as well as traditional temporal instruction scheduling. In addition, the compiler must explicitly manage all communication through the interconnect, including the global synchronization required at branch points. This paper describes RAWCC, the compiler we have developed for compiling general-purpose sequential programs to the distributed Raw architecture. We present performance results that demonstrate that although Raw machines provide no mechanisms for global communication the Raw compiler can schedule to achieve speedups that scale with the number of available functional units.	arithmetic logic unit;compiler;general-purpose modeling;instruction scheduling;instruction-level parallelism;mesh networking;microprocessor;parallel computing;register file;tcp global synchronization	Walter Lee;Rajeev Barua;Matthew I. Frank;Devabhaktuni Srikrishna;Jonathan Babb;Vivek Sarkar;Saman P. Amarasinghe	1998		10.1145/291069.291018	branch point;computer architecture;parallel computing;real-time computing;computer science;operating system;space time;instruction scheduling;programming language;register file	Arch	-11.52804732364263	46.68787567264787	67167
f396aeb9a8519f9cbb18f3b9957f9e88ae79a7b6	using virtual addresses with communication channels		While for single processor and SMP machines, memory is the allocatable quantity, for machines made up of large amounts of parallel computing units, each with its own local memory, the allocatable quantity is a single computing unit. Where virtual address management is used to keep memory coherent and allow allocation of more than physical memory is actually available, virtual communication channel references can be used to make computing units stay connected across allocation and swapping.	channel (communications);coherence (physics);computer data storage;paging;parallel computing	Oskar Schirmer	2013	CoRR		interleaved memory;parallel computing;real-time computing;base and bounds;computer science;physical address;virtual memory;operating system;static memory allocation;distributed computing;overlay;data diffusion machine;computing with memory;memory map;memory management	HPC	-12.224654061755077	48.12528021298698	67276
908bf918b1fa3cba625a0e1748e77fecf7e61ea8	improving write performance and extending endurance of object-based nand flash devices		Write amplification is a major cause of performance and endurance degradations in NAND flash-based storage systems. In an object-based NAND flash device (ONFD), two causes of write amplification are onode partial update and cascading update. Here, onode is a type of small-sized object metadata, and multiple onodes are stored in one NAND flash page. Updating one onode invokes partial page update (i.e., onode partial update), incurring unnecessary migration of the un-updated data. Cascading update denotes updating object metadata in a cascading manner due to object data update or migration. Although there are only several bytes that need to be updated in the object metadata, one or more pages have to be re-written accordingly. In this work, we propose a system design to alleviate the write amplification issue in the object-based NAND flash device. The proposed design includes (1) a multi-level garbage collection technique to minimize unnecessary data migration incurred by onode partial update and (2) a B+ table tree, Semantics-Aware Flexible (SAF) data layout, and selective cache design to reduce the write operations associated with cascading update. To guarantee system consistency, we also propose a power failure handling technique. Experiment results show that our proposed design can achieve up to 20% write reduction compared to the best states of the art.	adobe flash;byte;data access;dynamic random-access memory;flash memory;garbage collection (computer science);object-based language;page table;store and forward;systems design;tree (data structure);update (sql)	Jie Guo;Chuhan Min;Tao Cai;Yiran Chen	2017	ACM Trans. Embedded Comput. Syst.	10.1145/3105924	real-time computing;cache;parallel computing;systems design;write amplification;nand gate;data migration;byte;metadata;garbage collection;computer science	OS	-11.918479107408887	54.463851313195825	67511
2d4d1e7c2669c6ac943ca36c7754f674eb5d3296	performance issues of a superscalar microprocessor	performance evaluation;branch prediction;upper bound;computer architecture;dynamic scheduling	Abstract   Cache, dynamic scheduling, bypassing, branch prediction and fetch efficiency are primary issues concerning the performance of a superscalar microprocessor. This paper considers all these issues and shows their impact on performance by running a simulator on 17 different programs. The approach in handling branch prediction is shown to significantly decrease the bad branch penalty. Furthermore, results show that the average instruction fetch places an upper bound on speedup and is the most critical factor in determining overall performance. Its performance impact is greater than all other factors combined.	microprocessor;superscalar processor	Steven Wallace;Nader Bagherzadeh	1995	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(95)91858-2	embedded system;computer architecture;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;upper and lower bounds;branch predictor	EDA	-8.104340368875775	51.97919691619109	67533
8d564ee7c3064c3383d2f36324f7deec0edddbb5	a two-level caching mechanism for demand-based page-level address mapping in nand flash memory storage systems	storage allocation;nand flash memory;flash memory;cache storage;random access memory;temporal spatial cache configurations;storage system;demand based page level address mapping;block erase counts;system performance;extra address translation overhead;block erase counts two level caching mechanism demand based page level address mapping nand flash memory storage systems ram footprint flash translation layer design extra address translation overhead on demand page level address mappings temporal spatial cache configurations;computer architecture;two level caching mechanism;time factors;flash translation layer;storage allocation cache storage flash memories nand circuits;flash translation layer design;nand circuits;ash;spatial locality;ash random access memory computer architecture system performance time factors flash memory algorithm design and analysis;algorithm design and analysis;flash memories;ram footprint;on demand page level address mappings;nand flash memory storage systems	The increasing capacity of NAND flash memory leads to large RAM footprint on address mapping in the Flash Translation Layer (FTL) design. The demand-based approach can reduce the RAM footprint, but extra address translation overhead is also introduced which may degrade the system performance. This paper proposes a two-level caching mechanism to selectively cache the on-demand page-level address mappings by jointly exploiting the temporal locality and the spatial locality of workloads. The objective is to improve the cache hit ratio so as to shorten the system response time and reduce the block erase counts for NAND flash memory storage systems. By exploring the optimized temporal-spatial cache configurations, our technique can well capture the reference locality in workloads so that the hit ratio can be improved. Experimental results show that our technique can achieve a 31.51% improvement in hit ratio, which leads to a 31.11% reduction in average system response time and a 50.83% reduction in block erase counts compared with the previous work.	cache (computing);ftl: faster than light;flash file system;flash memory controller;hit (internet);locality of reference;overhead (computing);principle of locality;random-access memory;response time (technology)	Zhiwei Qin;Yi Wang;Duo Liu;Zili Shao	2011	2011 17th IEEE Real-Time and Embedded Technology and Applications Symposium	10.1109/RTAS.2011.23	flash file system;algorithm design;parallel computing;real-time computing;computer hardware;cache;computer science;computer performance	Embedded	-10.433815811652467	53.672167841407614	67596
08f13665ae747ac7d20cdd60d91c7e1baed2875d	a lock-free algorithm for concurrent bags	concurrent;text;shared memory;memory management;information technology;computer and information science;parallel programming;concurrent data structures;natural sciences;concurrency;lock freeness;data structure;non blocking;informationsteknik;distributed design	A lock-free bag data structure supporting unordered buffering is presented in this paper. The algorithm supports multiple producers and multiple consumers, as well as dynamic collection sizes. To handle concurrency efficiently, the algorithm was designed to thrive for disjoint-access-parallelism for the supported semantics. Therefore, the algorithm exploits a distributed design combined with novel techniques for handling concurrent modifications of linked lists using double marks, detection of total emptiness, and efficient memory management with hazard pointer handover. Experiments on a 24-way multi-core platform show significantly better performance for the new algorithm compared to previous algorithms of relevance.	concurrency (computer science);data structure;hazard pointer;linked list;memory management;multi-core processor;non-blocking algorithm;parallel computing;pointer (computer programming);relevance	Håkan Sundell;Anders Gidenstam;Marina Papatriantafilou;Philippas Tsigas	2011		10.1145/1989493.1989550	shared memory;parallel computing;real-time computing;concurrency;data structure;computer science;operating system;concurrency control;database;distributed computing;concurrent data structure;programming language;information technology;algorithm;concurrent object-oriented programming;memory management	PL	-14.894862926277534	49.2213128499043	67691
f01d7f6f64e7d3d26235d0f2163e8d27d43489d4	the impact of timestamp granularity in optimistic concurrency control		Optimistic concurrency control (OCC) can exploit the strengths of parallel hardware to provide excellent performance for uncontended transactions, and is popular in high-performance in-memory databases and transactional systems. But at high contention levels, OCC is susceptible to frequent aborts, leading to wasted work and degraded performance. Contention managers, mixed optimistic/pessimistic concurrency control algorithms, and novel optimistic-inspired concurrency control algorithms, such as TicToc [21], aim to address this problem, but these mechanisms introduce sometimes-high overheads of their own. We show that in real-world benchmarks, traditional OCC can outperform these alternative mechanisms by simply adding fine-grained version timestamps (using different timestamps for disjoint components of each record). With fine-grained timestamps, OCC gets 1.14× TicToc’s throughput in TPC-C at 128 cores (previous work reported TicToc having 1.8× higher throughput than OCC at 80 hyperthreads). Our study shows that timestamp granularity has a greater impact than previously thought on the performance of transaction processing systems, and should not be overlooked in the push for faster concurrency control schemes.	algorithm;baseline (configuration management);benchmark (computing);dhrystone;hyper-threading;ibm tivoli storage productivity center;in-memory database;multiple granularity locking;optimistic concurrency control;throughput;timestamp-based concurrency control;transaction processing system	Yihe Huang;Hao Bai;Eddie Kohler;Barbara Liskov;Liuba Shrira	2018	CoRR			OS	-14.393635166926872	50.16124138886749	67709
6652e369c7a56180c991cb2cbbab69ddbb27bff2	using profiling to reduce branch misprediction costs on a dynamically scheduled processor	branch prediction;narrow width operands;out of order;data speculation;profitability;instruction level parallelism;selection criteria;hardware implementation;value prediction;dynamically scheduled processors	Modern dynamically scheduled processors use branch prediction hardware to speculatively fetch and execute most likely executed paths in a program. Complex branch predictors have been proposed which attempt to identify these paths accurately such that the hardware can benefit from out-of-order (OOO) execution. Recent studies have shown that inspite of such complex prediction schemes, there still exist many frequently executed branches which are difficult to predict. Predicated execution has been proposed as an alternative technique to eliminate some of these branches in various forms ranging from a restrictive support to a full-blown support. We call the restrictive form of predicated execution as guarded execution. In this paper, we propose a new algorithm which uses profiling and selectively performs if-conversion for architectures with guarded execution support. Branch profiling is used to gather the taken, non-taken and misprediction counts for every branch. This combined with block profiling is used to select paths which suffer from heavy mispredictions and are profitable to if-convert. Effects of three different selection criterias, namely size-based, predictability-based and profiled-based, on net cycle improvements, branch mispredictions and mis-speculated instructions are then studied. We also propose new mechanisms to convert unsafe instructions to safe form to enhance the applicability of the technique. Finally, we explain numerous adjustments that were made to the selection criterias to better reflect the OOO processor behavior.	algorithm;branch misprediction;branch predictor;central processing unit;out-of-order execution;profiling (computer programming)	Srinivas Mantripragada;Alexandru Nicolau	2000		10.1145/335231.335251	parallel computing;real-time computing;branch;computer science;out-of-order execution;operating system;instruction-level parallelism;branch predictor;profitability index	HPC	-6.9890609405020605	51.39608573291436	67766
6e2375d9f8bdf820d3732cab9b72c049bdf04251	can high performance software dsm systems designed with infiniband features benefit from pci-express?	interconnection technologies;cache storage;protocols;pci x architecture;software distributed shared memory systems;peripheral interfaces;application software;perforation;newgendsm software dsm systems;software systems;software dsm protocols;software performance software systems software design bandwidth protocols delay computer architecture application software aggregates computer science;software performance;infiniband;system area networks newgendsm software dsm systems infiniband pci express bus technology software distributed shared memory systems interconnection technologies myrinet quadrics pci x architecture software dsm protocols cache coherency protocol;cache coherency protocol;computer architecture;system area network;quadrics;low latency;cache coherence protocol;system design;aggregates;peripheral interfaces distributed shared memory systems cache storage protocols;distributed shared memory systems;bandwidth;pci express bus technology;computer science;software design;system architecture;myrinet;high performance;distributed shared mem ory;system area networks	The performance of software distributed shared memory systems has traditionally been lower than other programming models primarily because of overhead in the coherency protocol, communication bottlenecks and slow networks. Software DSMs have benefited from interconnection technologies like Myrinet, InfiniBand and Quadrics, which offer low latency and high bandwidth communication. Additionally, some of their features like RDMA and atomic operations have been used to implement some portions of the software DSM protocols directly, further reducing overhead. Such network aware protocols are dependent on characteristics of these networks. The performance of the networks is in turn dependent on the system architecture, specially the I/O bus. PCI-Express the successor to the PCI-X architecture, offers improved latency and bandwidth characteristics. In this paper, we evaluate the impact of using an improved bus technology like PCI-Express, on the performance of software DSM protocols, which use the network features of InfiniBand. We can see a reduction in application execution time of up to 13% at four nodes, when PCI-Express is used instead of PCI-X.	benchmark (computing);diff utility;distributed shared memory;infiniband;input/output;interconnection;linearizability;overhead (computing);pci express;pci-x;remote direct memory access;run time (program lifecycle phase);scalability;systems architecture;thread (computing);zero-copy	Ranjit Noronha;Dhabaleswar K. Panda	2005	CCGrid 2005. IEEE International Symposium on Cluster Computing and the Grid, 2005.	10.1109/CCGRID.2005.1558663	communications protocol;computer architecture;application software;parallel computing;quadric;software performance testing;computer science;software design;operating system;distributed computing;bandwidth;systems architecture;software system;systems design;low latency	Arch	-10.402685964208548	47.01213342323603	67783
40e0beb36111ecd83bb13dc5992d022c86bef2e4	smartphone application launch with smarter scheduling	smartphone responsiveness;application delay;i o optimizations;application launch	The time it takes to launch a smartphone application is unpredictable. In this paper, we explore how these unpredictable launch times are affected by constraints associated with reading (writing) from (to) flash storage. We conduct the first large-scale measurement study on the Android I/O delay using the data collected from our Android application running on 1480 devices within 188 days. Among others, we observe that reads experience up to 626% slowdown when blocked by concurrent writes. We use this obtained knowledge to design a pilot solution, wherein by prioritizing reads over writes we are able to reduce the launch delay by up to 37.8%.	android;flash memory;mobile app;scheduling (computing);smartphone	David T. Nguyen;Ge Peng;Daniel Graham;Gang Zhou	2014		10.1145/2638728.2638763	embedded system;real-time computing;simulation;operating system;computer security	Mobile	-11.84949131625964	56.79718020898153	67798
0c3df3448d2fc079a24cab04b89c94a7d67aac8d	parallelization of genetic algorithms and sustainability on many-core processors		In this paper, we study and evaluate fault-tolerant technology for use in the parallel acceleration of evolutionary computation on many-core processors. Specifically, we show running evolutionary computation in parallel on a GPU results in a system that not only performs better as the number of processor cores increases, but is also robust against any physical faults (e.g., stuck-at faults) and transient faults (e.g., faults caused by noise), and makes it less likely that the application program will be interrupted while running. That is, we show that this approach is beneficial for the implementation of systems with sustainability.	automatic parallelization;genetic algorithm;parallel computing	Yuji Sato	2012		10.1007/978-81-322-1041-2_15	computational science;automatic parallelization	AI	-4.808570371729764	48.15244882706196	67897
069eafae5ee9df25ff5c457bb636f73b98d8f6e9	processing in memory: the terasys massively parallel pim array	data parallel;high level parallel language;random access memory;degradation;yarn;application software;data parallel bit c;integrated memory circuits;4 bit terasys massively parallel pim array processing in memory processor in memory chip standard 4 bit memory single bit alu supercomputer simd computing terasys workstations sun microsystems sparcstation 2 workstations address space pim memory high level parallel language data parallel bit c simd array multiple cray ymp processors supercomputer performance cray 3 memory 8 mb;prototypes;processor in memory chip;multiple cray ymp processors;4 bit;terasys workstations;terasys massively parallel pim array;cray 3 memory;supercomputer;chip;sun microsystems sparcstation 2 workstations;single bit alu;computer architecture;simd array;processor in memory;8 mb;workstations;sun;simd computing;workstations application software supercomputers random access memory prototypes costs computer architecture sun degradation;parallel processing workstations microprocessor chips integrated memory circuits;collaborative research;supercomputer performance;parallel languages;processing in memory;supercomputers;pim memory;standard 4 bit memory;address space;parallel processing;microprocessor chips	SRC researchers have designed and fabricated a processor-in-memory (PIM) chip, a standard 4-bit memory augmented with a single-bit ALU controlling each column of memory. In principle, PIM chips can replace the memory of any processor, including a supercomputer. To validate the notion of integrating SIMD computing into conventional processors on a more modest scale, we have built a half dozen Terasys workstations, which are Sun Microsystems Sparcstation-2 workstations in which 8 megabytes of address space consist of PIM memory holding 32K single-bit ALUs. We have designed and implemented a high-level parallel language, called data parallel bit C (dbC), for Terasys and demonstrated that dbC applications using the PIM memory as a SIMD array run at the speed of multiple Cray-YMP processors. Thus, we can deliver supercomputer performance for a small fraction of supercomputer cost. Since the successful creation of the Terasys research prototype, we have begun work on processing in memory in a supercomputer setting. In a collaborative research project, we are working with Cray Computer to incorporate a new Cray-designed implementation of the PIM chips into two octants of Cray-3 memory. >		Maya Gokhale;William Holmes;Ken Iobst	1995	IEEE Computer	10.1109/2.375174	chip;parallel processing;computer architecture;application software;supercomputer;parallel computing;degradation;workstation;computer hardware;computer science;operating system;prototype;4-bit;address space;computing with memory;supercomputer architecture	Visualization	-11.134560299624129	47.186628717561916	67985
35b6c189e068ea89ba30ebf90e9dbc209212bbea	architecture-aware mapping and scheduling of ima partitions on multicore platforms		Integrated Modular Avionics (IMA) architecture has emerged as the de-facto standard for hosting multiple avionic functions with different criticality levels on the same hardware platform. To further reduce size, weight, power and cost, the second generation IMA architecture aims to migrate multiple singlecore IMA partitions onto a multi-core hardware platform. In this paper, we propose a framework to safely allocate and schedule communicating, mixed-criticality IMA partitions on a cache-based multi-core platform with the added constraint that certain partition pairs should not be allocated on the same core for safety and security reasons. Simulation results demonstrate the effectiveness of our approach in allocating and scheduling partitions while respecting all constraints.	criticality matrix;integrated modular avionics;mixed criticality;multi-core processor;scheduling (computing);self-organized criticality;simulation	Chetan Limbachiya;Harini Ramaprasad	2018		10.1145/3273905.3273914	avionics;distributed computing;architecture;embedded system;cache;scheduling (computing);multi-core processor;criticality;computer science;static timing analysis;integrated modular avionics	Arch	-8.042397798015374	58.26987769860733	68027
21619ecab0ffa51014694d62a56ce865633576db	design space exploration to find the optimum cache and register file size for embedded applications	embedded processor	In the future, embedded processors must process more computation-intensive network applications and internet traffic and packet-processing tasks become heavier and sophisticated. Since the processor performance is severely related to the average memory access delay and also the number of processor registers affects the performance, cache and register file are two major parts in designing embedded processor architecture. Although increasing cache and register file size leads to performance improvement in embedded applications and packet-processing tasks in high traffic networks with too much packets, the increased area, power consumption and memory hierarchy delay are the overheads of these techniques. Therefore, implementing these components in the optimum size is of significant interest in the design of embedded processors. This paper explores the effect of cache and register file size on the processor performance to calculate the optimum size of these components for embedded applications. Experimental results show that although having bigger cache and register file is one of the performance improvement approaches in embedded processors, however, by increasing the size of these parameters over a threshold level, performance improvement is saturated and then, decreased.	cpu cache;central processing unit;computation;design space exploration;embedded system;memory hierarchy;network packet;processor register;register file	Mehdi Alipour;Mostafa E. Salehi;Hesamodin Shojaei Baghini	2011	CoRR		embedded system;pipeline burst cache;parallel computing;real-time computing;cache coloring;computer hardware;computer science;operating system;processor register;cache algorithms	Arch	-5.858228000277384	54.16508482135245	68097
2851ebde638ed3ce62683c52eff0d35fb11f8d95	an evaluation of multiprocessor cache coherence based on virtual memory support	multiprocessor interconnection networks;vm level system software;virtual memory;cache size;write through cache;performance evaluation;software maintenance;multiprocessor cache coherence;crossbar interconnections;buffer storage;virtual memory support;memory access latencies;trace driven simulations;memory access;bus interconnections;vm based cache coherence;shared memory systems;write back cache;virtual machines;multiprocessor cache coherence evaluation;shared memory multiprocessors;coherence software maintenance virtual manufacturing hardware multiprocessor interconnection networks computer science buildings bandwidth broadcasting access protocols;cache coherence;access protocols;arbitrary interconnection networks;bandwidth;coherence;vm based cache coherence multiprocessor cache coherence evaluation virtual memory support shared memory multiprocessors vm translation hardware memory access vm level system software multiprocessor cache coherence arbitrary interconnection networks trace driven simulations write through cache write back cache memory access latencies bus interconnections crossbar interconnections cache size;performance evaluation shared memory systems virtual storage buffer storage virtual machines;computer science;broadcasting;vm translation hardware;virtual manufacturing;virtual storage;buildings;hardware;shared memory multiprocessor	This paper presents an evaluation of the impact of several architectural parameters on the performance of Virtual Memory (VM) based cache coherence schemes for shared-memory multiprocessors. The VM-based cache coherence schemes use the traditional VM translation hardware on each processor to detect memory access attempts that might leave caches incoherent, and maintain coherence through VM-level system software. The implementation of this class of coherence schemes is exible and economical: It allows di erent consistency models, requires no special hardware for multiprocessor cache coherence, and supports arbitrary interconnection networks. We used trace-driven simulations to evaluate the e ect of di erent architectural parameters on the performance of the VM-based schemes. These parameters include VM page sizes, write-back and writethrough caches, memory access latencies, bus and crossbar interconnections, and di erent cache sizes. Our results show that for appropriate parameters VM-based cache coherence is an economical and practical approach for building shared-memory multiprocessors.	address space;cpu cache;cache (computing);cache coherence;central processing unit;compiler;consistency model;crossbar switch;dimm;directory (computing);electrical connection;ibm notes;interconnection;memory address;memory hierarchy;multi-level cell;multiprocessing;overhead (computing);page (computer memory);programmer;sequential consistency;shared memory;simulation;snoopy cache;supercomputer;uniprocessor system	Karin Petersen;Kai Li	1994		10.1109/IPPS.1994.288306	bus sniffing;cache coherence;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;mesi protocol;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;non-uniform memory access	Arch	-11.202890592324026	49.06884405754584	68101
d98eed65334d26ab1e2e9c40919d355537f57535	myrixen: message passing in xen virtual machines over myrinet and ethernet	virtual machine;dma;virtualization;ethernet;mx;i o;myrixen;xen;low latency;data access;message passing;myri 10g;linux;myrinet;high performance;virtualized i o;cloud computing	Data access in HPC infrastructures is realized via user-level networking and OS-bypass techniques through which nodes can communicate with high bandwidth and low-latency. Virtualizing physical components requires hardware-aided software hypervisors to control I/O device access. As a result, line-rate bandwidth or lower latency message exchange over 10GbE interconnects hosted in Cloud Computing infrastructures can only be achieved by alleviating software overheads imposed by the Virtualization abstraction layers, namely the VMM and the driver domains which hold direct access to I/O devices. In this paper, we present MyriXen, a framework in which Virtual Machines efficiently share network I/O devices bypassing overheads imposed by the VMM or the driver domains. MyriXen permits VMs to optimally exchange messages with the network via a high performance NIC, leaving security and isolation issues to the Virtualization layers. Smart Myri-10G NICs provide hardware abstractions that facilitate the integration of the MX semantics in the Xen split driver model. With MyriXen, multiple VMs exchange messages using the MX message passing protocol over Myri-10G interfaces as if the NIC was assigned solely to them. We believe that MyriXen can integrate message passing based application in clusters of VMs provided by Cloud Computing infrastructures with near-native performance.	cloud computing;data access;elegant degradation;hypervisor;input/output;message passing interface;network interface controller;network performance;operating system;performance evaluation;prototype;random access;speaker wire;user space;virtual machine manager	Anastassios Nanos;Nectarios Koziris	2009		10.1007/978-3-642-14122-5_45	data access;input/output;embedded system;parallel computing;message passing;virtualization;cloud computing;computer science;virtual machine;operating system;direct memory access;ethernet;mx record;linux kernel;computer network;low latency	Arch	-16.8799993072184	51.486465053724466	68200
bc8ecd9a75553edb88ae8ebea67b69a03bac5ab0	limiting factors of join performance on parallel processors	performance evaluation;limiting factor;maximum speedup join performance parallel processors parallel processing relational join operations join attribute values task processing parallel architecture bottleneck task synchronization;statistics parallel processing performance evaluation relational databases;statistics;relational databases;parallel architecture;parallel processing;parallel architectures parallel processing stochastic processes delay database machines proposals prototypes microprocessors message passing robustness	The effectiveness of parallel processing of relational join operations is examined. The skew in the distribution of join attribute values and the stochastic nature of the task processing times are identified as the major factors that can affect the effective utilization of parallelism. When many small processors are used in the parallel architecture, the skew can result in some processors becoming sources of bottleneck while other processors are being under utilized. Even in the absence of skew, the variations in the processing times of the parallel tasks belonging to a query can lead to high task synchronization delay and impact the maximum speedup achievable through parallel execution. Analytic expressions for join execution time are developed for different task time distributions with or without skew. >	microprocessor	M. Seetha Lakshmi;Philip S. Yu	1989		10.1109/ICDE.1989.47254	parallel processing;parallel computing;limiting factor;embarrassingly parallel;relational database;computer science;theoretical computer science;database;distributed computing;statistics	DB	-10.649220131763592	50.108160151707054	68286
3befa1b85b22ce5125eca9b8ec7f48f5ab6d48e1	high bandwidth on-chip cache design	pipelined cache cache design performance high bandwidth cache dual ported multicycle pipelined sram cache line buffer;cache storage;memory architecture cache storage;dynamic superscalar;chip;operating system;bandwidth delay microprocessors system on a chip buffer storage operating systems time measurement random access memory clocks frequency;memory architecture;banked cache;hitting time;spec95;superscalar processor;memory bandwidth;dual ported cache	In this paper, we evaluate the performance of high bandwidth cache organizations employing multiple cache ports, multiple cycle hit times, and cache port efficiency enhancements, such as load all and line buffer, to find the organization that provides the best processor performance. Using a dynamic superscalar processor running realistic benchmarks that include operating system references, we use execution time to measure processor performance. When the cache is limited to a single cache port without enhancements, we find that two cache ports increase processor performance by 25 percent. With the addition of line buffer and load all to a single pelted cache, the processor achieves 91 percent of the performance of the same processor containing a cache with two ports. When the processor is not limited to a single cache port, the results show that a large dual-ported multicycle pipelined SRAM cache with a line buffer maximizes processor performance. A large pipelined cache provides both a low miss rate and a high CPU clock frequency. Dual-porting the cache and using a line buffer provide the bandwidth needed by a dynamic superscalar processor. The line buffer makes the pipelined dual-ported cache the best option by increasing cache port bandwidth and hiding cache latency.		Kenneth M. Wilson;Kunle Olukotun	2001	IEEE Trans. Computers	10.1109/12.919276	chip;bus sniffing;least frequently used;pipeline burst cache;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;computer hardware;telecommunications;cache;computer science;write-once;cache invalidation;operating system;direct memory access;uncore;write buffer;smart cache;hitting time;mesi protocol;cache algorithms;cache pollution;memory bandwidth;mesif protocol;statistics;non-uniform memory access	EDA	-8.165579594493654	52.86948462140072	68396
c65e4da776e144df39180df3324c92163c407b15	characterization of data movement requirements for sparse matrix computations on gpus		Tight data movement lower bounds are known for dense matrix-vector multiplication and dense matrix-matrix multiplication and practical implementations exist on GPUs that achieve performance quite close to the roofline bounds based on operational intensity. For large dense matrices, matrix-vector multiplication is bandwidth-limited and its performance is significantly lower than matrix-matrix multiplication. However, in contrast, the performance of sparse matrix-matrix multiplication (SpGEMM) is generally much lower than that of sparse matrix-vector multiplication (SpMV). In this paper, we use a combination of lower-bounds and upper-bounds analysis of data movement requirements, as well as hardware counter based measurements to gain insights into the performance limitations of existing implementations for SpGEMM on GPUs. The analysis motivates the development of an adaptive work distribution strategy among threads and results in performance enhancement for SpGEMM code on GPUs.	concurrency (computer science);cryptographic hash function;data access;data element;graphics processing unit;hardware performance counter;matrix multiplication;requirement;sorting;sparse matrix;utility functions on indivisible goods	Süreyya Emre Kurt;Vineeth Thumma;Changwan Hong;Aravind Sukumaran-Rajam;P. Sadayappan	2017	2017 IEEE 24th International Conference on High Performance Computing (HiPC)	10.1109/HiPC.2017.00040	parallel computing;sparse matrix;computer science;computation;data analysis;multiplication;thread (computing);matrix (mathematics);general-purpose computing on graphics processing units	HPC	-10.402985819490835	49.942192648286515	68666
e1f53ca611fb4a728f7bb0c808f1f618c945eac2	key-value-links: a new data model for developing efficient rdma-based in-memory stores		This paper proposes a new data model, named Key-Value-Links (KVL), to help in memory store utilizes RDMA eciently. The KVL data model is essentially a key-value model with several extensions. This model organizes data as a network of items in which items are connected to each other through links. Each link is a pointer to the address of linked item and is embedded into the item establishing this link. Organizing datasets using the KVL model enables applications making use RDMA-Reads to directly fetch items at the server at very high speed. Since link chasing bypasses the CPU at the server side, this operation allows the client to read items at extremely low latency and reduces much workload at data nodes. Furthermore, our model well ts many real-life applications ranging from graph exploration and map matching to dynamic web page creation. We also developed an in-memory store utilizing the KVL model named KELI. The results of experiments on real-life workload indicate that KELI, without being applied much optimization, easily outperform Memcached, a popular in-memory key-value store, in many cases.	attribute–value pair;data model;remote direct memory access	Duc Hai Nguyen;The De Vu;Duc Hieu Nguyen;Minh Duc Le;Tien Hai Ho;Tran Vu Pham	2017	Informatica (Slovenia)		workload;computer science;pointer (computer programming);machine learning;latency (engineering);server-side;artificial intelligence;real-time computing;data model;remote direct memory access;map matching;ranging;distributed computing	DB	-15.13792192813793	54.496052649917715	68671
57da368cdc056f25e54cc2fb7386c71a2b187720	nilmtk v0.2: a non-intrusive load monitoring toolkit for large scale data sets: demo abstract	model predictive control;hvac;occupancy	In this demonstration, we present an open source toolkit for evaluating non-intrusive load monitoring research; a field which aims to disaggregate a household's total electricity consumption into individual appliances. The toolkit contains: a number of importers for existing public data sets, a set of preprocessing and statistics functions, a benchmark disaggregation algorithm and a set of metrics to evaluate the performance of such algorithms. Specifically, this release of the toolkit has been designed to enable the use of large data sets by only loading individual chunks of the whole data set into memory at once for processing, before combining the results of each chunk.	algorithm;benchmark (computing);importer (computing);information privacy;open-source software;preprocessor	Jack Kelly;Nipun Batra;Oliver Parson;Haimonti Dutta;William J. Knottenbelt;Alex Rogers;Amarjeet Singh;Mani B. Srivastava	2014		10.1145/2674061.2675024	hvac;real-time computing;simulation;computer science;operating system;data mining;occupancy;model predictive control	AI	-18.937801884061276	57.07815292488292	68689
2f2b6f2521f47b2eb4ab338b2cfd225075f9de68	real-time scheduling algorithms for maximum utilization of secondary battery in portable devices with discrete frequency control	frequency control;secondary battery;portable device;scheduling;real time scheduling;real time system	Abstract#R##N##R##N#The spread of portable devices has increased the importance of low-power energy techniques. The authors have already proposed a real-time scheduling algorithm which maximizes the utilization time of the secondary battery while preserving deadlines of real-time tasks with arbitrary frequency control. This paper addresses a scheduling problem which maximizes the utilization time of the secondary battery with discrete frequency control. First, a decision problem corresponding to the scheduling problem is NP-complete when the frequency can be changed on task switching. Second, when the frequency can be changed at an arbitrary time we propose an optimal scheduling algorithm which maximizes utilized time of the secondary battery under preserving a deadline of real-time tasks, and demonstrate the correctness of the algorithm and its computation complexity. For each arrival task, the computation complexity of the algorithm is O(n) in general, where n is the number of tasks awaiting execution, and is O(1) in the amortized analysis when each task arrives in the order of its deadline. © 2002 Scripta Technica, Syst Comp Jpn, 33(3): 41–51, 2002	algorithm;discrete frequency domain;personal digital assistant;real-time transcription;rechargeable battery;scheduling (computing)	Yukikazu Nakamoto;Yoshihiro Tsujino;Nobuki Tokura	2002	Systems and Computers in Japan	10.1002/scj.1112	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;real-time computing;earliest deadline first scheduling;simulation;real-time operating system;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;automatic frequency control;least slack time scheduling;scheduling	Embedded	-5.5821486058698	59.35747112189225	68828
38f8191262be1eac7649219add0addc64046d3e0	multi-level bitmap indexes for flash memory storage	flash memory;data management bitmap index flash memory;sensor node;97;storage system;query processing;sorting;power efficiency;performance;data management;indexing method;indexation;benchmarks;data warehousing;magnetic disks;magnetic storage devices;velocity data management bitmap index flash memory;storage	Due to their low access latency, high read speed, and power-efficient operation, flash memory storage devices are rapidly emerging as an attractive alternative to traditional magnetic storage devices. However, tests show that the most efficient indexing methods are not able to take full advantage of flash memory storage devices. In this paper, we present a set of multi-level bitmap indexes that can effectively utilize flash storage devices. These indexing methods use coarsely binned indexes to answer queries approximately, and then use finely binned indexes to refine the answers. Our new methods read significantly lower volumes of data at the expense of an increased disk access count, thus taking full advantage of the improved read speed and low access latency of flash devices. To demonstrate the advantage of these new indexes, we measure their performance on a number of storage systems using a standard data warehousing benchmark called the Set Query Benchmark. We observe that multilevel strategies on flash drives are up to 3 times faster than traditional indexing strategies on magnetic disk drives.	benchmark (computing);bitmap;flash memory;interrupt latency;magnetic storage;usb flash drive	Kesheng Wu;Kamesh Madduri;Shane Canon	2010		10.1145/1866480.1866497	flash file system;parallel computing;electrical efficiency;sensor node;computer hardware;performance;data management;computer science;sorting;data warehouse;database;sequential access memory	DB	-13.875071686396891	54.26372822562092	68829
49e8d44693032cdc192bc3d38881ffb9fbc6ea5b	performance based multiple server support for distributed real time applications	performance measure;queuing model;system performance;system design;communication delay;real time application;real time systems;time constraint	In real time systems, tasks must be performed correctly within time constraints. The system throughput is also a key performance measure. One of the difficulties, which the system designer faces, is to control these performance factors. A heavily requested computation, or a subtask that requires long service time, may reduce the system performance. This paper presents two different solutions for the above problem using multiple servers to support the execution of such a subtask. The first approach uses a pipeline subtask decomposition while the second approach uses a set of parallel servers. A queuing model, that takes into consideration the communication delay, is used for the analysis. In addition, the proposed approaches consider the operating environment, both system and user constraints, and the required resource cost.	computation;operating environment;pipeline (computing);queueing theory;real-time computing;systems design;throughput	Tahany A. Fergany;Howard A. Sholl	1990		10.1145/99412.99469	real-time computing;simulation;computer science;distributed computing	HPC	-11.622156817529854	59.73743563513978	68893
88dafd20226925927d96d90f3c50ef0cca079237	a data-intensive workflow scheduling algorithm for large-scale cooperative work platform	groupware;cooperative data intensive workflow scheduling;data transferring data intensive workflow scheduling algorithm large scale cooperative work platform collaboration;scheduling algorithm large scale systems collaborative work processor scheduling algorithm design and analysis computer networks computer science dynamic scheduling costs large scale integration;workflow management software groupware scheduling;large dataset;large scale;scheduling algorithm;scheduling;workflow management software;data transfer;cooperative work	With the development of the society and the advancement of technology, the collaboration is being more and more important. A large-scale cooperative work platform is a platform which integrates computational, storage and network resources distributed in various organizations or locations and utilize these resources cooperatively to achieve one goal, such as an e-science or e-business platform. The data-intensive workflow on these platforms has gained much more attentions in recent times. Data-intensive workflow needs to access, process and transfer large datasets that may each be replicated on different data hosts. In this paper, we introduce an algorithm MDTT to select the resource set which the task should be mapped. Our experiments show that our algorithm is able to minimize the total makespan of data-intensive workflow and the time of data transferring.	algorithm;data-intensive computing;e-science;electronic business;experiment;makespan;scheduling (computing)	Li-zhen Cui;Meng Xu;Haiyang Wang	2009	2009 13th International Conference on Computer Supported Cooperative Work in Design	10.1109/CSCWD.2009.4968106	fair-share scheduling;fixed-priority pre-emptive scheduling;real-time computing;dynamic priority scheduling;computer science;operating system;two-level scheduling;database;distributed computing;scheduling;scheduling;workflow management system;workflow engine;workflow technology	DB	-18.294270789976444	59.78036508814895	69085
6576abc481ab88dee2f1e8aaf332a6a93adeef36	scheduling multiple independent hard-real-time jobs on a heterogeneous multiprocessor	software defined radio;real time;time division multiplex;dataflow;scheduling;multiprocessor system on chip;wireless lan;multi processor;hard real time	This paper proposes a scheduling strategy and an automatic scheduling flow that enable the simultaneous execution of multiple hard-real-time dataflow jobs. Each job has its own execution rate and starts and stops independently from other jobs, at instants unknown at compile-time, on a multiprocessor system-on-chip. We show how a combination of Time-Division Multiplex (TDM) and static-order scheduling can be modeled as additional nodes and edges on top of the dataflow representation of the job using Single-Rate Dataflow semantics to enable tight worst-case temporal analysis. We also propose algorithms to find combined TDM/static order schedules for jobs that guarantee a requested minimum throughput and maximum latency, while minimizing the usage of processing resources. We illustrate the usage of these techniques for a combination of Wireless LAN and TD-SCDMA radio jobs running on a prototype Software-Defined Radio platform.	algorithm;best, worst and average case;compile time;compiler;computation;dataflow;job scheduler;job stream;mpsoc;micro job;multiplexing;multiprocessing;overhead (computing);prototype;real-time computing;real-time transcription;real-time web;requirement;scheduling (computing);slack variable;system on a chip;throughput;toad data modeler	Orlando Moreira;Frederico Valente;Marco Bekooij	2007		10.1145/1289927.1289941	embedded system;parallel computing;real-time computing;computer science;operating system;dataflow;software-defined radio;distributed computing;programming language;scheduling;multiprocessor scheduling	Embedded	-9.007643411252937	59.469179705169765	69114
72c2092f2952ce5668cf5c7bbaa12530774eb6af	energy-efficient, utility accrual scheduling under resource constraints for mobile embedded systems	control theory;resource constraint;energy efficient;utility accrual;efficiency;electronic equipment;satisfiability;embedded system;resource use;mutual exclusion;statistical properties;simulation experiment;energy consumption;real time scheduling;voltage regulation;time utility functions;energy efficient scheduling;polynomial time;utility accrual scheduling;algorithms;time utility function;real time systems;time constraint	We present an energy-efficient real-time scheduling algorithm called the Resource-constrained Energy-Efficient Utility Accrual Algorithm (or ReUA). ReUA considers an application model where activities are subject to time/utility function-time constraints, resource dependencies including mutual exclusion constraints, and statistical performance requirements including probabilistically satisfied, activity (timeliness) utility bounds. Further, ReUA targets mobile embedded systems where system-level energy consumption is a major concern. For such a model, we consider the scheduling objectives of (1) satisfying statistical performance requirements, and (2) maximizing system-level energy efficiency, while respecting resource dependencies. Since the problem is NP-hard, ReUA allocates resources using statistical properties of application cycle demands and heuristically computes schedules with a polynomial-time cost. We analytically establish several timeliness and non-timeliness properties of the algorithm. Further, our simulation experiments illustrate ReUA's effectiveness.	algorithm;aperiodic graph;correctness (computer science);descriptive video service;embedded system;experiment;heuristic;mutual exclusion;np-hardness;real-time clock;real-time locating system;requirement;scheduling (computing);simulation;time complexity;time-utility function;utility	Haisang Wu;Binoy Ravindran;E. Douglas Jensen;Peng Li	2004		10.1145/1017753.1017768	time complexity;mathematical optimization;real-time computing;mutual exclusion;computer science;efficiency;efficient energy use;programming language;algorithm;voltage regulation;satisfiability	Embedded	-5.708267852875092	59.521998035730704	69155
f2e63f47836c92ca910c5ba6ebb211be6daf7b56	an asynchronous checkpoint-based redundant multithreading architecture	multi threading;slave thread;instruction sets context registers multithreading computer architecture pipelines;fault tolerant;fault restoration asynchronous checkpoint redundant multithreading architecture fault detection master thread slave thread ac rmt architecture;master thread;asynchronous checkpoint;checkpointing;ac rmt architecture;computer architecture;fault tolerant computing;redundancy;parallel architectures;redundancy checkpointing fault tolerant computing multi threading parallel architectures;registers;pipelines;fault detection;fault tolerance;multithreaded architecture;checkpoint;smt;redundant multithreading;fault tolerance smt redundant multithreading checkpoint fault detection;context;redundant multithreading architecture;fault restoration;instruction sets;multithreading	Existing redundant multithreading (RMT) detects faults by comparing the result of each instruction between the master and slave threads, which can lead to huge comparison and communication overhead. To address this problem, the checkpoint-based RMT (like RVQ_F) was proposed, but in such architectures, master threads must wait for slave threads to arrive at the same position at each checkpoint, this may delay the release of resources occupied by master threads and decrease performance. This paper proposes an asynchronous checkpoint-based redundant multithreading architecture (AC-RMT), in which two context saving rooms are set aside for each thread, one for detecting faults, and the other for saving the last checkpoint used for fault restoration. Compared with RVQ_F, AC-RMT efficiently boosts performance because, by avoiding the waiting of master threads at checkpoints, resources can be released timely.	ac adapter;application checkpointing;circuit restoration;multithreading (computer architecture);overhead (computing);sensor;simultaneous multithreading;thread (computing);transaction processing system;virtual economy	Jie Yin;Jianhui Jiang	2010	2010 IEEE 16th Pacific Rim International Symposium on Dependable Computing	10.1109/PRDC.2010.27	reliability engineering;fault tolerance;computer architecture;parallel computing;real-time computing;multithreading;computer science;operating system;temporal multithreading;super-threading	Arch	-15.885614448628441	48.752789136453615	69170
c4b6222b4a91047598db268dfc05ac495f889389	a group-based job scheduling method for parallel volunteer computing	parallel computing parallel volunteer computing group based job scheduling method parallel vc model redundant computation expected completion probability worker defection rate;volunteer computing parallel processing scheduling;parallel computing volunteer computing credibility calculation;probability;processor scheduling;distributed processing;computer applications;computational modeling;redundancy;scheduling;computational modeling processor scheduling probability scheduling redundancy computer applications distributed processing	Toward the realization of parallel Volunteer Computing (VC), we propose a group-based job scheduling method based on the expected completion probability. A critical problem that must be addressed in the parallel VC is the volatility of nodes (workers), if any workers leave the VC system, jobs may never be completed due to the inability to communicate with the missing workers. We first define a new parallel VC model and then propose a group-based job scheduling method. We focus on the approach of redundant computation used for removing erroneous results and extend it to deal with the volatility of workers. In the proposed job scheduling method, groups of workers are determined adaptively for each job by calculating expected completion probability of the job considering worker defection rate. This method allows to increase the probability of job's completion, thus leading to the reduction in the computation time. Experimental results indicate that the proposed method reduces completion time of VC about 60%, compared to a simple method which does not consider the worker defection.	computation;job scheduler;job shop scheduling;job stream;scheduling (computing);time complexity;volatility;volunteer computing	Kaworu Ochi;Masaru Fukushi	2015	2015 Third International Symposium on Computing and Networking (CANDAR)	10.1109/CANDAR.2015.74	fair-share scheduling;parallel computing;real-time computing;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;two-level scheduling;distributed computing	HPC	-15.077132188573206	60.097743896903815	69326
462a3299479f313d4ceae8882f1917cc5e39bf11	selective buddy allocation for scheduling parallel jobs on clusters	processor scheduling;resource allocation;trace based simulation performance selective buddy allocation parallel job scheduling clusters contiguous node allocation backfilling job scheduler improved run time increased wait time;virtual machines;waiting time;virtual machines processor scheduling workstation clusters parallel processing resource allocation;processor scheduling laboratories runtime concurrent computing topology job production systems performance evaluation;workstation clusters;job scheduling;parallel processing	In this paper, we evaluate the performance implications of using a buddy scheme for contiguous node allocation, in conjunction with a backfilling job scheduler for clusters. When a contiguous node allocation strategy is used, there is a trade-off between improved run-time of jobs (due to reduced link contention and lower communication overhead) and increased wait-time of jobs (due to external fragmentation of the processor system). Using trace-based simulation, a buddy strategy for contiguous node allocation is shown to be unattractive compared to the standard non-contiguous allocation strategy used in all production job schedulers. A simple but effective scheme for selective buddy allocation is then proposed, that is shown to perform better than non-contiguous allocation.	buddy memory allocation;fragmentation (computing);job scheduler;job stream;overhead (computing);scheduling (computing);trace-based simulation	Vijay Subramani;Rajkumar Kettimuthu;Srividya Srinivasan;Jeanette Johnston;P. Sadayappan	2002		10.1109/CLUSTR.2002.1137735	parallel processing;parallel computing;real-time computing;resource allocation;computer science;virtual machine;job scheduler;operating system;static memory allocation;distributed computing	HPC	-15.26981976896635	59.25205493131965	69342
a9d0660cfc5f6fed771623dca546b648b16a154e	dynamically adaptive register file architecture for energy reduction in embedded processors	energy efficiency;embedded systems;low power;register file;processor architectures	Energy reduction in embedded processors is a must since most embedded systems run on batteries and processor energy reduction helps increase usage time before needing a recharge. Register files are among the most power consuming parts of a processor core. Register file power consumption mainly depends on its size (height as well as width), especially in newer technologies where leakage power is increasing. We provide a register file architecture that, depending on the application behavior, dynamically (i) adapts the width of individual registers, and (ii) puts partitions of temporarily unused registers into low-power mode, so as to save both static and dynamic power. We show that our scheme increases register file area by 3.6% and imposes 2.85% performance overhead on average. Our experimental results on OpenRISC 1200 processor and with selected MiBench benchmark suite show up to 29%, and 54% (24% and 49% on average) reduction in dynamic and static energy consumption of the register file, respectively. 2015 Elsevier B.V. All rights reserved.	benchmark (computing);central processing unit;embedded system;low-power broadcasting;multi-core processor;openrisc 1200;overhead (computing);power supply;rechargeable battery;register file;spectral leakage	Mohammad Khavari Tavana;Saba Ahmadian Khameneh;Maziar Goudarzi	2015	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2015.01.005	embedded system;parallel computing;real-time computing;computer hardware;control register;computer science;memory buffer register;operating system;register renaming;stack register;efficient energy use;processor register;register file	Arch	-5.547266249147708	55.351950108073915	69517
0ef104b581f3b05346b3995cdd69b9fad74b53a5	a distributed opencl framework using redundant computation and data replication	clusters;heterogeneous computing;redundant computation;data replication;runtime systems;programming models;opencl	Applications written solely in OpenCL or CUDA cannot execute on a cluster as a whole. Most previous approaches that extend these programming models to clusters are based on a common idea: designating a centralized host node and coordinating the other nodes with the host for computation. However, the centralized host node is a serious performance bottleneck when the number of nodes is large. In this paper, we propose a scalable and distributed OpenCL framework called SnuCL-D for large-scale clusters. SnuCL-D's remote device virtualization provides an OpenCL application with an illusion that all compute devices in a cluster are confined in a single node. To reduce the amount of control-message and data communication between nodes, SnuCL-D replicates the OpenCL host program execution and data in each node. We also propose a new OpenCL host API function and a queueing optimization technique that significantly reduce the overhead incurred by the previous centralized approaches. To show the effectiveness of SnuCL-D, we evaluate SnuCL-D with a microbenchmark and eleven benchmark applications on a large-scale CPU cluster and a medium-scale GPU cluster.	benchmark (computing);cuda;central processing unit;centralized computing;computation;fortran;gpu cluster;graphics processing unit;mathematical optimization;opencl api;overhead (computing);programming model;replication (computing);scalability;scheduling (computing)	Junghyun Kim;Gangwon Jo;Jaehoon Jung;Jungwon Kim;Jaejin Lee	2016		10.1145/2908080.2908094	parallel computing;computer science;operating system;distributed computing;programming paradigm;programming language;symmetric multiprocessor system;replication	PL	-13.051092316670267	46.85203289792141	69593
cb8db56c3a0dfcf19607aeae0a539192871cb8ef	an interference miss isolation mechanism based on skewed mapping for shared cache in chip multiprocessors	interference artificial intelligence tiles hardware program processors memory management benchmark testing	Inter-thread or intra-thread interference misses may be incurred due to the conflict among different threads when the least-recently-used (LRU) victim candidate is evicted from the shared last level cache in Chip Multiprocessors (CMP). To alleviate this problem, an interference miss isolation mechanism based on skewed mapping (IMI-SM) is proposed for the shared cache in this paper, which is aimed to mitigate the conflict miss phenomena when the on-chip cache is occupied competitively by multiple threads in CMP. The skewed mapping mechanism in IMI-SM is triggered once an interference miss is predicted. The new incoming data fetched from the off-chip memory can be dynamically placed in an dedicated isolation cache or the cache set which is under light pressure, so the negative impact caused by the interference misses in the shared cache is mitigated effectively. Experimental results based on full system simulator demonstrate that IMI-SM can reduce the interference misses in the shared last level cache to a certain extent, and the system performance is improved significantly with negligible hardware overhead.	cpu cache;central processing unit;computer architecture simulator;computer memory;interference (communication);internal market information system;overhead (computing);run time (program lifecycle phase)	Anwen Huang;Chao Song;Wei Guo;Peng Li;Minxuan Zhang	2013	2013 IEEE 10th International Conference on ASIC	10.1109/ASICON.2013.6811977	bus sniffing;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;computer hardware;cache;computer science;write-once;cache invalidation;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol	Arch	-9.440756996368346	52.591814034222914	69601
a380a83d1e4e9433578a356be4eb41a00e41dc97	register allocation for embedded systems to simultaneously reduce energy and temperature on registers	rotator;register allocation;bit transition activity;heat buildup	Energy and thermal issues are two important concerns for embedded system design. Diminished energy dissipation leads to a longer battery life, while reduced temperature hotspots decelerate the physical failure mechanisms. The instruction fetch logic associated with register access has a significant contribution towards the total energy consumption. Meanwhile, the register file has also been previously shown to exhibit the highest temperature compared to the rest of the components in an embedded processor. Therefore, the optimization of energy and the resolution of the thermal issue for register accesses are of great significance. In this article, register allocation techniques are studied to simultaneously reduce energy consumption and heat buildup on register accesses for embedded systems. Contrary to prevailing intuition, we observe that optimizing energy and optimizing temperature on register accesses conflict with each other. We introduce a rotator hardware in the instruction decoder to facilitate a balanced solution for the two conflicting objectives. Algorithms for register allocation and refinement are proposed based on the access patterns and the effects of the rotator. Experimental results show that the proposed algorithms obtain notable improvements of energy and peak temperature for embedded applications.	algorithm;central processing unit;embedded system;failure cause;graph coloring;hotspot (wi-fi);mathematical optimization;refinement (computing);register allocation;register file;systems design	Tiantian Liu;Alex Orailoglu;Chun Jason Xue;Minming Li	2013	ACM Trans. Embedded Comput. Syst.	10.1145/2539036.2539046	embedded system;parallel computing;real-time computing;computer hardware;computer science;operating system;register allocation	EDA	-5.811083920959007	55.59882514043621	69669
303905bd3b91ed929c490d309912639495b75248	a case for resource-conscious out-of-order processors: towards kilo-instruction in-flight processors	processor architecture;out of order;register file;memory latency	Modern out-of-order processors tolerate long-latency memory operations by supporting a large number of in-flight instructions. This is achieved in part through proper sizing of critical resources, such as register files or instruction queues. In light of the increasing gap between processor speed and memory latency, tolerating upcoming latencies in this way would require impractical sizes of such critical resources.To tackle this scalability problem, we make a case for resource-conscious out-of-order processors. We present quantitative evidence that critical resources are increasingly underutilized in these processors. We advocate that better use of such resources should be a priority in future research in processor architectures. In particular, we present some of our research having such observations as a basis to deal with future resource conscious processors.	central processing unit	Adrián Cristal;José F. Martínez;Josep Llosa;Mateo Valero	2004	SIGARCH Computer Architecture News	10.1145/1024295.1024296	computer architecture;parallel computing;real-time computing;distributed memory;cas latency;microarchitecture;computer science;out-of-order execution;operating system;register file	Arch	-8.290344468036675	52.016421288000686	69677
427218cb1d0eaecc7fb4eccb7d1a856eff06aa14	workload capacity considering nbti degradation in multi-core systems	minimum communication cost;64-core system show;workload balancing;dynamic task scheduling;balanced workload;device level;multi-core system;capacity rate;fractional nbti model;multicore systems;workload capacity;resource allocation;multiprocessing systems;nbti model;new workload;nbti degradation;reliability;dynamic zoning;negative bias temperature instability;mean-time-to-failure;device feature size;long-term reliability;feature sizes;threshold voltage;dynamic scheduling;mean time to failure;stress	As device feature sizes continue to shrink, long-term reliability such as Negative Bias Temperature Instability (NBTI) leads to low yields and short mean-time-to-failure (MTTF) in multi-core systems. This paper proposes a new workload balancing scheme based on device level fractional NBTI model to balance the workload among active cores while relaxing stressed ones. The proposed method employs the Capacity Rate (CR) provided by the NBTI model, applies Dynamic Zoning (DZ) algorithm to group cores into zones to process task flows, and then uses Dynamic Task Scheduling (DTS) to allocate tasks in each zone with balanced workload and minimum communication cost. Experimental results on 64-core system show that by allowing a small part of the cores to relax over a short time period (10 seconds), the proposed methodology improves multi-core system yield (percentage of core failures) by 20%, while extending MTTF by 30% with insignificant degradation in performance (less than 3%).	algorithm;elegant degradation;mean time between failures;multi-core processor;negative-bias temperature instability;scheduling (computing);task manager	Jin Sun;Roman L. Lysecky;Karthik Shankar;Avinash Karanth Kodi;Ahmed Louri;Janet Roveda	2010	2010 15th Asia and South Pacific Design Automation Conference (ASP-DAC)		embedded system;negative-bias temperature instability;parallel computing;real-time computing;mean time between failures;dynamic priority scheduling;resource allocation;engineering;operating system;reliability;stress;threshold voltage	EDA	-6.0894524927811835	56.83567412041119	69679
460adeaac01403a2da2cc053d954363479e4425a	multiple coherence and coordinated checkpointing protocols for dsm systems	protocols;cluster;sequential consistency;shared memory;protocols fault tolerant computing distributed shared memory systems workstation clusters system recovery;fault tolerance kerrighed dsm cluster system recovery protocol shared memory parallel application checkpointing cluster computing;fault tolerant computing;system recovery;design and implementation;checkpointing kernel containers fault tolerant systems memory management protocols linux operating systems random access memory research and development;distributed shared memory systems;checkpoint;cluster system;workstation clusters;distributed shared memory;parallel applications;rollback	In this article, we address two important issues in DSM research: improving performance and providing reliability. To improve performance, we designed a low-overhead multiple coherence protocol mechanism and to augment the reliability of the system, we propose a coordinated checkpointing/recovery mechanism. Both mechanisms were implemented and incorporated in JIAJIA, a DSM system that implements scope consistency with a write-invalidate protocol. Our results on an eight machine cluster with some popular benchmarks show, for the multiple coherence protocol strategy, a significant reduction on the number of messages exchanged, leading to better performance results. Also, our results for the checkpointing strategy show that the overhead introduced in failure-free executions is small when considering the benefits obtained.	application checkpointing;cache coherence;concatenation;overhead (computing);programmer	Ramamurthy Badrinath;Christine Morin;Geoffroy Vallée	2003	2005 International Conference on Parallel Processing Workshops (ICPPW'05)	10.1109/CCGRID.2003.1199403	distributed shared memory;shared memory;communications protocol;parallel computing;real-time computing;rollback;computer science;operating system;distributed computing;sequential consistency;cluster	HPC	-15.294345010039757	49.50724854804472	69724
eba75258cd6282256ee3b1d61faabfe117c09c68	dynamic indexing: leakage-aging co-optimization for caches	negative bias temperature instability;cache storage;optimisation;aging indexing logic gates stress computer architecture degradation threshold voltage;sram chips ageing cache storage negative bias temperature instability optimisation;reindexing functions dynamic indexing leakage aging co optimization low power states voltage scaling power gating aging phenomena negative bias temperature instability worst idleness pattern aging reductions power managed caches nbti reduced aging symmetric structure sram structures nbti effects value dependent recovery cache indexing function leakage optimization potential cache lifetime power managed units;reliability aging caches leakage power memories negative bias temperature instability nbti;ageing;sram chips	Traditional implementations of low-power states based on voltage scaling or power gating have been shown to have a beneficial effect on the aging phenomena caused by negative bias temperature instability (NBTI), which can be explained in terms of the intuitive correlation between the idleness and the reduced workload of a system. Such a joint benefit has been exploited only partially because of the different nature of energy and aging as cost functions: as a performance figure, aging is affected by the worst idleness pattern. Therefore, large potential energy savings usually result in limited aging reductions. In this paper, we address this problem in the context of power-managed caches, which represent a critical target for NBTI-reduced aging: given their symmetric structure, SRAM structures are, in particular, sensitive to NBTI effects because they cannot take advantage of the value-dependent recovery typical of NBTI. We propose a strategy called dynamic indexing, in which the cache indexing function is changed over time in order to uniformly distribute the idleness over all the various power managed units (e.g., lines). This distribution allows fully using the leakage optimization potential and extending the lifetime of a cache. We explore various alternatives, in particular different granularities of the power managed units as well as different reindexing functions. Experimental analysis shows that it is possible to simultaneously reduce leakage power and aging in caches, with minimal power consumption overhead.	best, worst and average case;cpu cache;dynamic voltage scaling;image scaling;low-power broadcasting;mathematical optimization;negative-bias temperature instability;overhead (computing);power gating;power management;search engine indexing;simulation;spectral leakage;static random-access memory;tracing (software)	Andrea Calimera;Mirko Loghi;Enrico Macii;Massimo Poncino	2014	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2013.2287187	ageing;negative-bias temperature instability;electronic engineering;parallel computing;real-time computing;engineering	EDA	-4.757303781686699	56.15070839711437	69747
02b9ee1bac641a7405eec20760445a62a58827b3	performance evaluation of computing systems with memory hierarchies	random access memory;kernel;degradation;performance evaluation;system configuration;time measurement;computer aided instruction;performance;data processing;system performance;software performance;model evaluation;memory hierarchies;performance computing systems data flow model evaluation memory hierarchies;system performance data flow computing degradation hardware computer aided instruction kernel statistics time measurement program processors software performance;statistics;computing systems;data flow computing;evaluation;data flow model;memory hierarchy;data flow;program processors;data transfer;hardware	Data transfers in computing systems with memory data flow statistics from programs or program sections. hierarchies usually prolong computing time and, consequently, cause iVlonitor and compiler software processings are treated degradation of system performance. A method to determine data processing rates and the relative utilization of memories for various das aor coutational jobs. Hofwaev perfrance system configurations under a variety of program loads is presented. degradation caused by the system software overhead is According to this method, a program-independent ultimate data not considered in the present analysis. processing rate is derived from characteristics of the processor and the fastest random access memory of the system, and degradation II. DEFINITIONS AND A DATA FLOW MODEL OF A factors are determined by combining statistics of the data flowof STORAGE HIERARCHY SYSTEM actual programs and hardware parameters of the processor and all memories. The statistics of data flow in the memory hierarchy are In a general purpose computer, the maximum perforobtained by analyzing a number of recorded address traces of exemance is attainable if all required instructions and data cuted programs. The method presented permits quick evaluation of are available in a fast execution store. However, since system performance for arbitrary time periods and for maximum and the fast store is usually limited in capacity, it must minimum concurrence of operation of processors and memories. constantly exchange information with backing stores	central processing unit;compiler;computer;data flow diagram;dataflow;elegant degradation;fastest;job stream;memory hierarchy;overhead (computing);performance evaluation;random access;random-access memory;tracing (software)	Wilhelm Anacker;Chu Ping Wang	1967	IEEE Trans. Electronic Computers	10.1109/PGEC.1967.264722	data flow diagram;parallel computing;kernel;real-time computing;degradation;distributed memory;software performance testing;data processing;performance;computer science;theoretical computer science;evaluation;operating system;computing with memory;statistics;time	Arch	-12.763725712538248	50.76031471363487	69780
850e869d2c5dfa21b8974c2cf28f6081e10dde66	design and evaluation of the high performance multi-processor server	analytical models;oltp;kernel execution;file servers;protocols;kernel;degradation;buffer storage multiprocessing systems instruction sets reduced instruction set computing file servers transaction processing;low latency cache to cache copy;8 way highly interleaved main memory;degradation kernel reduced instruction set computing analytical models network servers protocols throughput laboratories prototypes large scale systems;prototype risc multi processor server;prototypes;reduced instruction set computing;buffer storage;intensive i o activities;ping ponging;data read accesses;cache misses;network servers;low latency;kernel shared data;online transaction processing;large cache capacity high performance multi processor server prototype risc multi processor server business system oltp online transaction processing low latency cache to cache copy 8 way highly interleaved main memory multi processor system trace driven simulator kernel execution cache misses task migration ping ponging kernel shared data intensive i o activities ow latency cache to cache copy data read accesses;ow latency cache to cache copy;task migration;high performance multi processor server;multi processor system;multiprocessing systems;large cache capacity;transaction processing;business system;high performance;trace driven simulation;trace driven simulator;large scale systems;instruction sets;throughput	This paper discusses the architecture and performance of a prototype RISC multi-processor server designed for a business vstem like OLTP (On Line Transaction Processing). The combination of a low-latency cache-tocache copy and a 8-way highly interleaved main memory realize high performance for the OLTP program. He analyzed the activity qf the multi-processor system when executing the OLTP program by using the tracedriven simulator including kernel execution. The main jindings are that (1) cache misses due to task migration and ping-ponging of kernel shared data occupy a larger part of the total misses, especially in a large cache capacity because of intensive I/O activities; and (2) the low-latency cache-to-cache copy is very egective because 50-60?? of the data read accesses are supplied by the cache-to-cache copy in a large cache capacity.	cpu cache;computer data storage;input/output;kernel (operating system);multiprocessing;online transaction processing;prototype;server (computing)	M. Morioka;K. Kurosawa;S. Miura;T. Nakamikawa;S. Ishikawa	1994		10.1109/ICCD.1994.331856	embedded system;parallel computing;real-time computing;computer science;operating system;online transaction processing;cache pollution	Arch	-13.50869314942298	51.03247560929423	69803
f0fbf66d0492eb38998f2a1f047a1b88d0e3fca5	p3: partitioned path profiling	path profiling;parallel;distributed;divide and conquer	Acyclic path profile is an abstraction of dynamic control flow paths of procedures and has been found to be useful in a wide spectrum of activities. Unfortunately, the runtime overhead of obtaining such a profile can be high, limiting its use in practice. In this paper, we present partitioned path profiling (P3) which runs K copies of the program in parallel, each with the same input but on a separate core, and collects the profile only for a subset of intra-procedural paths in each copy, thereby, distributing the overhead of profiling. P3 identifies “profitable” procedures and assigns disjoint subsets of paths of a profitable procedure to different copies for profiling. To obtain exact execution frequencies of a subset of paths, we design a new algorithm, called PSPP. All paths of an unprofitable procedure are assigned to the same copy. P3 uses the classic Ball-Larus algorithm for profiling unprofitable procedures. Further, P3 attempts to evenly distribute the profiling overhead across the copies. To the best of our knowledge, P3 is the first algorithm for parallel path profiling. We have applied P3 to profile several programs in the SPEC 2006 benchmark. Compared to sequential profiling, P3 substantially reduced the runtime overhead on these programs averaged across all benchmarks. The reduction was 23%, 43% and 56% on average for 2, 4 and 8 cores respectively. P3 also performed better than a coarse-grained approach that treats all procedures as unprofitable and distributes them across available cores. For 2 cores, the profiling overhead of P3 was on average 5% less compared to the coarse-grained approach across these programs. For 4 and 8 cores, it was respectively 18% and 25% less.	algorithm;benchmark (computing);control flow;directed acyclic graph;overhead (computing);profiling (computer programming)	Mohammed Afraz;Diptikalyan Saha;Aditya Kanade	2015		10.1145/2786805.2786868	parallel computing;real-time computing;divide and conquer algorithms;computer science;parallel;distributed computing	SE	-13.459451972386123	47.42318289535251	69864
846a7ff4db185d07ec456b92af404739457d29c8	trends in non-software support for input-output functions	20-odd year;input-output subsystem architecture;alternative input-output subsystem architecture;ibm channel;lower level alternative;input-output function;central processor;divergent approach;current computer system;complex technique;non-software support;input output	Input-output subsystem architectures have evolved over the past 20-odd years to the point where two divergent approaches have found acceptance in current computer systems; the 'IBM channel' is the archetype of the lower level alternative, while the functionally more complex techniques involve a wide spectrum of distributed processor architectures supporting database and/or storage management functions independently with respect to the central processor. The paper traces the historical development of support (outside central processor based software) for input-output functions and concludes with a preliminary comparison of the relative merits of the software interfaces provided by the alternative input-output subsystem architectures.	central processing unit;computer;database;graphical user interface;tracing (software)	Ken J. McDonell	1977		10.1145/800180.810252	input/output;embedded system;spectrum;computer architecture;parallel computing;real-time computing;microarchitecture;computer science;operating system	DB	-12.535158250868902	46.41976202399294	69960
db5bb0bbb7d60df9ce479a351cbac9ac4c5c7b50	predictive memory allocation over skewed streams	skewed data streamscontinuous queries;continuous queries;skewed data streams;adaptive memory management;data stream management system	Adaptive memory management is a serious issue in data stream management. Data stream differ from the traditional stored relational model in several aspect such as the stream arrives online, high volume in size, skewed data distributions. Data skew is a common property of massive data streams. We propose the predicted allocation strategy, which uses predictive processing to cope with time varying data skew. This processing includes memory usage estimation and indexing with timestamp. Our experimental study shows that the predictive strategy reduces both required memory space and latency time for skewed data over varying time.	memory management	Hong-Won Yun	2009	J. Inform. and Commun. Convergence Engineering		real-time computing;computer science;data mining;database;data stream mining	DB	-18.3591425278845	54.992417235514225	70357
3575672275d267945bba8939bc3f158566a9342b	failure data analysis of hpc systems		Continuous availability of HPC systems built from commodity components have become a primary concern as system size grows to thousands of processors. In this paper, we present the analysis of 8-24 months of real failure data collected from three HPC systems at the National Center for Supercomputing Applications (NCSA). The results show that the availability is 98.7-99.8% and most outages are due to software halts. On the other hand, the downtime are mostly contributed by hardware halts or scheduled maintenance. We also used failure clustering analysis to identi fy several correlated failures.	brian;central processing unit;cluster analysis;continuous availability;distributed shared memory;downtime;goto;mainframe computer;nancy leveson;national center for supercomputing applications;reliability engineering;supercomputer;time complexity	Charng-Da Lu	2013	CoRR		parallel computing;real-time computing;operating system	HPC	-17.624793389376315	50.560678426779305	70489
667f5945f5b1f1c0cbbf1d0c408331db4084495e	performance/energy efficiency analysis of register files in superscalar processors	energy efficient;register file		superscalar processor	Shahzad Nazar;Behrooz Shirazi;Sungyong Jung	2004			register file;computer architecture;parallel computing;efficient energy use;computer science;superscalar	Arch	-5.92185306583064	54.06907943068298	70539
57fe8f2b922dc10145db4e66c07daadd08d4db27	a block management mechanism for multimedia files	block management mechanism;unix s5 file system;bitmap tree;contiguous allocation			Tae Il Jeong;Sung Jo Kim	1995	IEICE Transactions		computer science;theoretical computer science;operating system;unix file types;unix filesystem;database;file control block	DB	-18.962561068825906	51.607640260814115	71172
e70e883a8789e7036e1a7dc71dc09f071eb8af2b	online prediction of applications cache utility	histograms;multi threading;reconfigurable architectures;standard deviation;l2 cache utility;resource management;program compiler;reconfigurable architectures multi threading program compilers;runtime;surface mount technology;computer architecture;operating system;multithreaded architecture;throughput surface mount technology hardware proposals runtime predictive models resource management performance loss computer architecture histograms;predictive models;reconfigurable hardware architecture;power consumption;program compilers;online prediction;multithreading online prediction l2 cache utility reconfigurable hardware architecture single threaded architecture program compiler;dynamic adaptation;proposals;high performance;performance loss;hardware implementation;reconfigurable hardware;single threaded architecture;throughput;hardware;multithreading	General purpose architectures are designed to offer average high performance regardless of the particular application that is being run. Performance and power inefficiencies appear as a consequence for some programs. Reconfigurable hardware (cache hierarchy, branch predictor, execution units, bandwidth, etc.) has been proposed to overcome these inefficiencies by dynamically adapting the architecture to the application needs. However, nearly all the proposals use indirect measures or heuristics of performance to decide new configurations, what may lead to inefficiencies. In this paper we propose a runtime mechanism that allows to predict the throughput of an application on an architecture using a reconfigurable L2 cache. L2 cache size varies at a way granularity and we predict the performance of the same application on all other L2 cache sizes at the same time. We obtain for different L2 cache sizes an average error of 3.11%, a maximum error of 16.4% and standard deviation of 3.7%. No profiling or operating system participation is needed in this mechanism. We also give a hardware implementation that allows to reduce the hardware cost under 0.4% of the total L2 size and maintains high accuracy. This mechanism can be used to reduce power consumption in single threaded architectures and improve performance in multithreaded architectures that dynamically partition shared L2 caches.	bandwidth (signal processing);benchmark (computing);branch predictor;cpu cache;execution unit;field-programmable gate array;heuristic (computer science);kerrison predictor;memory address;multithreading (computer architecture);operating system;profiling (computer programming);sampling (signal processing);thread (computing);threaded code;throughput	Miquel Moretó;Francisco J. Cazorla;Alex Ramírez;Mateo Valero	2007	2007 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation	10.1109/ICSAMOS.2007.4285748	computer architecture;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;cache;computer science;cache invalidation;smart cache;cache algorithms;cache pollution	Arch	-6.4763129625253475	51.93400852248882	71179
947ac6bf827136b7f1a6d3bf4cf89a43d65936c0	tracking back references in a write-anywhere file system	conference paper;efficient implementation;file system	Many file systems reorganize data on disk, for example to defragment storage, shrink volumes, or migrate data between different classes of storage. Advanced file system features such as snapshots, writable clones, and deduplication make these tasks complicated, as moving a single block may require finding and updating dozens, or even hundreds, of pointers to it. We present Backlog, an efficient implementation of explicit back references, to address this problem. Back references are file system meta-data that map physical block numbers to the data objects that use them. We show that by using LSM-Trees and exploiting the write-anywhere behavior of modern file systems such as NetApp R © WAFL R © or btrfs, we can maintain back reference meta-data with minimal overhead (one extra disk I/O per 102 block operations) and provide excellent query performance for the common case of queries covering ranges of physically adjacent blocks.	data deduplication;input/output;netapp filer;overhead (computing)	Peter Macko;Margo I. Seltzer;Keith A. Smith	2010			fork;self-certifying file system;real-time computing;torrent file;indexed file;device file;computer file;computer hardware;computer science;class implementation file;stub file;versioning file system;operating system;fstab;unix file types;journaling file system;database;open;data file;file system fragmentation;file control block	OS	-16.338707723708062	52.955801531123626	71194
c654cedb9ae44052efcaab673343f8a85501145f	a model of a time-sharing system with two classes of processes	virtual memory;time sharing;input output;interactive system;memory allocation;analytic solution	We present a model of a multiprogrammed, virtual memory interactive system, in which the processes are assumed to form two different classes as regards characteristics such as total compute time, input-output rate and program locality. The effect of memory sharing among processes is explicitly taken into account via life-time functions. We use our model to examine the efficiency of two policies of controlling the admission of processes into real core in order to avoid thrashing with two fixed-partition memory allocation schemes. An approximate analytical solution for our model is obtained owing to an equivalence and decomposition approach.		Alexandre Brandwajn	1975		10.1007/3-540-07410-4_658	uniform memory access;distributed shared memory;shared memory;real-time computing;simulation;computer science;physical address;static memory allocation;distributed computing;overlay;flat memory model;data diffusion machine;memory map	Logic	-12.090223476690635	48.37056271952656	71618
114dfb004ec2a88f50c67ece9e42dff7eef96d87	bubble budgeting: throughput optimization for dynamic workloads by exploiting dark cores in many core systems		All the cores of a many-core chip cannot be active at the same time, due to reasons like low CPU utilization in server systems and limited power budget in dark silicon era. These free cores (referred to as bubbles) can be placed near active cores for heat dissipation so that the active cores can run at a higher frequency level, boosting the performance of applications that run on active cores. Budgeting inactive cores (bubbles) to applications to boost performance has the following three challenges. First, the number of bubbles varies due to open workloads. Second, communication distance increases when a bubble is inserted between two communicating tasks (a task is a thread or process of a parallel application), leading to performance degradation. Third, budgeting too many bubbles as coolers to running applications leads to insufficient cores for future applications. In order to address these challenges, in this paper, a bubble budgeting scheme is proposed to budget free cores to each application so as to optimize the throughput of the whole system. Throughput of the system depends on the execution time of each application and the waiting time incurred for newly arrived applications. Essentially, the proposed algorithm determines the number and locations of bubbles to optimize the performance and waiting time of each application, followed by tasks of each application being mapped to a core region. A Rollout algorithm is used to budget power to the cores as the last step. Experiments show that our approach achieves 50 percent higher throughput when compared to state-of-the-art thermal-aware runtime task mapping approaches. The runtime overhead of the proposed algorithm is in the order of 1M cycles, making it an efficient runtime task management method for large-scale many-core systems.	algorithm;central processing unit;dark silicon;dot-com bubble;elegant degradation;manycore processor;overhead (computing);run time (program lifecycle phase);server (computing);thermal management (electronics);throughput	Xiaohang Wang;Amit Kumar Singh;Bing Li;Yang Yang;Hong Li;Terrence Mak	2016	IEEE Transactions on Computers	10.1109/TC.2017.2735967	embedded system;throughput;real-time computing;simulation;computer hardware;computer science;resource management;operating system;silicon;server	HPC	-6.2988061097265335	53.9976620343041	71633
d2a2557ad6bf1eaa975257704af39b943bac6c89	circuit simulation algorithms on a distributed memory multiprocessor system	distributed memory;matrix factorization;multiprocessor systems;distributed memory architecture;memory access;circuit simulation;circuit simulation multiprocessing systems matrix decomposition sparse matrices costs equations memory architecture concurrent computing cache memory communication networks;vlsi;vlsi circuit analysis computing;communication cost;circuit analysis computing;vlsi circuits circuit simulation algorithms distributed memory multiprocessor system distributed memory architecture parallel source row target row directed matrix factorization algorithms processor utilization memory accesses communication costs;shared memory multiprocessor	Shared memory multiprocessors have failed to achieve large speedups because of the processor to memory bottleneck, which gets worse as more processors are used. The authors match a distributed memory architecture to the problem to overcome the processor to memory bottleneck. A study is made of parallel source row and target row directed matrix factorization algorithms where the operations are precompiled at the row level. The authors' contribution is in the formulation and analysis of these factorization algorithms for a distributed memory architecture. The authors evaluate the effectiveness of their approach for processor utilization, memory accesses and communication costs for large matrices corresponding to real VLSI circuits. It is shown quantitatively, using the above metrics, that the source row factorization scheme is the most effective. >	algorithm;distributed memory;electronic circuit simulation;multiprocessing	John A. Trotter;Prathima Agrawal	1990		10.1109/ICCAD.1990.129947	memory address;uniform memory access;distributed shared memory;shared memory;interleaved memory;computer architecture;semiconductor memory;parallel computing;distributed memory;memory refresh;computer science;physical address;theoretical computer science;computer memory;overlay;conventional memory;very-large-scale integration;extended memory;flat memory model;matrix decomposition;registered memory;computing with memory;cache-only memory architecture;memory map;non-uniform memory access;memory management	HPC	-10.61828261503691	49.37713821923727	71644
43274e3a1c4a59ab4bfa9bf468b455b221b8b60d	larrabee: a many-core x86 architecture for visual computing	software;simd;graphics architecture;software renderer larrabee many core x86 architecture visual computing vector processor unit fixed function logic gpu;throughput computing;computer architecture pipelines application software performance analysis vector processors graphics logic registers software standards scalability;gpgpu graphics architecture many core computing real time graphics software rendering throughput computing visual computing parallel processing simd;real time;vector processor unit;fixed function logic;vector processor systems microprocessor chips rendering computer graphics;gpu;visual computing;computer architecture;gpgpu;larrabee many core x86 architecture;real time graphics;registers;pixel;vector processor systems;tiles;software rendering;vector processor;rendering computer graphics;magnetic cores;parallel applications;parallel processing;many core computing;microprocessor chips;software renderer	The Larrabee many-core visual computing architecture uses multiple in-order x86 cores augmented by wide vector processor units, together with some fixed-function logic. This increases the architecture's programmability as compared to standard GPUs. The article describes the Larrabee architecture, a software renderer optimized for it, and other highly parallel applications. The article analyzes performance through scalability studies based on real-world workloads.	larrabee (microarchitecture);visual computing;x86	Larry Seiler;Doug Carmean;Eric Sprangle;Tom Forsyth;Pradeep Dubey;Stephen Junkins;Adam Lake;Robert Cavin;Roger Espasa;Ed Grochowski;Toni Juan;Michael Abrash;Jeremy Sugerman;Pat Hanrahan	2009	IEEE Micro	10.1109/MM.2009.9	reference architecture;parallel processing;space-based architecture;computer architecture;vector processor;parallel computing;simd;computer hardware;computer science;operating system;processor register;general-purpose computing on graphics processing units;pixel;software rendering	Visualization	-8.274349286848983	47.573958088386725	71646
bfe140e2f1ebf7673f2c62f44a1fee365064cf08	skvm: scaling in-memory key-value store on multicore	in memory kv store;data processing;hsha model scaling in memory key value store on multicore skvm concurrent data access concurrent data processing concurrent network connection half sync half async model;concurrent data access;computational modeling;multicore;data structures;message systems;multicore processing instruction sets scalability data processing computational modeling message systems data structures;multicore processing;scalability;concurrent data access in memory kv store multicore scalability;instruction sets;storage management concurrency control data handling multiprocessing systems	SKVM is a high performance in-memory Key-Value (KV) store for multicore, which is designed for high concurrent data access. There are some problems of existing systems dealing with high concurrent data processing on multicore: lock competition, cache coherency overhead, and large numbers of concurrent network connections. To solve the problems and make the in-memory KV store scale well on multicore, high concurrent data access is divided into two steps: high concurrent connection processing and high concurrent data processing. Half sync/half async model (HSHA) is adopted to eliminate network bottleneck, which can support high concurrent network connection. Through data partition, lock competition is eliminated and cache movement is reduced. Furthermore, consistent hash is adopted as data distribution strategy which can improve the scalability of system on multicore. Though some of these ideas appear elsewhere, SKVM is the first to combine them together. The experimental results show that SKVM can reach at most 2.4x higher throughput than Memcached, and scales near linearly with the number of cores under any workload.	attribute–value pair;cpu cache;cache coherence;concurrency (computer science);consistent hashing;data access;disk partitioning;hash table;image scaling;in-memory database;kinetic void;kirby 64: the crystal shards;master/slave (technology);memcached;multi-core processor;network congestion;overhead (computing);scalability;synchronization (computer science);throughput	Ran Zheng;Wenjin Wang;Hai Jin;Qin Zhang	2015	2015 IEEE Symposium on Computers and Communication (ISCC)	10.1109/ISCC.2015.7405580	multi-core processor;computer architecture;parallel computing;real-time computing;data processing;computer science;operating system	OS	-11.8003436245919	52.02313706782035	71708
2520cfc29a521f2333fda020d7ae41860f8dfebd	ermia: fast memory-optimized database system for heterogeneous workloads	heterogeneous workloads;append only storage;indirection arrays;multicore scalability;log manager;main memory databases;epoch based resource management;serial safety net;multiversion concurrency control	Large main memories and massively parallel processors have triggered not only a resurgence of high-performance transaction processing systems optimized for large main-memory and massively parallel processors, but also an increasing demand for processing heterogeneous workloads that include read-mostly transactions. Many modern transaction processing systems adopt a lightweight optimistic concurrency control (OCC) scheme to leverage its low overhead in low contention workloads. However, we observe that the lightweight OCC is not suitable for heterogeneous workloads, causing significant starvation of read-mostly transactions and overall performance degradation.  In this paper, we present ERMIA, a memory-optimized database system built from scratch to cater the need of handling heterogeneous workloads. ERMIA adopts snapshot isolation concurrency control to coordinate heterogeneous transactions and provides serializability when desired. Its physical layer supports the concurrency control schemes in a scalable way. Experimental results show that ERMIA delivers comparable or superior performance and near-linear scalability in a variety of workloads, compared to a recent lightweight OCC-based system. At the same time, ERMIA maintains high throughput on read-mostly transactions when the performance of the OCC-based system drops by orders of magnitude.	central processing unit;computer data storage;concurrency (computer science);database;elegant degradation;independence day: resurgence;optimistic concurrency control;overhead (computing);scalability;serializability;snapshot (computer storage);snapshot isolation;throughput;transaction processing system	Kangnyeon Kim;Tianzheng Wang;Ryan Johnson;Ippokratis Pandis	2016		10.1145/2882903.2882905	parallel computing;real-time computing;computer science;database;multiversion concurrency control	DB	-14.357879511919146	50.893338101435496	71925
1e99fd411974705f7b85fae61b6bfee121ed1293	a comparative evaluation of implicit coscheduling strategies for networks of workstations	application performance implicit coscheduling strategies networks of workstations parallel applications sequential jobs simulation sequential applications;software performance evaluation;software performance evaluation scheduling workstation clusters virtual machines;workstations processor scheduling time sharing computer systems computer networks concurrent computing network servers dynamic scheduling delay throughput communication system control;virtual machines;scheduling;network of workstation;simulation study;workstation clusters	Implicit coscheduling strategies enable parallel applica tions to dynamically share the machines in a Network of Workstation (NOW) with interactive, CPU and IO-bound sequential jobs. In this paper we present a simulation study that compares 12 coscheduling strategies in terms of their impact on the performance of parallel and sequential applications executed simultaneously on a NOW. Our results show that the coscheduling strategy has a strong impact on the performance of the applications (both parallel and sequential) composing the workload, and that no single strategy is able to effectively handle all workloads. In spite of that, our results can be used to identify the strategy that represents the best choice for a given application class, or the best compromise for various workloads. Moreover, we show that in many cases simple strategies outperform more	central processing unit;coscheduling;heuristic (computer science);i/o bound;input/output;job stream;preemption (computing);scheduling (computing);simulation;workstation	Cosimo Anglano	2000		10.1109/HPDC.2000.868653	parallel computing;real-time computing;computer science;virtual machine;operating system;distributed computing;scheduling	HPC	-14.866248399370024	59.26204892022793	71983
854e5edb79631f5c7fdb5d477afaafdb9c76203a	smartcon: smartcon: smart context switching for fast storage devices	solid state disk;context switch;nonvolatile memory;i o subsystem	Handling of storage IO in modern operating systems assumes that such devices are slow and CPU cycles are valuable. Consequently, to effectively exploit the underlying hardware resources, for example, CPU cycles, storage bandwidth and the like, whenever an IO request is issued to such device, the requesting thread is switched out in favor of another thread that may be ready to execute. Recent advances in nonvolatile storage technologies and multicore CPUs make both of these assumptions increasingly questionable, and an unconditional context switch is no longer desirable. In this article, we propose a novel mechanism called SmartCon, which intelligently decides whether to service a given IO request in interrupt-driven manner or busy-wait--based manner based on not only the device characteristics but also dynamic parameters such as IO latency, CPU utilization, and IO size. We develop an analytic performance model to project the performance of SmartCon for forthcoming devices. We implement SmartCon mechanism on Linux 2.6 and perform detailed evaluation using three different IO devices: Ramdisk, low-end SSD, and high-end SSD. We find that SmartCon yields up to a 39% performance gain over the mainstream block device approach for Ramdisk, and up to a 45% gain for PCIe-based SSD and SATA-based SSDs. We examine the detailed behavior of TLB, L1, L2 cache and show that SmartCon achieves significant improvement in all cache misbehaviors.	busy waiting;cpu cache;central processing unit;context switch;linux;modern operating systems;multi-core processor;operating system;pci express;serial ata;socket.io;solid-state drive;translation lookaside buffer	Jongmin Gim;Taeho Hwang;Youjip Won;Krishna Kant	2015	TOS	10.1145/2631922	parallel computing;real-time computing;non-volatile memory;computer hardware;computer science;operating system;database;context switch	OS	-9.482200079434412	53.51313787173256	72047
47dd0a677ace5ce27d02e0432ff999870422843e	energy-aware application scheduling on a heterogeneous multi-core system	suitability guided program scheduling mechanism;multi core processor;instruction dependency distance;job shop scheduling;processor scheduling;resource allocation;power efficiency;heterogeneous multicore processor;resource requirement;program execution;fuzzy logic;distance measurement;power aware computing;magnetic cores distance measurement processor scheduling program processors job shop scheduling scheduling hardware;random scheduling approach;scheduling;branch transition rate;resource allocation fuzzy logic power aware computing processor scheduling;workload balancing;energy aware application scheduling mechanism;power efficient computing;energy delay product;magnetic cores;random scheduling approach energy aware application scheduling mechanism heterogeneous multicore processor power efficient computing resource requirement workload balancing program execution fuzzy logic instruction dependency distance branch transition rate suitability guided program scheduling mechanism;program processors;hardware	Heterogeneous multi-core processors are attractive for power efficient computing because of their ability to meet varied resource requirements of diverse applications in a workload. However, one of the challenges of using a heterogeneous multi-core processor is to schedule different programs in a workload to matching cores that can deliver the most efficient program execution. This paper presents an energy-aware scheduling mechanism that employs fuzzy logic to calculate the suitability between programs and cores by analyzing important inherent program characteristics such as instruction dependency distance and branch transition rate. The obtained suitability is then used to guide the program scheduling in the heterogeneous multi-core system. The experimental results show that the proposed suitability-guided program scheduling mechanism achieves up to 15.0% average reduction in energy-delay product compared with that of the random scheduling approach. To the best of our knowledge, this study is the first to apply fuzzy logic to schedule programs in heterogeneous multi-core systems.	canonical account;central processing unit;fuzzy logic;ibm notes;multi-core processor;requirement;simd;scheduling (computing);superscalar processor	Jian Chen;Lizy Kurian John	2008	2008 IEEE International Symposium on Workload Characterization	10.1109/IISWC.2008.4636086	fuzzy logic;multi-core processor;fair-share scheduling;fixed-priority pre-emptive scheduling;job shop scheduling;computer architecture;parallel computing;real-time computing;electrical efficiency;dynamic priority scheduling;resource allocation;computer science;rate-monotonic scheduling;operating system;two-level scheduling;distributed computing;scheduling	Arch	-5.450675718068453	54.731077976560385	72162
3c2c6e4a60897c10102f0561648f2cc96f4852fd	fault tolerant adaptive parallel and distributed simulation through functional replication		This paper presents FT-GAIA, a software-based fault-tolerant parallel and distributed simulation middleware. FT-GAIA has being designed to reliably handle Parallel And Distributed Simulation (PADS) models, which are needed to properly simulate and analyze complex systems arising in any kind of scientific or engineering field. PADS takes advantage of multiple execution units run in multicore processors, cluster of workstations or HPC systems. However, large computing systems, such as HPC systems that include hundreds of thousands of computing nodes, have to handle frequent failures of some components. To cope with this issue, FT-GAIA transparently replicates simulation entities and distributes them on multiple execution nodes. This allows the simulation to tolerate crash-failures of computing nodes. Moreover, FT-GAIA offers some protection against Byzantine failures, since interaction messages among the simulated entities are replicated as well, so that the receiving entity can identify and discard corrupted messages. Results from an analytical model and from an experimental evaluation show that FT-GAIA provides a high degree of fault tolerance, at the cost of a moderate increase in the computational load of the execution units.	algorithm;byzantine fault tolerance;central processing unit;chandy-misra-haas algorithm resource model;cloud computing;cluster analysis;complex systems;computation;computer cluster;distributed computing;entity;execution unit;expect;heuristic (computer science);hoc (programming language);i/o request packet;inertial navigation system;load balancing (computing);misra c;mean time between failures;middleware;multi-core processor;optimistic concurrency control;overhead (computing);performance evaluation;prototype;rollback (data management);simulation;statistical model;virtual machine;workstation;x86 virtualization	Gabriele D'Angelo;Stefano Ferretti;Moreno Marzolla	2018	CoRR	10.1016/j.simpat.2018.09.012	real-time computing;computer science;fault tolerance;workstation;byzantine fault tolerance;software;multi-core processor;middleware	HPC	-18.763495286428313	49.61018731598634	72215
843133a6d32efbe5f4e865882a805e8f6c586ce5	an analysis of multilevel checkpoint performance models		Periodic application checkpointing to a parallel file system has for years been the standard strategy for providing high performance computing (HPC) systems with resilience to system failures. The traditional checkpoint/restart protocol has until recently proved sufficient for mitigating the impact of these failures on application performance. However, as system sizes approach exascale levels, the frequency of failures and the time required to checkpoint/restart an exascale-size application increases and the efficiency of traditional checkpointing decreases substantially, making it no longer a viable option for providing resilience to future HPC systems. The most frequently proposed solution for providing future systems with resilience has been to design multilevel checkpointing protocols. However, the relationship between system failure rates, checkpoint/restart overhead, and duration of time between successive checkpoints is complex and finding the optimal duration of time between successive checkpoints is an open and challenging problem. This work presents a novel execution time prediction model we have developed that takes into consideration execution events that have not been considered by previous multilevel checkpointing models. We show how this model can be used to select checkpoint intervals and demonstrate why consideration of these execution events is important. We validate our work through simulation and provide a comparison to several optimization strategies proposed in other work to demonstrate the advantage gained by considering these execution events.	application checkpointing;clustered file system;computation;computer simulation;mathematical optimization;mean time between failures;multi-level cell;nonlinear system;overhead (computing);run time (program lifecycle phase);simulation;supercomputer	Daniel Dauwe;Sudeep Pasricha;Anthony A. Maciejewski;Howard Jay Siegel	2018	2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2018.00125	psychological resilience;application checkpointing;distributed computing;computer science;file system;supercomputer	HPC	-18.453371709787568	49.632938874355396	72227
53e06a4961f6e2d355dad4656d451be3a433f30f	towards a practical snapshot algorithm	debugging;sensor systems and applications;latency practical snapshot algorithm atomic snapshot memory multiple location shared memory concurrent writing wait free implementations coordinated collect algorithm wait free atomic snapshot construction multiprocessor synchronization operations update complexity scan complexity simulated distributed shared memory multiprocessor coordinated collect lock free algorithms locking algorithms;wait free atomic snapshot construction;memory protocols;radar tracking;concurrent writing;coordinated collect;abstract data types;simulated distributed shared memory multiprocessor;multiprocessor synchronization operations;wait free implementations;checkpointing;memory architecture shared memory systems computational complexity abstract data types parallel algorithms memory protocols;shared memory systems;atomic snapshot memory;update complexity;registers;computational complexity;memory architecture;writing;read write memory standby generators registers writing throughput delay checkpointing debugging radar tracking sensor systems and applications;scan complexity;coordinated collect algorithm;latency;read write memory;locking algorithms;standby generators;practical snapshot algorithm;lock free algorithms;multiple location shared memory;throughput;parallel algorithms	An atomic snapshot memory is an implementation of a multiple-location shared memory that can be atomically read in its entirety without preventing concurrent writing. The design of wait-free implementations of atomic snapshot memories has been the subject of extensive theoretical research in recent years. This paper introduces the coordinated-collect algorithm, a novel wait-free atomic snapshot construction which we believe is a rst step in taking snapshots from theory to practice. Unlike previous algorithms, it uses currently available multiprocessor synchronization operations to provide an algorithm that has only O(1) update complexity and O(n) scan complexity, with very small constants. We evaluated the performance of known snapshot algorithms for a collection of benchmarks on a simulated distributed shared-memory multiprocessor. Our empirical evidence suggests that coordinated-collect will outperform all known wait-free, lock-free, and locking snapshot algorithms in terms of overall throughput and latency. c © 2001 Elsevier Science B.V. All rights reserved.	benchmark (computing);computational complexity theory;distributed shared memory;lock (computer science);message passing;multiprocessing;non-blocking algorithm;snapshot (computer storage);snapshot algorithm;throughput;workstation	Yaron Riany;Nir Shavit;Dan Touitou	1995		10.1109/ISTCS.1995.377039	throughput;latency;parallel computing;radar tracker;real-time computing;computer science;distributed computing;parallel algorithm;processor register;programming language;computational complexity theory;debugging;writing;abstract data type;non-blocking algorithm	Theory	-15.323681058160409	46.442504020513205	72241
0ff0ffb91ce185e216b91e63dcb0dd6c8a375c80	technical summary of the second ieee workshop on workstation operating systems	north america;south america;continuous media;cluster of workstations;spectrum;window manager;input output;memory access;operating system;storage capacity;communication protocol;object oriented technology;network services;local area network	The broad spectrum of universities, industrial research laboratories, and computer companies represented at the Second IEEE Workshop on Workstation Operating Systems provided a rich snapshot of current activities in operating systems. There were representatives of 19 operating system research projects among the participants and several from commercial offerings. The attendees came from seven countries on three continents: North America, Europe, and South America.Since the last Workshop in 1987, there have been more advances in hardware than in software functions. Software standards continue to emerge in the areas of operating system interfaces, page description languages, window management interfaces, and communication protocols. New software applications exist in the areas of multimedia and multi-node computing. Object-oriented technology is already present in running systems and gaining importance. The areas that the participants perceived needing most future work were operating system abstractions, workstation operation, system responsiveness, input output, network services, management of clusters of workstations, and failure handling.While processor speeds, main memory access speeds, memory density, and secondary storage capacity continue to increase fast, disk seek times have decreased only slightly, and the bandwidth of most local-area networks has not increased at all. FDDI networks are just beginning to be deployed. The software is adjusting to this hardware scenario by using caching at multiple levels of the systems.In the last two years large main memories at individual computing nodes and multi-node computer installations have become common. It is expected that most future computing nodes will have substantial local storage and that high-bandwidth networks will enable the support of continuous media like vocie and video. Input output, to disks, to networks, and to user-oriented devices, is expected to become the central problem in future systems.	areal density (computer storage);auxiliary memory;cache (computing);computer data storage;hard disk drive performance characteristics;input/output;operating system;page description language;responsiveness;snapshot (computer storage);thread-local storage;window manager;workstation	Luis-Felipe Cabrera	1991	SIGMETRICS Performance Evaluation Review	10.1145/122289.122292	local area network;input/output;spectrum;communications protocol;real-time computing;simulation;computer science;operating system;distributed computing;computer network	OS	-17.333996255742	50.85890142992905	72343
8cb026fcddff56a744f56ffe3ee0389ad62c49e7	hardware-level thread migration in a 110-core shared-memory multiprocessor	crosstalk;wires;system on chip;transistors;multicore processing;instruction sets;hardware	Advantages - significantly reduces traffic on high-locality workloads up to 14x reduction in traffic in some benchmarks - simple to implement and verify (indep. of core count, no transient states) - decentralized & trivially scalable (only # core ID bits, addr ↔ core mapping) Challenges - workloads should be optimized with memory model in mind (like allocating data on cache line boundaries but more coarse-grained) - automatically mapping allocation over cores not a trivial problem Opportunities - fine-grained migration is an enabling technology - since it's cheap and responsive, can be used for almost anything - e.g., if only some cores have FPUs, migrate to access FPU.	floating-point unit;locality of reference;multiprocessing;process migration;scalability;shared memory	Mieszko Lis;Keun Sup Shim;Brandon Cho;Ilia A. Lebedev;Srinivas Devadas	2013	2013 IEEE Hot Chips 25 Symposium (HCS)	10.1109/HOTCHIPS.2013.7478320	parallel computing;real-time computing;computer hardware;computer science	Arch	-5.642903509804297	53.93792444772042	72460
cde86aa43271ed830a4723d4fb5ba02fceb2fe37	boosting adaptivity of fault-tolerant scheduling for real-time tasks with service requirements on clusters	cluster;fault tolerant;real time;adaptivity;adaptive fault tolerance;scheduling;real time scheduling;fault tolerance	Abstract: Thank to the excellent extensibility and usability, computer clusters have become the dominating platform for parallel computing. Fault-tolerance is mandatory for safety-critical applications running on clusters. In this paper we propose a service-aware and adaptive fault-tolerant scheduling algorithm using overlapping technologies (SAO in short) that can tolerate a node's permanent failure at any time instant for real-time tasks with service requirements in heterogeneous clusters. SAO adopts the primary/backup model and considers the timing constraints, service requirements, and system resource utilization. To improve system resource utilization, we employ backup-backup (BB in short) and primary-backup (PB in short) overlapping technologies and analyze the overlapping constraints. In addition, SAO has high system adaptivity by dynamically adjusting the service levels of tasks based on system load. Furthermore, to improve resource utilization and schedulability, SAO makes backup copies adopt passive execution scheme or decrease the overlapping execution time of the primary copy and backup copy of a task as much as possible. Compared with a baseline algorithm SAWO (a service-aware and adaptive fault-tolerant scheduling algorithm without using overlapping technologies) and an existing algorithm DYFARS with simulation experiments, SAO achieves an average of 51.25% improvement in performability.	boosting (machine learning);fault tolerance;real-time clock;requirement;scheduling (computing)	Xiaomin Zhu;Chuan He;Rong Ge;Peizhong Lu	2011	Journal of Systems and Software	10.1016/j.jss.2011.04.067	fault tolerance;parallel computing;real-time computing;computer science;operating system;distributed computing	Embedded	-11.682240730286201	59.82358790629601	72569
491d45de95fe3f4594ec5181bdf169b2503ac19d	oap: an obstruction-aware cache management policy for stt-ram last-level caches	dram-based memory architecture;obstruction-aware cache management policy;last-level cache;memory technology;cache design;stt-ram last-level cache;stt-ram cache;cache access;emerging memory technology;4-core architecture;stt-ram l3 cache;system performance;computer architecture;system on chip;embedded systems;degradation	Emerging memory technologies are explored as potential alternatives to traditional SRAM/DRAM-based memory architecture in future microprocessor designs. Among various emerging memory technologies, Spin-Torque Transfer RAM (STT-RAM) has the benefits of fast read latency, low leakage power, and high density, and therefore has been investigated as a promising candidate for last-level cache (LLC). One of the major disadvantages for STT-RAM is the latency and energy overhead associated with the write operations. In particular, a long-latency write operation to STT-RAM cache may obstruct other cache accesses and result in severe performance degradation. Consequently, mitigation techniques to minimize the write overhead are required in order to successfully adopt this new technology for cache design. In this paper, we propose an obstruction-aware cache management policy called OAP. OAP monitors the cache to periodically detect LLC-obstruction processes, and manage the cache accesses from different processes. The experimental results on a 4-core architecture with an 8MB STT-RAM L3 cache shows that the performance can be improved by 14% on average and up to 42%, with a reduction of energy consumption by 64%.	cpu cache;dynamic random-access memory;elegant degradation;microprocessor;overhead (computing);robertson–seymour theorem;spectral leakage;static random-access memory	Jue Wang;Xiangyu Dong;Yuan Xie	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)		bus sniffing;system on a chip;embedded system;pipeline burst cache;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cpu cache;tag ram;cache;computer science;write-once;cache invalidation;operating system;write buffer;smart cache;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;non-uniform memory access	EDA	-7.358561842573209	54.893863953542805	73024
48b91647c39568e8adfb8af8f6a192cd99e6f842	developing of multistage virtual shared memory model for cluster based parallel systems	virtual shared memory;parallel systems	In this paper, we proposed a new multistage virtual shared memory model for cluster based parallel systems. This model can be expanded in hierarchical manner and covered many of the previous clusters of parallel system designs. Queuing theory and Jackson queuing networks are applied for constructing an analytical model. This model gives a closed-form solution for the system performance metrics, such as processor waiting time and system processing power. In development of this analytical model we used open queuing network rules for analyzing a closed queuing network and calculate the input rate of each service centre as a function of the input rate for previous service centre. The model can be used for evaluating the cluster based parallel processing systems or optimizing its specification on design space.	jackson;multistage amplifier;multistage interconnection networks;parallel computing;queueing theory;shared memory	Aye Aye Nwe;Khin Mar Soe;Than Nwe Aung;Thinn Thu Naing;Myint Kyi;Pyke Tin	2005			uniform memory access;distributed shared memory;shared memory;computer architecture;parallel computing;distributed memory;computer cluster;computer science;virtual memory;distributed computing;overlay;data diffusion machine;memory map	HPC	-9.701802087831275	47.94628808658983	73132
6f0f2752841b864a6183c593762e99ac9bbfff98	a method for the performance analysis of integrated application services - simulating the execution of integrated application services			profiling (computer programming)	Hiroshi Y Yamada;Akira Kawaguchi	2004			computer science;simulation	HPC	-18.152673589784705	47.01138990867852	73171
52dabf4f970bf447c3664867dfe0bcf9e7183e1e	techniques for improving performance of the fast (fully-associative sector translation) flash translation layer	flash memory;performance evaluation;fastftl;storage management;benchmark based workloads fast performance improvement technique fully associative sector translation flash translation layer block device interface flash memory disk based file systems erase before write feature rw log block reclamation fast ftl hftl;flash translation layer;file system;cleaning switches flash memory writing merging performance evaluation file systems;merging;writing;switches;flash memories;file systems;storage management flash memory flash translation layer fastftl;cleaning;storage management flash memories performance evaluation	A Flash Translation Layer (FTL) emulates a block device interface on top of flash memory to support traditional disk-based file systems. Due to the erase-beforewrite feature of flash memory, an FTL usually performs outof- place updates and uses a cleaning procedure to reclaim obsolete data. The FAST (Fully-Associative Sector Translation) FTL is one of the most well-known FTLs and has been used in many previous studies. It reserves a small portion of the flash storage as the log area and uses the blocks in this area (i.e., log blocks) to accommodate data overwrites. Among all the log blocks, one (called the SW log block) is used to accommodate sequential overwrites, while the others (called RW log blocks) are used to accommodate random overwrites. In this paper, two techniques are proposed to reduce the cleaning cost and hence to improve the performance of FAST. The first technique reduces the cost of RW log block reclamation by utilizing efficiency-driven cleaning policies, and the second technique replaces high-cost SW log block reclamation with low-cost RW log block reclamation. We have implemented the proposed techniques on the FAST FTL as well as the HFTL, an FTL based on FAST. The simulation results on six real/benchmark-based workloads show that the proposed techniques significantly reduce the cleaning cost, by up to 97.64% in FAST and 66.80% in HFTL, demonstrating the effectiveness of the proposed techniques1.	basic block;benchmark (computing);cpu cache;emulator;ftl: faster than light;flash file system;flash memory controller;plasma cleaning;read-write memory;sequential logic;shattered world;simulation;sputter cleaning	Chien-Yu Liu;Ying-Shiuan Pan;Hsin-Hung Chen;Ying-Chih Wu;Da-Wei Chang	2011	IEEE Transactions on Consumer Electronics	10.1109/TCE.2011.6131149	flash file system;parallel computing;computer hardware;network switch;computer science;operating system;writing	Arch	-12.054952492956849	53.89751544460309	73437
e9a9a2f475aeda6fdbe36827ddcfb10c10437f29	performance analysis of task migration in a portable parallel environment	debugging;software fault tolerance network operating systems operating systems computers processor scheduling software performance evaluation system recovery;intrepid environment;recovery support;transparent checkpointing;dynamic reconfiguration;elmo system;processor scheduling;network operating systems;resource management;ncube 2 performance analysis portable parallel programming environment portable parallel environment preemptive task migration algorithm elmo system migratable parallel tasks recovery support debugging intrepid environment transparent checkpointing dynamic reconfiguration adaptive scheduling load redistribution;software performance evaluation;software fault tolerance;portable parallel programming environment;runtime;resumes;checkpointing;preemptive task migration algorithm;migratable parallel tasks;system recovery;parallel programming environment;load redistribution;performance analysis;adaptive scheduling;cities and towns;performance analysis debugging checkpointing parallel processing runtime operating systems cities and towns adaptive scheduling resumes resource management;operating systems computers;parallel processing;ncube 2;operating systems;portable parallel environment	Parallel computers are evaluated by measuring their processor and communication speeds. But, for many largescale applications the I/O performance is the bottleneck rather than the computational or communication performance. 3-D seismic imaging is one of such applications. Seismic data sets, consisting of recorded pressure waves, can be very large, some times more than a terabyte in size. It is always not possible to read and keep all the required data and information in computer memory. Therefore the data are partially read and intermediate results are written out. In this article we have discussed an approach to handle the massive I/O requirements of seismic migration and show the performance of parallel seismic migration code on PARAM 10000 which is distributed memory parallel computer. Use of parallel I/O clearly indicates the improvement of more than 30% in execution time.	computer memory;distributed memory;input/output;param;parallel computing;profiling (computer programming);requirement;run time (program lifecycle phase);terabyte	Balkrishna Ramkumar;Gopal Chillariga	1996		10.1109/ICPP.1996.538575	parallel processing;parallel computing;real-time computing;computer science;resource management;operating system;distributed computing;programming language;debugging;software fault tolerance	HPC	-15.871095068313018	49.56010877495057	73450
6dc14746bf11d049bb50ad274b5bfe576db65a48	performance optimization for a sha-1 cryptographic workload expressed in opencl for fpga execution		The introduction of Field Programmable Gate Array (FPGA) based devices for OpenCL applications provides an opportunity to develop kernels which are executed on application specific compute units which can be optimized for specific workloads such as encryption. This work examines the optimization of the SHA-1 hashing algorithm developed in OpenCL for and FPGA based implementation. The implementation starts from the freely available SHA-1 implementation in OpenSWAN; ports the implementation to OpenCL; and optimizes the kernel for FPGA implementation using the Xilinx SDAcccel development environment for OpenCL applications. Through each stage, the implementation is benchmarked in order to examine latency, throughput, and power usage on FPGA, Graphics Processing Unit (GPU), and Central Processing Unit (CPU) systems.  While the programming model of OpenCL on FPGAs is identical to GPU and CPU, in order to optimize an application it is necessary to understand how the OpenCL concepts are implemented on FPGAs. In the platform model, one FPGA is considered a device. Inside the FPGA, a portion of the resources are dedicated to the fixed platform region. This region includes the memory interface, PCI Express interface, Direct Memory Access (DMA) controller, flash programming interface, and FPGA reconfiguration interface. The rest of the FPGA resources are dedicated to one or more OpenCL regions. By calling clCreateProgramWith-Binary, an OpenCL Region will be reprogrammed with the users chosen FPGA binary file. The FPGA binary file contains the configuration information to program one or more compute into the FPGA fabric for a kernel in the application. As a result of the flexibility of the FPGA fabric, these compute units can all work on the same kernel function or can be specialized to work on a set of kernel functions. All compute units, which target the same kernel, may be used by clEnqueueNDRange or out of order queues to implement concurrency. In addition to customizing the kernel compute units, FPGAs enable the creation of application specific memory architectures to minimize latency. Within the memory architectures supported by an FPGA, external and on-chip memories can be used within the OpenCL memory model. The external memory is mapped to the global and constant memory spaces while the on chip memory is mapped to the local, private and program scope global memory spaces. The on chip memory is implemented via BRAM and register resources inside the FPGA which are automatically inferred by the compiler. In addition to the levels of memory hierarchy available with FPGAs, the Xilinx SDAccel compiler analyzes the kernel data movement operations to automatically coalesce transactions and infer burst transactions to maximize usage of the available memory bandwidth.  The SHA-1 is an algorithm used for the hashing of messages. It has a block iterative structure which utilizes one way compression to achieve strong cryptographic integrity. In the algorithm, a message is broken into 512 bit blocks. For each block, an initialization hash from the previous block is combined with a portion of the message to determine a new hash. Inside the blocks, there is an 80 iteration loop used for scrambling the input data. The implementation of this scrambling loop is the main computational activity in the algorithm.  When optimizing the SHA-1 algorithm, it is important to consider the use model that will be employed by the application. In many cases, a continuous stream of messages will need to be hashed using the SHA-1 algorithm. In this scenario, the primary goal should be to focus on overall throughput of the algorithm. Assuming this use case, a dataflow implementation of the SHA-1 with a pipelined inner scrambling loop is chosen. Furthermore, the FPGA implementation of program scope global memory is utilized to provide an efficient dataflow paradigm.  After optimizing the SHA-1 computation kernel by refactoring the OpenCL kernel code and using compiler options in SDAccel, it is important characterize the operation of the kernel within the context of the FPGA accelerator card. For this reason, the implementation will be compared to other CPU and GPU based targets. For each target, latency, throughput, and power usage are measured with optimized kernels for each platform. To test latency, a single message will be queued for hashing on the accelerator card. For throughput, many messages will be queued for execution on the accelerator card. Finally, power usage will be collected in both scenarios to determine the overall throughput per watt of the system in high load and low load scenarios.  Executing the SHA-1 workload in an FPGA expressed as an OpenCL application achieves a 6x improvement when compared to the execution of the reference OpenSWAN implementation. This workload which is both computationally and memory intensive provides a good reference example to explain how OpenCL applications can be profiled and optimized for implementation on FPGA systems with a 25W power envelope.	adobe flash;algorithm;application programming interface;benchmark (computing);binary file;burst mode (computing);central processing unit;code refactoring;compiler;computation;concurrency (computer science);cryptography;dataflow;direct memory access;encryption;field-programmable gate array;graphics processing unit;hash function;iteration;mathematical optimization;memory architecture;memory bandwidth;memory hierarchy;null (sql);opencl api;openswan;pci express;performance per watt;programming model;programming paradigm;sha-1;throughput	Fernando Martinez-Vallina;Spenser Gilliland	2015		10.1145/2791321.2791328	parallel computing;real-time computing;computer hardware;computer science	Arch	-6.41677390107601	47.80566966024753	73529
d4133f86ed4da10d691a769e3fffdd2f930eacd6	detecting solid-state disk geometry for write pattern optimization	software;flash memory;logical disk;ash geometry time factors throughput switches computer architecture software;storage system;storage systems;storage management;geometry;storage management firmware flash memories geometry;solid state disk;firmware;computer architecture;solid state disk geometry detection write pattern optimization flash memory firmware data mapping wear leveling logical disk space geometry information flash management;time factors;ash;time factor;storage systems flash memory solid state disk;switches;flash memories;throughput;black box testing	Solid-state disks use flash memory as their storage medium, and adopt a firmware layer that makes data mapping and wear leveling transparent to the hosts. Even though solid-state disks emulate a collection of logical sectors, the I/O delays of accessing all these logical sectors are not uniform because the management of flash memory is subject to many physical constraints of flash memory. This work proposes a collection of black-box tests can detect the geometry inside of a solid-state disk. The host system software can arrange data in the logical disk space according to the detected geometry information to match the host write pattern with the device characteristic for reducing the flash management overhead in solid-state disks.	black box;disk space;firmware;flash memory;input/output;logical disk;overhead (computing);solid-state drive;wear leveling	Chun-Chieh Kuo;Jen-Wei Hsieh;Li-Pin Chang	2011	2011 IEEE 17th International Conference on Embedded and Real-Time Computing Systems and Applications	10.1109/RTCSA.2011.76	flash file system;embedded system;firmware;throughput;parallel computing;real-time computing;white-box testing;computer hardware;network switch;computer science;operating system;bad sector;logical disk;disk aggregation	Embedded	-14.497783944826145	55.602827080887444	73654
c8d32fbd1ab3e356a6265533f1cbc6b37966f24a	interdependent cache analyses for better precision and safety	cache storage;school of automation;data cache analysis interdependent cache analyses executable worst case execution time wcet cache behavior prediction cache content prediction memory accessing instructions abstract interpretation based must analysis cache hit estimation cache miss estimation wcet estimates may analysis must analysis cache update persistence analysis cache update;abstracts transfer functions concrete safety lattices program processors estimation;computer science automation formerly	One of the challenges for accurately estimating Worst Case Execution Time(WCET) of executables is to accurately predict their cache behavior. Various techniques have been developed to predict the cache contents at different program points to estimate the execution time of memory-accessing instructions. One of the most widely used techniques is Abstract Interpretation based Must Analysis, which determines the cache blocks guaranteed to be present in the cache, and hence provides safe estimation of cache hits and misses. However, Must Analysis is highly imprecise, and platforms using Must Analysis have been known to produce blown-up WCET estimates. In our work, we propose to use May Analysis to assist the Must Analysis cache update and make it more precise. We prove the safety of our approach as well as provide examples where our Improved Must Analysis provides better precision. Further, we also detect a serious flaw in the original Persistence Analysis, and use Must and May Analysis to assist the Persistence Analysis cache update, to make it safe and more precise than the known solutions to the problem. Finally, we propose an improvement in the original May Analysis, to make it more precise, especially for Data Cache Analysis.	abstract interpretation;block size (cryptography);cpu cache;dspace;executable;flaw hypothesis methodology;interdependence;persistence (computer science);rectifier (neural networks);run time (program lifecycle phase);worst-case execution time	Kartik Nagar;Y. N. Srikant	2012	Tenth ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMCODE2012)	10.1109/MEMCOD.2012.6292306	bus sniffing;least frequently used;pipeline burst cache;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;cache;computer science;write-once;cache invalidation;database;smart cache;programming language;cache algorithms;cache pollution	SE	-7.6442617491214255	56.850734607559076	73671
2e64db2a27ab8bb4939727b7b7dd8de7062e7959	blackjackbench: portable hardware characterization with automated results' analysis		DARPA’s AACE project aimed to develop Architecture Aware Compiler Environments. Such a compiler automatically characterizes the targetted hardware and optimizes the application codes accordingly. We present the BlackjackBench suite, a collection of portable micro-benchmarks that automate system characterization, plus statistical analysis techniques for interpreting the results. The BlackjackBench benchmarks discover the effective sizes and speeds of the hardware environment rather than the often unattainable peak values. We aim at hardware characteristics that can be observed by running executables generated by existing compilers from standard C codes. We characterize the memory hierarchy, including cache sharing and NUMA characteristics of the system, properties of the processing cores affecting instruction execution speed, and the length of the OS scheduler time slot. We show how these features of modern multicores can be discovered programmatically. We also show how the features could potentially interfere with each other resulting in incorrect interpretation of the results, and how established classification and statistical analysis techniques can reduce experimental noise and aid automatic interpretation of results. We show how effective hardware metrics from our probes allow guided tuning of computational kernels that outperform an autotuning library further tuned by the hardware vendor.	ansi c;benchmark (computing);code;compiler;executable;library (computing);memory hierarchy;operating system;scheduling (computing);software portability	Anthony Danalis;Piotr Luszczek;Gabriel Marin;Jeffrey S. Vetter;Jack J. Dongarra	2014	Comput. J.	10.1093/comjnl/bxt057	portable telemetry	Metrics	-6.994712969673358	47.46788607749908	73849
d06d6ba2ab70e73850e118a779a18cf811c6b192	anomalous behaviour of the fifty-percent rule in dynamic memory allocation	coefficient of variation;residence time distribution;simulation;storage fragmentation;fifty percent rule;allocation;allocative efficiency;dynamic memory;dynamic memory allocation;first fit	This paper reports simulation data showing that, in dynamic memory allocation, the average free-to-allocated-block ratio can differ considerably and in both directions from the predictions of the 50 percent rule. A new derivation is given, and it is shown that previous derivations make an assumption that may be violated frequently. On the basis of the simulation data and the derivation, it is hypothesized that the anomalous behavior results from the combined effects of systematic placement and the statistics of the release process. Additional simulations support this hypothesis. Systematic placement, which refers to the natural convention of always allocating storage requests against the same end of the free block selected by the allocation strategy, tends to order blocks within contiguous groups according to their allocation time. The degree of anomalous behavior depends on the extent to which allocated blocks are released in the order of their allocation. For non-Markovian release processes, the extent of the correlation between allocation order and release order varies approximately inversely with the coefficient of variation of the memory residence time distribution. The simulations show that allocation efficiency depends strongly on the residence time distribution; efficiency decreases as the distribution's coefficient of variation increases. Some practical implications are briefly discussed.	coefficient;memory management;simulation	John E. Shore	1977	Commun. ACM	10.1145/359863.359880	allocative efficiency;econometrics;dynamic random-access memory;computer science;operating system;residence time distribution;c dynamic memory allocation;coefficient of variation;statistics	OS	-6.949550049707202	51.18595820915702	73889
2471e6c22d642ce017953cab4c81548afd90f146	overlapping dependent loads with addressless preload	performance evaluation;satisfiability;out of order;data dependence;memory level parallelism;data prefetching;pointer chasing loads;instruction and issue window	Modern out-of-order processors with non-blocking caches exploit Memory-Level Parallelism (MLP) by overlapping cache misses in a wide instruction window. The exploitation of MLP, however, can be limited due to long-latency operations in producing the base address of a cache miss load. When the parent instruction is also a cache miss load, a serialization of the two loads must be enforced to satisfy the load-load data dependence.In this paper, we propose a mechanism that dynamically captures the load-load data dependences at runtime. A special Preload is issued in place of the dependent load without waiting for the parent load, thus effectively overlapping the two loads. The Preload provides necessary information for the memory controller to calculate the correct memory address upon the availability of the parent's data to eliminate any interconnect delay between the two loads. Performance evaluations based on SPEC2000 and Olden applications show that significant speedups up to 40% with an average of 16% are achievable using the Preload. In conjunction with other aggressive MLP exploitation methods, such as runahead execution, the Preload can make more significant improvement with an average of 22%.	base address;blocking (computing);central processing unit;data dependency;instruction window;memory address;memory controller;memory-level parallelism;non-blocking algorithm;preload (software);run time (program lifecycle phase);runahead;serialization	Zhen Yang;Xudong Shi;Feiqi Su;Jih-Kwon Peir	2006	2006 International Conference on Parallel Architectures and Compilation Techniques (PACT)	10.1145/1152154.1152196	parallel computing;real-time computing;computer hardware;computer science;out-of-order execution;memory-level parallelism;operating system;satisfiability	Arch	-9.296259549583091	52.906809985333744	73938
bb89b8ef4b4840b466d383ffd157ea1aba33fb4f	low execution efficiency: when general multi-core processor meets wireless communication protocol	protocols;base stations;会议论文;wireless communication;general processor limitation user plane protocol wireless communication protocol data centers;multicore processing;protocols multicore processing wireless communication instruction sets hardware base stations;instruction sets;hardware	Inefficient resource utilization of communication cell and base station is the major limitation of traditional Radio Access Network (RAN), which makes Cloud Radio Access Network (C-RAN) becomes a promising infrastructure. C-RAN data centers face physical constraints in space and power, so the efficient utilization of hardware is more critical. General multi-core processors are universally used in modern data centers, and it has become a critical factor of power, computational density and per-operation energy for modern data centers. In this paper, we select User Plane protocol from WCDMA wireless communication protocols, and implement it as a benchmark firstly. The User Plane protocol is responsible for processing and transferring high volume streaming data between user's mobile terminals and core network. Then, based on its hardware performance counters, we study performance of general multi-core processor when processing User Plane protocol. The evaluation results show that micro-architecture of dominant general multi-core processor is inefficient for User Plane protocol, that is, the dominant micro-architecture mismatches to needs of wireless communication protocols. The random memory access in a large memory address space is a typical characteristic in wireless communication applications, and it incurs high miss ratio of on-chip cache hierarchies. The frequent on-chip cache misses lead to amount of off-chip memory access operations, which incurs longer memory access latency as a dominant factor to degrade overall performance. Finally, we identify the processor's key micro-architecture characteristics that meet needs of wireless communication protocols, which would lead to improved power efficiency in C-RAN data centers.	address space;benchmark (computing);c-ran;cpu cache;central processing unit;communications protocol;computation;computer memory;data center;forwarding plane;hardware performance counter;memory address;microarchitecture;multi-core processor;performance per watt;radio access network;streaming media	Fenglong Song;Yasong Zheng;Futao Miao;Xiaochun Ye;Hao Zhang;Dongrui Fan;Zhiyong Liu	2013	2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing	10.1109/HPCC.and.EUC.2013.129	multi-core processor;embedded system;communications protocol;parallel computing;real-time computing;computer science;base station;operating system;instruction set;distributed computing;wireless lan controller;wi-fi array;computer security;wireless;computer network	Arch	-9.871766080982821	55.881491825390846	74133
2bad256706e162b465b2d0b0656adb1ade09bfab	joint write policy and fault-tolerance mechanism selection for caches in dsm technologies: energy-reliability trade-off	energy efficiency;microprocessors;cache storage;cache size;reliability;error correction codes;write through cache;fault tolerant;demand side management;energy efficient;write through caches;reliability write through cache write back cache energy consumption;write back caches;sec ded code;write back cache;energy consumption;fault tolerance;fault tolerance energy consumption energy efficiency error correction codes microprocessors reliability engineering power engineering and energy protection analytical models process design;microprocessors write through caches write back caches reliability energy consumption simple scalar tool cacti model sec ded code cache size fault tolerance mechanism selection;fault tolerance mechanism selection;cacti model;microprocessor chips cache storage demand side management energy consumption fault tolerance;program processors;benchmark testing;energy saving;simple scalar tool;microprocessor chips	Write-through caches potentially have higher reliability than write-back caches. However, write-back caches are more energy efficient. This paper provides a comparison between the write-back and write-through policies based on the combination of reliability and energy consumption criteria. In the experiments, SIMPLESCALAR tool and CACTI model are used to evaluate the characteristics of the caches. The results show that a write-through cache with one parity bit per word is as reliable as a write-back cache with SEC-DED code per word. Furthermore, the results show that the energy saving of the write-through cache over the write-back cache increases if any of the following changes happens: i) a decrease in the feature size, ii) a decrease in the L2 cache size, and iii) an increase in the L1 cache size. The results also show that when feature size is bigger than 32nm, the write-back cache is usually more energy efficient. However, for 32nm and smaller feature sizes the write-through cache can be more energy efficient.	cpu cache;cache (computing);copy-on-write;data rate units;dedicated hosting service;experiment;fault tolerance;parity bit	Mehrtash Manoochehri;Alireza Ejlali;Seyed Ghassem Miremadi	2009	2009 10th International Symposium on Quality Electronic Design	10.1109/ISQED.2009.4810401	bus sniffing;least frequently used;pipeline burst cache;cache coherence;fault tolerance;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cpu cache;tag ram;computer hardware;cache;computer science;cache invalidation;operating system;efficient energy use;smart cache;cache algorithms;cache pollution	Arch	-6.544111849580553	54.71338793951478	74284
5336491d998e7fa5d530b3cad091517b9e9c48e1	cache-conscious graph collaborative filtering on multi-socket multicore systems	parallel computing;cache conscious;collaborative filtering	Recommendation systems using graph collaborative filtering often require responses in real time and high throughput. Therefore, besides recommendation accuracy, it is critical to study high performance concurrent collaborative filtering on modern platforms. To achieve high performance, we study the graph data locality characteristics of collaborative filtering. Our experiments demonstrate that although an individual graph traversal exhibits poor data locality, multiple queries have a tendency of sharing their data footprints, especially in the case of queries with neighboring root vertices. Such characteristics lead to both inter- and intra-thread data locality, which can be utilized to significantly improve collaborative filtering performance.  Based on these observations, we present a cache-conscious system for collaborative filtering on modern multi-socket multicore platforms. In this system, we propose a cache-conscious query scheduling technique and an in-memory graph representation, and to maximize cache performance and minimize cross-core/socket communication overhead, we address both inter- and intra-thread data locality. To address the workload balancing issue, this study introduces a dynamic work-stealing mechanism to explore the tradeoff between workload balancing and cache-consciousness.  The proposed system was evaluated on a Power7+ system against the IBM Knowledge Repository graph dataset. The results demonstrated both good scalability and throughput. Compared with the basic system that does not perform cache-conscious scheduling, inter-thread scheduling improves throughput by up to 18%. Intra-thread scheduling can further improve throughput by as much as 22%. By enabling dynamic work-stealing, the proposed technique balances workloads across all threads with a low standard deviation of the per-thread processing time.	cpu cache;collaborative filtering;experiment;graph (abstract data type);graph traversal;in-memory database;locality of reference;multi-core processor;overhead (computing);recommender system;scalability;scheduling (computing);throughput;tree traversal;work stealing	Lifeng Nai;Yinglong Xia;Ching-Yung Lin;Bo Hong;Hsien-Hsin S. Lee	2014		10.1145/2597917.2597935	real-time computing;computer science;theoretical computer science;collaborative filtering;distributed computing	HPC	-10.52077934917226	52.21326102034467	74527
032f1cb261230f1241c5500404c9fb716f85330d	mitigating soft error rate through selective replication in hybrid architecture		With the rapid development of integrated circuit technology, soft error has increasingly become the major factor for the reliability of microprocessors. The researchers employ a variety of methods to reduce the influence of soft errors. Besides the lower delay and increasing bandwidth, 3D integration technology also has the ability of heterogeneous integration. STT-RAM is a new storage technology with broad prospects. The characteristic that STT-RAM is immune to soft errors makes it ideal candidate for improving reliability and STT-RAM can be integrated into the 3D chip through heterogeneous integration. In this paper, we proposed a selective replication mechanism for soft error rate reduction in hybrid reorder buffer architecture based on the 3D integration technology and STT-RAM. Instructions will be replicated or migrated to STT-RAM for reliability improvement in certain situations. The experimental results show that the soft error rate of the proposed hybrid structure is reduced by 15 % on average and the AVF decreased 54.3 % further on average through the in-buffer selective replication mechanism while the performance penalty is 2.8 %.	soft error	Chao Song;Minxuan Zhang	2015		10.1007/978-3-662-49283-3_5	real-time computing	Mobile	-9.292085683346983	54.74472869410852	74587
350f6ae536a4e8787f7f4513ba1be4a7d2d3b37b	empirical study of nvm storage: an operating system's perspective and implications	phase change materials;software;nonvolatile memory random access memory phase change materials performance evaluation buffer storage software throughput;random access memory;stt ram nvm storage performance buffer caching pcm;byte addressable i o nvm storage stt ram pcm legacy software layers dram software configurations hdd storage buffer caching read ahead synchronous i o direct i o block i o;performance evaluation;nvm;buffer storage;buffer caching;nonvolatile memory;storage management dram chips operating systems computers;stt ram;storage performance;pcm;throughput	As high performance NVM storage such as PCM and STT-RAM emerge, legacy software layers optimized for HDDs should be revisited. Specifically, as storage performance approaches DRAM performance, existing I/O mechanisms and software configurations should be reassessed. This paper explores the challenges and implications of using NVM storage with a broad range of experiments. We measure the performance of a system with NVM storage emulated by DRAM with proper timing parameters and compare it with that of HDD storage environments under various configurations. Our experimental results show that even with storage as fast as DRAM, the performance gain is not large for read operations as current I/O mechanisms do a good job of hiding the slow performance of HDD. To assess the potential benefit of fast storage media, we change various I/O configurations and perform experiments to quantify the effects of existing I/O mechanisms such as buffer caching, read-ahead, synchronous I/O, direct I/O, block I/O, and byte-addressable I/O on systems with NVM storage. We also investigate some unique performance characteristics of NVM in comparison with HDD by changing the number of accesses and the amount of data to be transferred. We anticipate that our results will provide directions in system software development in presence of ever faster storage devices.	asynchronous i/o;byte addressing;cache (computing);disk buffer;dynamic random-access memory;emulator;experiment;hard disk drive;input/output;legacy system;non-volatile memory;operating system;software development;the sync	Eunji Lee;Hyokyung Bahn;Seunghoon Yoo;Sam H. Noh	2014	2014 IEEE 22nd International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems	10.1109/MASCOTS.2014.56	pulse-code modulation;spin-transfer torque;throughput;parallel computing;real-time computing;non-volatile memory;computer hardware;telecommunications;computer science	OS	-13.37003292098792	52.907319962103635	74590
28b05efdc5253f204b65acfe59dba0f75571ba43	temperature-aware scheduling and assignment for hard real-time applications on mpsocs	driver assistance;hardware acceleration;mixed integer linear program;thermal management packaging;thermal analysis;temperature 24 66 c;real time;temperature real time systems steady state transient analysis cooling timing packaging costs runtime thermal conductivity;temperature 24 66 c temperature aware scheduling real time applications mpsoc thermal effects mixed integer linear programming minimal energy solutions steady state thermal analysis transient analysis;packaging;runtime;transient analysis;temperature aware scheduling;thermal management packaging integer programming integrated circuit packaging linear programming system on chip thermal analysis;thermal time;taillight detection;mixed integer linear programming;integer programming;system on chip;steady state thermal analysis;linear programming;thermal conductivity;real time video processing;real time applications;temperature;integrated circuit packaging;steady state analysis;mpsoc;hard real time;cooling;minimal energy solutions;thermal effects;steady state;real time systems;timing;time constraint	Thermal effects in MPSoCs may cause the violation of timing constraints in real-time systems. This paper presents a mixed integer linear programming based solution to this problem. Tasks are assigned and scheduled to an MPSoC to minimize peak temperature, subject to real-time constraints. The proposed approach outperforms existing methods, reducing peak temperature by up to 24.66°C and by an average of 8.75°C when compared to minimal-energy solutions. We also present a heuristic for use on large problem instances. Steady-state thermal analysis is used for tasks with long execution times compared to the RC thermal time constants of the cores. Transient analysis is used otherwise. The steady-state analysis based heuristic finds solutions with at most 3.40°C deviation from optimal peak temperature (0.22°C on average) while improving upon existing technique by as much as 25.71°C and 10.86°C on average. The transient analysis based heuristic further reduce peak temperature by 1°C in the best case and 0.17°C on average.	best, worst and average case;heuristic;integer programming;linear programming;mpsoc;real-time clock;real-time computing;real-time transcription;steady state;thermal time hypothesis;transient state	Thidapat Chantem;Robert P. Dick;Xiaobo Sharon Hu	2008	2008 Design, Automation and Test in Europe	10.1145/1403375.1403446	embedded system;electronic engineering;real-time computing;integer programming;computer science;linear programming;steady state	EDA	-5.2707459888353005	58.53165915582814	74619
d44b1bd18fdbfcbe1f8144278989a05860163718	analytical timing estimation for temporally decoupled tlms considering resource conflicts	silicon;energy harvesting;availability;synchronization;wireless sensor networks	Transaction level models (TLMs) can use temporal decoupling to increase the simulation speed. However, there is a lack of modeling support to time the temporally decoupled TLMs. In this paper, we propose a timing estimation mechanism for TLMs with temporal decoupling. This mechanism features an analytical model and novel delay formulas. Concepts such as resource usage and availability are used to derive the delay formulas. Based on them, a fast scheduling algorithm resolves resource conflicts and dynamically determines the timing of concurrent transaction sequences. Experiments show that the delay estimation formulas are capable of capturing the timing effects of resource conflicts. At the same time, the overhead of the scheduling algorithm is very low, hence the simulation speed remains high.	algorithm;coupling (computer programming);overhead (computing);scheduling (computing);simulation;transaction-level modeling	Kun Lu;Daniel Mueller-Gritschneder;Ulf Schlichtmann	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)		control engineering;embedded system;real-time computing;simulation;wireless sensor network;computer science;energy harvesting	EDA	-7.323504677750292	59.8561426875053	74626
4a0fbca310242c2908b252903e2ecc84a73ce447	scalable load and store processing in latency-tolerant processors	cadcam;cache storage;cycle time;secondary load buffer;cam;secondary store queue;latency tolerant processors;processor scheduling;data forwarding;load store processing algorithms;queueing theory;first in first out;power efficiency;buffer storage;latency tolerance;cycle critical processor resources;cache storage memory architecture program processors instruction sets parallel processing queueing theory computer aided manufacturing;computer architecture;memory ordering;store redo log;registers;memory architecture;pipelines;computer aided manufacturing;secondary store queue latency tolerant processors memory latency tolerant architectures cycle critical processor resources cam search functions memory ordering data forwarding load store processing algorithms secondary load buffer set associative structure store redo log;memory latency tolerant architectures;set associative structure;functional requirement;hierarchical design;proposals;program processors;search functions;structural similarity;parallel processing;memory latency;instruction sets;delay buffer storage computer aided manufacturing cadcam computer architecture processor scheduling pipelines memory architecture proposals registers	Memory latency tolerant architectures support thousands of in-flight instructions without scaling cycle-critical processor resources, and thousands of useful instructions can complete in parallel with a miss to memory. These architectures however require large queues to track all loads and stores executed while a miss is pending. Hierarchical designs alleviate cycle time impact of these structures but the CAM and search functions required to enforce memory ordering and provide data forwarding place high demand on area and power. We present new load-store processing algorithms for latency tolerant architectures. We augment primary load and store queues with secondary buffers. The secondary load buffer is a set associative structure, similar to a cache. The secondary store buffer, the Store Redo Log, is a first-in first-out structure recording the program order of all stores completed in parallel with a miss, and has no CAM and search functions. Instead of the secondary store queue, a cache provides temporary forwarding. The SRL enforces memory ordering by ensuring memory updates occur in program order once the miss returns. The new algorithms eliminate the CAM and search functions in the secondary load and store buffers, and remove fundamental sources of complexity, power, and area inefficiency in load/store processing. The new organization, while being area and power efficient, is competitive in performance compared to hierarchical designs.	algorithm;cas latency;cpu cache;central processing unit;computer data storage;fifo (computing and electronics);franklin electronic publishers;image scaling;interrupt latency;memory ordering;operand forwarding;redo log;web cache	Amit Gandhi;Haitham Akkary;Ravi Rajwar;Srikanth T. Srinivasan;Konrad Lai	2005	IEEE Micro	10.1109/ISCA.2005.46	embedded system;parallel processing;computer architecture;parallel computing;real-time computing;cam;electrical efficiency;cas latency;fifo and lifo accounting;cycle time variation;computer science;operating system;structural similarity;instruction set;pipeline transport;processor register;queueing theory;load/store architecture;functional requirement;memory ordering	Arch	-8.11430074413229	52.95204312340863	74668
1fbad498ea1ad348db9604d010927f0d32f57266	9th usenix symposium on operating systems design and implementation, osdi 2010, october 4-6, 2010, vancouver, bc, canada, proceedings		This paper analyzes the scalability of seven system applications (Exim, memcached, Apache, PostgreSQL, gmake, Psearchy, and MapReduce) running on Linux on a 48core computer. Except for gmake, all applications trigger scalability bottlenecks inside a recent Linux kernel. Using mostly standard parallel programming techniques— this paper introduces one new technique, sloppy counters—these bottlenecks can be removed from the kernel or avoided by changing the applications slightly. Modifying the kernel required in total 3002 lines of code changes. A speculative conclusion from this analysis is that there is no scalability reason to give up on traditional operating system organizations just yet.	kernel (operating system);linux;make;mapreduce;memcached;operating system;parallel computing;postgresql;scalability;source lines of code;speculative execution	Brad Chen;Haowen Chan;Anthony Cozzie;Azadeh Farzan;Ariel Feldman;Prem Gopalan;Collin Jackson;Eyal de Lara;Wyatt Lloyd;Tim Mann;Jim Mattson;David Mazières;David Meisner;Bryan Parno;Ryan S. Peterson;Donald E. Porter;David Shue;Ahren Studer;Shuo Tang;Amit Vasudevan;Arun Venkataramani;Bernard Wong	2010				OS	-15.614124062054199	50.06275658457265	74687
0fa20ada782bb79e84fe5c738d88173857bb4262	fast cascading replication strategy for data grid	remote access;data sharing;scientific experiment;multitier data grid;fast cascading replication strategy;engineering application;biological system modeling;bandwidth delay educational institutions grid computing data engineering heuristic algorithms resource management testing costs computer science;computational modeling;time factors;heuristic algorithms;large file remote access;data access;distributed databases;user requirements;bandwidth;grid computing;high performance;replicated databases grid computing;random access;replicated databases;data grid;optorsim;optorsim multitier data grid fast cascading replication strategy data sharing scientific experiment engineering application large file remote access;data models	More and more large amounts of data shared with researchers and workstations worldwide are generated by scientific experiments and engineering applications. High performance grids aim at facilitating the distribution of such data to geographically remote places. For high performance data grids where users require remote access to large files, dynamic replication is an efficient method to improve performance. In this paper, Fast Cascading, a new dynamic replication algorithm, is proposed for the multi-tier Data Grid. The simulation results obtained by Simulator OptorSim which has been modified show that this algorithm can reduce the average response time of data access greatly for random access pattern compared with other algorithms,such as Fast Spread, LRU and Economy-Zipf dynamic replication strategy, and the gain increases with the number of jobs submitted.	algorithm;data access;experiment;fast fourier transform;job stream;multitier architecture;random access;response time (technology);simulation;workstation;zipf's law	Liang Hong;Xue-dong Qi;Xia Li;Zhen Li;Wenxing Wang	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.624	data access;data modeling;computer science;user requirements document;operating system;data grid;database;distributed computing;computational model;world wide web;distributed database;bandwidth;random access;grid computing	HPC	-17.892869754619557	58.61202669388415	74731
04613586ec2692d7bcf0edfd5bddac4e7ec3e9d2	ipuloc — exploring dynamic program locality with the instruction processing unit for filling memory gap	instruction processing unit;data processing;dynamic program;chip;memory gap;ipuloc;high speed	Memory gap has become an essential factor influencing the peak performance of high-speed CPU-based systems. To fill this gap, enlarging cache capacity has been a traditional method based on static program locality principle. However, the order of instructions stored in I-Cache before being sent to Data Processing Unit (DPU) is a kind of useful information that has not ever been utilized before. So an architecture containing an Instruction Processing Unit (IPU) in parallel with the ordinary DPU is proposed. The IPU can prefetch, analyze and preprocess a large amount of instructions otherwise lying in the I-Cache untouched. It is more efficient than the conventional prefetch buffer that can only store several instructions for previewing. By IPU, Load Instructions can be preprocessed while the DPU is executing on data simultaneously. It is termed as “Instruction Processing Unit with LOokahead Cache” (IPULOC for short) in which the idea of dynamic program locality is presented. This paper describes the principle of IPULOC and illustrates the quantitative parameters for evaluation. Tools for simulating the IPULOC have been developed. The simulation results shows that it can improve program locality during program execution, and hence can improve the cache hit ratio correspondingly without further enlarging the on-chip cache that occupies a large portion of chip area.	cpu cache;cache (computing);central processing unit;hit (internet);locality of reference;parsing;prefetch input queue;preprocessor;principle of locality;simulation	Zhenchun Huang;Sanli Li	2002	Journal of Computer Science and Technology	10.1007/BF02962209	chip;parallel computing;real-time computing;data processing;computer hardware;cache;computer science;operating system;database;programming language;control unit	HPC	-8.82182186279351	54.12315757229	74785
896c227e6f69213d4bee53aac771733c54480110	reducing energy of dram/flash memory system by os-controlled data refresh	flash memory;cache storage;random access memory;memory management;swap cache;aging;power engineering and energy;random access memory flash memory energy consumption computer science batteries switches memory management power engineering and energy frequency aging;energy consumption;aging policy;batteries;aging policy dram flash memory system os controlled data refresh reduce energy consumption swap cache os controlled page allocation;operating systems computers cache storage dram chips flash memories;os controlled page allocation;reduce energy consumption;computer science;switches;dram;frequency;operating systems computers;dram chips;os controlled data refresh;flash memories;flash memory system	This paper presents a new approach to reduce energy consumption of DRAM/flash memory system by lowering the frequency of DRAM refreshes. The approach is based on two ideas: (1) a DRAM based swap-cache that reduces the number of writes to the flash memory by keeping dirty pages as long as possible; and (2) OS-controlled page allocation/aging policy that stops refreshing for banks, whose pages are clean and not accessed for a long time. Simulations show that the approach can reduce the DRAM refresh energy by 59-74% and the overall energy of DRAM/flash memory system by 8-24% without increase in the execution.	benchmark (computing);cpu cache;computer simulation;dynamic random-access memory;experiment;flash memory;handheld game console;memory management unit;memory refresh;operating system;overhead (computing);paging;profiling (computer programming);refresh rate;run time (program lifecycle phase)	Vasily G. Moshnyaga;Hua Vo;Glenn Reinman;Miodrag Potkonjak	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378515	parallel computing;real-time computing;memory rank;static random-access memory;cas latency;computer hardware;network switch;computer science;operating system;frequency;memory controller;universal memory;dram;quantum mechanics;memory management	Arch	-10.382108443929821	54.72318612266472	74851
c79a1e7b98d1a9e9b5e81d6da1959cfd87af29f9	memory garbage collection for an object-oriented processor	cache storage;storage management cache storage;caffeinemark benchmark object oriented processor reference counting object cache mark sweep heap concurrent memory garbage collection;reference counting;storage management;memory leaks;null;garbage collection;reference counting object cache;object oriented;caffeinemark benchmark;object oriented processor;concurrent memory garbage collection;mark sweep heap;java object oriented modeling computer languages virtual machining counting circuits cellular phones statistics memory management programming profession information technology	A model of two-level memory garbage collection with reference counting object cache has been introduced. Reference counting cache together with mark-sweep heap provides a concurrent memory garbage collection method without memory leaks. Evaluation of the performance of the reference counting cache has been conducted based on CaffeineMark benchmark. We found that only a small size of cache is sufficient to reclaim memory concurrently, but a larger size is required to produce reasonable hit ratio	benchmark (computing);garbage collection (computer science);hit (internet);memory leak;reference counting	C. Y. Ho;W. Y. Lo;Anthony Shi-Sheung Fong	2007	Fourth International Conference on Information Technology (ITNG'07)	10.1109/ITNG.2007.123	manual memory management;shared memory;reference counting;garbage;parallel computing;real-time computing;cache coloring;page cache;region-based memory management;cpu cache;computer hardware;cache;computer science;operating system;computer memory;garbage collection;programming language;object-oriented programming;cache algorithms;cache pollution;memory leak;cache-only memory architecture;memory map;non-uniform memory access	HPC	-11.94968393420092	50.402449266890486	74927
cdf72613e9703ef4653cfc360fc0905fbc85c068	the delay time-based (dtb) algorithm for energy-efficient server cluster systems	energy aware distributed systems energy efficient cluster systems green it power consumption model delay time based dtb algorithm;delay time based dtb algorithm;servers power demand clustering algorithms computational modeling time factors delays load modeling;workstation clusters energy conservation power aware computing resource allocation;energy aware distributed systems;power consumption model;energy efficient cluster systems;green it;communication delay delay time based algorithm dtb algorithm energy efficient server cluster systems improved power consumption laxity based algorithm ipclb algorithm cluster power consumption load balancer	The improved power consumption laxity-based (IPCLB) algorithm is discussed to select one of servers so that the total power consumption of a cluster can be reduced. However, a load balancer has to collect a state of every current process on servers of a cluster to calculate the estimated power consumption of each server. In addition, it is difficult to precisely estimate the power consumption of each server since the state of each process on the server is changed during the estimation. Especially, a process might terminate before the termination time is estimated if the computation time of the process is shorter than the communication delay time between the load balancer and the server. In this paper, we assume the computation time of each process is shorter than the communication delay time. Then, we propose a delay time-based (DTB) algorithm to select a server for each request process so that the total power consumption of a cluster to perform processes on the server can be reduced. In the DTB algorithm, it is not necessary to collect a state of every process on each server to estimate the power consumption laxity. In addition, the minimum computation time of a process is not required to be a priori defined in the DTB algorithm.	algorithm;computation;daisy digital talking book;load balancing (computing);server (computing);terminate (software);time complexity	Tomoya Enokido;Makoto Takizawa;S. Misbah Deen	2014	2014 Eighth International Conference on Complex, Intelligent and Software Intensive Systems	10.1109/CISIS.2014.40	embedded system;real-time computing;computer science;distributed computing	HPC	-17.781781696056754	59.310185290589835	74933
bd0ea3a50b08c0f232ff56646733d000c00e0567	improving communication in pgas environments: static and dynamic coalescing in upc	performance evaluation;one sided communication;unified parallel c;partitioned global address space;conference report	The goal of Partitioned Global Address Space (PGAS) languages is to improve programmer productivity in large scale parallel machines. However, PGAS programs may have many fine-grained shared accesses that lead to performance degradation. Manual code transformations or compiler optimizations are required to improve the performance of programs with fine-grained accesses. The downside of manual code transformations is the increased program complexity that hinders programmer productivity. On the other hand, most compiler optimizations of fine-grain accesses require knowledge of physical data mapping and the use of parallel loop constructs.  This paper presents an optimization for the Unified Parallel C language that combines compile time (static) and runtime (dynamic) coalescing of shared data, without the knowledge of physical data mapping. Larger messages increase the network efficiency and static coalescing decreases the overhead of library calls. The performance evaluation uses two microbenchmarks and three benchmarks to obtain scaling and absolute performance numbers on up to 32768 cores of a Power 775 machine. Our results show that the compiler transformation results in speedups from 1.15X up to 21X compared with the baseline versions and that they achieve up to 63% the performance of the MPI versions.	baseline (configuration management);compile time;elegant degradation;image scaling;mathematical optimization;optimizing compiler;overhead (computing);partitioned global address space;performance evaluation;programmer;programming productivity;unified parallel c (upc);universal product code	Michail Alvanos;Montse Farreras;Ettore Tiotto;José Nelson Amaral;Xavier Martorell	2013		10.1145/2464996.2465006	parallel computing;real-time computing;computer science;partitioned global address space;operating system;distributed computing;programming language	HPC	-7.899869044405261	48.52919792236172	75002
c1dcc62e371fc076547d42151a804ab2361739dc	an advanced optimizer for the ia-64 architecture	optimizing compilers registers performance analysis parallel processing prefetching delay processor scheduling information analysis program processors application software;performance evaluation;floating point arithmetic microprocessor chips performance evaluation;integer performance ia 64 architecture scalar optimizations high level optimization floating point performance;floating point;floating point arithmetic;microprocessor chips	Intel’s IA-64 architecture has a rich set of features, including control and data speculation, predication, large register files, and an advanced branch architecture. These features allow the compiler to exploit instruction-level parallelism (ILP) and optimize applications in many new ways. Intel’s IA-64 compiler incorporates the latest optimization techniques already known in the compiler community. In addition, some known techniques have been extended, and new techniques have been designed specifically for the IA-64 architecture. High-level optimizations include loop and data transformations to improve cache locality and parallelism. Compilers typically apply these techniques to program structures at a higher level of abstraction, compared with the program representation for scalar optimizations. For example, loop unrolling and loop unrolland-jam transformations exploit the large register file to eliminate redundant references to array elements and to expose more parallelism. Scalar replacement of memory references replaces array references with register references. Linear loop transformations, loop fusion, loop tiling, and loop distribution improve cache locality. Finally, data prefetching overlaps memory access latency with computation. A primary objective of scalar optimizations is to minimize the number of computations and the number of references to memory. The primary scalar optimization that achieves this objective is partial redundancy elimination (PRE), which minimizes the number of times an expression is evaluated. We extended PRE to use control and data speculation to eliminate more loads. PRE’s counterpart, called partial dead-store elimination (PDSE), serves to remove redundant stores to memory. State-of-the-art analysis techniques support optimizations in the IA-64 compiler. Memory disambiguation determines whether two memory references potentially access the same memory location. This information is critical in hiding memory latency, because knowing that a load doesn’t interfere with an earlier store is essential to scheduling the load earlier. The memory disambiguator includes support for data speculation. Interprocedural analysis and optimization (IPO) has proven effective in optimizing applications by exposing opportunities across procedure call boundaries. IPO gains importance for a processor with many functional units and registers. IA-64 enables more aggressive optimizations. For instance, effectively using the large register file eliminates memory operations. Optimizations use rotating registers to reduce the overhead of software register renaming in loops. Predication serves in situations such as removing hard-to-predict branches and implementing an efficient prefetching policy. The compiler uses control and data speculation to Rakesh Krishnaiyer Dattatraya Kulkarni Daniel Lavery Wei Li Chu-cheow Lim John Ng David Sehr	cas latency;cpu cache;compiler;computation;dead store;expanded memory;ia-64;instruction-level parallelism;interprocedural optimization;jam;locality of reference;loop invariant;loop unrolling;mathematical optimization;memory address;memory disambiguation;overhead (computing);parallel computing;partial redundancy elimination;processor register;rakesh agrawal (computer scientist);register file;register renaming;scalar processor;scheduling (computing);subroutine;tiling window manager;word-sense disambiguation	Rakesh Krishnaiyer;Dattatraya Kulkarni;Daniel M. Lavery;Wei Li;Chu-Cheow Lim;John Ng;David C. Sehr	2000	IEEE Micro	10.1109/40.888704	floating-point unit;computer architecture;parallel computing;real-time computing;binary scaling;computer science;floating point;operating system	Arch	-7.690588768525306	50.30216230504252	75006
a5857d685cc45af128869191aaf301ab32ebd92a	combining deduplication and delta compression to achieve low-overhead data reduction on backup datasets	prototypes;dupadj delta compression low overhead data reduction backup datasets storage systems digital data big data era dare deduplication aware resemblance detection and elimination scheme data deduplication data reduction efficiency duplicate adjacency based resemblance detection data chunks super feature approach data restore performance deduplication only systems;redundancy;indexing;feature extraction;feature extraction redundancy containers indexing educational institutions prototypes scalability;data reduction big data data compression;scalability;backup storage system data reduction delta compression deduplication;containers	Data reduction has become increasingly important in storage systems due to the explosive growth of digital data in the world that has ushered in the big data era. In this paper, we present DARE, a Deduplication-Aware Resemblance detection and Elimination scheme for compressing backup datasets that effectively combines data deduplication and delta compression to achieve high data reduction efficiency at low overhead. The main idea behind DARE is to employ a scheme, call Duplicate-Adjacency based Resemblance Detection (DupAdj), by considering any two data chunks to be similar (i.e., candidates for delta compression) if their respective adjacent data chunks are found to be duplicate in a deduplication system, and then further enhance the resemblance detection efficiency by an improved super-feature approach. Our experimental results based on real-world and synthetic backup datasets show that DARE achieves an additional data reduction by a factor of more than 2 (2X) on top of deduplication with very low overhead while nearly doubling the data restore performance of deduplication-only systems by supplementing delta compression to deduplication.	backup;big data;circuit restoration;data deduplication;delta encoding;digital data;dyadic transformation;fragmentation (computing);overhead (computing);rewriting;sensor;synthetic intelligence	Wen Xia;Hong Jiang;Dan Feng;Lei Tian	2014	2014 Data Compression Conference	10.1109/DCC.2014.38	search engine indexing;scalability;data deduplication;feature extraction;computer science;data mining;database;prototype;redundancy;information retrieval	OS	-14.140566033781464	54.996422289295424	75214
c0805fe8385a66823ee221c671a9915239708b23	wrjfs: a write-reduction journaling file system for byte-addressable nvram		Non-volatile random-access memory (NVRAM) becomes a mainstream storage device in embedded systems due to its favorable features, such as small size, low power consumption, and short read/write latency. Unlike dynamic random access memory (DRAM), NVRAM has asymmetric performance and energy consumption on read/write operations. Generally, on NVRAM, a write operation consumes more energy and time than a read operation. Unfortunately, current mobile/embedded file systems, such as EXT2/3 and EXT4, are very unfriendly for NVRAM devices. The reason is that current mobile/embedded file systems employ a journaling mechanism for increasing its data reliability. Although a journaling mechanism raises the safety of data in a file system, it also repeatedly writes data to a data storage while data is committed and checkpointed. Though several related works have been proposed to reduce the amount of write traffic to NVRAM, they still cannot effectively minimize the write amplification of a journaling mechanism. Such observations motivate us to design a two-phase write reduction journaling file system called wrJFS. In the first phase, wrJFS classified data into two categories: Metadata and user data. As the size of metadata is usually very small (few bytes), byte-enabled journaling strategy will handle metadata during commit and checkpoint stages. In contrast, the size of user data is very large relative to metadata; thus, user data will be processed in the second phase. In the second phase, user data will be compressed by hardware encoder to reduce the write size and managed compressed-enabled journaling strategy to avoid the write amplification on NVRAM. Moreover, we analyze the overhead of wrJFS and show that the overhead is negligible. According to the experimental results, the proposed wrJFS outperforms other journaling file systems even though the experiments include the overhead of data compression.	application checkpointing;best, worst and average case;byte;byte addressing;computer data storage;data compression;dynamic random-access memory;embedded system;encoder;experiment;non-volatile random-access memory;overhead (computing);random access;replication (computing);transaction processing system;two-phase commit protocol	Tseng-Yi Chen;Yuan-Hao Chang;Shuo-Han Chen;Chih-Ching Kuo;Ming-Chang Yang;Hsin-Wen Wei;Wei-Kuan Shih	2018	IEEE Transactions on Computers	10.1109/TC.2018.2794440	parallel computing;real-time computing;ext4;journaling file system;computer science;dynamic random-access memory;metadata;byte;write amplification;file system;computer data storage	OS	-11.536224396667068	54.66579772226661	75285
c6455dd74839e6a199a9230fb4f7c380e4644c1e	minimizing multiple-priority inversion protocol in hard real time system	hard real real time systems;multiprocessor scheduling;distributed semaphore hard real real time systems multiprocessor scheduling multiple priority inversion mpip;distributed semaphore;mpip;scheduling embedded systems multiprocessing systems protocols;multiple priority inversion;hard real time schedulability multiple priority inversion protocol hard real time system multiprocessor distributed system scheduling embedded systems mpip locking protocol distributed locking semaphore upper bound blocking time;protocols real time systems scheduling processor scheduling time factors synchronization ip networks	With the interesting of researchers to develop the scheduling of multiprocessor and distributed system in hard real time embedded systems, we present a new locking protocol called Minimizing multiple-Priority Inversion Protocol (MPIP), in this protocol, we proposed a novel mechanism to turn from preemptive to non-preemptive state to minimize multiple-priority inversion caused by low priority task on local processor. We focus on distributed locking semaphore and we showed its ability to lock the shared memory, thus, working in Multi-processor environment. We assumed all shared resources as global to avoid resource nested. With properties of our protocol, we define a new upper bound blocking time. We submit our proposal to experimental evaluation to compare it with MPCP protocol in term of hard real time schedulability.	blocking (computing);distributed computing;embedded system;lock (computer science);multiprocessing;priority inversion;real-time computing;scheduling (computing);shared memory	Furkan Rabee;Yong Liao;Maolin Yang	2013	2013 IEEE 11th International Conference on Dependable, Autonomic and Secure Computing	10.1109/DASC.2013.62	parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;distributed computing;priority ceiling protocol	Embedded	-10.161967898465269	60.29821504342709	75360
20ce4a42a1162d2f79c802783242cd09ba2f07cc	parallel geospatial raster data i/o using file view		The challenges of providing a divide-and-conquer strategy for tackling large geospatial raster data input/output (I/O) are longstanding. Solutions need to change with advances in the technology and hardware. After analyzing the reason for the problems of traditional parallel raster I/O mode, a parallel I/O strategy using file view is proposed to solve these problems. Message Passing Interface I/O (MPI-IO) is used to implement this strategy. Experimental results show how a file view approach can be effectively married to General Parallel File System (GPFS). A suitable file view setting provides an efficient solution to parallel geospatial raster data I/O. key words: parallel I/O, geospatial raster, file view, MPI	ibm gpfs;input/output;message passing interface;parallel i/o;raster data;serial ata	Wei Xiong;Ye Wu;Luo Chen;Ning Jing	2015	IEICE Transactions		parallel computing;computer science;message passing interface;theoretical computer science;database	HPC	-16.303019287865048	52.85521210817775	75372
0c02f06892e067feeed84787d34c6244a48c24b9	an integrated scratch-pad allocator for affine and non-affine code	integrated memory circuits;memory architecture;storage allocation;spm allocators;affine code;affine program allocation;integrated scratch-pad allocator;nonaffine code;scratch-pad memory	Scratch-pad memory (SPM) allocators that exploit the presence of affine references to arrays are important for scientific benchmarks. On the other hand, such allocators have so far been limited in their general applicability. In this paper we propose an integrated scheme that for the first time combines the specialized solution for affine program allocation with a general framework for other code. We find that our integrated framework does as well or outperforms other allocators for a variety of SPM sizes	algorithm;allocator (c++);benchmark (computing);slab allocation;super paper mario	Sumesh Udayakumaran;Rajeev Barua	2006	Proceedings of the Design Automation & Test in Europe Conference		embedded system;parallel computing;real-time computing;computer science;operating system;static memory allocation	EDA	-6.342254794758193	48.66725045310079	75449
592e6bb5e94e459071cf7cb33b6d97aa1e0e8a2d	transparent gpu memory management for dnns		Modern DNN frameworks exploit GPU acceleration by default to achieve high performance. The limitation of GPU memory capacity becomes a serious problem because DNNs are becoming deeper and larger. This paper proposes a purely software-based transparent solution, called tvDNN, to the GPU memory capacity problem. It is based on GPU memory swapping and memory object sectioning techniques. It also provides an efficient memory-object swapping schedule based on ILP (optimal) and heuristics (suboptimal). The experimental results show that tvDNN enables Caffe to build VGG-16 with a large batch size, such as 256 or 512, using a few GB of GPU memory without significant performance degradation.	algorithm;elegant degradation;graphics processing unit;heuristic (computer science);memory management;paging	Jung-Ho Park;Hyungmin Cho;Wookeun Jung;Jaejin Lee	2018		10.1145/3178487.3178531	parallel computing;theoretical computer science;computer science;artificial neural network;memory management;heuristics;software;exploit;swap (computer programming)	HPC	-10.248989080956655	51.247070496275754	75460
27ecf550c916f86f2315eb14d67ada7204069ae4	a hybrid cache replacement policy for heterogeneous multi-cores	multi core cache replacement cache miss resource utilization;shared memory systems cache storage performance evaluation resource allocation;lfu policy hybrid cache replacement policy heterogeneous multicores future generation computer architecture energy efficiency multiprocessor system shared memory system cache miss data movement power dissipation heterogeneous multicore system lru replacement policy least recently used replacement policy least frequently used replacement policy weighing values;libraries time frequency analysis	Future generation computer architectures are endeavoring to achieve high performance without compromise on energy efficiency. In a multiprocessor system, cache miss degrades the performance as the miss penalty scales by an exponential factor across a shared memory system when compared to general purpose processors. This instigates the need for an efficient cache replacement scheme to cater to the data needs of underlying functional units in case of a cache miss. Minimal cache miss improves resource utilization and reduces data movement across the core which in turn contributes to a high performance and lesser power dissipation. Existing replacement policies has several issues when implemented in a heterogeneous multi-core system. The commonly used LRU replacement policy does not offer optimal performance for applications with high dependencies. Motivated by the limitations of the existing algorithms, we propose a hybrid cache replacement policy which combines Least Recently Used (LRU) and Least Frequently Used (LFU) replacement policies. Each cache block has two weighing values corresponding to LRU and LFU policies and a cumulative weight is calculated using these two values. Conducting simulations over wide range of cache sizes and associativity, we show that our proposed approach has shown increased cache hit to miss ratio when compared with LRU and other conventional cache replacement policies.	algorithm;benchmark (computing);cpu cache;cache (computing);central processing unit;computer architecture;fifo (computing and electronics);gnu compiler collection;grand challenges;least frequently used;mike lesser;multi-core processor;multiprocessing;parser;shared memory;simulation;time complexity	K. M. AnandKumar;S Akash;Divyalakshmi Ganesh;Monica Snehapriya Christy	2014	2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2014.6968209	bus sniffing;pipeline burst cache;computer architecture;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;adaptive replacement cache;smart cache;cache algorithms;cache pollution;mesif protocol	Arch	-9.294941398249287	51.767654538406326	75477
969cdc43de7d5509ff684ca2ef37d6c8dc81c6c0	multiprogram scheduling parts 1 and 2: introduction and theory	scheduling algorithm	In order to exploit fully a fast computer which possesses simultaneous processing abilities, it should to a large extent schedule its own workload. The scheduling routine must be capable of extremely rapid execution if it is not to prove self-defeating. The construction of a schedule entails determining which programs are to be run concurrently and which sequentially with respect to each other. A concise scheduling algorithm is described which tends to minimize the time for executing the entire pending workload (or any subset of it), subject to external constraints such as precedence, urgency, etc. The algorithm is applicable to a wide class of machines.	algorithm;computer multitasking;scheduling (computing)	E. F. Codd	1960	Commun. ACM	10.1145/367297.367317	fair-share scheduling;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;genetic algorithm scheduling;operating system;two-level scheduling;distributed computing;least slack time scheduling;scheduling	Theory	-10.332683437474294	60.3626402560861	75673
4e9dbca4218d32a9f92d58c340f3f8f3c5020a44	best-offset hardware prefetching	prefetching hardware system on chip pollution history benchmark testing;spec cpu2006 benchmarks hardware prefetching high performance processors on chip caches performance reduction sandbox method;storage management cache storage parallel processing	Hardware prefetching is an important feature of modern high-performance processors. When the application working set is too large to fit in on-chip caches, disabling hardware pre-fetchers may result in severe performance reduction. A new prefetcher was recently introduced, the Sandbox prefetcher, that tries to find dynamically the best prefetch offset using the sandbox method. The Sandbox prefetcher uses simple hardware and was shown to be quite effective. However, the sandbox method does not take into account prefetch timeliness. We propose an offset prefetcher with a new method for selecting the prefetch offset that takes into account prefetch timeliness. We show that our Best-Offset prefetcher outperforms the Sandbox prefetcher on the SPEC CPU2006 benchmarks, with equally simple hardware.	cpu cache;central processing unit;link prefetching;prefetcher;reduced-offset lempel ziv;regular expression;sandbox (computer security);spec#;working set	Pierre Michaud	2016	2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2016.7446087	parallel computing;real-time computing;computer science;operating system	Arch	-8.138701702994439	52.94836423825612	75864
60ab25e77523bc84e56d1c5d9fd78145cc363830	an adaptive separation-aware ftl for improving the efficiency of garbage collection in ssds	storage management disc drives disc storage random access storage;cloud computing grid computing;minimal on device ram space adaptive separation aware ftl garbage collection efficiency ssds flash based solid state drives;hot data identification flash memory ssd hot cold separation	Hot/cold data separation in flash-based solid state drives has been considered important to the overall performance due to the costly garbage collection overhead. This work proposes a method that accurately and naturally identifies and separates hot/cold data while only incurs minimal overhead. The proposed method only requires minimal on-device RAM space. Simulation results have shown that the proposed ASA-FTL reduce the GC overhead by up to 33% and improve the overall response time by 9% against the most advanced existing FTL in both real workloads and synthetic workloads.	dhrystone;ftl: faster than light;garbage collection (computer science);overhead (computing);random-access memory;response time (technology);simulation;solid-state drive	Wei Xie;Yong P Chen	2014	2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing	10.1109/CCGrid.2014.52	parallel computing;real-time computing;computer hardware;computer science	Arch	-12.993740315463015	54.0112288193	75899
96a73a709a51f04f964e7427db6a51c519d67581	dynamic rescheduling of tasks in time predictable embedded systems		The work concerns an approach to the multi-threading and multitasking problem. The main research objective of the time predictable real-time systems is identified. A brief summary of the related works is given. Idea of thread interleaving is described and the architecture of the proposed time predictable embedded system is presented. The solution of the dynamic interleave controller of threads (DICT) with dynamically modified (reconfigured) priorities of tasks (threads) is analyzed. The paper investigates the model of the system and gives its simulation results. The entire system is synthesized in the hardware structure. Some experiments with that emphasize advantages of the proposed hardware implementation.	embedded system	Andrzej Pulka;Adam Milik	2012		10.3182/20120523-3-CZ-3015.00058	embedded system;real-time computing;simulation;engineering	Embedded	-9.420206649221603	58.25853333040096	75911
e30bbeb927ccb1798e8a5372ad2c2a5f33d132b6	exploring fine-grained heterogeneity with composite cores	microarchitecture;fine grain phases;adaptive architecture heterogeneous processors hardware scheduling fine grain phases;heterogeneous multicore systems microarchitectural simulation intelligent controller fine grained migration μengine migration overhead reduction composite cores energy efficient cores coarse grained phases performance requirements energy consumption energy characteristics varying performance characteristics;engines multicore processing registers pipelines microarchitecture program processors;engines;registers;multicore processing;pipelines;hardware scheduling;heterogeneous processors;adaptive architecture;program processors;power aware computing energy conservation energy consumption intelligent control multiprocessing systems	Heterogeneous multicore systems- comprising multiple cores with varying performance and energy characteristics-have emerged as a promising approach to increasing energy efficiency. Such systems reduce energy consumption by identifying application phases and migrating execution to the most efficient core that meets performance requirements. However, the overheads of migrating between cores limit opportunities to coarse-grained phases (hundreds of millions of instructions), reducing the potential to exploit energy efficient cores. We propose Composite Cores, an architecture that reduces migration overheads by bringing heterogeneity into a core. Composite Cores pairs a big and little compute μEngine that together achieve high performance and energy efficiency. By sharing architectural state between the μEngines, the migration overhead is reduced, enabling fine-grained migration and increasing the opportunities to utilize the little μEngine without sacrificing performance. An intelligent controller migrates the application between μEngines to maximize energy efficiency while constraining performance loss to a configurable bound. We evaluate Composite Cores using cycle accurate microarchitectural simulations and a detailed power model. Results show that, on average, Composite Cores are able to map 30 percent of the execution time to the little μEngine, achieving a 21 percent energy savings while maintaining 95 percent performance.	architectural state;microarchitecture;multi-core processor;overhead (computing);requirement;run time (program lifecycle phase);simulation	Andrew Lukefahr;Shruti Padmanabha;Reetuparna Das;Faissal M. Sleiman;Ronald G. Dreslinski;Thomas F. Wenisch;Scott A. Mahlke	2016	IEEE Transactions on Computers	10.1109/TC.2015.2419669	multi-core processor;embedded system;computer architecture;parallel computing;real-time computing;microarchitecture;computer science;operating system;pipeline transport;processor register	HPC	-5.111444303109391	53.678449087512824	75943
c70aa957a01682bcdf1ad6094a1ea5dd6ae723d7	scheduling parallel jobs on multicore clusters using cpu oversubscription	malleability;application reconfiguration;multicore clusters;mpi;cpu oversubscription;job scheduling	Job scheduling strategies in multiprocessing systems aim to minimize waiting times of jobs while satisfying user requirements in terms of number of execution units. However, the lack of flexibility in the requests leaves the scheduler a reduced margin of action for scheduling decisions. Many of such decisions consist on just moving ahead some specific jobs in the wait queue. In this work, we propose a job scheduling strategy that improves the overall performance and maximizes resource utilization by allowing jobs to adapt to variations in the load through CPU oversubscription and backfilling. The experimental evaluations include both real executions on multicore clusters and simulations of workload traces from real production systems. The results show that our strategy provides significant improvements over previous proposals like Gang Scheduling with Backfilling, especially in medium to high workloads with strong variations.	blocking (computing);central processing unit;computation;computer cluster;execution unit;gang scheduling;javascript style sheets;job scheduler;job shop scheduling;job stream;load balancing (computing);memory bandwidth;multi-core processor;multiprocessing;overhead (computing);overselling;requirement;roland gs;scalability;scheduling (computing);simulation;tcp global synchronization;tracing (software);unbalanced circuit;user requirements document	Gladys Utrera;Julita Corbalán;Jesús Labarta	2014	The Journal of Supercomputing	10.1007/s11227-014-1142-9	fair-share scheduling;parallel computing;real-time computing;dynamic priority scheduling;computer science;rate-monotonic scheduling;message passing interface;job scheduler;operating system;distributed computing;malleability	HPC	-15.022929431621492	59.909286820511454	76009
4f24817078ff3e64e1478c7cb0daca7f7b312404	distributed shared memory in kernel mode	shared memory kernel mechanisms distributed shared memory cluster of personal computers momemto memory configurations memory management overheads;momemto;kernel;shared memory;memory management;kernel mechanisms;personal computer;application software;workstation clusters distributed shared memory systems operating system kernels;kernel memory management application software operating systems microcomputers web server scalability writing database systems power system management;cluster of personal computers;power system management;distributed shared memory systems;database systems;writing;memory management overheads;scalability;web server;workstation clusters;operating system kernels;distributed shared memory;microcomputers;memory configurations;operating systems	In this paper, we introduce MOMEMTO (MOre MEMory Than Others) a new set of kernel mechanisms that allow users to have full control of the distributed shared memory on a cluster of personal computers. In contrast to many existing software DSM systems, MOMEMTO supports efficiently and flexibly global shared-memory allowing applications to address larger memory space than that available in a single node. MOMEMTO has been implemented in the Linux 2.4 kernel and preliminary performance results show that MOMEMTO has low memory management and communication overheads and that it can indeed perform very well for large memory configurations.	dspace;distributed shared memory;kernel (operating system);linux;memory management;numerical analysis;personal computer;protection ring;user space;web server	T. Trevisan;Vítor Santos Costa;Lauro Whately;Claudio Luis de Amorim	2002		10.1109/CAHPC.2002.1180772	memory address;uniform memory access;distributed shared memory;shared memory;interleaved memory;application software;parallel computing;kernel;scalability;distributed memory;memory geometry;computer science;physical address;virtual memory;operating system;microcomputer;distributed computing;overlay;conventional memory;extended memory;flat memory model;writing;data diffusion machine;web server;cache-only memory architecture;memory map;memory management	OS	-14.40755129593135	50.7511220432511	76057
22aa375f85e5f6a43b5ce9accda61e2eb551597d	on parallel-acting index registers	index registers;program loops;computer instructions;indexation;computer instructions index registers parallel processing program loops;parallel processing	It is shown how index registers can be made so as to act parallel to the computer instruction flow, thus not requiring a programmed instruction to decrement (or increment) and jump. This method would save one instruction per loop but would require more hardware.	increment and decrement operators;index register	David Mandelbaum	1971	IEEE Transactions on Computers	10.1109/T-C.1971.223246	parallel processing;parallel computing;real-time computing;memory type range register;computer hardware;computer science	Arch	-6.371099709835777	51.447844324471575	76330
533c5df22454d982501c21862ed287e225f2d7cc	bpe acceleration technique for s/w update for mobile phones	nand flash memory;software;flash memory;software updating;compression algorithm;random access memory;software engineering data compression encoding flash memories mobile handsets nand circuits program compilers;data compression;compression algorithms;byte pair encoding;software engineering;program code;acceleration;mobile phone;byte pair encoding algorithm;complex processes;large scale;program code compression;nand circuits;dictionaries;embedded device software updating program code compression byte pair encoding nand flash memory mobile phone;software updating functions bpe acceleration technique mobile phones complex processes program code nand flash memory compression algorithms byte pair encoding algorithm;mobile handsets;software updating functions;software algorithms;software debugging;acceleration mobile handsets embedded software random access memory large scale systems software debugging computer bugs costs read write memory compression algorithms;read write memory;mobile phones;functional requirement;program compilers;encoding;computer bugs;bpe acceleration technique;flash memories;acceleration techniques;code compression;large scale systems;embedded software;embedded device	Recently, the size of the software on embedded devices, e.g., mobile phones, has been increasing rapidly. Complex processes in large scale software, such as event handlers, require bugs to be fixed after shipment. NAND flash memory devices are adopted in such devices in order to reduce the cost. The program code in a NAND flash memory is loaded to RAM using demand paging technologies. In many cases, compressed program code is stored in the NAND flash memory and loaded by extracting the program code because the loading time for the uncompressed code is larger than the total time for loading and extracting the compressed code with some compression algorithms. Byte-Pair-Encoding (BPE) is a suitable algorithm for this purpose. The compression rate for BPE is slightly less than GZIP and the extraction speed is very fast. However, the compression speed is very slow. Software updating functions require the compression of program code on a device and have to compress quickly. This paper discusses the software update functions employed for such embedded devices, and proposes a BPE compression speed acceleration technique for software updating. The results obtained from an evaluation show that the proposed method is effective in compressing program code.	algorithm;byte pair encoding;data compression;embedded system;event (computing);flash memory;mobile phone;netbsd gzip / freebsd gzip;paging;patch (computing);random-access memory;software bug	Ryozo Kiyohara;Satoshi Mii	2010	2010 24th IEEE International Conference on Advanced Information Networking and Applications	10.1109/AINA.2010.65	data compression;parallel computing;real-time computing;computer hardware;computer science;operating system;database;statistics	Embedded	-6.002560252118533	50.151685436371274	76392
4683cfadac7cd53065fb8380937fef961b4fb796	many-core acceleration for model predictive control systems	software;human computer interaction;many core processor;model predictive control;computer networks and communications;speculative execution;computer vision and pattern recognition	This paper proposes a novel many-core execution strategy for real-time model predictive controls. The key idea is to exploit predicted input values, which are produced by the model predictive control itself, to speculatively solve an optimal control problem. It is well known that control applications are not suitable for multi- or many-core processors, because feedback-loop systems inherently stand on sequential operations. Since the proposed scheme does not rely on conventional thread-/data-level parallelism, it can be easily applied to such control systems. An analytical evaluation using a real application demonstrates the potential of performance improvement achieved by the proposed speculative executions.	central processing unit;control system;manycore processor;optimal control;parallel computing;real-time transcription;speculative execution	Satoshi Kawakami;Akihito Iwanaga;Koji Inoue	2013		10.1145/2489068.2489071	parallel computing;real-time computing;simulation;computer science	Robotics	-8.673819226338116	56.65177470936962	76508
cf8f4f46fe44a9a5ebe843d1ee7939eeef9b5846	real-time linux with budget-based resource reservation	budget reservation;qos;linux;real-time linux;rtai;real time;source code	The purpose of this paper is to propose a budget-based RTAI (Real-Time Application Interface) implementation for real-time tasks over Linux on x86 architectures, where RTAI provides a light-weight, high-performance interface for hard and soft real-time tasks over Linux. Our revised RTAI API’s are extended to enable programmers to specify a computation budget for each task, and backward compatibility is maintained with the original RTAI design. Different from the past work, we focus on the implementation of budget-based resource reservation for real-time tasks, which is made complicated by the relationship between RTAI and Linux. Modifications of RTAI are limited to a few procedures without any change made to the Linux source code, such as the timer interrupt handler, the RTAI scheduler, or rt_task_wait_period(). The feasibility of the proposed implementation is demonstrated by a system operating under Linux 2.4.0-test10 and RTAI 24.1.2 on PII and PIII platforms.	application programming interface;backward compatibility;central processing unit;computation;experiment;interrupt handler;linux;moving picture experts group;personally identifiable information;programmer;rtai;rtlinux;real-time clock;real-time computing;real-time transcription;scheduling (computing);synchronization (computer science);thread (computing);timer;user space;x86	Nei-Chiung Perng;Chin-Shuang Liu;Tei-Wei Kuo	2006	J. Inf. Sci. Eng.			Embedded	-9.461192440730773	58.12487938875105	76628
c94701b2d115dd07f3c1749155672c16c6f5ee48	online availability upgrades for parity-based raids through supplementary parity augmentations	raid reconstruction;raid;availability upgrade;trace driven simulation;analytical model;sequential monte carlo	In this article, we propose a simple but powerful online availability upgrade mechanism, Supplementary Parity Augmentations(SPA), to address the availability issue in parity-based RAID systems. The basic idea of SPA is to store and update the supplementary parity units on one or a few newly augmented spare disks for online RAID systems in the operational mode, thus achieving the goals of improving the reconstruction performance while tolerating multiple disk failures and latent sector errors simultaneously. By applying the exclusive OR operations appropriately among supplementary parity, full parity, and data units, SPA can reconstruct the data on the failed disks with a fraction of the original overhead that is proportional to the supplementary parity coverage, thus significantly reducing the overhead of data regeneration and decreasing recovery time in parity-based RAID systems. Our extensive trace-driven simulation study shows that SPA can significantly improve the reconstruction performance of the RAID5 and RAID5+0 systems, at an acceptable performance overhead imposed in the operational mode. Moreover, our reliability analytical modeling and sequential Monte-Carlo simulation demonstrate that SPA is consistently more than double the MTTDL of the RAID5 system and improves the reliability of the RAID5+0 system noticeably.	exclusive or;hot spare;monte carlo method;norm (social);overhead (computing);signed zero;simulation;standard raid levels	Lei Tian;Qiang Cao;Hong Jiang;Dan Feng;Changsheng Xie;Qin Xin	2011	TOS	10.1145/1970338.1970341	real-time computing;particle filter;computer science;operating system;computer security;parity drive;nested raid levels;standard raid levels;raid;raid processing unit	OS	-11.74068786378495	55.423998523493644	76666
96f31e97dc6c595535e68c5c46940346f15c16be	on optimal hierarchical configuration of distributed systems on mesh and hypercube	distributed system;resource scheduling;network routing optimal hierarchical configuration distributed systems mesh hypercube optimized system performance distributed monitoring resource scheduling;hierarchical partitioning;optimisation;performance evaluation;system monitoring hypercube networks performance evaluation optimisation resource allocation processor scheduling network routing;perforation;processor scheduling;resource allocation;hypercubes computerized monitoring cost function network topology communication system control routing distributed computing partitioning algorithms centralized control network servers;distributed computing;system monitoring;hierarchical control;system performance;interconnection network;network routing;optimization problem;local computation;hypercube networks	This paper studies hierarchical configuration of distributed systems for achieving optimized system performance. A distributed system consists of a collection of local processes which are distributed over the network of processors and cooperate to perform some functions. A hierarchical approach is to group and organize the distributed processes into a logical hierarchy of multiple levels, so as to coordinate the local computation/control activities to improve the overall system performance. It has been proposed as an effective way to solve various problems in distributed computing, such as distributed monitoring, resource scheduling, and network routing. The optimization problem considered in this paper is concerned with finding an optimal hierarchical partition of the processors so that the total cost is minimal. The problem in its general form has been known to be NP-hard. Therefore, we just focus on distributed computing jobs which require collecting information from all processors. One example of such job is distributed monitoring. By limiting the levels of the hierarchy to 2, we study optimal hierarchical configurations for two popular interconnection networks: mesh and hypercube. Based on analytical results, partitioning algorithms are proposed which achieve optimal communication cost attributed to information collection. We also discuss heuristic schemes for multiple-level hierarchical partitions. Although this paper is presented in terms of distributed monitoring, the approaches can also be applied to other hierarchical control problems in distributed computing.	algorithm;central processing unit;computation;distributed computing;grid network;heuristic;interconnection;job stream;mathematical optimization;mesh networking;np-hardness;optimization problem;routing;scheduling (computing)	Dajin Wang;Jiannong Cao	2003		10.1109/IPDPS.2003.1213311	optimization problem;system monitoring;distributed algorithm;routing;parallel computing;real-time computing;resource allocation;computer science;distributed computing;computer performance;distributed design patterns;replication;distributed concurrency control	HPC	-14.050660825758483	60.07263360289909	77143
ebd1827121375fa111e1cec84649f76c731c2e2e	software and hardware techniques to optimize register file utilization in vliw architectures	register file organization;instruction level parallel;cycle time;register requirements;modulo scheduling;spill code;power dissipation;clustered organization;register file;point of view;instruction scheduling;high performance;article	High-performance microprocessors are currently designed with the purpose of exploiting instruction level parallelism (ILP). The techniques used in their design and the aggressive scheduling techniques used to exploit this ILP tend to increase the register requirements of the loops. This paper reviews hardware and software techniques that alleviate the high register demands of aggressive scheduling heuristics on VLIW cores. From the software point of view, instruction scheduling can stretch lifetimes and reduce the register pressure. If more registers than those available in the architecture are required, some actions (such as the injection of spill code) have to be applied to reduce this pressure, at the expense of some performance degradation. From the hardware point of view, this degradation could be reduced if a high-capacity register file were included without causing a negative impact on the design of the processor (cycle time, area and power dissipation). Novel organizations for the register file based on clustering and hierarchical organization are necessary to meet the technology constraints. This paper proposes the used of a clustered organization and proposes an aggressive instruction scheduling technique that minimizes the negative effect of the limitations imposed by the register file organization.	cluster analysis;elegant degradation;heuristic (computer science);instruction cycle;instruction-level parallelism;microprocessor;parallel computing;point of view (computer hardware company);register allocation;register file;requirement;scheduling (computing);very long instruction word	Javier Zalamea;Josep Llosa;Eduard Ayguadé;Mateo Valero	2004	International Journal of Parallel Programming	10.1023/B:IJPP.0000042082.31819.6d	computer architecture;parallel computing;real-time computing;register window;control register;cycle time variation;computer science;dissipation;operating system;register renaming;stack register;instruction register;processor register;instruction scheduling;register file;memory address register	Arch	-5.451907990576674	53.520661653278374	77241
ccfe04f440ea9cee97e12533df721b3c3d2013c0	task transition scheduling for data-adaptable systems		Data-adaptable embedded systems operate on a variety of data streams, which requires a large degree of configurability and adaptability to support runtime changes in data stream inputs. Data-adaptable reconfigurable embedded systems, when decomposed into a series of tasks, enable a flexible runtime implementation in which a system can transition the execution of certain tasks between hardware and software while simultaneously continuing to process data during the transition. Efficient runtime scheduling of task transitions is needed to optimize system throughput and latency of the reconfiguration and transition periods. In this article, we provide an overview of a runtime framework enabling the efficient transition of tasks between software and hardware in response to changes in system inputs. We further present and analyze several runtime transition scheduling algorithms and highlight the latency and throughput tradeoffs for two data-adaptable systems. To evaluate the task transition selection algorithms, a case study was performed on an adaptable JPEG2000 implementation as well as three other synchronous dataflow systems characterized by transition latency and communication load.	algorithm;dataflow;embedded system;fifo (computing and electronics);field-programmable gate array;high-speed serial interface;jpeg 2000;manycore processor;microkernel;mutual exclusion;reconfigurable computing;rise time;schedule (project management);scheduling (computing);throughput	Nathan Sandoval;Casey Mackin;Sean Whitsitt;Vijay Shankar Gopinath;Sachidanand Mahadevan;Andrew Milakovich;Kyle Merry;Jonathan Sprinkle;Roman L. Lysecky	2017	ACM Trans. Embedded Comput. Syst.	10.1145/3047498	throughput;parallel computing;adaptability;latency (engineering);control reconfiguration;real-time computing;scheduling (computing);data stream mining;dataflow;computer science;runtime verification;distributed computing	Embedded	-9.25589152116241	58.39262975487044	77363
3fb187b9087541a3a26e87b908150b582cfba440	rank join queries in nosql databases		Rank (i.e., top-k) join queries play a key role in modern analytics tasks. However, despite their importance and unlike centralized settings, they have been completely overlooked in cloud NoSQL settings. We attempt to fill this gap: We contribute a suite of solutions and study their performance comprehensively. Baseline solutions are offered using SQLlike languages (like Hive and Pig), based on MapReduce jobs. We first provide solutions that are based on specialized indices, which may themselves be accessed using either MapReduce or coordinator-based strategies. The first index-based solution is based on inverted indices, which are accessed with MapReduce jobs. The second index-based solution adapts a popular centralized rank-join algorithm. We further contribute a novel statistical structure comprising histograms and Bloom filters, which forms the basis for the third index-based solution. We provide (i) MapReduce algorithms showing how to build these indices and statistical structures, (ii) algorithms to allow for online updates to these indices, and (iii) query processing algorithms utilizing them. We implemented all algorithms in Hadoop (HDFS) and HBase and tested them on TPC-H datasets of various scales, utilizing different queries on tables of various sizes and different score-attribute distributions. We ported our implementations to Amazon EC2 and ”in-house” lab clusters of various scales. We provide performance results for three metrics: query execution time, network bandwidth consumption, and dollar-cost for query execution.	algorithm;amazon elastic compute cloud (ec2);apache hbase;apache hadoop;apache hive;baseline (configuration management);bloom filter;centralized computing;database;ibm tivoli storage productivity center;inverted index;join (sql);mapreduce;nosql;run time (program lifecycle phase)	Nikos Ntarmos;Ioannis Patlakas;Peter Triantafillou	2014	PVLDB	10.14778/2732286.2732287	computer science;data mining;database;world wide web	DB	-16.048324560568762	54.73932598862753	77434
444defa172c6d0b12d116ea78f7aedc993c18dea	thread-based software synthesis for embedded system design	yarn;total execution time;thread based software synthesis;processor scheduling;resource allocation;data flow graphs;mixed static dynamic thread scheduler;resource management;embedded software embedded system yarn hardware delay dynamic scheduling coprocessors resource management multithreading processor scheduling;software engineering;coprocessors;embedded system;synchronisation;data flow graph;control data flow graph;embedded system design;communication overhead;scheduling;scheduling overhead;cdfg;resource allocation software engineering real time systems data flow graphs scheduling synchronisation;software synthesis;hardware software interface;control data flow graph thread based software synthesis embedded system design communication overhead hardware software interface cdfg mixed static dynamic thread scheduler scheduling overhead total execution time;embedded software;dynamic scheduling;hardware;multithreading;real time systems	We propose a thread-based software synthesis technique to reduce communication overhead incurred by the hardware-software interface in a system. We start from a CDFG that models the system. The CDFG is analyzed and partitioned into a set of threads. Then we generate a mixed static-dynamic thread scheduler. The scheduler statically schedules as many threads as possible to maximize the scheduling overhead. Then the scheduler dynamically schedules the remaining threads. Reduction of the total execution time including the communication overhead is demonstrated with some examples.	embedded system;overhead (computing);run time (program lifecycle phase);schedule (computer science);scheduling (computing);systems design;thread (computing)	Youngsoo Shin;Kiyoung Choi	1996		10.1109/EDTC.1996.494314	thread;scheduler activations;parallel computing;real-time computing;computer science;distributed computing;scheduling	EDA	-8.195277092959595	59.06412314893455	77438
1c8ee9bee23f06c57dd7bfaedfca696a81945ad2	cap: configurable resistive associative processor for near-data computing	memory management;non volatile memory associative processor approximate computing content addressable memory;energy consumption;cams;adders;nonvolatile memory;associative memory;cams associative memory nonvolatile memory energy consumption adders delays memory management;delays	Internet of Things is capable of generating huge amount of data, causing high overhead in terms of energy and performance if run on traditional CPUs and GPUs. This inefficiency comes from the limited cache size and memory bandwidth which result in large amount of data movement through memory hierarchy. In this paper, we propose a configurable associative processor, called CAP, which accelerates computation using multiple parallel memory-based cores capable of approximate or exact matching. CAP is integrated next to the main memory so it fetches the data directly from DRAM. To exploit data locality, CAMs adaptively split into highly and less frequent components and update at runtime. To further improve the CAP efficiency, we integrate a novel signature-based associative memory (SIGAM) beside each processing cores, to store highly frequent patterns and in runtime retrieve them in exact or approximate modes. Our experimental evaluations show that the CAP in approximate (exact) mode can achieve 9.4× and 5.3× (7.2× and 4.2×) energy improvement, and 4.l× and 1.3× speeds up compare to AMD GPU and ASIC CMOS-based designs while providing acceptable quality of service.	application-specific integrated circuit;approximation algorithm;block size (cryptography);cmos;central processing unit;computation;computer data storage;content-addressable memory;dynamic random-access memory;graphics processing unit;internet of things;locality of reference;memory bandwidth;memory hierarchy;overhead (computing);quality of service;run time (program lifecycle phase);the 3-d battles of worldrunner	Mohsen Imani;Tajana Simunic	2017	2017 18th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2017.7918340	uniform memory access;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;sense amplifier;non-volatile memory;memory refresh;computer hardware;computer science;operating system;content-addressable memory;computer memory;overlay;conventional memory;extended memory;flat memory model;registered memory;adder;computing with memory;memory map;non-uniform memory access;memory management	Arch	-9.413276049592403	53.56905964985773	77639
4087891c9bd4d0a368a6261dc3f887933b74415b	on simulation and design of parallel-systems schedulers: are we doing the right thing?	parallel systems schedulers;degradation;parallel job scheduling;performance evaluation;measurement;simulation of multiple processor systems;software performance evaluation;site level simulation;indexing terms;trace driven simulations;performance metric;feedback;creasy;human factors;parallel systems;feedback parallel job scheduling trace driven simulations open system model user behavior;optimal scheduling;scheduling;productivity throughput measurement job design degradation feedback costs delay;user aware design;open system model;open system;user centred design open systems parallel processing scheduling software performance evaluation;evaluation;user centred design;job design;open system trace driven simulations;user behavior;productivity;parallel job scheduling parallel systems schedulers open system trace driven simulations performance evaluation jobs packing performance metrics site level simulation creasy user satisfaction user aware design;open systems;modeling;performance metrics;jobs packing;user satisfaction;trace driven simulation;parallel processing;user model;throughput	It is customary to use open-system trace-driven simulations to evaluate the performance of parallel-system schedulers. As a consequence, all schedulers have evolved to optimize the packing of jobs in the schedule, as a means to improve a number of performance metrics that are conjectured to be correlated with user satisfaction, with the premise that this will result in a higher productivity in reality. We argue that these simulations suffer from severe limitations that lead to suboptimal scheduler designs and to even dismissing potentially good design alternatives. We propose an alternative simulation methodology called site-level simulation, in which the workload for the evaluation is generated dynamically by user models that interact with the system. We present a novel scheduler called CREASY that exploits knowledge on user behavior to directly improve user satisfaction and compare its performance to the original packing-based EASY scheduler. We show that user productivity improves by up to 50 percent under the user-aware design, while according to the conventional metrics, performance may actually degrade.	aggregate data;degradation (telecommunications);fairness measure;job stream;mcgurk effect;scheduling (computing);scripting language;set packing;simulation;software deployment;tracing (software)	Edi Shmueli;Dror G. Feitelson	2009	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2008.152	parallel processing;parallel computing;real-time computing;simulation;computer science;operating system;database;distributed computing;open system	Networks	-16.513057133170285	59.02767219553258	77694
1d1a2deab88cfe51685b15e97cd78a2387f1080e	decoupled speed scaling: analysis and evaluation	file servers;fairness;processor sharing decoupled speed scaling simple single server computer system fair sojourn protocol fsp scheduling policy ps srpt shortest remaining processing time;single machine scheduling;processor scheduling;efficiency;optimality speed scaling systems fairness efficiency;time factors energy consumption processor scheduling dynamic scheduling program processors mathematical model;optimality;speed scaling systems;single machine scheduling file servers processor scheduling	In this paper, we introduce the notion of decoupled speed scaling, wherein the speed scaling function is completely decoupled from the scheduling policy used in a simple single-server computer system. As an initial result, we first demonstrate that the Fair Sojourn Protocol (FSP) scheduling policy does not work properly with coupled (native) speed scaling, but that it can and does work well with decoupled speed scaling. We then compare the performance of PS, SRPT, and FSP scheduling policies under decoupled speed scaling, and demonstrate significant advantages for FSP. Our simulation results suggest that it might be possible to simultaneously achieve fairness, robustness, and near optimality with decoupled speed scaling.	emitter-coupled logic;fairness measure;image scaling;programming paradigm;ps (unix);robustness (computer science);scheduling (computing);server (computing);shortest remaining time;simulation;wavelet	B. Maryam Elahi;Carey L. Williamson;Philipp Woelfel	2012	2012 Ninth International Conference on Quantitative Evaluation of Systems	10.1109/QEST.2012.35	fair-share scheduling;fixed-priority pre-emptive scheduling;file server;parallel computing;real-time computing;dynamic priority scheduling;computer science;rate-monotonic scheduling;two-level scheduling;distributed computing;efficiency;round-robin scheduling	HPC	-11.523499518567407	59.7338522066008	77732
24121e6aca7e6f73e84c9812262bfada6801bed6	cache evaluation and the impact of workload choice	cache designer;cache design;workload choice;machine architecture;machine performance;line size;expected workload;cache evaluation;design estimate;realistic cache performance estimate;mapping algorithm;cache performance	"""The selection of the """"best"""" parameters for a cache design, such as size, mapping algorithm, fetch algorithm, line size, etc., is dependent on the expected workload. Similarly, the machine performance is sensitive to the cache performance which itself depends on the workload. Most cache designers have been greatly handicapped in their designs by the lack of realistic cache performance estimates. Published research generally presents data which is unrealistic in some respects, and available traces are often not representative. In this paper, we present measurements from a very wide variety of traces: there are 49 traces, taken from G machine architectures, (370, 360, VAX, MG8000, Z8000, CDC 6400), coded in 7 source languages. Statistics are shown for miss ratios, the effectiveness of prefetching in terms of both miss ratio and its effect on bus traffic, the frequency of writes, reads and instruction fetches, and the frequency of branches. Some general observations are made and a """"design estimate"""" set of miss ratios are proposed. Some """"fudge"""" factors are proposed by which statistics for workloads for one machine architecture can be used to estimate corresponding parameters for another (as yet unrealized) architecture. 1. I n t r o d u c t i o n Almost all medium and high performance machines and most high performance microprocessors now being designed will include cache memories to be used for instructions, for data or for both. There are a number of choices to be made regarding the cache including size, line size (block size), mapping algorithm, replacement algorithm, writeback algorithm, split (instructions/data) vs. unified, fetch algorithm, et cetera; see [Smit82] for a detailed discussion of these issues. Making the """"best"""" choices and selecting the *'best"""" parameters (with respect to cost and performance) depends greatly on the workload to be expected [Macd84]. For example, a cache which achieves a 99% hit ratio may cost 80% more than one which achieves 98%, may increase the CPU cost by 25°7o and may only boost overall CPU performance by 8%; that suggests that the higher performing cache is not cost effective. However, if the same two designs yield hit ratios of 90°70 and 80~ respectively, and if the performance increase would be 50~, then different conclusions might well be reached. Computer architects have been handicapped by the lack of generally available realistic cache workload estimates. *Tke material prtseated here i* bued on reJe~rck n p p o r t ~ ia part by the Nation*! Science Foundation under g n a t DCR-IU202591 and by tie Ddeue Ad*~nced Rele~rck Project; Agency ¢uder contract N000~9-82-CC,-¢235. Computer time 5 u been provided by tke Stanford Linear Accderatot Center nnder Department of EuerKy contract DEA@0~-?~hSF-00SIS. While there are hundreds of published papers on cache memories (see [Smit82] for a partial bibliography), only a few present usable data. A large fraction contain no measurements at all. Almost all of the papers that do present measurements rely on trace driven simulation using a small set of traces, and for reasons explained further below, those gra.:es are likely to be unrepresentative of the results to be expected in practice. There do exist some realistic numbers, as we note below, but they are hardly enough to constitute a design database. The purpose of this paper is discuss and explain workload selection as it relates to cache memory design, and to present data from which the designer can work. We have used 49 program address traces taken from 0 (or 5, if the 300 and 370 are the same) machine architectures (VAX, 370, 300/91, Z8000, CDC 0400, M08000), derived from 7 programming languages (Fortran, 370 Assembler, APL, C, LISP, AlgolW, Cobol) to compute overall, instruction and data miss ratios and bus traffic rates for various cache designs; these experiments show the variety of workload behavior possible. Characteristics of the traces are tabulated and the effects of some design choices are evaluated. Finally, we present what we consider to be a """"re~onable"""" set of numbers with which we believe designers can comfortably work. In that discussion, we also suggest some """"fudge"""" factors, which indicate how realistic (or available) number~ for machine architecture MI under workload conditions W1 can be used to estimate similar parameters for architecture M2 under workload WI*. In the remainder of this section, we discuss additional background for our measurement results. First we consider the advantages and disadvantages of trace driven simulation. Then we review some {possible) eases of performanee misprediction and also discuss some published and valid miss ratio figures. The second section discusees the traces used. The measurement results and analysis are in section 3, and in section 4 we propose target workload values and factors by which one workload can be used to estimate another. Section 5 summarizes our findings. 1.1 . T r a c e Dr iven Simula t ion A p r o g r a m a d d r e n t race is a trace of the sequence of (virtual) addresses accessed by a computer program or programs. T r a c e dr iven l imul&tlon involves driving a simulation model of a system with a trace of external stimuli rather than with a random number generator. Trace driven simulation is a very good way to study many aspects of cache design and performance, for a number of reasons. First, it is superior to either pure mathematical models or random number driven simulation because there do not currently exist any generally accepted or believable models for those characteristics of program behavior that determine cache performance; thus it is not possible to specify a realistic model nor to drive a simulator with a good representation of a program. A trace properly 0149-7111/85/0000/0064501.00 © 1985 IEEE 64 represents at least one real program, and in certain respects can be expected to drive the simulator correctly. It is important to note that a trace reflects not only the program traced and the functional architecture of the machine (instruction set) but also the design a r c h i t e c t u r e (higher level implementation). In particular, t h e n u m b e r o f m e m o r y references k affected by the width o f the data path to memory : fetching two four-byte instructions requires 4, 2 or 1 memory reference, depending on whether the memory interface is 2, 4 or 8 bytes wide. It also depends on how much """"memory"""" the interface itself has; if one request is for 4 bytes, the next request is for the next four bytes, and the interface is 8 bytes wide, then fewer fetches will result if the interface """"remembers"""" that it has the target four bytes of the second fetch rather than redoing the fetch. The interface can be quite complex, as with the lfetch buffer in the VAX 11/780 [Clar83] and can behave differently for instructions and data. (A trace should reflect, to the greatest possible extent, only the functional architecture; the design architecture should and usually can be emulated in the simulator.) A simulator is also much better in many ways than the construction of prototype designs. It is far faster to build a simulator, and the design being simulated can be varied easily, sometimes by just changing an input parameter. Conversely, a hardware prototype can require man-years to build and can be varied little if at all. Also, the results of a live workload tend to yield slightly different results (e.g. 1% to 3%) from run to run, depending on the random setting of initial conditions such as the angular position of the disks [Curt75]. For the reasons given above, trace driven simulation has been used for almost every research paper which presents cache measurements, with a few exceptions discussed below. There are, however, several reasons why the results of trace driven simulations should be taken with a grain of salt. (1) A trace driven simulation of a million memory addresses, which is fairly long, represents about 1/30 of a second for a machine such as the IBM 3081, and only about one second for an M68000; thus a trace is only a very small sample of a real workload. (2) Traces seldom are taken from the """"messiest"""" parts of large programs; more often they are traces of the initial portions of small programs. (3) It is very difficult to trace the operating system (OS) and few OS traces are available. On many machines, however, the OS dominates the workload. (4) Most real machines task switch every few thousand instructions and are constantly taking interrupts. It is difficult to include this effect accurately in a trace driven simulation and many simulators don't try. (5) The sequence of memory addresses presented to the cache can vary with hardware buffers such ~ prefetch buffers and loop buffers, and is certainly sensitive to the data path width. Thus the trace itself may not be completely accurate with ree.pect to the implementation of the architecture. (0) In running machines, a certain (usually small) fraction of the cache activity is due to input/output; this effect is seldom included in trace driven simulations. In this paper we are primarily concerned with items 1-3 immediately above. By presenting the results of a very large number of simulations, one can get an idea of the range of program behavior. Included are two traces of IBM's MVS operating system, which should have performance that is close to the worst likely to be observed. 1.2. Rea l W o r k l o a d s and Ques t ionab le Estlmattm There arc only a small number of papers in which provide measurements taken by hardware monitors from running machines. In [Mila75] it is reported that a 16K cache on an IBM 370/105-2 running VS2 had a 0.94 hit ratio, with 1.6 fetches per instruction and .22 stores/instruction; it is also found that 73% of the CPU cycles were used in supervisor state. Merrill [Merr74] found cache hit ratios for a 16K cache in the 370/168 of 0.932 to 0.997 for six applications programs, and also reports that the performance (MI"""	algol w;apl;alan jay smith;amdahl's law;angularjs;assembly language;block size (cryptography);branch misprediction;brian;byte;cdc 6400;cobol;cpu cache;cache (computing);central processing unit;circa;computer architecture;computer program;coreconnect;currah;digital footprint;emulator;experiment;fortran;front-side bus;heart rate variability;hierarchical storage management;hit (internet);ibm system/370;ieee micro;ifip working group 2.1;initial condition;input/output;international federation for information processing;interrupt;kaisa nyberg;lisp;locality of reference;mix;mathematical model;memory address;microprocessor;minoru 3d webcam;motorola 68000;multiprocessing;operating system;page replacement algorithm;paging;parameter (computer programming);programming language;prototype;random number generation;resources, events, agents (accounting model);salt (cryptography);simula;simulation;tracing (software);vax;vax-11;very-large-scale integration	Alan Jay Smith	1985		10.1145/327010.327132	bus sniffing;parallel computing;real-time computing;cache coloring;cpu cache;cache;computer science;write-once;cache invalidation;operating system;smart cache;cache algorithms;cache pollution	Arch	-13.01198088768605	46.84496489136479	77792
41f1512c94a4da6d1d25d144c3581a0797c6d327	an algorithm for optimally exploiting spatial and temporal locality in upper memory levels	belady;eficacia sistema;optimal memory management;optimisation;gestion memoire;performance evaluation;cache;optimizacion;storage management;performance systeme;cache memory;system performance;local memory;antememoria;algorithme;upper bound;algorithm;gestion memoria;antememoire;memory optimization;memory management hardware upper bound algorithm design and analysis timing cache memory cost function performance analysis testing data mining;optimization;performance evaluation storage management;optimal memory management temporal locality spatial locality performance upper bound current memory optimizations nearly minimum memory traffic spec95 benchmarks;algoritmo	ÐIn this study, we present an extension of Belady's MIN algorithm that optimally and simultaneously exploits spatial and temporal locality. Thus, this algorithm provides a performance upper bound of upper memory levels. The purpose of this algorithm is to assess current memory optimizations and to evaluate the potential benefits of future optimizations. We formally prove the optimality of this new algorithm with respect to minimizing misses and we show experimentally that the algorithm produces nearly minimum memory traffic on the SPEC95 benchmarks. Index TermsÐOptimal memory management, local memory, cache, Belady.	algorithm;cpu cache;experiment;locality of reference;lászló bélády;memory management	Olivier Temam	1999	IEEE Trans. Computers	10.1109/12.752656	parallel computing;real-time computing;cpu cache;cache;computer science;operating system;computer performance;upper and lower bounds;algorithm	EDA	-10.637390532710661	50.74628330922257	77819
e029c5df5e3578cde0c3d213bfbcb5d2d123f61a	kernel-assisted and topology-aware mpi collective communications on multicore/many-core platforms	cluster;hierarchical;collective communication;hpc;multicore;mpi	Multicore Clusters, which have become the most prominent form of High Performance Computing (HPC) systems, challenge the performance of MPI applications with non-uniform memory accesses and shared cache hierarchies. Recent advances in MPI collective communications have alleviated the performance issue exposed by deep memory hierarchies by carefully considering the mapping between the collective topology and the hardware topologies, as well as the use of single-copy kernel assisted mechanisms. However, on distributed environments, a single level approach cannot encompass the extreme variations not only in bandwidth and latency capabilities, but also in the capability to support duplex communications or operate multiple concurrent copies. This calls for a collaborative approach between multiple layers of collective algorithms, dedicated to extracting the maximum degree of parallelism from the collective algorithm by consolidating the intraand inter-node communications. In this work, we present HierKNEM, a kernel-assisted topology-aware collective framework, and the mechanisms deployed by this framework to orchestrate the collaboration between multiple layers of collective algorithms. The resulting scheme maximizes the overlap of intraand inter-node communications. We demonstrate experimentally, by considering three of the most used collective operations (Broadcast, Allgather and Reduction), that (1) this approach is immune to modifications of the underlying process-core binding; (2) it outperforms state-of-art MPI libraries (OpenMPI, MPICH2 and MVAPICH2) demonstrating up to a 30x speedup for synthetic benchmarks, and up to a 3x acceleration for a parallel graph application (ASP); (3) it furthermore demonstrates a linear speedup with the increase of the number of cores per compute node, a paramount requirement for scalability on future many-core	algorithm;benchmark (computing);collective intelligence;degree of parallelism;dhrystone;duplex (telecommunications);experiment;kernel (operating system);library (computing);mpich;manycore processor;memory hierarchy;message passing interface;multi-core processor;one-to-many (data model);open mpi;parallel computing;pipeline (computing);scalability;speedup	Teng Ma;George Bosilca;Aurelien Bouteiller;Jack J. Dongarra	2013	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2013.01.015	multi-core processor;supercomputer;parallel computing;real-time computing;computer science;message passing interface;operating system;distributed computing;hierarchy;cluster	HPC	-9.051649338563012	46.49847889897393	77926
29e486ea0dac93d60cf68c0e7b2024b1141bef4a	performance impacts of look-ahead execution in the conversation scheme	application environments;conversation scheme;performance evaluation;performance indicator;application environments performance impacts queueing network models look ahead execution conversation scheme synchronization overhead performance indicators system throughput;performance impacts;queueing theory;distributed processing;system throughput;look ahead;look ahead execution;computer networks;failure analysis;fault tolerant computing;applications programs computers;queueing theory computer networks fault tolerant computing performance evaluation;fault tolerance;queuing networks;performance indicators;queueing network models;computer systems performance;synchronization overhead;testing fault tolerance costs fault tolerant systems finishing throughput concurrent computing distributed computing application software hardware;hardware	One of the basic issues that should be resolved in order to broaden the application range of the conversation scheme for design and execution of fault-tolerant concurrent programs is the control of the execution overhead inherent in the scheme. Recently, it bas become known that under practical circumstances, the performance of a fault-tolerant multiprocessor/multicomputer system operating under the basic conversation execution scheme would be significantly affected by the synchronization required of the processes in exiting from a conversation. The look-ahead execution approach, that allows early finishing participant processes to exit from a conversation before other participants finish their conversation activities, is adopted here as a fundamental approach to reducing the synchronization overhead. Queueing network models are developed for both the system operating under the basic conversation execution scheme and the system operating under the execution scheme extended with the look-ahead capability. Based on the models, various performance indicators such as the system throughput, the average number of processors idling inside a conversation due to the synchronization required, and the average time spent in a conversation, are evaluated numerically for different application environments. The performances under the look-ahead execution scheme are compared against those under the basic conversation execution scheme. The results provide insights into the extent of benefits that can be brought in by the look-ahead execution approach.	central processing unit;fault tolerance;multiprocessing;numerical analysis;overhead (computing);parallel computing;performance;queueing theory;throughput	K. H. Kim;Seung-Min Yang	1989	IEEE Trans. Computers	10.1109/12.30872	embedded system;parallel computing;real-time computing;computer science;operating system;performance indicator;distributed computing;computer network	Embedded	-14.769541216296776	47.29997597593119	78105
7ec7cb67901e1d429e022a18e4c162b56d181158	implementing autosar scheduling and resource management on an embedded smt processor	embedded smt processor;multithreaded processor;task filtering method;autosar operating system interface;software development;implementing autosar scheduling;automotive domain;autosar os;embedded processor;resource management;autosar task;smt processor;autosar specification;simultaneous multithreading;operating system;register allocation;data structure;multi core processor;resource manager;priority ceiling protocol	The AUTOSAR specification provides a common standard for software development in the automotive domain. Its functional definition is based on the concept of single-threaded processors. Recent trends in embedded processors provide new possibilities for more powerful processors using parallel execution techniques like multithreading and multi-cores. We discuss the implementation of the AUTOSAR operating system interface on a modern simultaneous multithreaded (SMT) processor. Several problems in resource management arise when AUTOSAR tasks are executed concurrently on a multithreaded processor. Especially deadlocks, which should be averted through the priority ceiling protocol, can reoccur. We solve this problems by extending AUTOSAR OS by the Task Filtering Method to avoid deadlocks in multithreaded processors. Other synchronisation problems arising through the parallel execution of tasks are solved through the use of lock-free data structures. In the end, we propose some extensions to the AUTOSAR specification so it can be used in software development for SMT processors. We develop some additional requirements on such SMT processors to enable the use of the Task Filtering Method. Our work gives also perspectives for software development on upcoming multi-core processors in the automotive domain.	autosar;central processing unit;data structure;deadlock;embedded system;multi-core processor;multithreading (computer architecture);non-blocking algorithm;operating system;priority ceiling protocol;requirement;scheduling (computing);simultaneous multithreading;software development;thread (computing)	Florian Kluge;Chenglong Yu;Jörg Mische;Sascha Uhrig;Theo Ungerer	2009			multi-core processor;embedded system;computer architecture;parallel computing;real-time computing;data structure;computer science;software development;operating system;simultaneous multithreading;register allocation;priority ceiling protocol	Embedded	-8.560611685293653	58.25766022481142	78178
e4303aa06ac6bd5ecdb44da3d29a2d9462ef5cee	memory expansion technology (mxt): competitive impact	performance improvement;on the fly	Memory Expansion Technology (MXT) has been discussed in a number of forums. It is a hardware-implemented means for softwaretransparent on-the-fly compression of the main-memory content of a computer system. For a very broad set of workloads, it provides a compression ratio of 2:1 or better. This ability to compress and store data in fewer bytes effectively doubles the apparent capacity of memory at minimal cost. While it is clear that a doubling of memory at little cost is bound to improve the price/performance of a system that can be offered to customers, the magnitude of the impact of MXT on price/performance has not been quantified. This paper estimates the range of price/performance improvements for typical workloads from available data. To summarize, the results indicate that MXT improves price/performance by 25% to 70%. The competitive impact of such a large step function in price/performance from a single technology is profound; it is comparable to the entire gross margin in the competitive market for “PC servers.” Introduction Memory Expansion Technology (MXT*) has been discussed in a number of forums [1–7]. It is a hardwareimplemented means for software-transparent on-the-fly compression of the main-memory content of a computer system. For a very broad set of workloads, it provides compression of 2:1 or better [3, 4]. This ability to compress and store data in fewer bytes effectively doubles the apparent capacity of memory at minimal cost. Since memory cost is frequently the single most expensive core component in server systems, it is clear that doubling memory at little cost will surely improve the price/performance of a system. The magnitude or impact of MXT on price/performance has not been quantified or fully appreciated. This paper uses available data on workloads and pricing data to estimate the range of MXT price/performance improvements. To summarize, the results indicate that MXT improves price/performance by 25% to 70%. The competitive impact of such a large step function in price/performance from a single technology is profound. No known alternate technologies or approaches exist that would allow such a large “delta” between two otherwise equivalent implementations using otherwise identical technology. To overcome the price/performance advantage with a simple pricing adjustment would require	benchmark (computing);british undergraduate degree classification;bundle adjustment;byte;computer data storage;performance evaluation;period-doubling bifurcation;server (computing);throughput;transaction processing	T. Basil Smith;Bülent Abali;Dan E. Poff;R. Brett Tremaine	2001	IBM Journal of Research and Development	10.1147/rd.452.0303	simulation;telecommunications;computer science;engineering	Arch	-9.83425166800683	54.96011512090079	78290
8fe0f48bfc934eaccda272630a1dfd4f4cf71165	exact performance estimates for multiprocessor memory and bus interference	model design;cycle time;evaluation performance;interferencia;errors;time dependent;general and miscellaneous mathematics computing and information science;probability;storage access;performance evaluation;modelo markov;canal bus;multiprocessor;red petri;memoria acceso;circuito autobus;performance estimation;evaluacion prestacion;performance;multiprocessors;geometry;performance comparison;exact results;petri nets bus contention markov models memory interference multiprocessors performance comparison performance evaluation;bus contention;tiempo acceso;generation time;interference;mathematical logic;data mining;firing;memory access;computational modeling;markov model;calculateur mimd;time dependence;stochastic processes;program processors firing materials requirements planning petri nets computational modeling stochastic processes data mining;probability distribution;state space;array processors;acces memoire;materials requirements planning;markov models;memory interference;bus channel;algorithms;temps acces;modele markov;petri nets;multiprocesador;petri net;critical value;size;memory devices;program processors;mathematics 990210 supercomputers 1987 1989;reseau petri;mimd computer;access time;multiprocesseur	Exact results are given for the processing power in a multibus multiprocessor with constant memory cycle times and geometric interrequest times. Both uniform and nonuniform memory accesses are considered. Such results have not previously been obtained. In order to derive these results we use a method of introducing time into Petri nets, called Generalized Timed Petri Nets (GTPN), that we have developed. We describe the GTPN and how it is applied to the multiprocessor interference question. We reach several new conclusions. A commonly used definition of processing power can lead to substantial underestimation of the true processing power of the system. If the real system has a constant memory access time and any number of buses, then assuming an exponential access time can lead to substantial errors when estimating processing power probability distributions. In multibus systems with only a few buses a critical memory interrequest time exists. Performance close to that with a crossbar is attainable when the interrequest time is larger than the critical value. Obtaining these results illustrates the advantages, for moderate size state spaces, of the GTPN over simulation with respect to both model design and running time.	access time;cas latency;crossbar switch;interference (communication);multibus;multiprocessing;petri net;simulation;time complexity	Mark A. Holliday;Mary K. Vernon	1987	IEEE Transactions on Computers	10.1109/TC.1987.5009450	stochastic process;embedded system;parallel computing;real-time computing;telecommunications;computer science;theoretical computer science;operating system;markov model;petri net;algorithm;statistics	Arch	-9.841858764806155	49.381056602139196	78403
202f493f7fe19fdda31eb9aca7d5663c28f81338	deploying hardware locks to improve performance and energy efficiency of hardware transactional memory	hardware transactional memory;traditional implementation;execution time;hardware transactional memory system;gcommit deploys hardware lock;lazy-lazy systems result;energy consumption;deploying hardware lock;hardware lock;dedicated hardware;gcommit protocol;low cost hardware implementation;energy efficiency	In the search for new paradigms to simplify multithreaded programming, Transactional Memory (TM) is currently being advocated as a promising alternative to lock-based synchronization. Among the two most important alternatives proposed for conflict detection and data versioning in today's Hardware Transactional Memory systems (HTMs), the Lazy-Lazy one allows increased concurrency, potentially bringing higher performance levels in most cases. Unfortunately, the implementation of the commit protocol in Lazy-Lazy systems results in increased complexity and has severe impact on performance and energy consumption. In this work, we propose GCommit, an efficient and low cost hardware implementation of the SEQ commit protocol based on the use of hardware locks. Specifically, GCommit deploys hardware locks to ensure exclusive access to shared data at commit time. Implementing this functionality using dedicated hardware brings important benefits in terms of execution time as well as energy consumption with respect to traditional commit protocols that use the general-purpose interconnection network . Additionally, our proposal has negligible requirements in terms of area. Results for a 16-core CMP show that the GCommit protocol obtains average reductions of 15.7% and 13.7% in terms of execution time and energy consumption, respectively, compared with a traditional implementation of Scalable TCC with SEQ, a high-performance commit protocol proposed in the literature.	lock (computer science);transactional memory	Epifanio Gaona-Ramírez;José L. Abellán;Manuel E. Acacio;Juan Fernández Peinador	2013		10.1007/978-3-642-36424-2_19	three-phase commit protocol;embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing	Arch	-14.205336097676362	49.467104665408684	78442
56abc178c22f89eca36899eddcaf34d5319c9716	adaptive workload prediction of grid performance in confidence windows	grid performance;prediction method;heterogeneous resource nodes;workload characterization;prediction error;computational grid;mean square error methods grid computing kalman filters;fluctuations;processor scheduling;kalman filters;filters;kalman filter;autoregression method;testing;adaptive hybrid method;confidence windows;ahmodel;accuracy;prediction methods;adaptive workload prediction;hybrid method;distributed environment;mean square error;and parallel applications grid computing performance prediction workload characterization autoregression method kalman filter savitzky golay filter;fluctuations filters grid computing prediction methods predictive models processor scheduling measurement errors testing performance gain accuracy;savitzky golay filter;prediction accuracy;performance gain;mean square error methods;performance prediction;predictive models;auvergridgrid5000;computational grids;computational grids adaptive workload prediction grid performance confidence windows heterogeneous resource nodes distributed environment kalman filter savitzky golay filter adaptive hybrid method auvergridgrid5000 mean square error static point value autoregression prediction method ahmodel;and parallel applications;grid computing;parallel applications;static point value autoregression prediction method;measurement errors	Predicting grid performance is a complex task because heterogeneous resource nodes are involved in a distributed environment. Long execution workload on a grid is even harder to predict due to heavy load fluctuations. In this paper, we use Kalman filter to minimize the prediction errors. We apply Savitzky-Golay filter to train a sequence of confidence windows. The purpose is to smooth the prediction process from being disturbed by load fluctuations. We present a new adaptive hybrid method (AHModel) for load prediction guided by trained confidence windows. We test the effectiveness of this new prediction scheme with real-life workload traces on the AuverGrid and Grid5000 in France. Both theoretical and experimental results are reported in this paper. As the lookahead span increases from 10 to 50 steps (5 minutes per step), the AHModel predicts the grid workload with a mean-square error (MSE) of 0.04-0.73 percent, compared with 2.54-30.2 percent in using the static point value autoregression (AR) prediction method. The significant gain in prediction accuracy makes the new model very attractive to predict Grid performance. The model was proved especially effective to predict large workload that demands very long execution time, such as exceeding 4 hours on the Grid5000 over 5,000 processors. With minor changes of some system parameters, the AHModel can apply to other computational grids as well. At the end, we discuss extended research issues and tool development for Grid performance prediction.	autoregressive model;central processing unit;computation;grid computing;kalman filter;mean squared error;microsoft windows;parsing;performance prediction;real life;run time (program lifecycle phase);tracing (software)	Yongwei Wu;Kai Hwang;Yulai Yuan;Weimin Zheng	2010	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2009.137	kalman filter;real-time computing;computer science;theoretical computer science;machine learning;statistics	HPC	-17.360757550876798	59.48692120357627	78482
f2f47ecaddc7280104d72c5d10717c682a097c81	improving cache behavior of dynamically allocated data structures	libraries;scientific application;cache storage;cache behavior;performance evaluation;optimizing compiler;performance evaluation cache storage data structures;size reduction transformations;performance;ialloc;heterogeneous data;instance interleaving;tree data structures;performance tuning cache behavior dynamically allocated data structures field reorganization instance interleaving ialloc;interleaved codes;locality of reference;data structures;data structures libraries bandwidth interleaved codes tree data structures performance gain;performance gain;bandwidth;data layout;memory allocation;dynamically allocated;data structure;loop and data transformations;performance tuning;field reorganization	Poor data layout in memory may generate weak data locality and poor performance. Code transformations such as loop blocking or interchanging and array padding have addressed this issue for scientific applications. However many generalist applications do not use data arrays, but dynamically allocated heterogeneous data structures. In this paper, we explore two data layout techniques for dynamically allocated data structures: field reorganization, and instance interleaving. The application of these techniques may be guided by program profiling. This allows significant cache behavior improvements on some applications. To support instance interleaving, we developed a specific memory allocation library called ialloc. An ialloc-like library may be of great help in a toolbox for performance tuning of general-purpose applications.	benchmark (computing);blocking (computing);cpu cache;compiler;data mining;data structure;experiment;forward error correction;general-purpose markup language;locality of reference;mathematical optimization;memory hierarchy;memory management;natural language;numerical analysis;performance tuning;programmer;radiosity (computer graphics);software design pattern;translation lookaside buffer	D. N. Truong;François Bodin;André Seznec	1998		10.1109/PACT.1998.727268	locality of reference;parallel computing;real-time computing;data structure;performance;computer science;theoretical computer science;operating system;optimizing compiler;tree;programming language;bandwidth;memory management	HPC	-6.51166952165412	46.62162824563683	78508
a5f96802b3dc16a8ec1c1bdd7e0269c5f2c0022b	distributed storage allocation for heterogeneous systems	probability;storage management distributed processing probability resource allocation;resource allocation;storage management;distributed processing;resource management peer to peer computing approximation algorithms reliability optimization electrical engineering cloud computing;streaming code design distributed storage allocation heterogeneous systems distributed storage system storage nodes heterogeneous access probabilities unit size data object one level symmetric allocation storage budget multi k level symmetric allocation	We consider a distributed storage system where the storage nodes have heterogeneous access probabilities. The problem is to allocate a given storage budget across the nodes so as to store a unit-size data object with maximum reliability. We propose efficient algorithms for optimizing over several classes of allocations. In the basic one-level symmetric allocation, the storage budget is spread evenly over an appropriately chosen subset of nodes. In the multi k-level symmetric allocation, the budget is divided into k parts, each spread evenly over a different subset of nodes, such that the amount allocated to each node in the higher levels is multiple times that of the last level. These allocations are simpler and are shown to outperform existing methods in numerical experiments. We also describe an application of the symmetric allocations to the design of streaming codes.	clustered file system;code;computer data storage;experiment;norm (social);numerical analysis;streaming algorithm;time complexity	Zhao Li;Tracey Ho;Derek Leong;Hongyi Yao	2013	2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/Allerton.2013.6736541	real-time computing;converged storage;computer science;theoretical computer science;distributed computing	HPC	-15.256607441333411	57.44739827251058	78509
c2db455422d5a70ee5c29f337a5979a20274e9d0	hpc-gap: engineering a 21st-century high-performance computer algebra system	computational algebra;high performance computing;journal article;qa75 electronic computers computer science;parallelism;multicore	model of hierarchical distances. The coordination DSL HdpH exposes node-to-node latencies abstractly to the programmer as distances in a metric space. More precisely, the distance between nodes is a real-valued binary function satisfying the axioms of ultrametric spaces. As a result, the programmer need not be concerned with actual latencies and can instead express task scheduling constraints in terms of relative distances like here, close by or far away. We refer the reader to [58] for more detail about ultrametric distances in HdpH. Task scheduling HdpH offers two modes of work distribution: on-demand implicit task placement and eager explicit task placement. On-demand implicit task placement is realised by a distributed work stealing algorithm. Upon creation, a task is stored in the creating node’s local task pool. Stored with the task is its radius, as chosen by the programmer. The task radius is the maximum distance the task may travel away from its originating node. HdpH’s work stealing algorithm is based on random stealing and has two properties relating to radii: the task radius is a strict bound on the distance between victim and thief; thieves prefer to steal tasks within small radii. The radius can be viewed as expressing a size constraint on a task, where larger radii mean bigger tasks. Adopting this view, HdpH’s work stealing algorithm prefers to keep small tasks near their originating nodes, similar to [59], in the hope of minimising communication overheads related to work stealing. Many basic SGP2 skeletons, for instance parGAPCalls, rely on HdpH work stealing, typically without imposing constraints on work stealing distances; we refer to [4] for examples of SGP2 skeletons that make use of radii for bounding work stealing. © 2016 The Authors. Concurrency and Computation: Practice and Experience Published by John Wiley & Sons Ltd. Concurrency Computat.: Pract. Exper. 2016; 28:3606–3636 DOI: 10.1002/cpe HPC-GAP — ENGINEERING A HIGH-PERFORMANCE COMPUTER ALGEBRA SYSTEM 3623 On-demand random task placement performs well with irregular parallelism. However, it tends to under-utilise large scale architectures at the beginning of the computation. To combat this drawback, HdpH offers eager explicit task placement as a complementary placement mode. Explicit placement ships tasks to selected nodes, where they are executed immediately, taking priority over any implicitly placed tasks. Eager execution implies that these tasks are meant to perform coordination, e. g. create further tasks, rather than actual computation. The downside of explicit placement is that the programmer has to select target nodes, which may jeopardise portability. Fortunately, HdpH’s abstract ultrametric distances offer some help for the purpose of flooding large architectures with work. More precisely, the HdpH runtime identifies an equidistant basis, that is, a maximal set of nodes such that any two basis nodes are maximally far apart. At the beginning of a parallel computation, the programmer can explicitly place large tasks at the basis nodes, where each will generate smaller tasks that can be stolen quickly by nearby nodes; stealing may or may not be bounded, depending on the irregularity of the small tasks. SGP2 offers a number of skeletons implementing such a two-level task distribution, for example, the parMap2Level and parMap2LevelRelaxed skeletons used in Section 5.3, and detailed in [4]. Reliability. The coordination DSL HdpH is designed for transparent fault tolerance and [60] presents an alternative work stealing scheduler that monitors nodes and pessimistically replicates tasks that may have been lost to node failure. The fault tolerant work stealing is shown to have low runtime overheads. The fault tolerance properties of HdpH are, however, not yet inherited by SGP2 because the current GAP binding is not fault tolerant. 5. PERFORMANCE EVALUATION This section reports a performance and interworking evaluation of the HPC-GAP components. The primary comparators are the SumEuler benchmark implementations from the preceding sections, although we investigate some GAP5 concurrency overheads using microbenchmarks. We measure strong scaling, that is, speedups and runtimes, for GAP5 on a multicore (Section 5.1), and for MPIGAP and SymGridPar2 (Sections 5.2 and 5.4) on Beowulf clusters. Only weak scaling is appropriate for large architectures, and we measure SymGridPar2 on up to 32K cores of the HECToR HPC (Section 5.3). Section 5.2 demonstrates the interworking MPI-GAP and GAP4, and Section 5.4 demonstrates the interworking SymGridPar2 with both GAP4 and GAP5. 5.1. GAP5 evaluation We consider both the overheads on sequential programs introduced by managing concurrency in GAP5 and demonstrate the performance gains that can be obtained through parallelisation. 5.1.1. Sources of concurrency overheads. GAP5 exposes essentially the same programming language and libraries as GAP4, augmented with the task and region features described in Section 2. However, supporting multithreading requires many, and often significant, changes even where the functionality is unchanged. While these changes should be transparent to GAP programmers, they affect the performance of purely sequential code. These performance differences can be attributed to one or more of the following factors. 1. Whereas GAP4 has a sequential, generational, compacting garbage collector, GAP5 has a concurrent, non-generational, non-compacting garbage collector. Depending on the workload, either GAP4 or GAP5 can perform better; GAP4 where the generational collection wins out, GAP5 where concurrent collection (with a sufficient number of processors) makes it faster. Generational collection generally wins out when a program allocates a large number of shortlived objects. © 2016 The Authors. Concurrency and Computation: Practice and Experience Published by John Wiley & Sons Ltd. Concurrency Computat.: Pract. Exper. 2016; 28:3606–3636 DOI: 10.1002/cpe 3624 R. BEHRENDS ET AL. 2. Memory allocation in GAP4 occurs through a very fast bump allocator made possible by the compacting garbage collector. Memory allocation in GAP5 is more complicated in that it has to support concurrent allocation, which incurs a small, but non-zero, overhead. Most allocations occur through the thread-local allocator, which is nearly as fast as the GAP4 bump allocator, but larger chunks of memory cannot be handled this way and require that their allocation be serialised. 3. To ensure memory safety, the GAP5 kernel code had to be instrumented with checks that no GAP object is being accessed without the corresponding lock being held. Like bound checks for arrays, this incurs an unavoidable overhead. While this instrumentation can be disabled if speed is essential, it is generally assumed that users want these checks to be enabled when running parallel code. The instrumentation is currently performed (almost) fully automatically by a separate tool as part of the build process that performs dataflow analysis and attempts to optimise the instrumentation (such as hoisting checks out of loops). This automated instrumentation can be manually overridden where the optimiser fails to correctly identify such possibilities, though so far, we make very little use of that. 4. The current GAP5 implementation does not yet fully support GAP-to-C compilation for all new features. As a result, some core GAP modules that are compiled in GAP4 are still being interpreted in GAP5. 5. A few internal data structures of the GAP kernel had to be redesigned in order to make them thread-safe. One example is the implementation of global variables in GAP, which required non-trivial changes to ensure that multiple threads can access them concurrently, even where they, and their contents, are immutable. The overhead varies from program to program, and the combined effect of the concurrency overheads is that sequential code typically executes between 10% and 40% more slowly in GAP5 than in GAP4, based on our observations with various synthetic benchmarks and existing GAP code. Sections 5.1.3 and 5.3 report the overheads for specific programs, and it is reassuring to see that the overheads fall as the parallel system scales. We also expect to be able to eliminate the remaining avoidable overheads in the medium term. Nevertheless, our plans for GAP5 include a purely sequential build option for code that cannot effectively exploit parallelism. 5.1.2. Quantifying concurrency overheads. The first GAP 5 concurrency overhead we investigate is that of instrumenting the GAP5 kernel with guards against race conditions. We execute the Figure 10. Microbenchmarks to measure GAP5 instrumentation overheads. © 2016 The Authors. Concurrency and Computation: Practice and Experience Published by John Wiley & Sons Ltd. Concurrency Computat.: Pract. Exper. 2016; 28:3606–3636 DOI: 10.1002/cpe HPC-GAP — ENGINEERING A HIGH-PERFORMANCE COMPUTER ALGEBRA SYSTEM 3625 Table I. GAP5 instrumentation runtime overheads. GAP5 GAP5 (no checks) (checks) Overhead benchmark_base 3.4 s 3.4 s 0% benchmark_inc 12.8 s 13.7 s 7% benchmark_index 50.7 s 66.5 s 31% Figure 11. GAP5 SumEuler runtimes and speedups (multicore). micro-benchmarks in Figure 10 with and without instrumentation, taking the median value over five runs. All of the micro-benchmarks allocate only a small and fixed amount of memory and do not access global variables, so neither garbage collection nor global variable access has a measurable effect on their performance. Measurements in this section are performed on a 64-core machine with AMD Opteron 6376 processors. The results in Table I show that for benchmark_base there is almost no overhead for a basic for loop. The benchmark_inc result shows that there is a small amount of overhead for performing local variable increments. The benchmark_index result shows that there is around a 33% overhead (after subtracting the time for the for loop) for performing increments on a		Reimer Behrends;Kevin Hammond;Vladimir Janjic;Alexander Konovalov;Steve Linton;Hans-Wolfgang Loidl;Patrick Maier;Philip W. Trinder	2016	Concurrency and Computation: Practice and Experience	10.1002/cpe.3746	multi-core processor;supercomputer;parallel computing;computer science;theoretical computer science;operating system;symbolic-numeric computation;database;distributed computing;programming language;unconventional computing	PL	-13.289210077971564	46.79974373969374	78722
115ed00cea5a77d710a90e55f19c57865aee4d78	optimizing runtime reconfiguration decisions	software;reconfigurable system;reconfiguration management runtime reconfigurable systems;partial reconfiguration;reconfiguration management;reconfigurable architectures;hardware accelerator;runtime;system performance;coprocessors;monitoring;partially reconfigurable hardware accelerators;runtime reconfigurable systems;hardware runtime software coprocessors field programmable gate arrays monitoring delay;threshold based reconfiguration decision schemes;threshold based reconfiguration decision schemes runtime reconfiguration decisions partially reconfigurable hardware accelerators runtime reconfigurable network coprocessor;runtime reconfiguration decisions;field programmable gate arrays;simulation model;hardware;runtime reconfigurable network coprocessor	Partially reconfigurable hardware accelerators enable the offloading of computative intensive tasks from software to hardware at runtime. Beside handling the technical aspects, finding a proper reconfiguration point in time is of great importance for the overall system performance. Determination of a suitable point of reconfiguration demands the evaluation of performance degradation during runtime reconfiguration and expected performance benefit after reconfiguration. Three different approaches to determine a proper point of reconfiguration are discussed. Delays and weighted transitions are used to reduce the number of reconfigurations while keeping system performance at a maximum. Evaluation is done with a simulation model of a runtime reconfigurable network coprocessor. Results show that the number of reconfigurations can be reduced by about 35% for a given application scenario. By optimizing runtime reconfiguration decisions, the overall system performance is even higher than compared to pure threshold based reconfiguration decision schemes.	coprocessor;elegant degradation;field-programmable gate array;hardware acceleration;load profile;optimizing compiler;run time (program lifecycle phase);simulation;throughput	Thilo Pionteck;Steffen Sammann;Carsten Albrecht	2010	2010 IEEE/IFIP International Conference on Embedded and Ubiquitous Computing	10.1109/EUC.2010.16	embedded system;parallel computing;real-time computing;hardware acceleration;computer science;control reconfiguration;operating system;simulation modeling;computer performance;coprocessor;field-programmable gate array	EDA	-8.367693016190087	56.69758131062788	79071
366b90b6929c600d683c698c1c559579a5a08ce2	high performance mpi datatype support with user-mode memory registration: challenges, designs, and benefits	libraries;protocols;buffer storage;zero copy datatype communication high performance mpi datatype support user mode memory registration umr;data communication;receivers;data communication receivers benchmark testing bandwidth protocols buffer storage libraries;bandwidth;benchmark testing;parallel processing application program interfaces message passing	Noncontiguous data communication has been heavily adopted in scientific applications, especially for those written with MPI. Common strategies to handle noncontiguous data, like packing/unpacking, incur significant performance overhead during communication, which could become as a barrier of using MPI derived datatypes. Recently, a novel feature of Mellanox InfiniBand, called User-mode Memory Registration (UMR), has been introduced for noncontiguous data communication. UMR has the potential to support MPI derived datatype communication efficiently without the overhead of packing/unpacking. In this paper, we analyze the UMR feature and study its basic performance with InfiniBand verbs-level micro-benchmarks. With this knowledge, we propose UMR-based schemes to support zero-copy datatype communication at MPI level. We show that a naive integration of UMR with an MPI stack could not bring performance benefits over existing schemes. Thus we propose two schemes -- UMR Pool and UMR Cache -- to enable high performance MPI datatype communication with UMR. To the best of our knowledge, this is the first paper to study, analyze, and design MPI noncontiguous data communication using the UMR feature. We propose and implement UMR-based designs on top of MVAPICH2 library. The experimental results at the microbenchmark level show that the proposed UMR-based design is able to deliver 4X performance improvement in latency for large message vector benchmarks over the packing/unpacking scheme. At the application level, for a 3D stencil communication kernel with MPI derived datatype on 512 processes, the optimized UMR-based design outperforms the packing/unpacking scheme by 27% in execution time.	benchmark (computing);infiniband;message passing interface;overhead (computing);run time (program lifecycle phase);set packing;user space;zero-copy	Mingzhe Li;Hari Subramoni;Khaled Hamidouche;Xiaoyi Lu;Dhabaleswar K. Panda	2015	2015 IEEE International Conference on Cluster Computing	10.1109/CLUSTER.2015.41	communications protocol;benchmark;parallel computing;real-time computing;computer science;operating system;bandwidth	HPC	-9.757977239245607	47.317890092010046	79129
a35acd3d016f88a9a7556ffe6ead330d68715ef9	blockchain-based model for social transactions processing	load aware query routing;transaction processing;data consistency	The goal of this work in progress is to handle transactions of social applications by using their access classes. Basically, social users access simultaneously to a small piece of data owned by a user or a few ones. For instance, a new post of a Facebook user can create the reactions of most of his/her friends, and each of such reactions is related to the same data. Thus, grouping or chaining transactions that require the same access classes may reduce significantly the response time since several transactions are executed in one shot while ensuring consistency as well as minimizing the number of access to the persistent data storage. With this insight, we propose a middleware-based transaction scheduler that uses various strategies to chain transactions based on their access classes. The key novelties lie in (1) our distributed transaction scheduling devised on top of a ring to ensure communication when chaining transactions and (2) our ability to deal with multi-partitions transactions. The scheduling phase is based on Blockchain principle, which means in our context to record all transactions requiring the same access class into a master list in order to ensure consistency and to plan efficiently their processing. We designed and simulated our approach using SimJava and preliminary results show interesting and promising results.	algorithm;bitcoin;cloud computing;computer data storage;concurrency (computer science);concurrency control;distributed transaction;experiment;middleware;response time (technology);routing;scheduling (computing);transaction processing	Idrissa Sarr;Hubert Naacke;Ibrahima Gueye	2015		10.5220/0005519503090315	real-time computing;database transaction;transaction processing;distributed transaction;computer science;data mining;database;distributed computing;online transaction processing;compensating transaction;data consistency;world wide web	DB	-18.843316849632	56.5142637475759	79156
f2cf0ae16e83d397fa6c47dfb0bc123742055666	paracube: a scalable olap model based on distributed aggregate computing with sibling cubes	data volume;aggregation function;query processing;operational business intelligence;aggregates distributed computing concurrent computing data warehouses merging application software prototypes query processing acceleration material storage;distributed processing;weight distribution;distributed computing;distributed aggregate computing;data mining;median;optimized median aggregate computing algorithm;servers;query complexity;nondistributed computing aggregate functions;engines;query processing data mining data warehouses distributed processing;algebra;sibling cubes;median paracube sibling cube distributed aggregate;distributed aggregate;aggregates;asynchronous tunnel model paracube scalable olap model distributed aggregate computing sibling cubes data volume query volume query complexity data warehouse query processing nondistributed computing aggregate functions optimized median aggregate computing algorithm operational business intelligence shared nothing system multicore platforms;merging;asynchronous tunnel model;multicore platforms;business intelligence;data warehouses;data warehouse;query volume;sibling cube;paracube;scalable olap model;shared nothing system	"""The requirements of OLAP applications increase rapidly by dramatically increased data volume, users, query volume and query complexity. The requirement for shortening update period in data warehouse is another crucial factor for a scalable OLAP application. In this paper, we propose a scalable OLAP prototype to support the query processing with increasing data volume by distributing the whole fact tuples to multiple servers to construct a set of sibling cubes which can be merged together to obtain the whole cube. We employ a light weight distribution policy with fully duplicated dimension tables in each sibling server on the observation of very low proportion of space cost for dimension tables. OLAP query with distributed aggregate functions can be transformed into queries to be performed parallel in sibling servers. For non-distributed computing aggregate functions, such as median, the optimized median aggregate computing algorithm is proposed to reduce transmission volume between servers while computing the global median values. We also present a three-level framework in data warehouse to meet the requirement of shorter update period in """"operational business intelligence"""". An asynchronous tunnel model is proposed to reduce update latency by pre-fetching updated tuples to OLAP processing server. Finally, we set up prototype system ParaCube to evaluate performance in SN (shared-nothing) system and multi-core platforms."""	aggregate function;algorithm;cubes;database;decision tree model;distributed computing;multi-core processor;olap cube;online analytical processing;prototype;requirement;scalability;server (computing);shared nothing architecture	Yansong Zhang;Shan Wang;Wei Huang	2010	2010 12th International Asia-Pacific Web Conference	10.1109/APWeb.2010.31	online analytical processing;computer science;weight distribution;theoretical computer science;operating system;data warehouse;data mining;database;distributed computing;world wide web;median;server	DB	-18.918841044577782	53.96252929107194	79157
48ed0f50275f5d10cfe7fcb5d4f8863e1f717be7	efficient lists intersection by cpu-gpu cooperative computing	scheduling metric lists intersection cpu gpu cooperative computing web search engines single core cpu platform multicore cpu platform many core gpu platform synchronous mode query parallel gpu algorithm element thread mapping strategy load balancing online scheduling algorithm regression analysis;search engines coprocessors multiprocessing systems parallel algorithms query processing regression analysis resource allocation scheduling;multicore cpu platform;paper;element thread mapping strategy;query processing;cpu gpu cooperative computing;search engines;resource allocation;single core cpu platform;scheduling metric;presentation;query parallel gpu algorithm;many core gpu platform;coprocessors;tesla c1060;cuda;synchronous mode;lists intersection;scheduling;multicore processing;load management;load balancing;nvidia;web search;regression analysis;multiprocessing systems;data structures and algorithms;computer science;graphics processing unit;search engines delay graphics processing unit central processing unit query processing web search multicore processing algorithm design and analysis load management;online scheduling algorithm;web search engines;algorithm design and analysis;central processing unit;parallel algorithms	Lists intersection is an important operation in modern web search engines. Many prior studies have focused on the single-core or multi-core CPU platform or many-core GPU. In this paper, we propose a CPU-GPU cooperative model that can integrate the computing power of CPU and GPU to perform lists intersection more efficiently. In the so-called synchronous mode, queries are grouped into batches and processed by GPU for high throughput. We design a query-parallel GPU algorithm based on an element-thread mapping strategy for load balancing. In the traditional asynchronous model, queries are processed one-by-one by CPU or GPU to gain perfect response time. We design an online scheduling algorithm to determine whether CPU or GPU processes the query faster. Regression analysis on a huge number of experimental results concludes a regression formula as the scheduling metric. We perform exhaustive experiments on our new approaches. Experimental results on the TREC Gov and Baidu datasets show that our approaches can improve the performance of the lists intersection significantly.	algorithm;central processing unit;experiment;graphics processing unit;load balancing (computing);manycore processor;multi-core processor;response time (technology);scheduling (computing);single-core;text retrieval conference;throughput;web search engine	Di Wu;Fan Zhang;Naiyong Ao;Gang Wang;Xiaoguang Liu;Jing Liu	2010	2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)	10.1109/IPDPSW.2010.5470886	parallel computing;real-time computing;computer science;theoretical computer science	Embedded	-14.494552785937314	56.636651611413555	79345
4a8ea9785ca416804c407bd232534c16eeb1ecee	a memory controller with row buffer locality awareness for hybrid memory systems		This paper summarizes the idea and key contributions of the Dynamic Row Bu er Locality Aware Memory Controller (RBLA), which was published in ICCD 2012 [125], and examines the work’s signi cance and future potential. Non-volatile memory (NVM) is a class of promising scalable memory technologies that can potentially o er higher capacity than DRAM at the same cost point. Unfortunately, the access latency and energy of NVM is often higher than those of DRAM, while the endurance of NVM is lower. Many DRAM–NVM hybrid memory systems, also known as heterogeneous memory systems, use DRAM as a cache to NVM, to achieve the low access latency, low energy, and high endurance of DRAM, while taking advantage of the large capacity of NVM. A key question for a hybrid memory system is what data to cache in DRAM to best exploit the advantages of each technology while avoiding the disadvantages of each technology as much as possible.	cache (computing);charge-coupled device;dynamic random-access memory;fairness measure;locality of reference;memory controller;non-volatile memory;scalability;volatile memory	HanBin Yoon;Justin Meza;Rachata Ausavarungnirun;Rachael A. Harding;Onur Mutlu	2018	CoRR		parallel computing;latency (engineering);scalability;energy consumption;efficient energy use;cache;row;dram;computer science;memory controller	Arch	-8.880888849788976	54.132741218778904	79401
19fd54f6e599a0136e9f20ab11123d016c49cb1b	fblt: a real-time contention manager with improved schedulability	real-time contention manager;previous best contention manager;novel contention manager;transactional conflict;software transactional memory;improved schedulability;embedded multicore real-time software;concurrency control;competitor method;real-time schedulability;rochester stm framework;upper bound transactional retries;real time systems;multicore processing;synchronization;electronic countermeasures	We consider software transactional memory (STM) concurrency control for embedded multicore real-time software, and present a novel contention manager for resolving transactional conflicts, called FBLT. We upper bound transactional retries and task response times under FBLT, and identify when FBLT has better real-time schedulability than the previous best contention manager, PNF. Our implementation in the Rochester STM framework reveals that FBLT yields shorter or comparable retry costs than competitor methods.	concurrency (computer science);concurrency control;data structure;embedded software;embedded system;fifo (computing and electronics);job stream;latent class model;linux;maurice herlihy;multi-core processor;multiprocessing;non-functional requirement;overhead (computing);podc;preemption (computing);prenex normal form;real-time clock;real-time transcription;reliability-centered maintenance;retry;scheduling (computing);software transactional memory	Mohammed El-Shambakey;Binoy Ravindran	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;transactional memory;parallel computing;real-time computing;computer science;operating system	Embedded	-10.496588812985097	58.28492781266044	79648
19de90c933c20849c85d5428c8a643210b97ec83	recency-based tlb preloading	hardware software interaction;paged storage;energy simulator;caching;system energy;high performance computing;translation lookaside buffers;resource allocation;prefetching;prediction algorithms;translation lookaside buffer;maintenance engineering;latency tolerance;resource allocation paged storage;system performance;latency tolerating techniques;general purpose processor;translation lookaside buffers recency based tlb preloading caching latency tolerating techniques;low power architectures;mobile communication;sun;mobile handsets;energy optimization and estimation;memory systems;prefetching delay sun laboratories mobile communication mobile handsets high performance computing maintenance engineering system performance prediction algorithms;recency based tlb preloading;compiler optimizations	Caching and other latency tolerating techniques have been quite successful in maintaining high memory system performance for general purpose processors. However, TLB misses have become a serious bottleneck as working sets are growing beyond the capacity of TLBs. This work presents one of the first attempts to hide TLB miss latency by using preloading techniques. We present results for traditional next-page TLB miss preloading - an approach shown to cut some of the misses. However, a key contribution of this work is a novel TLB miss prediction algorithm based on the concept of “recency”, and we show that it can predict over 55% of the TLB misses for the five commercial applications considered.	algorithm;central processing unit;high memory;translation lookaside buffer	Ashley Saulsbury;Fredrik Dahlgren;Per Stenström	2000		10.1145/339647.339666	maintenance engineering;embedded system;computer architecture;parallel computing;real-time computing;mobile telephony;prediction;resource allocation;computer science;operating system;translation lookaside buffer;optimizing compiler	Arch	-8.750540326546737	54.35871742185848	79683
c27ad82f083722fa40fd4342e65624bb1f098b49	a lookahead read cache: improving read performance for deduplication backup storage	read cache;deduplication;dedupe;backup	Data deduplication (dedupe for short) is a special data compression technique. It has been widely adopted to save backup time as well as storage space, particularly in backup storage systems. Therefore, most dedupe research has primarily focused on improving dedupe write performance. However, backup storage dedupe read performance is also a crucial problem for storage recovery. This paper designs a new dedupe storage read cache for backup applications that improves read performance by exploiting a special characteristic: the read sequence is the same as the write sequence. Consequently, for better cache utilization, by looking ahead for future references within a moving window, it evicts victims from the cache having the smallest future access. Moreover, to further improve read cache performance, it maintains a small log buffer to judiciously cache future access data chunks. Extensive experiments with real-world backup workloads demonstrate that the proposed read cache scheme improves read performance by up to 64.3%	backup;cpu cache;data compression;data deduplication;experiment;identification scheme;parsing;reference counting;write buffer	Dongchul Park;Ziqi Fan;Youngjin Nam;David Hung-Chang Du	2017	Journal of Computer Science and Technology	10.1007/s11390-017-1680-8	parallel computing;data deduplication;computer science;operating system;database;cache algorithms	OS	-12.174168318693713	54.60310443216252	79843
c0c22554871572c9e65d3f28bb723f6523e64c04	traversal time for weakly synchronized can bus	can bus;network calculus;worst case traversal times	Scheduling frames with offsets has been shown in the literature to be very beneficial for reducing response times in realtime networks because it allows the workload to be better spread over time and thus to reduce peaks of load. Maintaining a global synchronization amongst the stations induces substantial overhead and complexity in networks not providing a global time service such as CAN. Indeed, on CAN, no global clock is implemented in practice and each station possesses its own local clock. Without a global clock, the de-synchronization between the streams of frames created by offsets remains local to each station. The first contribution of this work is to show that important gains with respect to the communication latencies, around 40% in our experiments, can be achieved if we implement bounded clock desynchronization, also refered to as bounded phases, between the stations. The second contribution of this work is to provide a set of network-calculus based timing analyses to handle systems with bounded phases and compare their performances.	can bus;experiment;network calculus;overhead (computing);performance;scheduling (computing);synchronization (computer science);tcp global synchronization	Hugo Daigmorte;Marc Boyer	2016		10.1145/2997465.2997477	embedded system;real-time computing;computer science;distributed computing	Embedded	-10.550496329615138	58.43883161895727	79859
1ec1daae1669f8f9d7e466bcd2a2ca1ab735c965	rams: dram rank-aware memory scheduling for energy saving	random access memory;memory management;memory request scheduling dram power and energy dram rank state cache replacement policy;dram power and energy;system performance;energy consumption;scheduling;cache replacement policy;random access memory energy consumption memory management power demand system performance handheld computers;dram rank state;power demand;power consumption cache storage dram chips;memory request scheduling;handheld computers;standby power consumption rams write traffic memory controller late level cache victim blocks power states prioritized cache block replacement method dram rank aware memory scheduling schemes time penalty state transitions idle energy consumption low power modes	DRAMs are one of the main players of the computer system energy consumption. Thus, reducing DRAM energy consumption has a big potential to save the entire system energy consumption. Because the standby power consumption of DRAM is significant, modern DRAMs provide low-power modes for reducing idle energy consumption. However, the use of low-power modes can degrade the performance because state transitions to/from low-power states involve a time penalty. To effectively utilize low-power modes, we propose DRAM rank-aware memory scheduling schemes. One scheme utilizes a prioritized cache block replacement method considering the power states of DRAM ranks to select victim blocks for the late level cache. Through this scheme, DRAM traffic and the number of state transitions of DRAM ranks can be reduced. The other scheme utilizes the memory controller by controlling write traffic to DRAM with the awareness of the DRAM rank states. DRAM rank idle times and state transitions can be reduced by this scheme. Our proposed schemes are shown to reduce DRAM energy consumption by 15.2 percent on average.	cpu cache;computer;dynamic random-access memory;low-power broadcasting;memory controller;parallel computing;rams;reduction (complexity);scheduling (computing);task parallelism	Yebin Lee;Soontae Kim	2016	IEEE Transactions on Computers	10.1109/TC.2016.2525994	embedded system;pipeline burst cache;parallel computing;real-time computing;memory rank;static random-access memory;cas latency;computer science;operating system;computer performance;memory controller;universal memory;registered memory;scheduling;memory management	Arch	-8.686089089391556	54.376860817393215	79985
189d77a5a00aa5883c3e31401fb54350b8cbccb8	optimal tile size selection guided by analytical models		As the memory bottleneck problem continues to grow, so does the relevance of the techniques that help improve program locality. A well-known technique in this category is tiling, which decomposes data sets to be used several times in a computation into a series of tiles that are reused before proceeding to process the next tile. This way, capacity misses are avoided. Finding the optimal tile size is a complex task. In this paper we present and compare a series of strategies to search the optimal tile size guided by an analytical model of the whole memory hierarchy and the CPU behavior. Our experiments show that our strategies find better tile sizes than traditional heuristic approaches proposed in the literature while requiring a small compile-time overhead. Iterative compilation can yield better results, but at the expense of very large overheads.	central processing unit;code;compile time;compiler;computation;experiment;feasible region;genetic algorithm;heuristic (computer science);iterative method;locality of reference;memory hierarchy;overhead (computing);relevance;tiling window manager;von neumann architecture	Basilio B. Fraguela;M. G. Carmueja;Diego Andrade	2005			parallel computing;theoretical computer science;tile;computer science;computation;locality;heuristic;bottleneck;central processing unit;memory hierarchy;overhead (business)	HPC	-5.4918531930385965	49.2416727010876	80021
4c8c33ec316ed55ff925950593a07a9e60c8e243	a three-dimensional resource scheduling algorithm for a network-aware grid	distributed algorithms;resource scheduling;data intensive application;heterogeneous grid resources;performance evaluation;grid computing environment;resource allocation;resource management;biological system modeling;scheduling distributed algorithms electronic data interchange grid computing performance evaluation resource allocation;computational resources;indexing terms;three dimensional;performance metric;data intensive applications;distributed resource allocation;indexes;mra3d;computational resources three dimensional resource scheduling algorithm network aware grid e infrastructure heterogeneous grid resources e science applications data transfers grid resource allocation distributed resource allocation algorithm mra3d multiple resource requirements grid computing environment data intensive applications performance metrics;scheduling algorithm;computational modeling;scheduling;bandwidth;multiple resource requirements;resource management computational modeling indexes bandwidth biological system modeling scheduling algorithm;distributed resource allocation algorithm;grid resource allocation;e infrastructure;network aware grid;data transfers;grid computing;performance metrics;e science applications;electronic data interchange;data transfer;three dimensional resource scheduling algorithm	An e-Infrastructure allows end-user's applications to easily and securely access heterogeneous grid resources (e.g., computing and storage elements). Since e-Science applications are often characterized by huge data transfers and high computational loads, the selection and allocation of grid resources dramatically affect their performance. This paper proposes a distributed resource allocation algorithm, referred to as MRA3D, capable of handling multiple resource requirements for jobs/tasks submitted to the grid computing environment of the e-Infrastructure. More specifically, MRA3D aims at minimizing the execution time of data-intensive applications by taking into account performance metrics describing both system and connectivity status of the computational resources. Simulations have been carried out to compare the performance of MRA3D with other resource allocation algorithms in a realistic environment by using synthetic as well as real workload traces.	algorithm;computation;computational resource;computer simulation;cyberinfrastructure;data-intensive computing;e-science;google compute engine;grid computing;memory management;network performance;requirement;run time (program lifecycle phase);scheduling (computing);synthetic intelligence;tracing (software)	Davide Adami;Christian Callegari;Stefano Giordano;Michele Pagano	2010	2010 IEEE Global Telecommunications Conference GLOBECOM 2010	10.1109/GLOCOM.2010.5683157	resource allocation;computer science;resource management;theoretical computer science;operating system;database;distributed computing;resource allocation;scheduling;grid computing	HPC	-19.07657225576085	58.95139816080624	80318
1571d88e2ac9dd171d83a8fd4c570504162e28e7	an instruction to accelerate software caches	motion compensated	In this paper we propose an instruction to accelerate software caches. While DMAs are very efficient for predictable data sets that can be fetched before they are needed, they introduce a large latency overhead for computations with unpredictable access behavior. Software caches are advantageous when the data set is not predictable but exhibits locality. However, software caches also incur a large overhead. Because the main overhead is in the access function, we propose an instruction that replaces the look-up function of the software cache. This instruction is evaluated using the Multidimensional Software Cache and two multimedia kernels, GLCM and H.264 Motion Compensation. The results show that the proposed instruction accelerates the software cache access time by a factor of 2.6. This improvement translates to a 2.1 speedup for GLCM and 1.28 for MC, when compared with the IBM software cache.	access time;cache (computing);central processing unit;computation;h.264/mpeg-4 avc;inline expansion;locality of reference;lookup table;motion compensation;overhead (computing);scratchpad memory;speedup	Arnaldo Azevedo;Ben H. H. Juurlink	2011		10.1007/978-3-642-19137-4_14	embedded system;parallel computing;real-time computing;computer science;operating system	Arch	-6.410157699733074	50.11582333150814	80332
85b0a124c6bdbcaaf2a1640f9af50d7d920c1638	towards dynamic green-sizing for database servers		This paper presents two techniques for reducing the power consumed by database servers. Both techniques are intended primarily for transactional workloads on servers with memory-resident databases. The first technique is databasemanaged dynamic processor voltage and frequency scaling (DVFS). We show that a DBMS can exploit its knowledge of the workload and performance constraints to obtain power savings that are more than twice as large as the power savings achieved when DVFS is managed by the operating system. The second technique is rank-aware memory allocation, the goal of which is to power memory that the database system needs and avoid powering memory it does not need. We present experiments that show rank-aware allocation allows unneeded memory to move to low-power states, reducing memory power consumption.	database server;dynamic frequency scaling;dynamic voltage scaling;experiment;image scaling;low-power broadcasting;operating system	Mustafa Korkmaz;Alexey Karyakin;Martin Karsten;Kenneth Salem	2015			workload;database tuning;memory management;database;real-time computing;database server;frequency scaling;server;exploit;computer science	DB	-5.863999298891316	55.42046485524933	80349
0ef24e6b91b7dceeeff8af7ad02ffba31eec4bd1	a dynamic rebuild strategy of multi-host system volume on transparence computing mode	software;two dimension local characteristic;complexity theory;transparent computation;time complexity;distributed computing dynamic system rebuild strategy multihost system volume transparence computing mode statistical analysis similar collection element two dimension local characteristics rewritten block time complexity optimistic forecast algorithm prefetching strategy i o processing;two dimensions;pervasive computing;storage management;distributed processing;prefetching;storage management computational complexity distributed processing statistical analysis;hard disks;arrays;indexes;two dimension local characteristic system volume restructuring similarity transparent computation;statistical analysis;computational complexity;similarity;software indexes arrays hard disks complexity theory prefetching pervasive computing;system volume restructuring	By statistical analysis of a few isomorphic hosts booting remotely and deployed separately from a shared system volume, a similar model is proposed on the transparence computing mode. A kind of information left on the rewritten SCE (similar collection elements) will instruct the hosts to quickly sense and rapidly position the rewritten SCE when they rebuild their system volumes. Based on the two dimension local characteristic (TDLC) of rewritten blocks, a system rebuild strategy (SRS) is proposed. The time complexity of the SRS in positioning rewritten address is O(1). An optimistic forecast algorithm and a prefetching strategy to accelerate the I/O processing are proposed. The experiment result shows that the multi-host system rebuild strategy (SRS) takes on a favorable performance and stability.	algorithm;booting;cpu cache;input/output;scalable cluster environment;time complexity	Huai-liang Tan;Yan Wang;Jian-hua Sun	2008	2008 10th IEEE International Conference on High Performance Computing and Communications	10.1109/HPCC.2008.27	time complexity;database index;two-dimensional space;parallel computing;real-time computing;similarity;computer science;theoretical computer science;operating system;distributed computing;computational complexity theory;computer security;ubiquitous computing	HPC	-16.866977174776885	54.9599202886835	80433
513b390bd3cdba6d6c9a49cce0f1c22a3431637b	deadline assignment in distributed hard real-time systems with relaxed locality constraints	real time systems processor scheduling computer science software maintenance performance analysis distributed computing application software software reusability parallel processing sensor systems;publikationer;system configuration;processor scheduling;heuristic programming;distributed processing;konferensbidrag;schedulability analysis;schedulability analysis deadline assignment distributed hard real time systems relaxed locality constraints global end to end deadlines high task schedulability component subtasks automatic distribution end to end deadlines task assignments large real time systems task assignment strategies circular dependency deadline distribution task assignment heuristic approach;heuristic programming distributed processing real time systems processor scheduling;hard real time system;artiklar;rapporter;task assignment;task scheduling;real time systems	In a real-time system, tasks are constrained by global end-to-end deadlines. In order to cater for high task schedulability, these deadlines must be distributed over component subtasks in an intelligent way. Existing methods for automatic distribution of end-to-end deadlines are all based on the assumption that task assignments are entirely known beforehand. This assumption is not necessarily valid for large real-time systems. Furthermore, most task assignment strategies require information on deadlines in order to make good assignments, thus forming a circular dependency between deadline distribution and task assignment. We present a heuristic approach that performs deadline distribution prior to task assignment. The deadline distribution problem is presented in the context of large distributed hard real-time systems with relaxed locality constraints, where schedulability analysis must be performed off-line, and only a subset of the tasks are constrained by predetermined assignments to specific processors. Using experimental results we identify drawbacks of previously-proposed techniques, and then show that our solution provides significantly better performance for a large variety of system configurations.	central processing unit;circular dependency;end-to-end principle;experiment;heuristic;locality of reference;online and offline;parallel computing;real-time clock;real-time computing;real-time locating system;real-time transcription;refinement (computing);scheduling (computing);scheduling analysis real-time systems	Jan Jonsson;Kang G. Shin	1997		10.1109/ICDCS.1997.598077	parallel computing;real-time computing;computer science;distributed computing	Embedded	-11.122616984972655	59.995108177372764	80480
2d1b2392585b09297dd79a14ca3fb853133d64e3	data movement aware computation partitioning		Data access costs dominate the execution times of most parallel applications and they are expected to be even more important in the future. To address this, recent research has focused on Near Data Processing (NDP) as a new paradigm that tries to bring computation to data, instead of bringing data to computation (which is the norm in conventional computing). This paper explores the potential of compiler support in exploiting NDP in the context of emerging many core systems. To that end, we propose a novel compiler algorithm that partitions the computations in a given loop nest into sub computations and schedules the resulting sub computations on different cores with the goal of reducing the distance-to-data on the on-chip network. An important characteristic of our approach is that it exploits NDP while taking advantage of data locality. Our experiments with 12 multi-threaded applications running on a state of-the-art commercial many core system indicate that the proposed compiler-based approach significantly reduces data movements on the on-chip network by taking advantage of NDP, and these benefits lead to an average execution time improvement of 18.4%. CCS CONCEPTS •Computer systems organization → Multi-core architectures;	algorithm;compiler;computation;data access;experiment;locality of reference;manycore processor;multi-core processor;network on a chip;programming paradigm;run time (program lifecycle phase);thread (computing)	Xulong Tang;Orhan Kislal;Mahmut T. Kandemir;Mustafa Karaköy	2017		10.1145/3123939.3123954	compiler;parallel computing;real-time computing;locality;computation;computer science;exploit;data access;schedule;data processing	Arch	-5.5390564631133214	49.60007313827029	80539
1ec7b2aeaf844632e3642d48f7a1cb6f8482614a	reducing data copies between gpus and nics	protocols;kernel;buffer storage;graphics processing units data communication kernel protocols buffer storage educational institutions data models;gpu;packet loss data copies nic cyber physical systems cps complex algorithms complex real world phenomena parallel processing data parallelism gpu memory i o devices zero copy processing data transmission transfer data;data communication;low latency gpu gpgpu data transfer;gpgpu;low latency;graphics processing units;parallel processing electronic data interchange graphics processing units;data transfer;data models	Cyber-physical systems (CPS) must perform complex algorithms at very high speed to monitor and control complex real-world phenomena. GPU, with a large number of cores and extremely high parallel processing, promises better computation if the data parallelism often found in real-world scenarios of CPS could be exploited. Nevertheless, its performance is limited by the latency incurred when data are transferred between GPU memory and I/O devices. This paper describes a method, based on zero-copy processing, for data transmission between GPUs and NICs. The arrangement enables NICs to directly transfer data to and from GPU. Experimental results show effective data throughput without packet loss.	algorithm;computation;computer memory;cyber-physical system;data parallelism;experiment;graphics processing unit;input/output;ioctl;network interface controller;network packet;parallel computing;protocol stack;random-access memory;stack (abstract data type);tcp offload engine;throughput;zero-copy	Anh Tuan Le Nguyen;Yusuke Fujii;Yuki Iida;Takuya Azumi;Nobuhiko Nishio;Shinpei Kato	2014	2014 IEEE International Conference on Cyber-Physical Systems, Networks, and Applications	10.1109/CPSNA.2014.15	embedded system;data modeling;communications protocol;parallel computing;kernel;computer hardware;computer science;operating system;general-purpose computing on graphics processing units;low latency	HPC	-5.266474679567191	46.61536229309482	80652
da72cfa8d0d8f10f0350fc084176fc6d27284958	the potential of thread-level speculation based on value profiling	thread level speculation;thread level parallelism;predictive value;parallelizing compilers	As processors continue to provide more and more parallelism, compilers have to keep up to identify operations that can execute independently, e.g. by exploiting program properties like independent loop iterationsor independent operations. Lately, various researchers have identified value localityas a potentially interesting program property: many operations produce predictable values for each execution of a program. We are interested in taking advantage of this property when compiling for a processor with thread-level parallelism. The key idea is to use predictable values to relax dependence constraints: either possible memory conflicts that rarely occur or true register dependences whose values are highly predictable. In this paper we describe our approach and present preliminary results for a partitioner that uses value profiling to group the operations of a loop body into independent threads for parallel execution.	central processing unit;compiler;parallel computing;speculative multithreading;task parallelism	Anthony DeWitt;Thomas R. Gross	1999	SIGARCH Computer Architecture News	10.1145/309758.309765	computer architecture;parallel computing;real-time computing;computer science;task parallelism	Arch	-7.74740645928273	48.528775301503536	80672
3067b0ccb22fdfcfab445e20971b12e773a4547d	i/o aware power shifting	performance hpc power;i o aware power shifting phase frequency fixed power bound high performance computing;fixed power bound;mathematics;high performance computing;performance;delays heuristic algorithms algorithm design and analysis schedules clustering algorithms data visualization;i o aware power shifting;computing;power aware computing;hpc;heuristic algorithms;phase frequency;data visualization;schedules;clustering algorithms;and information science;power aware computing parallel processing;article;power;algorithm design and analysis;parallel processing;delays	Power limits on future high-performance computing (HPC) systems will constrain applications. However, HPC applications do not consume constant power over their lifetimes. Thus, applications assigned a fixed power bound may be forced to slow down during high-power computation phases, but may not consume their full power allocation during low-power I/O phases. This paper explores algorithms that leverage application semantics -- phase frequency, duration and power needs -- to shift unused power from applications in I/O phases to applications in computation phases, thus improving system-wide performance. We design novel techniques that include explicit staggering of applications to improve power shifting. Compared to executing without power shifting, our algorithms can improve average performance by up to 8% or improve performance of a single, high-priority application by up to 32%.	algorithm;analysis of algorithms;best, worst and average case;capability-based security;computation;computer architecture simulator;input/output;java caps;job scheduler;job stream;low-power broadcasting;object lifetime;overhead (computing);scheduling (computing);supercomputer	Lee Savoie;David K. Lowenthal;Bronis R. de Supinski;Tanzima Zerin Islam;Kathryn Mohror;Barry Rountree;Martin Schulz	2016	2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)	10.1109/IPDPS.2016.15	parallel processing;computing;supercomputer;parallel computing;real-time computing;performance;computer science;theoretical computer science;operating system;power;distributed computing	Arch	-5.568200486440517	55.163690108093405	80808
4d852c27cbec98e4f1f5a612649c38f86e5f2551	dimvisual: data integration model for visualization of parallel programs behavior	concurrent computing;data integrity;high performance computing;prototypes;data collection;causal ordering;information gathering;col;operating system;monitoring;programming profession;execution environment;data visualization monitoring information analysis programming profession operating systems switches prototypes concurrent computing high performance computing context modeling;data visualization;switches;parallel programs;context modeling;high performance;information analysis;parallel applications;performance tuning;operating systems	The development of high performance parallel applications for clusters is considered a complex task. This can happen because the influence of the execution environment and the non-deterministic natural behavior of this kind of applications. In such development, the programmer uses application traces and cluster monitoring tools to register the events of the application and the underlying execution environment. Generally, the analysis of the information from each source is made independently, making the correlation of events from the application with events from the execution environment difficult. This paper presents DIMVisual, a Data Integration Model which addresses this problem by integrating information from different sources and providing a unified visualization. An implementation of this model is also presented, using as data sources traces from MPI and DECK applications, events from Ganglia and Performance Co-Pilot cluster monitoring tools and operating system context switches. The results show the information gathered by these data sources integrated and visualized together in the generic visualization tool Paj´e, allowing the programmer a more complete view of his application behavior.	ganglia;java;message passing interface;network switch;nondeterministic algorithm;operating system;programmer;prototype;tracing (software)	Lucas Mello Schnorr;Philippe Olivier Alexandre Navaux;Benhur de Oliveira Stein	2006	Sixth IEEE International Symposium on Cluster Computing and the Grid (CCGRID'06)	10.1109/CCGRID.2006.34	real-time computing;concurrent computing;network switch;computer science;operating system;data integrity;database;distributed computing;prototype;context model;data analysis;programming language;data visualization;data collection	HPC	-16.682204584531192	48.75169747222591	80945
25f41db76dee217232414cf68976274f131d0ad9	parallel fft algorithms for cache based shared memory multiprocessors	discrete fourier transforms cache memory parallel processing computer science memory architecture interference;cache memory;interference;memory architecture;computer science;discrete fourier transforms;parallel processing;shared memory multiprocessor	Shared memory multiprocessors with cache require careful consideration of cache parameters while implementing an algorithm to obtain optimal performance. In this paper, we study the implementation of some existing FFT algorithms and analyze the number of cache misses based on the problem size, number of processors, cache size, and block size. We also propose a new FFT algorithm which minimizes the number of cache misses.	algorithm;analysis of algorithms;block size (cryptography);cpu cache;central processing unit;fast fourier transform;shared memory	Akhilesh Kumar;Laxmi N. Bhuyan	1993	1993 International Conference on Parallel Processing - ICPP'93	10.1109/ICPP.1993.136	bus sniffing;uniform memory access;shared memory;pipeline burst cache;parallel processing;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;theoretical computer science;cache invalidation;operating system;interference;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;non-uniform memory access	HPC	-10.676678880965053	49.46372043059731	80997
7e5ddba54c9037ce61ac69d296df4d27f6b390fb	an enhanced relocation manager to speedup core allocation in fpga-based reconfigurable systems	storage allocation;resource allocation;reconfigurable architectures;resource management;storage allocation field programmable gate arrays reconfigurable architectures resource allocation;registers;schedules;hardware registers field programmable gate arrays tiles resource management schedules payloads;payloads;validation phase enhanced relocation manager speedup core allocation fpga based reconfigurable system self reconfigurable systems reconfiguration management functionalities configuration bitstreams reconfigurable fabric state empty space management online placement policy global problem online core allocation management runtime self reconfiguration runtime bitstreams relocation;tiles;field programmable gate arrays;hardware	Self Reconfigurable Systems are completely independent in their management, thus they have the need to internally host reconfiguration management functionalities, such as core allocation, and to store or be able to autonomously obtain configuration bit streams when needed. Within this scenario, the final system also needs to be able to autonomously perform choices relative to its internal management during computation, this requires, in particular, an internal solution for core allocation management, which includes maintaining information on the reconfigurable fabric state and being able to choose where to place new cores upon their arrival. We refer to these two aspects with the terms Empty Space Management and Online Placement Policy and to the global problem with the term Online Core Allocation Management. This paper presents a solution able to combine the online placement of cores with the runtime bit streams relocation to implement a complete solution that can be used in conjunction with the runtime self reconfiguration. The validation phase presents the results obtained in placement management and relocation, compared to the results obtained by the state of art solutions to this problem.	clock signal;computation;field-programmable gate array;fork (software development);fragmentation (computing);relocation (computing);self-reconfiguring modular robot;speedup;throughput	Marco D. Santambrogio;Fabio Cancare;Riccardo Cattaneo;S. Bhandariy;Donatella Sciuto	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.41	embedded system;payload;parallel computing;real-time computing;schedule;resource allocation;computer science;resource management;operating system;distributed computing;processor register;field-programmable gate array	EDA	-8.470804079364175	56.74469339739026	81077
22bef54fc5c31771b70de7ad8a29357bd4ad013c	approximate analysis of reader and writer access to a shared resource	operating system;waiting time	In this paper we present a queue that has two classes of customers: readers and writers. Readers access the resource concurrently and writers access the resource serially. The queue discipline is FCFS: readers must wait until all writers that arrived earlier have completed service, and vice versa. The approximation can predict both the expected waiting times for readers and writers and the capacity of the queue. The queue can be used for the analysis of operating system and software resources that can be accessed both serially and concurrently, such as shared files. We have used the queue to analyze the performance of concurrent B-tree algorithms.	algorithm;approximation;b-tree;concurrent computing;operating system;queue (abstract data type)	Theodore Johnson	1990		10.1145/98457.98517	double-ended queue;real-time computing;multilevel queue;computer science;operating system;database;distributed computing;queue management system	Metrics	-12.982805626328906	56.520717097857776	81191
21c3aa81370413f374ae08792b7b8e10fa32f0b0	a bailout protocol for mixed criticality systems	analytical models;program diagnostics;protocols;standards;measurement;automotive industry;bailout protocol;protocols standards measurement runtime interference robustness analytical models;scheduling program diagnostics;real time;run time behaviour;aerospace industry;interference;schedulability analysis;runtime;scheduling real time mixed criticality systems;mixed criticality systems;scheduling;application domains;offline slack;robustness;schedulability analysis bailout protocol mixed criticality systems run time behaviour application domains automotive industry aerospace industry offline slack static analysis;static analysis	To move mixed criticality research into industrial practice requires models whose run-time behaviour is acceptable to systems engineers. Certain aspects of current models, such as abandoning lower criticality tasks when certain situations arise, do not give the robustness required in application domains such as the automotive and aerospace industries. In this paper a new bailout protocol is developed that still guarantees high criticality tasks but minimises the negative impact on lower criticality tasks via a timely return to normal operation. We show how the bailout protocol can be integrated with existing techniques, utilising offline slack to further improve performance. Static analysis is provided for the strong schedulability guarantees, while scenario based evaluation via simulation is used to explore the effectiveness of the protocol.	application domain;criticality matrix;mixed criticality;online and offline;robustness (computer science);run time (program lifecycle phase);scheduling (computing);self-organized criticality;simulation;slack variable;static program analysis;systems engineering	Iain Bate;Alan Burns;Robert I. Davis	2015	2015 27th Euromicro Conference on Real-Time Systems	10.1109/ECRTS.2015.30	embedded system;communications protocol;real-time computing;simulation;computer science;automotive industry;operating system;interference;aerospace;scheduling;static analysis;measurement;robustness	Embedded	-8.695582661027908	60.21951453121503	81199
a3bd08680835e32dd78fcff7c7a9c65e39eb6e42	energy efficient soft real-time computing through cross-layer predictive control		The next decade of computing workloads is expected to be dominated by soft-real time applications such as multimedia and machine vision. Such workloads are characterized by transient spikes requiring over provisioning of compute servers, adversely affecting the cost, energy usage, and environmental impact of data centers. In many of these applications, although deadlines need to be met to provide QoS guarantees, other quality parameters of the application (for example, visual quality in video processing) can be tuned in conjunction with hardware parameters (for example, DVFS) to give acceptable performance under overload conditions. In this paper, we experimentally demonstrate a predictive control approach for improving overload capacity and energy efficiency by incorporating control variables from both the hardware and the application layer. Further, we illustrate the impact of the choice of multiprocessor real-time scheduling algorithms on the performance of the controller for heterogeneous workloads.	algorithm;control variable (programming);data center;dynamic voltage scaling;experiment;machine vision;multiprocessing;overhead (computing);provisioning;real-time clock;real-time computing;real-time operating system;real-time transcription;requirement;scheduling (computing);transient state;video processing	Guangyi Cao;Arun Ravindran	2014			application layer;control theory;quality of service;scheduling (computing);real-time computing;machine vision;provisioning;model predictive control;server;computer science	Embedded	-5.312697638819789	59.7026659354146	81346
b8d19609d8a253744c2e010d611090a88fb3636a	event-based performance perturbation: a case study	performance measure;perforation;parallel computer;perturbation analysis	Determining the performance behavior of parallel computations requires some form of intrusive tracing measurement. The greater the need for detailed performance data, the more intrusion the measurement will cause. Recovering actual execution performance jfrom perturbed performance measurements using eventbased perturbation analysis is the topic of this paper. We show that the measurement and subsequent analysis of synchronization operations (particularly, advance and await) can produce, in practice, accurate approximations to actual performance behavior. We use as testcases three Lawrence Livermore loops that execute as parallel DOACROSS loops on an Alliant FX/80. The results of our experiments suggest that a systematic application of performance perturbation analysis techniques will allow more detailed, accurate instrumentation than traditionally believed possible.	alliant computer systems;approximation;computation;experiment;livermore loops;perturbation theory;synchronization (computer science)	Allen D. Malony	1991		10.1145/109625.109646	mathematical optimization;parallel computing;simulation;computer science;perturbation theory	Metrics	-17.422533929659863	47.91633473683364	81410
164a6683cc486004b4b1b2cd23c2745de642bd03	vectorization past dependent branches through speculation	simd vectorization;speculative vectorization;vectorizable computation;double precision;optimizing compiler;empirical tuning;iterative optimizing compiler;automatic vectorization;single precision;vectorization past dependent branch;compiler optimization technique;speculation;compiler optimization;parallel processing;software architecture;vectors;atlas	Modern architectures increasingly rely on SIMD vectorization to improve performance for floating point intensive scientific applications. However, existing compiler optimization techniques for automatic vectorization are inhibited by the presence of unknown control flow surrounding partially vectorizable computations. In this paper, we present a new approach, speculative vectorization, which speculates past dependent branches to aggressively vectorize computational paths that are expected to be taken frequently at runtime, while simply restarting the calculation using scalar instructions when the speculation fails. We have integrated our technique in an iterative optimizing compiler and have employed empirical tuning to select the profitable paths for speculation. When applied to optimize 9 floating-point benchmarks, our optimizing compiler has achieved up to 6.8X speedup for single precision and 3.4X for double precision kernels using AVX, while vectorizing some operations considered not vectorizable by prior techniques.	advanced vector extensions;automatic vectorization;benchmark (computing);computation;control flow;double-precision floating-point format;iteration;mathematical optimization;optimizing compiler;performance tuning;run time (program lifecycle phase);simd;single-precision floating-point format;speculative execution;speedup	Majedul Haque Sujon;R. Clint Whaley;Qing Yi	2013	Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques		parallel processing;software architecture;computer architecture;speculation;parallel computing;computer science;theoretical computer science;operating system;atlas;optimizing compiler;programming language	HPC	-6.497369338104356	46.4660466187337	81533
083dc9dcfab20aa672ef6ed9c6a6bb8d6e03c970	synthesis of real-time embedded software with local and global deadlines	formal specification;processor scheduling;real time embedded software;real time;code generation;satisfiability;wireless communication;embedded systems;scheduling algorithm;real time petri nets;quasi dynamic scheduling;critical path;precedence constraint;bluetooth;software synthesis;petri nets;petri net;embedded software;dynamic scheduling	Current methods cannot synthesize real-time embedded software applications when the global deadline of a task is shorter than the total of all local deadlines along a critical path in the task. This creates unnecessary modeling limitations which directly affect the types of systems synthesizable. We propose a quasi-dynamic scheduling algorithm for simultaneously guaranteeing both local and global deadlines, while satisfying all precedence constraints among subtasks and among tasks. Through this scheduling procedure, we are able to formally synthesize real-time embedded software from a network of Real-Time Petri Nets specification. Application examples, including a driver for the Master/Slave role switch in Bluetooth wireless communication devices, are given to illustrate the feasibility of the scheduling algorithm.	algorithm;bluetooth;critical path method;embedded software;embedded system;master/slave (technology);petri net;real-time clock;real-time operating system;real-time transcription;scheduling (computing)	Pao-Ann Hsiung;Cheng-Yi Lin	2003	First IEEE/ACM/IFIP International Conference on Hardware/ Software Codesign and Systems Synthesis (IEEE Cat. No.03TH8721)	10.1145/944645.944679	parallel computing;real-time computing;computer science;distributed computing	Embedded	-7.972348948972466	59.8729494769847	81565
b8590aeb952ed31fbeb4b46d042ee47aac309174	concert-efficient runtime support for concurrent object-oriented programming languages on stock hardware	stock hardware;concurrent object-oriented programming language;concert-efficient runtime support;object oriented programming;object oriented languages;message passing;concurrent object oriented programming	Inefficient implementations of global namespaces, message passing, and thread scheduling on stock multicomputers have prevented concurrent object-oriented programming (COOP) languages from gaining widespread acceptance. Recognizing that the architectures of stock multicomputers impose a hierarchy of costs for these operations, the authors describe a runtime system which provides different versions of each primitive, exposing performance distinctions for optimization. They confirm the advantages of a cost-hierarchy based runtime system organization by showing a variation of two orders of magnitude in version costs for a CM5 implementation. Frequency measurements based on COOP application programs demonstrate that a 39% invocation cost reduction is feasible by simply selecting cheaper versions of runtime operations.	programming language	Vijay Karamcheti;Andrew A. Chien	1993		10.1109/SUPERC.1993.1263509	parallel processing;parallel computing;real-time computing;computer science;operating system;distributed computing;programming language;object-oriented programming	HPC	-14.208906432584294	46.648304978166166	81741
5382cdf5ae2a566f5a2805f319c946eb615b4514	locality-aware task scheduling and data distribution on numa systems	datorsystem;computer systems	Modern computer architectures expose an increasing number of parallel features supported by complex memory access and communication structures. Currently used task scheduling techniques perform poorly since they focus solely on balancing computation load across parallel features and remain oblivious to locality properties of support structures. We contribute with locality-aware task scheduling mechanisms which improve execution time performance on average by 44\% and 11\% respectively on two locality-sensitive architectures - the Tilera TILEPro64 manycore processor and an AMD Opteron 6172 processor based four socket SMP machine.Programmers need task performance metrics such as amount of task parallelism and task memory hierarchy utilization to analyze performance of task-based programs. However, existing tools indicate performance mainly using thread-centric metrics. Programmers therefore resort to using low-level and tedious thread-centric analysis methods to infer task performance. We contribute with tools and methods to characterize task-based OpenMP programs at the level of tasks using which programmers can quickly understand important properties of the task graph such as critical path and parallelism as well as properties of individual tasks such as instruction count and memory behavior.	locality of reference;schedule (project management);scheduling (computing)	Ananya Muddukrishna;Peter A. Jonsson;Vladimir Vlassov;Mats Brorsson	2013		10.1007/978-3-642-40698-0_12	computer architecture;parallel computing;real-time computing;computer science;operating system;programming language;task parallelism	OS	-8.20923770275373	48.92018842807008	81744
57de9d74f82fbf718c086fad10a14e32c66c4891	data structure engineering for byte-addressable non-volatile memory	memory management;crash simulation;emulation;testing;storage class memory;programming model;data structures;non volatile memory;dram;main memory	Storage Class Memory (SCM) is emerging as a viable alternative to traditional DRAM, alleviating its scalability limits, both in terms of capacity and energy consumption, while being non-volatile. Hence, SCM has the potential to become a universal memory, blurring well-known storage hierarchies. However, along with opportunities, SCM brings many challenges. In this tutorial we will dissect SCM challenges and provide an in-depth view of existing programming models that circumvent them, as well as novel data structures that stem from these models. We will also elaborate on fail-safety testing challenges -- an often overlooked, yet important topic. Finally, we will discuss SCM emulation techniques for end-to-end testing of SCM-based software components. In contrast to surveys investigating the use of SCM in database systems, this tutorial is designed as a programming guide for researchers and professionals interested in leveraging SCM in database systems.	byte;component-based software engineering;data structure;database;dynamic random-access memory;emulator;end-to-end principle;gaussian blur;non-volatile memory;scalability;universal memory;volatile memory	Ismail Oukid;Wolfgang Lehner	2017		10.1145/3035918.3054777	shared memory;emulation;interleaved memory;computer architecture;semiconductor memory;parallel computing;memory rank;non-volatile memory;data structure;memory refresh;computer hardware;computer science;computer data storage;computer memory;overlay;software testing;programming paradigm;non-volatile random-access memory;conventional memory;extended memory;flat memory model;programming language;registered memory;dram;memory map;memory management	DB	-11.758557902290418	51.9183696266196	81863
413e8ff74c109c3622d9180e36b7e8ca94184656	block cooperation: advancing lifetime of resistive memories by increasing utilization of error correcting codes		Block-level cooperation is an endurance management technique that operates on top of error correction mechanisms to extend memory lifetimes. Once an error recovery scheme fails to recover from faults in a data block, the entire physical page associated with that block is disabled and becomes unavailable to the physical address space. To reduce the page waste caused by early block failures, other blocks can be used to support the failed block, working cooperatively to keep it alive and extend the faulty page’s lifetime.  We combine the proposed technique with existing error recovery schemes, such as Error Correction Pointers (ECP) and Aegis, to increase memory lifetimes. Block cooperation is realized through metadata sharing in ECP, where one data block shares its unused metadata with another data block. When combined with Aegis, block cooperation is realized through reorganizing data layout, where blocks possessing few faults come to the aid of failed blocks, bringing them back from the dead.  Our evaluation using Monte Carlo simulation shows that block cooperation at a single level (or multiple levels) on top of ECP and Aegis, boosts memory lifetimes by 28% (37%) and 8% (14%) on average, respectively. Furthermore, using trace-driven benchmark evaluation shows that lifetime boost can reach to 68% (30%) exploiting metadata sharing (or data layout reorganization).	address space;benchmark (computing);city of heroes;error detection and correction;forward error correction;monte carlo method;physical address;program optimization;resistive touchscreen;simulation	Mohammad Khavari Tavana;Amirkoushyar Ziabari;David R. Kaeli	2018	TACO	10.1145/3243906	pointer (computer programming);parallel computing;real-time computing;metadata;error detection and correction;computer science;monte carlo method;block (data storage);resistive random-access memory;physical address;phase-change memory	Arch	-11.43613608248956	55.02099709610065	81905
0a2352aca94868327c0fab2e511dfbd6d59e66a1	edm: an endurance-aware data migration scheme for load balancing in ssd storage clusters	endurance storage cluster data migration load balancing solid state drive ssd;cluster wide aggregate erase count edm endurance aware data migration scheme load balancing ssd storage clusters performance improvement nand flash based ssd hdd storage clusters write amplification data placement performance degradation hdd based migration techniques;resource allocation data handling flash memories performance evaluation;data migration;equations mathematical model data models ash throughput time factors servers;solid state drive ssd;load balancing;storage cluster;endurance	Data migration schemes are critical to balance the load in storage clusters for performance improvement. However, as NAND flash based SSDs are widely deployed in storage systems, extending the lifespan of SSD storage clusters becomes a new challenge for data migration. Prior approaches designed for HDD storage clusters, however, are inefficient due to excessive write amplification during data migration, which significantly decrease the lifespan of SSD storage clusters. To overcome this problem, we propose EDM, an endurance aware data migration scheme with careful data placement and movement to minimize the data migrated, so as to limit the worn-out of SSDs while improving the performance. Based on the observation that performance degradation is dominated by the wear speed of an SSD, which is affected by both the storage utilization and the write intensity, two complementary data migration policies are designed to explore the trade-offs among throughput, response time during migration, and lifetime of SSD storage clusters. Moreover, we design an SSD wear model and quantitatively calculate the amount of data migrated as well as the sources and destinations of the migration, so as to reduce the write amplification caused by migration. Results on a real storage cluster using real-world traces show that EDM performs favorably versus existing HDD based migration techniques, reducing cluster-wide aggregate erase count by up to 40%. In the meantime, it improves the performance by 25% on average compared to the baseline system which achieves almost the same effectiveness of performance improvement as previous migration techniques.	aggregate data;amplifier;baseline (configuration management);computer cluster;computer data storage;elegant degradation;flash memory;garbage collection (computer science);hard disk drive;input/output;load balancing (computing);response time (technology);solid-state drive;throughput;tracing (software)	Jiaxin Ou;Jiwu Shu;Youyou Lu;Letian Yi;Wei Wang	2014	2014 IEEE 28th International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2014.86	parallel computing;real-time computing;data migration;computer hardware;computer science;load balancing;operating system	HPC	-12.412895738402378	54.17772445555556	81916
fb515699d21f0b2381c830850f3b70b5357e9512	popularity-aware differentiated distributed stream processing on skewed streams		Real-world stream data with skewed distribution raises unique challenges to distributed stream processing systems. Existing stream workload partitioning schemes usually use a “one size fits all” design, which leverage either a shuffle grouping or a key grouping strategy for partitioning the stream workloads among multiple processing units, leading to notable problems of unsatisfied system throughput and processing latency. In this paper, we show that the key grouping based schemes result in serious load imbalance and low computation efficiency in the presence of data skewness while the shuffle grouping schemes are not scalable in terms of memory space. We argue that the key to efficient stream scheduling is the popularity of the stream data. We propose and implement a differentiated distributed stream processing system, call DStream, which assigns the popular keys using shuffle grouping while assigns unpopular ones using key grouping. We design a novel efficient and light-weighted probabilistic counting scheme for identifying the current hot keys in dynamic real-time streams. Two factors contribute to the power of this design: 1) the probabilistic counting scheme is extremely computation and memory efficient, so that it can be well integrated in processing instances in the system; 2) the scheme can adapt to the popularity changes in the dynamic stream processing environment. We implement the DStream system on top of Apache Storm. Experiment results using large-scale traces from real-world systems show that DStream achieves a 2.3× improvement in terms of processing throughput and reduces the processing latency by 64% compared to state-of-the-art designs.	apache storm;computation;dspace;fits;kerrison predictor;keyboard shortcut;multiprocessing;real-time clock;scalability;scheduling (computing);stream cipher;stream processing;system call;throughput;tracing (software);world-system;xslt/muenchian grouping	Hanhua Chen;Fan Zhang;Hai Jin	2017	2017 IEEE 25th International Conference on Network Protocols (ICNP)	10.1109/ICNP.2017.8117551	latency (engineering);streams;throughput;stream processing;workload;scheduling (computing);scalability;computer science;distributed computing;distributed database	DB	-14.305381935207995	55.20601638303554	81976
d1628841fe19a56a397b6214417b59040afc58f1	spac: a synergistic prefetcher aggressiveness controller for multi-core systems	prefetching measurement optimized production technology multicore processing interference random access memory system performance;cache storage;random access memory;optimized production technology;fairness;cache;measurement;prefetching;interference;system performance;index terms prefetching;prefetcher aggressiveness controller;fairness prefetching cache dram prefetcher aggressiveness controller;multicore processing;dram	In multi-core systems, prefetch requests of one core interfere with the demand and prefetch requests of other cores at the shared resources, which causes prefetcher-caused interference. Prefetcher aggressiveness controllers play an important role in minimizing the prefetcher-caused interference. State-of-the-art controllers such as hierarchical prefetcher aggressiveness control (HPAC) select appropriate throttling levels that can lead to improvement in system performance. However, HPAC does not consider the interactions between the throttling decisions of multiple prefetchers, and loses opportunity to improve system performance further. For multi-core systems, state-of-the-art prefetcher aggressiveness controllers controls the aggressiveness based on prefetch metrics such as accuracy, bandwidth consumption and cache pollution. We propose a synergistic prefetcher aggressiveness controller (SPAC), which explores the interactions between the throttling decisions of prefetchers, and throttles the prefetchers based on the improvement in fair-speedup of multi-core systems.	cache pollution;fairness measure;forward secrecy;interaction;interference (communication);memory controller;multi-core processor;multiprocessing;overhead (computing);prefetcher;speedup;synergy	Biswabandan Panda	2016	IEEE Transactions on Computers	10.1109/TC.2016.2547392	multi-core processor;embedded system;parallel computing;real-time computing;cache;computer science;operating system;interference;computer performance;dram;measurement	HPC	-9.424499159175909	52.6585355585623	81992
790035bf97c20c717f5499697ea8bac923c16e1d	evaluating energy-efficiency of dram channel interleaving schemes for multithreaded programs			dynamic random-access memory;forward error correction;thread (computing)	Satoshi Imamura;Yuichiro Yasui;Koji Inoue;Takatsugu Ono;Hiroshi Sasaki;Katsuki Fujisawa	2018	IEICE Transactions		parallel computing;artificial intelligence;computer vision;interleaving;computer science;efficient energy use;communication channel;dram	Arch	-5.999900175235566	53.984435861849946	82082
40f0d4daf946a8262a4bfd1d573336527e5f7865	a static analysis of i/o characteristics of scientific applications in a production workload	data communication;input-output programs;performance evaluation;software performance evaluation;i/o considerations;san diego supercomputer center;high performance computers;production workload;scientific applications	Past research on high performance computers for scientific applications has concentrated on CPU speed and exploitation of parallelism, but has, until very recently, neglected I/O considerations. This paper presents a study of the production workload at the San Diego Supercomputer Center from an I/O requirements and characteristics perspective. Results of the analyses support the hypothesis that a significant proportion of I/O intensive, long running, frequently executed scientific applications have predictable I/O requirements.	input/output;static program analysis	Barbara K. Pasquale;George C. Polyzos	1993		10.1109/SUPERC.1993.1263486	computational science;parallel computing;performance;computer science;operating system;computer graphics;static analysis;regression analysis;data transmission	HPC	-8.803866150331187	47.3019643334258	82322
f19c3aa4889d80111a8cd1f23e9a6ecc0bb25a5f	on the effective parallel programming of multi-core processors	design model;multi core processor;tool support;parallel programming;programming model;feedback loop;performance analysis;power consumption;parallel programs;multi core processors	1. In the end, the only differences that will last between different families of multi-core architectures are granularity and size; everything else will be hidden by software layers. [this thesis] 2. Data-intensive applications are not multi-core friendly. Paradoxically, the increasing computation:communication ratio of these processors makes more and more applications data-intensive. [this thesis] 3. The Cell/B.E. started the multi-core revolution, but also exposed the programmability gap. [this thesis] 4. An application specification is complete only if it contains information about both performance requirements and I/O data sets. [this thesis] 5. Multi-core low-level optimizations are, like any other form of hacking, addictive. 6. Performance is in the eye of the beholder. 7. The use of generic programming models for specialized applications and architectures is a form of procrastination. 8. Deadlines limit both the number and the quality of scientific publications. 9. Any CS/CE PhD student should spend at least a few months as intern in a relevant industrial environment. 10. Positive discrimination is not a solution to increase diversity, but a necessary (and nonsustainable) patch. 11.Working with parallel applications/architectures has a disturbing influence on daily life behavior.	cell (microprocessor);central processing unit;computation;data-intensive computing;eye of the beholder;generic programming;high- and low-level;input/output;multi-core processor;parallel computing;requirement;scientific literature	Ana Lucia Varbanescu	2010			real-time computing;reactive programming;functional reactive programming;computer science;theoretical computer science;distributed computing;programming paradigm;procedural programming;inductive programming;parallel programming model	HPC	-7.741674500435827	47.0755660715951	82661
c29d77f44b2fb55e3c91bb3d2d49e4139d2b4d2a	efficient trace file compression design with locality and address difference	on the fly decompression;lossless coding;difference;trace compression;trace driven simulation	Trace-driven simulation is a simple, fast, and convenient approach to simulate computer architecture for power consumption, throughput, CPU time, and other factors. However, trace-driven simulation requires a massive storage space to save the trace files of benchmark programs. Therefore, an important task is how to design a compression method that reduces the storage space of trace files efficiently. In addition to the compression method, on-the-fly decompression is an important approach to decrease the time of running simulations. Rather than providing the compression method and being absent from the on-the-fly decompression, this paper proposes a novel compression method with a high trace file compression ratio, and provides on-the-fly decompression. To obtain higher compression ratios for compressing trace files compared to previous works, this study proposes a dynamic reference table that accounts for the locality of executed programs to compress the non-sequential addresses in a trace file. In addition, to use compressed trace files easily, this study also proposes an on-the-fly decompression method to decrease the duration of running simulations and efficiently reduce the time for decompressing compressed trace files. The simulation results showed that the proposed method yielded superior compression ratios compared to previous works, including PDATS, PDI, LBTC, and SBC. Conversely, the sizes of the compressed trace files using our proposed method are 58%, 8%, 11%, and 57% of those with PDATS, PDI, LBTC, and SBC, respectively. In addition, the proposed method showed improved compression time and decompression time than PDI, LBTC, and SBC, which considers both addresses and instructions.	benchmark (computing);central processing unit;computer architecture;data compression;locality of reference;lookup table;portable database image;simulation;throughput	Ching-Wen Chen;Chang-Jung Ku;Tzong-Jye Liu	2013	J. Inf. Sci. Eng.		embedded system;computer vision;data compression ratio;real-time computing;computer hardware;computer science;operating system;color difference	Arch	-6.641851452563108	50.24216979762209	82959
e9d7971d295d03db0fb8964e1db5b978ed0099ad	processing transactions on grip, a parallel graph reducer	transaction management	The GRIP architecture allows eecient execution of functional programs on a multi-processor built from standard hardware components. State-of-the-art compilation techniques are combined with sophisticated runtime resource-control to give good parallel performance. This paper reports the results of running GRIP on an application which is apparently unsuited to the basic functional model: a database transaction manager incorporating updates as well as lookup transactions. The results obtained show good relative speedups for GRIP, with real performance advantages over the same application executing on sequential machines.	compiler;database transaction;function model;lookup table;multiprocessing;transaction processing	Gert Akerholt;Kevin Hammond;Simon L. Peyton Jones;Philip W. Trinder	1993		10.1007/3-540-56891-3_51	parallel computing;real-time computing;computer science;distributed computing;transaction processing system	DB	-14.388490728206998	46.85345732246886	82969
7b0f31b1315aae4209d262438a8d83023fbebfcf	exploiting model independence for parallel pcs network simulation	channel allocation;digital simulation;parallel processing;personal communication networks;software performance evaluation;telecommunication computing;workstation clusters;pentium workstation cluster;swimnet;blocked calls;call arrival rates;channel allocation simulation;conservative simulation;event precomputation;experiments;mobile host densities;model independence;optimistic simulation;parallel pcs network simulation;parallel simulator;personal communication services	In this paper, we present a parallel simulator (SWiMNet) for PCS networks using a combination of optimistic and conservative paradigms. The proposed methodology exploits event precomputation permitted by model independence within the PCS components. The low percentage of blocked calls is exploited in the channel allocation simulation of precomputed events by means of an optimistic approach. %To illustrate and verify the developed approach, Experiments were conducted with various call arrival rates and mobile host densities on a cluster of Pentium workstations. Performance results indicate that the SWiMNet achieves a speedup of 6 employing 8 workstations, and a speedup of 12 with 16 workstations.	precomputation;simulation;speedup;workstation	Azzedine Boukerche;Sajal K. Das;Alessandro Fabbri;Oktay Yildiz	1999			real-time computing;computer science;theoretical computer science;distributed computing	Metrics	-15.33627678775106	58.326541105262166	83131
4a525d16b38ea4c2dfc66a81e2a254e31fd87f5a	managing leakage energy in cache hierarchies	cache memory;spectrum;chip;energy optimization;threshold voltage;high performance;energy management;embedded device	Energy management is important for a spectrum of systems ranging from high-performance architectures to low-end mobile and embedded devices. With the increasing number of transistors, smaller feature sizes, lower supply and threshold voltages, the focus on energy optimization is shifting from dynamic to leakage energy. In fact, leakage energy is projected to become the dominant portion of the chip power budget for 0.10 micron technology and below. Leakage energy is of particular concern in dense cache memories that form a major portion of the transistor budget. In this work, we present several architectural techniques that exploit the data duplication across the different levels of cache hierarchy. Specifically, we employ both state-preserving (data-retaining) and state-destroying leakage control mechanisms to L2 subblocks when their data also exist in L1. Using a set of MediaBench and SPEC CINT2000 benchmarks, we demonstrate the effectiveness of the proposed techniques through cycle-accurate simulation. We also compare our schemes with the previously proposed cache decay policy. This comparison indicates that one of our schemes generates competitive results with cache decay. Furthermore, we show how both techniques can be applied in conjunction to provide additional energy gains.	algorithm;cpu cache;compiler;control system;data deduplication;data item;embedded system;experiment;lazy evaluation;mathematical optimization;memory cell (binary);memory hierarchy;mobile device;run time (program lifecycle phase);specint;simulation;soft error;spectral leakage;speculative execution;transistor	Lin Li;Ismail Kadayif;Yuh-Fang Tsai;Narayanan Vijaykrishnan;Mahmut T. Kandemir;Mary Jane Irwin;Anand Sivasubramaniam	2003	J. Instruction-Level Parallelism		chip;embedded system;spectrum;parallel computing;real-time computing;cache coloring;cpu cache;telecommunications;computer science;operating system;threshold voltage;cache algorithms;cache pollution;energy management	Arch	-6.031801402357658	55.168470105521834	83274
9b725d184227ff32188179ed4c6eb059567a4a08	i/o optimization in the checkpointing of openmp parallel applications	software fault tolerance optimisation parallel processing shared memory systems;checkpointing;checkpointing instruction sets optimization runtime instruments fault tolerance fault tolerant systems;checkpointing openmp fault tolerance;fault tolerance;openmp nas parallel benchmarks i o optimization openmp parallel applications checkpointing shared memory systems fault tolerance support shared memory applications fault tolerance techniques storage resources network utilization shared memory parallel applications;openmp	Despite the increasing popularity of shared-memory systems, there is a lack of tools for providing fault tolerance support to shared-memory applications. Check pointing is one of the most popular fault tolerance techniques. However, check pointing cost in terms of computing time, network utilization or storage resources can be a limitation for its practical use. This work proposes different techniques for the optimization of the I/O cost in the check pointing of shared-memory parallel applications. The proposals are extensively evaluated using the OpenMP NAS Parallel Benchmarks. Results show a significant decrease of the check pointing overhead.	application checkpointing;fault tolerance;input/output;mathematical optimization;nas parallel benchmarks;openmp;overhead (computing);shared memory	Nuria Losada;María J. Martín;Gabriel Rodríguez;Patricia González	2015	2015 23rd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing	10.1109/PDP.2015.39	fault tolerance;computer architecture;parallel computing;computer science;operating system;distributed computing;software fault tolerance	HPC	-11.317006563525704	49.13007591229463	83344
aed356d56c8586e0cfd3d288daa2808971ad453b	wcet-aware task assignment and cache partitioning for wcrt minimization on multi-core systems	wcrt task assignment cache partitioning wcet;interference;cache partitioning;wcet;time factors;wcet aware task mapping multicore system wcet aware task assignment cache partitioning wcrt minimization worst case response time task assignment integer linear programming formulation ilp formulation;multicore processing;mathematical model;wcrt;task assignment;multicore processing mathematical model real time systems partitioning algorithms time factors interference;multiprocessing systems cache storage integer programming linear programming;partitioning algorithms;real time systems	In this paper, we investigate the problem of task assignment with cache partitioning (CP) for minimizing the Worst-Case Response Time (WCRT) of system. Task assignment and cache partitioning are interdependent on each other in term of WCRT of system. Therefore, they should be jointly considered to optimize the WCRT of the system. We propose an integer-linear programming (ILP) formulation to achieve this objective. Experimental evaluation shows that WCET-aware task mapping and cache partitioning based on ILP can reduce the WCRT of system by 7.6% for independent task set and by 15.4% for dependent task set on average compared with the traditional approach.	cpu cache;integer programming;interdependence;linear programming;multi-core processor;responsiveness;worst-case execution time	Zhi-Hua Gan;Zhi-Min Gu	2015	2015 Seventh International Symposium on Parallel Architectures, Algorithms and Programming (PAAP)	10.1109/PAAP.2015.36	computer architecture;parallel computing;real-time computing;computer science	Embedded	-6.207562646397526	58.53002080392432	83499
44a64e86355992bba7dee5ad0b95dcbbfc8f16db	pre-bud: prefetching for energy-efficient parallel i/o systems with buffer disks	energy conservation;energy efficient;prefetching;energy dissipation;buffer disks;cost effectiveness;parallel i o;dynamic power management;high performance;state transition;energy saving	A critical problem with parallel I/O systems is the fact that disks consume a significant amount of energy. To design economically attractive and environmentally friendly parallel I/O systems, we propose an energy-aware prefetching strategy (PRE-BUD) for parallel I/O systems with disk buffers. We introduce a new architecture that provides significant energy savings for parallel I/O systems using buffer disks while maintaining high performance. There are two buffer disk configurations: (1) adding an extra buffer disk to accommodate prefetched data, and (2) utilizing an existing disk as the buffer disk. PRE-BUD is not only able to reduce the number of power-state transitions, but also to increase the length and number of standby periods. As such, PRE-BUD conserves energy by keeping data disks in the standby state for increased periods of time. Compared with the first prefetching configuration, the second configuration lowers the capacity of the parallel disk system. However, the second configuration is more cost-effective and energy-efficient than the first one. Finally, we quantitatively compare PRE-BUD with both disk configurations against three existing strategies. Empirical results show that PRE-BUD is able to reduce energy dissipation in parallel disk systems by up to 50 percent when compared against a non-energy aware approach. Similarly, our strategy is capable of conserving up to 30 percent energy when compared to the dynamic power management technique.	buffer overflow;cpu cache;family computer disk system;floppy disk;input/output;link prefetching;parallel i/o;power management;pre-boot authentication	Adam Manzanares;Xiao Qin;Xiaojun Ruan;Shu Yin	2011	TOS	10.1145/1970343.1970346	parallel computing;real-time computing;cost-effectiveness analysis;energy conservation;computer hardware;computer science;dissipation;efficient energy use	HPC	-9.070012537689564	54.82421089595738	83625
10a6563a5420d8ac6ed6a0aeb5bc10c66267011c	scheduler simulation using ispd, an iconic-based computer grid simulator	grid model;performance evaluation;computer software selection and evaluation;resource allocation;scheduling policy grid simulation grid computing;scheduling policies;computer systems;user support;simulators;scheduling digital simulation grid computing resource allocation;grid simulations;simulated environment;scheduling algorithms;high performance computing resources;scheduling;parallel and distributed systems;computational modeling unified modeling language servers load modeling engines processor scheduling accuracy;trabalho apresentado em evento;computer grid;simgrid simulation scheduler simulation ispd tool iconic based computer grid simulator high performance computing resource resource accessibility user support performance evaluation tool iconic simulator for parallel and distributed systems distributed environment grid model allocation algorithm scheduling algorithm scheduling policy scheduler manager;grid computing;scheduling policy;grid simulation;digital simulation;distributed environments;managers	Increased accessibility to high-performance computing resources has created a demand for user support through performance evaluation tools like the iSPD (iconic Simulator for Parallel and Distributed systems), a simulator based on iconic modelling for distributed environments such as computer grids. It was developed to make it easier for general users to create their grid models, including allocation and scheduling algorithms. This paper describes how schedulers are managed by iSPD and how users can easily adopt the scheduling policy that improves the system being simulated. A thorough description of iSPD is given, detailing its scheduler manager. Some comparisons between iSPD and Simgrid simulations, including runs of the simulated environment in a real cluster, are also presented.	accessibility;algorithm;built-in self-test;distributed computing;grid computing;international symposium on physical design;level of detail;performance evaluation;scheduling (computing);simgrid;simulation;supercomputer;testbed;virtual reality	Denison Menezes;Aleardo Manacero;Renata Spolon Lobato;Diogo T. da Silva;Roberta Spolon Ulson	2012	2012 IEEE Symposium on Computers and Communications (ISCC)	10.1109/ISCC.2012.6249369	parallel computing;real-time computing;computer science;operating system;distributed computing;scheduling	HPC	-16.731040226027766	59.63728610723978	83815
ac1d5c4a8d3e1f426ba989b5118f7d787208afdf	query optimization approach with middle storage layer for spark sql		Currently, Spark SQL cannot optimize the multi-query tasks: tasks provided by batch processing are translated into different Spark jobs, and these jobs cannot share input data. To solve this problem, this paper explores the optimization of translating an SQL query into Spark jobs via two strategies: 1) A Middle Storage Layer is added between the persistent file system and the Spark core to address the data sharing problem among multiple queries. 2) When loading data into the Middle Storage Layer, the use of a selection strategy based on the cost model, in which the input data are distributed to multiple proper nodes for processing to achieve efficient utilization of cluster resources, is proposed. Based on this exploration, we develop a system called QOMS(Query Optimization approach with Middle Storage layer for Spark SQL), which offers better performance in improving query speed compared to Spark SQL with respect to the TPC-H benchmark.	analysis of algorithms;apache hadoop;batch processing;benchmark (computing);ibm tivoli storage productivity center;job stream;mathematical optimization;program optimization;query optimization;spark;sql;select (sql);test data	Aibo Song;Mingyu Zhai;Yingying Xue;Peng Chen;Mingyang Du;Yutong Wan	2018	2018 IEEE 22nd International Conference on Computer Supported Cooperative Work in Design ((CSCWD))	10.1109/CSCWD.2018.8465226	distributed computing;memory management;database;sql;batch processing;spark (mathematics);query optimization;data modeling;distributed database;computer science;file system	DB	-15.953422807814398	55.14400583654034	84067
d0c25780ebad1c079be986ff93bb685e9bce6d62	ibm 3850: mass storage system	time varying;storage system;mass storage system;random access	IBM's 3850, a hierarchical storage system, provides random access to stored data with capacity ranging from 35 X 109 to 472 X 109 bytes. The hierarchical architecture achieves access times varying from Direct Access Storage Device speeds to that of the Mass Storage Facility which can be as low as 10 seconds. The architecture of the Mass Storage System is examined to demonstrate its functional and performance capability.	byte;computer data storage;hierarchical storage management;mass storage;non-volatile memory;random access	Clayton Johnson	1975		10.1145/1499949.1500051	embedded system;storage area network;ibm 2321 data cell;computer hardware;direct-attached storage;engineering;operating system;nearline storage;sequential access memory;access method	OS	-14.634294765380735	52.36697770482838	84271
ded6b6983f9e5d95ad8976099b3f4267d0e2a13a	more effective synchronization scheme in ml using stale parameters	convergence;servers;computational modeling;synchronization;parallel processing;conferences;data models	In Machine learning (ML) the model we use is increasingly important, and the model's parameters, the key point of the ML, are adjusted through iteratively processing a training dataset until convergence. Although data-parallel ML systems often engage a perfect error tolerance when synchronizing the model parameters for maximizing parallelism, the synchronization of model parameters may delay in completion, a problem that generally gets worse at a large scale. This paper presents a Bounded Asynchronous Parallel (BAP) model of computation that allows computations using stale model parameters in order to reduce synchronization overheads. In the meanwhile, our BAP model ensures theoretical convergence guarantees for large scale data-parallel ML applications. This model permits distributed workers to use the stale parameters storing in the local cache, instead of waiting until the Parameter Server (PS) produces a new version. This expressively reduces the time workers spend on waiting. Furthermore, the BAP model guarantees the convergence of ML algorithm by bounding the maximum distance of the stale parameters. Experiments conducted on 4 cluster nodes with up to 32 GPUs showed that our model significantly improved the proportion of computing time relative to the waiting time and led to 1.2–2×speedup. Besides, we elaborated how to choose the staleness threshold when considering the tradeoff between Efficiency and Speed.	algorithm;data parallelism;error-tolerant design;graphics processing unit;machine learning;model of computation;parallel computing	Yabin Li;Han Wan;Bo Jiang;Xiang Long	2016	2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2016.0110	data modeling;parallel processing;synchronization;parallel computing;real-time computing;convergence;computer science;theoretical computer science;operating system;distributed computing;computational model;computer security;server;computer network	DB	-15.90133718835908	59.14226611722745	84481
b491677a0684319553fe092538ca4cd2a58f1406	makespan-optimal cache partitioning	cache storage;multicore systems;cache miss rates makespan optimal cache partitioning multicore systems cache memory multiple concurrent threads cache management replacement policy shared cache space;cache partitioning;makespan cache partitioning multicore systems;resource management instruction sets schedules partitioning algorithms multicore processing optimal scheduling algorithm design and analysis;multiprocessing systems cache storage;makespan;multiprocessing systems	In current multicore systems, cache memory is shared between multiple concurrent threads. Allocating the proper amount of cache to each thread is crucial to achieving high performance. Cache management in many existing systems is based on the least recently used replacement policy, which can lead to adverse contention between threads for shared cache space. Cache partitioning is a technique that reserves a certain amount of cache for each thread, and has been shown to work well in practice. We introduce the problem of determining the optimal cache partitioning to minimize the make span for completing a set of tasks. We analyze the problem using a model that generalizes a widely used empirical model for cache miss rates. Our first contribution is to give a mathematical characterization of the properties satisfied by an optimal partitioning. Second, we present an algorithm that finds a 1 +\epsilon approximation to the optimal partitioning in O(n log \frac{n}{\epsilon}log\frac{n}{\epsilon p}) time, where n is the number of tasks and p is a value that depends on the optimal solution. We compare our algorithm with several partitioning schemes used in practice or proposed in the literature. Simulations show that our algorithm achieves between 22-59% better make span compared to these algorithms.	algorithm;approximation;cpu cache;cache (computing);convex function;makespan;memory bandwidth;multi-core processor;on the fly;schedule (computer science);simulation;symmetric multiprocessing	Pan Lai;Rui Fan	2013	2013 IEEE 21st International Symposium on Modelling, Analysis and Simulation of Computer and Telecommunication Systems	10.1109/MASCOTS.2013.28	bus sniffing;job shop scheduling;pipeline burst cache;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;distributed computing;smart cache;cache algorithms;cache pollution;mesif protocol	Arch	-12.565463261573838	60.336836229542584	84562
7fda668864b4e3cac7d95c270e67cf66710a8956	language runtime support for nvm/dram hybrid main memory	garbage collection non volatile memory memory management language runtime;dynamic random access memory programming language runtime support nvm dram hybrid main memory architecture nonvolatile memory write operation nvm dram hybrid main memory management;storage management dram chips programming languages random access storage;nonvolatile memory random access memory abstracts runtime	Replacing of DRAM in main memory with non-volatile memory (NVM) has several merits. However, NVM under development has some limitations in write operation. To overcome it, some previous researches proposed NVM/DRAM hybrid memory architecture. In the architecture, it needs to determine data placements between NVM and DRAM. In this paper, we advocate that programming language runtimes are useful for management of NVM/DRAM hybrid main memory. In addition, we will propose a method to manage NVM/DRAM hybrid main memory with language runtime support.	computer data storage;dynamic random-access memory;non-volatile memory;programming language;volatile memory	Gaku Nakagawa;Shuichi Oikawa	2014	2014 IEEE COOL Chips XVII	10.1109/CoolChips.2014.6842949	uniform memory access;shared memory;interleaved memory;semiconductor memory;parallel computing;memory rank;static random-access memory;memory refresh;computer hardware;computer memory;overlay;memory controller;universal memory;non-volatile random-access memory;conventional memory;programming language;registered memory;memory map;memory management;memory ordering	Arch	-11.079032214700089	52.32639315740551	84605
f57d771f694023653029af9e36e5efcc7e1b133a	allowing shared libraries while supporting hardware isolation in multicore real-time systems		The desire to support real-time applications on multicore platforms has led to intense recent interest in techniques for reducing memory-related hardware interference. These techniques typically rely on mechanisms that ensure per-task isolation properties with respect to cache and memory accesses. In most prior work on such techniques, any sharing of memory pages by different tasks is defined away, as sharing breaks isolation. In reality, however, sharing is common. In this paper, one source of sharing is considered, namely, the usage of shared libraries. Such sharing can be obviated by statically linking libraries, but this solution can degrade schedulability by exhausting memory capacity. An alternative approach is proposed herein that allows library pages to be shared while preserving isolation properties. This approach is presented in the context of the MC2 framework and a schedulability-based evaluation of it is presented. Such an evaluation must necessarily consider memory-capacity limits. As a secondary contribution, this paper considers such limits for the first time in the context of MC2.	cpu cache;central processing unit;dynamic random-access memory;embedded system;home page;interference (communication);interleaved memory;library (computing);memory bank;multi-core processor;overhead (computing);physical address;protein data bank;provisioning;real-time operating system;real-time transcription;scheduling (computing);self-organized criticality	Namhoon Kim;Micaiah Chisholm;Nathan Otterness;James H. Anderson;F. Donelson Smith	2017	2017 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)	10.1109/RTAS.2017.14	cache;computer hardware;real-time computing;parallel computing;distributed shared memory;memory map;multi-core processor;uniform memory access;computer science;distributed computing;shared memory	Embedded	-9.45642579733743	52.05095883767367	84614
71a13dc117cdd992dbf872da5a9bb94fe74151fd	architectural exploration of last-level caches targeting homogeneous multicore systems	last level cache;gem5;multicore system	The Last-Level Cache (LLC) influences the overall system performance and power dissipation in multicore systems significantly. This paper evaluates five LLC architectures targeting execution time, dynamic and static power dissipation, and area consumption. They are measured using the widely adopted PARSEC benchmark suite for parallel shared-memory systems. Employing Gem5 full-system simulator and 32 nm technology characterization of the McPAT framework, this work had two interesting findings: (i) the shared LLC has the overall best performance under the PARSEC parallel workload, even for applications with less than 20% of shared data. (ii) A privately accessed cache can reduce up to 20 times the dynamic power dissipation on 32 nm technology and 25 times the area consumption when compared to shared-accessed caches.	benchmark (computing);cmos;cpu cache;cpu power dissipation;computer architecture simulator;lunar lander challenge;multi-core processor;parsec benchmark suite;requirement;run time (program lifecycle phase);shared memory	Rodrigo Cataldo;Guilherme Korol;Ramon Fernandes;Debora Matos;César A. M. Marcon	2016	2016 29th Symposium on Integrated Circuits and Systems Design (SBCCI)	10.1109/SBCCI.2016.7724050	embedded system;parallel computing;real-time computing;engineering;operating system	Arch	-4.646622416048199	55.057301695655106	84625
01d6a2bdc0b8124b99b7869eca73022eb1dbeb13	split-ordered lists: lock-free extensible hash tables	real time;concurrent data structures;non blocking synchronization;hash table;compare and swap;real time application;shared memory multiprocessor	"""We present the first lock-free implementation of an extensible hash table running on current architectures. It provides concurrent insert, delete, and search operations with an expected O(1) cost. It consists of very simple code, easily implementable using only load, store, and compare-and-swap operations. The new mathematical structure at the core of our algorithm is recursive split-ordering, a way of ordering elements in a linked list so that they can be repeatedly """"split"""" using a single compare-and-swap operation. Empirical tests conducted on a large shared memory multiprocessor show that even in non-multiprogrammed environments, the new algorithm significantly outperforms the most efficient known lock-based algorithm at all concurrency levels, exhibiting up to four times higher throughput at peak load. The incremental nature of our algorithm makes it well suited for real-time applications, as it offers predictable performance without unexpected breaks for resizing."""	compare-and-swap;concurrency (computer science);hash table;linked list;load profile;mathematical structure;multiprocessing;non-blocking algorithm;paging;real-time computing;real-time transcription;recursion;shared memory;throughput	Ori Shalev;Nir Shavit	2003		10.1145/872035.872049	hash table;parallel computing;real-time computing;hash function;perfect hash function;computer science;operating system;distributed computing;concurrent data structure;programming language;compare-and-swap;algorithm	OS	-15.015586087061408	46.89302792188578	84630
97e6c74c395f36de064d1b4c32ad5a04d3f282d1	fpga based control for real time systems	processor scheduling;reconfigurable architectures;real time systems field programmable gate arrays fabrics time frequency analysis adaptive scheduling processor scheduling dynamic scheduling;coprocessors;embedded systems;low power electronics;dynamic power consumption fpga based control real time systems worst case execution time wcet embedded devices scheduling methods adaptive reservation scheduling dynamic fraction computational resources advanced scheduling techniques computational overhead research hypothesis complex scheduling power saving techniques coprocessor fpga fabric reconfigurable device technology hybrid fpga cpu chips xilinx zynq extensible processing platform arm core axi bus low latency power efficient communication;field programmable gate arrays;reconfigurable architectures coprocessors embedded systems field programmable gate arrays low power electronics processor scheduling	Real time systems must guarantee tasks can be completed by a given deadline. Typically they are designed around the worst case execution time (WCET) of tasks in the system. In general this creates systems with excess slack compared to the average case. Since, real time systems are often embedded devices which are typically battery operated, developing systems with large amounts of slack is undesirable because more slack means more energy usage. There exist scheduling methods that try to adapt to the current environment, for example adaptive reservation scheduling, which assigns a dynamic fraction of the computational resources to each process. However these and more advanced scheduling techniques are rarely adopted in practice due to their high computational overhead. My research hypothesis is that the overheads of complex scheduling and power saving techniques in real time systems can be reduced through developing a coprocessor in the FPGA fabric. Recent developments in reconfigurable device technology include the introduction of new hybrid FPGA/CPU chips, such as the Xilinx Zynq extensible processing platform, where an ARM core is coupled to an FPGA fabric using an AXI bus. By locating the FPGA and CPU on the same die it possible to obtain low-latency power-efficient communication between user logic in the FPGA and tasks on the CPU. This paper presents my preliminary work, which uses a software application with real time deadlines, and creates a controller in the FPGA fabric to dynamically scale the operating frequency of the CPUs, while still guaranteeing that the deadlines can be met. This coprocessor is totally agnostic to the software running on the CPU and provided that some measure of slackness, which is the deadline period subtracted from the execution time, can be obtained it will be able to scale the frequency in a safe manner and reduce dynamic power consumption.	arm architecture;automated x-ray inspection;best, worst and average case;central processing unit;clock rate;computation;computational resource;coprocessor;embedded system;field-programmable gate array;offset binary;overhead (computing);real-time computing;run time (program lifecycle phase);scheduling (computing);slack variable;worst-case execution time	Shane T. Fleming;David B. Thomas	2013	2013 23rd International Conference on Field programmable Logic and Applications	10.1109/FPL.2013.6645610	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;parallel computing;real-time computing;dynamic priority scheduling;reconfigurable computing;computer science;rate-monotonic scheduling;operating system;two-level scheduling;least slack time scheduling;round-robin scheduling;coprocessor;field-programmable gate array;low-power electronics	Embedded	-8.515447509680786	58.56410441346512	84655
9e30e19e5152573277ff2b1a9af1ccd4b923be23	fcm: towards fine-grained gpu power management for closed source mobile games	power consumption computer games energy conservation graphics processing units mobile computing power aware computing;graphics processing units games land mobile radio estimation mathematical model complexity theory computational modeling;power management;power management gpu workload model dvfs mobile games fine grained;ddvfs;gpu workload model;power consumption reduction fcm fine grained gpu power management closed source mobile games embedded graphic processing units gpu dvfs policies graphics intensive games dynamic voltage and frequency scaling policies energy saving workload estimations frame complexity model game frame gpu workload quantification;mobile games;dvfs;fine grained	Contemporary mobile platforms employ embedded graphic processing units (GPUs) for graphics-intensive games, and dynamic voltage and frequency scaling (DVFS) policies are used to save energy without sacrificing quality. However, current GPU DVFS policies result in unnecessary power waste due to defective workload estimations of embedded GPUs during game play. In this paper, we propose the Frame-Complexity Model (FCM), a fine-grained estimation of the GPU workload in a game frame, to quantify the GPU workload with the real runtime demand for GPU computing resources of a game frame. In FCM, three constituents of a game frame (i.e., structure, textures and computation) are quantified without modification of mobile games. Preliminary experiments show that, compared with the default policy, the FCM-directed GPU DVFS policy can reduce more power consumption of games (11.3% to 25.8%) with good Quality of Service (QoS).	computation;dynamic frequency scaling;dynamic voltage scaling;embedded system;experiment;fuzzy cognitive map;general-purpose computing on graphics processing units;graphics processing unit;image scaling;mobile device;mobile game;power management;quality of service	Jiachen Song;Xi Li;Beilei Sun;Zhinan Cheng;Chao Wang;Xuehai Zhou	2016	2016 International Great Lakes Symposium on VLSI (GLSVLSI)	10.1145/2902961.2902989	embedded system;parallel computing;real-time computing;simulation;computer science;operating system	EDA	-6.14414193654396	56.423351892949235	84675
172da9c720ce16dc8c5f9d212c89349eb4ad4532	light nuca: a proposal for bridging the inter-cache latency gap	chip;dynamic routing	"""To deal with the """"memory wall"""" problem, microprocessors include large secondary on-chip caches. But as these caches enlarge, they originate a new latency gap between them and fast L1 caches (inter-cache latency gap). Recently, NonUniform Cache Architectures (NUCAs) have been proposed to sustain the size growth trend of secondary caches that is threatened by wire-delay problems. NUCAs are size-oriented, and they were not conceived to close the inter-cache latency gap. To tackle this problem, we propose Light NUCAs (L-NUCAs) leveraging on-chip wire density to interconnect small tiles through specialized networks, which convey packets with distributed and dynamic routing. Our design reduces the tile delay (cache access plus one-hop routing) to a single processor cycle and places cache lines at a finer granularity than conventional caches, reducing cache latency. Our evaluations show that in general, an L-NUCA improves simultaneously performance, energy, and area when integrated into both conventional or D-NUCA hierarchies."""	bridging (networking);cpu cache;cache (computing);instruction cycle;microprocessor;random-access memory;routing	Darío Suárez Gracia;Teresa Monreal Arnal;Fernando Vallejo;Ramón Beivide;Víctor Viñals	2009	2009 Design, Automation & Test in Europe Conference & Exhibition		parallel computing;real-time computing;computer hardware	Arch	-8.201293628697362	53.8002683703232	84689
720536ab3080525adc533cbc0d4abe82cf02257f	data partitioning strategies for graph workloads on heterogeneous clusters	data partitioning;data center;computational modeling;heterogeneous;engines;big data;synchronization;clustering algorithms;graph algorithms;cloud computing;partitioning algorithms;throughput	Large scale graph analytics are an important class of problem in the modern data center. However, while data centers are trending towards a large number of heterogeneous processing nodes, graph analytics frameworks still operate under the assumption of uniform compute resources. In this paper, we develop heterogeneity-aware data ingress strategies for graph analytics workloads using the popular PowerGraph framework. We illustrate how simple estimates of relative node computational throughput can guide heterogeneity-aware data partitioning algorithms to provide balanced graph cutting decisions. Our work enhances five online data ingress strategies from a variety of sources to optimize application execution for throughput differences in heterogeneous data centers. The proposed partitioning algorithms improve the runtime of several popular machine learning and data mining applications by as much as a 65% and on average by 32% as compared to the default, balanced partitioning approaches.	algorithm;data center;data mining;machine learning;throughput	Michael LeBeane;Shuang Song;Reena Panda;Jee Ho Ryoo;Lizy Kurian John	2015	SC15: International Conference for High Performance Computing, Networking, Storage and Analysis	10.1145/2807591.2807632	synchronization;data center;throughput;parallel computing;big data;cloud computing;computer science;theoretical computer science;operating system;database;distributed computing;cluster analysis;computational model	HPC	-17.56227515905857	57.42082927509502	84698
99da90033de8c848b8920abdc4e07892b0912ae8	leveraging 24/7 availability and performance for distributed real-time data warehouses	random access memory;loading distributed databases availability real time systems hardware random access memory;query processing;availability;resource allocation;loading;data replication and redundancy;performance optimization real time data warehousing availability fault tolerance data replication and redundancy distributed and parallel databases load balancing;fault tolerance;decision support systems;distributed databases;load balancing;tpc h decision support benchmark 24 7 availability distributed real time data warehouses continuous updates round robin algorithms shared nothing machine cluster distributed dw databases continuous availability data loading requirements data striping replication architecture fact table slave nodes query execution balancing performance benchmark measures dedicated database ram query workload query performance balancing;real time data warehousing;data warehouses;performance optimization;distributed and parallel databases;resource allocation data warehouses decision support systems distributed databases query processing;hardware;real time systems	Real-time Data Warehouses (DWs) must be able to deal with continuous updates while ensuring 24/7 availability. To improve their performance, distributing data using round-robin algorithms on clusters of shared-nothing machines is normally used. This paper proposes a solution for distributed DW databases that ensures its continuous availability and deals with frequent data loading requirements, while adding small performance overhead. We use a data striping and replication architecture to distribute portions of each fact table among pairs of slave nodes, where each slave node is an exact replica of its partner. This allows balancing query execution and replacing any defective node, ensuring the system's continuous availability. The size of each portion in a given node depends on its individual features, namely performance benchmark measures and dedicated database RAM. The estimated cost for executing each query workload in each slave node is also used for balancing query performance. We include experiments using the TPC-H decision support benchmark to evaluate the scalability of the proposed solution and show that it outperforms standard round-robin distributed DW setups.	algorithm;benchmark (computing);coefficient;continuous availability;data striping;decision support system;downtime;dreamwidth;experiment;hardware virtualization;ibm tivoli storage productivity center;in-memory database;load balancing (computing);online and offline;overhead (computing);random-access memory;real-time data;real-time operating system;requirement;response time (technology);round-robin dns;round-robin scheduling;scalability;shared nothing architecture	Ricardo Jorge Santos;Jorge Bernardino;Marco Vieira	2012	2012 IEEE 36th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2012.92	availability;fault tolerance;real-time computing;decision support system;resource allocation;computer science;load balancing;operating system;data warehouse;database;distributed computing;distributed database;computer security	DB	-18.78804241917052	54.03088453575355	84834
c153b84313682876bdc5f905c33830f81316f377	balancing indivisible real-valued loads in arbitrary networks		In parallel computing, a problem is divided into a set of smaller tasks that are distributed across multiple processing elements. Balancing the load of the processing elements is key to achieving good performance and scalability. If the computational costs of the individual tasks vary over time in an unpredictable way, dynamic load balancing aims at migrating them between processing elements so as to maintain load balance. During dynamic load balancing, the tasks amount to indivisible work packets with a real-valued cost. For this case of indivisible, realvalued loads, we analyze the balancing circuit model, a local dynamic load-balancing scheme that does not require global communication. We extend previous analyses to the present case and provide a probabilistic bound for the achievable load balance. Based on an analogy with the offline balls-into-bins problem, we further propose a novel algorithm for dynamic balancing of indivisible, real-valued loads. We benchmark the proposed algorithm in numerical experiments and compare it with the classical greedy algorithm, both in terms of solution quality and communication cost. We find that the increased communication cost of the proposed algorithm is compensated by a higher solution quality, leading on average to about an order of magnitude gain in overall performance.	benchmark (computing);computation;computer performance;experiment;greedy algorithm;indivisible;load balancing (computing);numerical analysis;online and offline;parallel computing;scalability	Ömer Demirel;Ivo F. Sbalzarini	2013	CoRR		network load balancing services;real-time computing;computer science;theoretical computer science;distributed computing	HPC	-14.955309528298557	59.83539941292775	84920
d2de7910b21abd558c729f31afc940a71cfcd146	exploring speculative procedure and loop level parallelism in splash2	thread level speculation;high performance computing;hpc;multicore;data dependence;tls	How to make use of multicore computing resources to accelerate high performance computing HPC applications has become a common concern problem. However, HPC applications have not yet been explored in thread level speculation TLS thoroughly, especially in the procedure level. This paper proposes a procedure and loop level speculation architecture model for speeding up HPC applications, including its speculative mechanism, analysis method, etc. It also takes several applications from SPLASH2 to analyse their speculative parallel potential together with performance impacting factors. The experimental results show that: 1 the best Barnes application can get a 90.9× speedup in loop level speculation while Lu application can get 40.2× speedup in procedure level speculation; 2 limited parallelism coverage and severe inter-thread data dependence violations badly affect both loop and procedure level speculative parallelism in some HPC applications; 3 It is found that although loop structure is the main source of speculative parallelism, procedure structure can be treated as its important supplement. Especially in applications that their 'hot' iteration body concludes multiple procedure calls, higher speculative procedure level speedup can be achieved than that in loop level speculation.	data parallelism;loop-level parallelism;parallel computing;speculative execution	Yaobin Wang;Zhiqin Liu;Huarong Chen;Xia Luo;Guotang Bi;Hong An	2014	IJHPSA	10.1504/IJHPSA.2014.061439	multi-core processor;supercomputer;parallel computing;real-time computing;computer science;operating system;distributed computing;transport layer security;speculative multithreading	Arch	-8.095912527740026	47.920384507302444	84991
f6741b471316aea6934b6b4251f624e68983587f	resilience characterization of a vision analytics application under varying degrees of approximation	baseline algorithm resilience characterization vision analytics application video summarization real time edge computing approximation techniques power efficiency performance efficiency;degradation;computer crashes;video signal processing approximation theory computer vision;approximation algorithms;approximation algorithms degradation streaming media real time systems approximate computing hardware computer crashes;streaming media;approximate computing;hardware;real time systems	In this work we study a state-of-the-art video summarization application that serves as a representative emerging workload for the domain of real time edge computing. We further examine three different approximation techniques to improve the power and performance efficiency of the workload. A detailed resiliency study of the application as well as its approximate versions show that the approximations do not degrade the resiliency of the baseline algorithm. Thus, we conclude that it is possible to realize safe, yet efficient approximations for this state-of-the-art video summarization algorithm from the point of view of performance, energy and reliability.	approximation algorithm;baseline (configuration management);edge computing;order of approximation;point of view (computer hardware company)	Radha Venkatagiri;Karthik Swaminathan;Chung-Ching Lin;Liang Wang;Alper Buyuktosunoglu;Pradip Bose;Sarita V. Adve	2016	2016 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2016.7581283	parallel computing;real-time computing;simulation;degradation;computer science;theoretical computer science;operating system;machine learning;approximation algorithm	Arch	-4.720249276907972	59.040185755398724	84998
70444a4745b7df9766c7f5c08ba9b70ff7beede0	mode-changes in cots time-triggered network hardware without online reconfiguration	time triggered networks;mode changes;scheduling	Time-triggered networks are widely used for safety-critical applications. Being offline scheduled, flexibility and adaptivity typically come at the price of very low resource utilization, if possible at all. In this paper, we present the Stacked Scheduling Approach (SSA) for time-triggered networks to enable mode changes and implicit adaptation in such networks by enabling reuse of network bandwidth reservations. We describe SSA in detail and conduct a case study to show that SSA can be implemented in COTS time-triggered network hardware and validate the approach by implementing an example in a COTS TTEthernet network.	networking hardware;online and offline;requirement;scheduling (computing)	Florian Heilmann;Ali Syed;Gerhard Fohler	2016	SIGBED Review	10.1145/3015037.3015046	embedded system;real-time computing;computer science;operating system;distributed computing;scheduling	Mobile	-9.186440283772944	58.188611273348485	85238
25233d201be5af4e1e8926d742af678ca5938223	performance optimization for short mapreduce job execution in hadoop	parallel computing;heart beat standards parallel processing delay optimization methods algorithm design and analysis;performance optimization mapreduce parallel computing job execution;task trackers performance optimization hadoop mapreduce job execution parallel computing framework data intensive problems pull model task assignment mechanism push model heartbeat based communication mechanism event notification job tracker;mapreduce;performance optimization;parallel processing;job execution	Hadoop MapReduce is a widely used parallel computing framework for solving data-intensive problems. To be able to process large-scale datasets, the fundamental design of the standard Hadoop places more emphasis on high-throughput of data than on job execution performance. This causes performance limitation when we use Hadoop MapReduce to execute short jobs that requires quick responses. In order to speed up the execution of short jobs, this paper proposes optimization methods to improve the execution performance of MapReduce jobs. We made three major optimizations: first, we reduce the time cost during the initialization and termination stages of a job by optimizing its setup and cleanup tasks, second, we replace the pull-model task assignment mechanism with a push-model, third, we replace the heartbeat-based communication mechanism with an instant message communication mechanism for event notifications between the Job Tracker and Task Trackers. Experimental results show that the job execution performance of our improved version of Hadoop is about 23% faster on average than the standard Hadoop for our test application.	apache hadoop;benchmark (computing);data-intensive computing;high-throughput computing;information retrieval;instant messaging;job stream;mapreduce;mathematical optimization;parallel computing;performance tuning;push technology;throughput	Jinshuang Yan;Xiaoliang Yang;Rong Gu;Chunfeng Yuan;Yihua Huang	2012	2012 Second International Conference on Cloud and Green Computing	10.1109/CGC.2012.40	parallel computing;real-time computing;computer science;data-intensive computing;distributed computing	HPC	-17.699878964954845	55.22605207347668	85246
4eb0130e6a0c3678d725bb636f4633fa8b3cd196	compiler optimizations for i/o-intensive computations	multiprocessor interconnection networks;data transformations;optimising compilers;distributed memory;distributed memory systems;electronic mail;concurrent computing;optimizing compiler;application software;intel paragon distributed memory message passing multiprocessor;intel paragon distributed memory message passing multiprocessor compiler optimizations i o intensive computations transformation techniques out of core programs locality data transformations file layouts;optimizing compilers application software electronic mail multiprocessor interconnection networks computer networks concurrent computing parallel machines file systems parallel architectures;out of core programs;locality;interconnection network;computer networks;parallel architectures;file layouts;loop transformation;i o intensive computations;parallel file system;compiler optimization;parallel computer;message passing;parallel machines;transformation techniques;message passing optimising compilers distributed memory systems;optimizing compilers;compiler optimizations;parallel applications;data transfer;file systems	This paper describes transformation techniques for out-of-core programs (i.e., those that deal with very large quantities of data) based on exploiting locality using a combination of loop and data transformations. Writing efficient out-of-core program is an arduous task. As a result, compiler optimizations directed at improving I/O performance are becoming increasingly important. We describe how a compiler can improve the performance of the code by determining appropriate file layouts for out-of-core arrays and finding suitable loop transformations. In addition to optimizing a single loop nest, our solution can handle a sequence of loop nests. We also show how to generate code when the file layouts are optimized. Experimental results obtained on an Intel Paragon distributed-memory message-passing multiprocessor demonstrate marked improvements in performance due to the optimizations described in this paper.	computation;distributed memory;input/output;intel paragon;locality of reference;message passing;multiprocessing;optimizing compiler;out-of-core algorithm	Mahmut T. Kandemir;Alok N. Choudhary;J. Ramanujam	1999		10.1109/ICPP.1999.797401	manifest expression;loop inversion;computer architecture;parallel computing;loop fission;concurrent computing;loop interchange;loop dependence analysis;computer science;loop nest optimization;loop optimization;superoptimization;operating system;loop unrolling;optimizing compiler;distributed computing;programming language	PL	-11.353237916815688	48.98258602262702	85321
4a64eda713f9473424b9e5b450f27bfdcd10b550	reducing connection memory requirements of mpi for infiniband clusters: a message coalescing approach	memory requirement;software libraries;high performance computing;storage management;mpi library;programming model;memory access;remote direct memory access;message coalescing approach;message passing interface;application program interfaces;high performance computer;number of clusters;programming model mpi library message passing interface memory requirement infiniband cluster message coalescing approach high performance computing resource usage resource performance remote direct memory access doe tri labs;message passing;storage management application program interfaces message passing software libraries;libraries laboratories computer networks message passing us department of energy degradation scalability computer science high performance computing writing;resource usage;high performance;doe tri labs;high speed;infiniband cluster;resource performance	Clusters in the area of high-performance computing have been growing in size at a considerable rate. In these clusters, the dominate programming model is the Message Passing Interface (MPI), so the MPI library has a key role in resource usage and performance. To obtain maximal performance, many clusters deploy a high-speed interconnect between compute nodes. One such interconnect, InfiniBand, has been gaining in popularity due to its various features including Remote Data Memory Access (RDMA), and high-performance. As a result, it is being deployed in a significant number of clusters and has been chosen as the standard interconnect for capacity clusters within the DOE Tri-Labs. As these clusters grow in size, care must be taken to ensure the resource usage does not increase too significantly with scale. In particular, the MPI library resource usage should not grow at a rate which will exhaust the node memory or starve user applications. In this paper we present our findings of current memory usage when all connections are created and design a message coalescing method to decrease memory usage significantly. Our models show that the default configuration of MVAPICH can grow to 1GB per process for 8K processes, while our enhancements reduce usage by an order of magnitude to around 120 MB per process while maintaining near-equal performance. We have validated our design on a 575-node cluster and shown no performance degradation for a variety of applications. We also increase the message rate attainable by over 150%.	8k resolution;best, worst and average case;elegant degradation;gigabyte;infiniband;mvapich;maximal set;message passing interface;network-attached storage;programming model;remote direct memory access;requirement;scalability;simulation;supercomputer;throughput	Matthew J. Koop;Terry Jones;Dhabaleswar K. Panda	2007	Seventh IEEE International Symposium on Cluster Computing and the Grid (CCGrid '07)	10.1109/CCGRID.2007.92	supercomputer;parallel computing;message passing;real-time computing;remote direct memory access;computer science;message passing interface;operating system;database;distributed computing;programming paradigm;programming language	HPC	-10.534892011285942	47.10828392017229	85396
f9cf47539216a3737f6353dca8a8f3f1e588413e	approximating warps with intra-warp operand value similarity	registers graphics processing units hardware writing computers microarchitecture;optimisation approximation theory energy conservation graphics processing units;energy efficiency intra warp operand value similarity value locality myriad optimization techniques micro architecture dennard scaling parallel accelerators gpu warp approximation	Value locality, the recurrence of a previously-seen value, has been the enabler of myriad optimization techniques in traditional processors. Value similarity relaxes the constraint of value locality by allowing values to differ in the lowest significant bits where values are micro-architecturally near. With the end of Dennard Scaling and the turn towards massively parallel accelerators, we revisit value similarity in the context of GPUs. We identify a form of value similarity called intra-warp operand value similarity, which is abundant in GPUs. We present Warp Approximation, which leverages intra-warp operand value similarity to trade off accuracy for energy. Warp Approximation dynamically identifies intra-warp operand value similarity in hardware, and executes a single representative thread on behalf of all the active threads in a warp, thereby producing a representative value with approximate value locality. This representative value can then be stored compactly in the register file as a value similar scalar, reducing the read and write energy when dealing with approximate data. With Warp Approximation, we can reduce execution unit energy by 37%, register file energy by 28%, and improve overall GPGPU energy efficiency by 26% with minimal quality degradation.	approximation algorithm;central processing unit;dennard scaling;elegant degradation;execution unit;general-purpose computing on graphics processing units;graphics processing unit;locality of reference;mathematical optimization;operand;register file;warp (information security)	Daniel Wong;Nam Sung Kim;Murali Annavaram	2016	2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2016.7446063	parallel computing;real-time computing;computer science;theoretical computer science;operating system	Arch	-6.716171054285832	54.30101549907645	85462
0dd817d1ea4a2accac0c86f80748cc90526c2c69	physical machine state migration	virtual machine;suspend;state migration;data centers physical machine state migration powerful functionality virtualization technology virtual machine fault tolerance load balancing linux machine nonvirtualized physical machines vm migration linux hibernation facility mysql dbms server structured query language database management system;fault tolerant;sql;resource allocation;virtualisation computer centres fault tolerant computing linux relational databases resource allocation sql virtual machines;power function;computer centres;data center;fault tolerant computing;virtual machines;resume;fault tolerance;linux;load balance;relational databases;fault tolerance suspend resume state migration data center;virtualisation;kernel servers benchmark testing hardware processor scheduling image restoration virtual machine monitors	A powerful functionality enabled by modern virtualization technologies is the ability to move a virtual machine (VM) from one physical machine to another, which enables unprecedented flexibility for system fault tolerance and load balancing. However, no similar capability exists for physical machines. This paper describes the first known successful implementation of migrating a physical machine's state from one physical Linux machine to another. This physical machine state migration (PMSM) capability greatly decreases the amount of disruption due to scheduled shut-down for non-virtualized physical machines, and is more challenging than VM migration because it cannot rely on a separate piece of software to perform the state transfer, e.g., the hyper visor in the case of VM migration. The PMSM prototype described in this paper is adapted from Linux's hibernation facility. The current PMSM prototype can migrate a physical machine running the MySQL DBMS server under 7 seconds.	algorithm;correctness (computer science);denial-of-service attack;hibernation (computing);hypervisor;linux;linux;load balancing (computing);mysql;operating system;prototype;real-time clock;server (computing);stress testing;system fault tolerance;virtual machine	Jui-Hao Chiang;Maohua Lu;Tzi-cker Chiueh	2011	2011 IEEE 17th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2011.100	embedded system;fault tolerance;parallel computing;real-time computing;computer science;virtual machine;operating system;database;distributed computing;programming language;computer security;computer network	OS	-17.525463289291775	52.22178968491711	85651
fd7e2f6db91f1b3bad414eb2dcb139a5b130082e	egs: an effective global i/o scheduler to improve the load balancing of ssd-based raid-5 arrays		With the expansion in data volumes, disk arrays require high speed and large volumes to adapt the cloud computing environment. Solid State Drives (SSDs) now have the trend to substitute the traditional external storage because of their better performance than Hard Disk Drives (HDDs), so they are proper to be utilized in disk arrays. However, traditional I/O scheduler in RAID systems use uniform I/O distribution to balance the workload, which is not suitable for SSD-based RAID. It is because that typical SSD has asymmetric performance on read/write operations, which can result in unbalanced I/O accesses in an SSD array. Furthermore, an even I/O distribution can have an uneven write distribution, which leads to unbalanced lifetime of various SSDs and affects the lifetime of SSD arrays significantly. Obviously, this problem cannot be well addressed through internal wear leveling mechanisms in SSDs. To address the above problems, in this paper, we propose an effective global I/O scheduler called “EGS”, which can optimize the I/O performance and prolong the lifetime of SSD-based RAID-5 arrays. According to different access time of read/write operations, we separate the read and write operations to give different weights, and make each disk has a same amount of window (consists of several read/write I/Os) simultaneously. On the other hand, to avoid unbalanced write distribution, we design an additional policy to prolong SSDs’ lifetime. To demonstrate the effectiveness of EGS, we conduct several simulations via Disksim. The results show that, compared to traditional I/O scheduling algorithms, EGS saves average response time by up to 40%, and increases the lifetime of SSD array by approximately 45%.	access time;algorithm;cloud computing;disk array;egs (program);external storage;hard disk drive;i/o scheduling;input/output;load balancing (computing);response time (technology);scheduling (computing);simulation;solid-state drive;standard raid levels;unbalanced circuit;wear leveling	Yanjun Lu;Chentao Wu;Jie Li	2017	2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC)	10.1109/ISPA/IUCC.2017.00050	parallel computing;human–computer interaction;raid;scheduling (computing);response time;disk array;wear leveling;input/output;load balancing (computing);cloud computing;computer science	Embedded	-12.567057509691768	54.09598417182281	85663
c731f2a8038f7cc9ea25afaac9f672af829bfe1d	optimized register renaming scheme for stack-based x86 operations	floating point unit;binary translation;floating point;register file	The stack-based floating point unit (FPU) in the x86 architecture limits its floating point (FP) performance. The flat register file can improve FP performance but affect x86 compatibility. This paper presents an optimized two-phase floating point register renaming scheme used in implementing an x86-compliant processor. The two-phase renaming scheme eliminates the implicit dependencies between the consecutive FP instructions and redundant operations. As two applications of the method, the techniques used in the second phase of the scheme can eliminate redundant loads and reduce the mis-speculation ratio of the load-store queue. Moreover, the performance of a binary translation system that translates instructions in x86 to MIPS-like ISA can also be boosted by adding the related architectural supports in this optimized scheme to the architecture.	binary translation;critical path method;floating-point unit;machine translation;mathematical optimization;optimal design;paging;prototype;register file;register renaming;stack machine;stack-oriented programming language;superscalar processor;two-phase locking;x86	Xuehai Qian;He Huang;Zhenzhong Duan;Junchao Zhang;Nan Yuan;Yongbin Zhou;Hao Zhang;Huimin Cui;Dongrui Fan	2007		10.1007/978-3-540-71270-1_4	floating-point unit;computer architecture;parallel computing;computer hardware;computer science;floating point;operating system;register renaming;register file	Arch	-9.257797438631831	53.23295589056048	85868
b2c06faa2e2dbd39d38312d553420b144e235194	props: a progressively pessimistic scheduler for software transactional memory		Software Transactional Memory (STM) is one promising abstraction to simplify the task of writing highly parallel applications. Nonetheless, in workloads lacking enough parallelism, STM’s optimistic approach to concurrency control can adversely degrade performance as transactions abort and restart often.	scheduling (computing);software transactional memory	Hugo Rito;João P. Cachopo	2014		10.1007/978-3-319-09873-9_13	parallel computing;real-time computing;distributed computing	OS	-14.08994975826882	48.833166055063366	85874
a39107bd293cf0f5d9674223c4194eed0761745e	the eh model: analytical exploration of energy-harvesting architectures		Energy-harvesting devices—which operate solely on energy collected from their environment—have brought forth a new paradigm of intermittent computing. These devices succumb to frequent power outages that would cause conventional systems to be stuck in a perpetual loop of restarting computation and never making progress. Ensuring forward progress in an intermittent execution model is difficult and requires saving state in non-volatile memory. In this work, we propose the EH model to explore the trade-offs associated with backing up data to maximize forward progress. In particular, we focus on the relationship between energy and forward progress and how they are impacted by backups/restores to derive insights for programmers and architects.	backup;computation;exception handling;excite;exponential hierarchy;interaction;non-volatile memory;programmer;programming paradigm;volatile memory	Joshua San Miguel;Karthik Ganesan;Mario Badr;Natalie D. Enright Jerger	2018	IEEE Computer Architecture Letters	10.1109/LCA.2017.2777834	real-time computing;computer science;energy harvesting;execution model;computation;non-volatile memory	Arch	-7.226790010989481	55.89840689708996	85930
1a8a2324b276fef90261761f37068e4d0313e037	control-flow aware communication and conflict analysis of parallel processes	viterbi decoding;formal verification;parallel processing;program control structures;shared memory systems;tlm systemc model;viterbi decoder;access conflicts;automated allocation approach;automated binding approach;conflict analysis;control-flow aware communication;global timing behavior;parallel processes;shared communication resources;system behavior;temporal environment models	In this paper, we present an approach for control-flow aware communication and conflict analysis of systems of parallel communicating processes. This approach allows to determine the global timing behavior of such a system and to detect communication that might produce conflicts on shared communication resources. Furthermore, we show the incorporation of temporal environment models in order to analyze their influence on the system behavior. Based on the determined conflicts, an automated allocation and binding approach for shared resources to resolve potential access conflicts is proposed. All analysis steps can be performed starting with a TLM SystemC model of the entire system without any need for user interaction. Finally, a SystemC model of a Viterbi decoder is used as case study to demonstrate the capability of our approach.	algorithm;best, worst and average case;control flow;data rate units;embedded system;name binding;response time (technology);sensor;simulation;static timing analysis;synchronization (computer science);systemc;temporal logic;viterbi decoder	Axel Siebenborn;Alexander Viehl;Oliver Bringmann;Wolfgang Rosenstiel	2007	2007 Asia and South Pacific Design Automation Conference		embedded system;parallel processing;parallel computing;real-time computing;computer science;theoretical computer science;operating system;programming language;viterbi decoder	EDA	-7.65171200745368	59.62060011831414	86018
e747f66490ce1a1df5ea1275e81f9bc6333537d1	review - using lifetime predictors to improve memory allocation performance	memory allocation		memory management	Richard T. Snodgrass	1999	ACM SIGMOD Digital Review		memory management;data mining;computer science	OS	-11.161342567557039	52.419893061410725	86029
a03cce0ac2980e869dca9ac30b9dc30bae4b3838	electronics and telecommunications research institute: a multiprocessor server with a new highly pipelined bus	cache storage;protocols;performance evaluation;protocols pipelines cache memory network servers;cache memory;information network;system buses;network servers;shared memory systems;information networks;shared memory system multiprocessor server highly pipelined bus design shared bus pipelined protocol hipi bus block transfer unbalanced pipeline local cache memory line size performance unbalanced data transfer responder queue bus interface simulation commercial applications ticom iii ticom ii information network korea;information networks shared memory systems system buses pipeline processing protocols cache storage performance evaluation network servers;data transfer;pipeline processing	In this paper, we explore the design issues of a shared bus with pipelinedprotocol, so called HiPi+Bus, which is implemented for a multiprocessor server. The characteristics and design parameters for the HiPi+Bus are described. In the viewpoint of a pipelined bus, a block transfer is no good because of involving complex and unbalanced pipeline. However, it is requested by a local cache memory of which line size tends to be increased. To get the best performance and compensate unbalanced data transfer characteristic caused by block transfer, a responder queue for the bus interface is also proposed. According to the simulation results, it is explored that the HiPi+Bus, with help of the responder queue, can provide balanced service for more than 16 processors, which is important in runningcommercial applications. The HiPi+Bus is implemented for the TICOM III, a successor of the TICOM II which is the main server of the national administrative information network in Korea.	cpu cache;central processing unit;crossbar switch;fairness measure;load balancing (computing);multiprocessing;operating system;server (computing);simulation;ticom;transfer function;unbalanced circuit	Woo-Jong Hahn;Ando Ki;Kee-Wook Rim;Soo-Won Kim	1996		10.1109/IPPS.1996.508104	bus sniffing;bus;embedded system;communications protocol;parallel computing;real-time computing;cpu cache;computer science;local bus;operating system;system bus;control bus;back-side bus;bus network;address bus;computer network	Arch	-11.272681986563395	47.556950716777244	86039
42888b4735eaba756535ba5267d1f9fffa675254	dora: a dynamic file assignment strategy with replication	dora;storage allocation;file access patterns;heat;assignment problem;file access patterns dora dynamic file assignment strategy static file assignment algorithms dynamic file allocation problem integrated file replication techniques parallel disk systems dynamic round robin with replication;integrated file replication techniques;replication;storage allocation replicated databases;resource management;dynamic file allocation problem;heating;data mining;system performance;file assignment problem;round robin delay heuristic algorithms file systems quality of service cost function measurement parallel processing computer science system performance;time factors;round robin;round robin file assignment problem replication dynamic heat;file system;heuristic algorithms;on the fly;parallel disk systems;static file assignment algorithms;dynamic file assignment strategy;dynamic round robin with replication;replicated databases;dynamic;file systems;cooling	Compared with numerous static file assignment algorithms proposed in the literature, very few investigations on the dynamic file allocation problem have been accomplished. Moreover, none of them has integrated file replication techniques into file assignment algorithms in a highly dynamic file system where files are created or deleted on the fly and their access patterns varied over time. We argue that file replication and file assignment can act in concert to boost the performance of parallel disk systems. In this paper, we propose a new dynamic file assignment strategy called DORA (dynamic round robin with replication). The advantages of DORA can be attributed to its two main characteristics. First, it takes the dynamic nature of file access patterns into account to adapt to a changing workload condition. Second, it utilizes file replication techniques to complement file assignment schemes so that system performance can be further improved. Experimental results demonstrate that DORA performs consistently better than existing algorithms.	algorithm;doraemon;on the fly	Jonathan Tjioe;Renata Widjaja;Abraham P Lee;Tao Xie	2009	2009 International Conference on Parallel Processing	10.1109/ICPP.2009.8	self-certifying file system;replication;parallel computing;torrent file;memory-mapped file;device file;computer science;class implementation file;versioning file system;resource management;operating system;journaling file system;database;assignment problem;open;distributed file system;file system fragmentation;global namespace;replication	DB	-16.010167237738937	56.301453734901585	86107
f7027be6df5207b75d3f6aa45a4064e94a66b5e5	performance evaluation of on-line scheduling algorithms for imprecise computation	transient overload;analytical models;imprecise computation;real time systems performance evaluation online scheduling algorithms imprecise computation imprecise task scheduling maximum error simulation transient overload task deadline data structures;performance evaluation;processor scheduling;maximum error;simulation;software performance evaluation;error analysis;scheduling algorithm;computational modeling;online scheduling algorithms;data structures;scheduling;performance analysis;task deadline;processor scheduling scheduling algorithm computational modeling data structures error analysis performance analysis algorithm design and analysis analytical models real time systems;real time systems software performance evaluation online operation scheduling;imprecise task scheduling;data structure;online operation;algorithm design and analysis;real time systems	This paper presents an algorithm for scheduling imprecise tasks to minimize maximum error and analyzes its performance through intensive simulation. The imprecise computation is used to manage transient overload of computation. Each task for imprecise systems consists of a mandatory part and an optional part that can be skipped when systems are overloaded The imprecise computation trades accuracy for meeting the deadline of tasks by skipping their optional parts. To increase accuracy for imprecise computation, we present a simple on-line scheduling algorithm that minimizes the maximum error. This algorithm executes mandatory parts of tasks first, then the optional parts in order to minimize maximum error. The proposed algorithm also increases schedulability by executing mandatory parts first. The proposed algorithm is simple and does not require any additional data structures such as the reservation list for maintaining mandatory parts.	algorithm;computation;data structure;function overloading;online and offline;performance evaluation;scheduling (computing);simulation	Jai-Hoon Kim;Kihyun Song;Kyunghee Choi;Gihyun Jung;Seunhun Jung	1998		10.1109/RTCSA.1998.726421	parallel computing;real-time computing;data structure;computer science;operating system;distributed computing;programming language;scheduling	Embedded	-11.529850377600182	59.82129704843855	86142
370180c58723530d0b42f2dc6421a9d643b61c86	hadoop characterization		In the last decade, Warehouse Scale Computers (WSC) have grown in number and capacity while Hadoop became the de facto standard framework for Big data processing. Despite the existence of several benchmark suites, sizing guides, and characterization studies, there are few concrete guidelines for WSC designers and engineers who need to know how real Hadoop workloads are going to stress the different hardware subsystems of their servers. Available studies have shown execution statistics of Hadoop benchmarks but have not being able to extract meaningful and reusable results. Secondly, existing sizing guides provide hardware acquisition lists without considering the workloads. In this study, we propose a simple Big data workload differentiation, deliver general and specific conclusions about how demanding the different types of Hadoop workloads are for several hardware subsystems, and show how power consumption is influenced in each case. HiBench and Big-Bench suites were used to capture real time memory traces, and CPU, disk, and power consumption statistics of Hadoop. Our results show that CPU intensive and disk intensive workloads have a different behavior. CPU intensive workloads consume more power and memory bandwidth while disk intensive workloads usually require more memory. These and other conclusions presented in the paper are expected to help WSC designers to decide the hardware characteristics of their Hadoop systems, and better understand the behavior of big data workloads in Hadoop.	algorithm;apache hadoop;benchmark (computing);big data;central processing unit;computer data storage;high memory;input/output;memory bandwidth;need to know;random-access memory;run time (program lifecycle phase);tracing (software);working set size;world sudoku championship	Icaro Alzuru;Kevin Long;Bhaskar Gowda;David Zimmerman;Tao Li	2015	2015 IEEE Trustcom/BigDataSE/ISPA	10.1109/Trustcom.2015.567	parallel computing;real-time computing;computer hardware;computer science	Arch	-5.35673000192364	52.036709976005376	86236
07cf87c4dc72771fe2f6ee4bc725ea5696d5f16c	optimization of cloud task processing with checkpoint-restart mechanism	google;conference_paper;checkpoint restart mechanism;checkpointing google fault tolerance fault tolerant systems cloud computing clouds probability distribution;large scale google data center cloud task processing optimization checkpoint restart mechanism optimizing fault tolerance techniques cloud computing failure event distributions failure probability distribution adaptive algorithm virtual machines berkeley lab checkpoint restart tool real cluster environment task failure events production trace;blcr cloud computing checkpoint restart mechanism optimal checkpointing interval google;virtual machines cloud computing failure analysis software fault tolerance software reliability statistical distributions;blcr;optimal checkpointing interval;cloud computing	In this paper, we aim at optimizing fault-tolerance techniques based on a checkpointing/restart mechanism, in the context of cloud computing. Our contribution is three-fold. (1) We derive a fresh formula to compute the optimal number of checkpoints for cloud jobs with varied distributions of failure events. Our analysis is not only generic with no assumption on failure probability distribution, but also attractively simple to apply in practice. (2) We design an adaptive algorithm to optimize the impact of checkpointing regarding various costs like checkpointing/restart overhead. (3) We evaluate our optimized solution in a real cluster environment with hundreds of virtual machines and Berkeley Lab Checkpoint/Restart tool. Task failure events are emulated via a production trace produced on a large-scale Google data center. Experiments confirm that our solution is fairly suitable for Google systems. Our optimized formula outperforms Young's formula by 3-10 percent, reducing wall-clock lengths by 50-100 seconds per job on average.	adaptive algorithm;application checkpointing;cloud computing;data center;emulator;fault tolerance;overhead (computing);virtual machine	Sheng Di;Yves Robert;Frédéric Vivien;Derrick Kondo;Cho-Li Wang;Franck Cappello	2013	2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)	10.1145/2503210.2503217	parallel computing;real-time computing;cloud computing;computer science;operating system;distributed computing	HPC	-18.278683003870835	60.42945542089762	86242
9b5d2078d39476237cd5a593cb4dbc6e11093242	using file systems for non-volatile main memory management		Non-volatile (NV) memory is next generation memory. It provides fast access speed comparable to DRAM and also persistently stores data without power supply. These features enable NV memory to be used as both main memory and secondary storage. While the active researches have been conducted on its use for either main memory or secondary storage, they were conducted independently. This paper proposes the integrated memory management methods, by which NV memory can be used as both main memory and secondary storage. The proposed methods use file systems as their basis for NV memory management. Such integration enables the memory allocation for processes and files from the same source, and processes can take advantage of a large amount of physical memory used for both main memory and storage. We implemented the proposed memory management methods in the Linux kernel. The evaluation results performed on a system emulator show that the memory allocation costs of the proposed methods are comparable to that of the existing DRAM and are significantly better than those of the page swapping.	auxiliary memory;computer data storage;direct method in the calculus of variations;dynamic random-access memory;emulator;linux;memory management;next-generation network;non-volatile memory;nv network;paging;pervasive informatics;power supply;process (computing);systems management	Shuichi Oikawa	2013			flash file system;self-certifying file system;shared memory;indexed file;memory-mapped file;device file;computer file;stub file;versioning file system;unix file types;journaling file system;overlay;open;file system fragmentation;memory map;file control block;memory management	OS	-14.532180170874962	52.71998506989068	86339
035941ac151141dbfb4303ee6bd941e885082207	the communication requirements of mutual exclusion	mutual exclusion;critical section	This paper examines the amount of communication that is required for performing mutual exclusion. It is assumed that n processors communicate via accesses to a shared memory that is physically distributed among the processors. We consider the possibility of creating a scalable mutual exclusion protocol that requires only a constant amount of communication per access to a critical section. We present two main results, First, we show that there does not exist a scalable mutual exclusion protocol that uses only read and write operations. This result solves an open problem posed by Yang and Anderson, Second, we prove that the same result holds even if test-and-set, compare-andswap, load-and-reserve and store-conditional operations are allowed in addition to read and write operations. Our results hold even if an amortized analysis of communication costs is used, an arbitrary amount of memory is available, and the processors have coherent caches. In contrast, a mutual exclusion protocol is known that uses only read, write and swap operations, performs a constant amount of communication per access to the critical section, uses a constant amount of memory per process, and does not require coherent caches. Our results suggest that many current generation microprocessors have instruction sets that are not well-suited to performing mutual exclusion in a shared memory environment.	amortized analysis;cache coherence;central processing unit;coherence (physics);critical section;microprocessor;mutual exclusion;paging;requirement;scalability;shared memory;test-and-set;yang	Robert Cypher	1995		10.1145/215399.215434	mutual exclusion;computer science;critical section;programming language	Theory	-15.1856070189466	46.59430845636946	86350
cc96932149b19d672c84fbd454232bc7c311fdbb	distributed round-robin and first-come first-serve protocols and their application to multiprocessor bus arbitration		Two new distributed protocols for fair and efficient bus arbitration are presented. The protocols implement round-robin (RR) and first-come first-serve (FCFS) scheduling, respectively. Both protocols use relatively few control lines on the bus, and their logic is simple. The round-robin protocol, which uses statically assigned arbitration numbers to resolve conflict during an arbitration, is more robust and simpler to implement than previous distributed RR protocols that are based on rotating agent priorities. The proposed FCFS protocol uses partly static arbitration numbers, and is the first practical proposal for a FCFS arbiter known to the authors. The proposed protocols thus have a better combination of efficiency, cost, and fairness characteristics than existing multiprocessor bus arbitration algorithms. Three implementations of our RR protocol, and two implementations of our FCFS protocol, are discussed. Simulation results are presented that address: 1) the practical potential for unfairness in the simpler implementation of the FCFS protocol, 2) the practical implications of the higher waiting time variance in the RR protocol, and 3) the allocation of bus bandwidth among agents with unequal request rates in each protocol. The simulation results indicate that there is very little practical difference in the performance of the two protocols.	algorithm;arbiter (electronics);bus (computing);bus mastering;communications protocol;emoticon;fairness measure;multiprocessing;observable;rapid refresh;round-robin scheduling;scheduling (computing);simulation	Mary K. Vernon;Udi Manber	1988				Networks	-11.05337478630182	58.90965683748704	86631
04eea2d3cb7f5e69659c77ee89b7638f7c06488f	giga+: scalable directories for shared file systems	semantic file system;metadata;information retrieval;virtual directory;enterprise search;faceted search;petabyte scale storage;file system;system design	There is an increasing use of high-performance computing (HPC) clusters with thousands of compute nodes that, with the advent of multi-core CPUs, will impose a significant challenge for storage systems: The ability to scale to handle I/O generated by applications executing in parallel in tens of thousands of threads. One such challenge is building scalable directories for cluster storage - i.e., directories that can store billions to trillions of entries and handle hundreds of thousands of operations per second.	central processing unit;directory (computing);flops;input/output;multi-core processor;scalability;supercomputer	Swapnil Patil;Garth A. Gibson;Samuel Lang;Milo Polte	2007		10.1145/1374596.1374604	computer science;operating system;database;metadata;world wide web;systems design	HPC	-15.353612044019354	52.969618905524506	86701
e80ee3c24d408f602ca5868adfb7b24381b0ba34	a novel optimization method to improve de-duplication storage system performance	storage allocation;storage pipeline;kernel;storage system;data compression;storage allocation data compression parallel programming;optimal method;parallel hash data de duplication performance optimization storage pipeline;parallel programming;data mining;system performance;computer architecture;parallel hash;indexes;content distribution;cryptography;pipelines;data de duplication;optimization methods system performance throughput cost function pipeline processing cryptography computer science laboratories information science performance analysis;parallel calculation data de duplication data intensive storage systems performance optimization methods task partition algorithm chunk content distribution algorithm;performance optimization;throughput	Data De-duplication has become a commodity component in data-intensive storage systems. But compared with other traditional storage paradigms, de-duplication system achieves elimination of data duplications or redundancies at the cost of bringing several additional layers or function components into the I/O path, and these additional components are either CPU-intensive or I/O intensive, largely hindering the overall system performance. Direct against the above potential system bottlenecks, this paper quantitatively analyzes the overhead of each main component introduced by de-duplication, and then proposes two performance optimization methods. The one is parallel calculation of content aware chunk identifiers, which fully utilizes the parallelism both inter and intra chunks by using a certain task partition and chunk content distribution algorithm. Experiments demonstrate that it can improve up to 150% of the system throughput, and at the same time much better utilize the multiprocessor resources. The other one is storage pipelining, which overlaps the CPU-bound, I/O-bound and network communication tasks. Through a dedicated five-stage storage pipeline design for file archival operations, experimental results show that the system throughput can increase up to 25% according to our workloads.	algorithm;archive;central processing unit;data deduplication;data-intensive computing;digital distribution;i/o bound;identifier;input/output;mathematical optimization;multiprocessing;overhead (computing);parallel computing;pipeline (computing);program optimization;throughput	Chuanyi Liu;Yibo Xue;Dapeng Ju;Dongsheng Wang	2009	2009 15th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2009.103	data compression;throughput;parallel computing;kernel;real-time computing;computer science;cryptography;theoretical computer science;operating system;database;distributed computing;computer performance	HPC	-11.5598662975803	50.89834016591638	86770
5ec06d318f4640d5f32175474d70363f79337a58	two fast and high-associativity cache schemes	cache storage;performance evaluation;multiplexing educational institutions sliding mode control;memory architecture cache storage content addressable storage performance evaluation;miss rate;memory architecture;cache performance;and trace driven simulation;cache associativity;content addressable storage;access time cache schemes high associativity cache performance sequential multicolumn cache column associative cache parallel multicolumn cache associativity low miss rate;mru;access time	n the race to improve cache performance, many researchers have proposed schemes that increase a cache's associativity. The associativity of a cache is the number of places in the cache where a block may reside. 1 In a direct-mapped cache, which has an associativity of 1, there is only one location to search for a match for each reference. In a cache with associativity n—an n-way set-associative cache—there are n locations (see the boxes on definitions and set-associative caches). Increasing associativity reduces the miss rate by decreasing the number of conflict , or interference, references. The column-associative cache 4 and the pre-dictive sequential associative cache 5 seem to have achieved near-optimal performance for an associativity of two. Increasing associativ-ity beyond two, therefore, is one of the most important ways to further improve cache performance. We propose two schemes for implementing associativity greater than two: the sequential multicolumn cache, which is an extension of the column-associative cache, and the parallel multicolumn cache. For an associativity of four, they achieve the low miss rate of a four-way set-associative cache. Our simulation results show that both schemes can effectively reduce the average access time. With a 4-Kbyte cache and a miss penalty of 20 cycles, the improvements in average access time over a direct-mapped cache were 9.8% for the SMC and 10.8% for the PMC. The improvement of the SMC in average access time reached 22.4% when the associativity was eight and the miss penalty increased to 100 cycles. The two schemes are effective for a wide range of cache sizes, from 1 to 128 Kbytes. Hill and Smith 6 reports that using a two-way set-associative cache instead of a direct-mapped cache reduces misses by about 30%. Increasing associativity, however, also increases hit access time. This generally leads to a longer cycle time because in most microprocessors access to the cache lies in the critical path. There are two reasons for the longer hit access time: • The multiplexing logic that selects the correct data point from the referenced set causes a delay. • Only after completing tag checking does the multiplexing logic know which data point to select and dispatch to the CPU. Therefore, it is difficult or impossible to implement a speculative dispatch of data to the CPU (or optimistic execution 2). In contrast, a direct-mapped cache requires no such multiplexing logic; the cache system can speculatively dispatch data to the …	access time;cpu cache;central processing unit;critical path method;data point;dynamic dispatch;interference (communication);kilobyte;microprocessor;multiplexing;operator associativity;simulation;speculative execution;symmetric multiprocessing	Chenxi Zhang;Xiaodong Zhang;Yong Yan	1997	IEEE Micro	10.1109/40.621212	bus sniffing;least frequently used;pipeline burst cache;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;access time;computer science;write-once;cache invalidation;operating system;adaptive replacement cache;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol	Arch	-7.8357769443601795	53.0341371955263	86989
6ae5e2797be3badb4a5eee72170363544b59ada7	a b-tree index extension to enhance response time and the life cycle of flash memory	automatic control;flash memory;b tree index extension;life cycle;index structure;write count reducing;mb tree;indexation;bftl;flash memory response time;flash memory life cycle	Flash memory has critical drawbacks such as long latency of its write operation and a short life cycle. In order to overcome these limitations, the number of write operations to flash memory devices needs to be minimized. The B-Tree index structure, which is a popular hard disk based index structure, requires an excessive number of write operations when updating it to flash memory. To address this, it was proposed that another layer that emulates a B-Tree be placed between the flash memory and B-Tree indexes. This approach succeeded in reducing the write operation count, but it greatly increased search time and main memory usage. This paper proposes a B-Tree index extension that reduces both the write count and search time with limited main memory usage. First, we designed a buffer that accumulates update requests per leaf node and then simultaneously processes the update requests of the leaf node carrying the largest number of requests. Second, a type of header information was written on each leaf node. Finally, we made the index automatically control each leaf node size. Through experiments, the proposed index structure resulted in a significantly lower write count and a greatly decreased search time with less main memory usage, than placing a layer that emulates a B-Tree. 2009 Elsevier Inc. All rights reserved.	b-tree;computer data storage;emulator;experiment;flash memory;hard disk drive;response time (technology);tree (data structure)	Hongchan Roh;Woo-Cheol Kim;Seung-Woo Kim;Sanghyun Park	2009	Inf. Sci.	10.1016/j.ins.2009.05.007	biological life cycle;interleaved memory;parallel computing;real-time computing;memory refresh;computer hardware;computer science;automatic control;computer memory;flat memory model;registered memory;memory management	DB	-12.385643780333645	54.369035700913	87001
34df5ddeb4d1c346cd42db63ab4921be5bf262ee	cable: a cache-based link encoder for bandwidth-starved manycores		Off-chip bandwidth is a scarce resource in modern processors, and it is expected to become even more limited on a per-core basis as we move into the era of high-throughput and massively-parallel computation. One promising approach to overcome limited bandwidth is off-chip link compression. Unfortunately, previously proposed latency-driven compression schemes are not a good fit for latency-tolerant manycore systems, and they often do not have the dictionary capacity to accommodate more than a few concurrent threads. In this work, we present CABLE, a novel CAche-Based Link Encoder that enables point-to-point link compression between coherent caches, re-purposing the data already stored in the caches as a massive and scalable dictionary for data compression. We show the broad applicability of CABLE by applying it to two critical off-chip links: (1) the memory link interface to off-chip memory, and (2) the cache-coherent link between processors in a multi-chip system. We have implemented CABLE's search pipeline hardware in Verilog using the OpenPiton framework to show its feasibility. Evaluating with SPEC2006, we find that CABLE increases effective off-chip bandwidth by 7.2x and system throughput by 3.78x on average, 83% and 258% better than CPACK, respectively.		Tri Minh Nguyen	2018	2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)	10.1109/MICRO.2018.00033	parallel computing;real-time computing;throughput;encoder;cpu cache;computer science;data compression;scalability;cache;thread (computing);bandwidth (signal processing)	Arch	-10.204561399295091	51.7602220987951	87111
a37f131b75ed8d5db577ce070af944ed6058dfd6	implementation and evaluation of global and partitioned scheduling in a real-time os	real time scheduling;real time operating systems;multicore processors;global edf;partitioned edf	In this work, we provide an experimental comparison between Global-EDF and Partitioned-EDF, considering the run-time overhead of a real-time operating system (RTOS). Recent works have confirmed that OS implementation aspects, such as the choice of scheduling data structures and interrupt handling mechanisms, impact real-time schedulability as much as scheduling theoretic aspects. However, these studies used real-time patches applied into a general-purpose OS. By measuring the run-time overhead of an RTOS designed from scratch, we show how close the schedulability ratio of task sets is to the theoretical hard real-time schedulability tests. Moreover, we show how a well-designed object-oriented RTOS allows code reuse of scheduling components (e.g., thread, scheduling criteria, and schedulers) and easy real-time scheduling extensions. We compare our RTOS to a real-time patch for Linux in terms of the task set schedulability ratio of several generated task sets. In some cases, Global-EDF considering the overhead of the RTOS is superior to Partitioned-EDF considering the overhead of the patched Linux, which clearly shows how different OSs impact hard real-time schedulers.	arm cortex-a9;algorithm;cpu cache;car–parrinello molecular dynamics;central processing unit;code reuse;data structure;earliest deadline first scheduling;embedded system;equivalence partitioning;general-purpose modeling;hardware performance counter;kilobyte;linux;multi-core processor;open-source software;overhead (computing);point of sale;real-time clock;real-time computing;real-time operating system;real-time transcription;real-time web;requirement;scheduling (computing);semiconductor industry;theory;total variation diminishing	Giovani Gracioli;Antônio Augusto Fröhlich;Rodolfo Pellizzoni;Sebastian Fischmeister	2013	Real-Time Systems	10.1007/s11241-013-9183-3	multi-core processor;fair-share scheduling;parallel computing;real-time computing;real-time operating system;dynamic priority scheduling;computer science;operating system;round-robin scheduling	Embedded	-13.28223788511228	48.98164021850819	87142
a7a24f882aec173c01a9ed1eb52589f71d6c80f8	disengaged scheduling for fair, protected access to fast computational accelerators	gpus;fairness;hardware accelerators;scheduling;operating system protection	Today's operating systems treat GPUs and other computational accelerators as if they were simple devices, with bounded and predictable response times. With accelerators assuming an increasing share of the workload on modern machines, this strategy is already problematic, and likely to become untenable soon. If the operating system is to enforce fair sharing of the machine, it must assume responsibility for accelerator scheduling and resource management.  Fair, safe scheduling is a particular challenge on fast accelerators, which allow applications to avoid kernel-crossing overhead by interacting directly with the device. We propose a disengaged scheduling strategy in which the kernel intercedes between applications and the accelerator on an infrequent basis, to monitor their use of accelerator cycles and to determine which applications should be granted access over the next time interval.  Our strategy assumes a well defined, narrow interface exported by the accelerator. We build upon such an interface, systematically inferred for the latest Nvidia GPUs. We construct several example schedulers, including Disengaged Timeslice with overuse control that guarantees fairness and Disengaged Fair Queueing that is effective in limiting resource idleness, but probabilistic. Both schedulers ensure fair sharing of the GPU, even among uncooperative or adversarial applications; Disengaged Fair Queueing incurs a 4% overhead on average (max 18%) compared to direct device access across our evaluation scenarios.	fair queuing;fairness measure;graphics processing unit;interaction;kernel (operating system);open-shop scheduling;operating system;overhead (computing);scheduling (computing)	Konstantinos Menychtas;Kai Shen;Michael L. Scott	2014		10.1145/2541940.2541963	parallel computing;real-time computing;computer science;operating system;distributed computing;round-robin scheduling;scheduling	Arch	-10.276570797319854	56.663718922990206	87201
a97d7670ac9f17284f2a3266621c097dcbeba156	superscalar instruction execution in the 21164 alpha microprocessor	microprocessors;cmos integrated circuits;microprocessors pipelines clocks cmos technology delay reduced instruction set computing computer applications computer interfaces databases computational modeling;chip;300 mhz superscalar instruction execution 21164 alpha microprocessor cmos chip high clock rate low operational latency high throughput nonblocking memory systems 0 5 micron;memory systems;multilevel caches;high throughput;superscalar chips;memory;cmos integrated circuits microprocessor chips instruction sets;microprocessor chips;instruction sets	The 21164 is a new quad-issue, superscalar Alpha microprocessor that executes 1.2 billion instructions per second. The 300-MHz, 0.5-μm CMOS chip delivers an estimated 345/505 SPECint32/SPECfp92 performance. The design's high clock rate, low operational latency, and high-throughput/nonblocking memory systems contribute to this performance.	cmos;clock rate;dec alpha;high-throughput computing;microprocessor;superscalar processor;throughput	John H. Edmondson;Paul I. Rubinfeld;Ronald P. Preston;Vidya Rajagopalan	1995	IEEE Micro	10.1109/40.372349	chip;high-throughput screening;computer architecture;parallel computing;real-time computing;telecommunications;computer science;operating system;instruction set;memory;cmos	Arch	-6.75710023690618	52.57066554527619	87305
5293cd1a0357fe0c719c345b720a179424e5f911	an approach to exploiting skewed associative memories in avionics systems	avionics;associative memory aerospace electronics scheduling algorithm runtime heuristic algorithms aircraft space vehicles analytical models dynamic scheduling sorting;time complexity;processor scheduling;dynamical processes;scheduling algorithm;aerospace computing;computational complexity;processor scheduling avionics aerospace computing content addressable storage computational complexity search problems;spacecraft process scheduling algorithms avionics systems skewed associative memories search operation simulation analytical models time complexity runtime overhead dynamic scheduling algorithms aircraft;associative memory;search problems;content addressable storage;process scheduling;analytical model;dynamic scheduling	There are two main types of process scheduling algorithms commonly used in aircraft/spacecraft avionics systems. The first category consists of dynamic algorithms, which dynamically assign priorities to processes on the basis of runtime parameters. The second category consists of static algorithms, which statically determine priorities before runtime. The main disadvantage of applying dynamic process scheduling algorithms to avionics systems is the extra runtime overhead produced by these algorithms. This overhead is mainly related to the time required to sort active processes in the ready queue upon each process preemption or the arrival of each new process. The mentioned overhead encourages the use of static algorithms. But static algorithms have their own disadvantages. In fact, these algorithms bound the maximum available CPU utilization and have difficulties with non-periodic processes. This paper proposes and evaluates an approach to exploiting skewed associative memories in order to replace the time-consuming sorting operation by an efficient search operation. Both analytical models and simulation results show that the proposed approach can reduce the time complexity of the runtime overhead of dynamic scheduling algorithms (in terms of n the number of active processes) from O(nlogn) to O(n). This can considerably increase the performance of dynamic scheduling algorithms and make them much more feasible to be used in aircraft/spacecraft avionics systems.	algorithm;avionics;central processing unit;dynamic problem (algorithms);overhead (computing);preemption (computing);process state;scheduling (computing);simulation;sorting;time complexity	Mohsen Sharifi;Behrouz Zolfaghari	2002		10.1109/ICPADS.2002.1183450	avionics;parallel computing;real-time computing;computer science;operating system;database;distributed computing;scheduling	Embedded	-12.405321605063378	59.840556846199945	87429
336741129cfcb7fe049f416eafc07e9e5a029268	scalable and low-latency data processing with stream mapreduce	databases;fault tolerant;distributed processing;distributed computing;data processing;real time data;programming fault tolerance fault tolerant systems scalability databases real time systems instruction sets;programming model;fault tolerant system;distributed computing event stream processing complex event processing mapreduce;low latency;fault tolerant systems;fault tolerance;complex event processing;scalability;mapreduce;data handling;point of view;real time data stream scalable low latency data processing streammapreduce paradigm event stream processing scalable programming model latency critical application;stream processing;event stream processing;programming;distributed processing data handling;instruction sets;real time systems	We present StreamMapReduce, a data processing approach that combines ideas from the popular MapReduce paradigm and recent developments in Event Stream Processing. We adopted the simple and scalable programming model of MapReduce and added continuous, low-latency data processing capabilities previously found only in Event Stream Processing systems. This combination leads to a system that is efficient and scalable, but at the same time, simple from the user's point of view. For latency-critical applications, our system allows a hundred-fold improvement in response time. Notwithstanding, when throughput is considered, our system offers a ten-fold per node throughput increase in comparison to Hadoop. As a result, we show that our approach addresses classes of applications that are not supported by any other existing system and that the MapReduce paradigm is indeed suitable for scalable processing of real-time data streams.	apache hadoop;benchmark (computing);event stream processing;fault tolerance;in-memory database;interrupt latency;login;mapreduce;programmer;programming model;real-time data;response time (technology);responsiveness;scalability;spectral density estimation;state (computer science);throughput;virtual synchrony;word lists by frequency	Andrey Brito;Andre Martin;Thomas Knauth;Stephan Creutz;Diogo Becker de Brum;Stefan Weigert;Christof Fetzer	2011	2011 IEEE Third International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2011.17	fault tolerance;parallel computing;real-time computing;data processing;computer science;operating system;distributed computing	DB	-18.423150824963294	54.96866429605878	87436
da803f49eb3fac6c8dba5a866efe1684e63e81a2	page replacement using marginal loss functions		This paper describes a new technique to reduce page-faults in multiprocessing systems by supplying compile-time information about application's access patterns to the kernel. Runtime support is used to determine parameters unknown at compile-time and to summarize all this information as a marginal-loss function. Marginal loss functions describe the extra number of page faults that a process incurs when one of its pages is removed from main memory. System calls convey the marginal loss function of each active memory segment to the kernel which uses this information to determine the allocation and replacement of pages. We outline how the compiler analyzes access patterns and how the marginal loss function can be computed for common patterns. Simulation results demonstrate a significant reduction in page faults compared to typical approaches used by current operating systems.	loss function;marginal model;page replacement algorithm	Manuel Ujaldon;Shamik D. Sharma;Joel H. Saltz	1997		10.1109/SC.1997.10041	parallel computing;kernel;real-time computing;page fault;page replacement algorithm;biological classification;computer science;theoretical computer science;operating system;atmosphere;programming language;computational model;static timing analysis;client–server model;loss function	ML	-11.79931851503782	50.70778000890946	87598
7fb6eff49f9e8bf895893621b1be957ef128be2b	ibm zec12 processor subsystem	topology;reliability;history;microcomputers cache storage dram chips logic design;time factors;planets;enterprise class computing environment ibm zec12 processor subsystem mainframe smp system multilevel shared cache hierarchy;topology cache storage benchmark testing program processors time factors operating systems;benchmark testing;operating systems	The IBM zEC12 has a robust, multi-level shared cache hierarchy that is designed to meet the needs of the enterprise class computing environment and represents a significant growth in system capacity and performance from its predecessor.	channel capacity;ibm personal computer	Robert Sonnelitter	2013	2013 IEEE Hot Chips 25 Symposium (HCS)	10.1109/HOTCHIPS.2013.7478304	pipeline burst cache;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;cache coloring;cache;computer science;operating system;smart cache;cache algorithms;cache pollution;ibm san volume controller	Embedded	-11.468632130535356	48.551893438370605	87619
fdf43436c0e20e462a99ba730a58c3fab9e58f77	analytical leakage/temperature-aware power modeling and optimization for a variable speed real-time system	temperature aware power management;leakage aware power optimization;dynamic voltage scaling;markov model;real time systems	We consider a DVS-enabled single-processor firm real-time (FRT) system with Poisson arrival jobs having exponential execution times and generally distributed relative deadlines. The queue size of the system bounds the number of jobs which may be available therein. Further, the processor speed depends on the number of jobs in the system which varies because of the job arrivals, service completions, and deadline misses. Thus, the processor power consumption, including both the dynamic and leakage powers, depends on the stochastic nature of the system. More specifically, the instantaneous dynamic power consumption lonely depends on the number of jobs at that moment. However, the instantaneous leakage power consumption depends on both the number of jobs and the instantaneous processor temperature. In turn, the temperature is affected by both the dynamic and leakage power consumptions. Taking all the aforementioned inter-effects into account, this paper analytically models the timing, power and temperature behaviors of such a variable speed FRT system. The analysis is then employed to address the problem of the system average power (and thus, energy) minimization subject to guaranteeing some upper bound on the system loss probability. Simulation results are also put against the analytical ones to show the accuracy level of the proposed analytical method as well as the efficacy of the optimizations.	clock rate;dynamic voltage scaling;information leakage;job stream;mathematical optimization;program optimization;real-time clock;real-time computing;real-time transcription;simulation;spectral leakage;time complexity	Morteza Mohaqeqi;Mehdi Kargahi;Ali Movaghar-Rahimabadi	2012		10.1145/2392987.2392997	mathematical optimization;electronic engineering;real-time computing;computer science	Metrics	-5.294972630158973	58.910346847084895	87623
c2b599b24c2273aa771edd77aedf97301960fb35	enhancing multiprocessor architecture simulation speed using matched-pair comparison	multi threading;multi threaded application;multiprocessor system multiprocessor architecture simulation matched pair comparison multi threaded application smarts statistical sampling technique;multiprocessor systems;smarts statistical sampling technique;statistical method;computer architecture;computational modeling;sampling technique;statistical analysis;multiprocessor architecture;sampling methods benchmark testing computer architecture digital simulation instruction sets multi threading multiprocessing systems;paired comparison;arm;multiprocessing systems;computer science;sampling methods;system architecture;multiprocessor architecture simulation;matched pair comparison;multiprocessor system;simulation tool;computer simulation;gaussian distribution;benchmark testing;digital simulation;sampling methods computational modeling computer architecture computer simulation computer science statistical analysis multiprocessing systems arm gaussian distribution;instruction sets	While cycle-level, full-system architecture simulation tools are capable of estimating performance at arbitrary accuracy, the time to simulate an entire application is usually prohibitive. Moreover, simulating multi-threaded applications further exacerbates this problem as most simulation tools are single-threaded. Recently, statistical sampling techniques, such as SMARTS, have managed to bring down the simulation time significantly by making it possible to only simulate about 1% of the code with sufficient accuracy. However, thousands of simulation points throughout the benchmark must still be simulated. First of all, we propose to use the well-established statistical method matched-pair comparison and motivate why this will bring down the number of simulation points needed to achieve a given accuracy. We apply it to single-processor as well as multiprocessor simulation and show that it is capable of reducing the number of needed simulation points by one order of magnitude. Secondly, since we apply the technique to single- as well as multiprocessors, we study for the first time the efficiency of statistical sampling techniques in multiprocessor systems to establish a baseline to compare with. We show theoretically and confirm experimentally, that while the instruction throughput vary significantly on each individual processor, the variability of instruction throughput across processors in a multiprocessor system decreases as we increase the number of processors for some important workloads. Thus, a factor of P fewer simulation points, where P is the number of processors, are needed to begin with when sampling is applied to multiprocessors	baseline (configuration management);benchmark (computing);central processing unit;computer simulation;experiment;multiprocessing;sampling (signal processing);smiles arbitrary target specification;spatial variability;statistical model;systems architecture;thread (computing);throughput;whole earth 'lectronic link	Magnus Ekman;Per Stenström	2005	IEEE International Symposium on Performance Analysis of Systems and Software, 2005. ISPASS 2005.	10.1109/ISPASS.2005.1430562	computer simulation;sampling;computer science;statistics	Arch	-5.376446996492335	51.426411172687196	88176
ca7ef3533fb08882ae6e0124d08b4df43bc14df2	finding probably best system configurations quickly		Computer systems often have many possible configurations, and designing a high performance system often requires selecting the best configuration. Unfortunately, the performance of complex systems can often be estimated only via simulations, or with measurements of real systems. Since longer simulation times are required to estimate the performance more accurately, it is often computationally intractable to estimate the performance of all configurations accurately via simulations. (Measurements of real systems can take even longer.)	complex systems;computational complexity theory;hardware description language;simulation	Takayuki Osogami	2006	SIGMETRICS Performance Evaluation Review	10.1145/1215956.1215972		Arch	-5.402417920995838	50.941254993726204	88202
c3227c09aad676917d5c253e47e5ccbd51da927a	the feasibility research of cloud storage based on global file system	read and write performance;high availability;reliability;information retrieval;storage management;file system recovery;data management;gfs;system performance;massively parallel computer;file system;public service;cloud storage;security of data;parallel processing;file systems cloud computing storage area networks linux servers computer architecture educational institutions;storage management cloud computing information retrieval parallel processing reliability security of data;read and write performance cloud storage gfs file system recovery;support function;reading and writing;cloud computing;write performance feasibility research cloud storage global file system gfs environment file access massive data management public service support functions massively parallel computing scalability secure data storage reliable data storage file system recovery node damage read performance	Cloud storage is the extension and development based on the cloud computing, which should not only provide the conventional file access just like POSIX, but also must be able to support the massive data management and the public service support functions. GFS (Global File System) is a solution for massively parallel computing and cloud storage, and its high availability and good scalability can assure the data that it can be more secure and reliable stored and more efficiently scheduled. In this paper we test the GFS environment, and study the file system recovery when node damage happening, and compare the GFS with ext3, NFS system in read and write performance. The experiment result and the practical system performance shows, GFS can provide effective support for the cloud storage, and provide good storage and calculation for cloud computing.	cloud computing;cloud storage;global file system;high availability;posix;parallel computing;scalability	Kefei Cheng;Nan Wang	2012	2012 9th International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2012.6233946	parallel processing;support function;parallel computing;cloud computing;data management;computer science;operating system;reliability;database;high availability	HPC	-18.213115839153446	51.984712229868315	88294
c31eefdc58508cf7b7ccdcd879d7689b50497c5d	let caches decay: reducing leakage energy via exploitation of cache generational behavior	cache decay;online algorithm;salida;storage system;dissipation energie;turn off;cache memory;energy dissipation;fuite;chip;antememoria;algorithme;algorithm;antememoire;generational behavior;low power;leakage power;power dissipation;systeme memoire;leak;disipacion energia;energy minimization;sistema memoria;high performance;cache memories;algoritmo	"""Power dissipation is increasingly important in CPUs ranging from those intended for mobile use, all the way up to high-performance processors for highend servers. Although the bulk of the power dissipated is dynamic switching power, leakage power is also beginning to be a concern. Chipmakers expect that in future chip generations, leakage's proportion of total chip power will increase significantly. This article examines methods for reducing leakage power within the cache memories of the CPU. Because caches comprise much of a CPU chip's area and transistor counts, they are reasonable targets for attacking leakage. We discuss policies and implementations for reducing cache leakage by invalidating and """"turning off"""" cache lines when they hold data not likely to be reused. In particular, our approach is targeted at the generational nature of cache line usage. That is, cache lines typically have a flurry of frequent use when first brought into the cache, and then have a period of """"dead time"""" before they are evicted. By devising effective, low-power ways of deducing dead time, our results show that in many cases we can reduce L1 cache leakage energy by 4x in SPEC2000 applications without having an impact on performance. Because our decay-based techniques have notions of competitive online algorithms at their roots, their energy usage can be theoretically bounded at within a factor of two of the optimal oracle-based policy. We also examine adaptive decay-based policies that make energy-minimizing policy choices on a per-application basis by choosing appropriate decay intervals individually for each cache line. Our proposed adaptive policies effectively reduce L1 cache leakage energy by 5x for the SPEC2000 with only negligible degradations in performance."""	cpu cache;cpu power dissipation;cache (computing);cache-oblivious algorithm;central processing unit;low-power broadcasting;microprocessor;online algorithm;spectral leakage;transistor	Zhigang Hu;Stefanos Kaxiras;Margaret Martonosi	2002	ACM Trans. Comput. Syst.	10.1145/507052.507055	bus sniffing;embedded system;parallel computing;real-time computing;cache coloring;cache;computer science;dissipation;cache invalidation;operating system;distributed computing;cache algorithms;cache pollution	Arch	-6.208532017866765	55.13606854604748	88301
74fcc3a4806da111405f057dc84de39f8fed15d7	techniques for efficient processing in runahead execution engines	cache storage;runahead execution engines;performance evaluation;application software;clocks;prefetching;dynamic energy;out of order;process design;performance improvement;cache storage performance evaluation instruction sets power consumption;engines;energy consumption;runahead processor;performance analysis;long latency cache miss;terminology;power consumption;energy consumption runahead execution engines long latency cache miss dynamic energy runahead processor out of order processor;out of order processor;engines prefetching energy consumption delay process design application software performance analysis clocks out of order terminology;instruction sets	Runahead execution is a technique that improves processor performance by pre-executing the running application instead of stalling the processor when a long-latency cache miss occurs. Previous research has shown that this technique significantly improves processor performance. However, the efficiency of runahead execution, which directly affects the dynamic energy consumed by a runahead processor, has not been explored. A runahead processor executes significantly more instructions than a traditionalout-of-order processor, sometimes without providing any performance benefit, which makes it inefficient. In this paper, we describe the causes of inefficiency in runahead execution and propose techniques to make a runahead processor more efficient, thereby reducing its energy consumption and possibly increasing its performance. Our analyses and results provide two major insights: (1) the efficiency of runahead execution can be greatly improved with simple techniques that reduce the number of short, overlapping, and useless runahead periods, which we identify as the three major causes of inefficiency, (2) simple optimizations targeting the increase of useful prefetches generated in runahead mode can increase both the performance and efficiency of a runahead processor. The techniques we propose reduce the increase in the number of instructions executed due to runahead execution from 26.5% to 6.2%, on average, without significantly affecting the performance improvement provided by runahead execution.	benchmark (computing);cpu cache;mathematical optimization;out-of-order execution;prefetcher;runahead;speculative execution	Onur Mutlu;Hyesoon Kim;Yale N. Patt	2005	32nd International Symposium on Computer Architecture (ISCA'05)	10.1109/ISCA.2005.49	process design;computer architecture;application software;parallel computing;real-time computing;computer science;out-of-order execution;operating system;instruction set;terminology	Arch	-7.4165131853553845	53.12170212847882	88330
85029306db36c3ee8e3ae2f9a42318680db58558	energy-efficient dynamic task scheduling algorithms for dvs systems	energy efficient;real time;dynamic voltage scaling;optimal scaling;scheduling algorithm;energy consumption;optimal scaling factor;low power design;energy minimization;task scheduling;dvs system;energy saving;dynamic task scheduling	Dynamic voltage scaling (DVS) is a well-known low-power design technique that reduces the processor energy by slowing down the DVS processor and stretching the task execution time. However, in a DVS system consisting of a DVS processor and multiple devices, slowing down the processor increases the device energy consumption and thereby the system-level energy consumption. In this paper, we first use system-level energy consideration to derive the “optimal ” scaling factor by which a task should be scaled if there are no deadline constraints. Next, we develop dynamic task-scheduling algorithms that make use of dynamic processor utilization and optimal scaling factor to determine the speed setting of a task. We present algorithm duEDF, which reduces the CPU energy consumption and algorithm duSYS and its reduced preemption version, duSYS_PC, which reduce the system-level energy. Experimental results on the video-phone task set show that when the CPU power is dominant, algorithm duEDF results in up to 45&percent; energy savings compared to the non-DVS case. When the CPU power and device power are comparable, algorithms duSYS and duSYS_PC achieve up to 25&percent; energy saving compared to CPU energy-efficient algorithm duEDF, and up to 12&percent; energy saving over the non-DVS scheduling algorithm. However, if the device power is large compared to the CPU power, then we show that a DVS scheme does not result in lowest energy. Finally, a comparison of the performance of algorithms duSYS and duSYS_PC show that preemption control has minimal effect on system-level energy reduction.	algorithm;central processing unit;dynamic voltage scaling;earliest deadline first scheduling;image scaling;low-power broadcasting;preemption (computing);run time (program lifecycle phase);scheduling (computing)	Jianli Zhuo;Chaitali Chakrabarti	2008	ACM Trans. Embedded Comput. Syst.	10.1145/1331331.1331341	embedded system;parallel computing;real-time computing;computer science;operating system;efficient energy use;energy minimization;scheduling	Embedded	-5.311101359799621	58.59149451414169	88473
a42906baf2859879ec74f0a2c22f7c1b07ea809c	exploring the energy-time tradeoff in mpi programs on a power-scalable cluster	power consumption message passing workstation clusters application program interfaces;energy efficiency;microprocessors;energy efficient;high performance computing;energy levels;energy time tradeoff;message passing interface;gears;energy consumption;application program interfaces;energy states;high performance computer;standard benchmark suite;message passing;predictive models;power scalable cluster;mpi;technical report;computer science;frequency energy consumption predictive models gears computer science supercomputers energy efficiency potential energy energy states microprocessors;workstation clusters;power consumption;potential energy;frequency;high performance;off the shelf;energy time tradeoff mpi message passing interface power scalable cluster high performance computing energy consumption standard benchmark suite;supercomputers;energy saving	Recently, energy has become an important issue in high-performance computing. For example, supercomputers that have energy in mind, such as BlueGene/L, have been built; the idea is to improve the energy efficiency of nodes. Our approach, which uses off-the-shelf, high-performance cluster nodes that are frequency scalable, allows energy saving by scaling down the CPU. This paper investigates the energy consumption and execution time of applications from a standard benchmark suite (NAS) on a power-scalable cluster. We study via direct measurement and simulation both intra-node and inter-node effects of memory and communication bottlenecks, respectively. Additionally, we compare energy consumption and execution time across different numbers of nodes. Our results show that a power-scalable cluster has the potential to save energy by scaling the processor down to lower energy levels. Furthermore, we found that for some programs, it is possible to both consume less energy and execute in less time when using a larger number of nodes, each at reduced energy. Additionally, we developed and validated a model that enables us to predict the energy-time tradeoff of larger clusters.	benchmark (computing);blocking (computing);blue gene;bottleneck (software);central processing unit;code;elegant degradation;energy level;fastest;image scaling;message passing interface;network-attached storage;norm (social);parallel computing;run time (program lifecycle phase);scalability;simulation;supercomputer;synchronization (computer science)	Vincent W. Freeh;Feng Pan;Nandini Kappiah;David K. Lowenthal;Robert Springer	2005	19th IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2005.214	supercomputer;parallel computing;real-time computing;computer hardware;computer science;energy level;message passing interface;operating system;distributed computing;efficient energy use	HPC	-5.566716436346276	48.508752069169724	88546
7c06889d32a1bd35a1a01b6ef72c153f64757239	dynamic voltage and cache reconfiguration for low power	cache subsystem;combined dynamic voltage scaling;workload requirement;online algorithm;ideal voltage;low power;dynamically adapts;cache configuration;processor speed;dynamic cache reconfiguration;os scheduler;abstract syntax tree;optimization;real time;earliest deadline first;low power electronics	In this work, we propose a combined Dynamic Voltage Scaling (DVS) and Dynamic Cache Reconfiguration (DCR) online algorithm that dynamically adapts the processor speed (i.e., voltage) and the cache subsystem to the workload requirements for the purposes of saving energy. The workload is considered to be a set of tasks with real-time deadlines. Our online algorithm is invoked as part of the OS scheduler, which performs standard earliest deadline first (EDF) task scheduling first. Then, our online algorithm, determines an ideal voltage/cache configuration for the current executing task.	cache coherence;clock rate;dynamic voltage scaling;earliest deadline first scheduling;online algorithm;operating system;real-time clock;requirement;scheduling (computing)	André C. Nácul;Tony Givargis	2004	Proceedings Design, Automation and Test in Europe Conference and Exhibition		embedded system;online algorithm;mathematical optimization;cache-oblivious algorithm;parallel computing;real-time computing;earliest deadline first scheduling;cache coloring;cache;computer science;cache invalidation;operating system;smart cache;cache algorithms;cache pollution;abstract syntax tree;low-power electronics	EDA	-5.385927761210859	58.73292281917541	88680
495afcb2c4274b4e1da4283c4cce9e4b5f8cd400	designing next generation clusters: evaluation of infiniband ddr/qdr on intel computing platforms	intel architecture;multiprocessor interconnection networks;random access memory;hpcc randomly ordered ring bandwidth benchmark next generation clusters infiniband ddr qdr intel computing platform high performance computing i o requirement high speed interconnect nehalem intel architecture double data rate infiniband interconnects quad data rate;workstation clusters multiprocessor interconnection networks parallel architectures performance evaluation;hpcc randomly ordered ring bandwidth benchmark;double data rate;performance evaluation;nehalem;clocks;high performance computing;intel computing platform;qdr;data mining;next generation clusters;infiniband;computer architecture;parallel architectures;high performance computer;high speed interconnect;next generation;bandwidth;qdr infiniband nehalem;infiniband interconnects;workstation clusters;infiniband ddr qdr;nas parallel benchmarks;high speed;computer architecture power system interconnection bandwidth high performance computing performance gain computer science design engineering application software costs us department of energy;program processors;i o requirement;benchmark testing;quad data rate	"""Clusters based on commodity components continue to be very popular for high-performance computing (HPC).These clusters must be careful to balance both computational as well as I/O requirements of applications. This I/O requirement is generally fulfilled by a high-speed interconnect such as InfiniBand. The balance of computational and I/O performance is often changing, with the latest change being made by the Intel """"Nehalem"""" architecture that can dramatically increase computing power.In this paper we explore how this balance has changed and how different speeds of InfiniBand interconnects including Double Data Rate (DDR) and Quad Data Rate(QDR) InfiniBand HCAs. We explore micro benchmarks, the """"communication balance"""" ratio of intra-node to inter-node performance as well as end application performance. We show up to 10% improvement when using a QDR interconnect for Nehalem systems versus a DDR interconnection the NAS Parallel Benchmarks. We also see up to 25% performance gain with the HPCC randomly ordered ring bandwidth benchmark."""	benchmark (computing);computation;double data rate;electrical connection;hpcc;infiniband;input/output;intel core (microarchitecture);interconnection;nas parallel benchmarks;nehalem (microarchitecture);quad data rate sram;randomness;requirement;supercomputer;uncompressed video	Hari Subramoni;Matthew J. Koop;Dhabaleswar K. Panda	2009	2009 17th IEEE Symposium on High Performance Interconnects	10.1109/HOTI.2009.24	embedded system;benchmark;computer architecture;supercomputer;parallel computing;computer science;operating system;double data rate;bandwidth	HPC	-9.198832505220164	47.50839891277401	88768
3df536810cdd308ff2e7d6ed4fefbd1587c80f2d	investigating the performance impacts of i/o operations and disk cache on operating systems for wearables		Recent researches on wearable computing focus on hardware optimization as a way to support new evolutions. However, the software side also performs an essential role in this environment, raising the need to understand what its influence and its weight. This work presents an investigation concerning the performance of input/output operations and disk cache memory into wearables operating systems.	android wear;benchmark (computing);cpu cache;disk buffer;input/output;mathematical optimization;open-source software;operating system;page cache;sensor;wearable computer	Vicente J. P. Amorim;Saul E. Delabrida;Ricardo A. O. Oliveira	2018		10.1145/3167132.3167411	wearable computer;operating system;input/output;software;disk buffer;cpu cache;computer science	OS	-14.790918270172694	52.78036259868376	89052
2cc43e8d65fa4918e8dec1bffddc1344d807a0cf	on jitter in time partitioned real-time systems	engineering and technology;teknik och teknologier;scheduling;jitter;virtualisation;real time systems	Recent trends towards adopting hypervisors, hierarchical scheduling, and other virtualization technologies that achieve partitioned access to the CPU and other resources impose significant impact with respect to jitter performance in embedded real-time systems. In this paper we make a first step towards characterization, modeling and calculation of this jitter.	central processing unit;embedded system;hypervisor;interference (communication);partition problem;real-time clock;real-time computing;scheduling (computing);unavailability	Kristian Sandström;Thomas Nolte;Moris Behnam;Reinder J. Bril	2012	Proceedings of 2012 IEEE 17th International Conference on Emerging Technologies & Factory Automation (ETFA 2012)	10.1109/ETFA.2012.6489731	embedded system;parallel computing;real-time computing;jitter;computer science;operating system;scheduling	Embedded	-8.641688755773886	59.302680051114905	89263
1c5cafd8951fcd38cd221df7f8df0d9e9099d650	multiprocessor platform for partitioned real-time systems	communications;ada;real time;multiprocessors;posix;partitioned systems;operating systems	Two current trends in the real-time and embedded systems are the multiprocessor architectures and the partitioning technology that enables several isolated applications with different criticality levels to share the same computer. This paper presents a real-time platform for multiprocessor and partitioned systems, in which communication requirements are also considered. The paper describes the adaptation of MaRTE OS a monoprocessor real-time operating system to the XtratuM hypervisor for the multiprocessor Intel x86 architecture. This adaptation makes two contributions to ease the development process of future mixed-criticality applications: firstly, it integrates the hypervisor technology and the fully partitioned scheduling in a multiprocessor environment, and secondly, it provides the basis to interconnect partitioned and non-partitioned applications via a homogeneous communication subsystem. Copyright © 2016 John Wiley u0026 Sons, Ltd.	multiprocessing;real-time clock;real-time computing	Héctor Pérez;Mario Aldea Rivas;Daniel Medina Ortega	2017	Softw., Pract. Exper.	10.1002/spe.2404	computer architecture;parallel computing;real-time computing;ada;computer science;operating system;posix	Embedded	-8.715957815214917	57.770273882151635	89289
0b40a15285f0fc063718315cdbaf913ecbc33aac	improved tardiness bounds for global edf	multiprocessor platforms;program processors schedules scheduling algorithm additives optimal scheduling semantics;processor scheduling;earliest deadline first;tardiness bounds;semantics;implicit deadline sporadic tasks tardiness bounds earliest deadline first scheduling algorithm multiprocessor platforms;additives;earliest deadline first scheduling algorithm;implicit deadline sporadic tasks;scheduling algorithm;optimal scheduling;processor scheduling multiprocessing systems;schedules;multiprocessing systems;task scheduling;program processors	The Earliest Deadline First scheduling algorithm (EDF) is known to not be optimal under global scheduling on multiprocessor platforms. Results have been obtained that bound the maximum tardiness&#x2013; the amount of time by which deadlines may be missed&#x2013; of any feasible system of implicit-deadline sporadic tasks scheduled using global EDF. However, it is known that these bounds are not tight. In this paper, we derive an algorithm for obtaining tardiness bounds that are superior to previously known bounds. In contrast to prior algorithms, which compute a single tardiness bound for all the tasks in the system, our algorithm derives a separate tardiness bound for each task. Particularly upon task systems in which the parameters of the tasks are very dissimilar, our bounds are significantly better than prior bounds. Our algorithm also yields a simple sufficient test for the tardiness verification problem: given a task system with maximum acceptable tardiness bounds per task, is the system guaranteed to be scheduled by global EDF such that these tardiness constraints are not violated?	algorithm;earliest deadline first scheduling;experiment;multiprocessing;real-time clock;real-time computing;scheduling (computing);simulation	Jeremy P. Erickson;UmaMaheswari Devi;Sanjoy K. Baruah	2010	2010 22nd Euromicro Conference on Real-Time Systems	10.1109/ECRTS.2010.25	parallel computing;real-time computing;earliest deadline first scheduling;schedule;food additive;computer science;operating system;distributed computing;semantics;scheduling	Embedded	-10.707965000574282	60.43174651826085	89508
35e1e59ceb4985fa5482049e6c8810010ebe25e6	a performance analysis of a buddy system for fault tolerance	sistema fila espera;tolerancia falta;modelizacion;distributed system;systeme attente;evaluation performance;systeme reparti;performance evaluation;fault tolerant;queuing system;evaluacion prestacion;sistema informatico;computer system;modelisation;sistema repartido;fault tolerance;performance analysis;systeme informatique;modeling;tolerance faute	Abstract   A model for fault tolerant computing in a distributed computing system is presented and analyzed. Each time a job is submitted, two copies of it are stored: one at its original node, where it will normally be executed, and the other at a second node, called the buddy node. If the original node fails, the copy at the buddy node will be executed, providing fault tolerance. By means of an iterative procedure, the average queue length and the average response time may be calculated, with some simplifying assumptions. Comparison with simulation results shows excellent agreement. Numerical results are presented to show the effects of varying the parameters on the performance of the system.	buddy system;fault tolerance;profiling (computer programming)	David Finkel;Satish K. Tripathi	1990	Perform. Eval.	10.1016/0166-5316(90)90010-G	embedded system;fault tolerance;real-time computing;simulation;computer science	HPC	-18.770902942046153	46.45027777483489	89571
545a31bb5d24babe6695a719717a6bf26b4a5f4d	a kernel for energy-neutral real-time systems with mixed criticalities	real time systems energy consumption energy harvesting operating system kernels power aware computing;operating system kernel energy neutral real time systems energy constraints time constraints energy budget monitoring enos mixed time criticality levels energy criticality mode persistent memory;real time systems energy consumption batteries kernel time factors monitoring systems operation	Energy-neutral real-time systems harvest the entire energy they use from their environment, making it essential to treat energy as an equally important resource as time. As a result, such systems need to solve a number of problems that so far have not been addressed by traditional real-time systems. In particular, this includes the scheduling of tasks with both time and energy constraints, the monitoring of energy budgets, as well as the survival of blackout periods during which not enough energy is available to keep the system fully operational. In this paper, we address these issues presenting ENOS, an operating-system kernel for energy-neutral real-time systems. ENOS considers mixed time criticality levels for different energy criticality modes, which enables a decoupling of time and energy constraints during phases when one is considered less critical than the other. When switching the energy criticality mode, the system also changes the set of tasks to be executed and is therefore able to dynamically adapt its energy consumption depending on external conditions. By keeping track of the energy budget available, ENOS ensures that in case of a blackout the system state is safely stored to persistent memory, allowing operations to resume at a later point when enough energy is harvested again.	coupling (computer programming);criticality matrix;elegant degradation;interrupt;kernel (operating system);peripheral;persistent memory;real-time clock;real-time computing;real-time locating system;real-time transcription;scheduling (computing);self-organized criticality	Peter Wägemann;Tobias Distler;Heiko Janker;Phillip Raffeck;Volkmar Sieh	2016	2016 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)	10.1109/RTAS.2016.7461320	embedded system;real-time computing;simulation;operating system	Embedded	-6.279576367750922	57.968803211563824	89883
081057ccbe315ad97e3878053a9bc7e727654540	register allocation with instruction scheduling: a new approach	replication;register allocation;incremental collection;concurrent collection;real time garbage collection;instruction scheduling;copying garbage collection	We present a new framework in which considerations of both register allocation and instruction scheduling can be applied uniformly and simultaneously. In this framework an optimal coloring of a graph, called the parallel interference graph, provides an optimal register allocation and preserves the property that no false dependences are introduced, thus all the options for parallelism are kept for the scheduler to handle. For this framework we provide heuristics for trading off parallel scheduling with register spilling.	graph coloring;heuristic (computer science);instruction scheduling;interference (communication);parallel computing;register allocation	Shlomit S. Pinter	1993		10.1145/155090.155114	replication;parallel computing;real-time computing;computer science;instruction register;distributed computing;instruction scheduling;register allocation	Arch	-13.762694951536886	47.30871776320815	90263
5aaeddc38c65972fa54bcc93ff3f5c166c75be93	zero-copy protocol for mpi using infiniband unreliable datagram	libraries;protocols;reliability;message passing library;memory protocols;memory management;network protocol;transport protocols application program interfaces memory protocols message passing;unreliable datagram;connection oriented transport;infiniband user level networking technology;receivers;transport protocols;remote direct memory access;protocols receivers reliability libraries bandwidth memory management scalability;reliable connection;infiniband unreliable datagram transport;application program interfaces;message passing;optimal design;bandwidth;mpi;scalability;zero copy message transfer;zero copy message transfer zero copy protocol mpi infiniband unreliable datagram transport high performance computing system memory copy message passing library infiniband user level networking technology remote direct memory access connection oriented transport;zero copy protocol;high performance;high performance computing system;memory copy	Memory copies are widely regarded as detrimental to the overall performance of applications. High-performance systems make every effort to reduce the number of memory copies, especially the copies incurred during message passing. State of the art implementations of message-passing libraries, such as MPI, utilize user-level networking protocols to reduce or eliminate memory copies. InfiniBand is an emerging user-level networking technology that is gaining rapid acceptance in several domains, including HPC. In order to eliminate message copies while transferring large messages, MPI libraries over InfiniBand employ ldquozero-copyrdquo protocols which use remote direct memory access (RDMA). RDMA is available only in the connection-oriented transports of InfiniBand, such as reliable connection (RC). However, the unreliable datagram (UD) transport of InfiniBand has been shown to scale much better than the RC transport in regard to memory usage. In an optimal design, it should be possible to perform zero-copy message transfers over scalable transports (such as UD). In this paper, we present our design of a novel zero-copy protocol which is directly based over the scalable UD transport. Thus, our protocol achieves the twin objectives of scalability and good performance. Our analysis shows that uni-directional messaging bandwidth can be within 9% of what is achievable over RC for messages of 64 KB and above. Application benchmark evaluation shows that our design delivers a 21% speedup for the in.rhodo dataset for LAMMPS over a copy-based approach, giving performance within 1% of RC.	benchmark (computing);communications protocol;connection-oriented communication;datagram;hotspot (wi-fi);infiniband;library (computing);message passing interface;nas parallel benchmarks;network interface controller;optimal design;remote direct memory access;scalability;speedup;throughput;urban dictionary;user space;zero-copy	Matthew J. Koop;Sayantan Sur;Dhabaleswar K. Panda	2007	2007 IEEE International Conference on Cluster Computing	10.1109/CLUSTR.2007.4629230	communications protocol;parallel computing;computer science;operating system;distributed computing;computer network	HPC	-10.470354991257212	47.018588334645116	90283
3c6668b0daa4e8d16f79556ebb7e61b3d0147814	hybrid aggregates: combining ssds and hdds in a single storage pool	disk failure;rdp code;recovery algorithm;raid recovery	Relative to traditional hard disk drives (HDDs), solid state drives (SSDs) provide a very large number of I/Os per second, but they have limited capacity. From a cost-effectiveness perspective, SSDs provide significantly better random I/O throughput per dollar than a typical disk, but the capacity provided per dollar spent on SSDs limits them to the most demanding of datasets. Traditionally, Data ONTAP® storage aggregates have been provisioned using a single type of disk. This restriction limits the costeffectiveness of the storage pool to that of the underlying disks.  The Hybrid Aggregates project within the Advanced Technology Group (ATG) explored the potential to combine multiple disk types within a single aggregate. One of the primary goals of the project was to determine whether a hybrid aggregate, composed of SSDs (for their cost-effective performance) and Serial-ATA (SATA) disks (for their cost-effective capacity), could simultaneously provide better cost/performance and cost/throughput ratios than an all Fibre-Channel (FC) solution.  The project has taken a two-pronged approach to building a prototype system capable of supporting hybrid aggregates. The first part of the project investigated the changes necessary for Data ONTAP RAID and WAFL® layers to support a hybrid aggregate. This included propagating disk-type information to WAFL, modifying WAFL to support the allocation of blocks from a particular storage class (i.e., disk type), and repurposing the existing writeafter- read and segment-cleaning infrastructure to support the movement of data between storage classes. The second part of the project examined potential policies for allocating and moving data between storage classes within a hybrid aggregate. Through proper policies, it is possible to automatically segregate the data within the aggregate such that the SSD-backed portion of the aggregate absorbs a large fraction of the I/O requests, leaving the SATA disks to contribute capacity for colder data.  This paper describes the implementation of the Hybrid Aggregates prototype and the policies for automatic data placement and movement that have been evaluated. It also presents some performance results from the prototype system.	acm transactions on computer systems;adobe flash;aggregate data;c syntax;conquest;dave lebling;fibre channel;glider (conway's life);hard disk drive;hierarchical storage management;ieee/acm transactions on networking;input/output;international standard book number;international standard serial number;logo;netapp filer;paul brainerd;plasma cleaning;prototype;raid;random-access memory;run-length encoding;scalability;serial ata;server (computing);solid-state drive;systems design;throughput;web cache	John D. Strunk	2012	Operating Systems Review	10.1145/2421648.2421656	real-time computing;computer hardware;operating system	OS	-12.804177345759358	52.63789065435387	90380
87e25e08f0b0e2e6de87b519c6b8d60fb41a31b4	improving the accuracy of dynamic branch prediction using branch correlation	branch prediction;prediction accuracy;high performance	Long branch delay is a well–known problem in today’s high performance superscalar and supetpipeline processor designs. A common technique used to alleviate this problem is to predict the direction of branches during the instruction fetch. Counter-based branch prediction, in particular, has been reported as an effective scheme for predicting the direction of branches. However, its accuracy is generally limited by branches whose future behavior is also dependent upon the history of other branches. To enhance branch prediction accuracy with a minimum increase in hardware COSLwe propose a correlation-based scheme and show how the prediction accuracy can be improved by incorporating information, not only from the history of a specific brsncb but also from the history of other branches. Specifically, we use the information provided by a proper subhistory of a branch to predict the outcome of that branch. The proper subhistory is selected based on the outcomes of the most recently executed M branches. The new scheme is evaluated using traces collected from running the SPEC benchmark suite on an IBM RISC System/6000 workstation. The results show that, ascompared with the 2-bit counter-based prediction scheme, the correlation-based branch prediction achieves up to 11 ~0 additional accuracy at the extra hardware cost of one shift register. The results also show that the accuracy of the new scheme surpasses that of the counter–based branch predction at saturation.	benchmark (computing);branch predictor;color depth;rs/6000;shift register;superscalar processor;tracing (software);workstation	Shien-Tai Pan;Kimming So;Joseph T. Rahmeh	1992		10.1145/143365.143490	computer science;operating system;branch predictor	Arch	-7.1904602317044475	51.44465565046151	90547
cb15134e3d0520dbfc3ee6e4eeb0d49ccd284d03	workload-adaptation in memory controllers	memory controller	13 Declaration 15 Copyright 16 Acknowledgements 17	memory controller	Mohsen Ghasempour	2015			memory refresh;registered memory	Logic	-11.193837042327267	52.537244951482734	90561
a255ae0171555407dc6bb325e4f7624e7e84aa50	${\rm s}^{2}$-raid: parallel raid architecture for fast data recovery	software;raid reconstruction;parallel reconstruction;i o traces;storage management;prototypes;raid;disk failure;raid50;layout;raid5 software;disk volume;arrays;data storage;data reconstruction process;microsoft;parallel architectures;rm s 2 raid;s 2 raid system;production;parity declustering;linux;fast data recovery;parallel raid architecture;data loss;linux operating system;raid reconstruction process;parallel processing;disk array	As disk volume grows rapidly with terabyte disk becoming a norm, RAID reconstruction process in case of a failure takes prohibitively long time. This paper presents a new RAID architecture, S2-RAID, allowing the disk array to reconstruct very quickly in case of a disk failure. The idea is to form skewed sub-arrays in the RAID structure so that reconstruction can be done in parallel dramatically speeding up data reconstruction process and hence minimizing the chance of data loss. We analyse the data recovery ability of this architecture and show its good scalability. A prototype S2-RAID system has been built and implemented in the Linux operating system for the purpose of evaluating its performance potential. Real world I/O traces including SPC, Microsoft, and a collection of a production environment have been used to measure the performance of S2-RAID as compared to existing baseline software RAID5, Parity Declustering, and RAID50. Experimental results show that our new S2-RAID speeds up data reconstruction time by a factor 2 to 4 compared to the traditional RAID. Meanwhile, S2-RAID keeps comparable production performance to that of the baseline RAID layouts while online RAID reconstruction is in progress.	baseline (configuration management);data recovery;deployment environment;disk array;disk space;disk storage;hot spare;iscsi;input/output;linux;nested raid levels;operating system;prototype;scalability;standard raid levels;terabyte;tracing (software)	Jiguang Wan;Jibin Wang;Changsheng Xie;Qing Yang	2014	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2013.225	layout;parallel processing;parallel computing;data loss;disk array;computer hardware;computer science;operating system;computer data storage;volume;prototype;linux kernel;nested raid levels;raid	OS	-15.283997473364918	51.989314635769645	90580
1a79b96812bf17ea229cdbdb8b6e80735dae192c	data migration algorithms in heterogeneous storage systems: a comparative performance evaluation		In large scale storage systems such as data centers, data layouts need to be reconfigured over time for load balancing or in the event of system failure/upgrades. The data-migration problem pertains to computing an efficient plan to migrate data to their target locations. Most of the previous results on data-migration assume that storage devices have similar capabilities and can perform only one data transfer at a time. In this paper, we consider the heterogeneous data-migration problem where we associate a transfer constraint to each of the storage nodes, representing the number of simultaneous transfers that each of the nodes can handle. We introduce new data-migration algorithms for the heterogeneous case and perform an empirical comparative study of the performance of these algorithms against algorithms from [1] and [2].	algorithm;data center;greedy algorithm;load balancing (computing);matching (graph theory);performance evaluation;random graph;randomized algorithm;regular expression	Gary Roberts;Sixia Chen;Chadi Kari;Vivek K. Pallipuram	2017	2017 IEEE 16th International Symposium on Network Computing and Applications (NCA)	10.1109/NCA.2017.8171340	computer science;data migration;scheduling (computing);data transmission;approximation algorithm;distributed computing;algorithm;schedule;load balancing (computing)	Arch	-14.736576577596857	57.47430259897848	90594
2f8f0e3f3718cbf5333c77a1bb5b4a23f7aaa2d9	flashb-tree: a novel b-tree index scheme for solid state drives	data storage;solid state drive;index manager;online transition;indexation;tree structure;systems and applications;b tree;hard disk drive;low power consumption;flashb tree;service life	Solid State Drive (SSD) is rapidly deployed as data storage for embedded and tablet computers due to its shock resistance, fast access, and low power consumption. However, it has some intractable characteristics such as erase-before-write, asymmetric read/write/erase speed, and limited number of write/erase cycles. Due to these hardware characteristics, the hard disk drive (HDD)-based systems and applications could hardly make full use of the advantages of SSD when directly adopting themselves on it. In addition, the frequent changes of B-tree can degrade the performance of SSD and reduce the service life. Most previous works have been battling these thorny problems and improved the performance to some extent. In this paper, we first analyze two existing mechanisms which are suitable to write-intensive and read-intensive workloads, respectively. And we pointed out the drawbacks of them. As a solution, we propose a novel B-tree index implementation scheme, a FlashB-tree, which eliminates the number of reorganizing of the tree structure. And we adopt Online Transition Algorithm for utilizing the superiority of two representative mechanisms. With several indexing profiles having different mixture of insert, select and delete operations of indices, we measured the access time performance, it is sure that the proposed methodology could significantly enhance the efficiency of using index on SSDs.	access time;algorithm;b-tree;computer data storage;disk storage;embedded system;hard disk drive;solid-state drive;tablet computer;tree structure	Rize Jin;Se Jin Kwon;Tae-Sun Chung	2011		10.1145/2103380.2103390	real-time computing;computer hardware;computer science;operations management;fractal tree index	OS	-12.60653878077786	54.575970628537135	90619
ece55aa4aebaa7cb76c83e53f5fa439e620c86ce	ghost: gpgpu-offloaded high performance storage i/o deduplication for primary storage system	paper;storage system;cost saving;primary storage;performance;cloud;nvidia geforce gtx 480;gpgpu;data cache;deduplication;nvidia;computer science;high performance	Data deduplication has been an effective way to eliminate redundant data mainly for backup storage systems. Since the recent primary storage systems in cloud services are expected to have the redundancy, the deduplication technique can also bring significant cost saving for the primary storage. However, the primary storage system requires high performance requirement about several GBs per second. Most conventional deduplication techniques targeted the performance requirement of 200-300MB/s.  In an attempt to achieve a high performance storage deduplication system at the primary storage, we thoroughly analyze the performance bottleneck of previous deduplication systems to enhance the system to meet the requirement of the primary storage. The new performance bottleneck of deduplication in the primary storage lies on not only key-value store lookup, also computation for data segmentation and fingerprinting due to recent technology improvement of flash devices such as SSD. To overcome the bottlenecks, we propose a new deduplication system utilizing GPGPU. Our proposed system, termed GHOST, includes the followings to offload and optimize the deduplication processing in GPGPU: (1) In-Host Data Cache, (2) Destage-aware Data offloading to GPGPU and (3) In-GPGPU Table Cache of key-value store. These techniques improve the offloaded deduplication processing about 10-20% on the reasonable workload of the primary storage compared to the naive approach. Our proposed deduplication system can achieve 1.5GB/s in maximum which is about 5 times of the deduplication systems used CPU only.	attribute–value pair;backup;central processing unit;cloud computing;computation;computer data storage;data deduplication;fingerprint (computing);general-purpose computing on graphics processing units;gigabyte;input/output;key-value database;lookup table;solid-state drive	Chulmin Kim;Ki-Woong Park;Kyu Ho Park	2012		10.1145/2141702.2141705	parallel computing;data deduplication;computer hardware;computer science;operating system	OS	-13.30126712457546	54.68526326573939	90753
63c4795a7dbfa42f89f3d38b2930bb93a9bd2325	efficient manipulation of large datasets on heterogeneous storage systems	storage system;perforation;large dataset;filters;isosurfaces;parallel processing subcontracting filters data visualization laboratories isosurfaces dynamic scheduling pipelines computer science educational institutions;pipelines;data visualization;computer science;subcontracting;parallel processing;dynamic scheduling	In this paper we are concerned with the efficient use of a collection of disk-based storage systems and computing platforms in a heterogeneous setting for retrieving and processing large scientific datasets. We demonstrate, in the context of a data-intensive visualization application, how heterogeneity affects performance and show a set of optimization techniques that can be used to improve performance in a component-based framework. In particular, we examine the application of parallelism via transparent copies of application components in the pipelined processing of data.	component-based software engineering;data-intensive computing;isosurface;load (computing);mathematical optimization;parallel computing	Michael D. Beynon;Tahsin M. Kurç;Ümit V. Çatalyürek;Alan Sussman;Joel H. Saltz	2002		10.1109/IPDPS.2002.1015655	parallel processing;parallel computing;dynamic priority scheduling;computer science;theoretical computer science;operating system;database;distributed computing;pipeline transport;data visualization	HPC	-17.00713322913097	53.768915789233006	90775
628d97590a3588a9c14654ce475341005aff0448	a dynamic voltage scaling algorithm for dynamic-priority hard real-time systems using slack time analysis	comparable energy saving;dvs algorithm largelydepends;slack time analysis;dynamic voltage scaling algorithm;novel dvs;improved slackestimation algorithm;energy consumption;slack estimation;thatour algorithm;theexisting dvs algorithm;dynamic-priority hard real-time systems;improved slack estimation;energy efficiency;circuits;timing analysis;low power electronics;microcomputers;drams;energy efficient;real time systems;embedded systems;real time;threshold voltage	Dynamic voltage scaling (DVS), which adjusts the clockspeed and supply voltage dynamically, is an effective techniquein reducing the energy consumption of embedded real-timesystems. The energy efficiency of a DVS algorithm largelydepends on the performance of the slack estimation methodused in it. In this paper, we propose a novel DVS algorithmfor periodic hard real-time tasks based on an improved slackestimation algorithm. Unlike the existing techniques, the proposedmethod takes full advantage of the periodic characteristicsof the real-time tasks under priority-driven schedulingsuch as EDF. Experimental results show that the proposed algorithmreduces the energy consumption by 20~40% over theexisting DVS algorithm. The experiment results also show thatour algorithm based on the improved slack estimation methodgives comparable energy savings to the DVS algorithm basedon the theoretically optimal (but impractical) slack estimationmethod.	algorithm;clock rate;dynamic voltage scaling;earliest deadline first scheduling;embedded system;image scaling;real-time clock;real-time computing;slack variable	Woonseok Kim;Jihong Kim;Sang Lyul Min	2002			embedded system;parallel computing;real-time computing;computer science;operating system;efficient energy use;least slack time scheduling	EDA	-5.0593962306316245	58.753077293830756	90799
fd197974afeb14200434bb5284812957a478ef51	input/output characteristics of scalable parallel applications	output request;individual application input;access pattern;ongoing parallel input;output system;output structure;output requirement;output characterization effort;output characteristic;output volume;scalable parallel application;electron scattering;electrons;application software;intrusion detection;hacking;environmental economics;application development;chemistry;quantum chemistry;computer security;anomaly detection;computer science;concurrent computing;spectrum;terrain rendering;auditing;input output	Rapid increases in computing and communication performance are exacerbating the long-standing problem of performance-limited input/output. Indeed, for many otherwise scalable parallel applications. input/output is emerging as a major performance bottleneck. The design of scalable input/output systems depends critically on the input/output requirements and access patterns for this emerging class of large-scale parallel applications. However, hard data on the behavior of such applications is only now becoming available. In this paper, we describe the input-output requirements of three scalable parallel applications (electron scattering, terrain rendering, and quantum chemistry, on the Intel Paragon XP/S. As part of an ongoing parallel input/output characterization effort, we used instrumented versions of the application codes to capture and analyze input/output volume, request size distributions, and temporal request structure. Because complete traces of individual application input/output requests were captured, in-depth, off-line analyses were possible. In addition, we conducted informal interviews of the application developers to understand the relation between the codes' current and desired input/output structure. The results of our studies show a wide variety of temporal and spatial access patterns, including highly read-intensive and write-intensive phases, extremely large and extremely small request sizes, and both sequential and highly irregular access patterns. We conclude with a discussion of the broad spectrum of access patterns and their profound implications for parallel file caching and prefetching schemes.	input/output;scalability	Phyllis E. Crandall;Ruth A. Aydt;Andrew A. Chien;Daniel A. Reed	1995		10.1109/SUPERC.1995.39	intrusion detection system;input/output;spectrum;hacker;terrain rendering;anomaly detection;application software;parallel computing;real-time computing;simulation;electron scattering;concurrent computing;computer science;electron;theoretical computer science;operating system;rapid application development;audit;quantum chemistry;quantum mechanics	HPC	-15.969632577789772	49.7180037328839	90926
259ec2f8eaee4d58ab3e8c71efd35dcbf6b48113	muclouds: parallel simulator for large-scale cloud computing systems	big data processing;virtualization;virtualization parallel simulation cloud computing datacenter big data processing;datacenter;interacting events muclouds parallel simulation framework large scale cloud computing systems stage and partition technique sap event based simulation framework simulation synchronization noninteracting events;synchronisation cloud computing discrete event simulation parallel processing;computational modeling cloud computing correlation servers instruction sets load modeling conferences;parallel simulation;cloud computing	This paper introduces Muclouds, a parallel simulation framework for simulating large-scale cloud computing systems. Muclouds improves the simulation performance for these large systems, such that the simulation time can be minimized. To achieve this goal, in this paper we propose a novel technique named stage and partition (SAP). SAP is based on the following observation: in an event-based simulation framework for cloud computing, the existence of the interaction and/or correlation of the events, which imposes simulation synchronization, is the major factor that prevents efficient parallelization. To achieve improved parallelization performance, SAP divides events into two main categories: non-interacting events and interacting events. Non-interacting events can be simulated simultaneously to achieve high simulation speed, whereas interacting events must be simulated sequentially to preserve accuracy. Using Muclouds, we constructed several scenarios to evaluate parallelization effectiveness and efficiency. We also conduct several experiments for performance comparisons with state-of-the-art schemes. Evaluation results show that Muclouds facilitates considerable performance improvements when compared with existing approaches.	big data;central processing unit;cloud computing;cloudsim;experiment;interaction;multi-core processor;parallel computing;scalability;simulation	Jinzhao Liu;Yue-Zhi Zhou;Di Zhang;Yujian Fang;Wei Han;Yaoxue Zhang	2014	2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops	10.1109/UIC-ATC-ScalCom.2014.104	embedded system;data center;real-time computing;virtualization;cloud computing;computer science;theoretical computer science;operating system;cloud testing;database;distributed computing	HPC	-16.85303310179028	57.43421969792304	90983
1f3b006186e716cde783cae489056ff08d6b8f1c	job aware scheduling algorithm for mapreduce framework	machine learning algorithms;cluster algorithm;scheduling learning artificial intelligence;availability;runtime;large scale;scheduling algorithm;vectors;machine learning;scheduling;heuristic algorithms;machine learning algorithms runtime machine learning availability clustering algorithms heuristic algorithms vectors;machinelearning;clustering algorithms;yahoo capacity scheduler job aware scheduling algorithm mapreduce framework large scale computing batch oriented workloads machine learning based solutions;mapreduce;learning artificial intelligence;job scheduling;heuristic algorithm;mapreduce cloud computing job scheduling machinelearning;cloud computing	MapReduce framework has received a wide acclaim over the past few years for large scale computing. It has become a standard paradigm for batch oriented workloads. As the adoption of this paradigm has increased rapidly, scheduling of these MapReduce jobs has become a problem of great interest in research community. We propose an approach which tries to maintain harmony among the jobs running on the cluster, and in turn decrease their runtime. In our model, the scheduler is made aware of different types of jobs running on the cluster. The scheduler tries to allocate a task on a node if the incoming task does not affect the tasks already running on that node. From the list of available pending tasks, our algorithm selects the one that is most compatible with the tasks already running on that node. We bring up heuristic and machine learning based solutions to our approach and try to maintain a resource balance on the cluster by not overloading any of the nodes, thereby reducing the overall runtime of the jobs. The results show a saving of runtime of around 21% in the case of heuristic based approach and around 27% in the case of machine learning based approach when compared to Yahoo's Capacity scheduler.	algorithm;function overloading;heuristic;job stream;machine learning;mapreduce;programming paradigm;race condition;scheduling (computing);selection algorithm;virtual machine	Radheshyam Nanduri;Nitesh Maheshwari;A. Reddyraja;Vasudeva Varma	2011	2011 IEEE Third International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2011.112	parallel computing;real-time computing;computer science;operating system;distributed computing;scheduling	HPC	-17.30708835298205	57.828235427907025	91028
e160c714a266a1e0b73a01637e28769219c7745a	interconnection topologies for shared nothing dbms architectures			interconnection;shared nothing architecture	Martin Hitz;Thomas A. Mück	1993			database;computer science;shared nothing architecture;interconnection;network topology;distributed computing	EDA	-18.412769271412518	52.386516300734314	91056
8cfa1cb394fbaa22ec31805cbdff424f924acadb	accelerating inclusion-based pointer analysis on heterogeneous cpu-gpu systems	graph theory;program diagnostics;graphics processing units semantics australia benchmark testing weaving;c language;rewriting systems;synchronisation free execution heterogeneous cpu gpu system andersen inclusion based pointer analysis c programs graph algorithm workload imbalance dynamic workload distribution scheme graph rewriting rules;graphics processing units;rewriting systems c language graph theory graphics processing units program diagnostics	This paper describes the first implementation of Andersen's inclusion-based pointer analysis for C programs on a heterogeneous CPU-GPU system, where both its CPU and GPU cores are used. As an important graph algorithm, Andersen's analysis is difficult to parallelise because it makes extensive modifications to the structure of the underlying graph, in a way that is highly input-dependent and statically hard to analyse. Existing parallel solutions run on either the CPU or GPU but not both, rendering the underlying computational resources underutilised and the ratios of CPU-only over GPU-only speedups for certain programs (i.e., graphs) unpredictable. We observe that a naive parallel solution of Andersen's analysis on a CPU-GPU system suffers from poor performance due to workload imbalance. We introduce a solution that is centered around a new dynamic workload distribution scheme. The novelty lies in prioritising the distribution of different types of workloads, i.e., graph-rewriting rules in Andersen's analysis to CPU or GPU according to the degrees of the processing unit's suitability for processing them. This scheme is effective when combined with synchronisation-free execution of tasks (i.e., graph-rewriting rules) and difference propagation of points-to information between the CPU and GPU. For a set of seven C benchmarks evaluated, our CPU-GPU solution outperforms (on average) (1) the CPU-only solution by 50.6%, (2) the GPU-only solution by 78.5%, and (3) an oracle solution that behaves as the faster of (1) and (2) on every benchmark by 34.6%.	algorithm;benchmark (computing);central processing unit;computational resource;constraint graph;directed graph;graph rewriting;graphics processing unit;list of algorithms;pointer (computer programming);pointer analysis;software propagation;sparse matrix	Yu Su;Ding Ye;Jingling Xue	2013	20th Annual International Conference on High Performance Computing	10.1109/HiPC.2013.6799110	parallel computing;real-time computing;computer science;graph theory;theoretical computer science;operating system;distributed computing;programming language	HPC	-13.397390936229105	47.192128402479796	91110
47b3ed49e934e6d0f8ed5aa2833ec6af0605aee6	thermal-aware task scheduling for peak temperature minimization under periodic constraint for 3d-mpsocs	three dimensional displays optimization heat sinks mathematical model solid modeling algorithm design and analysis temperature distribution;thermal optimization thermal aware task scheduling peak temperature minimization periodic constraint 3d mpsoc vertical thermal correlation power density intelligent task mapping dvfs 3d architecture;system on chip logic design minimisation multiprocessing systems;three dimensional displays;solid modeling;mathematical model;optimization;heat sinks;temperature distribution;algorithm design and analysis	3D-MPSoC offer great performance and scalability benefits. However, due to strong vertical thermal correlation and increased power density, thermal challenges in 3D-MPSoC are critical. In this paper, we propose a novel thermal aware task scheduling technique that combine intelligent task mapping with DVFS to minimize the peak temperature of the system. Particularly, our approach leverages on the fundamental thermal characteristics of 3D architecture when mapping tasks to processing cores and employing DVFS at design time followed by a simple thermal optimization step at run time. Our experiments validate the efficiency of our approach in peak temperature minimization up to 14°C compared to other existing methods.	dynamic voltage scaling;experiment;mpsoc;mathematical optimization;run time (program lifecycle phase);scalability;scheduling (computing)	Vivek Chaturvedi;Amit Kumar Singh;Wei Zhang;Thambipillai Srikanthan	2014	2014 25nd IEEE International Symposium on Rapid System Prototyping	10.1109/RSP.2014.6966900	algorithm design;mathematical optimization;parallel computing;real-time computing;computer science;mathematical model;heat sink;solid modeling	EDA	-4.984903099354841	57.88656850822796	91283
4690d9ec168f39336d0befe11cc63f026425adb8	a distributed data replication protocol for file versioning with optimal node assignments	distributed data;analytical models;protocols resource management analytical models cloud computing distributed databases scalability memory;protocols;optimal resource allocation;read requests;resource allocation;simulation;resource management;data replication;read requests distributed data replication protocol optimal node assignment cloud storage service file versioning subscription service optimal resource allocation;simulation cloud storage data replication file versioning optimal node assignment;optimal node assignment;subscription service;distributed databases;resource allocation cloud computing file organisation;file versioning;scalability;cloud storage;cloud storage service;memory;analytical model;distributed data replication protocol;cloud computing;file organisation	Some cloud storage services have recently introduced file versioning features by which more than one version of a file can be maintained. For providing file versioning with limited storage resources, it is essential to divide the resources among versions in accordance with the varied needs of numerous users. In this paper, we focus on applications in which newer versions of a file are more likely to be requested, which may be true in the case of many subscription services. We propose a new distributed data replication protocol supporting the file versioning feature. We also construct an analytical model that can derive an optimal allocation of the resources when the total number of replica nodes in a system and the distribution of the frequency of read requests for each version are given. In addition, we present some numerical examples obtained by simulations to show the good scalability and dependability of our system by assuming some realistic parameters.	blocking (computing);cloud storage;computation;dependability;mathematical optimization;monte carlo method;nonlinear system;numerical analysis;overhead (computing);replication (computing);scalability;simulation	Takahiko Ikeda;Mamoru Ohara;Satoshi Fukumoto;Masayuki Arai;Kazuhiko Iwasaki	2010	2010 IEEE 16th Pacific Rim International Symposium on Dependable Computing	10.1109/PRDC.2010.40	fork;self-certifying file system;communications protocol;scalability;torrent file;device file;cloud computing;resource allocation;computer science;stub file;versioning file system;resource management;operating system;unix file types;ssh file transfer protocol;journaling file system;database;distributed computing;open;distributed file system;memory;file system fragmentation;replication;computer network	HPC	-18.475076314956556	60.36624033799828	91343
18fc1d07e5e9887f246cfb1d9a5543c88d66f571	software aging analysis of the linux operating system	analytical models;program diagnostics;kernel;memory management;linux kernel;trend analysis software aging linux kernel;software system;statistical analysis linux operating system kernels program diagnostics safety critical software;software aging analysis;radiation detectors;software systems;trend analysis;aging dynamics;reliability of linux kernel;aging;aging kernel memory management linux radiation detectors analytical models;kernel subsystem;statistical analysis;operating system;aging related bugs;degrading performance;kernel tracing tool;safety critical software;failure occurrence rate;aging dynamics software aging analysis linux operating system software system degrading performance failure occurrence rate safety critical application aging related bugs linux kernel kernel internal behaviour kernel tracing tool kernel subsystem statistical analysis;safety critical application;linux;kernel internal behaviour;operating system kernels;linux operating system;software reliability;software aging	Software systems running continuously for a long time tend to show degrading performance and an increasing failure occurrence rate, due to error conditions that accrue over time and eventually lead the system to failure. This phenomenon is usually referred to as \textit{Software Aging}. Several long-running mission and safety critical applications have been reported to experience catastrophic aging-related failures. Software aging sources (i.e., aging-related bugs) may be hidden in several layers of a complex software system, ranging from the Operating System (OS) to the user application level. This paper presents a software aging analysis at the Operating System level, investigating software aging sources inside the Linux kernel. Linux is increasingly being employed in critical scenarios; this analysis intends to shed light on its behaviour from the aging perspective. The study is based on an experimental campaign designed to investigate the kernel internal behaviour over long running executions. By means of a kernel tracing tool specifically developed for this study, we collected relevant parameters of several kernel subsystems. Statistical analysis of collected data allowed us to confirm the presence of aging sources in Linux and to relate the observed aging dynamics to the monitored subsystems behaviour. The analysis output allowed us to infer potential sources of aging in the kernel subsystems.	applicative programming language;booting;catastrophic interference;data structure;design of experiments;experiment;kernel (operating system);linux;memory management;operating system;reboot (computing);scheduling (computing);software aging;software bug;software rejuvenation;software system;system call	Domenico Cotroneo;Roberto Natella;Roberto Pietrantuono;Stefano Russo	2010	2010 IEEE 21st International Symposium on Software Reliability Engineering	10.1109/ISSRE.2010.24	embedded system;real-time computing;computer science;operating system;software aging;linux kernel;software system	Embedded	-17.74415592957119	48.01539341234685	91444
712752a4d481269513016a79b3a521bec6328625	the tofu interconnect 2	mainframes;optical transceivers;parallel machines;fujitsu supercomputer prime hpc fx10;tofu interconnect 2;bit rate 25 gbit/s;byte rate 11.46 gbyte/s;byte rate 45.82 gbyte/s;one-way communication;optical transceivers;physical transport medium;successor model;system interconnect;atomic operation;cache injection;high-performance computing;interconnect;mutual atomicity;non-blocking collective communication;system-on-chip	The Tofu Interconnect 2 (Tofu2) is a system interconnect designed for the successor model of the FUJITSU Supercomputer PRIME HPC FX10. Tofu2 uses a 25 Gbps transmission technology that is about two times faster than those of existing HPC interconnects, and uses optical transceivers in an extremely high ratio. Despite the major change of physical transport medium, Tofu2 inherits and enhances the features of Tofu1. This paper describes the specifications including frame format, implementation, and preliminary evaluation results of Tofu2. The effective throughput of Put transfer was evaluated to be 11.46 GB/s that was about 92% link efficiency. The total throughput of simultaneous 4-way transfer was evaluated to be 45.82 GB/s. Tofu2 reduced one-way communication latency by about 0.2 usec. The elimination of the host bus contributed about 60 nsec of reduction, and the rest of the reduction had been derived from the cache injection technique.	data rate units;electrical connection;gigabyte;intel quickpath interconnect;one-way function;supercomputer;throughput;torus interconnect;transceiver	Yuichiro Ajima;Tomohiro Inoue;Shinya Hiramoto;Shun Ando;Masahiro Maeda;Takahide Yoshikawa;Koji Hosoe;Toshiyuki Shimizu	2014	2014 IEEE 22nd Annual Symposium on High-Performance Interconnects	10.1109/HOTI.2014.21	system on a chip;high-throughput screening;electricity generation;embedded system;three-dimensional space;benchmark;routing;throughput;supercomputer;parallel computing;real-time computing;linearizability;telecommunications;computer science;network interface;operating system;interconnection;processor register;optical switch;network topology;computer network;transceiver	Arch	-10.704833331896205	46.75981193826856	91560
3e4e28acb63181bcad1acde661c8c69879cf865d	characterizing load and communication imbalance in parallel applications	trace analysis;juser;public records;parallel programming;websearch;informatik;performance analysis;mpi	The amount of parallelism in modern supercomputers currently grows from generation to generation, and is expected to reach orders of millions of processor cores in a single system in the near future. Further application performance improvements therefore depend to a large extend on software-managed parallelism: in particular, the software must organize data exchange between processing elements efficiently and optimally distribute the workload between them. Performance analysis tools help developers of parallel applications to evaluate and optimize the parallel efficiency of their programs by pinpointing specific performance bottlenecks. However, existing tools are often incapable of identifying complex imbalance patterns and determining their performance impact reliably. This dissertation presents two novel methods to automatically extract imbalance-related performance problems from event traces generated by MPI programs and intuitively guide the performance analyst to inefficiencies whose optimization promise the highest benefit. The first method, the delay analysis, identifies the root causes of wait states. A delay occurs when a program activity needs more time on one process than on another, which leads to the formation of wait states at a subsequent synchronization point. Wait states, which are intervals through which a process is idle while waiting for the delayed process, are the primary symptom of load imbalance in parallel programs. While wait states themselves are easy to detect, the potentially large temporal and spatial distance between wait states and the delays causing them complicates the identification of wait-state root causes. The delay analysis closes this gap, accounting for both short-term and long-term effects. To this end, the delay analysis comprises two contributions of this dissertation: (1) a cost model and terminology to describe the severity of a delay in terms of the overall waiting time it causes; and (2) a scalable algorithm to identify the locations of delays and determine their cost. The second new analysis method is based on the detection of the critical path. In contrast to the delay analysis, which characterizes the formation of wait states, this critical-path analysis determines the effect of imbalance on program runtime. The critical path is the longest execution path in a parallel program without wait states: optimizing an activity on the critical path will reduce the programs run time. Comparing the duration of activities on the critical path with their duration on each process yields a set of novel, compact performance indicators. These indicators allow users to evaluate load balance, identify performance bottlenecks, and determine the performance impact of load imbalance at first glance by providing an intuitive understanding of complex performance phenomena. Unlike existing statistics-based load balance metrics, these indicators are applicable to both SPMD and MPMD-style programs. Both analysis methods leverage the scalable event-trace analysis technique employed by the Scalasca toolset: by replaying event traces in parallel, the bottleneck search algorithms can harness the distributed memory and computational resources of the target system for the analysis, allowing them to process even large-scale program runs. The scalability and performance insight that the novel analysis approaches provide are demonstrated by evaluating a variety of real-world HPC codes in configurations with up to 262,144 processor cores.	analysis of algorithms;bottleneck (software);code;computational resource;critical path method;distributed memory;flynn's taxonomy;load balancing (computing);mathematical optimization;message passing interface;parallel computing;path analysis (statistics);run time (program lifecycle phase);spmd;scalability;search algorithm;speedup;supercomputer;synchronization (computer science);tracing (software);wait state	David Böhme	2014			parallel computing;computer science;theoretical computer science;distributed computing	HPC	-17.167720132917143	48.262508630577635	91698
fcfa9e860eeec236238cec860eb218ec2c808492	using intradisk parallelism to build energy-efficient storage systems	energy conservation;disc drives;storage management disc drives energy conservation power consumption;storage system;magnetic heads;cost disk drives storage parallelism power;energy efficient;storage management;disk drives;data mining;drives;assembly;parallelism;time factors;energy efficiency energy storage parallel processing disk drives energy consumption assembly aggregates magnetic recording arm clocks;power consumption;cost;i o request stream intradisk parallelism server storage systems power consumption disk drives;high performance;storage;power;parallel processing	Server storage systems use numerous disks to achieve high performance, thereby consuming a significant amount of power. This paper discussed the intradisk parallelism that can significantly reduce such systems' power consumption by letting disk drives exploit parallelism in the I/O request stream. By doing so, it's possible to match, and even surpass, a storage array's performance for these workloads using a single, high-capacity disk drive.	disk array;disk storage;input/output;parallel computing	Sudhanva Gurumurthi;Sriram Sankar;Mircea R. Stan	2009	IEEE Micro	10.1109/MM.2009.21	parallel processing;parallel computing;real-time computing;energy conservation;computer hardware;computer science;operating system;power;assembly;efficient energy use	OS	-11.148310831801686	53.12532096606477	91735
a73d60c054a3a7df51a4c18ea94a77aac40c5c4e	dimvhcm: an on-line distributed monitoring data collection model	distributed data;distributed system;distributed database;distributed processing computerised monitoring data visualisation;behavioral analysis;prototypes;data collection;distributed processing;monitoring prototypes data models data visualization distributed databases benchmark testing registers;visualization monitoring on line monitoring distributed systems behavioral analysis;data model;data visualisation;visualization;monitoring system;monitoring;registers;visual analysis;data visualization;distributed databases;distributed systems;computerised monitoring;nas parallel benchmarks;on line monitoring;behavior analysis;distributed systems behavior visual analysis dimvhcm online distributed monitoring data collection model hierarchical distributed data collection model behavior online analysis dim visual prototype triva visualization tool nas parallel benchmarks;benchmark testing;collective model;data models	In this work we present a hierarchical distributed data collection model for monitoring systems, called DIMVHCM. Its goal is to provide data for the on-line analysis of the behavior of distributed systems and applications. In our work we focus on analysis through visualization. We implemented a prototype of this model, which was integrated to the DIM Visual prototype and to the TRIVA visualization tool. This way, we obtained a prototype on-line monitoring tool. We measured the time needed to send monitoring data from its origin (a collector) to a client. We also evaluated the performance of the client. Last, we measured the performance overhead caused by DIMVHCM to the programs that compose the NAS Parallel Benchmarks. In this experiment we measured less than 5% intrusiveness from DIMVHCM. We also show that it can support the visual analysis of distributed systems behavior.	client (computing);distributed computing;nas parallel benchmarks;online and offline;overhead (computing);prototype	Rafael Keller Tesser;Philippe Olivier Alexandre Navaux	2012	2012 20th Euromicro International Conference on Parallel, Distributed and Network-based Processing	10.1109/PDP.2012.83	data modeling;benchmark;real-time computing;visualization;data model;computer science;operating system;data mining;database;distributed computing;prototype;processor register;distributed database;data visualization;data collection	HPC	-16.71988223331945	48.78357287278292	91777
77af1ef7c069d34d9f71d735df7d731f6af751f7	high-speed storage nodes for the cloud	virtual machine;heterogeneous infrastructure as a service high speed storage node cloud based storage i o intensive application virtual machines storage array single route i o virtualization storage i o bandwidth hypervisor;virtualization;virtual machining;synthetic aperture sonar;computer architecture;virtual machining hardware blades servers security synthetic aperture sonar computer architecture;servers;virtual machines;infrastructure as a service;virtualisation cloud computing virtual machines virtual storage;infrastructure as a service virtualization storage virtual machines cloud computing;blades;security;storage;high speed;virtual storage;virtualisation;cloud computing;hardware	In this paper, we describe an architecture for high-speed storage nodes intended for supporting cloud-based storage I/O intensive applications such as file servers, backup servers and databases. The nodes can host multiple virtual machines each having direct access to a storage array via Single Route I/O Virtualization (SR-IOV). This is done in a way which does not compromise security. We demonstrate that SR-IOV imposes negligible overhead for storage I/O and provides the hosted virtual machines with four times the storage I/O bandwidth than is available from the same hardware when I/O is redirected through the hyper visor. We describe how the storage nodes are incorporated into a heterogeneous Infrastructure as a Service (IaaS) consisting of mixed storage and compute nodes.	access control;backup;cloud computing;control point (mathematics);database;disk array;fairness measure;file server;giove;hypervisor;indirection;operating system;operating-system-level virtualization;overhead (computing);quality of service;random access;reference monitor;single-root input/output virtualization;virtual machine	Nigel Edwards;Mark Watkins;Matt Gates;Alistair N. Coles;Eric Deliot;Aled Edwards;Anna Fischer;Patrick Goldsack;Tom Hancock;Donagh McCabe;Tim Reddin;J. P. Sullivan;Peter Toft;Lawrence Wilcock	2011	2011 Fourth IEEE International Conference on Utility and Cloud Computing	10.1109/UCC.2011.14	embedded system;real-time computing;storage area network;virtualization;converged storage;cloud computing;computer science;virtual machine;information security;operating system;emc invista;information repository;i/o scheduling	HPC	-18.106091385286263	52.23489885970753	91997
af0e2e50fc623d12b089740a2bfa11c5ee97f523	taruc: a topology-aware resource usability and contention benchmark	gpu computing;parallel architectures;performance analysis;numa;memory bandwidth;memory latency	Computer architects have increased hardware parallelism and power efficiency by integrating massively parallel hardware accelerators (coprocessors) into compute systems. Many modern HPC clusters now consist of multi-CPU nodes along with additional hardware accelerators in the form of graphics processing units (GPUs). Each CPU and GPU is integrated with system memory via communication links (QPI and PCIe) and multi-channel memory controllers. The increasing density of these heterogeneous computing systems has resulted in complex performance phenomena including non-uniform memory access (NUMA) and resource contention that make application performance hard to predict and tune. This paper presents the Topology Aware Resource Usability and Contention (TARUC) benchmark. TARUC is a modular, open-source, and highly configurable benchmark useful for profiling dense heterogeneous systems to provide insight for developers who wish to tune application codes for specific systems. Analysis of TARUC performance profiles from a multi-CPU, multi-GPU system is also presented.	benchmark (computing);cache coherence;central processing unit;channel memory;code;computer data storage;computer graphics;computer memory;coprocessor;graphics processing unit;hardware acceleration;heterogeneous computing;input/output;intel quickpath interconnect;knights;memory management;non-uniform memory access;open-source software;pci express;parallel computing;performance per watt;pipeline (computing);profiling (computer programming);random-access memory;resource contention;streaming media;uniform memory access;usability;xeon phi	Gavin Baker;Chris Lupo	2017		10.1145/3030207.3030230	uniform memory access;interleaved memory;computer architecture;parallel computing;real-time computing;cas latency;computer science;memory bandwidth;general-purpose computing on graphics processing units;computing with memory;cache-only memory architecture;non-uniform memory access	HPC	-8.305976510765829	46.41084452099764	92115
a39146e9524f41f6c9722e884647c739774a2e7c	priority-driven spatial resource sharing scheduling for embedded graphics processing units		Many visual tasks in modern personal devices such smartphones resort heavily to graphics processing units (GPUs) for their fluent user experiences. Because most GPUs for embedded systems are nonpreemptive by nature, it is important to schedule GPU resources efficiently across multiple GPU tasks. We present a novel spatial resource sharing (SRS) technique for GPU tasks, called a budget-reservation spatial resource sharing (BR-SRS) scheduling, which limits the number of GPU processing cores for a job based on the priority of the job. Such a priority-driven resource assignment can prevent a high-priority foreground GPU task from being delayed by background GPU tasks. The BR-SRS scheduler is invoked only twice at the arrival and completion of jobs, and thus, the scheduling overhead is minimized as well. We evaluated the performance of our scheduling scheme in an Android-based smartphone, and found that the proposed technique significantly improved the performance of high-priority tasks in comparison to the previous temporal budget-based multi-task scheduling. © 2017 Published by Elsevier B.V.	android;computer graphics;computer multitasking;embedded system;graphics processing unit;job stream;overhead (computing);scheduling (computing);smartphone	Yunji Kang;Woo Hyun Joo;Sungkil Lee;Dongkun Shin	2017	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2017.04.002	embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing	Embedded	-10.614382273598702	57.42155419583988	92428
3c28d5967db86e8f5e4c37d03518967c285a32bf	simultaneous multikernel: fine-grained sharing of gpus	resource management;kernel graphics processing units resource management context throughput switches hardware;graphics processing units;multitasking;switches;throughput	Studies show that non-graphics programs can be less optimized for the GPU hardware, leading to significant resource under-utilization. Sharing the GPU among multiple programs can effectively improve utilization, which is particularly attractive to systems (e.g., cloud computing) where many applications require access to the GPU. However, current GPUs lack proper architecture features to support sharing. Initial attempts are very preliminary in that they either provide only static sharing, which requires recompilation or code transformation, or they do not effectively improve GPU resource utilization. We propose Simultaneous Multikernel (SMK), a fine-grained dynamic sharing mechanism, that fully utilizes resources within a streaming multiprocessor by exploiting heterogeneity of different kernels. We extend the GPU hardware to support SMK, and propose several resource allocation strategies to improve system throughput while maintaining fairness. Our evaluation of 45 shared workloads shows that SMK improves GPU throughput by 34 percent over non-shared execution and 10 percent over a state-of-the-art design.	cloud computing;computer multitasking;fairness measure;graphics processing unit;graphics software;multikernel;multiprocessing;simultaneous multithreading;throughput	Zhenning Wang;Jun Yang;Rami G. Melhem;Bruce R. Childers;Youtao Zhang;Minyi Guo	2016	IEEE Computer Architecture Letters	10.1109/LCA.2015.2477405	throughput;computer architecture;parallel computing;real-time computing;human multitasking;network switch;computer science;resource management;operating system;computer network	Arch	-9.117256888140238	50.63269803545846	92444
101d6553ba9223342d23edf295f5e9298c149bb3	visualising large-scale neural network models in real-time	computational agent;spinnaker programmable neuromimetic system;experimental networks;neural nets;network abstraction;biological system modeling;general purpose visualisation platform;brain models;data visualization computational modeling neurons biological system modeling brain models real time systems;data visualisation;simulation data;computational modeling;compute time minimisation large scale neural network models simulation data network dynamics computational agent experimental networks real time visualisation platform spinnaker programmable neuromimetic system network abstraction general purpose visualisation platform;system on chip;data visualization;network dynamics;multiprocessing systems;neurons;real time visualisation platform;system on chip data visualisation multiprocessing systems neural nets real time systems;compute time minimisation;large scale neural network models;real time systems	As models of neural networks scale in concert with increasing computational performance, gaining insight into their operation becomes increasingly important. This paper proposes an efficient and generalised method to access simulation data via in-system aggregation, providing visualised representation at all layers of the network in real-time. Enabling neural networks for real-time visualisation allows a user to gain insight into the network dynamics of their systems as they operate over time. This visibility also permits users (or a computational agent) to determine whether early intervention is required to adjust parameters, or even to terminate operation of experimental networks that are not operating correctly. Conventionally the determination of correctness would occur post-simulation, so with sufficient `in-flight' insight, a significant advantage may be obtained, and compute time minimised. For this paper we apply the real-time visualisation platform to the SpiNNaker programmable neuromimetic system and a variety of neural network models. The visualisation platform is shown to be capable across a range of diverse simulations, and at supporting differing layers of network abstraction, requiring minimal configuration to represent each model. The resulting general-purpose visualisation platform for neural networks, is effective at presenting data to users in order to aid their comprehension of the network dynamics during operation, and scales from small to biologically-significant network sizes.	artificial neural network;computation;correctness (computer science);general-purpose modeling;real-time clock;real-time locating system;simulation;spinnaker;terminate (software)	Cameron Patterson;Francesco Galluppi;Alexander D. Rast;Steve B. Furber	2012	The 2012 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2012.6252490	system on a chip;real-time computing;simulation;computer science;theoretical computer science;network dynamics;machine learning;computational model;data visualization;statistics	Robotics	-16.869478887855944	58.1487383324241	92447
88cd69a3d1926ff7caa30727e0c2b2c2a9532656	modelling resilience of data processing capabilities of cps		Modern CPS should process large amount of data with high speed and reliability. To ensure that the system can handle varying volumes of data, the system designers usually rely on the architectures with the dynamically scaling degree of parallelism. However, to guarantee resilience of data processing, we should also ensure system fault tolerance, i.e., integrate the mechanisms for dynamic reconfiguration in the data processing architecture. In this paper, we present an approach to formal modelling and assessment of a reconfigurable dynamically scaling systems that guarantees resilience of data processing. We rely on modelling in Event-B to formally define the dynamic architecture of the system with the integrated dynamically scaling parallelism and reconfiguration. The formal development allows us to derive a complex system architecture and verify correctness. To quantitatively assess resilience of data processing architecture, we rely on statistical model checking and evaluate the likelihood of successful data processing under different system parameters. The proposed integrated approach facilitates design space exploration and improves predictability in the development of complex data processing capabilities.	b-method;complex system;correctness (computer science);degree of parallelism;design space exploration;formal methods;image scaling;model checking;parallel computing;reconfigurable computing;refinement (computing);simulation;statistical model;system fault tolerance;systems architecture	Linas Laibinis;Dmitry M. Klionskiy;Elena Troubitsyna;Anatoly Dorokhov;Johan Lilius;Mikhail Kupriyanov	2014		10.1007/978-3-319-12241-0_5	engineering;real-time computing;reliability engineering;control reconfiguration;complex system;architecture;design space exploration;degree of parallelism;correctness;systems architecture;complex data type	DB	-7.235348406763224	58.754671955340484	92467
d3c1cb67b967cf3ac585c4d809b61bbe2688b5d1	booster: reactive core acceleration for mitigating the effects of process variation and application imbalance in low-voltage chips	voltage control;rails;process variation;multi threading;power supply;chip;power aware computing;low voltage;performance improvement;synchronization;power aware computing microprocessor chips multiprocessing systems multi threading;32 core system reactive core acceleration process variation application imbalance low voltage chips supply voltage lowering microprocessor power consumption reduction low overhead framework performance heterogeneity rebalancing booster cmp gating circuit on chip governor boost budget management booster var core to core frequency variation booster sync multithreaded applications parsec splash2 benchmarks;multiprocessing systems;rails synchronization instruction sets switches reactive power voltage control regulators;power consumption;switches;regulators;microprocessor chips;instruction sets;reactive power	Lowering supply voltage is one of the most effective techniques for reducing microprocessor power consumption. Unfortunately, at low voltages, chips are very sensitive to process variation, which can lead to large differences in the maximum frequency achieved by individual cores. This paper presents Booster, a simple, low-overhead framework for dynamically rebalancing performance heterogeneity caused by process variation and application imbalance. The Booster CMP includes two power supply rails set at two very low but different voltages. Each core can be dynamically assigned to either of the two rails using a gating circuit. This allows cores to quickly switch between two different frequencies. An on-chip governor controls the timing of the switching and the time spent on each rail. The governor manages a “boost budget” that dictates how many cores can be sped up (depending on the power constraints) at any given time. We present two implementations of Booster: Booster VAR, which virtually eliminates the effects of core-to-core frequency variation in near-threshold CMPs, and Booster SYNC, which additionally reduces the effects of imbalance in multithreaded applications. Evaluation using PARSEC and SPLASH2 benchmarks running on a simulated 32-core system shows an average performance improvement of 11% for Booster VAR and 23% for Booster SYNC.	best, worst and average case;booster (electric power);clock rate;microprocessor;overhead (computing);parsec;power supply unit (computer);sync;thread (computing)	Timothy N. Miller;Xiang Pan;Renji Thomas;Naser Sedaghati;Radu Teodorescu	2012	IEEE International Symposium on High-Performance Comp Architecture	10.1109/HPCA.2012.6168942	chip;embedded system;synchronization;parallel computing;real-time computing;multithreading;telecommunications;network switch;computer science;operating system;instruction set;ac power;low voltage;process variation	Arch	-5.209544553088538	55.01797890736557	92541
eccd194c80b536dc0710f74cef53cb43af7ecf58	a case study of trace-driven simulation for analyzing interconnection networks: cc-numas with ilp processors	cc numa architectures;multiprocessor interconnection networks;analytical models;instruction level parallel;splash2 suite case study trace driven simulation interconnection network performance evaluation cc numa architectures ilp processors instruction level parallelism time intensive simulations resource intensive simulations cache coherent nonuniform memory access nondeterministic memory accesses execution driven simulation evaluation cost network traces parallel application execution time network design space accuracy simulation time temporal dependencies message dependencies cache coherent protocol fast fourier transform radix application;cache storage;cache coherent protocol;network design;electronic mail;memory protocols;performance evaluation;non uniform memory access;network design space;computer aided software engineering analytical models multiprocessor interconnection networks computational modeling telecommunication traffic traffic control costs predictive models performance analysis electronic mail;splash2 suite;traffic control;network performance;network traces;resource intensive simulations;interconnection network;fast fourier transform;memory access;accuracy;telecommunication traffic;computer aided software engineering;computational modeling;cache coherent nonuniform memory access;message dependencies;temporal dependencies;interconnection network performance evaluation;cache coherence protocol;parallel architectures;virtual machines;parallel systems;evaluation cost;performance analysis;execution driven simulation;cache coherence;ilp processors;fast fourier transforms;coherence;predictive models;nondeterministic memory accesses;instruction level parallelism;time intensive simulations;radix application;simulation time;trace driven simulation;parallel applications;coherence multiprocessor interconnection networks performance evaluation parallel architectures virtual machines cache storage memory protocols fast fourier transforms	The evaluation of network performance under real application loads is carried out by detailed time-intensive and resourceintensive simulations. Moreover, the use of ILP processors in cc-NUMA architectures introduces non-deterministic memory accesses; the resulting parallel system must be modeled by a detailed execution-driven simulation, further increasing the evaluation cost. This work introduces a simulation methodology, based on network traces, to estimate the impact that a given network has on the execution time of parallel applications. This methodology allows the study of the network design space with a level of accuracy close to that of execution-driven simulations but with much shorter simulation times. The network trace, extracted from an execution-driven simulation, is processed to substitute the temporal dependencies produced by the simulated network with an estimation of the message dependencies caused by both the application and the applied cache-coherent protocol. This methodology has been tested on two direct networks, with 16 and 64 nodes respectively, running the FFT and Radix applications of the SPLASH2 suite. The trace-driven simulation is 3 to 4 times faster than the execution-driven one with an average error of 4% in total execution time.	cache coherence;central processing unit;coherence (physics);deterministic memory;fast fourier transform;interconnection;network performance;network planning and design;non-uniform memory access;run time (program lifecycle phase);simulation;tracing (software)	Valentin Puente;J. M. Prellezo;Cruz Izu;José-Ángel Gregorio;Ramón Beivide	2000		10.1109/EMPDP.2000.823409	computer architecture;parallel computing;real-time computing;computer science;network simulation;network traffic simulation	Metrics	-9.115947636654491	47.96294941158481	92565
79fd9e31c0003a04b0a28486e6a27efa086017d2	starting workflow tasks before they're ready	semantics;runtime;computational modeling;engines;monitoring;schedules;parallel processing	Today's science is more and more driven by collecting and evaluating increasing amounts of data. Utilizing Scientific Workflows is one suitable method how to organize processing pipelines for this purpose. In this work, we show that performance improvements on the execution of existing workflows can be achieved, if the conditions for starting selected tasks with certain data access characteristics are loosened. We provide a scheme how to identify eligible tasks in a given workflow and demonstrate a technique how an earlier start of tasks can be realized in Pegasus WMS by transforming the workflow DAG and by using a wrapper around the task executable during runtime. Our implemented wrapper handles the reading data accesses for task instances so that existing original workflows can be executed without the need to modify them. We evaluate our approach in simulations and experiments on real distributed computing resources, and are able to observe performance improvements for the Montage workflow by a significant reduction of total execution time.	data access;directed acyclic graph;distributed computing;executable;experiment;montagejs;pegasus;pipeline (computing);run time (program lifecycle phase);simulation;web map service;wrapper library	Wladislaw Gusew;Björn Scheuermann	2016	2016 IEEE 12th International Conference on e-Science (e-Science)	10.1109/eScience.2016.7870896	real-time computing;computer science;database;distributed computing;workflow management system;workflow engine;workflow technology	HPC	-18.469843626057227	56.121941453115376	92667
8c030adee8b8c4c5887b7e4ac64a3111fad22263	multi-jagged: a scalable parallel spatial partitioning algorithm	partitioning algorithms program processors scalability distributed databases software algorithms heuristic algorithms indexes;resource allocation data handling minimisation parallel processing;recursive bisection;zoltan data movement minimization recursive multisection recursive coordinate bisection algorithm multidimensional jagged geometric partitioner geometric data locality load balancing dynamic applications geometric partitioning scalable parallel spatial partitioning algorithm;jagged partitioning;mathematics and computing;indexes;geometric partitioning;load balancing geometric partitioning spatial partitioning recursive bisection jagged partitioning;geometric partitioning spatial partitioning recursive bisection jagged partitioning load balancing;heuristic algorithms;data handling minimisation parallel processing resource allocation;distributed databases;load balancing;software algorithms;mathematics and computing geometric partitioning;scalability;program processors;scalable parallel spatial partitioning algorithm zoltan data movement minimization recursive multisection recursive coordinate bisection algorithm multidimensional jagged geometric partitioner geometric data locality load balancing dynamic applications geometric partitioning;partitioning algorithms;spatial partitioning	Geometric partitioning is fast and effective for load-balancing dynamic applications, particularly those requiring geometric locality of data (particle methods, crash simulations). We present, to our knowledge, the first parallel implementation of a multidimensional-jagged geometric partitioner. In contrast to the traditional recursive coordinate bisection algorithm (RCB), which recursively bisects subdomains perpendicular to their longest dimension until the desired number of parts is obtained, our algorithm does recursive multi-section with a given number of parts in each dimension. By computing multiple cut lines concurrently and intelligently deciding when to migrate data while computing the partition, we minimize data movement compared to efficient implementations of recursive bisection. We demonstrate the algorithm's scalability and quality relative to the RCB implementation in Zoltan on both real and synthetic datasets. Our experiments show that the proposed algorithm performs and scales better than RCB in terms of run-time without degrading the load balance. Our implementation partitions 24 billion points into 65,536 parts within a few seconds and exhibits near perfect weak scaling up to 6K cores.	best, worst and average case;binary space partitioning;bisection method;central processing unit;dhrystone;dijkstra's algorithm;experiment;image scaling;load balancing (computing);locality of reference;randomness;recursion;scalability;simulation;system migration	Mehmet Deveci;Sivasankaran Rajamanickam;Karen D. Devine;Ümit V. Çatalyürek	2016	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2015.2412545	database index;parallel computing;scalability;computer science;space partitioning;load balancing;theoretical computer science;database;distributed computing;distributed database	HPC	-16.68589944329513	55.29822254029114	92712
7efb4455ff49aab66b0015f8485d9e07c9b36c4a	impact of microarchitectural differences of risc-v processor cores on soft error effects		In this paper, we compare how radiation-induced soft errors affect the execution results of user-level applications on different processor cores that implement the same instruction set architecture (ISA). We target two processor cores that support the same RISC-V ISA but have significant differences in their microarchitectural implementations (in-order versus out-of-order). The observed results from fault injection experiments show very strong correlations between the resulting effects from those two processor cores. This strong correlation property is not observed between the processor cores that have different ISAs. Based on this observation, we discuss how the resulting effects of soft errors on a target processor core can be predicted using a reduced set of fault injection experiments with a small number of benchmark applications. Using our heuristic method of selecting the applications to be used for the fault injection experiments on the target processor core, we achieved a high prediction accuracy with prediction errors of less than 7%. Our approach can be used for rapid error resilience evaluation of system designs that have the same ISA.	benchmark (computing);experiment;fault injection;heuristic;microarchitecture;multi-core processor;risc-v;soft error;user space	Hyungmin Cho	2018	IEEE Access	10.1109/ACCESS.2018.2858773	instruction set;parallel computing;computer science;distributed computing;fault injection;risc-v;multi-core processor;out-of-order execution;small number;soft error;microarchitecture	Arch	-7.191416614586662	51.02298632308601	92792
e7686031d4db849ede245d5825639bd50913fcfd	study and optimize the process of batch small files replication	storage allocation;storage management batch processing computers linux storage allocation;parallel writing target files;performance optimizing;storage system;performance evaluation;consecutive reading source files;storage management;linux file system;file system batch small files performance optimizing;system performance;replication process;network replication batch small files replication i o performance storage system performance storage architecture i o optimization storage devices replication process linux file system aggregating algorithm latency estimation consecutive reading source files parallel writing target files;i o performance;batch small files;file system;storage architecture;storage system performance;batch processing computers;aggregating algorithm;linux;optimization;delay file systems educational technology data communication system performance computer science data storage systems computer science education application software geography;i o optimization;batch small files replication;performance optimization;storage devices;latency estimation;network replication;file systems;throughput	I/O performance is always the traditional criterion for the evaluation of storage system. Many researches have been being carried on how to improve the storage system performance, mainly focusing on the storage architecture and I/O optimization for the storage devices. In many application systems, the phenomenon of replicating batch small files between two locations widely exists and always represents poor performance in systems. This paper analyzes and optimizes replication process for batch small files in Linux file system. In local case, six algorithms are achieved by using parallel, consecutive and aggregating polices in different stages of the whole process. In network case, achieve and compress strategies are also introduced and compared with aggregating algorithm. Moreover, the average latency of basic operations in each stage of file I/O can be estimated accurately, which is helpful for future research of file system. The experiment shows that the algorithm of consecutive reading source files and parallel writing target files have the best performance in local replication, and aggregating algorithm also do in network replication.	access time;algorithm;computer data storage;directory (computing);input/output;linux;mathematical optimization;parallel computing;sequential access	Liang Xiao;Qiang Cao;Changsheng Xie;Chuanwen Wu	2008	2008 Japan-China Joint Workshop on Frontier of Computer Science and Technology	10.1109/FCST.2008.32	embedded system;throughput;real-time computing;computer hardware;computer science;operating system;database;computer performance;linux kernel	OS	-12.703338792227951	52.67351670389309	93205
cddfb34a35924b2958950deac3a6075f450e4519	the tiledb array data storage manager		We present a novel storage manager for multi-dimensional arrays that arise in scientific applications, which is part of a larger scientific data management system called TileDB. In contrast to existing solutions, TileDB is optimized for both dense and sparse arrays. Its key idea is to organize array elements into ordered collections called fragments. Each fragment is dense or sparse, and groups contiguous array elements into data tiles of fixed capacity. The organization into fragments turns random writes into sequential writes, and, coupled with a novel read algorithm, leads to very efficient reads. TileDB enables parallelization via multi-threading and multiprocessing, offering thread-/process-safety and atomicity via lightweight locking. We show that TileDB delivers comparable performance to the HDF5 dense array storage manager, while providing much faster random writes. We also show that TileDB offers substantially faster reads and writes than the SciDB array database system with both dense and sparse arrays. Finally, we demonstrate that TileDB is considerably faster than adaptations of the Vertica relational column-store for dense array storage management, and at least as fast for the case of sparse arrays.	algorithm;atomicity (database systems);column-oriented dbms;data hub;data storage tag;database;experiment;hierarchical data format;lock (computer science);multiprocessing;parallel computing;programmer;scidb;semiconductor consolidation;sparse matrix;thread (computing)	Stavros Papadopoulos;Kushal Datta;Samuel Madden;Timothy G. Mattson	2016	PVLDB	10.14778/3025111.3025117	parallel computing;computer science;database;distributed computing;sparse array	DB	-15.282349563484512	53.26230160365234	93238
5c5cd6af907704b3931089566561497a725767d4	efficient classloading strategies for interprocedural analyses in the presence of dynamic classloading	optimising compilers;degradation;dynamic classloading;yarn;interprocedural analysis;sequential consistency;java programming;virtual machining;interprocedural analyses;optimising compilers java;adaptive systems;java delay performance analysis optimization methods yarn costs virtual machining adaptive systems degradation throughput;performance analysis;java programs;performance degradation;java programs classloading strategy interprocedural analyses dynamic classloading interprocedural analysis performance degradation unoptimized code;unoptimized code;classloading strategy;throughput;java;optimization methods	In the presence of dynamic classloading, performing interprocedural analysis (IPA) too early can lead to repeatedly performing the IPA as new classes are loaded, while performing it too late will cause a performance degradation by running unoptimized code for too long. This paper investigates how programs load classes and how this affects the performance when performing IPA. From this investigation, two classloading techniques are described and evaluated. We first describe a classloader that eagerly preloads classes whose names appear in the constant pool. By loading classes as early as possible, the preloading classloader reduces the chances of new classes being loaded after an IPA is performed, and thus can improve performance by reducing the number of IPAs performed. We next describe a technique that delays the IPA, causing it to be performed late enough to dramatically reduce the need for re-analyses and early enough to obtain the benefits of the IPA. When these two classloading techniques are used with a real IPA . an escape analysis to enforce sequential consistency in Java programs . the speedups relative to the default nonpreloading classloader are 1.87 and 2.30 respectively, on average.	elegant degradation;escape analysis;interprocedural optimization;java classloader;sequential consistency	Kyungwoo Lee;Qasim Ali;Samuel P. Midkiff	2007	Fifth International Workshop on Dynamic Analysis (WODA '07)	10.1109/WODA.2007.2	throughput;parallel computing;real-time computing;degradation;computer science;adaptive system;operating system;programming language;java;sequential consistency	Arch	-13.216585519259437	50.45787623289005	93262
9e972f8526fdef94cb465a411a05cdbddbe36205	runtime pipeline scheduling system for heterogeneous architectures	streams;runtime;concurrency;scheduling;heterogeneous architecture;over lapping;pipeline	Heterogeneous architectures can improve the performance of applications with computationally intensive, data-parallel operations. Even when these architectures may reduce the execution time of applications, there are opportunities for additional performance improvement as the memory hierarchy of the central processor cores and the graphics processor cores are separate. Applications executing on heterogeneous architectures must allocate space in the GPU global memory, copy input data, invoke kernels, and copy results to the CPU memory. This scheme does not overlap inter-memory data transfers and GPU computations, thus increasing application execution time. This research presents a software architecture with a runtime pipeline system for GPU input/output scheduling that acts as a bidirectional interface between the GPU computing application and the physical device. The main aim of this system is to reduce the impact of the processor-memory performance gap by exploiting device I/O and computation overlap. Evaluation using application benchmarks shows processing improvements with speedups above 2x with respect to baseline, non-streamed GPU execution.	baseline (configuration management);central processing unit;computation;graphics processing unit;heterogeneous system architecture;input/output;memory hierarchy;peripheral;random-access memory;run time (program lifecycle phase);scheduling (computing);software architecture;streaming media	Julio C. Olaya;Rodrigo A. Romero	2014		10.1145/2616498.2616547	computer architecture;parallel computing;real-time computing;computer science	HPC	-6.501336246844281	48.269493356481036	93374
000e5a3f1d0c01a04b7d5c7e56944c441c6ea51b	trigger memoization in self-triggered control	self triggered implementation;stability;control system;wcet;memoization	Self-triggered implementations of controllers have been proposed as an alternative to traditional time-triggered implementations. In a self-triggered implementation, the control task computes the actuator signal as well as a triggering time that specifies the next time instant at which the control task should be run. Self-triggered implementations have the potential to decrease communication costs and CPU requirements over time-triggered ones, e.g., by running the steady-state plant in open loop for long intervals if there is no disturbance. We show that commonly claimed gains for self-triggered implementations are too optimistic. The analysis of most self-triggering algorithms ignore the execution times for computing the trigger times. We show, using implementations of several self-triggering algorithms proposed in the literature on common embedded platforms, that the execution time to compute the trigger time can be non-negligible compared to the trigger times, and may even be higher than the trigger time itself, rendering a naive implementation infeasible.  We propose a hybrid implementation scheme for self-triggered control using state quantization and memoization of trigger times in a cache. We perform trigger-time computation tasks with low priority, and fall back on a time-triggered implementation when the trigger time computations are not guaranteed to finish in time (but use the computed results to update the cache). Our implementation achieves communication costs similar to self-triggered implementations and computation costs close to time-triggered implementations, while providing a bound for the region of practical stability.	algorithm;central processing unit;computation;embedded system;memoization;optimistic concurrency control;requirement;run time (program lifecycle phase);steady state	Indranil Saha;Rupak Majumdar	2012		10.1145/2380356.2380378	embedded system;parallel computing;real-time computing;memoization;stability;computer science;control system;operating system;programming language;statistics	Embedded	-10.00651007841727	59.644763054330824	93441
94241c338ef4973fb932c3b0bb1531d6f8a94563	avoiding tree saturation in the face of many hotspots with few buffers	buffering approach multistage network hotspots feedback damping p processor system switch organization non hotspot message probability;control systems;multistage interconnection network hotspot;routing;radiation detectors;multistage interconnection network;multistage interconnection networks buffer storage computational complexity message switching multiprocessing systems;arrays;hotspot;face;ports computers;program processors radiation detectors routing arrays ports computers control systems face;program processors	In a multistage network, hotspots induce tree saturation. The known solutions employ a variety of techniques, including combining (which works only for certain kinds of messages), feedback damping (which appears to provide low utilization in the absence of hot spots), and large numbers of buffers. In practice, the approach used today is to provide large numbers of buffers: in a P-processor system, the rule of thumb appears to be to provide 10P buffers, but 10P buffers maybe too expensive for systems containing 105 or more processors. Even employing Omega(P) buffers does not appear to provide any guarantees, however. We show that by organizing the switches so that the messages addressed to a particular processor can use only certain of the buffers, many hotspots can be tolerated with few buffers. For example, a switch with O(log P) buffers can tolerate a single hotspot with probability 1, and allows the first few hotspots to have a large number of buffers before being declared a hotspot. A switch with B buffers can be organized so that it blocks a particular non-hotspot message with probability less than O(1/s) if there are O(B/log s) hotspots, and can handle a factor of O(B (log log s)/log s) more hotspots before the probability becomes a constant. A similar approach can also be used to improve caching behavior in a multithreaded system in which one of the threads tries to consume all of the cache.	cache (computing);central processing unit;hotspot (wi-fi);java hotspot virtual machine;multistage amplifier;multithreading (computer architecture);network switch;organizing (structure);protocol buffers;thread (computing)	Bradley C. Kuszmaul;William Kuszmaul	2014	2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)	10.1109/HPCC.2014.79	face;routing;parallel computing;real-time computing;hotspot;computer science;control system;operating system;distributed computing;particle detector;computer network	Arch	-14.438211263368153	50.90811970224578	93597
1103873228f5513d45e42b30d986e010a746845f	energy-efficient instruction scheduling utilizing cache miss information	microprocessor;energy efficient;perforation;power efficiency;memory wall;memory access;energy consumption;critical path;power consumption;functional unit;instruction scheduling;high performance;low power consumption	Current microprocessors require both high performance and low-power consumption. In order to reduce energy consumption with maintaining computing performance, we propose to utilize the information regarding instruction criticality. Microprocessors we are proposing have two types of functional units distinguished in terms of their execution latency and power consumption. Only critical instructions are executed on power-hungry functional units, and thus the total energy consumption can be reduced without severe performance loss. In order to achieve large energy reduction, it is required to execute instructions on power-efficient units as frequently as possible. In this paper, we propose a new instruction scheduling method utilizing cache miss information over the above mentioned scheduling technique. As a performance gap between microprocessors and main memories is increasing, it is possible that critical instructions are executed in power-efficient units as well as non-critical ones while main memory access is occurring. Our simulation results reveal that the modified instruction scheduling achieves 27.3%  ED  2  P  reduction with 1.4% performance degradation.	cpu cache;instruction scheduling;scheduling (computing)	Akihiro Chiyonobu;Toshinori Sato	2006	SIGARCH Computer Architecture News	10.1145/1147349.1147361	computer architecture;parallel computing;real-time computing;electrical efficiency;computer hardware;computer science;operating system;critical path method;efficient energy use;instruction scheduling	Arch	-6.012460573216399	55.447205488943936	93650
d939baac2b503104295fd1de4641b2b3c8864b10	a small non-volatile write buffer to reduce storage writes in smartphones	databases;nonvolatile memory;mobile communication;cache;mobile devices;mobile computing	Storage write behavior in mobile devices, e.g., smartphones, is characterized by frequent overwrites of small data. In our work, we first demonstrate a small non-volatile write buffer is effective in coalescing such overwrites to reduce storage writes. We also present how to make the best use of write buffer resource the size of which is limited by the requirement of small form factor. We present two new methods, shadow tag and SQLite-aware buffer management both of which aim at identifying hot storage data to keep in the write buffer. We also investigate the storage behavior of multiple mobile applications and show that their interference can reduce the effectiveness of write buffer. In order to resolve this problem, we propose a new dynamic buffer allocation method. We did experiments with real mobile applications running on a smartphone and a Flash memory-based storage system and obtained average 56.2% and 50.2% reduction in storage writes in single and multiple application runs, respectively.	computer data storage;experiment;flash memory;interference (communication);mobile app;mobile device;non-volatile memory;sqlite;small form factor;smartphone;write buffer	Mungyu Son;Sungkwang Lee;Kyungho Kim;Sungjoo Yoo;Sunggu Lee	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;parallel computing;computer hardware;cache;computer science;operating system;buffer underrun;write buffer;write combining	DB	-12.067392492868404	54.226527398670875	93795
0ac82a991367a53a21f66882a018218cb32a9df7	avoiding performance impacts by re-replication workload shifting in hdfs based cloud storage				Thanda Shwe;Masayoshi Aritsugi	2018	IEICE Transactions		artificial intelligence;workload;computer vision;computer science;cloud storage;real-time computing	HPC	-13.877035393807144	55.142404941433654	93919
5c307996b68bb756f05b2f2f217ae22956abffbb	a hard real-time capable multi-core smt processor	real time;performance;worst case execution time;design;smt;multi core;multithreading	Hard real-time applications in safety critical domains require high performance and time analyzability. Multi-core processors are an answer to these demands, however task interferences make multi-cores more difficult to analyze from a worst-case execution time point of view than single-core processors. We propose a multi-core SMT processor that ensures a bounded maximum delay a task can suffer due to inter-task interferences. Multiple hard real-time tasks can be executed on different cores together with additional non real-time tasks. Our evaluation shows that the proposed MERASA multi-core provides predictability for hard real-time tasks and also high performance for non hard real-time tasks.	best, worst and average case;central processing unit;multi-core processor;real-time clock;real-time computing;real-time locating system;real-time operating system;run time (program lifecycle phase);simultaneous multithreading;single-core;worst-case execution time	Marco Paolieri;Jörg Mische;Stefan Metzlaff;Mike Gerdes;Eduardo Quiñones;Sascha Uhrig;Theo Ungerer;Francisco J. Cazorla	2013	ACM Trans. Embedded Comput. Syst.	10.1145/2442116.2442129	design;computer architecture;parallel computing;real-time computing;multithreading;performance;computer science;operating system	Embedded	-8.696731265213996	59.599818075464945	94160
47e508ee5dce8e7301ce20b1f839f940fd504b5d	resilient computational applications using coarray fortran		Abstract With the increase in the number of hardware components and layers of the software stack in High Performance Computing (HPC) there will likely be an increment in number of hardware and software failures, which will be user-visible. Even under the most optimistic assumptions about the individual components reliability, probabilistic amplification from using millions of nodes has a dramatic impact on the Mean Time Between Failure (MTBF) of the entire platform. Although several techniques to address this problem have been developed, the support provided by the programming model, for the user to mitigate or work around this issue, is still insufficient. The Fortran 2018 standard defines failed images , a new feature that allows the programmer to detect and manage image failures in a parallel program. In this paper we show how to use failed images and teams , another feature defined in the Fortran 2018 standard, to implement resilient computational applications.		Alessandro Fanfarillo;Sudip Kumar Garain;Dinshaw S. Balsara;Dan Nagle	2019	Parallel Computing	10.1016/j.parco.2018.12.002	parallel computing;computer science;mean time between failures;probabilistic logic;software;programming paradigm;coarray fortran;fortran;programmer;supercomputer	HPC	-18.01185873089661	49.297383401504085	94324
2c4b5924ab2ca01b7ac21deb79ce51c05ba5cef6	conmr: concurrent mapreduce programming model for large scale shared-data applications	erbium;remuneration;parallel programming;programming model;concurrency;large data processing conmr model concurrent mapreduce programming model large scale shared data applications map to one reduce framework distributed file system time overhead;programming file systems remuneration erbium optimization data models distributed databases;concurrency control;distributed databases;parallel programming concurrency control data handling;optimization;mapreduce;data handling;programming;file systems;data models	The rapid growth of large-data processing has brought in the MapReduce programming model as a widely accepted solution. However, MapReduce limits itself to a one map-to-one-reduce framework. Meanwhile, it lacks built-in support and optimization when the input datasets are shared among concurrent applications and/or jobs. The performance might be improved when the shared and frequently accessed data is read from local instead of distributed file system.To enhance the performance of big data applications, this paper presents Concurrent MapReduce, a new programming model built on top of MapReduce that deals with large amount of shared data items. Concurrent MapReduce provides support for processing heterogeneous sources of input datasets and offers optimization when the datasets are partially or fully shared. Experimental evaluation has shown an execution runtime speedup of 4X compared to traditional nonconcurrent MapReduce implementation with a manageable time overhead.	big data;built-in self-test;clustered file system;job stream;mapreduce;mathematical optimization;overhead (computing);programming model;speedup	Fan Zhang;Qutaibah M. Malluhi;Tamer M. Elsyed	2013	2013 42nd International Conference on Parallel Processing	10.1109/ICPP.2013.134	data modeling;programming;parallel computing;real-time computing;erbium;concurrency;computer science;operating system;concurrency control;group method of data handling;database;programming paradigm;distributed database	DB	-17.43908570626547	53.97308191542982	94394
0f3db7441e0c53589d5caff292ac18c361c1620c	an energy-aware scheduling for real-time task synchronization using dvs and leakage-aware methods	real time scheduling;priority ceiling protocol;priority inversion	Due to the importance of resource allocation and energy efficiency, this paper considers minimizing priority inversion and energy consumptions in the embedded real-time systems. While dynamic voltage scaling (DVS) is known to reduce dynamic power consumption, it also causes increased blocking time of lower priority tasks and leakage energy consumption due to increased execution. We proposed a concept of latency locking to prevent priority inversion using sleeping mode and define a block-free interval in which both DVS and leakage-aware methods can be applied. In order to compute the optimal sleeping time and its duration and to meet the timing constraints, we also propose a weighted directed graph (WDG) to obtain additional task information. By traversing WDG, task information can be updated online and the scheduling decisions could be done in linear time complexity.	blocking (computing);directed graph;dynamic voltage scaling;embedded system;graph theory;image scaling;lock (computer science);priority inversion;real-time clock;real-time computing;scheduling (computing);spectral leakage;time complexity	Da-Ren Chen;You-Shyang Chen	2012	SIGBED Review	10.1145/2452537.2452540	priority inversion;priority inheritance;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;deadline-monotonic scheduling;distributed computing;priority ceiling protocol	Embedded	-5.789332025205839	59.08392020111844	94441
517810fbf29624444b11a685d4f5f5265e115317	using a reconfigurable l1 data cache for efficient version management in hardware transactional memory	software;microprocessors;cache storage;architectural design;decoding;reconfigurable architectures;hardware transactional memory;reconfigurable architectures cache storage concurrency control configuration management memory architecture parallel programming;parallel programming;computer architecture;indexes;data cache;reconfigurable cache hardware transactional memory version management;memory architecture;indexation;concurrency control;version management;transactional memory;parallel programs;proposals;configuration management;eager rdc htm systems reconfigurable l1 data cache version management hardware transactional memory parallel programming transactional updates software log lazy version management hardware buffers logical data lazy rdc htm systems;computer architecture microprocessors hardware proposals indexes software decoding;reconfigurable cache;hardware	Transactional Memory (TM) potentially simplifies parallel programming by providing atomicity and isolation for executed transactions. One of the key mechanisms to provide such properties is version management, which defines where and how transactional updates (new values) are stored. Version management can be implemented either eagerly or lazily. In Hardware Transactional Memory (HTM) implementations, eager version management puts new values in-place and old values are kept in a software log, while lazy version management stores new values in hardware buffers keeping old values in-place. Current HTM implementations, for both eager and lazy version management schemes, suffer from performance penalties due to the inability to handle two versions of the same logical data efficiently. In this paper, we introduce a reconfigurable L1 data cache architecture that has two execution modes: a 64KB general purpose mode and a 32KB TM mode which is able to manage two versions of the same logical data. The latter allows to handle old and new transactional values within the cache simultaneously when executing transactional workloads. We explain in detail the architectural design and internals of this Reconfigurable Data Cache (RDC), as well as the supported operations that allow to efficiently solve existing version management problems. We describe how the RDC can support both eager and lazy HTM systems, and we present two RDC-HTM designs. Our evaluation shows that the Eager-RDC-HTM and Lazy-RDC-HTM systems achieve 1.36x and 1.18x speedup, respectively, over state-of-the-art proposals. We also evaluate the area and energy effects of our proposal, and we find that RDC designs are 1.92x and 1.38x more energy-delay efficient compared to baseline HTM systems, with less than 0.3% area impact on modern processors.	atomicity (database systems);baseline (configuration management);cpu cache;central processing unit;html;hierarchical temporal memory;in-place algorithm;lazy evaluation;parallel computing;speedup;transaction processing;transactional memory;version control	Adrià Armejach;Azam Seyedi;J. Rubén Titos Gil;Ibrahim Hur;Adrián Cristal;Osman S. Unsal;Mateo Valero	2011	2011 International Conference on Parallel Architectures and Compilation Techniques	10.1109/PACT.2011.67	computer architecture;transactional memory;parallel computing;real-time computing;computer science;operating system;programming language	Arch	-10.692064747450152	52.7098553566977	94617
b10460f47e241224494e73f36b643b9e00435435	online scheduling and placement of hardware tasks with multiple variants on dynamically reconfigurable field-programmable gate arrays		Hardware task scheduling and placement at runtime plays a crucial role in achieving better system performance by exploring dynamically reconfigurable Field-Programmable Gate Arrays (FPGAs). Although a number of online algorithms have been proposed in the literature, no strategy has been engaged in efficient usage of reconfigurable resources by orchestrating multiple hardware versions of tasks. By exploring this flexibility, on one hand, the algorithms can be potentially stronger in performance; however, on the other hand, they can suffer much more runtime overhead in selecting dynamically the best suitable variant on-the-fly based on its runtime conditions imposed by its runtime constraints. In this work, we propose a fast efficient online task scheduling and placement algorithm by incorporating multiple selectable hardware implementations for each hardware request; the selections reflect trade-offs between the required reconfigurable resources and the task runtime performance. Experimental studies conclusively reveal the superiority of the proposed algorithm in terms of not only scheduling and placement quality but also faster runtime decisions over rigid approaches. 2013 Elsevier Ltd. All rights reserved.	bitstream;coupling (computer programming);field-programmability;field-programmable gate array;moe;online algorithm;overhead (computing);reconfigurability;reconfigurable computing;run time (program lifecycle phase);scheduling (computing)	Thomas Marconi	2014	Computers & Electrical Engineering	10.1016/j.compeleceng.2013.07.004	parallel computing;real-time computing;computer science;operating system;distributed computing	EDA	-13.254477880705213	58.03220507307317	94633
282c4a0fb9fe4dd6f2bf8c3ce0c27715be22fda3	infiniband & openfabrics - infiniband and openfabrics at sc06	show-floor wide infiniband network;software interface;general discussion;rdma ethernet nics;openfabrics driver	General discussion about InfiniBand, what it is, how it works, and how the OpenFabrics driver stack provides the same software interface for both InfiniBand and 10gig RDMA ethernet nics. We will also include late breaking news about what we learned deploying a show-floor wide InfiniBand network as part of SCInet.	infiniband;remote direct memory access;scinet consortium	Troy Benjegerdes	2006		10.1145/1188455.1188497	parallel computing;computer science;operating system;sockets direct protocol;computer network	HPC	-11.122709096821518	46.581613212177295	94698
2641914225abdb6d6e3dab2ef9cbf3cec0482770	timing validation of automotive software	time budget;system modeling;holistic approach;upper bound;hard real time system;timing analysis;worst case response time;networked systems;performance optimization;time constraint	Embedded hard real-time systems need reliable guarantees for the satisfaction of their timing constraints. During the last years sophisticated analysis tools for timing analysis at the code-level, controllerlevel and networked system-level have been developed. This trend is exemplified by two tools: AbsInt’s timing analyzer aiT, and and Symtavision’s SymTA/S. aiT determines safe upper bounds for the execution times (WCETs) of non-interrupted tasks. SymTA/S computes the worst-case response times (WCRTs) of an entire system from the task WCETs and from information about possible interrupts and their priorities. A seamless integration between both tools provides for a holistic approach to timing validation: starting from a system model, a designer can perform timing budgeting, performance optimization and timing verification, thus covering both the code and the system aspects. However, the precision of the results and the efficiency of the analysis methods are highly dependent on the predictability of the execution platform. Especially on multi-core architectures this aspect becomes of critical importance. This paper describes an industry-strength tool flow for timing validation, and discusses prerequisites at the hardware level for ascertaining high analysis precision.	automotive software;best, worst and average case;embedded system;holism;interrupt;mathematical optimization;multi-core processor;real-time clock;real-time computing;seamless3d;static timing analysis	Daniel Kästner;Reinhard Wilhelm;Reinhold Heckmann;Marc Schlickling;Markus Pister;Marek Jersak;Kai Richter;Christian Ferdinand	2008		10.1007/978-3-540-88479-8_8	real-time computing;simulation;systems modeling;upper and lower bounds;static timing analysis	Embedded	-7.895230309487873	59.768890014637805	94842
422168a89f4d4590874ad9807ff0dd66613fe10e	conflict avoidance scheduling using grouping list for transactional memory	software;history;storage management;registers message systems hardware scheduling history synchronization software;registers;synchronization;message systems;scheduling;high contention transactional memory contention management conflict avoidance scheduling;concurrency control;log tm se conflict avoidance scheduling grouping list transactional memory performance degradation conflict characteristic group information concurrent execution;storage management concurrency control scheduling;transactional memory;contention management;conflict avoidance scheduling;hardware;high contention	Conventional Transactional Memory (TM) systems may experience performance degradation in applications with high contention, given the fact that execution of transaction will frequently restart due to conflicts. The restarting of transaction essentially requires rollback that is a wasteful operation. To address this point, we developed a system to reduce the overhead caused by high contention. In this paper, we present a method called Conflict Avoidance Scheduling (CAS), which prevents the conflicts in high contention by use of conflict characteristic. In CAS, threads that execute transactions which have high probability of conflicts are grouped together. Based on the group information, concurrent execution of threads in the same group is restricted. Therefore, threads that may cause conflict are serially executed. We evaluate the performance of the proposed design by comparing it with Log TM-SE. The simulation results show that our system improves the performance by 23% on an average in applications with high contention, as compared with the conventional Log TM-SE.	ats;baseline (configuration management);elegant degradation;log-space reduction;overhead (computing);performance evaluation;rollback (data management);run time (program lifecycle phase);scheduling (computing);simulation;thread (computing);transactional memory	Dongmin Choi;Seung-Hun Kim;Won Woo Ro	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.66	synchronization;transactional memory;parallel computing;real-time computing;computer science;operating system;concurrency control;distributed computing;processor register;scheduling	Arch	-10.1861445741045	49.97463471916075	94905
750fcd66bca7941f0950455f79b4b1b4e3821e5b	measuring and understanding extreme-scale application resilience: a field study of 5,000,000 hpc application runs	hybrid machines;torque;random access memory;xenon blades graphics processing units hardware random access memory torque servers;extreme scale;data driven resilience;supercomputer;data analysis;servers;application resilience;resilience;graphics processing units;blades;xenon;application failure probability extreme scale application resilience hpc application runs blue waters cray hybrid supercomputer system errors system failures workload logs error failure logs logdiver;system recovery cray computers failure analysis parallel machines parallel processing system monitoring;data driven resilience supercomputer resilience data analysis application resilience extreme scale hybrid machines;hardware	This paper presents an in-depth characterization of the resiliency of more than 5 million HPC application runs completed during the first 518 production days of Blue Waters, a 13.1 petaflop Cray hybrid supercomputer. Unlike past work, we measure the impact of system errors and failures on user applications, i.e., the compiled programs launched by user jobs that can execute across one or more XE (CPU) or XK (CPU+GPU) nodes. The characterization is performed by means of a joint analysis of several data sources, which include workload and error/failure logs. In order to relate system errors and failures to the executed applications, we developed LogDiver, a tool to automate the data pre-processing and metric computation. Some of the lessons learned in this study include: i) while about 1.53% of applications fail due to system problems, the failed applications contribute to about 9% of the production node hours executed in the measured period, i.e., the system consumes computing resources, and system-related issues represent a potentially significant energy cost for the work lost, ii) there is a dramatic increase in the application failure probability when executing full-scale applications: 20x (from 0.008 to 0.162) when scaling XE applications from 10,000 to 22,000 nodes, and 6x (from 0.02 to 0.129) when scaling GPU/hybrid applications from 2000 to 4224 nodes, and iii) the resiliency of hybrid applications is impaired by the lack of adequate error detection capabilities in hybrid nodes.	blue waters;central processing unit;compiler;computation;data pre-processing;error detection and correction;flops;full scale;graphics processing unit;ibm websphere extreme scale;image scaling;job stream;preprocessor;supercomputer	Catello Di Martino;William Kramer;Zbigniew T. Kalbarczyk;Ravishankar K. Iyer	2015	2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks	10.1109/DSN.2015.50	supercomputer;parallel computing;real-time computing;computer science;operating system;distributed computing;torque;data analysis;xenon;computer security;server	HPC	-16.976331830715793	50.06577832233354	94909
f2c7399f6705a746992d307890d4c83e613d4db6	scalable cooperative caching with rdma-based directory management for large-scale data processing	library i o cache rdma cooperative caching large scale data processing;cache storage;large scale data processing;cache;cooperative caching;i o;random accesses scalable cooperative caching rdma based directory management large scale data processing virtual file cache low latency remote direct memory access concentrated requests cache block access concentration data transfer cache management k computer fujitsu primehpc fx10 strided accesses sequential accesses;library parallel i o cache rdma cooperative caching large scale data processing;rdma;parallel i o;electronic data interchange cache storage;library;electronic data interchange	Cooperative caching provides an extensive virtual file cache by combining file caches on all nodes. We propose a novel cooperative caching method that addresses two problems of existing methods: lack of utilization of high-throughput, low-latency remote direct memory access (RDMA) and low scalability against concentrated requests for a particular cache block. The proposed method uses only RDMA to provide cooperative caching, namely, to lookup a cache block location in the cache directory, transfer a cache block between nodes, etc. In addition, nodes are partitioned into groups and managed semi-independently to achieve high scalability through mitigating access concentration on a particular node and to reduce data transfer for cache management. We implemented the proposed method as a library and evaluated it on the K computer and Fujitsu PRIMEHPC FX10. Results showed speedups for sequential, random and strided accesses.	cache (computing);remote direct memory access	Junya Arai;Yutaka Ishikawa	2012		10.1109/SC.Companion.2012.312	bus sniffing;input/output;cache-oblivious algorithm;snoopy cache;parallel computing;cache coloring;page cache;remote direct memory access;cache stampede;false sharing;cpu cache;library;cache;computer science;write-once;cache invalidation;operating system;electronic data interchange;database;smart cache;mesi protocol;cache algorithms;cache pollution	DB	-13.93777640645371	52.16879501316249	95120
45ccd3baf1d95ef7e911416c31d29ef3fd95208b	understanding big data analytics workloads on modern processors	social network services;electronic commerce;search engines;data analysis;performance optimization big data analytics workload characterization micro architectural characteristics;big data;big data social network services program processors search engines electronic commerce data analysis;big data analytics workload characterization micro architectural characteristics performance optimization;program processors	Big data analytics workloads are very significant ones in modern data centers, and it is more and more important to characterize their representative workloads and understand their behaviors so as to improve the performance of data center computer systems. In this paper, we embark on a comprehensive study to understand the impacts and performance implications of the big data analytics workloads on the systems equipped with modern superscalar out-of-order processors. After investigating three most important application domains in Internet services in terms of page views and daily visitors, we choose 11 representative data analytics workloads and characterize their micro-architectural behaviors by using hardware performance counters. Our study reveals that the big data analytics workloads share many inherent characteristics, which place them in a different class from the traditional workloads and the scale-out services. To further understand the characteristics of big data analytics workloads, we perform correlation analysis to identify the most key factors that affect cycles per instruction (CPI). Also, we reveal that the increasing complexity of the big data software stacks will put higher pressures on the modern processor pipelines.	big data;central processing unit;cycles per instruction;data center;hardware performance counter;page view;pipeline (computing);scalability;superscalar processor	Zhen Jia;Lei Wang;Jianfeng Zhan;Lixin Zhang;Chunjie Luo;Ninghui Sun	2017	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2016.2625244	analytics;parallel computing;big data;computer science;data science;operating system;database;data analysis;world wide web	Arch	-6.04948526615748	48.730885795771464	95253
041e59c8befa39abe98f629dd0370a20016939cb	the five-minute rule ten years later, and other computer storage rules of thumb	rule of thumb;indexation	Simple economic and performance arguments suggest appropriate lifetimes for main memory pages and suggest optimal page sizes. The fundamental tradeoffs are the prices and bandwidths of RAMs and disks. The analysis indicates that with today's technology, five minutes is a good lifetime for randomly accessed pages, one minute is a good lifetime for two-pass sequentially accessed pages, and 16 KB is a good size for index pages. These rules-of-thumb change in predictable ways as technology ratios change. They also motivate the importance of the new Kaps, Maps, Scans, and $/Kaps, $/Maps, $/TBscan metrics.	algorithm;auxiliary memory;bandwidth (signal processing);byte;computer data storage;disk sector;five-minute rule;kilobyte;randomness;sequential access	Jim Gray;Goetz Graefe	1997	SIGMOD Record	10.1145/271074.271094	computer science;artificial intelligence;database;rule of thumb;operations research	Metrics	-12.736566838677808	55.58793013322249	95612
3d658c5c758b2a567c7d9150759b2b6d6ddda50b	energy efficiency for large-scale mapreduce workloads with significant interactive analysis	energy efficiency;interaction analysis;empirical analysis;energy efficient;data analysis;large scale;mapreduce;energy saving	MapReduce workloads have evolved to include increasing amounts of time-sensitive, interactive data analysis; we refer to such workloads as MapReduce with Interactive Analysis (MIA). Such workloads run on large clusters, whose size and cost make energy efficiency a critical concern. Prior works on MapReduce energy efficiency have not yet considered this workload class. Increasing hardware utilization helps improve efficiency, but is challenging to achieve for MIA workloads. These concerns lead us to develop BEEMR (Berkeley Energy Efficient MapReduce), an energy efficient MapReduce workload manager motivated by empirical analysis of real-life MIA traces at Facebook. The key insight is that although MIA clusters host huge data volumes, the interactive jobs operate on a small fraction of the data, and thus can be served by a small pool of dedicated machines; the less time-sensitive jobs can run on the rest of the cluster in a batch fashion. BEEMR achieves 40-50% energy savings under tight design constraints, and represents a first step towards improving energy efficiency for an increasingly important class of datacenter workloads.	data center;job stream;mapreduce;real life;tracing (software)	Yanpei Chen;Sara Alspaugh;Dhruba Borthakur;Randy H. Katz	2012		10.1145/2168836.2168842	real-time computing;computer science;operating system;data mining;efficient energy use	OS	-18.65211001154484	57.497756042113345	95628
e735ac75e51b1e25f1217e139651b1f44151e40d	modeling speedup of spmd applications on the intel paragon: a case study	interconnection network;queueing network model	Interconnection networks with nearly distance independent communication latency are a key feature of recent architectures. However , shared resources such as network channels can become bottlenecks that degrade performance and limit workload scalability. In this paper, the problem of network contention in medium scale multi-computers is addressed. A queueing network model that predicts application speedup on the Paragon is presented. The model has been experimentally validated under the SUNMOS operating system. The experimental and modeling results are in good agreement and suggest ways to avoid internal network contention of communication intensive applications on the Paragon.	bottleneck (software);computer;experiment;intel paragon;interconnection;intranet;network model;operating system;queueing theory;spmd;sunmos;scalability;speedup	Evgenia Smirni;Emilia Rosti	1995		10.1007/BFb0046615	computer architecture;parallel computing;computer science;operating system	Metrics	-10.077432028771835	46.79006899948186	95730
02cdfde0b3a02780b1f30291c6c9f068b08545fb	interrupt handling in the loosely synchronized tmr system		Abstract#R##N##R##N#This paper discusses problems in the implementation of interrupt handling in the loosely synchronized Triple Modular Redundancy (TMR) system where synchronization, majority-decision and fault diagnosis are performed by software. The method of solution for the problems also is discussed. The major problems are as follows: (1) For the interrupt coming into three processors with a certain time difference in the task progress, the consistency among the global areas of the processors must be maintained; (2) The synchronization must be maintained among processors, which do not recognize interrupts due to the interrupt-inhibit period containing synchronization, and the processors, which recognized the interrupts; (3) When interrupts at different levels arrive at a processor, each processor must identify the interrupt with the highest priority consistently. To solve these problems, we have developed a multitask scheduler, which provides the variable representing the number of access to the global area, the flag indicating the interrupt-inhibit area, and the variable keeping the record of the accepted levels. This scheduler also controls the interrupt process by mutually referring those variables and flags. The system is implemented in SAFE system, and the overhead in the interrupt handling was measured.	interrupt;triple modular redundancy	Tomohiro Yoneda;Takashi Suzuoka;Yoshihiro Tohma	1985	Systems and Computers in Japan	10.1002/scj.4690160506	interrupt vector table;double fault;reentrancy;embedded system;real-time computing;interrupt handler;interrupt priority level;computer science;operating system;interrupt;interrupt latency;interrupts in 65xx processors;programmable interrupt controller	Robotics	-9.786979785829171	58.92031820089198	95792
48e51c059e98f74a9a59e1146ce9e311d5122ab9	improving the energy efficiency of big cores	kernel;overhead costs;computer architecture;parallel processing;energy efficiency	"""Traditionally, architectural innovations designed to boost single-threaded performance incur overhead costs which significantly increase power consumption. In many cases the increase in power exceeds the improvement in performance, resulting in a net increase in energy consumption. Thus, it is reasonable to assume that modern attempts to improve singlethreaded performance will have a negative impact on energy efficiency. This has led to the belief that """"Big Cores"""" are inherently inefficient. To the contrary, we present a study which finds that the increased complexity of the core microarchitecture in recent generations of the IntelR Core™ processor have reduced both the time and energy required to run various workloads. Moreover, taking out the impact of process technology changes, our study still finds the architecture and microarchitecture changes ---such as the increase in SIMD width, addition of the frontend caches, and the enhancement to the out-of-order execution engine--- account for 1.2x improvement in energy efficiency for these processors. This paper provides real-world examples of how architectural innovations can mitigate inefficiencies associated with """"Big Cores"""" ---for example, micro-op caches obviate the costly decode of complex x86 instructions--- resulting in a core architecture that is both high performance and energy efficient. It also contributes to the understanding of how microarchitecture affects performance, power and energy efficiency by modeling the relationship between them"""	central processing unit;intel core (microarchitecture);micro-operation;microarchitecture;out-of-order execution;overhead (computing);simd;thread (computing);x86	Kenneth Czechowski;Victor W. Lee;Ed Grochowski;Ronny Ronen;Ronak Singhal;Richard W. Vuduc;Pradeep Dubey	2014	2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)		computer architecture;parallel computing;kernel;real-time computing;computer hardware;computer science;operating system	Arch	-8.061096929554942	53.096366115286045	95827
2a4da8c57c2cd4a39778b1225e1bdbf9ef4c04e8	supporting insertions and deletions in striped parallel filesystems	file systems concurrent computing bridges round robin disk drives message passing bandwidth turning throughput joining processes;distributed processing;message passing multiprocessor insertions deletions striped parallel filesystems processing rates parallel computers distributed file structure;distributed file structure;multiprocessing systems distributed processing file organisation message passing;parallel file system;parallel computer;message passing;parallel computers;multiprocessing systems;insertions;processing rates;message passing multiprocessor;deletions;striped parallel filesystems;file organisation	The dramatic improvements in the processing rates of parallel computers are turning many compute-bound jobs into IO-bound jobs. Parallel le systems have been proposed to better match IO throughput to processing power. Many parallel le systems stripe les across numerous disks; each disk has its own controller. A striped le can be appended (or prepended) to and maintain its structure. However, a block can't be inserted into or deleted from the middle of the le, since doing so would destroy the regular striping structure of the le. In this paper, we present a distributed le structure that maintains les in indexed striped extents on a message passing multiprocessor. This approach allows highly parallel random and sequential reads, and also allows insertion and deletion into the middle of the le.	computer;data striping;disk storage;i/o bound;insertion sort;job stream;message passing;multiprocessing;parallel computing;random access;supercomputer;throughput	Theodore Johnson	1993		10.1109/IPPS.1993.262921	fork;parallel computing;torrent file;device file;computer file;computer science;stub file;versioning file system;operating system;fstab;unix file types;journaling file system;distributed computing;open;distributed file system;everything is a file;data file;file system fragmentation;file control block	HPC	-12.944266095040282	51.80235709112039	96118
0897bb3acccb5599bd4747eebc213407787b4607	tradeoffs in buffering memory state for thread-level speculation in multiprocessors	cache storage;thread level speculation;multi threading;performance evaluation;memory management proposals taxonomy merging pollution;cache storage multi threading performance evaluation multiprocessing systems parallel architectures;parallel architectures;multiple variable versions buffering memory state thread level speculation multiprocessors architectural support hard to analyze code parallel code distributed caches taxonomy multi version speculative memory state complexity benefit tradeoff analysis performance evaluation;multiprocessing systems	Thread-level speculation provides architectural support to aggressively run hard-to-analyze code in parallel. As speculative tasks run concurrently, they generate unsafe or speculative memory state that needs to be separately buffered and managed in the presence of distributed caches and buffers. Such state may contain multiple versions of the same variable. In this paper, we introduce a novel taxonomy of approaches to buffer and manage multi-version speculative memory state in multiprocessors. We also present a detailed complexity-benefit tradeoff analysis of the different approaches. Finally, we use numerical applications to evaluate the performance of the approaches under a single architectural framework. Our key insights are that support for buffering the state of multiple speculative tasks and versions per processor is more complexity-effective than support for merging the state of tasks with main memory lazily. Moreover, both supports can be gainfully combined and, in large machines, their effect is nearly fully additive. Finally, the more complex support for future state in main memory can boost performance when buffers are under pressure, but hurts performance when squashes are frequent.	computer data storage;enterprise architecture framework;numerical analysis;speculative execution;speculative multithreading;thread (computing);utility functions on indivisible goods	María Jesús Garzarán;Milos Prvulovic;José María Llabería;Víctor Viñals;Lawrence Rauchwerger;Josep Torrellas	2003		10.1109/HPCA.2003.1183537	computer architecture;parallel computing;real-time computing;multithreading;computer science;operating system;cache-only memory architecture	Arch	-11.199283523439712	50.26171866573436	96236
6bc72e51cb92dda1e2816c17d7e21430e0b8c9a5	incremental elasticity for nosql data stores		Service elasticity, the ability to rapidly expand or shrink service processing capacity on demand, has become a first-class property in the domain of infrastructure services. Scalable NoSQL data stores are the de-facto choice of applications aiming for scalable, highly available data persistence. The elasticity of such data stores is still challenging, due to the complexity and performance impact of moving large amounts of data over the network to take advantage of new resources (servers). In this paper we propose incremental elasticity, a new mechanism that progressively increases processing capacity in a fine-grain manner during an elasticity action by making sub-sections of the transferred data available for access on the new server, prior to completing the full transfer. In addition, by scheduling data transfers during an elasticity action in sequence (rather than as simultaneous transfers) between each pre-existing server involved and the new server, incremental elasticity leads to smoother elasticity actions, reducing their overall impact on performance.	aggregate data;apache cassandra;complexity;data store;elasticity (cloud computing);elasticity (data store);nosql;persistence (computer science);persistent data structure;scalability;scheduling (computing);server (computing);throughput	Antonis Papaioannou;Kostas Magoutis	2017	2017 IEEE 36th Symposium on Reliable Distributed Systems (SRDS)	10.1109/SRDS.2017.26	throughput;nosql;distributed computing;elasticity (data store);database;cloud computing;computer science;elasticity (economics);scheduling (computing);server;benchmark (computing)	OS	-18.281851493567952	54.301314231510595	96259
51d28dd3066aad5cb465b4277d66f4ec432bfb1d	taming performance hotspots in cloud storage with dynamic load redistribution		Cloud storage services are associated with high latency variance, and degraded throughput which is problematic when users are fetching and storing content for interactive applications. This can be attributed to performance hotspots created by slow nodes in a storage cluster, and performance interference caused by multi-tenancy, and background tasks such as data scrubbing, backfilling, recovery, etc. In this paper, we present DLR, a system that improves the performance of cloud storage services in the presence of hardware heterogeneity, and performance interference through a dynamic load redistribution technique. We designed DLR to dynamically adjust the load serving ratio of storage servers based on the system-level performance measurements from the storage cluster. We implemented DLR using Ceph, a popular distributed object storage system, and evaluated its performance on NSFCloud's Chameleon testbed using Ceph's Rados benchmark. Experimental results show that DLR improves the average throughput and latency of Ceph storage by up to 65%, and 41% respectively compared to the default case. Compared to Ceph's in-built load balancing technique, DLR improves the throughput by up to 98%, and latency by 96%.	algorithm;benchmark (computing);cloud storage;computer data storage;data scrubbing;distributed object;dynamic language runtime;high- and low-level;interference (communication);load balancing (computing);multitenancy;object storage;open-source software;processor affinity;software-defined storage;tame;testbed;throughput	Ridwan Rashid Noel;Palden Lama	2017	2017 IEEE 10th International Conference on Cloud Computing (CLOUD)	10.1109/CLOUD.2017.15	parallel computing;throughput;latency (engineering);real-time computing;computer science;cloud storage;data scrubbing;cloud computing;server;computer data storage;load balancing (computing)	HPC	-15.60815112619726	53.859912899145215	96322
1d8f38f5d6dc440e010b99274e73458df9896607	dquob: managing large data flows using dynamic embedded queries	high volume data streams;large data flows;large scale visualizations;video streaming;software libraries;query processing;high performance computing;sql;global atmospheric modeling;cost metric dynamic embedded queries dquob system high volume data streams large data flows large scale visualizations video streaming business transactions relational database tables sql quoblet compiled code data model query optimization global atmospheric modeling experiment high performance computing;data stream;relational database tables;filters;software performance evaluation;query optimization;relational database;satisfiability;data model;dynamic embedded queries;compiled code;dquob system;streaming media;data visualization;bandwidth;scientific information systems query processing relational databases sql data models software performance evaluation;streaming media software libraries delay large scale systems data visualization relational databases bandwidth filters educational institutions data models;relational databases;experiment;quoblet;business transactions;cost metric;scientific information systems;large data;large scale systems;data models;volume data	The dQUOB system satis es client need for speci c information from high-volume data streams. The data streams we speak of are the ow of data existing during large-scale visualizations, video streaming to large numbers of distributed users, and high volume business transactions. We introduces the notion of conceptualizing a data stream as a set of relational database tables so that a scientist can request information with an SQL-like query. Transformation or computation that often needs to be performed on the data en-route can be conceptualized as computation performed on consecutive views of the data, with computation associated with each view. The dQUOB system moves the query code into the data stream as a quoblet; as compiled code. The relational database data model has the significant advantage of presenting opportunities for e cient reoptimizations of queries and sets of queries. Using examples from global atmospheric modeling, we illustrate the usefulness of the dQUOB system. We carry the examples through the experiments to establish the viability of the approach for high performance computing with a baseline benchmark. We de ne a cost-metric of end-to-end latency that can be used to determine realistic cases where optimization should be applied. Finally, we show that end-to-end latency can be controlled through a probability assigned to a query that a query will evaluate to true.	baseline (configuration management);benchmark (computing);compiler;computation;data model;electrical resistivity tomography;embedded system;end-to-end encryption;end-to-end principle;experiment;mathematical optimization;relational database;sql;streaming media;supercomputer;table (database)	Beth Plale;Karsten Schwan	2000		10.1109/HPDC.2000.868658	sargable;query optimization;parallel computing;relational database;computer science;theoretical computer science;data mining;database;programming language;view;data visualization	HPC	-11.320380872350338	49.76159753254194	96366
a353767caa2972898783a655f1fa606bc924c135	hardware acceleration in ceph distributed file system	ceph multicore hardware accelerators clustered storage distributed file system;network operating systems;metadata hardware acceleration ceph distributed file system cloud computing data services software services distributed data storage layer acceleration mechanisms multicore network soc overall cluster responsiveness open source distributed file system posix api;public domain software;hardware system on chip engines monitoring file systems multicore processing;system on chip;system on chip application program interfaces cloud computing meta data network operating systems public domain software;application program interfaces;meta data;cloud computing	Cloud computing becomes more and more popular providing the means to organize and deliver almost any kind of data and software services. Since it is inherently scalable to support rapid economic growth and productivity, its distributed data storage layer needs to be capable to address these requirements. A distributed file system contains a large number of cluster nodes and a large number of clients interacting with it. In this article, we propose two acceleration mechanisms based on multi-core network SoC to maximize each cluster node performance. First, the requests to the cluster node are balanced evenly on the platform cores, and second the requests are classified to decrease latency of sensitive operations and improve the overall cluster responsiveness and availability. For implementation, tests and measurements we have used a novel open source distributed file system: Ceph. Among many advantages over the competitors of Ceph we mention preservation of POSIX API, completely decoupled data and metadata and usage of object storage devices instead of block devices.	application programming interface;cloud computing;clustered file system;computer data storage;dce distributed file system;hardware acceleration;interaction;multi-core processor;object storage;open-source software;posix;requirement;responsiveness;scalability;system on a chip	Sorin Andrei Pistirica;Florica Moldoveanu;Alin Moldoveanu;Victor Asavei;Claudiu Mihai Caraman	2013	2013 IEEE 12th International Symposium on Parallel and Distributed Computing	10.1109/ISPDC.2013.35	system on a chip;self-certifying file system;parallel computing;real-time computing;cloud computing;network file system;object storage;computer science;operating system;distributed computing;open;distributed file system;programming language;metadata;public domain software;replication;computer network	HPC	-16.995460922934882	53.22151709152319	96386
ff5a1b2977a6f7f48b17a4d42116e9cb2d8d5522	an enforcement of real time scheduling in spark streaming	reliability;spark streaming;streaming processing;processor scheduling;data processing;spark streaming streaming processing big data real time scheduling;unstable event streams real time scheduling system spark streaming continuous data streams real time streaming processing open source frameworks low latency stream processing real time stream processing framework commodity hardware real time event processing effective scheduling strategy worst case event processing time dynamic adjusting time window batch intervals real time enhancement spark framework streaming benchmarks;big data;scheduling;real time scheduling;sparks;scheduling media streaming public domain software;delays sparks real time systems scheduling processor scheduling data processing reliability;delays;real time systems	With the exponential growth in continuous data streams, real time streaming processing has been gaining a lot of popularity. Spark Streaming is one of the open source frameworks for reliable, high-throughput and low latency stream processing. Though it is a near real time stream processing framework running on commodity hardware, real time event processing is not guaranteed in its scheduling system. Profiling results indicate that the total delay time of events with unstable inputs is more volatile and presents big fluctuations. In this paper, we propose a simple, yet effective scheduling strategy to reduce the worst case event processing time by dynamic adjusting the time window of batch intervals. It is a real time enhancement to Spark Streaming based on Spark's framework. The proposed strategy is evaluated using two streaming benchmarks and our preliminary results demonstrate the feasibility of our approach with unstable event streams.	best, worst and average case;commodity computing;complex event processing;control theory;database;display resolution;high-throughput computing;online and offline;open-source software;process architecture;real-time computing;real-time operating system;spark;scheduling (computing);stream processing;streaming algorithm;streaming media;throughput;time complexity	Xinyi Liao;Zhiwei Gao;Weixing Ji;Yizhuo Wang	2015	2015 Sixth International Green and Sustainable Computing Conference (IGSC)	10.1109/IGCC.2015.7393730	fair-share scheduling;embedded system;real-time computing;dynamic priority scheduling;computer science;operating system	DB	-18.445613056768167	55.362982848415975	96474
16c65a79250ed683f49cf19c3a54054438a2f38f	highly efficient synchronization based on active memory operations	distributed memory systems;distributed memory systems parallel processing synchronisation;active messages;active messages synchronization techniques active memory operation parallel application network latency approach processor cycles large scale multiprocessor home memory controller processor centric atomic instruction memory side atomic instructions;synchronisation;large scale;synchronization techniques active memory operation parallel application network latency approach processor cycles large scale multiprocessor home memory controller processor centric atomic instruction memory side atomic instructions active messages;synchronisation distributed memory systems parallel processing;delay concurrent computing large scale systems cities and towns protection random access memory system performance reduced instruction set computing libraries distributed processing;parallel applications;parallel processing	"""Summary form only given. Synchronization is a crucial operation in many parallel applications. As network latency approaches thousands of processor cycles for large scale multiprocessors, conventional synchronization techniques are failing to keep up with the increasing demand for scalable and efficient synchronization operations. We present a mechanism that allows atomic synchronization operations to be executed on the home memory controller of the synchronization variable. By performing atomic operations near where the data resides, our proposed mechanism can significantly reduce the number of network messages required by synchronization operations. Our proposed design also enhances performance by using fine-grained updates to selectively """"push """" the results of offloaded synchronization operations back to processors when they complete (e.g., when a barrier count reaches the desired value). We use the proposed mechanism to optimize two of the most widely used synchronization operations, barriers and spin locks. Our simulation results show that the proposed mechanism outperforms conventional implementations based on load-linked/store-conditional, processor-centric atomic instructions, conventional memory-side atomic instructions, or active messages. It speeds up conventional barriers by up to 2.1 (4 processors) to 61.9 (256 processors) and spin locks by a factor of up to 2.0 (4 processors) to 10.4 (256 processors)."""	active message;add-ons for firefox;algorithm;central processing unit;failure;ll grammar;ll parser;linearizability;load-link/store-conditional;lock (computer science);mac os x 10.4 tiger;memory controller;multiprocessing;parallel computing;programming model;scalability;simulation;spinlock	Lixin Zhang;Zhen Fang;John B. Carter	2004	18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.	10.1109/IPDPS.2004.1302981	uniform memory access;parallel processing;synchronization;interleaved memory;parallel computing;real-time computing;distributed memory;computer science;operating system;distributed computing;computer memory;data synchronization;synchronization;registered memory;compare-and-swap	Arch	-11.999182046033921	49.61900369284491	96477
0f6e1c097f2a40594913778a041bcbae723a9255	another kind of virtual tape: optimized for backup & recovery			virtual tape library	Bill Chait;Paul Linstead	2000			embedded system;computer hardware;backup;virtual tape library;computer science	DB	-18.6325389076722	51.26697445093331	96580
7e7b9566a4ee159fb6fc60a798cf2c6458967774	a generic availability model for clustered computing system	system availability generic availability model clustered computing systems cluster manager processing nodes active processing nodes application software management software stochastic petri net models cluster availability state aggregation fixed point iteration uniqueness switchover time coverage ratio;generic availability model;cluster computing;concurrent computing;processing nodes;application software;stochastic processes workstation clusters fault tolerant computing petri nets;availability;stochastic petri net;switchover time;software management;system availability;distributed computing;stochastic petri net models;state aggregation;fixed point iteration;technology management;fixed point;computer architecture;fault tolerant computing;availability application software hardware operating systems stochastic processes concurrent computing distributed computing computer architecture sun technology management;stochastic processes;sun;cluster availability;clustered computing systems;workstation clusters;coverage ratio;petri nets;active processing nodes;existence and uniqueness;uniqueness;management software;operating systems;hardware;cluster manager	"""We study the availability of a clustered computing system with one cluster manager and """"N+M"""" processing nodes, where M processing nodes serve as spares for the N active processing nodes. The functionality of an individual processing node is dissected into application software, management software, OS and hardware. The dependency among these entities is considered. Stochastic Petri net models are constructed to investigate the cluster availability. In order to deal with a cluster of a very large size, a solution based on state aggregation and fixed-point iteration is proposed. The existence and uniqueness of the fixed point is proved. The impact of a cluster manager, switchover time and coverage ratio are quantitatively studied. From the numerical results of a simple cluster with """"2+1"""" processing nodes, we find that: (1) the availability of the cluster manager does not have a significant impact on the system availability, (2) system availability increases with the coverage ratio and decreases with the switchover time. Mechanisms to improve the system availability are discussed."""	computer cluster	Hairong Sun;James J. Han;Haim Levendel	2001		10.1109/PRDC.2001.992704	fixed-point iteration;embedded system;availability;application software;real-time computing;stochastic petri net;concurrent computing;computer cluster;computer science;technology management;operating system;distributed computing;fixed point;debt service coverage ratio;petri net	HPC	-18.570517941411655	57.619720545030084	96588
ac7bd84df09eca6d9ec5ec7110047a761c84ee05	a strategy to emulate nor flash with nand flash	nand flash memory;flash memory;nand;information extraction;mobile phone;embedded system design;nor;data prefetching;data access	This work is motivated by a strong market demand for the replacement of NOR flash memory with NAND flash memory to cut down the cost of many embedded-system designs, such as mobile phones. Different from LRU-related caching or buffering studies, we are interested in prediction-based prefetching based on given execution traces of application executions. An implementation strategy is proposed for the storage of the prefetching information with limited SRAM and run-time overheads. An efficient prediction procedure is presented based on information extracted from application executions to reduce the performance gap between NAND flash memory and NOR flash memory in reads. With the behavior of a target application extracted from a set of collected traces, we show that data access to NOR flash memory can respond effectively over the proposed implementation.	cpu cache;data access;embedded system;flash memory;mobile phone;static random-access memory;tracing (software)	Yuan-Hao Chang;Jian-Hong Lin;Jen-Wei Hsieh;Tei-Wei Kuo	2010	TOS	10.1145/1807060.1807062	flash file system;data access;parallel computing;computer hardware;computer science;operating system;flash memory emulator;computer memory;nand gate;universal memory;information extraction	Embedded	-10.006931764626243	54.52967692514675	96682
21fbd3111936fc99d66287a1b40d7e0abb9301ff	an efficient write buffer management scheme considering the parallelism in solid-state drives	write buffer manager;temporal locality;solid state drives ssds		parallel computing;solid-state drive;write buffer	Seongjoon Do;Eunji Lee;Taeseok Kim	2013	IEICE Electronic Express	10.1587/elex.10.20130018	locality of reference;parallel computing;real-time computing;computer hardware;computer science;write buffer;write combining	DB	-11.50377191267573	52.35383187009044	96730
5d87801d0125894fbb55e68b2d69f9e92d89552a	loop transforming for reducing data alignment on multi-core simd processors	loop transformation;simd architecture;data alignment;multi processors	Multimedia SIMD extensions are commonly employed today to speed up media processing. When performing vectorization for SIMD architectures, one of the major issues is to handle the problem of memory alignment. Prior study focused on either vectorizing loops with all memory references being properly aligned, or introducing extra operations to deal with the misaligned memory references. On the other hand, multi-core SIMD architectures require coarse-grain parallelism. Therefore, it is an important problem to study how to parallelize and vectorize loop nests with the awareness of data misalignments. This paper presents a loop transformation scheme that maximizes the parallelism of outermost loops, while the misaligned memory references in innermost loops are reduced. The basic idea of our technique is to align each level of loops in the nest, considering the constraint of dependence relations. To reduce the data misalignments, we establish a mathematical model with a concept of offset-collection and propose an effective heuristic algorithm. For coarser-grain parallelism, we propose some rules to analyze the outermost loop. When transformations are applied, the inner loops are involved to maximize the parallelism. To avoid introducing more data misalignments, the involved innermost loop is handled from other levels of loops. Experimental results show that 7 % to 37 % (on average 18.4 %) misaligned memory references can be reduced. The simulations on CELL show that 1.1x speedup can be reached by reducing the misaligned data, while 6.14x speedup can be achieved by enhancing the parallelism for multi-core.	data structure alignment;multi-core processor;simd	Yi Wang;Linfeng Pan;Zili Shao;Yong Guan;Minyi Guo	2014	Signal Processing Systems	10.1007/s11265-013-0754-2	loop fusion;embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;data structure alignment;programming language;algorithm	Arch	-4.760421132744063	50.60361726679027	96867
a3cca23869d0ca59149949f3175043949e1eb552	an improved distributed file system based on gpu acceleration		HDFS is a popular distributed file system, widely used in many commercial fields, which can store TB, even PB level data. Fast data reading and writing is the most important problem for HDFS. However, with the volume of data increasing sharply, the traditional HDFS, built on the PC cluster platform, is no longer suitable for fast data reading and writing. GPU is a highly parallel computing unit. Its power of calculation, reading and writing is hundreds of times as fast as CPU. Hence, this paper proposes an improved distributed file system, which uses GPU as an accelerator. Firstly, the improved HDFS uses GPU instead of CPU response data reading and writing requests. Secondly, the improved HDFS uses GPU’s cache as a buffer memory for data reading and writing. These two strategies significantly improve the performance of the distributed file system. The experimental results have proved the effectiveness of the improved algorithm.	algorithm;apache hadoop;cpu cache;central processing unit;clustered file system;dce distributed file system;graphics processing unit;parallel computing;terabyte	Songtao Shang;Yong Gan;Huaiguang Wu	2018	2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2018.8466439	acceleration;distributed file system;parallel computing;cache;central processing unit;parallel processing;benchmark (computing);computer science	HPC	-13.605796640820767	52.501464786677566	96885
511b8b723dd02403235ae9c55a4ba0bbb0836314	parallel block vectors: collection, analysis, and uses	performance and reliability;performance evaluation;parallel programming;parallel block vector profiles open source tool harmony architectural design pbv application perturbation parallel program hardware multithreaded application;performance analysis and design aids;software engineering;system performance;computer programs;computer architecture;programming techniques;parallel architectures;parallel block vector;performance measures;metrics measurement;parallel architectures multiprocessing systems;measurements;multiprocessing systems;parallel block vector parallel programming programming techniques software engineering performance analysis and design aids performance and reliability hardware performance measures metrics measurement;programming;computer architecture parallel processing system performance computer programs performance evaluation programming hardware measurements;parallel processing;hardware	Parallel block vector profiles (PBVs) establish a mapping between a multithreaded application's basic blocks and the degree of parallelism the application exhibits each time a block executes. PBVs offer a new perspective that helps users both reason about parallel programs' hardware and software interactions and identify opportunities for performance improvements. Here, the authors present two PBV applications for architectural design and discuss further opportunities to apply PBVs in other fields. They also demonstrate how the open-source tool Harmony lets programmers collect PBVs with minimal programmer effort and application perturbation.	basic block;degree of parallelism;interaction;open-source software;parallel computing;programmer;thread (computing)	Melanie Kambadur;Kui Tang;Martha A. Kim	2013	IEEE Micro	10.1109/MM.2013.25	parallel processing;programming;computer architecture;parallel computing;computer science;theoretical computer science;operating system;programming language;measurement	Arch	-7.43764517851992	47.22332268717028	97153
b0947b169aba058ebe06e7244783998444b2b748	efficient execution of augmented reality applications on mobile programmable accelerators	heterogeneous cgra mobile programmable accelerators augmented reality applications mobile devices smartphones tablets user experiences interactive interface visual interface battery life system on chip soc coarse grained reconfigurable architecture cgra hot loops software pipelining multimedia applications energy requirements simd capabilities energy efficiency data level parallelism dlp instruction level parallelism ilp simd cores replicating configuration fetching configuration ring network recycle buffer memory access units memory banks simd data access handling simd mode code generation resource utilization simd core resource utilization performance improvement;software recycling augmented reality acceleration computer architecture schedules buffer storage;performance evaluation;system on chip augmented reality mobile computing parallel processing performance evaluation pipeline processing power aware computing program compilers program control structures reconfigurable architectures smart phones;reconfigurable architectures;program control structures;smart phones;power aware computing;system on chip;augmented reality;program compilers;mobile computing;parallel processing;pipeline processing	Mobile devices are ubiquitous in daily lives. From smartphones to tablets, customers are constantly demanding richer user experiences through more visual and interactive interface with prolonged battery life. To meet the demands, accelerators are commonly adopted in system-on-chip (SoC) for various applications. Coarse-grained reconfigurable architecture (CGRA) is a promising solution, which accelerates hot loops with software pipelining. Although CGRAs have shown that they can support multimedia applications efficiently, more interactive applications such as augmented reality put much more pressure on performance and energy requirements. In this paper, we extend heterogeneous CGRA to provide SIMD capabilities, which improves performance and energy efficiency significantly for augmented reality applications. We show that if we can exploit data level parallelism (DLP), it is more beneficial to run on SIMD natively than to transform it into instruction level parallelism (ILP) and run on CGRA. To utilize this property, multiple processing elements in CGRA are grouped to form homogeneous SIMD cores. To reduce the hardware overhead of fetching and replicating configuration in SIMD mode, we propose a ring network and a recycle buffer to pass the configuration around as well as to temporarily store it, which has minimized impact on throughput. Also, we modify memory access units and memory banks to support split memory transactions with forwarding for handling SIMD data access. To adapt to the proposed extension, we introduce a compile technique for SIMD mode code generation to maximize the resource utilization of each SIMD core. Experimental results show that it is possible to achieve an average of 17.6% performance improvement while saving 16.9% energy over heterogeneous CGRA.	augmented reality;baseline (configuration management);code generation (compiler);compiler;data access;data parallelism;digital light processing;experience;instruction-level parallelism;memory bank;mobile app;mobile operating system;overhead (computing);parallel computing;pipeline (computing);requirement;ring network;simd;scalability;smartphone;software pipelining;software transactional memory;speedup;system on a chip;throughput;user interface;vii	Jason Jong Kyu Park;Yongjun Park;Scott A. Mahlke	2013	2013 International Conference on Field-Programmable Technology (FPT)	10.1109/FPT.2013.6718350	system on a chip;embedded system;parallel processing;augmented reality;parallel computing;real-time computing;computer science;operating system;mobile computing	Arch	-9.200970779137773	53.92741261677143	97189
b4c68609541063598981f2885fd3620dd8041c3b	a quantitative analysis of the performance and scalability of distributed shared memory	protocols;gestion memoire;controller occupancy;performance evaluation;systeme multiprocesseur memoire repartie;flash;implementation;storage management;performance comparison;cache memory;access protocol;memory overhead quantitative analysis performance scalability distributed shared memory cache coherence protocols shared memory multiprocessors bit vector coarse vector protocols sci based protocols coma protocols stanford flash multiprocessor;antememoria;ejecucion;large scale;gestion memoria;antememoire;magic;scalable multiprocessors;cache coherence protocol;cache coherence protocols;distributed shared memory systems;sistema multiprocesador memoria distribuida;analyse performance;performance analysis scalability delay coherence access protocols time measurement large scale systems data structures clocks laboratories;performance analysis;quantitative analysis;flexible node controller;distributed memory multiprocessor system;protocole acces;dynamic pointer allocation;distributed shared memory;performance evaluation protocols distributed shared memory systems;acceso protocolo;coma;sci;shared memory multiprocessor;analisis eficacia;bitvector	cache coherence protocols have become the key technology for creating moderate to large-scale shared-memory multiprocessors. Although the performance of such multiprocessors depends critically on the performance of the cache coherence protocol, little comparative performance data is available. Existing commercial implementations use a variety of different protocols including bit-vector/coarse-vector protocols, SCI-based protocols, and COMA protocols. Using the program-mable protocol processor of the Stanford FLASH multiprocessor, we provide a detailed, implementation-oriented evaluation of four popular cache coherence protocols. In addition to measurements of the characteristics of protocol execution (e.g. memory overhead , protocol execution time, and message count) and of overall performance, we examine the effects of scaling the processor count from 1 to 128 processors. Surprisingly, the optimal protocol changes for different applications and can change with processor count even within the same application. These results help identify the strengths of specific protocols and illustrate the benefits of providing flexibility in the choice of cache coherence protocol.	bit array;cpu cache;cache coherence;central processing unit;communications protocol;cyrix coma bug;distributed shared memory;image scaling;multiprocessing;overhead (computing);pointer (computer programming);run time (program lifecycle phase);scalability	Mark Heinrich;Vijayaraghavan Soundararajan;John L. Hennessy;Anoop Gupta	1999	IEEE Trans. Computers	10.1109/12.752662	bus sniffing;distributed shared memory;embedded system;communications protocol;pipeline burst cache;parallel computing;real-time computing;cache coloring;cpu cache;cache;msi protocol;computer science;quantitative analysis;write-once;cache invalidation;coma;operating system;distributed computing;magic;mesi protocol;implementation;cache algorithms;moesi protocol;mesif protocol	Arch	-12.082828756519762	49.30447568872577	97360
dea49635947e1746231e51716266d3b1a0b3298b	ptl: pram translation layer	nand flash memory;pram;storage systems;flash translation layer;pram translation layer	In this paper, we attempt to replace NAND Flash memory with PRAM, while PRAM initially targets replacing NOR Flash memory. To achieve it, we need to handle wear-leveling issue of PRAM since the maximum number of writes in PRAM is only 10^6. Thus, we have proposed PRAM Translation Layer (PTL) to resolve endurance problem for a PRAM-based storage system. We modified FlashSim to support both PRAM and NAND Flash memory and measured the performance by using real workloads from PC and server. In our experiment, PRAM shows up to 300% performance improvement compared to NAND Flash memory. Moreover, our results revealed that the PRAM's endurance is improved up to 25% compared to NAND Flash memory due to no erase operation. All these results suggest that PRAM is a viable candidate to replace NAND Flash memory.	pass transistor logic	Gyu Sang Choi;Byung-Won On;Kwonhue Choi;Sungwon Yi	2013	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2012.07.002	flash file system;embedded system;parallel computing;computer hardware;computer science;operating system	EDA	-11.835644542092037	53.77512655415196	97782
c0ba4aabd12cddf044503640a337d7ffec5dd14c	power gating strategies on gpus	leakage;energy efficient;leakage reduction;power gating;gpu;clock gating;low power;leakage power;graphic processing unit;power reduction;dynamic voltage and frequency scaling	As technology continues to shrink, reducing leakage is critical to achieving energy efficiency. Previous studies on low-power GPUs (Graphics Processing Units) focused on techniques for dynamic power reduction, such as DVFS (Dynamic Voltage and Frequency Scaling) and clock gating. In this paper, we explore the potential of adopting architecture-level power gating techniques for leakage reduction on GPUs. We propose three strategies for applying power gating on different modules in GPUs. The Predictive Shader Shutdown technique exploits workload variation across frames to eliminate leakage in shader clusters. Deferred Geometry Pipeline seeks to minimize leakage in fixed-function geometry units by utilizing an imbalance between geometry and fragment computation across batches. Finally, the simple time-out power gating method is applied to nonshader execution units to exploit a finer granularity of the idle time. Our results indicate that Predictive Shader Shutdown eliminates up to 60% of the leakage in shader clusters, Deferred Geometry Pipeline removes up to 57% of the leakage in the fixed-function geometry units, and the simple time-out power gating mechanism eliminates 83.3% of the leakage in nonshader execution units on average. All three schemes incur negligible performance degradation, less than 1%.	clock gating;computation;dynamic frequency scaling;dynamic voltage scaling;elegant degradation;execution unit;fixed-function;geometry pipelines;graphics processing unit;low-power broadcasting;power gating;shader;shutdown (computing);spectral leakage	Po-Han Wang;Chia-Lin Yang;Yen-Ming Chen;Yu-Jung Cheng	2011	TACO	10.1145/2019608.2019612	parallel computing;real-time computing;computer hardware;computer science;operating system;leakage;efficient energy use;clock gating	Arch	-6.773099048177142	55.04335349924707	97833
1c46372c6c68371c71d0c76b00cd63db9922836a	using virtual lines to enhance locality exploitation	temporal locality;cache architecture;chip;memory access;performance improvement;data cache;hardware design;numerical codes;spatial locality;memory hierarchy;memory latency	Because the spatial locality of numerical codes is significant, the potential for performance improvements is important. However, large cache lines cannot be used in current on-chip data caches because of the important pollution they breed. In this paper, we propose a hardware design, called the Virtual Line Scheme, that allows the utilization of large virtual cache lines when fetching data from memory for better exploitation of spatial locality, while the actual physical cache line is smaller than currently found cache lines for better exploitation of temporal locality. Simulations show that a 17% to 64% reduction of the average memory access time can be obtained for a 20-cycle memory latency. It is also shown how simple software informations can be used to significantly decrease memory traffic, a flaw associated with the utilization of large cache lines.	average memory access time;cas latency;cpu cache;cache (computing);code;computer simulation;flaw hypothesis methodology;locality of reference;numerical analysis;principle of locality	Olivier Temam;Yvon Jégou	1994		10.1145/181181.181559	chip;locality of reference;bus sniffing;uniform memory access;least frequently used;interleaved memory;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cas latency;computer hardware;cache;computer science;write-once;cache invalidation;smart cache;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;memory map;non-uniform memory access	Arch	-9.000935138304655	52.73008397752612	97859
2b22336c408feb0ab0d872dd4474a75ee0b6b5ac	providing a shared file system in the hare posix multikernel	electrical engineering and computer science;thesis	Hare is a new multikernel operating system that provides a single system image for multicore processors without cache coherence. Hare allows applications on different cores to share files, directories, file descriptors, sockets, and processes. The main challenge in designing Hare is to support shared abstractions faithfully enough to run applications that run on traditional shared-memory operating systems with few modifications, and to do so while scaling with an increasing number of cores. To achieve this goal, Hare must support shared abstractions (e.g., file descriptors shared between processes) that appear consistent to processes running on any core, but without relying on hardware cache coherence between cores. Moreover, Hare must implement these abstractions in a way that scales (e.g., sharded directories across servers to allow concurrent operations in that directory). Hare achieves this goal through a combination of new protocols (e.g., a 3-phase commit protocol to implement directory operations correctly and scalably) and leveraging properties of non-cache coherent multiprocessors (e.g., atomic low-latency message delivery and shared DRAM). An evaluation on a 40-core machine demonstrates that Hare can run many challenging Linux applications (including a mail server and a Linux kernel build) with minimal or no modifications. The results also show these applications achieve good scalability on Hare, and that Hare’s techniques are important to achieving scalability. Thesis Supervisor: Frans Kaashoek Title: Professor Thesis Supervisor: Nickolai Zeldovich Title: Associate Professor	cache (computing);cache coherence;central processing unit;coherence (physics);directory (computing);dynamic random-access memory;image scaling;linux;multi-core processor;multikernel;operating system;posix;process (computing);scalability;server (computing);shard (database architecture);shared memory;single system image	Charles Gruenwald	2014			computing;computer science;software engineering;computer engineering	OS	-13.085482117789095	49.55798103481996	97914
a351129da0a547ecf76abe8b8a7442da8067ef70	an efficient implementation scheme of concurrent object-oriented languages on stock multicomputers (extended abstract)		Concurrent object-oriented programming languages (concurrent OOPLs) are promising approach for building large and complex applications on multicomputers, but their dynamic/asynchronous nature makes efficient implementation difficult--support of dynamic concurrent object creation involves scheduling of multiple objects on each node, and sending of asynchronous messages between objects cause buffering of messages, both of which could result in significant overhead. One possible direction to achieve efficiency is to combine dedicated hardware[3, 1] and software tailored for it[2, 7]. The hardware has crucial support for concurrent OOPLs--low overhead remote communication and efficient multithreading. While they are attractive, the majority of current commercial multicomputers such as AP1000[4] and CM5 have no such support. This note summarizes our implementation scheme[5, 8] on these conventional multicomputers, and shows that concurrent OOPLs can greatly benefit from their superior node-local performance while communication performance can be made comparable to dedicated architectures. Basic operations such as message passing or object creation were implemented very efficiently. Intra-node message passing takes about 15 instructions in addition to the cost of procedure call, unless the receiver object is already executing some other method. Inter-node message passing latency is within 10ps for smM1 messages. An application benchmark showed good speedup (440 times speedup on 512 nodes system) for a large program.		Kenjiro Taura;Satoshi Matsuoka;Akinori Yonezawa	1992		10.1007/BFb0018667	real-time computing;computer science;theoretical computer science;distributed computing	Theory	-14.045658265035813	46.761529152190754	97962
42ba2c20256d6a1c984739d8d1c219360c3345e4	on optimal real-time subsystem-interface generation in the presence of shared resources	bounded delay resource open environment server optimal real time subsystem interface generation shared resources hierarchical scheduling framework compositional schedulability analysis embedded software systems real time properties resource sharing cpu resources hard real time tasks;processor scheduling;shared resources;resource allocation;real time;real time properties;resource management;embedded software systems;schedulability analysis;optimal real time subsystem interface generation;cpu resources;embedded systems;servers;compositional schedulability analysis;program processors real time systems resource management servers processor scheduling delay;resource sharing;resource allocation embedded systems processor scheduling;hard real time tasks;hierarchical scheduling framework;program processors;bounded delay resource open environment server;hard real time;embedded software;real time systems	The Hierarchical Scheduling Framework (HSF) has been introduced as a design-time framework enabling compositional schedulability analysis of embedded software systems with real-time properties. However, supporting resource sharing in HSF is a major challenge, since it increases the amount of CPU resources required to guarantee schedulability of the hard real time tasks, and it decreases the composability at the system level. In this paper, we focus on a compositional framework called the bounded-delay resource open environment (BROE) server, and we identify key parameters of this framework that have a great effect on how the framework will utilize CPU resources. In addition, we show how to select optimal values for these parameters in order to reduce the required CPU resource.	central processing unit;composability;embedded software;embedded system;real-time clock;real-time computing;scheduling (computing);scheduling analysis real-time systems;server (computing);software system	Moris Behnam;Thomas Nolte;Nathan Fisher	2010	2010 IEEE 15th Conference on Emerging Technologies & Factory Automation (ETFA 2010)	10.1109/ETFA.2010.5641324	shared resource;real-time computing;embedded software;resource allocation;computer science;resource management;operating system;distributed computing;server	Embedded	-8.634221704515486	60.119930933937326	98018
7200da9e5d972953095a11812efbbb159776ab05	compiler-assisted leakage energy reduction for cache memories	vliw processor;instruction cache;energy efficient;cache memory;chip;data cache;low power;energy consumption	With the scaling of technology, leakage energy reduction has become increasingly important for microprocessor design. Being the major consumer of the on-chip transistor budget, it is particularly critical to mitigate cache leakage energy. In contrast to many recent studies that attempt to minimize cache leakage by exploiting architectural-level information, this chapter introduces two compiler-assisted approaches to manage the cache leakage dissipation without significant impact on either performance or the dynamic energy consumption. More specifically, the first approach exploits static and profiling information to detect the sub-bank transitions at the compilation time, which can improve the energy efficiency of the drowsy instruction caches. The second approach exploits the fact that only a small portion of the data caches will be accessed during the loop execution, the compiler can provide hints to place other non-active cache blocks into the low power mode during the loop execution to save the data cache leakage energy. Our experiments on a state-of-the-art VLIW processor indicate that the proposed compiler-based approaches can improve the energy-efficiency of both instruction and data caches effectively.	cpu cache;compiler;spectral leakage	Wei Zhang	2007	Advances in Computers	10.1016/S0065-2458(06)69003-7	chip;bus sniffing;pipeline burst cache;computer architecture;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;efficient energy use;smart cache;mesi protocol;cache algorithms;cache pollution	PL	-6.262102477673172	55.034349371746146	98308
00661e9ba15595196f6697a348602606254306d2	locality of sampling and diversity in parallel system workloads	workload characterization;workload modeling;locality;parallel systems;adaptive system	Observing the workload on a computer system during a short (but not too short) time interval may lead to distributions that are significantly different from those that would be observed over much longer intervals. Rather than describing such phenomena using involved non-stationary models, we propose a simple global distribution coupled with a localized sampling process. We quantify the effect by the maximal deviation between the global distribution and the distribution as observed over a limited slice of time, and find that in real workload data from parallel supercomputers this deviation is significantly larger than would be observed at random. Likewise, we find that the workloads at different sites also differ from each other. These findings motivate the development of adaptive systems, which adjust their parameters as they learn about their workloads, and also the development of parametrized workload models that exhibit such locality of sampling, which are required in order to evaluate adaptive systems.	adaptive system;computer;locality of reference;maximal set;sampling (signal processing);stationary process;supercomputer	Dror G. Feitelson	2007		10.1145/1274971.1274982	parallel computing;real-time computing;computer science;adaptive system;distributed computing	HPC	-15.71717487872797	57.860533095087945	98376
701541f93d193038c63e88d0bb8a14dc95df4fa8	poster: mint: a fast and green synchronization technique	simultaneous multi threaded;phase synchronization;low power;synchronization;simultaneous multi threading;electric power;power consumption	Shadow Thread library is designed and implemented to utilize SMT to tolerate the latencies of memory and communication. It is desired to have new thread library with fast thread synchronization and low electric power consumption. In this paper, a novel thread synchronization technique, named MINT, is proposed. The key idea of the MINT is to utilize the monitor and mwait instructions which most of Intel processors provide. The combination of the monitor/mwait instructions and classic two-phase synchronization technique, in which firstly spin-wait synchronization takes place for a while and then the monitor/mwait synchronization takes place, is shown to be best from the view points of the synchronization speed and power consumption.	central processing unit;mint;synchronization (computer science);two-phase locking;while	Atsushi Hori;Yutaka Ishikawa	2011		10.1145/2148600.2148613	embedded system;synchronization;parallel computing;real-time computing;electric power;computer hardware;phase synchronization;computer science;operating system;distributed computing;data synchronization;synchronization;frame synchronization	HPC	-11.077908067560834	48.80629402359358	98403
7c7f2a7c46bb43b2798a67b9e671403b76b5183d	data-driven resource shaping for compute clusters			computer cluster;noise shaping	Francesco Pace;Dimitrios Milios;Damiano Carra;Daniele Venzano;Pietro Michiardi	2018		10.1145/3267809.3275469	real-time computing;computer science;cluster (physics);data-driven;distributed computing	HPC	-17.967779492304732	47.19881555678135	98413
829f9bfbb25c86a5c37c4a00ae278aa0f5a6fe0f	directory-based conflict detection in hardware transactional memory	energy conservation;conflict detection;hardware transactional memory;flash disk;rare event;hybrid disk array;file reallocation	One of the key design points of any hardware transactionalmemory (HTM) system is the conflict detection mechanism, and its efficientimplementation becomes critical when conflicts are not a rare event.While many contemporary proposals rely on the coherence protocol tocarry out conflict detection at the private cache levels, this approachis not optimal for systems that use a directory to maintain coherenceover an unordered, scalable network, such as tiled CMPs. In this paper,we present a new scheme of conflict detection for HTM systems, whichmoves this key mechanism from the private caches to the directory level.We propose a novel transactional book-keeping method and describe howthis detection can be carried out more efficiently at the directory. Simulationresults show that our approach obtains reductions in execution timebetween 25 and 55% for transactional benchmarks with a high numberof conflicts, with an average improvement over LogTM-SE of 15%.	acknowledgement (data networks);benchmark (computing);cache coherence;coherence (physics);directory (computing);electronic hardware;entity;extreme value theory;file synchronization;floating-point unit;html;olami–feder–christensen model;run time (program lifecycle phase);scalability;serial digital video out;software transactional memory;synergy;tiling window manager	J. Rubén Titos Gil;Manuel E. Acacio;José M. García	2008		10.1007/978-3-540-89894-8_47	transactional memory;parallel computing;real-time computing;energy conservation;computer science;operating system;distributed computing	Arch	-10.408384169154218	52.27529827122583	98442
799e96c065245220f7e9157e554df8af4c6c34fa	practical parallel external memory algorithms via simulation of parallel algorithms	multi core processor;cluster computing;parallel algorithm;large data sets;bulk synchronous parallel;model of computation;external memory;external memory algorithms;em algorithm;data structure	This thesis introduces PEMS2, an improvement to PEMS (Parallel External Memory System). PEMS executes Bulk-Synchronous Parallel (BSP) algorithms in an External Memory (EM) context, enabling computation with very large data sets which exceed the size of main memory. Many parallel algorithms have been designed and implemented for Bulk-Synchronous Parallel models of computation. Such algorithms generally assume that the entire data set is stored in main memory at once. PEMS overcomes this limitation without requiring any modification to the algorithm by using disk space as memory for additional “virtual processors”. Previous work has shown this to be a promising approach which scales well as computational resources (i.e. processors and disks) are added. However, the technique incurs significant overhead when compared with purpose-built EM algorithms. PEMS2 introduces refinements to the simulation process intended to reduce this overhead as well as the amount of disk space required to run the simulation. New functionality is also introduced, including asynchronous I/O and support for multi-core processors. Experimental results show that these changes significantly improve the runtime of the simulation. PEMS2 narrows the performance gap between simulated BSP algorithms and their hand-crafted EM counterparts, providing a practical system for using BSP algorithms with data sets which exceed the size of RAM.	asynchronous i/o;bulk synchronous parallel;central processing unit;computational resource;computer data storage;disk space;input/output;model of computation;multi-core processor;overhead (computing);parallel algorithm;random-access memory;simulation	David Edward Robillard	2009	CoRR		model of computation;multi-core processor;interleaved memory;parallel computing;out-of-core algorithm;distributed memory;data structure;expectation–maximization algorithm;computer cluster;computer science;theoretical computer science;operating system;analysis of parallel algorithms;distributed computing;parallel algorithm;algorithm;bulk synchronous parallel	DB	-12.772012437908097	50.62542993331878	98479
09243cbc0af63255bcd3611c8521cf2359dcd1bd	thermal fingerprinting - multi-dimensional analysis of computational loads		Digital fingerprinting is used in several domains to identify and track variable activities and processes. In this paper, we propose a novel approach to categorize and recognize computational tasks based on thermal system information. The concept focuses on all kinds of data center environments to control required cooling capacity dynamically. The concept monitors basic thermal sensor data from each server and chassis entity. The respective, characteristic curves are merged with additional general system information, such as CPU load behavior, memory usage, and I/O characteristics. This results in two-dimensional thermal fingerprints, which are unique and achievable. The fingerprints are used as input for an adaptive, pre-active air-conditioning control system. This allows a precise estimation of the data center health status. First test cases and reference scenarios clarify a huge potential for energy savings without any negative aspects regarding health status or durability. In consequence, we provide a cost-efficient, light-weight, and flexible solution to optimize the energy-efficiency for a huge number of existing, conventional data center environments.	baseline (configuration management);categorization;central processing unit;chassis;computer cooling;control system;cost efficiency;data center;digital video fingerprinting;durability (database systems);fingerprint (computing);fingerprint recognition;input/output;money;server (computing);statistical classification;system information (windows);tcp/ip stack fingerprinting;test case	Matthias Vodel;Marc Ritter	2017				HPC	-17.90807336763191	56.420960054020426	98524
6f82ac1d3c6d5670005076c0477e558471aa76c0	sensitivity of performance prediction of message passing programs	message passing system;mpi performance prediction;multi user;dimemas;message passing;performance prediction;parallel architecture;trace driven simulation;communication pattern	This paper discusses the issues related to the accuracy of performance prediction tools for message passing programs. We present the results of two sets of experiments to quantify the effect of the instrumentation overhead and variance in the accuracy of Dimemas. The results show that this performance prediction tool can be used with a high level of confidence as the effect of instrumentation overhead on the predicted performance is minimal. We also show that it is possible to carry out instrumentation runs in highly loaded multi-user environments and still be able to accurately analyze the performance of the application as if it had run alone.	experiment;high-level programming language;message passing;multi-user;overhead (computing);performance prediction	Sergi Girona;Jesús Labarta	1999	The Journal of Supercomputing	10.1023/A:1026567408307	parallel computing;message passing;real-time computing;computer science;distributed computing;programming language	Metrics	-16.61992675842414	47.68670160407036	98580
d86e9f6a6d4542478d8950acaed7f5404f6b1895	preemptibility-aware responsive multi-core scheduling	utility;real time;real time processing;target sensitivity;adaptivity;operating system;scheduling;overload;process scheduling	We propose a novel responsive scheduling technique to minimize the scheduling latency of a real-time process in the multi-core architecture, called the preemptibility-aware scheduling (PAS). Modern complex operating systems contain numerous long interrupt-disabled and non-preemptible sections, and consequently these sections obstruct the immediate handling of urgent interrupts and the rapid scheduling of interrupt-driven real-time tasks, causing significant latency between interrupt arrival and process scheduling. The proposed PAS guarantees that before an urgent interrupt occurs, at least one among multiple CPU cores is always in both interrupt-enabled and preemptible sections, so that the incoming urgent interrupt can always be handled with no significant delay by such CPU core. Experimental results show that the worst-case latency can be reduced by 27-96%.	best, worst and average case;central processing unit;intel core (microarchitecture);interrupt;multi-core processor;operating system;preemption (computing);real-time clock;scheduling (computing)	Jupyung Lee;Geunsik Lim;Sang-Bum Suh	2011		10.1145/1982185.1982348	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;real-time computing;earliest deadline first scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;stride scheduling;least slack time scheduling;round-robin scheduling;interrupt latency;scheduling	Embedded	-9.844808608864582	59.62166190646183	98656
b553e24a1e581fb46b944104453e5bb558a2c390	an analysis of cache performance for a hypercube multicomputer	user code;direct mapped cache performance;code analysis;performance evaluation;perforation;storage management;address traces;buffer storage;parallel programming;data access patterns;index termshypercube multicomputer;indexing terms;data partitioning;message passing code;application specific datapartitioning;cache performance;storage management buffer storage hypercube networks parallel programming performance evaluation;data access;processornodes;message passing;communication frequency;system accesses;trace driven simulation;parallel applications;cache simulation;communication distribution;parallel application;hypercube networks;intel ipsc 2;performance analysis hypercubes computational modeling analytical models cache memory frequency parallel processing computer networks distributed computing multiprocessor interconnection networks;message passing code hypercube multicomputer cache simulation address traces intel ipsc 2 processor nodes parallel application direct mapped cache performance application specific data partitioning data access patterns communication distribution communication frequency system accesses user code code analysis time distribution;time distribution	Multicomputer cache simulation results derived from address traces collected from an Intel iPSC/2 hypercube multicomponent are presented. The primary emphasis is on examining how increasing the number of processor nodes executing a parallel application affects the overall multicomputer cache performance. The effects on multicomputer direct-mapped cache performance of application-specific data partitioning, data access patterns, communication distribution, and communication frequency are illustrated. The effects of system accesses on total cache performance are explored, as well as the reasons for application-specific differences in cache behavior for system and user accesses. Comparing user code results with full user and system code analysis reveals the significant effect of system accesses, and this effect increases with multicomputer size. The time distribution of an application's message-passing operations is found to more strongly affect cache performance than the total amount of time spent in message-passing code. >	parallel computing	Craig B. Stunkel;W. Kent Fuchs	1992	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.149961	data access;pipeline burst cache;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;message passing;cache coloring;page cache;index term;cache;computer science;write-once;cache invalidation;operating system;distributed computing;smart cache;programming language;cache algorithms;cache pollution;static program analysis	Arch	-9.608269669134625	48.04666027207695	98661
4f32fb4628df8b4ca281c4a791834a829ad86c86	towards rtos support for mixed time-triggered and event-triggered task sets	scheduling embedded systems operating systems computers production engineering computing;production engineering computing;embedded systems;engineering and technology;teknik och teknologier;scheduling;aperiodic requests rtos support mixed time triggered task set event triggered task set embedded systems timing constraints flexibility requirements offline planning industrial systems real time operating system table driven dispatcher preemptive scheduler event driven tasks dedicated time slots processing capacity slotshifting commercial rtos rtos implementation run time overheads;operating systems computers	Many embedded systems have complex timing constraints and, at the same time, have flexibility requirements which prohibit offline planning of the entire system. To support a mixture of time-triggered and event-triggered tasks, some industrial systems deploy a real-time operating system (RTOS) with a table-driven dispatcher complemented with a preemptive scheduler to allocate free time slots to event-driven tasks. Rather than allocating dedicated time-slots to time-triggered tasks, we propose to dynamically re-allocate time-slots of time-triggered tasks within a pre-computed time range to maximize the available processing capacity for event-triggered tasks. Although the concept — called slotshifting — is not new, we are unaware of a commercial RTOS with such support. After identifying the mechanisms for an RTOS implementation of slotshifting, we discuss the run-time overheads for admitting aperiodic requests into the system 1 .	fuzzy set;real-time operating system	Martijn M. H. P. van den Heuvel;Reinder J. Bril;Johan J. Lukkien;Damir Isovic;Gowri Sankar Ramachandran	2012		10.1109/ETFA.2012.6489733	embedded system;embedded operating system;parallel computing;real-time computing;computer science;operating system;scheduling	NLP	-9.477063332903647	59.65563359343267	98804
1d6264837f91fd093a2469108f3e503582528688	thermal constrained resource management for mixed ilp-tlp workloads in dark silicon chips	lightweight;thermal aware task application mapping thermal constrained resource management mixed ilp tlp workloads dark silicon chips on chip resources power gated operation resource management technique dsrem management technique active core selection instruction level parallelism thread level parallelism active core heat generation;speck;block cipher;throughput power demand resource management silicon heating mathematical model thermal management;simon;internet of things;thermal management packaging integrated circuit design logic design microprocessor chips processor scheduling resource allocation	In dark silicon chips, a significant amount of on-chip resources cannot be simultaneously powered on and need to stay dark, i.e., power gated, in order to avoid thermal emergencies. This paper presents a resource management technique, called DsRem, that selects the number of active cores jointly with their voltage/frequency (v/f) levels, considering the high Instruction Level Parallelism (ILP) or Thread Level Parallelism (TLP) nature of different applications, in order to maximize the overall system performance. DsRem leverages the positioning of dark cores, to efficiently dissipate the heat generated by the active cores. This facilitates increasing the v/f level of the active cores, which leads to further performance improvement. Compared to state-of-the-art thermal-aware task application mapping, DsRem achieves up to 46% performance gain, while avoiding any thermal emergencies. Additionally, DsRem outperforms the boosting technique with 26%.	active directory;dark silicon;instruction-level parallelism;integrated circuit;parallel computing;task parallelism	Heba Khdr;Santiago Pagani;Muhammad Shafique;Jörg Henkel	2015	2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2744769.2744916	embedded system;block cipher;electronic engineering;parallel computing;real-time computing;computer science;engineering;operating system;internet of things	EDA	-5.342033826254993	55.64814866011289	98843
4ce2925f76b279b2b31bf303c4d8f150ab5679ec	disk built-in caches: evaluation on system performance	cache storage;disc drives;peripheral interfaces;system performance disk drives file systems manufacturing costs hard disks operating systems read write memory delay writing;disc storage;512 kb disk built in cache system performance evaluation disk drive ide ata drive small computer system interface scsi drive file system response time readahead buffer writing cache 16 mb 2 mb;system performance;performance improvement;file system;power consumption;peripheral interfaces cache storage disc storage disc drives;system simulation	Disk drive manufacturers are putting increasingly larger built-in caches into disk drives. Today, 2 MB buffers are common on low-end retail IDE/ATA drives, and some SCSI drives are now available with 16 MB. However, few published data are available to demonstrate that such large built-in caches can noticeably improve overall system performance. In this paper, we investigated the impact of the disk built-in cache on file system response time when the file system buffer cache becomes larger. Via detailed file system and disk system simulation, we arrive at three main conclusions: (1) With a reasonably-sized file system buffer cache (16 MB or more), there is very little performance benefit of using a built-in cache larger than 512 KB. (2) As a readahead buffer, the disk built-in cache provides noticeable performance improvements for workloads with read sequentiality, but has little positive effect on performance if there are more concurrent sequential workloads than cache segments. (3) As a writing cache, it also has some positive effects on some workloads, at the cost of reducing reliability. The disk drive industry is very cost-sensitive. Our research indicates that the current trend of using large built-in caches is unnecessary and a waste of money and power for most users. Disk manufacturers could use much smaller built-in caches to reduce the cost as well as powerconsumption, without affecting performance.	built-in self-test;cpu cache;cache (computing);canonical account;disk storage;family computer disk system;page cache;readahead;response time (technology);scsi;serial ata;simulation	Yingwu Zhu;Yiming Hu	2003		10.1109/MASCOT.2003.1240675	bus sniffing;disk enclosure;hard disk drive performance characteristics;parallel computing;real-time computing;cache coloring;page cache;disk sector;disk controller;computer hardware;disk formatting;cache;computer science;disk array controller;data recovery;cache invalidation;operating system;disk buffer;computer performance;logical disk;cache algorithms;cache pollution	OS	-11.316243786254114	53.569594967320896	98927
059f856f03d684998597f83e4b2e6aa047155fa9	nm2h: design and implementation of nosql extension for hdfs metadata management	reliability;metadata;availability;computer architecture;servers;scalability;hadoop;nosql;memory;file systems;hdfs;java	As a distributed MapReduce framework, Hadoop has been widely adopted in big data processing, in which HDFS (Hadoop Distributed File System) is mostly used for data storage. Though the single master architecture of HDFS simplifies the design and implementation, it suffers from issues such as SPOF (Single Point Of Failure) and scalability, which further may become performance bottleneck. To address these problems, this paper proposes NM2H, a NoSQL based metadata management approach for HDFS. NM2H separates the storage and query of metadata in contrast to the traditional architecture which mixed them up, and manages to keep the interfaces among the metadata service, Data Nodes and clients unchanged through a novel mapping mechanism between the original metadata structures to NoSQL documents. Therefore, the new approach can not only take advantages of NoSQL's better scalability and fault tolerance, but also deliver transparency to client applications, in which way existing programs can run on the new architecture without any modification. The prototype of NM2H was designed and implemented with widely adopted NoSQL system MongoDB. Extensive performance evaluation was conducted and the experimental results indicated the improvement of NM2H, while the overhead introduced was acceptable.	apache hadoop;big data;computer data storage;dce distributed file system;fault tolerance;mapreduce;mongodb;nosql;overhead (computing);performance evaluation;prototype;scalability;single point of failure	Ruini Xue;Zhongyang Guan;Shengli Gao;Lixiang Ao	2014	2014 IEEE 17th International Conference on Computational Science and Engineering	10.1109/CSE.2014.246	availability;scalability;computer science;operating system;reliability;database;memory;java;metadata;world wide web;server	DB	-14.770225584655986	53.61558946621662	98943
adfa50c6dd6283050ad770a07263960f37894a6c	a dynamic scheduling algorithm for distributed kahn process networks in a cluster environment	resource utilization;static job scheduling;dynamic load balancing;multiprocessing programs;load balancing distributed kahn process networks cluster environment task scheduling parallel applications distributed multiprocessor cluster static job scheduling;processor scheduling;resource allocation;task analysis multiprocessing programs parallel processing resource allocation scheduling;dynamic scheduling clustering algorithms heuristic algorithms scheduling algorithm system recovery processor scheduling channel capacity application software signal processing algorithms computer networks;satisfiability;distributed multiprocessor cluster;computational modeling;data dependence;scheduling;heuristic algorithms;task analysis;load balancing;kahn process network;task scheduling;load modeling;process migration;job scheduling;program processors;parallel applications;parallel processing;cluster environment;dynamic scheduling;data models;distributed kahn process networks	In this paper, a novel dynamic task scheduling algorithm is proposed for parallel applications modeled in Kahn process networks (KPN) running in a distributed multi-processor cluster. Static job scheduling algorithms do not work for the purpose for that the complexity of a KPN model remains unpredictable at compile time. Dynamic load balancing strategies ignore the explicit data dependences among tasks and may lead to inappropriate process migrations. The algorithm presented in this paper is based on the sequence of dynamic recorded events of each task at runtime. It then predicts the execution efficiency of a KPN model in various scheduling (task-processor assignments) through the estimation of average resource utilization rate. Simulations have shown satisfying results.	algorithm;compile time;compiler;computation;computer simulation;context of computational complexity;experiment;input/output;job scheduler;kahn process networks;load balancing (computing);multiprocessing;process migration;run time (program lifecycle phase);scheduling (computing)	Zhengping Qian;Ming Zeng;Deyu Qi;Kefu Xu	2008	2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application	10.1109/PACIIA.2008.190	fair-share scheduling;data modeling;parallel processing;in situ resource utilization;parallel computing;real-time computing;process migration;dynamic priority scheduling;resource allocation;computer science;load balancing;job scheduler;operating system;task analysis;distributed computing;computational model;scheduling;satisfiability	HPC	-13.659179179684617	60.26229672143934	99004
29fa621a41f879e5e02f7cbcc437e20098e0a988	evector: an efficient vector implementation - using virtual memory for improving memory	efficient vector implementation;resizable array;object-oriented environment;operating system;virtual memory;memory handling;programming language;hardware memory management unit;modern operating system;reference implementation;standard c;vector;c	Every modern operating system provides some form of virtual memory to its applications. Usually, a hardware memory management unit (MMU) exists to efficiently support this. Although most operating systems allow user programs to indirectly control the MMU, few programs or programming languages actually make use of this facility. This article explores how the MMU can be used to enhance memory handling for resizable arrays. A reference implementation in C++ demonstrates its usability and superiority compared to the standard C++ vector class, and how to combine the scheme with an object-oriented environment. A number of other improvements, based on newly emerged insights in C++ are also presented.		Dries Kimpe;Stefan Vandewalle;Stefaan Poedts	2006	Scientific Programming		memory model;parallel computing;real-time computing;computer hardware;vector;computer science;virtual memory;operating system;overlay;conventional memory;extended memory;c dynamic memory allocation;programming language;memory map;memory management	HPC	-12.704667292592713	49.36469119498022	99093
ad265f389a2d67cf64442c1953c0dba53431a1b3	coding for distributed fog computing	bandwidth encoding edge computing redundancy time factors encoding computer architecture;sorting;distributed fog computing redundancy fog networks unified coding framework system performance fog architecture;computer architecture;time factors;redundancy;bandwidth;encoding;edge computing;distributed processing redundancy software architecture	Redundancy is abundant in fog networks (i.e., many computing and storage points) and grows linearly with network size. We demonstrate the transformational role of coding in fog computing for leveraging such redundancy to substantially reduce the bandwidth consumption and latency of computing. In particular, we discuss two recently proposed coding concepts, minimum bandwidth codes and minimum latency codes, and illustrate their impacts on fog computing. We also review a unified coding framework that includes the above two coding techniques as special cases, and enables a trade-off between computation latency and communication load to optimize system performance. At the end, we will discuss several open problems and future research directions.	code;computation;data compression;fog computing	Songze Li;Mohammad Ali Maddah-Ali;Amir Salman Avestimehr	2017	IEEE Communications Magazine	10.1109/MCOM.2017.1600894	real-time computing;computer science;sorting;theoretical computer science;distributed computing;redundancy;bandwidth;encoding	HPC	-15.323613881912394	55.838075779054165	99124
fcae2fcef595059529ebe553431ab41b44062ae4	efficient deterministic multithreading without global barriers	lazy release consistency;deterministic execution;multithreading	Multithreaded programs execute nondeterministically on conventional architectures and operating systems. This complicates many tasks, including debugging and testing. Deterministic multithreading (DMT) makes the output of a multithreaded program depend on its inputs only, which can totally solve the above problem. However, current DMT implementations suffer from a common inefficiency: they use frequent global barriers to enforce a deterministic ordering on memory accesses. In this paper, we eliminate that inefficiency using an execution model we call deterministic lazy release consistency (DLRC). Our execution model uses the Kendo algorithm to enforce a deterministic ordering on synchronization, and it uses a deterministic version of the lazy release consistency memory model to propagate memory updates across threads. Our approach guarantees that programs execute deterministically even when they contain data races. We implemented a DMT system based on these ideas (RFDet) and evaluated it using 16 parallel applications. Our implementation targets C/C++ programs that use POSIX threads. Results show that RFDet gains nearly 2x speedup compared with DThreads-a start-of-the-art DMT system.	c++;debugging;deterministic algorithm;digital monetary trust;lazy evaluation;multithreading (computer architecture);nondeterministic algorithm;operating system;posix threads;release consistency;speedup;thread (computing)	Kai Lu;Xu Zhou;Tom Bergan;Xiaoping Wang	2014		10.1145/2555243.2555252	parallel computing;real-time computing;multithreading;computer science;distributed computing;programming language	OS	-13.427898913094934	47.9735566609617	99393
a47025b3f311c708bde61aff99772fd74b3d3a20	energy-efficient task scheduling for multi-core platforms with per-core dvfs	energy efficient;task characteristics;task scheduling;dvfs;multi core	Energy-efficient task scheduling is a fundamental issue in many application domains, such as energy conservation for mobile devices and the operation of green computing data centers. Modern processors support dynamic voltage and frequency scaling (DVFS) on a per-core basis, i.e., the CPU can adjust the voltage or frequency of each core. As a result, the core in a processor may have different computing power and energy consumption. To conserve energy in multi-core platforms, we propose task scheduling algorithms that leverage percore DVFS and achieve a balance between performance and energy consumption. We consider two task execution modes: the batch mode, which runs jobs in batches; and the online mode in which jobs with different time constraints, arrival times, and computation workloads co-exist in the system. For tasks executed in the batch mode, we propose an algorithm that finds the optimal scheduling policy; and for the online mode, we present a heuristic algorithm that determines the execution order and processing speed of tasks in an online fashion. The heuristic ensures that the total cost is minimal for every time interval during a task’s execution. Furthermore, we analyze and derive algorithms with low time complexity for each mode. Email addresses: deathsimon@iis.sinica.edu.tw (Ching-Chi Lin), arosusti@gmail.com (You-Cheng Syu), crchang@iis.sinica.edu.tw (Chao-Jui Chang), wuj@iis.sinica.edu.tw (Jan-Jan Wu), pangfeng@csie.ntu.edu.tw (Pangfeng Liu), sting@itri.org.tw (Po-Wen Cheng), victor.hsu@itri.org.tw (Wei-Te Hsu) Preprint submitted to Elsevier July 24, 2015	algorithm;batch processing;chi;central processing unit;chao (sonic);clock rate;computation;data center;dynamic frequency scaling;dynamic voltage scaling;email;frequency scaling;greedy algorithm;heuristic (computer science);image scaling;interactivity;job (computing);job stream;load balancing (computing);marginal model;mobile device;multi-core processor;multiprocessing;polynomial;run time (program lifecycle phase);scheduling (computing);time complexity;trace-based simulation	Ching-Chi Lin;You-Cheng Syu;Chao-Jui Chang;Jan-Jan Wu;Pangfeng Liu;Po-Wen Cheng;Wei-Te Hsu	2015	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2015.08.004	multi-core processor;fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;distributed computing;efficient energy use	Embedded	-5.76035637607537	59.10438160214681	99406
024b6a92800d47ab55e63dce36ef2fe2a0791c65	stretching the limits of clock-gating efficiency in server-class processors	flip flops;clock gating;leakage power;elastic pipeline clock gating server class processors dynamic power management high end commercial microprocessor temperature drop active power reduction leakage power savings high performance processors transparent pipeline clock gating;power reduction;dynamic power management;high performance;computer power supplies pipeline processing microprocessor chips flip flops;computer power supplies;pipeline processing;microprocessor chips;clocks pipelines microprocessors cmos technology energy management temperature power generation jacobian matrices technology management power system management	Clock-gating has been introduced as the primary means of dynamic power management in recent high-end commercial microprocessors. The temperature drop resulting from active power reduction can result in additional leakage power savings in future processors. In this paper we first examine the realistic benefits and limits of clock-gating in current generation high-performance processors (e.g. of the POWER4/spl trade/ or POWER5/spl trade/ class). We then look beyond classical clock-gating: we examine additional opportunities to avoid unnecessary clocking in real workload executions. In particular, we examine the power reduction benefits of a couple of newly invented schemes called transparent pipeline clock-gating and elastic pipeline clock-gating. Based on our experiences with current designs, we try to bound the practical limits of clock gating efficiency in future microprocessors.	central processing unit;clock gating;clock rate;microprocessor;power management;programming paradigm;server (computing);spectral leakage	Hans M. Jacobson;Pradip Bose;Zhigang Hu;Alper Buyuktosunoglu;Victor V. Zyuban;Richard J. Eickemeyer;Lee Eisen;John Griswell;Doug Logan;Balaram Sinharoy;Joel M. Tendler	2005	11th International Symposium on High-Performance Computer Architecture	10.1109/HPCA.2005.33	embedded system;parallel computing;real-time computing;computer science;operating system;clock gating;low-power electronics	Arch	-7.222851031047818	53.70925489457912	99459
3ae2d3f84478f3a7fd5b0e0b784ca88d8262f1d3	scalable performance tuning of hadoop mapreduce: a noisy gradient approach		Hadoop MapReduce is a popular framework for distributed storage and processing of large datasets and is used for big data analytics. It has various configuration parameters which play an important role in deciding the performance i.e., the execution time of a given big data processing job. Default values of these parameters do not result in good performance and therefore it is important to tune them. However, there is inherent difficulty in tuning the parameters due to two important reasons - first, the parameter search space is large and second, there are cross-parameter interactions. Hence, there is a need for a dimensionality-free method which can automatically tune the configuration parameters by taking into account the cross-parameter dependencies. In this paper, we propose a novel Hadoop parameter tuning methodology, based on a noisy gradient algorithm known as the simultaneous perturbation stochastic approximation (SPSA). The SPSA algorithm tunes the selected parameters by directly observing the performance of the Hadoop MapReduce system. The approach followed is independent of parameter dimensions and requires only 2 observations per iteration while tuning. We demonstrate the effectiveness of our methodology in achieving good performance on popular Hadoop benchmarks namely Grep, Bigram, Inverted Index, Word Co-occurrence and Terasort. Our method, when tested on a 25 node Hadoop cluster shows 45-66% decrease in execution time of Hadoop jobs on an average, when compared to prior methods. Further, our experiments also indicate that the parameters tuned by our method are resilient to changes in number of cluster nodes, which makes our method suitable to optimize Hadoop when it is provided as a service on the cloud.	algorithm;apache hadoop;big data;bigram;clustered file system;experiment;gradient;interaction;inverted index;iteration;job stream;mapreduce;performance tuning;run time (program lifecycle phase);simultaneous perturbation stochastic approximation	Sandeep Kumar;Sindhu Padakandla;Chandrashekar Lakshminarayanan;Priyank Parihar;K. Gopinath;Shalabh Bhatnagar	2017	2017 IEEE 10th International Conference on Cloud Computing (CLOUD)	10.1109/CLOUD.2017.55	computer science;performance tuning;scalability;inverted index;real-time computing;approximation algorithm;big data;simultaneous perturbation stochastic approximation;distributed data store;bigram	HPC	-17.711978497536105	57.79905749067869	99547
8b41617fe732610d5f9caf38e42e77d57f531eb6	autobest: a united autosar-os and arinc 653 kernel	automotive engineering;kernel;standards;kernel standards schedules aerospace electronics automotive engineering synchronization;automotive systems autobest rtos kernel realtime operating system autosar os kernel arinc 653 kernel automotive domain avionics domain domain specific requirements small partitioning microkernel based design embedded microcontrollers memory protection support mpu support unified task model user space libraries futexes abstraction lazy priority switching abstraction domain specific synchronization mechanism avionics safety;synchronization;aerospace electronics;schedules;road safety aerospace engineering automobiles embedded systems operating system kernels	This paper presents AUTOBEST, a united AUTOSAR-OS and ARINC 653 RTOS kernel that addresses the requirements of both automotive and avionics domains. We show that their domain-specific requirements have a common basis and can be implemented with a small partitioning microkernel-based design on embedded microcontrollers with memory protection (MPU) support. While both, AUTOSAR and ARINC 653, use a unified task model in the kernel, we address their differences in dedicated user space libraries. Based on the kernel abstractions of futexes and lazy priority switching, these libraries provide domain specific synchronization mechanisms. Our results show that thereby it is possible to get the best of both worlds: AUTOBEST combines avionics safety with the resource-efficiency known from automotive systems.	autosar;avionics;domain-specific language;embedded system;futex;kernel (operating system);lazy evaluation;library (computing);mpu-401;memory protection;microcontroller;microkernel;real-time operating system;requirement;user space	Alexander Zuepke;Marc Bommert;Daniel Lohmann	2015	21st IEEE Real-Time and Embedded Technology and Applications Symposium	10.1109/RTAS.2015.7108435	embedded system;synchronization;kernel;real-time computing;schedule;computer science;operating system	Embedded	-8.317738178892165	57.92797256124177	99606
aa6c0c45aa476d5f031620e7228bc752bda4124c	performance evaluation of multiprocessor system modelled as t-out-of-s system	performance evaluation;multiprocessor systems;multiprocessor performance;interconnection network;memory interference;bandwidth	Simple generalized closed-form analytic solutions, using t-out-of-s system principle, are presented for tightly coupled multiprocessor systems employing multiplebus and crossbar interconnection networks. The system bandwidth is obtained taking into account the memory and bus interference. A single equation represents both uniform and local reference models of memory access. This expression holds for any demand pattern of memory access by the processors unlike the solutions given by others, which consider only some restricted memory access patterns. The results predicted by these models have been shown to be very close to the results of simulation studies done by others.	multiprocessing;performance evaluation	E. V. Prasad;Anil K. Sarje	1993	International Journal of High Speed Computing	10.1142/S0129053393000050	uniform memory access;interference theory;shared memory;parallel computing;real-time computing;distributed memory;computer science;distributed computing;multiprocessor scheduling;bandwidth;symmetric multiprocessor system	Arch	-10.05782186989072	49.31818341291101	99782
5e1d3f34aac99b8378b59615562a7bc694317dd3	performance analysis of parallel simulation on distributed systems	distributed system;distributed computing;multistage interconnection network;parallel and distributed processing;mathematical analysis;performance metric;difference scheme;performance analysis;performance model;parallel simulation;analytical model	This paper presents an analytical model to evaluate the performance of parallel simulation on distributed computing platforms. The proposed model is formalized by two important time components in parallel and distributed processing: computation time and communication time. A conservative parallel simulation of multistage interconnection networks is used as an example in our analytical model. Performance metrics such as elapsed time, speedup and simulation bandwidth associated with different schemes for partitioning/mapping parallel simulation onto distributed processors are evaluated. Our mathematical analysis identifies the major constituents of simulation overheads in these mapping strategies necessary for improving parallel simulation efficiency. We also show that a perfectly balanced workload distribution may not necessarily translate into better performance. On the contrary, we have shown that a balanced mapping of workload may increase communication overheads resulting in a longer simulation elapsed time. Our performance model has been validated against implementation results from a parallel simulation model. The analytical framework is also practical to evaluate the runtime efficiency of other simulation applications which are based on the conservative paradigm.	balanced ternary;central processing unit;computation;distributed computing;multistage interconnection networks;profiling (computer programming);programming paradigm;series and parallel circuits;simulation;speedup;time complexity	Yong Meng Teo;Seng Chuan Tay	1996	Distributed Systems Engineering	10.1088/0967-1846/3/1/004	parallel computing;real-time computing;computer science;operating system;distributed computing	HPC	-9.763512943872218	48.76460227516959	99811
9b8282506710dc8ca14b076d3157ff97eec80fd6	a comparison of memory allocators for multicore and multithread applications: a quantitative approach	storage management middleware multiprocessing systems multi threading software performance evaluation;multi threading;memory fragmentation;comparative analysis;memory allocator algorithms comparison memory allocation memory fragmentation;storage management;resource manager;resource management;software performance evaluation;ptmalloc version 3 memory allocation operations multicore applications multithread applications software design tcmalloc memory allocator artificial benchmark tests real world middleware applications response time memory consumption memory fragmentation processor cores;mercury metals middleware instruction sets multicore processing linux resource management analysis of variance;memory allocator algorithms comparison;multicore processing;mercury metals;analysis of variance;middleware;linux;multiprocessing systems;memory allocation;software design;instruction sets	The performance of memory allocation operations is a very important aspect to be considered in software design, however it is frequently neglected. This paper presents a comparative analysis of seven largely adopted memory allocators. Unlike other related works, based on artificial benchmark tests, we evaluate the selected allocators using real-world middleware applications. In order to compare the performance of the investigated allocators we consider the response time, memory consumption, and memory fragmentation. All tests are evaluated with respect to different combinations of processor cores. The results indicate that for workloads based on memory allocations up to 64 bytes and all combinations of processor cores up to four, the best average response time and memory usage is obtained using the TCMalloc memory allocator, followed by the Ptmalloc version 3.	allocator (c++);benchmark (computing);byte;fragmentation (computing);memory management;middleware;multi-core processor;qualitative comparative analysis;response time (technology);software design;thread (computing)	Taís Borges Ferreira;Rivalino Matias;Autran Macedo;Lucio Borges de Araujo	2011	2011 Brazilian Symposium on Computing System Engineering	10.1109/SBESC.2011.29	uniform memory access;shared memory;parallel computing;real-time computing;distributed memory;computer science;operating system;static memory allocation;computing with memory;non-uniform memory access	Embedded	-8.803267711782624	49.60636859073233	99899
e66be218858b0ea019448f607fcbefca1962a6b5	improving gpu cache hierarchy performance with a fetch and replacement cache		In the last few years, GPGPU computing has become one of the most popular computing paradigms in high-performance computers due to its excellent performance to power ratio. The memory requirements of GPGPU applications widely differ from the requirements of CPU counterparts. The amount of memory accesses is several orders of magnitude higher in GPU applications than in CPU applications, and they present disparate access patterns. Because of this fact, large and highly associative Last-Level Caches (LLCs) bring much lower performance gains in GPUs than in CPUs.	cpu cache;graphics processing unit	Francisco Candel;Salvador Petit;Alejandro Valero;Julio Sahuquillo	2018		10.1007/978-3-319-96983-1_17	parallel computing;cache;hierarchy;associative property;fetch;general-purpose computing on graphics processing units;distributed computing;computer science	Arch	-10.474999762175225	52.387873042251705	99934
b9827786654401db03d73cdda99e986b7ef26fe1	improving ibm power8 performance through symbiotic job scheduling		Symbiotic job scheduling, i.e., scheduling applications that co-run well together on a core, can have a considerable impact on the performance of processors with simultaneous multithreading (SMT) cores. SMT cores share most of their microarchitectural components among the co-running applications, which causes performance interference between them. Therefore, scheduling applications with complementary resource requirements on the same core can greatly improve the throughput of the system. This paper enhances symbiotic job scheduling for the IBM POWER8 processor. We leverage the existing cycle accounting mechanism to build an interference model that predicts symbiosis between applications. The proposed models achieve higher accuracy than previous models by predicting job symbiosis from throttled CPI stacks, i.e., CPI stacks of the applications when running in the same SMT mode to consider the statically partitioned resources, but without interference from other applications. The symbiotic scheduler uses these interference models to decide, at run-time, which applications should run on the same core or on separate cores. We prototype the symbiotic scheduler as a user-level scheduler in the Linux operating system and evaluate it on an IBM POWER8 server running multiprogram workloads. The symbiotic job scheduler significantly improves performance compared to both an agnostic random scheduler and the default Linux scheduler. Across all evaluated workloads in SMT4 mode, throughput improves by 12.4 and 5.1 percent on average over the random and Linux schedulers, respectively.	central processing unit;computer multitasking;heterogeneous system architecture;interference (communication);job scheduler;linux;microarchitecture;multi-core processor;multithreading (computer architecture);operating system;overhead (computing);prototype;requirement;run time (program lifecycle phase);sampling (signal processing);scheduling (computing);server (computing);simultaneous multithreading;thread (computing);threaded code;throughput;user space	Josué Feliu;Stijn Eyerman;Julio Sahuquillo;Salvador Petit;Lieven Eeckhout	2017	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2017.2691708	parallel computing;stack (abstract data type);real-time computing;computer science;power8;simultaneous multithreading;throughput;job scheduler;scheduling (computing);multi-core processor;distributed computing;fixed-priority pre-emptive scheduling	OS	-9.23050889676569	51.07848374699707	100001
3fe7031f0c065d8946d7123ee461a06be826edbe	sel-tm: selective eager-lazy management for improved concurrency in transactional memory	selective eager lazy htm;cache storage;lazily managed memory addresses;memory block;intelligent hardware scheme;conflict detection;concurrent computing;memory management;hybrid management;sel tm;transaction processing cache storage concurrency control configuration management;htm system;hardware transactional memory;buffer storage;lazy version management;runtime;conflict point discovery;lazy version management sel tm selective eager lazy management concurrency hardware transactional memory htm system conflict detection eager system lazy system complex cache protocol performance degradation memory block transaction write set selective eager lazy htm eagerly managed memory addresses lazily managed memory addresses intelligent hardware scheme hybrid management stamp benchmark;memory management concurrent computing hardware instruction sets runtime pathology buffer storage;concurrency;eagerly managed memory addresses;complex cache protocol;concurrency control;version management;selective eager lazy management;stamp benchmark;eager system;transaction write set;lazy system;transaction processing;performance degradation;configuration management;pathology;sel tm hardware transactional memory version management conflict point discovery;instruction sets;hardware	Hardware Transactional Memory (HTM) systems implement version management and conflict detection in hardware to guarantee that each transaction is atomic and executes in isolation. In general, HTM implementations fall into two categories, namely, eager systems and lazy systems. Lazy systems have been shown to exploit more concurrency from potentially conflicting transactions. However, lazy systems manage a transaction's entire write set lazily, which gives rise to two main disadvantages: (a) a complex cache protocol and implementation are required to maintain the speculative modifications, and, (b) the latency of committing the entire write set often leads to severe performance degradation of the whole system. It is observed in a wide range of workloads that more than 55% of the transaction aborts are due to conflicts on only three memory blocks. Thus we argue that an eager HTM system can achieve the same level of concurrency as lazy systems by managing only a small portion of a transaction's write set lazily. In this paper, we present Selective-Eager-Lazy HTM (SEL-TM), a new HTM implementation to adopt complementary version management schemes within a transaction whose write set is divided into eagerly- and lazily-managed memory addresses at runtime. An intelligent hardware scheme is designed to select the memory addresses for lazy management as well as determining whether each dynamic instance of a transaction benefits from hybrid management. Experimental results using the STAMP benchmarks show that, on average, SEL-TM improves performance by 14% over an eager system and 22% over a lazy system. The speedup demonstrates that our design is capable of harvesting the concurrency benefit of lazy version management while avoiding some of the performance penalties in lazy HTMs.	best, worst and average case;cpu cache;cache (computing);concurrency (computer science);elegant degradation;emoticon;file synchronization;html;lazy evaluation;run time (program lifecycle phase);speculative execution;speedup;systems engineering laboratories;transactional memory;version control;xml:tm	Lihang Zhao;Woojin Choi;Jeffrey T. Draper	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2012.19	demand paging;optimistic concurrency control;transactional memory;parallel computing;real-time computing;concurrent computing;concurrency;transaction processing;computer science;operating system;concurrency control;instruction set;database;distributed computing;configuration management;programming language;memory management	Arch	-14.521467390890814	49.03319481828436	100104
18a2c29e1e981c210856789b7361dee6242e385a	eml: an i/o scheduling algorithm in large-scale-application environments	eml;i o scheduling algorithm;multi level queue	Distributed file systems have been widely used in many applications to provide high performance. However, large amounts of data-intensive applications often access the data server concurrently, the average completion time is enlarged due to the long request queue on data servers, especially when requests of applications cause a series of synchronous I/O requests. This paper proposes an I/O scheduling algorithm, called EML (equal-length multi-level algorithm) to solve this problem, it can reduce the average response time significantly. We demonstrate the performance improvement versus multi-level queue through both theoretical and experimental analysis. The experiments show that EML algorithm can effectively reduce the average completion time by 30% in 64 concurrent write applications and 50% in 64 concurrent read applications.	algorithm;emotion markup language;i/o scheduling;input/output;scheduling (computing)	Zhipeng Tan;Li Du;Dan Feng;Wei Zhou	2018	Future Generation Comp. Syst.	10.1016/j.future.2017.04.019	real-time computing;scheduling (computing);i/o scheduling;database server;distributed computing;performance improvement;computer science;algorithm;server;response time;queue	HPC	-12.746337876446798	55.00155868390628	100170
ccc847756c047ba43ca438d51d0ff3361d6d161e	supporting small accesses for the parallel file subsystem on distributed shared memory systems	telecommunication traffic tiles application software software performance software systems laboratories software prototyping prototypes degradation programming profession;parallel programming;file organisation distributed shared memory systems parallel programming;system performance;network traffic;distributed shared memory systems;system development;performance degradation small accesses parallel file subsystem distributed shared memory systems network traffic page based software dsm systems system performance cohesion treadmarks parallel file request root node teamster software cache mechanism asynchronous file offset mechanism;distributed shared memory;file organisation	The main goal of parallel file subsystem on Distributed Shared Memory (DSM) systems is to reduce the network traffic in page-based software DSM systems, thereby improving system performance. Our laboratory has built a prototype of the parallelJile subsystem on two DSM systems, namely Cohesion and TreadMarks. But these two prototypes have several limitations: users must read/write the whole parallelJle in a single access; users cannot modifjl an existing parallel file; the parallel file request must be issued from the root node. In our new parallel file subsystem on Teamster; a new DSM system developed by our laboratory, we eliminate the limitations revealing in the two previous parallel file subsystems. In addition, we have developed two new mechanisms, the software cache mechanism and the asynchronous Jile offset mechanism, to lessen the performance degradation caused by the frequent small accesses.	cohesion (computer science);distributed shared memory;elegant degradation;network packet;prototype;single-access key;treadmarks;tree (data structure)	Y.-Z. Liu;Su-Cheong Mac;Ce-Kuen Shieh	1998		10.1109/ICPADS.1998.741171	flash file system;distributed shared memory;shared resource;self-certifying file system;parallel computing;torrent file;memory-mapped file;device file;computer file;computer science;stub file;versioning file system;operating system;unix file types;ssh file transfer protocol;journaling file system;database;distributed computing;computer performance;open;distributed file system;file system fragmentation;replication;computer network;file control block;virtual file system	HPC	-14.54922254283069	50.67456727140804	100330
3e54bf228cf89d2c5265f7d34df77d62c3bfb3b6	evaluating trace cache energy efficiency	trace cache;instruction cache;engineering design;energy efficient;branch prediction;fetch engine energy efficiency	Future fetch engines need to be energy efficient. Much research has focused on improving fetch bandwidth. In particular, previous research shows that storing concatenated basic blocks to form instruction traces can significantly improve fetch performance. This work evaluates whether this concatenating of basic blocks translates to significant energy-efficiency gains. We compare processor performance and energy efficiency in trace caches compared to instruction caches. We find that, although trace caches modestly outperform instruction cache only alternatives, it is branch-prediction accuracy that really determines performance and energy efficiency. When access delay and area restrictions are considered, our results show that sequential trace caches achieve very similar performance and energy efficiency results compared to instruction cache-based fetch engines and show that the trace cache's failure to significantly outperform the instruction cache-based fetch organizations stems from the poorer implicit branch prediction from the next-trace predictor at smaller areas. Because access delay limits the theoretical performance of the evaluated fetch engines, we also propose a novel ahead-pipelined next-trace predictor. Our results show that an STC fetch organization with a three-stage, ahead-pipelined next-trace predictor can achieve 5--17% IPC and 29% ED2 improvements over conventional, unpipelined organizations.	bandwidth (signal processing);basic block;branch predictor;cpu cache;concatenation;kerrison predictor;tracing (software)	Michele Co;Dee A. B. Weikle;Kevin Skadron	2006	TACO	10.1145/1187976.1187980	parallel computing;real-time computing;computer hardware;computer science;efficient energy use;cache algorithms;engineering design process;branch predictor	Arch	-8.575195600813805	53.31252642619307	100486
b2a3f2ffb9a34908392d0997a1cf8975cf76a382	mixed-radix fft for improving cache performance	fast fourier transforms cache storage digital arithmetic;radix 2 fft mixed radix fft cache performance processors dynamic memories speeds fft programs cache addresses;abstracts software registers	The increasing difference between processors and dynamic memories speeds causes that the problem of cache performance is an issue of ever growing importance. In the paper it is proposed to increase cache performance in FFT programs by implementing mixed-radix FFTs for N=2rKs, where K is an odd number, and Ks is close, but smaller than some power of 2. This guarantees that data samples processed in one FFT butterfly have different cache addresses, hence, the number of cache conflicts is substantially diminished. Computer simulations for Ks=243, and 125 show that this is the case, indeed, and that in spite of higher numbers of arithmetical operations for conservative cache miss delays the mixed-radix FFTs do perform better than the radix-2 FFT.	algorithm;cpu cache;central processing unit;computer memories inc.;computer simulation;fast fourier transform;locality of reference;power of two	Ryszard Stasinski;Jacek Potrymajlo	2004	2004 12th European Signal Processing Conference		bus sniffing;pipeline burst cache;computer architecture;cache-oblivious algorithm;parallel computing;cache coloring;computer hardware;cache;computer science;write-once;cache invalidation;smart cache;cache algorithms;cache pollution	HPC	-8.066881317643674	51.95243500268501	100582
d61cbc99b6ae86d95226ba7a344cb5c501d46412	pmc: a requirement-aware dram controller for multicore mixed criticality systems		We propose a novel approach to schedule memory requests in Mixed Criticality Systems (MCS). This approach supports an arbitrary number of criticality levels by enabling the MCS designer to specify memory requirements per task. It retains locality within large-size requests to satisfy memory requirements of all tasks. To achieve this target, we introduce a compact time-division-multiplexing scheduler, and a framework that constructs optimal schedules to manage requests to off-chip memory. We also present a static analysis that guarantees meeting requirements of all tasks. We compare the proposed controller against state-of-the-art memory controllers using both a case study and synthetic experiments.	access time;best, worst and average case;computer memory;criticality matrix;dynamic random-access memory;experiment;locality of reference;mathematical optimization;mixed criticality;multi categories security;multi-core processor;multiplexing;network switch;overhead (computing);requirement;scheduling (computing);self-organized criticality;static program analysis;synthetic intelligence	Mohamed Hassan;Hiren D. Patel;Rodolfo Pellizzoni	2017	ACM Trans. Embedded Comput. Syst.	10.1145/3019611	flat memory model;registered memory;parallel computing;memory controller;real-time computing;memory management;memory refresh;interleaved memory;overlay;mixed criticality;computer science	Embedded	-8.002937925577253	58.2739104749016	100638
f51fa45346ac2c545dd728ee1227e91ee08e0e41	task mapping for redundant multithreading in multi-cores with reliability and performance heterogeneity	software reliability aging redundancy hardware multithreading timing;aging;redundancy;software reliability;software reliability computer architecture instruction sets algorithm design and analysis multithreading time complexity redundancy;hardware;multithreading;timing	"""Due to the architectural design, process variations and aging, individual cores in many-core systems exhibit heterogeneous performance. In many-core systems, a commonly adopted soft error mitigation technique is Redundant Multithreading (RMT) that achieves error detection and recovery through redundant thread execution on different cores for an application. However, <italic>task mapping</italic> and <italic>the task execution mode</italic> (i.e., whether a task executes in a reliable mode with RMT or unreliable mode without RMT) need to be considered for achieving resource-efficient reliability. This paper explores how to efficiently assign the tasks onto different cores with heterogeneous performance properties and determine the execution modes of tasks in order to achieve high reliability and satisfy the tolerance of timeliness. We demonstrate that the task mapping problem under heterogeneous performance can be solved by employing Hungarian Algorithm as subroutine to efficiently assign the tasks onto the cores to optimize the system reliability with polynomial time complexity. To obtain the efficient task execution modes, we also propose an iterative mode adaptation technique and guarantee the tolerable timing constraint. Our results illustrate that compared to state-of-the-art, the proposed approaches achieve up to <inline-formula><tex-math notation=""""LaTeX""""> $80$</tex-math><alternatives><inline-graphic xlink:type=""""simple"""" xlink:href=""""chen-ieq1-2532862.gif""""/></alternatives> </inline-formula> percent reliability improvement (on average <inline-formula><tex-math notation=""""LaTeX""""> $20$</tex-math><alternatives><inline-graphic xlink:type=""""simple"""" xlink:href=""""chen-ieq2-2532862.gif""""/></alternatives> </inline-formula> percent) under different scenarios of chip frequency variation maps."""	compiler;data dependency;deterministic routing;error detection and correction;greedy algorithm;hungarian algorithm;interaction;iterative method;manycore processor;map;matching (graph theory);minimum weight;multithreading (computer architecture);online and offline;operating system;overhead (computing);polynomial;run time (program lifecycle phase);scheduling (computing);simultaneous multithreading;soft error;subroutine;thread (computing);time complexity;virtual economy;xlink	Kuan-Hsun Chen;Jian-Jia Chen;Florian Kriebel;Semeen Rehman;Muhammad Shafique;Jörg Henkel	2016	IEEE Transactions on Computers	10.1109/TC.2016.2532862	embedded system;computer architecture;parallel computing;real-time computing;multithreading;computer science;operating system;temporal multithreading;redundancy;software quality	Embedded	-6.070384783839124	58.02991625293472	100702
daa5538192e0058e12a83bd64fd19866c01adcf6	thread criticality predictors for dynamic performance, power, and resource management in chip multiprocessors	caches;building block;chip multiprocessor;intel tbb;chip;thread criticality prediction;energy optimization;load balance;runtime system;memory hierarchy;dvfs;parallel programs;parallel applications;parallel processing;energy saving	With the shift towards chip multiprocessors (CMPs), exploiting and managing parallelism has become a central problem in computing systems. Many issues of parallelism management boil down to discerning which running threads or processes are critical, or slowest, versus which are non-critical. If one can accurately predict critical threads in a parallel program, then one can respond in a variety of ways. Possibilities include running the critical thread at a faster clock rate, performing load balancing techniques to offload work onto currently non-critical threads, or giving the critical thread more on-chip resources to execute faster.  This paper proposes and evaluates simple but effective thread criticality predictors for parallel applications. We show that accurate predictors can be built using counters that are typically already available on-chip. Our predictor, based on memory hierarchy statistics, identifies thread criticality with an average accuracy of 93% across a range of architectures.  We also demonstrate two applications of our predictor. First, we show how Intel's Threading Building Blocks (TBB) parallel runtime system can benefit from task stealing techniques that use our criticality predictor to reduce load imbalance. Using criticality prediction to guide TBB's task-stealing decisions improves performance by 13-32% for TBB-based PARSEC benchmarks running on a 32-core CMP. As a second application, criticality prediction guides dynamic energy optimizations in barrier-based applications. By running the predicted critical thread at the full clock rate and frequency-scaling non-critical threads, this approach achieves average energy savings of 15% while negligibly degrading performance for SPLASH-2 and PARSEC benchmarks.	benchmark (computing);cilk plus;clock rate;counter (digital);criticality matrix;dynamic voltage scaling;frequency scaling;image scaling;kerrison predictor;load balancing (computing);memory hierarchy;overhead (computing);parsec;parallel computing;runtime system;self-organized criticality;threading building blocks	Abhishek Bhattacharjee;Margaret Martonosi	2009		10.1145/1555754.1555792	chip;embedded system;parallel processing;computer architecture;parallel computing;real-time computing;telecommunications;computer science;load balancing;operating system	Arch	-5.334412163312553	54.22896857707995	100714
d9b47764db442dc1bc1dad1570c85367002afe4a	anatomy: an analytical model of memory system performance	dram;memory system performance;analytical model	Memory system design is increasingly influencing modern multi-core architectures from both performance and power perspectives. However predicting the performance of memory systems is complex, compounded by the myriad design choices and parameters along multiple dimensions, namely (i) technology, (ii) design and (iii) architectural choices. In this work, we construct an analytical model of the memory system to comprehend this diverse space and to study the impact of memory system parameters from latency and bandwidth perspectives. Our model, called ANATOMY, consists of two key components that are coupled with each other, to model the memory system accurately. The first component is a queuing model of memory which models in detail various design choices and captures the impact of technological choices in memory systems. The second component is an analytical model to summarize key workload characteristics, namely row buffer hit rate (RBH), bank-level parallelism (BLP), and request spread (S) which are used as inputs to the queuing model to estimate memory performance. We validate the model across a wide variety of memory configurations on 4, 8 and 16 cores using a total of 44 workloads. ANATOMY is able to predict memory latency with an average error of 8.1%, 4.1% and 9.7% over 4, 8 and 16 core configurations. We demonstrate the extensibility and applicability of our model by exploring a variety of memory design choices such as the impact of clock speed, benefit of multiple memory controllers, the role of banks and channel width, and so on. We also demonstrate ANATOMY's ability to capture architectural elements such as scheduling mechanisms (using FR_FCFS and PAR_BS) and impact of DRAM refresh cycles. In all of these studies, ANATOMY provides insight into sources of memory performance bottlenecks and is able to quantitatively predict the benefit of redressing them.	algorithm;cas latency;clock rate;dynamic random-access memory;extensibility;memory refresh;multi-core processor;parallel computing;queueing theory;refresh rate;scheduling (computing);semiconductor intellectual property core;systems design	Nagendra Dwarakanath Gulur;Mahesh Mehendale;R. Manikantan;Renganayaki Govindarajan	2014		10.1145/2591971.2591995	uniform memory access;real-time computing;simulation;distributed memory;computer hardware;computer science;operating system;flat memory model;dram;computing with memory	Metrics	-7.6690337365661465	51.344659634269384	100778
315b7d0fd0aca44892d9a5891b795bc9dcd6c1a7	popularity-driven dynamic replica placement in hierarchical data grids	distributed datasets;data intensive application;replication;bandwidth data grids replication replica placement execution time latency;history;optorsim hierarchical data grids popularity driven dynamic replica placement geographically distributed storage distributed datasets replica placement resource availability network latency;execution time;storage management;data grids;hierarchical data;geographically distributed storage;delay distributed computing grid computing availability costs computer peripherals application software large scale systems heuristic algorithms bandwidth;large scale;servers;aggregates;heuristic algorithms;replicated data;replica placement;data access;distributed databases;bandwidth;resource availability;latency;hierarchical data grids;network latency;grid computing;popularity driven dynamic replica placement;replicated databases;storage management grid computing replicated databases;geographic distribution;data grid;optorsim;data models;dynamic behavior	Data grids provide geographically distributed storage for large-scale data-intensive applications. Ensuring efficient access to such large and widely distributed datasets is hindered by high latencies. To speed up data access, data grid systems replicate data in multiple locations so a user can access the data from a nearby site. In addition to reducing data access time, replication also aims to use network and storage resources efficiently. While replication is a well-known technique, the problem of replica placement has not been widely studied for data grid environments. To obtain the best possible gains from replication, strategic placement of the replicas is critical. In a grid environment resource availability, network latency, and userspsila requests can vary. To address these issues a placement strategy is needed that adapts to dynamic behavior. This paper proposes a new dynamic replica placement algorithm for hierarchical data grids based on file ldquopopularityrdquo. Our goal is to place replicas close to the clients to reduce access time while using the network and storage efficiently thereby effectively balancing storage cost and access latency. We evaluate our algorithm using OptorSim which shows that our approach outperforms other techniques in terms of access time and bandwidth used.	access time;algorithm;clustered file system;data access;data-intensive computing;hierarchical database model;run time (program lifecycle phase);self-replicating machine;simulation	Mohammad Shorfuzzaman;Peter C. J. Graham;M. Rasit Eskicioglu	2008	2008 Ninth International Conference on Parallel and Distributed Computing, Applications and Technologies	10.1109/PDCAT.2008.64	latency;parallel computing;computer science;operating system;data grid;database;distributed computing;distributed database	HPC	-17.871515820816338	58.21390177357446	100974
5bff6bc66a6917586e1340350a57698d421e6e6f	a reliable mtd design for mlc flash-memory storage systems	flash memory;reliability;storage system;mirror;raid;mlc;chip;bad block;error correction;error rate;error recover;experience base;device driver	The reliability of flash-memory chips has dropped dramatically in recent years. In order to solve this problem, a reliable memory technology device (MTD) design is proposed to address this concern at the device driver layer so as to release the design complexity of flash-memory management software/firmware and to improve the maintainability and portability of flash management designs for existing and future products. The proposed design was evaluated through a series of experiments based on realistic traces to show that the proposed approach could significantly improve the reliability of flash memory with limited overheads.	device driver;experiment;firmware;flash memory;mtd-f;memory technology device;memory management;multi-level cell;software portability;tracing (software)	Yuan-Hao Chang;Tei-Wei Kuo	2010		10.1145/1879021.1879045	chip;flash file system;embedded system;real-time computing;error detection and correction;computer hardware;word error rate;computer science;operating system;reliability;raid	Embedded	-11.598546880880383	55.29717477115666	101008
aa196c6c9d4a33de06c86d9108610a603e1b1266	energy-efficient data temporal consistency maintenance for iot systems		In many Internet of Things systems, it is required to process a good supply of real-time data from the physical world. An important goal when designing such systems is to maintain data temporal consistency while consuming less power. In this paper, we propose, to our knowledge, the first solution to the energy-efficient temporal consistency maintenance problem on Dynamic Voltage and Frequency Scaling (DVFS)-capable multicore platforms. We consider the problem of how to minimize the overall total power consumption on multicore, while the temporal consistency of real-time data objects can be maintained. To end this, firstly, we propose an efficient per-CPU DVFS solution, under which the transaction set can be scheduled to meet the temporal consistency requirement while resulting in significant energy savings. Next, by adopting the proposed unicore DVFS techniques on each core, we further propose new energy-efficient mapping techniques to explore energy savings for multicore platforms. Finally, extensive simulation experiments are conducted and the results demonstrate the proposed solutions outperforms existing methods in terms of energy consumption (up to (55%)).		Guohui Li;Chunyang Zhou;Jianjun Li;Bing Guo	2018		10.1007/978-3-030-05054-2_39	maintenance problem;distributed computing;voltage;energy consumption;efficient energy use;computer science;frequency scaling;multi-core processor;database transaction;internet of things	AI	-5.274834705347019	59.09742935334668	101113
2a30ddcf1ad0bd1c32008d485b5c5f940b06eae4	a limit study on the potential of compression for improving memory system performance, power consumption, and cost	perforation;low power;low latency;memory systems;floating point;exponential growth;power consumption	Continuing exponential growth in processor performance, combined with technology, architecture, and application trends, place enormous demands on the memory system to allow information storage and exchange at a high-enough performance (i.e., to provide low latency and high bandwidth access to large amounts of information), at low power, and cost-effectively. This paper comprehensively analyzes the redundancy in the information (addresses, instructions, and data) stored and exchanged between the processor and the memory system and evaluates the potential of compression in improving performance, power consumption, and cost of the memory system. Traces obtained with Sun Microsystems’ Shade simulator simulating SPARC executables of eight integer and seven floating-point programs in the SPEC CPU2000 benchmark suite and five programs from the MediaBench suite, and analyzed using Markov entropy models, existing compression schemes, and CACTI 3.0 and SimplePower timing, power, and area models yield impressive results.	benchmark (computing);digital footprint;executable;markov chain;sparc;simulation;time complexity	Nihar R. Mahapatra;Jiangjiang Liu;Krishnan Sundaresan;Srinivas Dangeti;Balakrishna V. Venkatrao	2005	J. Instruction-Level Parallelism		embedded system;exponential growth;parallel computing;real-time computing;telecommunications;computer science;floating point;operating system;low latency	Arch	-5.160098948359561	50.91970654925031	101165
03e0eec3a638240f535e59b22696686cf2d962a3	meeting cpu constraints by delaying playout of multimedia tasks	multimedia application;mobile phone;playout delay;scheduling algorithm;data dependence;scheduling multimedia tasks;buffering	Multimedia applications today constitute a significant fraction of the workload running on portable devices such as mobile phones, PDAs and MP3 players. However, the processors in such devices are usually not powerful enough to support multiple concurrently executing multimedia tasks. In this context, different processor scheduling algorithms have attracted a lot of attention. This paper attempts to address the CPU constraint problem from a different perspective. It is based on the observation that by increasing the playout delay of a multimedia task, the minimum processor frequency required to run the task decreases. This is due to the high data-dependent variability in the execution requirements of multimedia tasks. We also present a framework, using which it is possible to compute the minimum processor frequency corresponding to any playout delay. Given a set of concurrently executing multimedia tasks, using our framework it is possible to compute the playout delays for each of these tasks, such that the sum of their corresponding processor cycle requirements do not exceed the maximum frequency supported by the processor.	algorithm;central processing unit;data dependency;heart rate variability;instruction cycle;mp3;mobile phone;personal digital assistant;playout;requirement;scheduling (computing);windows nt processor scheduling	Balaji Raman;Samarjit Chakraborty;Wei Tsang Ooi	2005		10.1145/1065983.1066021	embedded system;parallel computing;real-time computing;computer science;data buffer;operating system;scheduling	DB	-4.7020256031563665	59.37519644686778	101191
8653a552fdbcaeba1d0937162021d566aeee84e7	workload characterization for shared resource management in multi-core systems	multicore processing sensitivity resource management interference degradation partitioning algorithms hardware;resource allocation cache storage multiprocessing systems optimisation pattern classification performance evaluation;memory reuse cross core interference workload characterization cache partitioning shared resources instructions per cycle ipc;full system simulator workload characterization shared resource management multicore systems multicore industry optimal performance performance degradation multicore processors hardware resource sharing classification schemes dynamic cache partitioning mechanism speccpu 2006 benchmark suite simics	The multi-core industry is facing a number of challenges which need to be addressed to achieve optimal performance from the multiple cores. One of the dominant causes of performance degradation that arises in multi-core processors is due to hardware resource sharing between cores. The degradation in performance is largely dependent on the nature of the applications. To minimize this degradation, application characteristics need to be analyzed and based on this, a solution can be devised to reduce degradation. This paper aims at presenting a consolidated survey of present classification schemes and propose a new classification scheme which can be applied on-line. Also, using this scheme, a dynamic cache partitioning mechanism has been proposed. The experimentation has been conducted by constructing workloads using the SPECCPU 2006 benchmark suite and experimentation has been conducted on SIMICS (full system simulator).	benchmark (computing);cpu cache;central processing unit;comparison and contrast of classification schemes in linguistics and metadata;computer architecture simulator;elegant degradation;multi-core processor;online and offline;simics;simulation	Sapna U. Prabhu;Rohin Daruwala	2014	2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2014.6968243	computer architecture;parallel computing;real-time computing;computer science	HPC	-8.843919645252964	50.67261426570305	101358
11d0556136f873abe165292aa23e013177e6dc43	the performance and energy consumption of embedded real-time operating systems	echidna;microcontrollers;µc os ii;processor scheduling;real time;model performance;simbed;real time operating system;embedded real time operating system;energy modelling energy consumption embedded real time operating systems rtos simbed embedded systems execution driven simulation testbed microcontroller architectural model spl mu c os ii echidna nos multirate task scheduler motorola m core processor task invocations disrupt timing preemptive systems cooperative systems chimera power modelling;embedded system;public domain;real time operating systems rtos;embedded systems;virtual computers microprocessors operating systems scheduling power demand;low power;mechanical index;cooperative systems;virtual machines;energy consumption;execution driven simulation;technical report;power consumption;task scheduling;chimera;power consumption embedded systems virtual machines microcontrollers operating systems computers digital simulation processor scheduling;power and energy modeling;performance modeling;article;motorola m core;operating systems computers;simulation environment;digital simulation	This paper presents the modeling of embedded systems with SimBed, an execution-driven simulation testbed that measures the execution behavior and power consumption of embedded applications and RTOSs by executing them on an accurate architectural model of a microcontroller with simulated real-time stimuli. We briefly describe the simulation environment and present a study that compares three RTOSs: C/OS-II, a popular public-domain embedded real-time operating system; Echidna, a sophisticated, industrial-strength (commercial) RTOS; and NOS, a bare-bones multirate task scheduler reminiscent of typical “roll-your-own” RTOSs found in many commercial embedded systems. The microcontroller simulated in this study is the Motorola M-CORE processor: a low-power, 32-bit CPU core with 16-bit instructions, running at 20MHz. Our simulations show what happens when RTOSs are pushed beyond their limits and they depict situations in which unexpected interrupts or unaccounted-for task invocations disrupt timing, even when the CPU is lightly loaded. In general, there appears no clear winner in timing accuracy between preemptive systems and cooperative systems. The power-consumption measurements show that RTOS overhead is a factor of two to four higher than it needs to be, compared to the energy consumption of the minimal scheduler. In addition, poorly designed idle loops can cause the system to double its energy consumption—energy that could be saved by a simple hardware sleep mechanism.	16-bit;32-bit;central processing unit;consensus dynamics;digital signal processor;embedded system;idle scan;interrupt;low-power broadcasting;message passing;microcontroller;m·core;nos;overhead (computing);preemption (computing);real-time clock;real-time operating system;real-time transcription;requirement;scheduling (computing);semaphore (programming);simulation;testbed;windows task scheduler	Kathleen Baynes;Chris Collins;Eric Fiterman;Brinda Ganesh;Paul Kohout;Christine Smit;Tiebing Zhang;Bruce Jacob	2003	IEEE Trans. Computers	10.1109/TC.2003.1244943	microcontroller;embedded system;mechanical index;embedded operating system;parallel computing;public domain;real-time computing;real-time operating system;computer science;virtual machine;technical report;operating system;chimera	Embedded	-5.969087060973034	51.02014148289185	101451
0be2ea6c2cd7b8cc38f8303cbd8b4449e52517ac	parallel load balancing for problems with good bisectors	resource allocation;parallel algorithms resource allocation;upper bound;simulation experiment;property a;load management computational modeling partial differential equations finite element methods reactive power upper bound parallel processing throughput runtime numerical simulation;load balance;worst case load imbalance load balancing bisection properties running time parallel variants;parallel algorithms	Parallel load balancing is studied for problems with certain bisection properties. A class of problems has /spl alpha/-bisectors if every problem p of weight w(p) in the class can be subdivided into two subproblems whose weight (load) is at least an a-fraction of the original problem. A problem p is to be split into N subproblems such that the maximum weight among them is as close to w(p)/N as possible. It was previously known that good load balancing can be achieved for such classes of problems using Algorithm HF, a sequential algorithm that repeatedly bisects the subproblem with maximum weight. Several parallel variants of Algorithm HF are introduced and analyzed with respect to worst-case load imbalance, running-time, and communication overhead. For fixed /spl alpha/, all variants have running-time O(log N) and provide constant upper bounds on the worst-case load imbalance. Results of simulation experiments regarding the load balance achieved in the average case are presented.	load balancing (computing)	Stefan Bischof;Ralf Ebner;Thomas Erlebach	1999		10.1109/IPPS.1999.760528	mathematical optimization;parallel computing;computer science;distributed computing	Theory	-13.31234217822729	59.77844455721174	101455
d85d056a8b32e4591a4b169a7f333b791f7ec51e	cooperative caching in the pcfs parallel cluster file system	fibre channel shared devices;cache storage;parallel i o access;storage area network;video streaming;cooperative caching;api;large data sets;spectrum;storage area networks;fibre channel;cooperative caching architecture;memory architecture;file system;application program interfaces;parallel cluster file system;parallel i o;disk cluster file system;high performance;cache management;cooperative caching file systems storage area networks computer architecture computer applications computer networks concurrent computing high performance computing streaming media databases;parallel processing;storage area networks application program interfaces cache storage memory architecture parallel processing;storage area network parallel cluster file system cooperative caching architecture disk cluster file system api parallel i o access fibre channel shared devices	This short paper describes the cooperative caching architecture of pCFS, a shared disk cluster file system (CFS) which aims to achieve high performance in a broad spectrum of I/O intensive applications ranging from computational access to large data sets to video streaming and databases, and includes an extended API for parallel I/O access. pCFS is targeted at small to medium sized clusters where data is stored in fibre channel shared devices on a storage area network (SAN) and exploits two interconnect fabrics: a SAN to access on-disk data, and a LAN, used both for the exchange of control information (related to locking and cache management) and for cooperative caching dataflow	cache (computing);clustered file system;database;dataflow;fibre channel;filesystem-level encryption;input/output;lock (computer science);parallel i/o;storage area network;streaming media	Paulo Afonso Lopes;Pedro D. Medeiros	2006	2006 15th IEEE International Conference on High Performance Distributed Computing	10.1109/HPDC.2006.1652177	parallel processing;parallel computing;real-time computing;storage area network;computer science;operating system	HPC	-11.441318153235086	46.64024921148447	101470
01e4434464aaba1d81b7366c5af3728e802519be	clock selection for performance optimization of control-flow intensive behaviors	resource constraint;performance enhancing clock periods performance optimization control flow intensive behaviors clock selection algorithm deeply nested loops branch probabilities allocation constraints optimal clock period critical path analysis engine target behaviors;clocks;resource allocation;nested loops;high level synthesis;clocks optimization resource management circuit synthesis delay estimation gears nominations and elections performance analysis engines testing;scheduling;critical path;control flow;circuit optimisation;clocks delays high level synthesis scheduling circuit optimisation resource allocation;performance optimization;delays	This paper presents a clock selection algorithm for control-flow intensive behaviors that are characterized by the presence of conditionals and deeply-nested loops. Unlike previous works, which are primarily geared rewards data-dominated behaviors, this algorithm examines the effects of branch probabilities and their interaction with allocation constraints. We demonstrate, using examples, how changing branch probabilities and resource allocation can dramatically affect the optimal clock period, and hence, the performance of the schedule, and show that the interaction of these two factors must also be taken into account when searching for an optimal clock period. We then introduce the clock selection algorithm, which employs a fast critical-path analysis engine that allows it to evaluate what effect different clock periods, branch probabilities, and resource allocations may ultimately have on the performance of the behavior. When evaluating the critical path, we exploit the fact that our target behaviors exhibit locality of execution. We tested our algorithm using a number of benchmarks from various sources. A series of experiments demonstrates that our algorithm is quickly capable of selecting a small set of performance enhancing clock periods, among which the optimal clock period typically lies. Another experiment demonstrates that the algorithm can adapt to varying resource constraints.	control flow	Kamal S. Khouri;Niraj K. Jha	2000		10.1109/ICVD.2000.812661	clock synchronization;embedded system;real-time computing;simulation;nested loop join;resource allocation;computer science;operating system;critical path method;distributed computing;timing failure;high-level synthesis;control flow;scheduling	EDA	-5.676940558989185	51.765948208708195	101516
356100b33d589bb48fa1a6518a85efb551a13d9b	gsc: greedy shard caching algorithm for improved i/o efficiency in graphchi	graph computaion;graphchi;gsc	Disk-based large scale graph computation on a single machine has been attracting much attention, with GraphChi as one of the most well-accepted solutions. However, we find out that the performance of GraphChi becomes I/O-constrained when memory is moderately abundant, and from some point adding more memory does not help with the performance any more. In this work, a greedy caching algorithm GSC is proposed for GraphChi to make better use of the memory. It alleviates the I/O constraint by caching and delaying the write-backs of GraphChi shards that have already been loaded into the memory. Experimental results show that by minimizing unnecessary I/Os, GSC can be up to 4x faster during computation than standard GraphChi under memory constraint, and achieve about 3x performance gain when sufficient memory is available.	cache (computing);computation;gsc bus;greedy algorithm;input/output;shard (database architecture)	Dagang Li;Zehua Zheng	2017	2017 IEEE 25th International Conference on Network Protocols (ICNP)	10.1109/ICNP.2017.8117588	algorithm design;computation;algorithm;computer science;input/output;distributed computing;shard;graph	Visualization	-15.881003319409606	54.392005141960055	101622
31cb0c077942fe0a70e8687d51603b2c43cf1704	optimizing a semantic comparator using cuda-enabled graphics hardware	kernel;semantic indexing;semantics indexes servers search engines kernel graphics processing unit tensile stress;paper;tensile stress;bloom filter;green;search engines;computer graphic equipment;semantics;semantic comparator;dot product computation;coprocessors;green computing semantic comparator gpgpu dot product computation semantic router core bloom filter;cuda;distributed search engine data center semantic comparator optimization cuda enabled graphics hardware semantic search technique concept trees cuda enabled gpgpu coprocessor hash computations bloom filter parallel reduction intel x86 processor sequential processing;gpgpu;indexes;servers;hashing;parallel architectures;search engines computer graphic equipment coprocessors information filters parallel architectures;nvidia;computer science;tesla c870;graphics processing unit;information filters;semantic router core;green computing	"""Emerging semantic search techniques require fast comparison of large """"concept trees"""". This paper addresses the challenges involved in fast computation of similarity between two large concept trees using a CUDA-enabled GPGPU co-processor. We propose efficient techniques for the same using fast hash computations, membership tests using Bloom Filters and parallel reduction. We show how a CUDA-enabled mass produced GPU can form the core of a semantic comparator for better semantic search. We experiment run-time, power and energy consumed for similarity computation on two platforms: (1) traditional sever class Intel x86 processor (2) CUDA enabled graphics hardware. Results show 4x speedup with 78% overall energy reduction over sequential processing approaches. Our design can significantly reduce the number of servers required in a distributed search engine data center and can bring an order of magnitude reduction in energy consumption, operational costs and floor area."""	bloom filter;cuda;comparator;computation;coprocessor;data center;distributed web crawling;general-purpose computing on graphics processing units;graphics hardware;graphics processing unit;optimizing compiler;semantic search;speedup;web search engine;x86	Aalap Tripathy;Suneil Mohan;Rabi N. Mahapatra	2011	2011 IEEE Fifth International Conference on Semantic Computing	10.1109/ICSC.2011.56	database index;green computing;parallel computing;kernel;hash function;computer hardware;computer science;theoretical computer science;bloom filter;database;semantics;stress;programming language;world wide web;general-purpose computing on graphics processing units;coprocessor;server	HPC	-5.031181347252024	47.804723983376164	101735
04b18c49a01b11bfd27fa273fe3869acff776001	3dftl: a three-level demand-based translation strategy for flash device	flash memory;cache;mlc;three level;ftl;compression;article			Peera Thontirawong;Chundong Wang;Weng-Fai Wong;Mongkol Ekpanyapong;Prabhas Chongstitvatana	2015	IEICE Electronic Express	10.1587/elex.12.20150211	flash file system;parallel computing;real-time computing;computer hardware;cache;computer science;compression	HCI	-10.869400209704438	54.35237830445011	101909
be77fd5d86729d649859d7c457d9ca92e04bba3d	workload-aware elastic striping with hot data identification for ssd raid arrays	memory management;performance evaluation;arrays fault tolerance fault tolerant systems prototypes performance evaluation memory management;prototypes;arrays;fault tolerant systems;fault tolerance;workload awareness ssd raid raid level garbage collection elastic striping	Redundant array of independent disk (RAID) offers a good option to provide device-level fault tolerance for solid-state drives (SSDs). However, parity update with either read–modify–write or read–reconstruct–write may introduce a lot of extra I/Os and thus significantly degrades SSD RAID performance. To reduce the parity update cost, elastic striping chooses to reconstruct new stripes with only the newly updated data chunks instead of directly updating parity chunks. However, it necessitates an RAID-level garbage collection (GC) process, which may incur a very high cost due to the mixture of hot and cold data chunks. To address this problem, we follow the idea of elastic striping and propose a workload-aware scheme (WAS) to reduce the RAID-level GC cost so as to improve the performance and endurance of SSD RAID. In particular, we first develop a novel lightweight hot data identification scheme which requires only a very small computation time and memory cost, then propose a hotness-aware elastic striping approach to separately write data chunks with different hotness to different regions in SSD RAID. To evaluate the effectiveness and efficiency of our WAS, we implement a prototype system on RAID-5 and RAID-6 arrays composed of commercial SSDs. Experimental results show that compared to original elastic striping, our scheme reduces 30.0%–70.6% (and 23.9%–63.2%) of chunk writes under the RAID-5 (and RAID-6) settings, and also reduces the average response time by 60.9%–79.3% (and 56.8%–80.9%) for RAID-5 (and RAID-6), respectively. Besides, our scheme also improves the endurance and reliability of SSD RAID compared to original elastic striping.	computation;data striping;experiment;fault tolerance;garbage collection (computer science);identification scheme;locality of reference;prototype;read-modify-write;response time (technology);solid-state drive;standard raid levels;stripes;time complexity	Yongkun Li;Biaobiao Shen;Yubiao Pan;Yinlong Xu;Zhipeng Li;John C. S. Lui	2017	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2016.2604292	embedded system;fault tolerance;parallel computing;real-time computing;computer science;engineering;operating system;prototype;non-standard raid levels;parity drive;nested raid levels;standard raid levels;raid;raid processing unit;memory management	OS	-12.248441773293582	53.80537896240055	102267
aed11dc1278095dcbbfcca1da65af408a08d8510	a direct array injection technique in a fine-grain multithreading execution model	storage allocation;direct array injection technique;i structures;yarn;concurrent computing;performance evaluation;application software;performance;consumer activation;manipulator dynamics;nonstrict array access;performance evaluation storage allocation data structures parallel processing data handling concurrency control;multithreaded computing environment;multithreading delay parallel processing multiprocessing systems yarn concurrent computing production manipulator dynamics application software data structures;global heap space;fine grain multithreading execution model;network traffic;data structures;dait;processor utilization;structure store;concurrency control;production;array production;dynamic memory allocation;multiprocessing systems;data handling;global heap space direct array injection technique fine grain multithreading execution model optimized array handling scheme dait i structures parallel array access nonstrict array access multithreaded computing environment array production processor utilization performance producer activation consumer activation structure store network traffic dynamic memory allocation;optimized array handling scheme;parallel processing;parallel array access;producer activation;multithreading	We propose on optimized array handling scheme called the direct array injection technique (DAIT) which can be used in conjunction with I-Structures to provide non-strict and parallel array accesses in a fine/medium grain multithreaded computing environment. By overlapping array production and consumption, better processor utilization can be achieved resulting in higher performance. The direct array injection technique differs from the I-Structure approach in that array elements are forwarded directly from the producer activation to the consumer activation without being buffered in a structure store. The advantages of this technique over I-Structures are: network traffic is reduced; and dynamic memory allocation from the global heap space to store arrays is avoided. >	simultaneous multithreading;thread (computing)	Chinhyun Kim;Jean-Luc Gaudiot	1994		10.1109/IPPS.1994.288302	hashed array tree;embedded system;parallel computing;real-time computing;computer science;sparse array	Arch	-11.36275470247302	51.28411933892069	102328
f327caf0761ca8bdb550874a251e8497224da266	the nvax and nvax+ high-performance vax microprocessor		The NVAX and NVAX+ CPU chips are high-performance VAX microprocessors that use techniques traditionally associated with RISC microprocessor designs to dramatically improve VAX performance. The two chips provide an upgrade path for existing VAX systems and a migration path from VAX systems to the new Alpha AXP systems. The design evolved throughout the project as timeto-market, performance, and complexity trade-offs were made. Special design features address the issues of debug, maintenance, and analysis.	central processing unit;dec alpha;debugging;microprocessor;vax	G. Michael Uhler;Debra Bernstein;Larry L. Biro;John F. Brown;John H. Edmondson;Jeffrey D. Pickholtz;Rebecca L. Stamm	1992	Digital Technical Journal		computer architecture;computer science;ethanol;membrane;permeation;transdermal nitroglycerin;vinyl acetate;ethylene-vinyl acetate;angina	Arch	-16.808253198308453	49.56833607619034	102345
4e221f4bc9285f611bf61e29385e4f2b18c4206d	a reusability-aware cache memory sharing technique for high-performance low-power cmps with private l2 caches	l2 cache;chip multiprocessors cmps;cache storage;memory management;shared l2 cache;l2 cache organization;radiation detectors;performance;cache memory;system on a chip;embedded systems;shared l2 cache reusability aware cache memory sharing chip multiprocessors on chip cache memories l2 cache organization private l2 cache;low power;multiprocessing systems cache storage microprocessor chips;private l2 cache;chip multiprocessors;energy consumption;cache memory delay embedded system energy consumption permission mobile handsets research and development computer science power engineering and energy writing;performance l2 cache architecture chip multiprocessors cmps embedded systems low power;multiprocessing systems;organizations;reusability aware cache memory sharing;architecture;on chip cache memories;program processors;microprocessor chips	Chip multiprocessors (CMPs) emerge as a dominant architectural alternative in high-end embedded systems. Since off-chip accesses require a long latency and consume a large amount of power, CMPs are typically based on multiple levels of on-chip cache memories. To meet the performance demand and power budget, an efficient support for memory hierarchy is important. We propose an on-chip L2 cache organization which takes advantage of both a private L2 cache and a shared L2 cache to improve the performance and reduce energy consumption. Our L2 cache organization is based on a private L2 cache organization which has the short access latency. When a cache block in the private L2 cache is selected for an eviction, our proposed organization first evaluates the reusability of the cache block. If the cache block is likely to be reused, we save the evicted cache block in one of peer L2 caches which may have efficiently invalid blocks. By selectively writing evicted cache blocks to peer L2 caches, the proposed L2 cache organization can effectively simulate a shared L2 cache. Experimental results using a CMP simulator showed that the proposed L2 cache organization improved the average memory latency by up to 27% and reduced energy consumption by up to 16.6% over a 256KB private L2 cache organization for the SPLASH2 benchmark programs..	benchmark (computing);cas latency;cpu cache;embedded system;low-power broadcasting;memory hierarchy;simulation	Sungjune Youn;Hyunhee Kim;Jihong Kim	2007	Proceedings of the 2007 international symposium on Low power electronics and design (ISLPED '07)	10.1145/1283780.1283793	bus sniffing;least frequently used;pipeline burst cache;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;operating system;adaptive replacement cache;smart cache;memory organisation;mesi protocol;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture	Arch	-6.710048293912933	53.69198019812413	102374
1d1b2113c067533f55797254d93131f8aa3c0679	tpnfs: efficient support of small files processing over pnfs	servers layout distributed databases throughput aggregates writing bandwidth;load balance pnfs parallel io file system;input output programs;input output programs file organisation;nfsv4 tpnfs large scale data intensive application i o bandwidth booting domu clients scientific computing environments data petabytes;file system;pnfs;load balance;parallel io;file organisation	Large scale data-intensive applications that consume and produce terabytes or even pet bytes of data raise an ever increasing demand on I/O bandwidth. In order to meet this demand, NFSv4 architects design parallel NFS (pNFS), an NFS extension allowing clients to read/write data from/to multiple data servers in parallel. Though pNFS can support large files processing efficiently, we found that it has deficiency in processing small files. Unfortunately, small files dominate for a large number of applications in scientific computing environments. To deal with the problem, this paper presents tpNFS, an extension to pNFS that adds a transport driver to the pNFS metadata servers to make data of files, no matter small files or large files, be stripped more evenly onto multiple data servers. Our experiments with booting DomU clients from tpNFS and manipulating a large number of files show that tpNFS has better performance than pNFS for small files processing, especially when many clients read/write concurrently. As for large files processing, tpNFS introduces nearly no overhead when compared with pNFS.	angular defect;booting;byte;computational science;data-intensive computing;experiment;input/output;overhead (computing);parallel i/o;performance evaluation;terabyte	Bo Wang;Jinlei Jiang;Guangwen Yang	2013	2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum	10.1109/IPDPSW.2013.36	real-time computing;computer science;operating system;database	Arch	-15.892238658393287	52.788953486965134	102382
0dd5e83f87d884e1db9cf6a6597224c56f3b78fe	a scalable low power issue queue for large instruction window processors	instruction level parallel;cycle time;issue queue;low power architecture;out of order;complexity effective architecture;low power;energy consumption;issue logic;indexation;wakeup logic;school of automation;superscalar processor;energy delay product;computer science automation formerly;energy saving	Large instruction windows and issue queues are key to exploiting greater instruction level parallelism in out-of-order superscalar processors. However, the cycle time and energy consumption of conventional large monolithic issue queues are high. Previous efforts to reduce cycle time segment the issue queue and pipeline wakeup. Unfortunately, this results in significant IPC loss. Other proposals which address energy efficiency issues by avoiding only the unnecessary tag-comparisons do not reduce broadcasts. These schemes also increase the issue latency.To address both these issues comprehensively, we propose the Scalable Lowpower Issue Queue (SLIQ). SLIQ augments a pipelined issue queue with direct indexing to mitigate the problem of delayed wakeups while reducing the cycle time. Also, the SLIQ design naturally leads to significant energy savings by reducing both the number of tag broadcasts and comparisons required.A 2 segment SLIQ incurs an average IPC loss of 0.2% over the entire SPEC CPU2000 suite, while achieving a 25.2% reduction in issue latency when compared to a monolithic 128-entry issue queue for an 8-wide superscalar processor. An 8 segment SLIQ improves scalability by reducing the issue latency by 38.3% while incurring an IPC loss of only 2.3%. Further, the 8 segment SLIQ significantly reduces the energy consumption and energy-delay product by 48.3% and 67.4% respectively on average.	central processing unit;instruction window;instruction-level parallelism;microsoft windows;parallel computing;register renaming;scalability;superscalar processor	Rajesh Vivekanandham;Bharadwaj S. Amrutur;Renganayaki Govindarajan	2006		10.1145/1183401.1183427	embedded system;parallel computing;real-time computing;cycle time variation;computer science;out-of-order execution;operating system	HPC	-7.931982526484228	53.43355274461033	102470
39c84afaece2b2b6df439743d24c846024a6a3f2	job management requirements for nas parallel systems and clusters	distributed memory;project management;management system;numerical aerodynamic simulation;resource allocation;distributed processing;management systems;computational fluid dynamics;parallel systems;scheduling;workstations;high performance computer;parallel computers;high performance;job scheduling;memory computers;supercomputers	A job management system is a critical component of a production supercomputing environment, permitting oversubscribed resources to be shared fairly and ef ficiently. Job management systems that were originally designed for traditional vector supercomputers are not appropriate for the distributed-memory parallel supercomputers that are becoming increasingly important in the high performance computing industry . Newer job management systems of fer new functionality but do not solve fundamental problems. W e address some of the main issues in resource allocation and job scheduling we have encountered on two parallel computers — a 160-node IBM SP2 and a cluster of 20 high performance workstations located at the Numerical Aerodynamic Simulation facility . We describe the requirements for resource allocation and job management that are necessary to provide a production supercomputing environment on these machines, prioritizing according to dif ficulty and importance, and advocating a return to fundamental issues.	computer cluster;deployment environment;distributed memory;java message service;job scheduler;job stream;overselling;parallel computing;portable batch system;production system (computer science);requirement;scheduling (computing);simulation;supercomputer;vector processor;workstation	William Saphir;Leigh Ann Tanner;Bernard Traversat	1995		10.1007/3-540-60153-8_37	project management;computer architecture;parallel computing;computer science;operating system;management system;distributed computing	HPC	-16.552625661179047	59.59454240797128	102580
66f670d6d55f9abd142c1929e54c6f3ddae96b66	readex: linking two ends of the computing continuum to improve energy-efficiency in dynamic applications		In both the embedded systems and High Performance Computing domains, energy-efficiency has become one of the main design criteria. Efficiently utilizing the resources provided in computing systems ranging from embedded systems to current petascale and future Exascale HPC systems will be a challenging task. Suboptimal designs can potentially cause large amounts of underutilized resources and wasted energy. In both domains, a promising potential for improving efficiency of scalable applications stems from the significant degree of dynamic behaviour, e.g., runtime alternation in application resource requirements and workloads. Manually detecting and leveraging this dynamism to improve performance and energy-efficiency is a tedious task that is commonly neglected by developers. However, using an automatic optimization approach, application dynamism can be analysed at design time and used to optimize system configurations at runtime. The European Union Horizon 2020 READEX (Runtime Exploitation of Application Dynamism for Energy-efficient eX-ascale computing) project will develop a tools-aided auto-tuning methodology inspired by the system scenario methodology used in embedded systems. Dynamic behaviour of HPC applications will be exploited to achieve improved energy-efficiency and performance. Driven by a consortium of European experts from academia, HPC resource providers, and industry, the READEX project aims at developing the first of its kind generic framework to split design time and runtime automatic tuning while targeting heterogeneous system at the Exascale level. This paper describes plans for the project as well as early results achieved during its first year. Furthermore, it is shown how project results will be brought back into the embedded systems domain.		Per Gunnar Kjeldsberg;Andreas Gocht;Michael Gerndt;Lubomir Riha;Joseph Schuchart;Umbreen Sabir Mian	2017	Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017		embedded system;programming;electronic engineering;real-time computing;simulation;computer science;engineering;electrical engineering	HPC	-4.992907026765846	49.66071171854126	102674
3d12b22e9cf63dfda5ef7e6bf5fd8398b69ff4cb	scheduler activations for interference-resilient smp virtual machine scheduling		The wide adoption of SMP virtual machines (VMs) and resource consolidation present challenges to efficiently executing multi-threaded programs in the cloud. An important problem is the semantic gaps between the guest OS and the hypervisor. The well-known lock-holder preemption (LHP) and lock-waiter preemption (LWP) problems are examples of such semantic gaps, in which the hypervisor is unaware of the activities in the guest OS and adversely deschedules virtual CPUs (vCPUs) that are executing in critical sections. Existing studies have focused on inferring a high-level semantic state of the guest OS to aid hypervisor-level scheduling so as to avoid the LHP and LWP problems.  In this work, we find a reverse semantic gap - the guest OS is oblivious of the scheduling events at the hypervisor, leaving the potential of addressing the LHP and LWP problems in the guest OS unexploited. Inspired by scheduler activations (SAs) in hybrid threading, we proposed interference-resilient scheduling (IRS), a guest-hypervisor coordinated approach to enhancing load balancing in the guest. IRS informs the guest OS before vCPU preemption happens at the hypervisor to activate in-guest load balancing. As such, critical threads on preempted vCPUs can be migrated to other running vCPUs so that the LHP and LWP problems are all alleviated. Experimental results with Xen and Linux guests show as much as 42%, 43%, and 46% performance improvement for parallel programs with blocking, spinning synchronizations, and multithreaded server workloads, respectively.	blocking (computing);central processing unit;cloud computing;critical section;high- and low-level;hypervisor;institute for operations research and the management sciences;interference (communication);light-weight process;linux;load balancing (computing);loop heat pipe;operating system;preemption (computing);scheduler activations;scheduling (computing);semiconductor consolidation;server (computing);symmetric multiprocessing;thread (computing);virtual machine;whole earth 'lectronic link	Yong Zhao;Kun Suo;Luwei Cheng;Jia Rao	2017		10.1145/3135974.3135975	virtualization;parallel computing;hypervisor;scheduler activations;virtual machine;scheduling (computing);thread (computing);computer science;load balancing (computing);preemption	OS	-12.79499018636406	49.312083855647025	102739
211350734572f1413707c85afc2b3f6451878d2a	a case for heterogeneous flash in the datacenter	ash measurement nonvolatile memory flash memories throughput servers switches;nand circuits flash memories;datacenter storage workload heterogeneous nand flash data access pattern;nand circuits;flash memories	We explore the idea of heterogeneous NAND flash which possesses pages and blocks of multiple sizes. This heterogeneity can then be exploited to accommodate the diversity in data access patterns found in most datacenter storage workloads. We identify various tradeoffs offered by such pages and blocks. By characterizing seven real-world I/O traces, we identify metrics that have a bearing on the efficacy as well as design of such a heterogeneous flash. We use the observations of our workload characterization to discuss the pros and cons of different implementation styles, management schemes and implications for datacenters.	adobe flash;data access;data center;flash memory;input/output;tracing (software)	Di Wang;Anand Sivasubramaniam;Bhuvan Urgaonkar	2013	2013 IEEE 33rd International Conference on Distributed Computing Systems Workshops	10.1109/ICDCSW.2013.65	flash file system;parallel computing;real-time computing;computer hardware;computer science	DB	-14.915643174954653	53.10569225747772	102785
6580b68099bec3377d4520917573f1943a4f5821	cost, performance, and size tradeoffs for different levels in a memory hierarchy*	memory system;dissimilar technology;different level;different new technology;ccd technology;memory hierarchy;level hierarchy;primary memory;electromechanically addressable device;computer system designer;different levels;charge-coupled device;size tradeoffs;mos memory;different memory hierarchy;cmos1and integrated injection logic;adjacent level;ccd speed;domain tip propagation;primary memory requirement;access gap;addressable device;figure of merit;petri nets;system design;high level languages;charged couple device	This paper evaluates the effect of cost and performance tradeoffs on memory system hierarchies achieved by varying the total amount of memory at any two adjacent levels. The hierarchy is analyzed in a multiprogramming mode by using a two server cyclic queuing model. As an example, a two level hierarchy of Bipolar, MOS and a three level hierarchy of Bipolar, MOS, and CCD for the primary memory are compared. A figure of merit that is a function of the number of instructions executed by a given processor is used to evaluate the different memory hierarchies. It is shown that up to 3:1 advantage in performance can be achieved by using a three level rather than the two level hierarchy at the same total cost. The effect on the performance of the memory hierarchy due to the change in the degree of multiprogramming, the speed and cost of CCD technology used, the speed of the CPU used and the amount of CCD and MOS memory used are then evaluated. The performance of two and three level hierarchies is also analyzed as a function of the primary memory requirements versus the CCD speed.	central processing unit;charge-coupled device;computer data storage;computer multitasking;memory hierarchy;queueing theory;requirement;server (computing)	Satish L. Rege	1976	Computer	10.1145/800110.803551	embedded system;semiconductor memory;computer science;cmos	Metrics	-11.715117059196562	48.23253031621171	102797
225c22444b20a82481d699fcb4810a37dfc0b2bf	the effects of sequence and delay on crowd work	continuity;interruptions;efficiency;workflows;human computation;crowdsourcing	A common approach in crowdsourcing is to break large tasks into small microtasks so that they can be parallelized across many crowd workers and so that redundant work can be more easily compared for quality control. In practice, this can result in the microtasks being presented out of their natural order and often introduces delays between individual microtasks. In this paper, we demonstrate in a study of 338 crowd workers that non-sequential microtasks and the introduction of delays significantly decreases worker performance. We show that interruptions where a large delay occurs between two related tasks can cause up to a 102% slowdown in completion time, and interruptions where workers are asked to perform different tasks in sequence can slow down completion time by 57%. We conclude with a set of design guidelines to improve both worker performance and realized pay, and instructions for implementing these changes in existing interfaces for crowd work.	crowdsourcing;parallel computing	Walter S. Lasecki;Jeffrey M. Rzeszotarski;Adam Marcus;Jeffrey P. Bigham	2015		10.1145/2702123.2702594	workflow;real-time computing;simulation;computer science;efficiency;management;crowdsourcing	HCI	-11.778486599341855	57.750568914123534	102886
077de7f477be9885498816d2559ee92b18e30118	tuning the ext4 filesystem performance for android-based smartphones		The storage performance plays an important role in today’s smartphones. However, most file systems have been optimized for hard disk drives and general filesystem workloads. This paper aims at improving the performance of the Ext4 filesystem, a de facto filesystem in Android-based smartphones, by taking into account the filesystem workloads in the Android platform and the characteristics of the underlying NAND flash-based storage device. We have considered five tuning parameters of the Ext4 filesystem. Our evaluation on a real Android-based smartphone shows that the new Ext4 filesystem, where all the tuning parameters are applied, improves the Postmark performance by up to 13% compared to the original Ext4 filesystem.	adaptive partition scheduler;android;flash memory;hard disk drive;input/output;overhead (computing);smartphone;stat (system call)	Hyeong-Jun Kim;Jin-Soo Kim	2011		10.1007/978-3-642-27552-4_98	embedded system;computer hardware;engineering;operating system	OS	-14.65794623518455	52.901743127733575	102908
0f3b0df0a62018f647cb9bea6416bcf3c324a066	group-based optimizaton for parallel job scheduling with scojo-pect-o	optimisation;scheduling optimisation parallel machines;parallel job scheduling;time sharing computer systems;application software;high performance computing;processor scheduling;time sharing;coarse grain time sharing scheduler;runtime;discrete event simulation parallel job scheduling dynamic job arrival optimization;time factors;coarse grain time sharing scheduler group based optimization parallel job scheduling parallel machines jobs dynamic arrival;optimal scheduling;scheduling;schedules;parallel machines;genetic algorithms;optimization;jobs dynamic arrival;computer science;point of view;coarse grained;group based optimization;job scheduling;dynamic job arrival;dynamic scheduling;optimal scheduling delay runtime dynamic scheduling computer science processor scheduling parallel machines time sharing computer systems high performance computing application software;discrete event simulation	Typical job scheduling for parallel machines is done on a one-by-one basis because of the dynamic arrival of jobs and the different priorities of the jobs. In the general case, this does not provide the optimum solution from a global point of view. Thus, we propose an approach which selects a group of jobs from the waiting queue and hierarchically optimizes a longer-term scheduling plan. This becomes feasible because the approach is embedded in our Scojo-PECT coarse-grain time sharing scheduler which separates jobs by priority. We demonstrate that we can obtain up to a 30% improvement in relative response times.	embedded system;job scheduler;job shop scheduling;job stream;scheduling (computing);time-sharing	Angela C. Sodan;Arun Kanavallil;Bryan Esbaugh	2008	2008 22nd International Symposium on High Performance Computing Systems and Applications	10.1109/HPCS.2008.19	application software;supercomputer;parallel computing;real-time computing;genetic algorithm;dynamic priority scheduling;schedule;computer science;rate-monotonic scheduling;discrete event simulation;job scheduler;operating system;distributed computing;job queue;scheduling;time-sharing	HPC	-13.37538626808719	60.223393998628595	102910
debcca1eda23320339920eaa75cf4fea5bace458	photon: remote memory access middleware for high-performance runtime systems	libraries;photonics;memory management;completion notification rdma network middleware;semantics;runtime;completion notification;network middleware;synchronization;rdma;middleware;storage management middleware parallel processing software libraries;remote direct memory access remote memory access middleware high performance runtime systems photon rdma middleware library remote memory access semantics network interconnect technologies network abstraction communication overheads message handling overheads high performance applications object manipulation global address space one sided communication models rendezvous communication models communication pattern put with completion pwc completion notification path active message driven computation message latency throughput metrics;photonics runtime libraries synchronization semantics memory management middleware	We introduce the Photon RDMA middleware library that enables consistent remote memory access semantics over a number of network interconnect technologies. A primary goal of Photon is to expose a lightweight and flexible network abstraction that minimizes communication and message handling overheads for high-performance applications and runtime systems, in particular those that require the manipulation of objects within a global address space. Both one-sided and rendezvous communication models are supported and asynchronous network progress is exposed at a fine granularity. Photon implements a novel communication pattern called put-with-completion (PWC) that optimizes a completion notification path with variable size data for realizing active message-driven computation. The results of our performance evaluation show that our PWC model is comparable, and often improves upon, existing one-sided RDMA libraries in message latency and throughput metrics.	active message;computation;data structure alignment;electrical connection;event (computing);image scaling;library (computing);middleware;partitioned global address space;performance evaluation;remote direct memory access;runtime system;throughput	Ezra Kissel;D. Martin Swany	2016	2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2016.120	synchronization;parallel computing;real-time computing;photonics;remote direct memory access;computer science;operating system;middleware;distributed computing;semantics;programming language;computer network;memory management	Arch	-10.816958221632746	46.580276408736395	102966
035082215b22d56d154eb1d34b83ff9f7d60c177	performance of the vax-11/780 translation buffer: simulation and measurement	time sharing;translation look aside buffer;trace driven simulation	A virtual-address translation buffer (TB) is a hardware cache of recently used virtual-to-physical address mappings. The authors present the results of a set of measurements and simulations of translation buffer performance in the VAX-11/780. Two different hardware monitors were attached to VAX-11/780 computers, and translation buffer behavior was measured. Measurements were made under normal time-sharing use and while running reproducible synthetic time-sharing work loads. Reported measurements include the miss ratios of data and instruction references, the rate of TB invalidations due to context switches, and the amount of time taken to service TB misses. Additional hardware measurements were made with half the TB disabled. Trace-driven simulations of several programs were also run; the traces captured system activity as well as user-mode execution. Several variants of the 11/780 TB structure were simulated.	cpu cache;cache (computing);computer;network switch;physical address;protection ring;simulation;synthetic intelligence;terabyte;time-sharing;tracing (software);user space;vax-11	Douglas W. Clark;Joel S. Emer	1985	ACM Trans. Comput. Syst.	10.1145/214451.214455	parallel computing;real-time computing;computer hardware;computer science;operating system;distributed computing;time-sharing	Arch	-12.667695091238796	50.3580969849858	103008
c9bbe5d24e7777af58a6b9e7b5a136384b1628ec	offline selective data deduplication for primary storage systems		Data deduplication is a technology that eliminates redundant data to save storage space. Most previous studies on data deduplication target backup storage, where the deduplication ratio and throughput are important. However, data deduplication on primary storage has recently been receiving attention; in this case, I/O latency should be considered equally with the deduplication ratio. Unfortunately, data deduplication causes high sequential-read-latency problems. When a file is created, the file system allocates physically contiguous blocks to support low sequential-read latency. However, the data deduplication process rearranges the block mapping information to eliminate duplicate blocks. Because of this rearrangement, the physical sequentiality of blocks in a file is broken. This makes a sequentialread request slower because it operates like a random-read operation. In this paper, we propose a selective data deduplication scheme for primary storage systems. A selective scheme can achieve a high deduplication ratio and a low I/O latency by applying different data-chunking methods to the files, according to their file access characteristics. In the proposed system, file accesses are characterized by recent access time and the access frequency of each file. No chunking is applied to update-intensive files since they are meaningless in terms of data deduplication. For sequential-readintensive files, we apply big chunking to preserve their sequentiality on the media. For random-read-intensive files, small chunking is used to increase the deduplication ratio. Experimental evaluation showed that the proposed method achieves a maximum of 86% of an ideal deduplication ratio and 97% of the sequential-read performance of a native file system. key words: data deduplication, selective deduplication, rank based deduplication	access time;backup;block (data storage);computer data storage;data deduplication;input/output;online and offline;random access;sequential access;shallow parsing;throughput	Sejin Park;Chanik Park	2016	IEICE Transactions		data deduplication;computer science	OS	-13.207333836343162	54.79762170829296	103054
1f7a948fde19df1a7297839a44f03d7466eac147	a multicore vacation scheme for thermal-aware packet processing	network application;power saving;packet processing;thermal loading;heating;general purpose multicore processor multicore vacation scheme thermal aware packet processing processor power density power control thermal control on off execution pattern temperature consumption power consumption power aware thermal management algorithm multicore processors vacation scheme os idle states c state cpu thermal constraint heterogeneous load distribution temperature constraint vacation algorithm network application;multicore processing power demand servers heating thermal management thermal loading throughput;servers;vacation and runtime adaptation network application packet processing multi core processors power saving thermal aware technique;multicore processing;vacation and runtime adaptation;power aware computing microprocessor chips multiprocessing systems;power demand;thermal management;multi core processors;throughput;thermal aware technique	As processor power density increases, thermal and power control becomes critical for application processing. In this paper, we consider network applications which feature ON/OFF execution pattern, that causes frequent temperature and power consumption changes in the processor. A novel power aware thermal management algorithm is designed to achieve power saving in multicore processors by employing a vacation scheme. We implement the scheme through the idle states (C-state) provided by the OS in the CPU and show their effectiveness both through analysis and experimental data. Then, we apply our scheme with the thermal constraint and propose a heterogeneous load distribution, which creates more opportunities for power saving. Besides maintaining processor temperature below the temperature constraint, our technique achieves higher sustainable load and better power saving with minimum latency increase compared to existing thermal management techniques. To the best of our knowledge, this is the first work to discuss and develop vacation algorithm considering power, temperature and latency for network application on a general purpose multicore processor.	algorithm;central processing unit;execution pattern;idle (cpu);load balancing (computing);multi-core processor;network packet;operating system;server (computing);thermal management of high-power leds	Chih-Hsun Chou;Laxmi N. Bhuyan	2015	2015 33rd IEEE International Conference on Computer Design (ICCD)	10.1109/ICCD.2015.7357166	multi-core processor;embedded system;parallel computing;real-time computing;computer science;operating system	HPC	-5.496368892837602	55.667497320858025	103061
d0e9189645316a3d6a708f6c36b713893bdd08ad	application-centric ssd cache allocation for hadoop applications		Flash-based Solid State Drive (SSD) is widely used in the virtualization environment, usually as the cache of the hard disk drive-based Virtual Machine (VM) storage, to improve the IO performance. Existing SSD caching schemes are mainly driven by VM-centric metrics. They treat the VMs as independent units and focus on critical low-level performance metrics of individual VMs, such as the working set, the IO latency, or the throughput. However, for elastic Hadoop applications consisting of multiple VMs, the workload is rapidly changing, and the importance of differnet VMs may be different even if they have the same low-level IO pattern. In this situation, the VM-centric SSD caching schemes may not lead to the best performance, i.e., the shortest job completion time. Considering the importance of VMs and relationships among VMs inside the application may potentially better improve the performance, which we regard as the application-centric metrics. We propose the Application-Centric SSD caching for Hadoop applications (ACSSD), which reduces the job completion time from the application level. AC-SSD uses the genetic algorithm based approach to calculate the nearly optimal weights of virtual machines for allocating SSD cache space and controlling the I/O Operations Per Second (IOPS) based on the importance of the VMs. Moreover, AC-SSD introduces the closed-loop adaptation to face the rapidly changing workload. The evaluation shows that AC-SSD reduces the job completion time by up to 39% for IO sensitive workloads, and up to 29% for rapidly changing workloads.	apache hadoop;disk storage;elegant degradation;genetic algorithm;hard disk drive;hardware virtualization;high- and low-level;input/output;openvms;socket.io;solid-state drive;throughput;virtual machine;working set;z/vm	Zhen Tang;Wei Wang;Yu Huang;Heng Wu;Jun Wei;Tao Huang	2017		10.1145/3131704.3131708	cache;workload;parallel computing;solid-state drive;latency (engineering);virtualization;working set;virtual machine;real-time computing;iops;computer science	HPC	-14.939875493172481	55.83804504005416	103126
15dae0e41bd1a110712325b29a691fb9a9b8fd50	a simulator for data-intensive job scheduling	data intensive computing scheduling simulation;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems	Despite the fact that size-based schedulers can give excellent results in terms of both average response times and fairness, data-intensive computing execution engines generally do not employ size-based schedulers, mainly because of the fact that job size is not known a priori. In this work, we perform a simulation-based analysis of the performance of size-based schedulers when they are employed with the workload of typical data-intensive schedules and with approximated size estimations. We show results that are very promising: even when size estimation is very imprecise, response times of size-based schedulers can be definitely smaller than those of simple scheduling techniques such as processor sharing or FIFO.	apache hadoop;approximation algorithm;cp/m;data-intensive computing;fifo (computing and electronics);fairness measure;job scheduler;job shop scheduling;locality of reference;makespan;mapreduce;operations research;optimizing compiler;scheduling (computing);shortest remaining time;simulation	Matteo Dell'Amico	2013	CoRR		generalized processor sharing;parallel computing;real-time computing;simulation;computer science;operating system;distributed computing;round-robin scheduling	HPC	-16.16124129252875	60.35475186770103	103304
43584c87b24784a4247a4defc50df5e7a6937e89	effective compaction for kernel memory allocator using workload distribution		Low latency is one of the most important requirements for smart devices. Even though they have a small amount of memory, end users often want to run multiple applications concurrently without delays. Memory fragmentation is a critical obstacle to this aspect. To recover from it, the operating system uses compaction to create contiguous space from scattered free pages. Memory compaction tends to secure as many contiguous memory pages as it can, which may affect the user experience adversely at application launches. In this paper, a new page compaction method is proposed to improve the user experience. This method employs an efficient mechanism to find candidate pages to move. It also uses a bitmap to represent the status of the physical pages and find compactable pages effectively. The experiment performed on the proposed method demonstrates that the interruptions for compaction occur more frequently, but with a significantly lower workload than conventional compaction. According to the experiment, the success rate after compaction increased by about 22%, and the elapsed time to scan free pages decreased dramatically compared to conventional compaction. Moreover, the proposed method reduced the worst-case time for a new process creation by about 67%.	benchmark (computing);best, worst and average case;bitmap;booting;data compaction;fork (system call);fragmentation (computing);kernel (operating system);operating system;overhead (computing);requirement;response time (technology);skip list;slab allocation;smart device;system call;usability;user experience	Jinho Lim;Hwansoo Han	2018	IEEE Transactions on Consumer Electronics	10.1109/TCE.2018.2843279	computer science;end user;artificial intelligence;memory management;computer vision;workload;bitmap;latency (engineering);real-time computing;compaction;allocator;fragmentation (computing)	Arch	-12.873744426002384	54.013421292086626	103313
b25554228df0c9abfce3c906217e666498f08e1f	stabilizing pre-run-time schedules with the help of grace time	error recovery;fault tolerant;real time;real time processing;satisfiability;hard real time system;fault tolerance;pre run time scheduling stability;grace time;error processing;backward error;article;real time systems;time constraint	This paper discusses the stability of a feasible pre-run-time schedule for a transient overload introduced by processes re-execution during an error recovery action. It shows that the stability of a schedule strictly tuned to meet hard deadlines is very small, invalidating thus backward error recovery. However, the stability of the schedule always increases when a real-time process is considered as having a nominal and a hard deadline separated by a non-zero grace time. This is true for sets of processes having arbitrary precedence and exclusion constraints, and executed on a single or multiprocessor based architecture. Grace time is not just the key element for the realistic estimation of the timing constraints of real-time error processing techniques. It also allows backward error recovery to be included in very efficient pre-run-time scheduled systems when the conditions stated in this paper are satisfied. This is a very important conclusion, as it shows that fault-tolerant hard real-time systems do not have to be extremely expensive and complex.	algorithm;fault tolerance;multiprocessing;real-time clock;real-time computing;real-time transcription;scheduling (computing)	Antonio Pessoa Magalhães;João Gabriel Silva	1999	Real-Time Systems	10.1023/A:1008089513108	fault tolerance;real-time computing;computer science;algorithm	Embedded	-10.113493736318436	59.907921461492265	103333
150f9cd27b1e570dc75afc70b3653136955fe282	bottom-up performance analysis considering time slice based software scheduling at system level	bottom up;real time;internal control;embedded system;resource use;scheduling;performance analysis;access method;communicating processes;inter process communication	In this paper, a novel approach for integrating time slice based resource access in global performance analysis of distributed real-time critical embedded systems is presented. The performance analysis approach itself is based on bottom-up analysis of communicating processes under consideration of synchronization by inter-process communication and complex internal control flows of the processes. This general analysis methodology is extended concerning concurrent occupation of shared resources using time slice based access methods. The defined extensions are parameterizable for describing arbitrary communication media access schedules and software schedules on shared computation resources, although the explicit focus in this paper is on software scheduling. The applicability of the analysis extensions is presented by a case study of a multimedia subsystem implemented in SystemC.	bottom-up parsing;computation;embedded system;inter-process communication;preemption (computing);profiling (computer programming);real-time clock;scheduling (computing);systemc;top-down and bottom-up design	Alexander Viehl;Michael Pressler;Oliver Bringmann	2009		10.1145/1629435.1629493	real-time computing;simulation;computer science;distributed computing	Embedded	-7.63805474723815	60.18096636806847	103517
beadf34f7ba9893ada68ce6363c270ef6a623687	an analytic response time model for single-and dual-density disk systems	disk arm contention;queueing theory;disk arm contention equipment replacement m g 1 queues queueing models response times single and dual density disks;single channel;equipment replacement;m g 1 queue;queueing model;single and dual density disks;m g 1 queues;response times;simulation model;queueing models;analytical model	The question of replacing a single-density, two-channel, two-controller disk system with a cheaper, plug-compatible, dual-density, single-channel system having the same capacity is considered. An analytical model is explored to examine the effect of such a replacement on average response time, that is, the time between issuing an I/O request and completion of the request. Queueing theory is used to obtain curves of response time versus arrival rate, and the results are compared with corresponding curves obtained by a simulation model.	family computer disk system;input/output;plug compatible;queueing theory;response time (technology);responsiveness;simulation;single density	Mark A. Franklin;Amitava Sen	1974	IEEE Transactions on Computers	10.1109/T-C.1974.223846	mean value analysis;m/m/1 queue;m/d/c queue;real-time computing;m/m/c queue;m/m/∞ queue;bulk queue;computer science;m/d/1 queue;simulation modeling;layered queueing network;distributed computing;m/g/k queue;m/g/1 queue;fork–join queue;queueing theory;kendall's notation;statistics	Metrics	-11.018558240915452	57.17799310460007	103897
1977af739e52d6ece7808a53f4a52a24336e8602	optimal 2d data partitioning for dma transfers on mpsocs	cell simulator optimal 2d data partitioning dma transfers mpsoc off chip memory access latency embedded multicore platforms multicore computation fabric data blocks direct memory access engine dma engine double buffering scheme multiple buffering scheme processor idling two dimensional data arrays dma call array elements memory structure asymmetry one dimensional horizontal memory pieces data element optimization problem mean filter application;double buffering;cell processor;arrays shape microprocessors computational modeling memory management;system on chip;double buffering data paralleization direct memory access dma cell processor;multiprocessing systems;data paralleization;system on chip file organisation multiprocessing systems;file organisation;direct memory access dma	"""Reducing the effects of off-chip memory access latency is a key factor in exploiting efficiently embedded multicore platforms. We consider architectures that admit a multi-core computation fabric, having its own fast and small memory to which the data blocks to be processed are fetched from external memory using a DMA (direct memory access) engine, employing a double- or multiple-buffering scheme to avoid processor idling. In this paper we focus on application programs that process two dimensional data arrays and we determine automatically the size and shape of the portions of the data array which are subject to a single DMA call, based on hardware and applications parameters. When the computation on different array elements are completely independent, the asymmetry of memory structure leads always to prefer one-dimensional horizontal pieces of memory, while when the computation of a data element shares some data with its neighbors, there is a pressure for more """"square"""" shapes to reduce the amount of redundant data transfers. We provide an analytic model for this optimization problem and validate our results by running a mean filter application on the CELL simulator."""	cell (microprocessor);computation;computer memory;data element;direct memory access;embedded system;glossary of computer graphics;mathematical optimization;multi-core processor;multiple buffering;optimization problem;simulation	Selma Saidi;Pranav Tendulkar;Thierry Lepley;Oded Maler	2012	2012 15th Euromicro Conference on Digital System Design	10.1109/DSD.2012.99	system on a chip;uniform memory access;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;multiple buffering;distributed memory;memory refresh;computer science;physical address;operating system;computer memory;overlay;redundant array of independent memory;conventional memory;extended memory;flat memory model;registered memory;sequential access memory;computing with memory;memory map;non-uniform memory access;memory management	EDA	-8.94861912573312	52.86398069878737	103973
9c9c3710d4964cd5b86082aeb7bbe5fc13f1107b	performance of cooperative loosely coupled microprocessor architectures in an interactive data base task	computer architecture;microcomputers;measurement;hardware;real time systems;helium;system testing;concurrent computing	Continuing technological advances in single-chip intelligence and storage cell density, and in bulk store performance provide increasing opportunities to construct multiple microprocessor systems. The objective of this experimental study was to explore the performance of selected system architectures in a manner sufficiently detailed, quantitative and realistic to 1) contribute to our understanding of the fundamental behavior of such systems and 2) permit practical designs to follow from the results.	experiment;loose coupling;microprocessor	John J. Lenahan;Fergus K. Fung	1980	IEEE Transactions on Computers	10.1109/TC.1980.1675540	embedded system;parallel computing;real-time computing;simulation;computer science;electrical engineering;operating system	Visualization	-11.95595481224236	50.090275833436955	104099
446786a84626a6f6c7d1cf96e2963e97da8963f0	establishing applicability of ssds to lhc tier-2 hardware configuration	cluster computing;performance metric;data analysis;high performance;high speed	Solid State Disk technologies are increasingly replacing high-speed hard disks as the storage technology in high-random-I/O environments. There are several potentially I/O bound services within the typical LHC Tier-2 in the back-end, with the trend towards many-core architectures continuing, worker nodes running many single-threaded jobs and storage nodes delivering many simultaneous files can both exhibit I/O limited efficiency. We estimate the effectiveness of affordable SSDs in the context of worker nodes, on a large Tier-2 production setup using both low level tools and real LHC I/O intensive data analysis jobs comparing and contrasting with high performance spinning disk based solutions. We consider the applicability of each solution in the context of its price/performance metrics, with an eye on the pragmatic issues facing Tier-2 provision and upgrades International Conference on Computing in High Energy and Nuclear Physics (CHEP) 2010 Taipei, Taiwan	hard disk drive;i/o bound;input/output;job stream;large hadron collider;manycore processor;solid-state drive;thread (computing)	Samuel C. Skipsey;Wahid Bhimji;Mike Kenyon	2010	CoRR	10.1088/1742-6596/331/5/052019	computer cluster;data analysis	HPC	-15.640469487166383	52.249090669251736	104127
21ef58b52e4d2c607c5eb030a4c9121732df0824	reduction of task migrations and preemptions in optimal real-time scheduling for multiprocessors by using dynamic t-l plane		A significant number of task preemptions and task migrations is reduced by D-TLPA compared with state-of-the-art algorithms.The proposed algorithm is totally executed on-line making it affective when dealing with systems where the condition changes dynamically at run-time, for example, when tasks are frequently added or removed.The concept of the proposed idea is simple and easy to realize in practice. The complexity bound is also presented to prove the feasibility of the proposed algorithm. A new method for optimally scheduling tasks in multiprocessors is proposed in this paper: Dynamic Time and Local Remaining Execution Time Plane Abstraction (D-TLPA). Unlike the conventional T-L plane abstraction method where the positions of T-L planes are fixed according to the fluid schedule of tasks, in this algorithm, the T-L planes can move dynamically up and down in order to reduce unnecessary occurrences of events, resulting in a reduction in the number of task preemptions and migrations. A sub-procedure called T-L Planes Movement (TLPM) is presented to move T-L planes in a proper manner so that all tasks are schedulable. To further reduce task preemptions, tasks preferentially maintain their execution status through consecutive T-L planes with the Running Task Execute First (RTEF) algorithm. Simulations were run on various random task sets, and significant improvements in the number of task preemptions and migrations were seen with this algorithm compared to other state-of-the-art algorithms. The complexity bound is also presented to prove the feasibility of the proposed algorithm.	algorithm;computer simulation;preemption (computing);real-time clock;scheduling (computing)	Ngoc-Son Pham;Youngmin Kim;Kwang-Hyun Baek;Chan-Gun Lee	2017	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2017.07.003	real-time computing;parallel computing;scheduling (computing);computer science	Embedded	-10.967137485033858	59.85607761593527	104225
14dade0b2e336a91a66cd5dd33fcc8a1c171615d	analysis and optimisation of hierarchically scheduled multiprocessor embedded systems		We present an approach to the analysis and optimisation of heterogeneous multiprocessor embedded systems. The systems are heterogeneous not only in terms of hardware components, but also in terms of communication protocols and scheduling policies. When several scheduling policies share a resource, they are organised in a hierarchy. In this paper, we first develop a holistic scheduling and schedulability analysis that determines the timing properties of a hierarchically scheduled system. Second, we address design problems that are characteristic to such hierarchically scheduled systems: assignment of scheduling policies to tasks, mapping of tasks to hardware components, and the scheduling of the activities. We also present several algorithms for solving these problems. Our heuristics are able to find schedulable implementations under limited resources, achieving an efficient utilisation of the system. The developed algorithms are evaluated using extensive experiments and a real-life example.	algorithm;embedded system;experiment;heterogeneous system architecture;heuristic (computer science);holism;mathematical optimization;multiprocessing;real life;scheduling (computing);scheduling analysis real-time systems	Traian Pop;Paul Pop;Petru Eles;Zebo Peng	2007	International Journal of Parallel Programming	10.1007/s10766-007-0059-9	fair-share scheduling;communications protocol;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;distributed computing;multiprocessor scheduling	Embedded	-7.862038144076201	60.33105363571232	104507
ec2a0ca610628ad93eef3013a150e15cb52e6b96	analytical modeling of the energy consumption for the high performance linpack	distributed processing;algorithm parameter energy consumption high performance linpack hpc system high performance computing analytical modeling hpl operating system queue manager performance analytical model energy analytical model gflops per watt prediction;power aware computing distributed processing energy consumption operating systems computers;analytical models energy consumption program processors energy measurement niobium current measurement predictive models;power aware computing;energy consumption;energy models;hpl;hpl energy aware computing energy models high performance linpack;energy aware computing;high performance linpack;operating systems computers	Comparable to time performance models, it is now possible to estimate performance based upon energy consumption for HPC systems. The predictive ability of the analytical modeling is an interesting feature that motivates us to approach this methodology for the case of energy consumption. In this paper, we present an analytical model for predicting the energy consumption for the High Performance Linpack (HPL). The derived model can be used to know in advance the energy consumed by the HPL over a target architecture, and can be integrated into the schedulers of operating systems or queue managers. We established an experimental setup using a standard metered PDU that allowed us to measure the energy consumption for the HPL benchmark on our cluster. With the monitoring system in place, we can obtain the architectural and algorithmic parameters associated for both performance and energy analytical models. Also this has made possible watts and gflops-per-watt prediction when we execute Linpack executions with concrete algorithm parameters in our cluster.	algorithm;benchmark (computing);flops;library (computing);linear algebra;lunpack;multi-core processor;operating system;parallel algorithm;watts humphrey	Alberto Cabrera Pérez;Francisco Almeida;Vicente Blanco Pérez;Domingo Giménez	2013	2013 21st Euromicro International Conference on Parallel, Distributed, and Network-Based Processing	10.1109/PDP.2013.56	embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing	HPC	-5.718406651490632	48.626470333688204	104537
b899f6303b855f6e433affab528db6f2eef1f808	power-efficient clustering via incomplete bypassing	energy efficiency;microarchitecture delay voltage wires energy consumption registers computer architecture permission power engineering computing power engineering and energy;microarchitecture;voltage scaling clustering incomplete bypass power;clocks;power efficient clustering;power efficiency;wires;incomplete bypass based clustered microarchitecture;incomplete bypass;layout;bypass wires;lower average power consumption power efficient clustering energy efficiency traditional clustered microarchitectures register file partitioning incomplete bypass based clustered microarchitecture associated functional units bypass wires execute stage delay reduction voltage scaling;computer architecture;integrated circuit design;power aware computing;traditional clustered microarchitectures;workstation clusters computer architecture integrated circuit design microprocessor chips power aware computing;registers;clustering;execute stage delay reduction;associated functional units;register file;workstation clusters;power consumption;functional unit;voltage scaling;register file partitioning;power;lower average power consumption;pipeline processing;microprocessor chips	"""Researchers have proposed clustered microarchitectures for performance and energy effciency. Typically, clustered microarchitectures offer fast, local bypassing between instructions within clusters but global bypasses are slower. Traditional clustered microarchitectures (TCM) are implemented by partitioning the register file and associated functional  units to clusters. This paper demonstrates an alternate implementation - Incomplete bypass-based clustered microarchitecture (IBCM). IBCM reduces the length of bypass wires by 42.4% resulting in an 8.9% reduction of """"Execute"""" stage delay. This delay reduction in the critical EX stage enables voltage scaling that results in significantly lower average power consumption (between 11.7% and 19.5% lower) while achieving identical performance."""	cluster analysis;computer cluster;dynamic voltage scaling;image scaling;microarchitecture;nehalem (microarchitecture);register file	Eric P. Villasenor;Daeho Seo;Mithuna Thottethodi	2008	Proceeding of the 13th international symposium on Low power electronics and design (ISLPED '08)	10.1145/1393921.1394019	layout;embedded system;computer architecture;parallel computing;real-time computing;electrical efficiency;microarchitecture;computer science;operating system;power;efficient energy use;processor register;cluster analysis;register file;integrated circuit design	Arch	-6.096405340966592	53.93070168587346	104618
2ef0b47398df2a07c680c9125e95f19b7f4a0add	merasa: multicore execution of hard real-time applications supporting analyzability	software;hard real time embedded systems;embedded multicore processors;industrial case study;merasa project;real time;hard real time embedded systems merasa project hardware design embedded multicore processors;embedded system;multiprocessing systems embedded systems;embedded systems;worst case execution time;computational modeling;multicore;industrial case study hardware software embedded systems multicore real time wcet analysis;estimation;multicore processing;hardware design;multicore processors;multiprocessing systems;wcet analysis;multicore processing estimation real time systems instruction sets hardware computational modeling;hard real time;instruction sets;hardware;real time systems	The Merasa project aims to achieve a breakthrough in hardware design, hard real-time support in system software, and worst-case execution time analysis tools for embedded multicore processors. The project focuses on developing multicore processor designs for hard real-time embedded systems and techniques to guarantee the analyzability and timing predictability of every feature provided by the processor.	best, worst and average case;central processing unit;embedded system;multi-core processor;real-time clock;real-time computing;real-time transcription;run time (program lifecycle phase);worst-case execution time	Theo Ungerer;Francisco J. Cazorla;Pascal Sainrat;Guillem Bernat;Zlatko Petrov;Christine Rochange;Eduardo Quiñones;Mike Gerdes;Marco Paolieri;Julian Wolf;Hugues Cassé;Sascha Uhrig;Irakli Guliashvili;Michael Houston;Florian Kluge;Stefan Metzlaff;Jörg Mische	2010	IEEE Micro	10.1109/MM.2010.78	multi-core processor;computer architecture;parallel computing;real-time computing;computer science;operating system	Embedded	-8.026385680163244	59.023906342871776	104844
f796c8eea567a59a371867d37d67ee392333341f	compiler support for data forwarding in scalable shared-memory multiprocessors	microarchitecture;cache hierarchy performance;microarchitectural support;parallelizing compiler;data forwarding;delay context microarchitecture radio access networks upper bound;upper bound;shared memory systems;simulations compiler support data forwarding scalable shared memory multiprocessors cache hierarchies communication induced misses parallelizing compiler microarchitectural support loop iterations;cache coherence protocols;shared memory multiprocessors;parallelising compilers;digital simulation shared memory systems parallelising compilers;communication induced misses;loop iterations;compiler algorithms to hide memory latency;context;compiler support;digital simulation;radio access networks;cache hierarchies;scalable shared memory multiprocessors	As the difference in speed between processor and memory system continues to increase, it is becoming crucial to develop and refine techniques that enhance the effectiveness of cache hierarchies. One promising technique in the context of scalable shared-memory multiprocessors is data forwarding. Forwarding hides the latency of communication-induced misses by having producer processors send data to the caches of potential consumer processors in advance. Forwarding can hide the latency effectively, has low instruction overhead, and uses few machine resources. This paper presents a complete implementation of a data forwarding pass in an industrial-strength parallelizing compiler. Complete Fortran applications are analyzed for dependences and, based on the analysis, automatically annotated with forwarding directives. We propose a forwarding framework that includes 4 new instructions: write-forward, write-broadcast, write-update, and write-through. New microarchitectural support is proposed. In our analysis, we assume that the assignment of loop iterations to processors is known. We perform simulations of multiprocessors with different cache, memory, machine sharing, and process migration parameters. We conclude that data forwarding delivers large speedups (six 32-processor applications ran an average of 40% faster), gets close to the upper bound in performance, and needs compiler support of only medium complexity.	compiler;shared memory	David Koufaty;Josep Torrellas	1999		10.1109/ICPP.1999.797403	computer architecture;parallel computing;real-time computing;microarchitecture;computer science;operating system;upper and lower bounds	Arch	-9.266834802310145	49.24076615861987	104988
38bcebc819aa2d8de85d474b687f3f96e0f00b21	a precise bandwidth control arbitration algorithm for hard real-time soc buses	hard real-time soc buses;well-known existing arbitration algorithm;shared bus system;bandwidth requirement;different application;hard real-time guarantee;bandwidth control arbitration algorithm;precise bandwidth control arbitration;different ip core;system-on-chip;system buses;arbiter;bus access;rb_lottery;soc bus;innovative arbitration algorithm;arbitration algorithm;system on chip	On an SoC bus, contentions occur while different IP cores request the bus access at the same time. Hence an arbiter is mandatory to deal with the contention issue on a shared bus system. In different applications, IPs may have real-time and/or bandwidth requirements. It is very difficult to design an arbitration algorithm to simultaneously meet these two requirements. In this paper, we propose an innovative arbitration algorithm, RB_lottery, to meet both of the requirements. It can provide not only the hard real-time guarantee but also the precise bandwidth controllability. The experimental results show that RBJottery outperforms several well-known existing arbitration algorithms.	algorithm;arbiter (electronics);bandwidth management;real-time clock;real-time computing;real-time transcription;requirement	Bu-Ching Lin;Geeng-Wei Lee;Juinn-Dar Huang;Jing-Yang Jou	2007	2007 Asia and South Pacific Design Automation Conference		system on a chip;embedded system;parallel computing;real-time computing;computer science;engineering;local bus;operating system;system bus;computer network	EDA	-8.087481663472655	58.736792697650806	105158
ae6dc96dbe93fb8729e064f99021d983c4a7c4b0	power-performance co-optimization of throughput core architecture using resistive memory	optimising compilers;cache storage;multi threading;performance evaluation;power aware computing;shared memory systems;sram chips cache storage dram chips graphics processing units multi threading optimising compilers parallel memories performance evaluation power aware computing shared memory systems;performance improvement power performance cooptimization throughput core architecture resistive memory parallel computing throughput computers myriad memory accesses caches off chip dram on chip scratchpad memory throughput architecture spin transfer torque ram write power overhead write latencies gpu shared memory register file accesses register file organization differential memory update mechanism register write mechanism write back buffer multithreaded gpu register write accesses hybrid shared memory design stt mram sram leakage dynamic power savings gpgpu graphics workloads dynamic power performance degradation;graphics processing units;registers graphics processing units random access memory magnetic tunneling arrays system on chip;parallel memories;dram chips;sram chips	Massively parallel computing on throughput computers such as GPUs requires myriad memory accesses to register files, on-chip scratchpad, caches, and off-chip DRAM. Unlike CPUs, these processors have a large register file and on-chip scratchpad memory, which consume a significant portion of compute core power (35%-45%). In this paper, we introduce novel throughput architecture by integrating resistive memory (Spin Transfer Torque RAM) inside the compute core, which reduces leakage significantly, but introduces write power overhead and longer write latencies in GPU shared memory and register file accesses. We enhance the compute core by introducing register file organization with differential memory update mechanism to remove update redundancy during write operations. Furthermore, using merged register-write-mechanism and write-back buffer, we coalesce multithreaded GPU register write accesses to save write energy. In addition, we introduce hybrid shared memory design using SRAM and STT-MRAM that provides significant leakage/dynamic power savings without affecting performance. On average, across 23 GPGPU/graphics workloads, our schemes save 46% dynamic power due to register access (83% leakage power saving) with negligible performance degradation. On average, hybrid shared memory provides 10% reduction in dynamic power with maximum 1.6× performance improvement for the current workloads at no additional area overhead.	cache (computing);central processing unit;computer;dynamic random-access memory;elegant degradation;general-purpose computing on graphics processing units;graphics processing unit;intel core (microarchitecture);magnetoresistive random-access memory;mathematical optimization;multiple buffering;null (sql);overhead (computing);parallel computing;register file;scratchpad memory;shared memory;spectral leakage;static random-access memory;thread (computing);throughput	Nilanjan Goswami;Bingyi Cao;Tao Li	2013	2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2013.6522331	cuda pinned memory;uniform memory access;distributed shared memory;shared memory;interleaved memory;computer architecture;semiconductor memory;parallel computing;dynamic random-access memory;multithreading;sense amplifier;static random-access memory;computer hardware;computer science;operating system;computer memory;non-volatile random-access memory;processor register;registered memory;cache-only memory architecture;memory address register;memory management	Arch	-9.19805323976519	53.38827680112671	105451
a90d07de5eedd02a140b73486858cf42bb70b1ce	a compilation technique for varying communication cost numa architectures	distributed memory;distributed memory architecture;scheduling algorithm;communication cost	In an earlier work, a Threshold Scheduling Algorithm was proposed to schedule the functional parallelism in a program on distributed memory systems. In this work, we address the issue of regeneration of the schedule for a set of distributed memory architectures with different communication costs. A new concept of dominant edges of a schedule is introduced to denote those edges which dictate schedule regeneration due to the changes in their communication costs. It is shown that under certain conditions, schedule on the whole or at least part of the graph can be reused for a different architecture reducing the cost of program re-partitioning and re-scheduling. The usefulness of this method is demonstrated by incorporating it in the scheduler of the compiler backend for targeting Sisal (Streams and Iterations in a Single Assignment Language) on a family of Intel i860 architectures: Gamma, Delta and Paragon which vary in their communication costs. It is shown that almost 30 to 65 % of the schedule can be reused, thereby, avoiding program re-partitioning to a large degree.		Santosh Pande;Kleanthis Psarris	1994		10.1007/3-540-58184-7_89	uniform memory access;distributed shared memory;shared memory;computer architecture;parallel computing;real-time computing;distributed memory;computer science;cache-only memory architecture;non-uniform memory access	EDA	-6.819609609227844	48.583676055740476	105670
e29b7326239bcc8a1988320d7905e2df5fc8bd6d	system-level versus user-defined checkpointing	checkpointing programming profession operating systems program processors fault tolerance runtime library fault tolerant systems communication channels;software fault tolerance;system recovery;operating system;parallel machines system recovery software fault tolerance;transient fault;rollback recovery;parallel machines;commercial parallel machine system level checkpointing user defined checkpointing rollback recovery transient fault tolerance preventive shutdowns application programmer operating system level experimental study	Checkpointing and rollback recovery is a very effective technique to tolerate transient faults and preventive shutdowns. In the past, most of the checkpointing schemes published in the literature were supposed to be transparent to the application programmer and implemented at the operating-system level. In the recent years, there has been some work on higher-level forms of checkpointing. In this second approach, the user is responsible for the checkpoint placement and is required to specify the checkpoint contents. In this paper, we compare the two approaches: systemlevel and user-defined checkpointing. We discuss the pros and cons of both approaches and we present an experimental study that was conducted on a commercial parallel machine.	application checkpointing;core dump;debugging;disaster recovery plan;experiment;external data representation;fault tolerance;high- and low-level;input/output;mathematical optimization;mobile data terminal;operating system;overhead (computing);paging;parallel computing;programmer;software bug;stable storage;state (computer science);system call;transaction processing system	Luís Moura Silva;João Gabriel Silva	1998		10.1109/RELDIS.1998.740476	embedded system;parallel computing;real-time computing;computer science;operating system;software fault tolerance	HPC	-16.830430641553292	49.465308832211186	105675
05c5e1a8dfe9815289c5bbad3911135fe7ccc14e	prospects of collaboration between compute providers by means of job interchange	first come first serve;parallel computer;list scheduling	This paper empirically explores the advantages of the collaboration between different parallel compute sites in a decentralized grid scenario. To this end, we assume independent users that submit their jobs to their local site installation. The sites are allowed to decline the local execution of jobs by offering them to a central job pool. In our analysis we evaluate the performance of three job sharing algorithms that are based on the commonly used algorithms First-Come-First-Serve, EASY Backfilling, and List-Scheduling. The simulation results are obtained using real workload traces and compared to single site results. We show that simple job pooling is beneficial for all sites even if the local scheduling systems remain unchanged. Further, we show that it is possible to achieve shorter response times for jobs compared to the best single-site scheduling results.	computation;computational intelligence;german research centre for artificial intelligence;goodyear mpp;greedy algorithm;job scheduler;job stream;list scheduling;response time (technology);schedule (project management);scheduling (computing);simulation;tracing (software)	Christian Grimme;Joachim Lepping;Alexander Papaspyrou	2007		10.1007/978-3-540-78699-3_8	parallel computing;real-time computing;computer science;job scheduler;operating system;distributed computing	HPC	-18.54631120641747	59.82870362735104	105787
4084513e75673cd8a838c1423958bfe656f2b999	the v-way cache: demand-based associativity via global replacement	microprocessors;cache storage;optimized production technology;ipc improvement;intelligent design;memory management;history;delay optimized production technology memory management history costs engineering management microprocessors hardware upper bound energy consumption;v way cache;processor speed;set associative cache;upper bound;memory access;memory architecture cache storage digital storage;miss rate reduction;energy consumption;memory architecture;engineering management;variable way;spec cpu2000 suite;tag store entry;digital storage;ipc improvement v way cache demand based associativity global replacement processor speed memory latency intelligent design secondary cache management set associative cache memory access tag store entry constant hit latency variable way miss rate reduction spec cpu2000 suite;secondary cache management;demand based associativity;global replacement;constant hit latency;memory latency;uniform distribution;hardware	As processor speeds increase and memory latency becomes more critical, intelligent design and management of secondary caches becomes increasingly important. The efficiency of current set-associative caches is reduced because programs exhibit a non-uniform distribution of memory accesses across different cache sets. We propose a technique to vary the associativity of a cache on a per-set basis in response to the demands of the program. By increasing the number of tag-store entries relative to the number of data lines, we achieve the performance benefit of global replacement while maintaining the constant hit latency of a set-associative cache. The proposed variable-way, or V-Way, set-associative cache achieves an average miss rate reduction of 13% on sixteen benchmarks from the SPEC CPU2000 suite. This translates into an average IPC improvement of 8%.	bipartite dimension;cas latency;cpu cache;cache (computing);canonical account;operator associativity;power management	Moinuddin K. Qureshi;David Thompson;Yale N. Patt	2005	32nd International Symposium on Computer Architecture (ISCA'05)	10.1109/ISCA.2005.52	bus sniffing;least frequently used;pipeline burst cache;computer architecture;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cas latency;computer hardware;cache;computer science;write-once;cache invalidation;operating system;clock rate;intelligent design;smart cache;uniform distribution;upper and lower bounds;cache algorithms;cache pollution;memory management	Arch	-7.745326948187508	53.37643011607924	105864
b71253e3d5971f0be890f1090d571de5486c9c21	extended analysis with reduced pessimism for systems with limited parallelism	coprocessors;interference coprocessors parallel processing reconfigurable logic pattern analysis equations computer aided software engineering computer science delay system testing;temporal pattern;symmetric multiprocessor systems limited parallelism remote coprocessors worst case response time analysis temporal patterns;worst case response time;coprocessors parallel processing;parallel processing	Under limited parallelism, processes competing for a single processor may issue at any time operations on remote co-processors, during which the processor is not idled but granted to other ready processes instead. We reduce the pessimism in existing worst-case response time (WCRT) analysis for such systems by examining temporal patterns of local/remote execution. We extend to multi-CPU variants of the model and offer a WCRT-based feasibility test for symmetric multiprocessor (SMP) systems.	best, worst and average case;central processing unit;code::blocks;parallel computing;response time (technology);symmetric multiprocessing	Konstantinos Bletsas;Neil C. Audsley	2005	11th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA'05)	10.1109/RTCSA.2005.48	embedded system;parallel processing;parallel computing;real-time computing;computer science;theoretical computer science;operating system;coprocessor	Embedded	-10.86014831250463	49.68143968846907	105916
7fc623ffcd3856477fe8c28bcdfc81d34872950c	aagc: an efficient associativity-aware garbage collection scheme for hybrid ftls	flash memory;garbage collection;hybrid ftl;data storage;nand ash memory;low power consumption;trace driven simulation	NAND ash memory has many advantages such as low-power consumption, shock-resistance, and non-volatility. For these reasons, NAND ash memory is used for data storage purposes in mobile, personal and even enterprise computer systems. When we use NAND ash memory as a means of data storage, it requires a garbage collection procedure due to its erase-before-write characteristic. The efficiency of this garbage collection is an important issue because it affects the performance of the NAND ash memory. In this paper, we propose an efficient garbage collection scheme that can be combined with previous hybrid FTLs. Our garbage collection scheme considers the associativity of the blocks when selecting a victim block. The proposed scheme effectively reduces the number of time-consuming block erasures. Our trace-driven simulation results show that the proposed garbage collection scheme can reduce the number of block erasures by as much as 14%. It can also reduce the average response time and maximum response time by nearly 10% and 70%, respectively.	block size (cryptography);computer data storage;computer memory;garbage collection (computer science);low-power broadcasting;non-volatile memory;response time (technology);simulation;volatility	Bongjae Kim;Minkyu Park;Cheol Jeon;Chang Oan Sung;Yookun Cho;Jiman Hong	2012		10.1145/2245276.2232066	parallel computing;real-time computing;computer hardware;computer science;computer data storage;garbage collection;programming language	OS	-9.673975574478197	54.7478119610993	105957
9466de3483e8d932ddcca54128838a4822fa3d8e	lag in multiprocessor virtual reality	network processor;virtual reality	Lag in virtual reality (VR), i.e., the delay between performing an action and seeing the result of that action, is critical when trying to achieve immersion. While multiple, networked processors have been used to increase throughput, we concentrate on using multiple processors to reduce lag. To that end, we present a complete list of all possible lag sources in VR applications, review available lag reduction techniques, and investigate how these reduction techniques interrelate. We also introduce a new process-synchronization scheme that reduces lag. We evaluate the effectiveness of this synchronization scheme by software simulation as well as by actual lag measurements in our sample application.	central processing unit;computer simulation;immersion (virtual reality);multiprocessing;synchronization (computer science);virtual reality	Matthias M. Wloka	1995	Presence: Teleoperators & Virtual Environments	10.1162/pres.1995.4.1.50	embedded system;real-time computing;simulation;computer science;artificial intelligence;operating system;virtual reality;network processor	HPC	-10.279427970781981	57.742013373871046	105983
429f9f2449316dcad59b979d5681486a587b3b8a	vtrim: a performance optimization mechanism for ssd in virtualized environment	para virtualization ssd trim virtual disk image i o performance;trim;i o performance;virtual machines;ssd;virtual disk image;virtual machines hard discs;para virtualization;virtual machining semantics file systems kernel hardware real time systems;average response time vtrim performance optimization mechanism ssd virtualized environment solid state disk hard drive disk high bandwidth trim instruction space management deletion semantic machine disk image paravirtualization virtual machine;hard discs	Solid state disk, compared with hard drive disk, has low delay, low energy consumption, high throughput, high bandwidth, and many other advantages. But it is important that, we will meet challenges when using the solid state disk in virtualized environment, because of its own structure and characteristics. But the trim instruction, that plays an important role in space management of SSD, cannot be passed through in a virtualized environment to the underlying solid state disk. So, how to make the trim instruction be handled in virtualized environment will be an urgent problem. An optimized system used in virtualized environment, named Vtrim, is proposed to solve the problem. In the paper, Vtrim monitors the deletion operation in virtual machine and sends the deletion semantic to the Domain 0 immediately. Then the deletion semantic from virtual machine will be translated into the deletion operation in Domain 0. This deletion operation can punch hole on virtual machine disk image, then trigger the trim instruction. In this way, the performance of the SSD in virtualized environment will be improved greatly. Experiments prove that the random write performance will be improved more than 100% at most in the para-virtualization virtual machine with the Vtrim. The average response time will also be reduced to about 40% of the original one in the best case.	best, worst and average case;disk image;hard disk drive;optimization mechanism;random access;response time (technology);sandbox (computer security);solid-state drive;throughput;vmdk;virtual machine	Xiaofei Liao;Jia Yu;Hai Jin	2012	2012 IEEE 12th International Conference on Computer and Information Technology	10.1109/CIT.2012.66	real-time computing;computer hardware;computer science;virtual machine;operating system;trim;database;logical disk	Metrics	-12.575993339181965	54.09766651726821	106139
fa0855a9a8727533a0000e50f2068690620f9238	backup to the future: how workload and hardware changes continually redefine data domain file systems		Disk-based backup systems use data deduplication to replace redundant data chunks with references. These systems initially supported workloads from their tape-based predecessors, but changes to hardware and applications have forced them to adapt in interesting and challenging ways.	backup;data deduplication;data domain	Yamini Allu;Fred Douglis;Mahesh Kamat;Philip Shilane;R. Hugo Patterson;Benjamin Zhu	2017	Computer	10.1109/MC.2017.187	redundancy (engineering);data domain;computer science;workload;computer hardware;metadata;fingerprint recognition;database;backup;data deduplication;computer data storage	OS	-16.06514937698673	53.175352238678315	106164
27cb0c2229299a82cf767d19dcc68aa1e5f0f233	beyond block i/o: rethinking traditional storage primitives	databases;atomic layer deposited;flash memory;magnetic disc storage;atomic layer deposition semantics databases media computer crashes solids ash;performance evaluation;computer crashes;mysql innodb transactional storage engine;database management systems;sql;semantics;computer system;higher order;proof of concept;disk based seek interfaces;media;log based flash translation layer;acid transaction semantics block i o storage primitives computer system storage subsystems magnetic disks nonvolatile memory flash memory enterprise grade storage medium disk based seek interfaces disk based read write interfaces log based flash translation layer mysql innodb transactional storage engine database management systems;flash translation layer;enterprise grade storage medium;operating system;storage primitives;nonvolatile memory;sql flash memories magnetic disc storage performance evaluation random access storage;disk based read write interfaces;ash;random access storage;block i o;atomic layer deposition;magnetic disks;storage subsystems;high performance;database management system;flash memories;acid transaction semantics;solids;reading and writing	Over the last twenty years the interfaces for accessing persistent storage within a computer system have remained essentially unchanged. Simply put, seek, read and write have defined the fundamental operations that can be performed against storage devices. These three interfaces have endured because the devices within storage subsystems have not fundamentally changed since the invention of magnetic disks. Non-volatile (flash) memory (NVM) has recently become a viable enterprise grade storage medium. Initial implementations of NVM storage devices have chosen to export these same disk-based seek/read/write interfaces because they provide compatibility for legacy applications. We propose there is a new class of higher order storage primitives beyond simple block I/O that high performance solid state storage should support. One such primitive, atomic-write, batches multiple I/O operations into a single logical group that will be persisted as a whole or rolled back upon failure. By moving write-atomicity down the stack into the storage device, it is possible to significantly reduce the amount of work required at the application, filesystem, or operating system layers to guarantee the consistency and integrity of data. In this work we provide a proof of concept implementation of atomic-write on a modern solid state device that leverages the underlying log-based flash translation layer (FTL). We present an example of how database management systems can benefit from atomic-write by modifying the MySQL InnoDB transactional storage engine. Using this new atomic-write primitive we are able to increase system throughput by 33%, improve the 90th percentile transaction response time by 20%, and reduce the volume of data written from MySQL to the storage subsystem by as much as 43% on industry standard benchmarks, while maintaining ACID transaction semantics.	acid;atomicity (database systems);computer;database engine;disk storage;ftl: faster than light;flash file system;flash memory controller;innodb;input/output;mysql;non-volatile memory;operating system;persistence (computer science);response time (technology);solid-state drive;solid-state electronics;technical standard;throughput	Xiangyong Ouyang;David W. Nellans;Robert Wipfel;David Flynn;Dhabaleswar K. Panda	2011	2011 IEEE 17th International Symposium on High Performance Computer Architecture	10.1109/HPCA.2011.5749738	flash file system;sql;parallel computing;higher-order logic;media;converged storage;non-volatile memory;computer hardware;computer science;operating system;solid;database;semantics;atomic layer deposition;programming language;proof of concept	Arch	-13.889134167739103	52.47746517444098	106252
b58370dce5a89cac26cca915868cea86862253a3	reducing non-deterministic loads in low-power caches via early cache set resolution	cache;performance;low power;latency;experimental evaluation;power consumption	Many of the recently proposed techniques to reduce power consumption in caches introduce an additional level of non-determinism in cache access latency. Due to this additional latency, instructions dependent on a load speculatively issued must be squashed and re-issued as they will not have the correct data in time. Our experiments show that there is a large performance degradation and associated dynamic energy wastage due to these effects of instruction squashing. To address this problem, we propose an early cache set resolution scheme. Our experimental evaluation shows that this technique is quite effective in mitigating the problem. 2006 Elsevier B.V. All rights reserved.	cpu cache;cache (computing);displacement mapping;elegant degradation;experiment;goto;low-power broadcasting;nondeterministic algorithm	Soontae Kim;Narayanan Vijaykrishnan;Mary Jane Irwin	2007	Microprocessors and Microsystems	10.1016/j.micpro.2006.10.002	bus sniffing;latency;parallel computing;real-time computing;cache coloring;computer hardware;performance;cache;computer science;cache invalidation;operating system;smart cache;cache algorithms;cache pollution	Arch	-8.475489249321388	54.27067196701535	106320
fd7dd928398fb1ca00d1fb464a70af9bf9a7b0d6	a new slack reclaiming algorithm for real-time systems	scheduling algorithm;server-based;slack reclaiming;real time systems	Real-time applications are ubiquitous in generalpurpose computing environments, while the real-time systems are growing in complexity. Thus in these hybrid real-time systems, schedulers must guarantee that all hard real-time jobs be completed before their deadlines and improve QoS of soft real-time tasks as much as possible. Towards this goal we have proposed a new slack reclaiming algorithm for server-based real-time systems, and have also implemented it in a real time emulator (RTSIM). This algorithm, named HBASH, which enhances the Constant Bandwidth Server (CBS) by slack reclaiming, allocates slack generated from the running process to the task that needs the slack most, and then this selected task will be scheduled immediately. Hence the algorithm is able to make full use of slack and reduce the response time of soft realtime tasks as much as possible. In this paper, we proved that our algorithm does not violate the schedulability of tasks, and we also evaluated the performance of this algorithm. The experimental results demonstrate that HBASH outperforms other slack reclaiming algorithms and improves soft real-time performance significantly.	algorithm;emulator;job stream;quality of service;real-time clock;real-time computing;real-time transcription;response time (technology);scheduling (computing);server (computing);slack variable	Wenzhi Chen;Qingsong Shi;Weifang Hu;Wei Hu;Sha Liu	2009	JSW		parallel computing;real-time computing;computer science;operating system;distributed computing;least slack time scheduling;scheduling	Embedded	-10.756119946044738	59.91903800016618	106450
412f81242609c3426b854dfb936a01b187ebac24	efficient data migration to conserve energy in streaming media storage systems	energy efficiency;energy conservation;synthetic traces energy conservation energy consumption reduction large scale streaming media storage systems energy efficiency explicit energy saving disk cooling eesdc data migration overhead energy efficient data layouts simulated disk system;streaming media energy efficiency layout distributed databases videos algorithm design and analysis quality of service;disc storage;data migration;layout;streaming media;media streaming cooling disc storage energy conservation;distributed databases;media streaming;storage energy conservation data layout streaming media data migration;data layout;quality of service;storage;algorithm design and analysis;cooling;videos	Reducing energy consumption has been an important design issue for large-scale streaming media storage systems. Existing energy conservation techniques are inadequate to achieve high energy efficiency for streaming media computing environments due to high data migration overhead. To address this problem, we propose in this paper a new energy-efficient method called Explicit Energy Saving Disk Cooling or EESDC. EESDC significantly reduces data migration overhead because of two reasons. First, a set of disks referred to Explicit Energy Saving Disks (EESD) is explicitly fixed according to temporal system load. Second, all the migrated data in EESDC directly contribute on extending the idle time of EESD to conserve more energy efficiently. Therefore, the EESDC method is conducive to saving more energy by quickly achieving energy-efficient data layouts without unnecessary data migrations. We implement EESDC in a simulated disk system, which is validated against a prototype system powered by our EESDC. Our experimental results using both real-world traces and synthetic traces show that EESDC can save up to 28.13-29.33 percent energy consumption for typical streaming media traces. Energy efficiency of streaming media storage systems can be improved by 3.3-6.0 times when EESDC is coupled.	computer cooling;digital footprint;family computer disk system;floppy disk;load (computing);overhead (computing);prototype;streaming media;synthetic intelligence;tracing (software)	Yunpeng Chai;Zhihui Du;David A. Bader;Xiao Qin	2012	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2012.63	layout;embedded system;algorithm design;real-time computing;data migration;quality of service;energy conservation;computer hardware;computer science;database;efficient energy use;distributed database	HPC	-15.076018635237492	56.245276030566856	106548
311d93746ee1c1601cab988592b7df99f3695bd5	iommu: strategies for mitigating the iotlb bottleneck	memory throttling;power;throughput	The input/output memory management unit (IOMMU) was recently introduced into mainstream computer architecture when both Intel and AMD added IOMMUs to their chip-sets. An IOMMU provides memory protection from I/O devices by enabling system software to control which areas of physical memory an I/O device may access. However, this protection incurs additional direct memory access (DMA) overhead due to the required address resolution and validation.  IOMMUs include an input/output translation lookaside buffer (IOTLB) to speed-up address resolution, but still every IOTLB cache-miss causes a substantial increase in DMA latency and performance degradation of DMA-intensive workloads. In this paper we first demonstrate the potential negative impact of IOTLB cache-misses on workload performance. We then propose both system software and hardware enhancements to reduce IOTLB miss rate and accelerate address resolution. These enhancements can lead to a reduction of over 60% in IOTLB miss-rate for common I/O intensive workloads.	cpu cache;computer architecture;computer data storage;device driver;direct memory access;elegant degradation;input/output;input–output memory management unit;memory protection;memory-mapped i/o;overhead (computing);sorting;translation lookaside buffer	Nadav Amit;Muli Ben-Yehuda;Ben-Ami Yassour	2010		10.1007/978-3-642-24322-6_22	throughput;computer architecture;parallel computing;real-time computing;computer hardware;computer science;operating system;power;direct memory access	Arch	-10.470711566138464	51.498801678195896	106577
0fb0e95e8eb39961b9276854eb3bc712f8ac7acf	nwperf: a system wide performance monitoring tool for large linux clusters	general and miscellaneous mathematics computing and information science;performance monitoring;performance evaluation;efficiency;performance;system monitoring;linux cluster;system monitoring workstation clusters parallel machines performance evaluation linux;performance metric;computer architecture;large scale;monitoring;parallel machines;linux;n codes;workstation clusters;monitoring linux performance analysis laboratories measurement relational databases aggregates statistics processor scheduling memory;pnnl;1954 cpu production linux cluster system wide performance monitoring tool linux clusters fine granularity performance metric data analysis large scale supercomputing clusters user applications nwperf architecture	We present NWPerf, a new system for analyzing fine granularity performance metric data on large-scale supercomputing clusters. This tool is able to measure application efficiency on a system wide basis from both a global system perspective as well as providing a detailed view of individual applications. NWPerf provides this service while minimizing the impact on the performance of user applications. We describe the type of information that can be derived from the system, and demonstrate how the system was used detect and eliminate a performance problem in an application application that improved performance by up to several thousand percent. The NWPerf architecture has proven to be a stable and scalable platform for gathering performance data on a large 1954-CPU production Linux cluster at PNNL.	anomaly detection;computer cluster;debugging;information;input/output;linux;linux;scalability;supercomputer	Ryan W. Mooney;Ken P. Schmidt;R. Scott Studham	2004	2004 IEEE International Conference on Cluster Computing (IEEE Cat. No.04EX935)	10.1109/CLUSTR.2004.1392637	system monitoring;parallel computing;computer hardware;performance;computer cluster;computer science;operating system;efficiency;linux kernel	HPC	-16.174122960336508	49.461948563912685	106651
044cafde686e811d1a6aa19a93fe97d0e4d8ab51	bulk disambiguation of speculative threads in multiprocessors	transactional memory;java;multi threading;thread level speculation	Transactional Memory (TM), Thread-Level Speculation (TLS), and Checkpointed multiprocessors are three popular architectural techniques based on the execution of multiple, cooperating speculative threads. In these environments, correctly maintaining data dependences across threads requires mechanisms for disambiguating addresses across threads, invalidating stale cache state, and making committed state visible. These mechanisms are both conceptually involved and hard to implement. In this paper, we present Bulk, a novel approach to simplify these mechanisms. The idea is to hash-encode a thread's access information in a concise signature, and then support in hardware signature operations that efficiently process sets of addresses. Such operations implement the mechanisms described. Bulk operations are inexact but correct, and provide substantial conceptual and implementation simplicity. We evaluate Bulk in the context of TLS using SPECint2000 codes and TM using multithreaded Java workloads. Despite its simplicity, Bulk has competitive performance with more complex schemes. We also find that signature configuration is a key design parameter.	antivirus software;application checkpointing;automatic parallelization;code;communications of the acm;digital signature;encode;fink;franklin electronic publishers;ibm research;international symposium on computer architecture;java;jikes;parallel computing;program optimization;shared memory;speculative execution;speculative multithreading;thread (computing);transactional memory;type signature;virtual machine;word-sense disambiguation	Luis Ceze;James Tuck;Josep Torrellas;Calin Cascaval	2006	33rd International Symposium on Computer Architecture (ISCA'06)	10.1145/1150019.1136506	hash table;computer architecture;transactional memory;parallel computing;real-time computing;multithreading;computer science;bloom filter;operating system;distributed computing;programming language;java;longest prefix match	Arch	-14.521887691309331	48.85759713837061	106999
53e77dfed5cc2c9edfaa2e4dfb1b937d1597cd07	distance-aware l2 cache organizations for scalable multiprocessor systems	scalable multiprocessor systems;computacion informatica;cc numa;l2 cache organization;multiprocessor systems;long distance;cache replacement;ciencias basicas y experimentales;cache replacement policy;remote memory access;grupo a;distance awareness	In this paper, we suggest an LRU/distance-aware combined second-level (L2) cache for scalable CC-NUMA multiprocessors, which is composed of a traditional LRU cache and an additional cache maintaining the distance information of individual cache blocks. The LRU cache selects a victim using age information, while the distance-aware cache does this using distance information. Both work together to reduce the overall distance effectively upon cache misses by keeping long-distance blocks as well as recently used blocks. It has been observed that the proposed cache outperforms the traditional LRU cache by up to 28% in the execution time. It is also found to perform even better than an LRU cache of twice the size.	cpu cache;central processing unit;multiprocessing;run time (program lifecycle phase);scalability;thomas j. watson research center	Sung Woo Chung;Hyong-Shik Kim;Chu Shik Jhon	2003	Euromicro Symposium on Digital System Design, 2003. Proceedings.	10.1016/j.sysarc.2004.07.006	bus sniffing;embedded system;least frequently used;pipeline burst cache;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;cache invalidation;operating system;distributed computing;adaptive replacement cache;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;non-uniform memory access	HPC	-12.368735555786952	51.37965578899577	107110
be6c4480cc11339b57ae956a3ab1e5849b314e46	mac: migration-aware compilation for stt-ram based hybrid cache in embedded systems	cache consistency;migration;energy efficient;hybrid cache;compiler;embedded system;data access;data layout;reading and writing	Hybrid caches consisting of both STT-RAM and SRAM have been proposed recently for energy efficiency. To explore the advantages of hybrid cache, most work on hybrid caches employs migration based strategies to dynamically move write-intensive data from STT-RAM to SRAM. Migrations require additional read and write operations for data movement and may lead to significant overheads. To address this issue, this paper proposes a Migration-Aware Compilation (MAC) approach to improve the energy efficiency and performance of STT-RAM based hybrid cache. By re-arranging data layout, the data access pattern in memory blocks is changed such that the number of migrations is reduced without any hardware modification. The reduction of migration overheads in turn improves energy efficiency and performance. The experimental results show that with the proposed approach, on average, the number of write operations on STT-RAM is reduced by 13.4%, the number of migrations is reduced by 16.1%, the total dynamic energy is reduced by 8.5%, and the total latency is reduced by 12.1%.	cpu cache;compiler;data access;embedded system;hybrid kernel;static random-access memory	Qing'an Li;Jianhua Li;Liang Shi;Chun Jason Xue;Yanxiang He	2012		10.1145/2333660.2333738	data access;embedded system;compiler;parallel computing;real-time computing;cache coloring;computer hardware;cache;computer science;human migration;operating system;efficient energy use;cache algorithms;cache pollution	HPC	-9.921929125995694	54.290837272857836	107116
02e2aa76414abfc79cb61c7b579745c822235997	a low power strategy for future mobile terminals	mobile handsets;multiprocessing systems;power consumption;cmp;chip multiprocessor;dynamic power consumption;mobile terminals;power consumption reduction;power-aware scheduling;power-saving strategies;static power consumption	In this paper, we have investigated the efficiency of two power-saving strategies that reduces both static and dynamic power consumption when applied to a chip-multiprocessor (CMP). They are evaluated under two workload scenarios and compared against a conventional uni-processor architecture and a CMP without any power-aware scheduling. The results show that energy due to static and dynamic power consumption can be reduced by up to 78% and that further 8% energy can be saved at the expense of response-time of non-critical applications.Furthermore, a small study on the potential impact of system-level events showed that system calls can contribute significantly to the total energy consumed.	multiprocessing;scheduling (computing);system call;system on a chip	Mladen Nikitovic;Mats Brorsson	2004	Proceedings Design, Automation and Test in Europe Conference and Exhibition		system on a chip;co-design;embedded system;electronic engineering;parallel computing;real-time computing;microarchitecture;reconfigurable computing;computer science;engineering;operating system;place and route	EDA	-5.416153336312633	55.78368447966208	107211
99bbeb52aca59f1ada55b06fc791b8b8e6d807fd	earncache: self-adaptive incremental caching for big data applications		Memory caching plays a crucial role in satisfying the requirements for (quasi-)real-time processing of exploding data on big-data clusters. As big data clusters are usually shared by multiple computing frameworks, applications or end users, there exists intense competition for memory cache resources, especially on small clusters that are supposed to process comparably big datasets as large clusters do, yet with tightly limited resource budgets. Applying existing on-demand caching strategies on such shared clusters inevitably results in frequent cache thrashing when the conflicts of simultaneous cache resource demands are not mediated, which will deteriorate the overall cluster efficiency.	big data;cache (computing)	Yifeng Luo;Junshi Guo;Shuigeng Zhou	2018		10.1007/978-3-319-96893-3_29	thrashing;end user;data mining;cache;cpu cache;computer science;big data;distributed computing	DB	-19.047689933326332	58.259794465580555	107410
2b3d6fea30107ce9e957b1588e33ed681904a8f6	reconfiguration time aware processing on fpgas	reconfigurable system;004;reconfigurable computing;real time partial reconfiguration reconfiguration time scheduling;scheduling algorithm;single machine;adaptive architecture	Considering nowadays FPGAs, the reconfiguration time is a non-negligible element of reconfigurable computation. Moreover, run-time environments that ignore the reconfiguration time can quickly lack applicability. Thus, methods to respect this additional time are required. Looking for suitable analogies in already evaluated fields seems to be reasonable and shall be investigated in this work. In the single machine environment, several scheduling algorithms exist that allow to quantify schedules with respect to feasibility, optimality, etc. In contrast, reconfigurable devices execute tasks in parallel, which intentionally collides with the single machine principle and seems to require new methods and evaluation strategies for scheduling. However, the reconfiguration phases of adaptable architectures usually take place sequentially. Run-time adaptation is realized using an exclusive port, which again is occupied for some reasonable time during reconfiguration. We have to handle the duration and the sequential exclusiveness of reconfiguration phases. Here, we can find an analogy to the single machine environment, as both scenarios must derive a sequential schedule for an exclusive resource. Thus, we investigate the appliance of single processor scheduling algorithms to task reconfiguration on reconfigurable systems in this paper. We determine necessary adaptations and propose methods to evaluate the scheduling algorithms.	algorithm;blue (queue management algorithm);column (database);computation;field-programmable gate array;graph coloring;optimizing compiler;preemption (computing);real-time clock;real-time transcription;reconfigurable computing;schedule (computer science);scheduling (computing);self-propelled particles;time of arrival;uniprocessor system;windows nt processor scheduling	Florian Dittmann	2006			embedded system;parallel computing;real-time computing;computer science	EDA	-9.750321182024978	59.68118271696172	107628
7e2608f9af95119a09d7f47de758da87822162f5	disk cache replacement policies for network fileservers	distributed system;file servers;performance evaluation;common least recently used policy;file type disk cache replacement policies network fileservers trace driven simulations network file servers locality based approaches common least recently used policy distributed systems frequency based approaches;workstations frequency file servers computer networks computational modeling network servers system performance;buffer storage;disk cache replacement policies;file type;system performance;trace driven simulations;computer networks;network servers;computational modeling;cache replacement;locality based approaches;network file servers;workstations;performance evaluation buffer storage digital simulation file servers;frequency based approaches;network fileservers;distributed systems;frequency;trace driven simulation;least recently used;digital simulation	Trace driven simulations are used to study the performance of several disk cache replacement policies for network leservers. It is shown that locality based approaches, such as the common Least Recently Used (LRU) policy, which are known to work well on standalone disked workstations and at client workstations in distributed systems, are inappropriate at a leserver. Quite simple frequency based approaches do better. More sophisticated frequency based policies (e.g., that take into account the le type) may ooer additional improvements.	disk buffer;distributed computing;locality of reference;page cache;simulation;workstation	Darryl L. Willick;Derek L. Eager;Richard B. Bunt	1993		10.1109/ICDCS.1993.287729	file server;parallel computing;real-time computing;workstation;computer science;operating system;frequency;database;distributed computing;adaptive replacement cache;computer performance;file format;computational model;cache algorithms;computer network	Metrics	-15.012350903098772	51.068008908957346	107670
cb377b3b2e0dc5cf06a9df0bb0a23080a3d40e07	hysteresis re-chunking based metadata harnessing deduplication of disk images	storage management;metadata related overhead hysteresis re chunking based metadata harnessing deduplication system disk images real duplication elimination ratio deduplication throughput input data stream mhd algorithm duplication distribution based hysteresis re chunking strategy bimodal algorithm subchunk algorithm sparse indexing algorithms data storage system;magnetohydrodynamics random access memory algorithm design and analysis hysteresis throughput merging indexes;metadata harnessing;metadata harnessing data deduplication;meta data;data deduplication;storage management meta data	Metadata-related overhead can significantly impact the performance of data deduplication systems, including the real duplication elimination ratio and the deduplication throughput. The amount of metadata produced is mainly determined by the chunking mechanism for the input data stream. In this paper, we propose a metadata harnessing deduplication (MHD) algorithm utilizing a duplication-distribution-based hysteresis re-chunking strategy. MHD harnesses the metadata by dynamically merging multiple non-duplicate chunks into one big chunk represented by one hash value while dividing big chunks straddling duplicate and non-duplicate data regions into small chunks represented with multiple hashes. Experimental results show that the proposed algorithm achieves a lower metadata overhead and a higher deduplication throughput for a given duplication elimination ratio, as compared with other state-of-the-art algorithms such as the Bimodal, Sub Chunk and Sparse Indexing algorithms.	algorithm;data deduplication;hash function;hysteresis;input/output;overhead (computing);shallow parsing;sparse;super high material cd;throughput	Bing Zhou;Jiangtao Wen	2013	2013 42nd International Conference on Parallel Processing	10.1109/ICPP.2013.48	parallel computing;data deduplication;computer science;theoretical computer science;operating system;data mining;database;metadata	HPC	-14.111258456919028	55.00624229635457	107924
083765452acb7c8cca09c16c7ddadb2c4486175a	energy-efficient in-memory data stores on hybrid memory hierarchies		Increasingly large amounts of data are stored in main memory of data center servers. However, DRAM-based memory is an important consumer of energy and is unlikely to scale in the future. Various byte-addressable non-volatile memory (NVM) technologies promise high density and near-zero static energy, however they suffer from increased latency and increased dynamic energy consumption.  This paper proposes to leverage a hybrid memory architecture, consisting of both DRAM and NVM, by novel, application-level data management policies that decide to place data on DRAM vs. NVM. We analyze modern column-oriented and key-value data stores and demonstrate the feasibility of application-level data management. Cycle-accurate simulation confirms that our methodology reduces the energy with least performance degradation as compared to the current state-of-the-art hardware or OS approaches. Moreover, we utilize our techniques to apportion DRAM and NVM memory sizes for these workloads.	byte addressing;column-oriented dbms;computer data storage;data center;data store;dynamic random-access memory;elegant degradation;memory hierarchy;non-volatile memory;operating system;simulation;volatile memory	Ahmad Hassan;Hans Vandierendonck;Dimitrios S. Nikolopoulos	2015		10.1145/2771937.2771940	interleaved memory;parallel computing;real-time computing;static random-access memory;computer hardware;operating system;database;universal memory;registered memory;nvm express	Arch	-11.238239232877131	54.13468795895791	107952
31036a5cce91465120af4b527df50a4eba1dae3a	energy-aware scheduling for multiprocessor real-time systems	en francais	Real-time applications have become more sophisticated and complex in their behavior and interaction over the time. Contemporaneously, multiprocessor architectures have emerged to handle these sophisticated applications. Inevitably, these complex real-time systems, encompassing a range from small-scale embedded devices to large-scale data centers, are increasingly challenged to reduce energy consumption while maintaining assurance that timing constraints will be met. To address this issue in real-time systems, many software-based approaches such as dynamic voltage and frequency scaling and dynamic power management have emerged. Yet their flexibility is often matched by the complexity of the solution, with the accompanying risk that deadlines will occasionally be missed. As the computational demands of real-time embedded systems continue to grow, effective yet transparent energy-management approaches will become increasingly important to minimize energy consumption, extend battery life, and reduce thermal losses. We believe that powerand energy-efficiency and scheduling of real-time systems are closely related problems, which should be tackled together for best results. By exploiting the characteristic parameters of real-time application tasks, the energy-consciousness of scheduling algorithms and the quality of service of real-time applications can be significantly improved. To support our thesis, this dissertation proposes novel approaches for energymanagement within the paradigm of energy-aware scheduling for soft and hard realtime applications, which are scheduled over identical multiprocessor platforms. Our first contribution is a Two-level Hierarchical Scheduling Algorithm (2L-HiSA) for multiprocessor systems, which falls in the category of restricted-migration scheduling. 2L-HiSA addresses the sub-optimality of EDF scheduling algorithm in multiprocessors by dividing the problem into a two-level hierarchy of schedulers. Our second contribution is a dynamic power management technique, called the Assertive Dynamic Power Management (AsDPM) technique. AsDPM serves as an admission control technique for real-time tasks, which decides when exactly a ready task shall execute, thereby reducing the number of active processors, which eventually reduces energy consumption. Our third contribution is a dynamic voltage and frequency scaling technique, called the Deterministic Stretch-to-Fit (DSF) technique, which falls in the category of inter-task DVFS techniques and works in conjunction with global scheduling algorithms. DSF comprises an online Dynamic Slack Reclamation algorithm (DSR), an Online Speculative speed adjustment Mechanism (OSM), and an m-Task Extension (m-TE) technique. Our fourth and final contribution is a generic power/energy management scheme for multiprocessor systems, called the Hybrid Power Management (HyPowMan) scheme. HyPowMan serves as a top-level entity that, instead of designing new power/energy management policies (whether DPM or DVFS) for specific operating conditions, takes a set of well-known existing policies. Each policy in the selected policy set performs well for a given set of operating conditions. At runtime, the best-performing policy for given workload is adapted by the HyPowMan scheme through a machine-learning algorithm. te l-0 05 99 98 0, v er si on 1 12 J un 2 01 1	ansi escape code;algorithm;central processing unit;computation;consciousness;data center;dynamic frequency scaling;dynamic voltage scaling;earliest deadline first scheduling;embedded system;frequency scaling;image scaling;multiprocessing;openstreetmap;power management;programming paradigm;quality of service;real-time clock;real-time computing;real-time locating system;real-time operating system;real-time transcription;real-time web;reliability engineering;scheduling (computing);slack variable;speculative execution;test engineer;transponder timing	Muhammad Khurram Bhatti	2011			parallel computing;real-time computing;computer science;distributed computing;multiprocessor scheduling	Embedded	-5.61912740685215	59.606731210640426	108123
a1f42bf57505c2f06f07d93dbbfcac3445b0032f	a serial data bus control method	control method	Th~ explosion of Distributed Processing Systems in recent years has created requirements for interconnecting large numbers of diverse processing elements. The performance characteristics of these processors span virtually the entire spectrum of speed and capability from small minicomputers to large-scale, general-purpose, multiprocessor systems. Implicit, therefore, in the communications interconnection are features such as high-speed, high-survivability, fault tolerance, and expandability. Sperry Univac has developed a Serial Data Bus as part of a contract with the Canadian government. Many of the system design problems which were addressed during the development period required unique solutions; the principal issues being Bus Access Time and Casualty Reconfiguration. The Bus Access Time issue eventually evolved into a design problem which demanded a highly sophisticated and efficient Bus Arbitration Mechanism. This paper provides the background and detailed technical descriptions of the actual designs which were chosen for flrese two key areas.	access time;central processing unit;fault tolerance;general-purpose markup language;interconnection;minicomputer;multiprocessing;requirement;systems design;univac	Steven C. Andersen	1979	Computer Networks	10.1016/0376-5075(79)90016-3	embedded system;parallel computing;real-time computing;telecommunications;computer science;local bus;operating system;system bus;control bus;computer security;computer network	Arch	-17.214958066823343	55.841271280075624	108129
7dbb9705ef989a1b87ecbc169544de32010e99ba	dbdb: optimizing dmatransfer for the cell be architecture	local memory;loop blocking;embedded system;memory access;design and implementation;data dependence;performance model;timing analysis;multi core system	In heterogeneous multi-core systems, such as the Cell BE or certain embedded systems, the accelerator core has its own fast local memory without hardware supported coherence. It is software's responsibility to dynamically transfer the working set when the total data set is too large to fit in the local memory. The data can be transferred through a software controlled cache which maintains correctness and exploits reuse among references, especially when complicated aliasing or data dependence exists. However, the software controlled cache introduces the extra overhead of cache lookup. In this paper we present the design and implementation of a Direct Blocking Data Buffer (DBDB) which combines compiler analysis and runtime management to optimize local memory utilization. We use compile time analysis to identify regular references in a loop body, block the innermost loop according to the access patterns and available local memory space, insert DMA operations for the blocked loop, and substitute references to local buffers. The runtime is responsible for allocating local memory for DBDB, especially for disambiguating aliased memory accesses which could not be resolved at compile time. We further optimize noncontiguous references by taking advantage of the DMA-list feature provided by the Cell BE. A practical performance model is presented to guide the DMA transfer scheme selection among single-DMA, multi-DMA and DMA-list. We have implemented DBDB in the IBM XL C/C++ for Multicore Acceleration for Linux, and have conducted experiments with selected test cases from the NAS OpenMP and SPEC benchmarks. The results show that our method performs well compared with traditional software cache approach. We have observed a speedup of up to 5.3x and 4x in average.	aliasing;c++;cpu cache;cell (microprocessor);compile time;correctness (computer science);dspace;data buffer;data dependency;direct memory access;embedded system;experiment;linux;lookup table;multi-core processor;openmp;optimizing compiler;overhead (computing);speedup;test case;word-sense disambiguation;working set;xl c/c++ compilers	Tao Liu;Haibo Lin;Tong Chen;Kevin O'Brien;Ling Shao	2009		10.1145/1542275.1542286	uniform memory access;parallel computing;real-time computing;computer hardware;computer science;operating system;programming language;static timing analysis;cache-only memory architecture;memory map;non-uniform memory access	HPC	-6.32092779287819	49.80958689892858	108533
55929f5f5c4214683980caca9f105e4d39118fab	retention time based data management policy for improving the lifespan of ssds		As the density of the flash memory continues to increase, the endurance issue of the flash memory based SSD is becoming more important. Reducing the retention time is one of the solutions to improve the lifespan of SSD. This is because the data stored on the SSD with short retention time requires less energy power when the data is erased by garbage collection; the less energy power can prolong the lifespan of SSD. For this reason, some researchers focus on the retention time to enhance the lifespan of SSD. However, previous work considers only the temporal locality, and therefore it reveals limited enhancement on the lifespan of SSD. In this paper, we propose a novel data classification scheme that considers the retention time of each flash block for efficient retention management and ultimately extends the lifespan of SSD. Our experiments show that the proposed scheme improves the lifespan of SSD by up to 7.2 times over the conventional SSD, with negligible performance degradation.	comparison and contrast of classification schemes in linguistics and metadata;elegant degradation;experiment;flash memory;garbage collection (computer science);locality of reference;solid-state drive	Yongju Song;Minho Lee;Young Ik Eom	2018	2018 IEEE International Conference on Big Data and Smart Computing (BigComp)	10.1109/BigComp.2018.00081	retention management;memory management;locality of reference;data management;real-time computing;garbage collection;data integrity;server;computer science;flash memory	DB	-11.258230209740466	54.47280092731856	108593
d739a38f599111703b4438b7f4f4fba7c8cc21c6	on the applicability of simple cache models for modern processors		Cache performance estimation is the first step in assuring good cache utilization and hence application performance. However, it is difficult to create good cache models as the implementation of commercial caches is complex, constantly evolving, and, protected information. As a result many practical compilers use simple cache models such as Fully Associative LRU Cache (FALC) model. In this paper we quantify the applicability of the FALC model for three modern processors. Our investigation reveals that the applicability is both application and architecture dependent. This insight is used to develop a model for an early (i.e. no profiling required) identification of applicability: Early Picking Criterion. The Early Picking Criterion is developed using synthetic benchmarks and validated with 15 memory intensive SPEC CPU2006 benchmarks. All applications identified by the Early Picking Criterion demonstrate high applicability.	approximation;cpu cache;central processing unit;compiler;dhrystone;electronic product code;profiling (computer programming)	Rakhi Hemani;Subhasis Banerjee;Apala Guha	2016	2016 2nd International Conference on Green High Performance Computing (ICGHPC)	10.1109/ICGHPC.2016.7508062	cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;computer science;operating system;cache algorithms	HPC	-7.211765892330891	49.43583970044449	108678
9a90bed722f93f5cd3679dfdda1de5fb462486bf	tmt: a tlb tag management framework for virtualized platforms		Virtualization is a convenient way to efficiently utilize the numerous on-chip resources in modern physical platforms. However, it is important to ensure a high performance for the workloads running on such virtualized platforms. One factor which reduces the performance of these virtualized workloads is the frequent flushing of hardware-managed Translation Lookaside Buffers (TLBs). To avoid these flushes and reduce the TLB miss rate, we propose the Tag Manager Table (TMT), a hardware architecture for generating and managing process-specific TLB tags. Since the TMT approach is software-transparent, it is equally applicable for virtualized and non-virtualized environments. Using a full-system simulation approach, we investigate the reduction in the TLB miss rate achieved by using the TMT. We also analyze the variation of this reduction with factors like the size of the TMT, the TLB architecture and the workload characteristics and estimate the relative importance of these factors in determining this reduction.	address space;computer architecture simulator;hit (internet);identifier;input/output;interaction;mac address;memory footprint;network switch;operating-system-level virtualization;pointer (computer programming);semiconductor consolidation;server (computing);simulation;throughput;translation lookaside buffer;x86	Girish Venkatasubramanian;Renato J. O. Figueiredo;Ramesh Illikkal;Donald Newell	2009	2009 21st International Symposium on Computer Architecture and High Performance Computing	10.1007/s10766-011-0189-y	embedded system;real-time computing;operating system	Arch	-12.458962664288265	52.9459412955767	108982
b0d0e06f9152a25151de39f72a75a69dc9ba7abd	memory-aware sizing for in-memory databases	databases;analytical models;sap hana optimization in memory databases performance closed queueing networks approximation;time factors;optimization;approximation methods;databases instruction sets time factors approximation methods analytical models optimization parallel processing;parallel processing;instruction sets	In-memory database systems are among the technological drivers of big data processing. In this paper we apply analytical modeling to enable efficient sizing of in-memory databases. We present novel response time approximations under online analytical processing workloads to model thread-level fork-join and per-class memory occupation.We combine these approximations with a non-linear optimization program to minimize memory swapping in in-memory database clusters. We compare our approach with state-of-the-art response time approximations and trace-driven simulation using real data from an SAP HANA in-memory system and show that our optimization model is significantly more accurate than existing approaches at similar computational costs.	approximation;big data;computation;in-memory database;linear programming;mathematical optimization;multitenancy;nonlinear programming;nonlinear system;online analytical processing;paging;provisioning;response time (technology);routing;sap hana;shared memory;simulation;tracing (software)	Karsten Molka;Giuliano Casale;Thomas Molka;Laura Moore	2014	2014 IEEE Network Operations and Management Symposium (NOMS)	10.1109/NOMS.2014.6838359	parallel processing;computer science;theoretical computer science;operating system;instruction set;data mining;database	DB	-16.240010179858576	55.362076596266164	109025
a7e9ba0e643510169345598b54afc1d802d15451	performance analysis of a 3d wireless massively parallel computer		In previous work, the authors presented a 3D hexagonal wireless direct-interconnect network for a massively parallel computer, with a focus on analysing processor utilisation. In this study, we consider the characteristics of such an architecture in terms of link utilisation and power consumption. We have applied a store-and-forward packet-switching algorithm to both our proposed architecture and a traditional wired 5D direct network (the same as IBM’s Blue Gene). Simulations show that for small and medium-size networks the link utility of the proposed architecture is comparable with (and in some cases even better than) traditional 5D networks. This work demonstrates that there is a potential for wireless processing array concepts to address High-Performance Computing (HPC) challenges whilst alleviating some significant physical construction drawbacks of traditional systems.	algorithm;blue gene;computer simulation;network packet;packet switching;parallel computing;profiling (computer programming);store and forward;while	Amir Mansoor Kamali Sarvestani;Chris Bailey;Jim Austin	2018	J. Sensor and Actuator Networks	10.3390/jsan7020018	computer network;wireless network;computer science;architecture;massively parallel;packet switching;wireless;hexagonal crystal system;distributed computing	HPC	-10.08859899736412	46.420199729837385	109059
b5d8a31aa186b96ff59da4c702f8266d39362bb9	on-chip multiple superscalar processors with secondary cache memories	four wave interleaved secondary cache;split bus protocol;shared memory;clocks;very large scale integration;buffer storage;bicmos technology;cache memory;250 mhz;bicmos integrated circuits;0 3 micron on chip multiple superscalar processors secondary cache memories microprocessor chip bicmos technology clock rate primary caches four wave interleaved secondary cache split bus protocol shared memory conflicts vlsi architecture 250 mhz 1000 mips;system on a chip;1000 mips;chip;0 3 micron;primary caches;on chip multiple superscalar processors;clock rate;access protocols;vlsi;vlsi bimos integrated circuits buffer storage microprocessor chips multiprocessing systems;superscalar processor;microprocessor chip;multiprocessing systems;system on a chip cache memory access protocols central processing unit bicmos integrated circuits very large scale integration large scale systems laboratories microprocessor chips clocks;secondary cache memories;high performance;central processing unit;shared memory conflicts;bimos integrated circuits;large scale systems;microprocessor chips;vlsi architecture	The development of an experimental high-performance microprocessor chip based on a 0.3- mu m BiCMOS technology is discussed. It is designed to operate at a 250-MHz clock rate. It includes two processors, each of which executes two instructions in parallel. The chip performs 1000 MIPS when instructions and data are fetched from primary caches. It also includes a four-wave interleaved secondary cache assessed in parallel according to a split-bus protocol, to reduce shared memory conflicts. The VLSI architecture and design results of this chip are described. >	blocking (computing);cpu cache;debugging;microprocessor;non-blocking algorithm;non-uniform memory access;r10000;scalability;superscalar processor;system bus	M. Hanawa;Tadahiko Nishimukai;O. Nishii;Masataka Nakazawa;K. Yano;M. Hiraki;S. Shukuri;T. Nishida	1991		10.1109/ICCD.1991.139862	multi-core processor;embedded system;computer architecture;parallel computing;computer hardware;computer science;operating system;very-large-scale integration	Arch	-7.166614929475086	52.180901349731506	109107
f784f15361c6febf4eabe89606238d4999faa253	parallel-architecture windowing display	parallel architecture	Abstract   A terminal display screen can be divided into separate logical windows, each window representing a virtual terminal which provides a connection to a different task. By allowing the windows to be moved and overlapped a user can organize the display to suit the current application. The paper describes an experimental display system which exploits a parallel hardware architecture to support a window management mechanism. Parallelism is provided through the use of separate, selfcontained window modules, one for each window that can be displayed. A module consists of the display memory associated with the window plus control logic in the form of a window management unit. Interaction between the modules occurs as the display image is generated so that the visibility of the overlapped windows can be determined.		Martin Colley	1988	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(88)90145-7	embedded system;real-time computing;title bar;computer hardware;computer science;z-order;operating system;compositing window manager;status bar	EDA	-18.404472937552598	50.90980666082957	109239
d33c005ba213f8dff39433811650b2c42b45afcf	erfair scheduler with processor suspension for real-time multiprocessor embedded systems	energy aware scheduling;erfair scheduling;static energy reduction;real time;proportional fairness;leakage awareness;procrastination scheduling	Proportional fair schedulers with their ability to provide optimal schedulability along with hard timeliness and quality-of-service guarantees on multiprocessors form an attractive alternative in real-time embedded systems that concurrently run a mix of independent applications with varying timeliness constraints. This article presents ERfair Scheduler with Suspension on Multiprocessors (ESSM), an efficient, optimal proportional fair scheduler that attempts to reduce system wide energy consumption by locally maximizing the processor suspension intervals while not sacrificing the ERfairness timing constraints of the system. The proposed technique takes advantage of higher execution rates of tasks in underloaded ERfair systems and uses a procrastination scheme to search for time points within the schedule where suspension intervals are locally maximal. Evaluation results reveal that ESSM achieves good sleep efficiency and provides up to 50% higher effective total sleep durations as compared to the Basic-ERfair scheduler on systems consisting of 2 to 20 processors.	algorithm;central processing unit;dynamic voltage scaling;embedded system;maximal set;online and offline;proportionally fair;quality of service;real-time clock;real-time transcription;run time (program lifecycle phase);scheduling (computing);shutdown (computing);simulation;symmetric multiprocessing	Piyoosh Purushothaman Nair;Arnab Sarkar;N. M. Harsha;Megha Gandhi;P. P. Chakrabarti;Sujoy Ghose	2016	ACM Trans. Design Autom. Electr. Syst.	10.1145/2948979	embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing	Embedded	-5.5691349457873995	58.77322348314688	109410
16e26a97372bd07459df47d29d626a181d57327e	an application-level approach for privacy-preserving virtual machine checkpointing	virtual machine checkpointing;checkpointing;virtual machines checkpointing cloud computing data privacy linux;virtual machines;data privacy;gedit text editors application level privacy preserving virtual machine checkpointing virtualization cloud computing platform server consolidation vm checkpointing persistent snapshot private user data confidential data privacy preserving checkpointing application programmer memory locations vm execution information flow analysis ppc system virtualbox vm linux operating system vim text editors;privacy virtual machine checkpointing;linux;checkpointing registers kernel virtual machining libraries credit cards reactive power;privacy;cloud computing	Virtualization has been widely adopted in recent years in the cloud computing platform to improve server consolidation and reduce operating cost. Virtual Machine (VM) checkpointing refers to the act of saving a persistent snapshot (or checkpoint) of a VM's state at any instant. VM checkpointing can drastically prolong the lifetime and vulnerability of confidential or private user data in applications that execute within VMs. Simply encrypting the checkpoint does not reduce the lifetime of confidential data that should be quickly discarded after its use. In this paper, we present an application-level approach, called Privacy-preserving Checkpointing (PPC), which excludes confidential data from VM checkpoints, instead of encrypting such data. PPC enables an application programmer to register memory locations that represent the origins of confidential data. During the VM's execution, PPC performs information flow analysis to automatically track the propagation of confidential data through the application and various components of the VM, including the guest operating system. During VM checkpointing, the locations identified during the information flow analysis are excluded from the persistent checkpoint. We present the design and implementation of the PPC system in VirtualBox VMs running the commodity Linux operating system. We demonstrate the use of our system using the vim and gedit text editors. We also show that PPC introduces acceptable performance overhead.	application checkpointing;circuit restoration;cloud computing;confidentiality;data-flow analysis;encryption;institute for operations research and the management sciences;inter-process communication;kernel (operating system);linux;operating system;overhead (computing);privacy;programmer;semiconductor consolidation;server (computing);snapshot (computer storage);software propagation;text editor;transaction processing system;user space;vim;virtual machine;virtualbox;web application;z/vm	Yaohui Hu;Tianlin Li;Ping Yang;Kartik Gopalan	2013	2013 IEEE Sixth International Conference on Cloud Computing	10.1109/CLOUD.2013.28	real-time computing;cloud computing;information privacy;computer science;virtual machine;operating system;database;privacy;linux kernel	OS	-17.549909153791184	52.91776280369742	109571
5979211fb33edcc546041e3d28d1a343f5c2a062	design methodology and run-time management for predictable many-core systems	power aware computing multiprocessing systems;real time systems optimization routing energy consumption clustering algorithms design methodology hardware;many core systems;energy consumption run time management design methodology predictable many core systems high performance multiapplication systems power management thermal management hardware resources self adaptive mechanisms hybrid application mapping complex timing analysis optimization algorithm rm;timing predictability many core systems run time management design space exploration;run time management;design space exploration;timing predictability	Many-core systems provide a feasible means to build high-performance multi-application systems. They are increasingly exposed to dynamic changes due to varying and online modified application mixes, as well as unavailability of hardware resources due to thermal and power management or faults. Particularly, when applications with real-time requirements are executed, these constraints may become a problem. Here, self-adaptive mechanisms are a means to optimally and feasibly manage such a system at run-time. This paper gives an overview of a design flow for hybrid application mapping for predictable many-core systems. The idea behind this concept is to perform a complex timing analysis and verification of an application during design time and then exploit this information for RM of the system. We propose a novel optimization algorithm to perform RM, which statically calculates a set of application mappings that are guaranteed to fulfill real-time requirements while heuristically optimizing the system objectives. Here, minimization of the energy consumption is exemplary chosen. The algorithm achieves significantly better results than a state-of-the-art approach, especially for systems with a high resource congestion.	algorithm;design flow (eda);heuristic;manycore processor;mathematical optimization;network congestion;network on a chip;power management;real-time clock;real-time computing;real-time transcription;requirement;run time (program lifecycle phase);static timing analysis;unavailability	Stefan Wildermann;Andreas Weichslgartner;Jürgen Teich	2015	2015 IEEE International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing Workshops	10.1109/ISORCW.2015.48	embedded system;parallel computing;real-time computing;simulation;system of systems;computer science;operating system;distributed computing;systems design	Embedded	-5.713602965364431	57.525430034573674	109654
58160198e9e508066b8e72ad7b8bc45b94e44be6	scheduling for energy efficiency and fault tolerance in hard real-time systems	original system;power aware computing;scheduling;proposed technique;fault tolerant computing;energy conservation;frame-based real-time system;fault tolerance;continuous voltage scaling;heuristic based scheduling technique;energy consumption;hard real-time system;l voltage level;comparable system;energy efficiency;real-time systems;proposed heuristic-based scheduling technique;hard real time systems;polynomial time;energy efficient;system on chip;network on chip;frequency;fault tolerant;job shop scheduling;nickel;real time systems;optimization;silicon	This paper studies the dilemma between fault tolerance and energy efficiency in frame-based real-time systems. Given a set of K tasks to be executed on a system that supports L voltage levels, the proposed heuristic-based scheduling technique minimizes the energy consumption of tasks execution when faults are absent, and preserves feasibility under the worst case of fault occurrences. The proposed technique first finds out the optimal solution in a comparable system that supports continuous voltage scaling, then converts the solution to the original system. The runtime complexity is only (LK2). Experimental results show that the proposed approach produces near-optimal results in polynomial time.	best, worst and average case;dynamic voltage scaling;fault tolerance;heuristic;image scaling;real-time clock;real-time computing;scheduling (computing);time complexity	Yu Liu;Han Liang;Kaijie Wu	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)		embedded system;job shop scheduling;mathematical optimization;fault tolerance;parallel computing;real-time computing;computer science;engineering;efficient energy use;network on a chip	EDA	-5.54668380398541	58.72271814068216	109903
c6e5b6e3f2c05e9e8b99db9b66bef6b8d4da1fed	integration of task scheduling with replica placement in data grid for limited disk space of resources	scientific application;disk storage;resource allocation;disc storage;scheduling algorithm data communication computer architecture heuristic algorithms scheduling strontium;strontium;data communication;minmin heuristic;disk storage task scheduling data grid distributed resource minmin heuristic heuristic replica placement algorithm;computer architecture;scheduling algorithm;limited disk space;distributed resource;scheduling;heuristic algorithms;task analysis;replica placement;limited disk space data grid task scheduling replica placement;data access;task scheduling;heuristic replica placement algorithm;grid computing;geographic distribution;data grid;task analysis disc storage grid computing resource allocation scheduling	Data grid integrates geographically distributed resources for solving data-sensitive scientific applications. As tasks are sensitive to data, dealing with large amount of data makes the requirement for efficiency in data access more critical. The goal of replica placement is to shorten data access time for enhancing the task execution performance. Therefore, replica placement strategies are often integral to task scheduling algorithms. However, all existing integration strategies make an assumption that the disk space of resources in data grid is unlimited. In this paper, we extended MinMin heuristic to cater to the situation where the disk space of a computational resource is limited. In addition, a heuristic replica placement algorithm is proposed, in which the limited disk space of a storage resource is considered as well. Another character of this heuristic replica placement algorithm is that it can map more than one hot file to several storage resources. We study our approach and evaluate it through simulation. The result shows that the integration of the two algorithms has improved the performance of data grid especially when the whole disk space of storage resources is relatively smaller than the amount of all data files.	access time;algorithm;computation;computational resource;data access;disk space;heuristic;one-hot;schedule (project management);scheduling (computing);simulation	Kan Yi;Feng Ding;Heng Wang	2010	2010 Fifth Annual ChinaGrid Conference	10.1109/ChinaGrid.2010.29	parallel computing;real-time computing;computer science;distributed computing	HPC	-17.54309105475711	59.048191614952074	109915
d69db04e82558e5c2dfecb512fbfc91acf6596ac	performance modeling using the motorola powerpc timing simulator	application development;system design;performance model	Published and projected benchmark numbers do not necessarily help predict the behavior of a user application on a particular microprocessor system. The real test comes when a user application is executed and timed on the microprocessor[5], which usually stresses the system in a different fashion than most benchmarks do. Unfortunately, the lack of stable platforms in earlier stages of system design prevents a developer from performing such evaluation tasks. This paper illustrates how application developers looking for a head start can use the Motorola Timing Simulator for the PowerPC 603&trade; microprocessor to analyze the expected behavior of a user program for various memory subsystems and gather fairly accurate timing information. A study of some SPEC&trade; benchmarks is also done in this respect.	benchmark (computing);microprocessor;powerpc;systems design	Tariq Afzal	1995	SIGARCH Computer Architecture News	10.1145/218864.218867	embedded system;computer architecture;parallel computing;real-time computing;computer hardware;computer science;operating system;rapid application development;systems design	Arch	-6.126256643685179	50.98033631166184	109972
d6c9fd0a5eba746c866fed62269887bdf28694dc	rose: a novel flash translation layer for nand flash memory based on hybrid address translation	nand flash memory;flash memory;coarse grained address translation;flash translation layer ftl storage management performance nand flash memory;memory management;merge aware cleaning policy;fine grained address translation;storage management;performance;rose ftl;flash translation layer;hybrid address translation;file system;merge aware cleaning policy flash translation layer nand flash memory hybrid address translation disk based file systems erase before write feature coarse grained address translation fine grained address translation rose ftl;storage management flash memories;disk based file systems;merging;writing;coarse grained;switches;article;cleaning flash memory switches memory management writing file systems merging;flash memories;flash translation layer ftl;file systems;cleaning;erase before write feature	A Flash Translation Layer (FTL) provides a block device interface on top of flash memory to support disk-based file systems. Due to the erase-before-write feature of flash memory, an FTL usually performs out-of-place updates and uses a cleaning procedure to reclaim stale data. A hybrid address translation (HAT)-based FTL combines coarse-grained and fine-grained address translation to achieve good performance while keeping the size of the mapping information small. In this paper, we propose a new HAT-based FTL, called ROSE, which includes three novel techniques for reducing the cleaning cost. First, it reduces high-cost reclamation by preventing data in an entire-block sequential write from being placed into multiple physical blocks while eliminating the cleaning cost resulting from mispredicting random or semisequential writes as sequential ones. Second, it uses a merge-aware cleaning policy that considers both the block age and the merge cost in a HAT-based FTL for improving the cleaning efficiency. Third, it delays the erasure of obsolete blocks and reuses their free pages for buffering more writes. Simulation results show that the proposed FTL outperforms existing HAT-based FTLs in terms of both cleaning cost and flash write time by up to 47 times and 1.6 times, respectively.	ftl: faster than light;film-type patterned retarder;flash file system;flash memory controller;multi-level cell;plasma cleaning;rose;sequential access;simulation;sputter cleaning	Mong-Ling Chiao;Da-Wei Chang	2011	IEEE Transactions on Computers	10.1109/TC.2011.67	flash file system;parallel computing;computer hardware;performance;network switch;computer science;operating system;writing;memory management	OS	-11.952650451071115	53.9284372952643	109985
1b7fca67cac01e3e550fbf5572d5667c739ef447	exploring tag-bit memory operations in hybrid memory cubes	3d memory;barrier synchronization;hybrid memory cube;full empty bits;tag bits;concurrency;memory architecture	The recent advances in multi-dimensional or stacked memory devices have led to a significant resurgence in research and effort associated with exploring more expressive memory operations in order to improve application throughput. The goal of these efforts is to provide memory operations in the logic layer of a stacked device that provide pseudo processing near memory capabilities to reduce the bandwidth required to perform common operations across concurrent applications.  One such area of concern in applications is the ability to provide high performance, low latency mutexes and associated barrier synchronization techniques. Previous attempts at performing cache-based mutex optimization and tiered barrier synchronization provide some degree of application speedup, but still induce sub-optimal scenarios such as cache line contention and large degrees of message traffic. However, several previous architectures have presented techniques that extend the core physical address storage with additional, more expressive bit storage in order to provide fine-grained concurrency mechanisms in hardware.  This work presents a novel methodology and associated implementation for providing in-situ extended memory operations in an HMC Gen2 device. The methodology provides a single lock, or tag bit for every 64-bit word in memory using the in-situ storage. Further, we present an address inversion technique that enables the tag-bit operations to execute their respective read-arbitrate-commit operations concurrently with a statistically low collision between the tag-bit storage and the data storage. We conclude this work with results from utilizing the commands to perform a traditional multi-threaded mutex algorithm as well as a multi-threaded static tree barrier that exhibit sub-linear scaling.	64-bit computing;algorithm;barrier (computer science);computer data storage;concurrency (computer science);cubes;extended memory;flash memory;hybrid memory cube;image scaling;independence day: resurgence;mathematical optimization;mutual exclusion;physical address;speedup;thread (computing);throughput	John D. Leidel;Yong Chen	2016		10.1145/2989081.2989105	memory address;uniform memory access;distributed shared memory;shared memory;interleaved memory;semiconductor memory;parallel computing;real-time computing;memory barrier;memory refresh;computer hardware;computer science;physical address;computer data storage;computer memory;overlay;conventional memory;extended memory;flat memory model;registered memory;computing with memory;cache-only memory architecture;memory map;memory management	Arch	-10.391060245088902	50.06152637318696	110292
737fe14fe86bd206d3a177d6534802f7ae212699	dynamic memory control in a parallel implementation of an operational weather forecast model	forecasting;atmospheric circulation;weather;general circulation models;weather forecasting;environmental sciences;computerized simulation;algorithms;parallel implementation;parallel processing;mathematics computers information science management law miscellaneous	Dynamic memory allocation is a useful feature for the UM as it allows the pre-compilation of a large amount of the model code. Production runs of differing resolutions can then use this pre-compiled object code thus reducing their compilation time. Although sequentially free lists are effective for the UM on the KSR1, a shared free list for the memory allocator {open_quote}malloc{close_quote} can seriously degrade performance in parallel. Thread based free lists or the separate allocation of memory from a threads stack alleviates this problem.	memory management	Rupert Ford;David F. Snelling;A. Dickinson	1995			parallel computing;simulation;computer science;theoretical computer science	HPC	-15.577839876076244	49.48201838622719	110370
147a33caba3bb6b42fa3000cc4666e3700346c5f	a comparative study of distributed resource sharing on multiprocessors	multistage dynamic network;general and miscellaneous mathematics computing and information science;routing;processor scheduling;probability density function;resource management;data processing;mathematical logic;data mining;interconnection network;computer networks;computer network;distributed scheduling;shared bus address mapping crossbar switch multistage dynamic network queueing delay resource sharing;scheduling algorithm;shared bus;network configuration;array processors;comparative evaluations;resource sharing;processing 990200 mathematics computers;algorithms;delay switches resource management processor scheduling routing data mining probability density function;switches;distributed data processing;data tagging;crossbar switch;queueing delay;logical process;dynamic networks;address mapping	In this paper we have studied the interconnection of resources to multiprocessors and the distributed scheduling of these resources. For a given interconnection network, the resource-mapping problem entails the search of one of the free resources which can be connected to each requesting processor. To prevent the bottleneck of sequential scheduling, a request without any destination address is given to the network, and the network is responsible for finding the necessary resource and connecting it to the processor. The addressing mechanism is thus distributed in the network. Three different classes of networks have been investigated: namely, single shared bus, multiple shared buses, and multistage dynamic networks. In each case, the scheduling algorithm is described, and the tradeoffs of different network configurations are studied. The resource-sharing networks are a generalization of conventional interconnection networks with routing tags in which all the resources are of different types.		Benjamin W. Wah	1984	IEEE Trans. Computers	10.1109/TC.1984.5009356	shared resource;routing;probability density function;mathematical logic;parallel computing;real-time computing;data processing;network switch;computer science;resource management;crossbar switch;distributed computing;scheduling;computer network	Arch	-12.668615228634074	57.80191142387381	110597
8659a73f0bbf3055e82d3d1e7bff81dd359d7cf0	enabling compositionality for multicore timing analysis	energy efficiency;semi partitioning;multi core systems;voltage and frequency islands;dynamic voltage and frequency scaling;hard real time systems	Timing compositionality is assumed by almost all multicore timing analyses. In this paper, we show that compositional timing analysis can be incorrect even for simple microarchitectures with in-order execution. We then introduce three approaches to enable sound compositional analysis: two based on analysis and one based on a hardware modification. In the experimental evaluation we explore the strengths and weaknesses of these three approaches. One of the two analysis-based approaches provides an attractive trade-off between analysis cost and precision, enabling sound compositional timing analysis even for microarchitectures with out-of-order execution.	multi-core processor;out-of-order execution;static timing analysis	Sebastian Hahn;Michael Jacobs;Jan Reineke	2016		10.1145/2997465.2997471	embedded system;real-time computing;computer science;theoretical computer science	PL	-7.001764270855632	57.37309725318043	110813
6e38503d1c5adf0739fa50d08a1a42756c97103f	co-manage power delivery and consumption for manycore systems using reinforcement learning		Maintaining high energy efficiency has become a critical design issue for high-performance systems. Many power management techniques have been proposed for the processor cores such as dynamic voltage and frequency scaling (DVFS). However, very few solutions consider the power losses suffered on the power delivery system (PDS), despite the fact that they have a significant impact on the system overall energy efficiency. With the explosive growth of system complexity and highly dynamic workloads variations, it is challenging to find the optimal power management policies which can effectively match the power delivery with the power consumption. To tackle the above problems, we propose a reinforcement learning-based power management scheme for manycore systems to jointly monitor and adjust both the PDS and the processor cores aiming to improve system overall energy efficiency. The learning agents distributed across power domains not only manage the power states of processor cores but also control the on/off states of on-chip VRs to proactively adapt to the workload variations. Experimental results with realistic applications show that when the proposed approach is applied to a large-scale system with a hybrid PDS, it lowers the system overall energy-delay-product (EDP) by 41% than a traditional monolithic DVFS approach with a bulky off-chip VR.		Haoran Li;Zhongyuan Tian;Rafael K. V. Maeda;Xuanqi Chen;Jun Feng;Jiang Xu	2018	2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1145/3240765.3240787	electronic engineering;voltage regulator;system on a chip;embedded system;efficient energy use;reinforcement learning;computer science;frequency scaling;power management;multi-core processor;power domains	EDA	-4.635026672762209	55.615675451305606	110842
55a1f6867599e3764e8d025313fc65dad85778b8	wcet analysis of the shared data cache in integrated cpu-gpu architectures		By taking the advantages of both CPU and GPU as well as the shared DRAM and cache, the integrated CPU-GPU architecture has the potential to boost the performance for a variety of applications, including real-time applications as well. However, before being applied to the hard real-time and safety-critical applications, the time-predictability of the integrated CPU-GPU architecture needs to be studied and improved. In this work, we study the shared data Last Level Cache (LLC) in the integrated CPU-GPU architecture and propose to use an access interval based method to improve the time-predictability of the LLC. The results show that the proposed technique can effectively improve the accuracy of the miss rate estimation in the LLC. We also find that the improved LLC miss rate estimations can be used to further improve the WCET estimations of GPU kernels running on such an architecture.	best, worst and average case;cpu cache;central processing unit;dynamic random-access memory;graphics processing unit;lunar lander challenge;network on a chip;real-time clock;real-time computing;scheduling (computing);worst-case execution time	Yijie Huangfu;Wei Zhang	2017	2017 IEEE High Performance Extreme Computing Conference (HPEC)	10.1109/HPEC.2017.8091059	kernel (linear algebra);computer architecture;architecture;embedded system;cache;central processing unit;dram;space-based architecture;computer science	Arch	-7.881077397208406	58.0389740623278	110897
2f94a310699a92d901cfc645a0fc7fbd38698e1a	exploiting temporal locality in drowsy cache policies	leakage current;temporal locality;data stream;drowsy cache policies;most recently used;low power;energy consumption;set associative caches;power consumption;high performance;reuse information	Technology projections indicate that static power will become a major concern in future generations of high-performance microprocessors. Caches represent a significant percentage of the overall microprocessor die area. Therefore, recent research has concentrated on the reduction of leakage current dissipated by caches. The variety of techniques to control current leakage can be classified as non-state preserving or state preserving. Non-state preserving techniques power off selected cache lines while state preserving place selected lines into a low-power state. Drowsy caches are a recently proposed state-preserving technique. In order to introduce low performance overhead, drowsy caches must be very selective on which cache lines are moved to a drowsy statePast research on cache organization has focused on how best to exploit the temporal locality present in the data stream. In this paper we propose a novel drowsy cache policy called Reuse Most Recently used On (RMRO), which makes use of reuse information to trade off performance versus energy consumption. Our proposal improves the hit ratio for drowsy lines by about 67%, while reducing the power consumption by about 11.7% (assuming 70nm technology) with respect to previously proposed drowsy cache policies.	cpu cache;cache (computing);die (integrated circuit);hit (internet);locality of reference;low-power broadcasting;microprocessor;overhead (computing);shutdown (computing);spectral leakage	Salvador Petit;Julio Sahuquillo;Jose M. Such;David R. Kaeli	2005		10.1145/1062261.1062321	embedded system;parallel computing;real-time computing;computer science;cache algorithms	Arch	-6.447687242087249	55.07651733719544	110931
af24164f37ddc77adb6876d4143efa4e4e2e00e6	on the load balancing techniques for gpu applications based on prefix-scan	resource allocation graphics processing units;prefix scan;instruction sets arrays message systems registers graphics processing units indexes search problems;gpu;dynamic work unit mapping load balancing techniques gpu applications prefix scan parallel applications gpu threads workload decomposition strategies workload mapping strategies multiphase search advanced dynamic technique gpu device characteristics;load balancing	Prefix-scan is one of the most common operation and building block for a wide range of parallel applications for GPUs. It allows the GPU threads to efficiently find and access in parallel to the assigned data. Nevertheless, the workload decomposition and mapping strategies that make use of prefix-scan can have a significant impact on the overall application performance. This paper presents a classification of the mapping strategies at the state of the art and their comparison to understand in which problem they best apply. Then, it presents Multi-Phase Search, an advanced dynamic technique that addresses the workload unbalancing problem by fully exploiting the GPU device characteristics. In particular, the proposed technique implements a dynamic mapping of work-units to threads through an algorithm whose complexity is sensibly reduced with respect to the other dynamic approaches in the literature. The paper shows, compares, and analyses the experimental results obtained by applying all the mapping techniques to different datasets, each one having very different characteristics and structure.	algorithm;graphics processing unit;load balancing (computing);semiconductor industry	Federico Busato;Nicola Bombieri	2015	2015 IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip	10.1109/MCSoC.2015.15	parallel computing;real-time computing;computer science;theoretical computer science	Arch	-5.923687874764436	46.55062729419404	111001
3059f8b632c4f8e564f46589c7da4a900c307142	the post-game analysis framework - developing resource management strategies for concurrent systems	analytical models;distributed system;iterative refinement;concurrent programming paradigms;cluster algorithm;concurrent computing;performance evaluation;execution time;application software;game analysis;data gathering;resource management concurrent computing application software nasa hardware perturbation methods timing proposals analytical models algorithm design and analysis;perforation;post game analysis framework;rule based;resource manager;resource management;multiprocessors;distributed computing;rule based architecture;indexing terms;operations research;timing data;programming model;objective function;perturbation methods;multiprocessor architectures;concurrent systems;resource management strategies;multiprocessor architecture;multiprocessor architectures post game analysis framework resource management strategies concurrent systems distributed computations multiprocessors execution time program partitioning optimization subgoals timing data simulations rule based architecture concurrent programming paradigms;distributed computations;concurrent programs;performance evaluation operations research parallel processing;load balance;program partitioning;nasa;proposals;algorithm design and analysis;parallel processing;optimization subgoals;task allocation;hardware;timing	Research has been conducted to determine how distributed computations can be mapped to multiprocessors to minimize execution time. The approach described here, known as post-game analysis, incrementally changes the program partitioning in between program execution time in subsequent runs. Post-game analysis differs from conventional iterative refinement or controlled opportunistic perturbation in that no abstract program models or any single objective function are employed to determine the relative merits of two alternative mappings. Multiple optimization subgoals are formulated, based on actual timing data gathered during program execution. Heuristics, based on various optimization subgoals, are then applied to propose changes to the current mapping. Finally, a mapping generation process which prioritizes and resolves conflicting proposals is applied. Results obtained from simulations show that post-game analysis consistently out-performs random placement, load-balancing, and clustering algorithms by 15%. Few iterations are required for simulations involving more than 200 processes and 64 sites. A rule-based architecture enables incremental strategy refinement, thus making post-game analysis easily tailorable to programs written in many concurrent programming paradigms and multiprocessor architectures. >	concurrency (computer science)	Jerry C. Yan;Stefen F. Lundstrom	1989	IEEE Trans. Knowl. Data Eng.	10.1109/69.87976	algorithm design;application software;parallel computing;real-time computing;index term;computer science;load balancing;resource management;database;distributed computing;programming paradigm;programming language;data collection	DB	-15.001357001776848	58.98686309082812	111047
5fb5e84b7c97f08d2f72c06a066a3cdc01076f40	power failure protection scheme for reliable high-performance solid state disks	supercapacitor	Solid-state disks (SSDs) have received much attention as replacements for hard disk drives (HDDs). One of their noticeable advantages is their high-speed read/write operation. To achieve good performance, SSDs have an internal memory hierarchy which includes several volatile memories, such as DRAMs and SRAMs. Furthermore, many SSDs adopt aggressive memory management schemes under the assumption of stable power supply. Unfortunately, the data stored in the volatile memories are lost when the power supplied to SSDs is abruptly shut off. Such power failure is often observed in portable devices. For this reason, it is critical to provide a power failure protection scheme for reliable SSDs. In this work, we propose a power-failure protection scheme for SSDs to increase their reliability. The contribution of our work is three-fold. First, we design a power failure protection circuit which incorporates super-capacitors as well as rechargeable batteries. Second, we provide a method to determine the capacity of backup power sources. Third, we propose a data backup procedure when the power failure occurs. We implemented our method on a real board and applied it to a notebook PC with a contemporary SSD. The board measurement and simulation results prove that our method is robust in cases of sudden power failure. key words: solid state disk, power failure protection, data backup, supercapacitor, rechargeable battery	backup;computer data storage;emergency power system;floppy disk;hard disk drive;laptop;memory hierarchy;memory management;mobile device;non-volatile memory;power supply;read-write memory;rechargeable battery;shutdown (computing);simulation;solid-state drive;static random-access memory	Kwanhu Bang;Kyung-Il Im;Dong-gun Kim;Sang-Hoon Park;Eui-Young Chung	2013	IEICE Transactions		embedded system;real-time computing;computer hardware;supercapacitor	OS	-11.137049146568282	55.194708102729614	111125
2f7ab9278c2762b9c9ceca771e31596ff5b4c4ea	laziness pays! using lazy synchronization mechanisms to improve non-blocking constructions	distributed computing;compact routing;interval routing;broadcasting	We present a simple and efficient wait-free implementation of Lazy Large Load-Linked/Store-Conditional (Lazy-LL/SC), which can be used to atomically modify a dynamically-determined set of shared variables in a lock-free manner. The semantics of Lazy-LL/SC is weaker than that of similar objects used by us previously to design lock-free and wait-free constructions, and as a result can be implemented more efficiently. However, we show that Lazy-LL/SC is strong enough to be used in existing non-blocking universal constructions and to build new ones.	blocking (computing);ll grammar;ll parser;lazy evaluation;load-link/store-conditional;non-blocking algorithm;shared variables	Mark Moir	2000		10.1145/343477.343520	real-time computing;computer science;theoretical computer science;distributed computing;broadcasting;algorithm	Theory	-15.268779987387603	46.44567106519514	111157
c6c5effbb349da454a1d0ba8e694692a65eb93ce	graph analytics on manycore memory systems		Graphs are ubiquitous, and graph analytics has been widely adopted in many big data applications such as social computation and natural language processing, as well as web-search and recommendation systems. Prior research focuses on processing large-scale graphs on distributed environments or a single multi-core machine with several terabytes of RAM. Increasing complex memory systems and on-chip interconnects are developed to mitigate the data movement bottlenecks in manycore processors such as Xeon Phi KNL CPU with heterogeneous memory, with up to 72 dual-core tiles. This paper presents a detailed study on the characteristics of manycore memory systems and their impact on the efficiency of graph analytics. Based on this paper, we introduce Ants, the first graph analytics platform on manycore memory systems. First, Ants differentially allocates graph data according to their access patterns and the behavior of heterogeneous memory. Second, to reduce excessive memory access and ease congestion on interconnects and memory controllers, Ants develops a fine-grained and effective task partitioning strategy for many cores. A detailed experiment on a 64 dual-core tile machine shows that Ants outperforms the state-of-the-art graph analytics platform-Ligra by up to 8.97X for real-world graphs.	big data;central processing unit;computation;electrical connection;manycore processor;natural language processing;network congestion;random-access memory;recommender system;terabyte;xeon phi	Yuxuan Xing;Zhiguang Chen;Nong Xiao;Fang Liu;Yutong Lu	2018	IEEE Access	10.1109/ACCESS.2018.2869463	task analysis;recommender system;big data;xeon phi;computation;distributed computing;computer science;analytics;graph;terabyte	DB	-5.329908568509955	48.89618558431207	111166
c5ad37dc347c6aff25d42262782b43ccb914306f	adaptive object storage system for mobile computing environments	energy conservation;fiabilidad;reliability;computadora personal;systeme intelligent;secondary cell;ordinateur personnel;storage system;almacenamiento informacion;personal computer;acumulador electroquimico;sistema inteligente;performance;mobile computer;systeme adaptatif;information storage;portable equipment;fiabilite;accumulateur electrochimique;conservation energie;adaptive system;intelligent system;conservacion energetica;sistema adaptativo;stockage information;rendimiento;appareil portatif;aparato portatil	This paper describes an adaptive object storage system for mobile computing environments. The object storage system adopts the object graph framework[1], and changes the conguration of its structure according to computing environments. The power management has become an important consideration in the design of new hardware and software. The disk is a promising candidate for power management because it is a device with which the user does not interact with directly. With proper management by the operation system, the disk may be spun up and down without the user noticing much dierence in performance or reliability. However, if the portable computers have connected to an AC power supply, spinning down of a disk is not necessary, and performance is more important than energy consumption. On the other hand, if remaining battery is not sucient, the reliability of modied objects becomes more important since the computer may be shutdown at any moment when the battery becomes empty. Therefore, three metrics, performance, reliability, battery consumption should be taken into account for object storage systems in mobile computing environments. When a portable computer is connected to an AC power supply, performance is the most important. However, when the computer is driven by the battery, battery consumption is the most important. On the other hand, if remaining battery is not sucient, reliability is the most important. The adaptive object storage system should take into account the tradeo between the three metrics. Our object storage system is constructed by an object graph that consists of four objects. The object OF is an object that implements the interface of the object storage system and mechanisms for managing objects. The adaptive object MM contains memory management policies that determines which objects should be cached in physical memory. The adaptive object PS contains policies for moving modied objects to the object DM. The adaptive object changes the object processing messages from other objects according to computing environments. The object that currently receives messages is called an active object. Two objects are contained in PS by considering the tradeo between reliability and performance. PS 1 copies objects whenever objects are modied for ensuring the consistency of objects that reside in physical memory and a disk. This policy increases the reliability by sacricing the performance. PS 2 copies objects to a disk periodically for decreasing the number of disk accesses. The object sacrices the reliability for increasing the performance.	active object;cache (computing);computer data storage;memory management;mobile computing;object graph;object storage;operating system;portable computer;power management;power supply;ps (unix);shutdown (computing)	Tatsuo Nakajima	1997		10.1007/3-540-63875-X_67	embedded system;simulation;energy conservation;performance;computer science;artificial intelligence;adaptive system;operating system;reliability;mobile computing	DB	-9.148885563403509	56.080497919787184	111186
69743194ca177ef816d31a99475c3ba3ff97808c	multiscale: memory system dvfs with multiple memory controllers	energy conservation;operating system;energy consumption;memory systems;memory system;voltage scaling;heuristic algorithm;dynamic voltage and frequency scaling	The fraction of server energy consumed by the memory system has been increasing rapidly and is now on par with that consumed by processors. Recent work demonstrates that substantial memory energy can be saved with only a small, tightly-controlled performance degradation using memory Dynamic Frequency and Voltage Scaling (DVFS). Prior studies consider only servers with a single memory controller (MC); however, multicore server processors have begun to incorporate multiple MCs. We propose MultiScale, the first technique to coordinate DVFS across multiple MCs, memory channels, and memory devices. Under operating system control, MultiScale monitors application bandwidth requirements across MCs. It then uses a heuristic algorithm to select and apply a frequency combination that will minimize the overall system energy within user-specified per-application performance constraints. Our results demonstrate that MultiScale reduces system energy consumption significantly, compared to prior approaches, while respecting the user-specified performance constraints.	algorithm;central processing unit;dynamic voltage scaling;elegant degradation;heuristic (computer science);memory controller;multi-core processor;operating system;requirement;server (computing)	Qingyuan Deng;David Meisner;Abhishek Bhattacharjee;Thomas F. Wenisch;Ricardo Bianchini	2012		10.1145/2333660.2333727	heuristic;uniform memory access;embedded system;interleaved memory;electronic engineering;parallel computing;real-time computing;distributed memory;energy conservation;computer science;operating system;static memory allocation;flat memory model;computing with memory	Arch	-5.34667858208704	55.521423216847886	111196
4e69e8872fae8bbcba14603109341646c995588a	stabilizing cpu frequency and voltage for temperature-aware dvfs in mobile devices	android based mobile devices cpu frequency temperature aware dvfs off the shelf mobile devices dynamic voltage frequency scaling dynamic thermal management;performance evaluation;temperature sensors;mobile processors;system on chip;power management;mobile handsets;temperature aware design;low power design;smartphone dynamic thermal management power management dynamic voltage and frequency scaling;thermal management packaging power aware computing smart phones;smartphone;system on chip thermal management performance evaluation mobile handsets program processors cooling temperature sensors;thermal management;program processors;dynamic thermal management;dynamic voltage and frequency scaling;cooling	Recent mobile devices adopt high-performance processors to support various functions. As a side effect, higher performance inevitably leads to power density increase, eventually resulting in thermal problems. In order to alleviate the thermal problems, off-the-shelf mobile devices rely on dynamic voltage-frequency scaling (DVFS)-based dynamic thermal management (DTM) schemes. Unfortunately, in the DVFS-based DTM schemes, an excessive number of DTM operations worsen not only performance but also power efficiency. In this paper, we propose a temperature-aware DVFS scheme for Android-based mobile devices to optimize power or performance depending on the option. We evaluate our scheme in the off-the-shelf mobile device. Our evaluation results show that our scheme saves energy consumption by 12.7%, on average, when we use the power optimizing option. Our scheme also enhances the performance by 6.3%, on average, by using the performance optimizing scheme, still reducing the energy consumption by 6.7%.	android;central processing unit;dynamic voltage scaling;elegant degradation;frequency scaling;image scaling;mobile device;performance per watt;sensor;smartphone;thermal management of high-power leds	Jae Min Kim;Young Geun Kim;Sung Woo Chung	2015	IEEE Transactions on Computers	10.1109/TC.2013.188	system on a chip;embedded system;real-time computing;thermal management of electronic devices and systems;mobile processor;computer science;operating system	Mobile	-5.165544926773888	55.8945639384784	111229
71b23cd5b4ab8fad012a8eca8e9f58fe4de153f7	gpu register file virtualization	microarchitecture;virtualization;metadata;energy efficient computing;gpgpu;registers;graphics processing units;register file;context;parallel processing;instruction sets	To support massive number of parallel thread contexts, Graphics Processing Units (GPUs) use a huge register file, which is responsible for a large fraction of GPU's total power and area. The conventional belief is that a large register file is inevitable for accommodating more parallel thread contexts, and technology scaling makes it feasible to incorporate ever increasing size of register file. In this paper, we demonstrate that the register file size need not be large to accommodate more threads context. We first characterize the useful lifetime of a register and show that register lifetimes vary drastically across various registers that are allocated to a kernel. While some registers are alive for the entire duration of the kernel execution, some registers have a short lifespan. We propose GPU register file virtualization that allows multiple warps to share physical registers. Since warps may be scheduled for execution at different points in time, we propose to proactively release dead registers from one warp and re-allocate them to a different warp that may occur later in time, thereby reducing the needless demand for physical registers. By using register virtualization, we shrink the architected register space to a smaller physical register space. By under-provisioning the physical register file to be smaller than the architected register file we reduce dynamic and static power consumption. We then develop a new register throttling mechanism to run applications that exceed the size of the under-provisioned register file without any deadlock. Our evaluation shows that even after halving the architected register file size using our proposed GPU register file virtualization applications run successfully with negligible performance overhead.	deadlock;division by two;file virtualization;graphics processing unit;image scaling;kernel (operating system);overhead (computing);provisioning;register file;throttling process (computing)	Hyeran Jeon;Gokul Subramanian Ravi;Nam Sung Kim;Murali Annavaram	2015		10.1145/2830772.2830784	parallel processing;computer architecture;parallel computing;virtualization;register window;computer file;computer hardware;microarchitecture;control register;computer science;memory buffer register;operating system;register renaming;stack register;instruction set;instruction register;index register;open;processor register;flags register;metadata;register allocation;register file;status register;general-purpose computing on graphics processing units;memory data register;memory address register	Arch	-9.32036437129773	52.11114389708926	111244
5cc69ccd810d741eb59bc5eb82989044cfff45ea	task-accurate performance modeling in systemc for real-time multi-processor architectures	virtual processing components;real-time multi-processor architecture;novel framework;task accuracy;multiple processor;task-accurate performance modeling;arbitrary scheduling strategy;small simulation overhead;real time;resource management;software design;hardware;real time systems;processor architecture;computational modeling;multiprocessing;software systems;operating systems;xml;modeling and simulation;computer architecture	We propose a framework, called virtual processing components (VPC) that permits the modeling and simulation of multiple processors running arbitrary scheduling strategies in SystemC. The granularity is given by task accuracy that guarantees a small simulation overhead	central processing unit;multiprocessing;overhead (computing);performance prediction;real-time clock;scheduling (computing);simulation;systemc;virtual machine;virtual private cloud	Martin Streubühr;Joachim Falk;Christian Haubelt;Jürgen Teich;Rainer Dorsch;Thomas Schlipf	2006	Proceedings of the Design Automation & Test in Europe Conference		computer architecture;parallel computing;real-time computing;multiprocessing;microarchitecture;computer science;operating system;modeling and simulation	EDA	-8.161583687836925	58.98782181474759	111245
e4159eb83bdb1cc49e6d4c535200e8e9bfdb027b	exploit real-time fine-grained access patterns to partition write buffer to improve ssd performance and life-span	software;flash memory;disc drives;flash memory ssd access patterns write buffer;access patterns;buffer storage;os buffer cache fine grained access pattern partition write buffer ssd performance life span solid state drives shock resistance flash memory out of place update wearing out asymmetric performance log structured block based ftl flasht translation layer write buffer design;write buffer;ssd;flash memories buffer storage disc drives;blogs flash memories software switches;switches;blogs;flash memories	Solid State Drives (SSDs) have become very popular recently due to their high performance and other benefits such as shock-resistance. However, SSDs pose some unique and serious challenges to I/O and file system designers because of flash memory's unique properties, such as out-of-place update, wearing-out, and highly asymmetric performance for read, write and erase operations. Most SSDs employ a log-structured block-based FlashTranslation-Layer (FTL) to solve the out-of-place update problem. The performance of FTLs is often highly sensitive to access patterns, especially the write access patterns. For example, sequential write requests see lower overhead than random writes. Moreover, sequential write requests that are not aligned to the flash page boundary may cause extra write and garbage collection operations, increasing overhead and wear-out. In this paper, we present a novel write buffer design based on sophisticated, fine-grain write access pattern analysis. Our scheme identifies access patterns in a per-process per-stream granularity in the OS buffer cache. These patterns are then used to guide the write buffer to improve the write performance of SSDs that employ a log-structured block-based FTL. Simulation results show that our solutions can improve write performance by up to 38%. Moreover, the schemes reduce SSD erase cycles by up to 56%, which is directly translated to a major improvement on the life-span of SSDs.	ftl: faster than light;file system permissions;flash memory;garbage collection (computer science);input/output;operating system;overhead (computing);page cache;pattern recognition;real-time clock;sequential access;simulation;solid-state drive;write buffer	Mingyang Wang;Yiming Hu	2013	2013 IEEE 32nd International Performance Computing and Communications Conference (IPCCC)	10.1109/PCCC.2013.6742772	parallel computing;real-time computing;computer hardware;network switch;computer science;write buffer;write combining;computer network	HPC	-11.890861207347672	53.937096136100344	111299
0d7dd4ba1a88b8812f8ca99fe38484dc8e262cf2	a bit string content aware chunking strategy for reduced cpu energy on cloud storage		In order to achieve energy saving and reduce the total cost of ownership, green storage has become the first priority for data center. Detecting and deleting the redundant data are the key factors to the reduction of the energy consumption of CPU, while high performance stable chunking strategy provides the groundwork for detecting redundant data. The existing chunking algorithm greatly reduces the system performance when confronted with big data and it wastes a lot of energy. Factors affecting the chunking performance are analyzed and discussed in the paper and a new fingerprint signature calculation is implemented. Furthermore, a Bit String Content Aware Chunking Strategy (BCCS) is put forward. This strategy reduces the cost of signature computation in chunking process to improve the system performance and cuts down the energy consumption of the cloud storage data center. On the basis of relevant test scenarios and test data of this paper, the advantages of the chunking strategy are verified.		Bin Zhou;Shudao Zhang;Deng-Yi Zhang;JiaHao Tan	2015	J. Electrical and Computer Engineering	10.1155/2015/242086	parallel computing;real-time computing;computer science;theoretical computer science	OS	-13.083758576700957	55.087831674805734	111419
1a6c2af35f561bc6b82a78a5405152b1ae10c5bf	static dataflow with access patterns: semantics and analysis	analytical models;resource constraint;access patterns;clocks;data flow graphs;semantics;multimedia application;computational modeling;system recovery;dataflow;clocks semantics computational modeling timing analytical models throughput system recovery;signal processing;access patterns dataflow semantics;data access;resource constraint access patterns signal processing multimedia application static cyclo static dataflow models compile time analyzability execution properties deadlock absence channel boundedness throughput sdf ap data access correctness constraint;signal processing data flow graphs;throughput;timing	Signal processing and multimedia applications are commonly modeled using Static/Cyclo-Static Dataflow (SDF/CSDF) models. SDF/CSDF explicitly specifies how much data is produced and consumed per firing during computation. This results in strong compile-time analyzability of many useful execution properties such as deadlock absence, channel boundedness, and throughput. However, SDF/CSDF is limited in its ability to capture how data is accessed in time. Hence, using these models often leads to implementations that are sub-optimal (i.e., use more resources than necessary) or even incorrect (i.e., use insufficient resources). In this work, we advance a new model called Static Dataflow with Access Patterns (SDF-AP) that captures the timing of data accesses (for both production and consumption). This paper formalizes the semantics of SDF-AP, defines key properties governing model execution, and discusses algorithms to check these properties under correctness and resource constraints. Results are presented to evaluate these analysis algorithms on practical applications modeled by SDF-AP.	ap computer science principles;algorithm;compile time;compiler;computation;correctness (computer science);dataflow;deadlock;signal processing;throughput	Arkadeb Ghosal;Rhishikesh Limaye;Kaushik Ravindran;Stavros Tripakis;Ankita Prasad;Guoqiang Wang;Trung N. Tran;Hugo A. Andrade	2012	DAC Design Automation Conference 2012	10.1145/2228360.2228479	data access;embedded system;throughput;parallel computing;real-time computing;computer science;theoretical computer science;operating system;dataflow;signal processing;semantics;programming language;computational model;algorithm	EDA	-7.301344898293626	59.730301546793704	111442
ec1108f542080716bbb0c61b83145583d54a981f	cache aware pre-runtime scheduling	cache;task switching;search strategy;scheduling algorithm;worst case execution time;scheduling;precedence constraint;abstract interpretation;hard real time	We present a novel pre-runtime scheduling method for uniprocessors which precisely takes the effects of task switching on the processor cache into consideration. Tasks are modelled as a sequence of non preemptable segments with precedence constraints. The cache behavior of each task segment is statically determined by abstract interpretation. For the sake of efficiency, the scheduling algorithm uses a heuristically guided search strategy. Each time a new task segment is added to a partial schedule, its worst case execution time is calculated based on the cache state at the end of the preceding partial schedule.	abstract interpretation;algorithm;best, worst and average case;cpu cache;correctness (computer science);earliest deadline first scheduling;experiment;heuristic (computer science);program analysis;run time (program lifecycle phase);scheduling (computing);uniprocessor system;worst-case execution time	Daniel Kästner;Stephan Thesing	1999	Real-Time Systems	10.1023/A:1008142508047	fair-share scheduling;fixed-priority pre-emptive scheduling;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;dynamic priority scheduling;cache;computer science;cache invalidation;operating system;distributed computing;cache algorithms;scheduling	Embedded	-11.087500176479853	59.07811047540542	111540
1de55faba85d812483252e067b8bee0a924e6558	transparent resource allocation to exploit idle cluster nodes in computational grids	idle cluster nodes;instruments;computational grid;high performance computing;grid applications;workstation clusters grid computing natural sciences computing resource allocation;resource allocation;e science;resource manager;resource management;distributed computing;idle cluster resources;cluster of workstations;transparent resource allocation;large scale;computational modeling;workstations;cluster resource management idle cluster nodes computational grids workstation clusters e science transparent resource allocation idle cluster resources formal allocation request;computational grids;computer science;workstation clusters;natural sciences computing;grid computing;cluster resource management;resource management grid computing high performance computing workstations large scale systems supercomputers computer science instruments distributed computing computational modeling;supercomputers;formal allocation request;large scale systems	Clusters of workstations are one of the most suitable resources to assist e-scientists in the execution of large-scale experiments that demand processing power. The utilization rate of these machines is usually far from 100%, and hence this should motivate administrators to share their clusters to grid communities. However, exploiting these resources in computational grids is challenging and brings several problems. This paper presents a transparent resource allocation strategy to harness idle cluster resources aimed at executing grid applications. This novel approach does not make use of a formal allocation request to cluster resource managers. Moreover, it does not interfere with local cluster users, being non-intrusive, and hence motivating cluster administrators to publish their resources to grid communities. We present experimental results regarding the effects of the proposed strategy on the attendance time of both cluster and grid requests and we also analyze its effectiveness in clusters with different utilization rates	experiment;workstation	Marco Aurélio Stelmar Netto;Rodrigo N. Calheiros;Rafael K. S. Silva;César A. F. De Rose;Caio Northfleet;Walfredo Cirne	2005	First International Conference on e-Science and Grid Computing (e-Science'05)	10.1109/E-SCIENCE.2005.83	parallel computing;real-time computing;workstation;resource allocation;computer science;resource management;operating system;distributed computing;computational model;grid computing	HPC	-16.617750533210522	59.2384334431686	111761
22f930ff069814cc81c14d1844b086592f9706d2	adaptive virtual partitioning for olap query processing in a database cluster		OLAP queries are typically heavy-weight and ad-hoc thus requiring high storage capacity and processing power. In this paper, we address this problem using a database cluster which we see as a cost-effective alternative to a tightly-coupled multiprocessor. We propose a solution to efficient OLAP query processing using a simple data parallel processing technique called adaptive virtual partitioning which dynamically tunes partition sizes, without requiring any knowledge about the database and the DBMS. To validate our solution, we implemented a Java prototype on a 32 node cluster system and ran experiments with typical queries of the TPC-H benchmark. The results show that our solution yields linear, and sometimes super-linear, speedup. In many cases, it outperforms traditional virtual partitioning by factors superior to 10.	adaptive algorithm;attribute–value pair;benchmark (computing);best, worst and average case;black box;data parallelism;experiment;high-availability cluster;hoc (programming language);ibm tivoli storage productivity center;java;lattice problem;middleware;multiprocessing;online analytical processing;parallel computing;performance;postgresql;prototype;relational database management system;scalability;shared nothing architecture;speedup;table (database);winsock	Alexandre A. B. Lima;Marta Mattoso;Patrick Valduriez	2004			online analytical processing;computer science;theoretical computer science;data mining;database	DB	-14.956446926713925	51.4669495240819	111808
5fcb7e1e312017ea82b7199547ef10ca43d98595	optimization of message passing services on power8 infiniband clusters	infiniband;infiniband verbs;pami;gpu direct;mpi point to point communication;mpi	We present scalability and performance enhancements to MPI libraries on POWER8 InfiniBand clusters. We explore optimizations in the Parallel Active Messaging Interface (PAMI) libraries. We bypass IB VERBS via low level inline calls resulting in low latencies and high message rates. MPI is enabled on POWER8 by extension of both MPICH and Open MPI to call PAMI libraries. The IBM POWER8 nodes have GPU accelerators to optimize floating throughput of the node. We explore optimized algorithms for GPU-to-GPU communication with minimal processor involvement. We achieve a peak MPI message rate of 186 million messages per second. We also present scalable performance in the QBOX and AMG applications.	algorithm;graphics processing unit;infiniband;library (computing);mpich;message passing;open mpi;scalability;throughput	Sameer Kumar;Robert Blackmore;Sameh Sharkawi;K. A. Nysal Jan;Amith R. Mamidala;T. J. Chris Ward	2016		10.1145/2966884.2966909	computer architecture;parallel computing;computer science;operating system	HPC	-10.636845789492869	46.45293345075581	111969
9ec071f3b8aea168fa5d0c45258c55543b471ec0	understanding the performance of two production supercomputers	mainframes;cachebench;luna;parallel machines mainframes;memtime;blasbench;typhoon;luna los alamos national laboratory production supercomputers typhoon lanl applications;partisn;conceptual application performance analysis luna typhoon xrage partisn cachebench blasbench memtime;conceptual;xrage;typhoons sockets production measurement computer architecture supercomputers bandwidth;parallel machines;application performance analysis	Luna and Typhoon are two Top500 supercomputers running production scientific workloads at LANL. Interestingly, users have reported substantially better-than-expected performance gains from Luna, the newer of the two. In this paper we present our methodology for investigating the source of this improvement and determining what architectural changes for future supercomputers would most benefit LANL applications.	64-bit computing;benchmark (computing);central processing unit;luna;memory management;network interface controller;nonlinear system;supercomputer;top500;typhoon;working set size	Scott Pakin;Michael Lang	2013	2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum	10.1109/IPDPSW.2013.82	parallel computing;simulation;engineering;operations research	Arch	-8.826240950998363	47.53316966811595	112054
b3a6fd8a957b3e27f88069733c9b86fba7f6660d	resource manager with multi-core support for parallel desktop	processor architecture;parallel desktop;kernel;user interface;processor scheduling;resource allocation;resource manager;resource management;best effort;soft real time;resource management processor scheduling multicore processing hardware environmental management throughput yarn operating systems user interfaces middleware;general purpose operating system;interactive application;operating system;general purpose operating system resource management multicore support parallel desktop multicore processors job scheduling;scheduling;multicore processing;multicore support;linux;multicore processors;job scheduling;program processors;parallel processing;benchmark testing;real time systems;scheduling parallel processing resource allocation	Due to the fall in the price of multicore processors, today's non-dedicated clusters tend to include this kind of hardware in their configurations. However, most current resource managers and job schedulers are optimized with the aim of maximizing the throughput in single-core environments. Additionally, job schedulers balance thread loads without considering such aspects as cache affinity, resource contention or other additional criteria. In the same way, general purpose Operating System schedulers will support requirements like the coexistence of soft-real time, best effort or interactive applications are open questions that need to be addressed carefully. For these reasons, new user interfaces, middlewares, and environments that can fill the gap between the user and the new processor architectures are needed. Our CISNE environment has been extended to take into account issues discussed earlier: the mixture of different types of applications considering the multicore features of current desktop environments.	best-effort delivery;central processing unit;cluster state;coexist (image);desktop computer;division algorithm;linux;multi-core processor;operating system;parallel computing;processor affinity;requirement;resource contention;scheduling (computing);single-core;throughput;user interface;wiki	Jose Ramon García;Josep L. Lérida;Porfidio Hernández	2009	2009 IEEE International Conference on Cluster Computing and Workshops	10.1109/CLUSTR.2009.5289134	multi-core processor;parallel computing;real-time computing;computer science;resource management;operating system	HPC	-15.28427285187707	58.860129919619965	112071
bf23d61c792f323319f73070ebbe91d57a2157c2	dvs scheduling in a line or a star network of processors	networks;dynamic voltage scaling;sequencing;optimization	Dynamic voltage scaling (DVS) is a technique which allows the processors to change speed when executing jobs. Most of the previous works study either single processor or multiple parallel processors. In this paper, we consider a network of DVS enabled processors. Every job needs to go along a certain path in the network and has a certain workload finished on any processor it goes through before it moves on to the next processor. Our objective is to minimize the total energy consumption while finishing every job before its deadline. Due to the intrinsic complexity of this problem, we only focus on line networks with two nodes and a simple one-level tree network (a star). We show that in some of these simple cases, the optimal schedule can be computed efficiently and interleaving is not needed to achieve optimality. However, in both types of networks, how to find the optimal sequence of execution remains a big challenge for jobs with general workloads.	a* search algorithm;central processing unit;dynamic voltage scaling;scheduling (computing);star network	Zongxu Mu;Minming Li	2015	J. Comb. Optim.	10.1007/s10878-013-9668-y	mathematical optimization;parallel computing;real-time computing;computer science;sequencing;distributed computing	Theory	-6.274266559916504	59.430930105805714	112086
ef3e36f7e019f483453a24524a4765b8d15dd8f1	gpgpu virtualisation with multi-api support using containers		Virtualisation of GPGPUs using PCI-Passthrough is limited to costly specialised hardware. API-Interception provides an alternative software-based approach to GPGPU virtualisation that has been shown to provide good performance and increased utilisation on High Perfor- mance and High Throughput Computing systems. Furthermore, user applications can transparently access many non-local GPGPU resources. However, current API-Interception implementations have either limited batch system support or none at all. This paper introduces a new multi- component system that supports multiple API-Interception implemen- tations on several batch systems. The system consists of: (a) a factory component that produces lightweight Linux Containers using Docker, where each Container supports one or more API-Interception implemen- tations and controls a single GPGPU; (b) a registry service that man- ages the Container resources; and, (c) a set of plugin scripts that bridge between the batch system and the registry. This paper also evaluates the performance of benchmarks on the prototype.		John Walsh;Jonathan Dukes	2015		10.1007/978-3-319-27308-2_64	parallel computing;real-time computing;computer hardware;operating system;database;distributed computing	Robotics	-16.913187981187473	52.268428020195095	112220
aa1862904bf5c17cc270089ea43fef4e6232d4ed	rip: run-based intra-query parallelism for scalable complex event processing	parallelism;pattern matching;complex event processing cep;stream processing	Recognition of patterns in event streams has become important in many application areas of Complex Event Processing (CEP) including financial markets, electronic health-care systems, and security monitoring systems. In most applications, patterns have to be detected continuously and in real-time over streams that are generated at very high rates, imposing high-performance requirements on the underlying CEP system. For scaling CEP systems to increasing workloads, parallel pattern matching techniques that can exploit multi-core processing opportunities are needed. In this paper, we propose RIP - a Run-based Intra-query Parallelism technique for scalable pattern matching over event streams. RIP distributes input events that belong to individual run instances of a pattern's Finite State Machine (FSM) to different processing units, thereby providing fine-grained partitioned data parallelism. We compare RIP to a state-based alternative which partitions individual FSM states to different processing units instead. Our experiments demonstrate that RIP's partitioned parallelism approach outperforms the pipelined parallelism approach of this state-based alternative, achieving near-linear scalability that is independent from the query pattern definition.	complex event processing;data parallelism;experiment;finite-state machine;multi-core processor;parallel computing;pattern matching;real-time clock;requirement;scalability	Cagri Balkesen;Nihal Dindar;Matthias Wetter;Nesime Tatbul	2013		10.1145/2488222.2488257	parallel computing;real-time computing;stream processing;computer science;operating system;pattern matching;database;distributed computing;data parallelism;programming language;instruction-level parallelism;task parallelism	DB	-18.5869615288723	55.099881955678875	112396
068a006fb55ce250e0e2825f4e87bf1561ca4a32	autotuning and adaptivity approach for energy efficient exascale hpc systems: the antarex approach		The main goal of the ANTAREX 1 project is to express by a Domain Specific Language (DSL) the application self-adaptivity and to runtime manage and autotune applications for green and heterogeneous High Performance Computing (HPC) systems up to the Exascale level. Key innovations of the project include the introduction of a separation of concerns between self-adaptivity strategies and application functionalities. The DSL approach will allow the definition of energy-efficiency, performance, and adaptivity strategies as well as their enforcement at runtime through application autotuning and resource and power management.	auto-tune;digital subscriber line;domain-specific language;holism;performance per watt;power management;programming paradigm;requirement;run time (program lifecycle phase);separation of concerns	Cristina Silvano;Giovanni Agosta;Andrea Bartolini;Andrea R. Beccari;Luca Benini;João Bispo;Radim Cmar;João M. P. Cardoso;Carlo Cavazzoni;Jan Martinovic;Gianluca Palermo;Martin Palkovic;Pedro Pinto;Erven Rohou;Nico Sanna;Katerina Slaninová	2016	2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)		mathematical optimization;compiler;supercomputer;parallel computing;real-time computing;digital subscriber line;computer science;distributed computing;efficient energy use;programming language;weaving	HPC	-4.788220579284246	49.34785599310511	112744
04be98efb085cb7ca6bf485b7d0635f5186d36b3	a hybrid flash memory ssd scheme for enterprise database applications	databases;physical stability;oltp;nand flash memory;flash memory;database system;mobile device;performance evaluation;database management systems;database;flash memory nonvolatile memory stability personal digital assistants production facilities delay file systems database systems solid state circuits power generation economics;flash memory solid state disk;wear leveling;single level chip;personal digital assistants;chip;stability;hybrid flash memory;computer architecture;solid state circuits;data storage;flash translation layer;nonvolatile storage;business data processing;nonvolatile memory;hybrid flash memory nand flash memory oltp database ftl;database systems;production facilities;ash;ftl;cost efficiency;mobile handsets;multilevel chip;flash memories business data processing database management systems;high performance;hybrid flash memory ssd scheme;benchmark testing;flash memories;power generation economics;file systems;flash translation layer hybrid flash memory ssd scheme enterprise database applications nonvolatile storage physical stability data storage single level chip multilevel chip wear leveling flash memory solid state disk;enterprise database applications	Flash memory has many advantages such as high performance, low electronic power, non-volatile storage and physical stability, over hard-disks. For this reason, flash memory has been deployed as data storage for mobile devices, including PDAs, MP3 players, laptop-computers and database systems. According to the cell type, flash memory can be divided into SLC(Single Level Chip) and MLC(Multi Level Chip). In general, SLC is known to have high performance and longer lifetime (i.e. more than 100K wear-leveling) while MLC is to offer larger capacity and with low price but have wear leveling of not longer than 10K. In this paper, we show that it is possible to design a fast and cost-efficient storage by combining two types of flash memories in a hybrid fashion. Specifically, we propose a hybrid flash memory solid state disk(SSD) scheme using FAST FTL for enterprise applications, where SLC chip is used as the log space for FAST while MLC chips store the normal data blocks. SLC chips allow fast and durable performance for write while MLC chips provide the large capacity. And, this is mainly due to the FAST FTL algorithm’s characteristics: it tends to direct the random writes to SLC chips and direct the other most random read to MLC chips. By taking the advantages of both chip types, we can find an economically desirable flash SSD design option. Experimental results show that our hybrid flash SSD scheme outperforms MLC-only flash scheme by far both in terms of performance and price.	algorithm;computer data storage;cost efficiency;database;disk storage;enterprise software;experiment;ftl: faster than light;flash memory;garbage collection (computer science);l (complexity);laptop;mp3;mobile device;multi-level cell;non-volatile memory;personal digital assistant;random access;sequential access;solid-state drive;volatile memory;wear leveling	Byung-Woo Nam;Gap-Joo Na;Sang-Won Lee	2010	2010 12th International Asia-Pacific Web Conference	10.1109/APWeb.2010.70	chip;flash file system;embedded system;benchmark;parallel computing;non-volatile memory;stability;computer hardware;computer science;operating system;computer data storage;mobile device;database;online transaction processing;cost efficiency	DB	-11.916450501678334	54.41519387665764	112929
c1412129d1b91c3cbe52d55a242fd0757da6a32d	sac: rethinking the cache replacement policy for ssd-based storage systems	flash memory;storage system;solid state drive;operating system;cache replacement;ssd;cache replacement policy;sac;storage;trace driven simulation	Solid-state drives (SSDs) are widely used in storage systems. However, algorithms adopted by existing operating systems generally consider the underlying devices as hard disks, and thus are rarely optimized for SSDs. In this paper, we focus on a classical research issue, the cache replacement policy, and design a new policy by taking the parallelism of SSDs into account.  A typical SSD contains several parallel channels. Some channels contain more hot data, thus are busy with read requests. The other channels may only contain cold data. So, workloads among these channels may be unbalanced. Requests issued to busy channels may take a while to address, whereas requests issued to idle channels may be served rapidly. We design a new cache replacement policy for read data, which considers the unbalanced workloads among channels. The policy gives higher priority to evicting pages from idle channels because they are more easily retrieved. On the other hand, pages from busy channels are protected. In this manner, the average latency for obtaining pages is reduced. However, SSDs are black boxes, with operating systems are blind to the channel from which a page comes. Therefore, we propose a simple scheme that determines whether a page is from a busy or an idle channel. The scheme monitor the page requests issued to the underlying storage device. When a page request is returned, and many page requests issued earlier than it have not been returned, the page is assumed to be from an idle channel. We compare our cache replacement policy with others via trace-driven simulations. The performance is measured in terms of average response time. Comparison candidates include LRU, ARC and the policy adopted by Linux. The experimental results show that our policy outperforms the other policies on most traces when workloads among channels are unbalanced.	algorithm;arc (programming language);black box;cpu cache;disk storage;hard disk drive;input/output;linux;non-volatile memory;operating system;parallel computing;precondition;raid;requests;response time (technology);sac;simulation;solid-state drive;tracing (software);unbalanced circuit;while	Zhiguang Chen;Nong Xiao;Fang Liu	2012		10.1145/2367589.2367598	parallel computing;real-time computing;page cache;computer hardware;cache;computer science;adaptive replacement cache	OS	-12.714823903806534	53.6859277849045	112968
7457d9f784ccddded1104d93f9e5b5cee41805a7	tiptop: hardware performance counters for the masses	cache storage;radiation detectors monitoring measurement linux benchmark testing hardware computer architecture;performance evaluation;measurement;tool;pmu;radiation detectors;performance;program compilers cache storage computerised monitoring linux multiprocessing systems performance evaluation;analysis tool;computer architecture;tool performance pmu hardware counters;monitoring;command line tool tiptop hardware performance monitoring counters computing system quality improvement compilers code generation ipc instruction execution cache miss ratio multicore environment user level tool data collection linux platforms;linux;multiprocessing systems;program compilers;computerised monitoring;hardware counters;benchmark testing;hardware	Hardware performance monitoring counters have recently received a lot of attention. They have been used by diverse communities to understand and improve the quality of computing systems: for example, architects use them to extract application characteristics and propose new hardware mechanisms, compiler writers study how generated code behaves on particular hardware, software developers identify critical regions of their applications and evaluate design choices to select the best performing implementation. In this paper, we propose that counters be used by all categories of users, in particular non-experts, and we advocate that a few simple metrics derived from these counters are relevant and useful. For example, a low IPC (number of executed instructions per cycle) indicates that the hardware is not performing at its best, a high cache miss ratio can suggest several causes, such as conflicts between processes in a multicore environment. We also introduce a new simple and flexible user-level tool that collects these data on Linux platforms, and we illustrate its practical benefits through several use cases.	cpu cache;compiler;hardware performance counter;instructions per cycle;linux;multi-core processor;software developer;user space	Erven Rohou	2012	2012 41st International Conference on Parallel Processing Workshops	10.1109/ICPPW.2012.58	benchmark;parallel computing;real-time computing;performance;computer science;operating system;programming language;particle detector;linux kernel;measurement	HPC	-7.450247009572742	47.54531425589041	113018
c152fcd52b51beda5557b02cb5abb30b8cc8e230	on the efficiency of nearest neighbor load balancing for random loads.	independent random variables;nearest neighbor;load distribution;load balance	Nearest neighbor load balancing algorithms like load diiusion are popular due to their simplicity, exibility and robustness. In this paper we show that they are also asymptotically very eecient when a random rather than a worst case initial load distribution is considered. For processor loads described by independent random variables we show that diiusion needs ? (log n) 2=d time to achieve good load balance on a d-dimensional mesh or torus network with n d processors. We also argue that some but not all of the nearest neighbor algorithms known to perform better than diiusion in the worst case also perform better for random loads. In addition, we use the maximum norm for deening the quality of load balancing which has more direct implications for the execution time of the underlying application than previous deenitions. Previouly known results for worst case instances are adapted to the new quality criterion.	algorithm;best, worst and average case;central processing unit;load balancing (computing);run time (program lifecycle phase)	Peter Sanders	1996			mathematical optimization;load balancing (computing);k-nearest neighbors algorithm;computer science	Theory	-13.372592321466039	58.2555930311672	113102
a3e163c37c9abc17dbc64028876a87ed7fe578eb	fema: a fairness and efficiency caching management algorithm in shared cache	degradation;fairness;performance evaluation;efficiency;fairness improvement fema algorithm fairness and efficiency caching management algorithm shared cache prefetching cache concurrence paradigm prefetching algorithm concurrent heterogeneous streams fema ada framework rate aware adjustment prefetching degree partition size replacement scheme round robin allocation scheme linux kernel prefetching algorithm lkp algorithm amp algorithm adaptive multistream prefetching algorithm;prefetching;resource management;shared cache;shared cache prefetching fairness efficiency heterogeneous;heterogeneous;prefetching partitioning algorithms degradation resource management algorithm design and analysis performance evaluation equations;storage management cache storage linux operating system kernels;algorithm design and analysis;partitioning algorithms	This paper is motivated by our three key observations: (1) there exists a degradation of performance as the interleaved accesses of heterogeneous streams, (2) for the slow stream, sequential accesses suffer huge misses in the prefetching cache, (3) in concurrence paradigm, providing fairness and QoS to concurrent streams is very important which always ignored by the traditional prefetching algorithms. Therefore, we present Fema, a caching management algorithm that enforces the fairness and efficiency for concurrent heterogeneous streams. Fema focuses on three key designs: (1) An adaptive framework (Fema Ada) for prefetching. In the Fema Ada, we propose a rate-aware adjustment of prefetching degree and analysis the optimal partition size. (2) A novel replacement scheme (Fema Rep) in which the accessed data will be firstly evicted to improve the performance. (3) A round robin allocation scheme (Fema Rou) to achieve fairness while as least performance degradation as possible. Results show that Fema is able to achieve averages 81.4% performance improvement over the LRU algorithm, 53.5% over the default Linux Kernel prefetching (LKP) algorithm and 19.0% over the recently proposed practical AMP (adaptive multi-stream prefetching) algorithm. Fema achieves average 74.2% fairness improvement (metric in fair speedup) over the LKP algorithm and 56.5% over the AMP algorithm.	ada;algorithm;cpu cache;cache (computing);elegant degradation;fairness measure;hit (internet);linux;programming paradigm;round-robin scheduling;speedup;synthetic intelligence	Yong Yue Li;Dan Feng;Lingfang Zeng;Zhan Shi	2014	2014 9th IEEE International Conference on Networking, Architecture, and Storage	10.1109/NAS.2014.14	shared memory;algorithm design;parallel computing;real-time computing;degradation;computer science;resource management;operating system;database;efficiency;computer network	HPC	-10.832717875480357	51.82957613943255	113116
10a509c986d22b1c470ebca81d36aeaac109a720	specific read only data management for memory hierarchy optimization	read only data;cache;data management;memory hierarchy	The multiplication of the number of cores inside embedded systems has raised the pressure on the memory hierarchy. The cost of coherence protocol and the scalability problem of the memory hierarchy is nowadays a major issue. In this paper, a specific data management for read-only data is investigated because these data can be duplicated in several memories without being tracked. Based on analysis of standard benchmarks for embedded systems, this analysis shows that read-only data represent 62% of all the data used by applications and 18% of all the memory accesses. A specific data path for read-only data is then evaluated by using simulations. On the first level of the memory hierarchy, removing read-only data of the L1 cache and placing them in another read-only cache improve the data locality of the read-write data by 30% and decrease the total energy consumption of the first level memory by 5%.	benchmark (computing);cpu cache;cache coherence;embedded system;locality of reference;memory hierarchy;read-only memory;read-write memory;scalability;simulation	Gregory Vaumourin;Thomas Dombek;Alexandre Guerre;Denis Barthou	2014	SIGBED Review	10.1145/2724942.2724951	uniform memory access;shared memory;interleaved memory;cache-oblivious algorithm;semiconductor memory;parallel computing;real-time computing;thrashing;cache coloring;distributed memory;cpu cache;data management;cache;computer science;operating system;data hierarchy;database;overlay;flat memory model;registered memory;cache pollution;cache-only memory architecture;memory map;non-uniform memory access;memory management	HPC	-9.986947341005843	52.941055049252235	113121
e4101045f15ee8cede3b22331c48b6583644173b	gcmix: an efficient data protection scheme against the paired page interference		In multi-level cell (MLC) NAND flash memory, two logical pages are overlapped on a single physical page. Even after a logical page is programmed, the data can be corrupted if the programming of the coexisting logical page is interrupted. This phenomenon is called paired page interference.  This article proposes a novel software technique to deal with the paired page interference without any additional hardware or extra page write. The proposed technique utilizes valid pages in the victim block during garbage collection (GC) as the backup against the interference, and pairs them with incoming pages written by the host. This approach eliminates undesirable page copy to backup pages against the interference. However, such a strategy has an adverse effect on the hot/cold separation policy, which is essential to improve the efficiency of GC. To limit the downside, we devise a metric to estimate the benefit of GCMix on-the-fly so that GCMix can be adaptively utilized only when the benefit outweighs the overhead. Evaluations using synthetic and real workloads show GCMix can effectively deal with the paired page interference, reducing the write amplification factor by up to 17.5%compared to the traditional technique, while providing comparable I/O performance.	backup;durability (database systems);ftl: faster than light;flash memory;garbage collection (computer science);information privacy;input/output;interference (communication);interrupt;least significant bit;locality of reference;multi-level cell;overhead (computing);paging;synthetic intelligence	Sang-Hoon Kim;Jinhyuk Lee;Jin-Soo Kim	2017	TOS	10.1145/3149373	computer science;real-time computing;parallel computing;software;nand gate;write amplification;backup;flash memory;garbage collection;interference (wave propagation);phenomenon	Arch	-11.771727665455478	53.475464720557966	113123
2706c60d5809d4d1ab75693a56a5d8ca5afb3144	a study of slipstream processors	multi threading;perforation;chip multiprocessor;performance improvement;instruction fetching slipstream processors chip multiprocessor simultaneous multithreading slipstreaming program sequencing mechanism;microprocessor chips multi threading;simultaneous multithreading;data flow;surface mount technology world wide web multithreading clocks bandwidth operating systems hardware computer aided instruction event detection;microprocessor chips;instructions per cycle	A slipstreamprocessorreducesthe lengthof a running programby dynamicallyskippingcomputationnon-essential for correct forward progress.Theshortenedprogram runs fasteras a result,but it is speculative. Soa second, unreducedcopy of the program is run concurrently with andslightly behindthereducedcopy— leveraging a chip multiprocessor (CMP) or simultaneousmultithreading (SMT).Theshortprogrampassesits control anddataflow outcomesto the full program for checking. And as it checks the short program, the full program fetches and executesmoreefficientlydueto havinganaccuratepicture of the future. Both programsare spedup: combined,they outperform con ventional non-r edundant e xecution. We study slipstr eaming with the following k ey results. 1. A 12% average performanceimprovementis achieved by harnessingan otherwiseunused,additionalprocessor in a CMP. Slipstreamingusingtwo smallsuperscalar coresoftenachievessimilar instructions-per -cycle as one large superscalar core, but with a potentially faster clock and a mor e flexible architecture. 2. A majority of the benchmarksshowsignificantreduction in the short program (about50%). Slipstreaming usingan 8-waySMTprocessorimprovestheir performance fr om 10% to 20%. 3. For some benchmarks, including gcc, performance improvementis due to the short program resolving branch mispredictions in advance. Others benefit largely due to value predictionsfrom the short program,andtheeffectis notalwaysreproduciblebyconventional value pr ediction tables. 4. As execution bandwidth is increased,slipstreaming provides less of a performanceadvantage — unless instructionsare removed in the short programbefore they arefetched. A simpleprogramsequencingmechanism is developed to bypass instruction fetc hing.	benchmark (computing);branch predictor;central processing unit;gnu compiler collection;instructions per cycle;multi-core processor;multiprocessing;naruto shippuden: clash of ninja revolution 3;speculative execution;superscalar processor	Zachary Purser;Karthik Sundaramoorthy;Eric Rotenberg	2000		10.1145/360128.360155	data flow diagram;computer architecture;parallel computing;real-time computing;multithreading;computer science;operating system;simultaneous multithreading;instructions per cycle	Arch	-8.162161070180245	51.03790550546338	113165
194d8ac037b10bb9230097e455b71c50ea5221d0	gpu-based parallel edf-schedulability analysis of multi-modal real-time systems	parallel computing;gpu;schedulability analysis;real time systems graphics processing units hardware algorithm design and analysis clustering algorithms parallel algorithms;graphics processing units;multi modal systems;parallel computing real time systems multi modal systems edf schedulability analysis gpu;clustering algorithms;edf;algorithm design and analysis;hardware;real time systems;parallel algorithms	Real-time multi-modal systems are useful in modeling embedded systems that dynamically change computational requirements over time (e.g., adaptive cruise control systems). For meeting timing constraints of such multi-modal systems, Earliest-Deadline-First (EDF) is an attractive real-time scheduling algorithm due to its optimality on uniprocessor platforms. However, checking EDF-schedulability of a real-time multi-modal system is a difficult problem that requires substantial computational effort. Today's cost efficient and massively parallel GPU platforms can be effectively leveraged to solve this difficult problem. Existing algorithms for EDF-schedulability of real-time multi-modal systems cannot exploit the entire computational power of a GPU, therefore, in this research, we develop a parallel algorithm leveraging the advantages of a GPU device. Experimental results establish the superior performance of our proposed algorithm upon a low end GPU over the implementation of existing algorithms on a cluster of computers using either MPI or OpenMP. In addition to performance, our proposed algorithm is a cost effective and power efficient alternative against comparable algorithms for multi-core and parallel computing platforms.	computer;control system;cost efficiency;earliest deadline first scheduling;embedded system;graphics processing unit;message passing interface;mobile device;modal logic;multi-agent system;multi-core processor;multimodal interaction;openmp;parallel algorithm;parallel computing;real-time clock;real-time operating system;requirement;simd;scheduling (computing);scheduling analysis real-time systems;shared memory;speedup;uniprocessor system	Masud Ahmed;Safraz Rampersaud;Nathan Fisher;Daniel Grosu;Loren Schwiebert	2013	2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing	10.1109/HPCC.and.EUC.2013.45	algorithm design;parallel computing;real-time computing;computer science;operating system;analysis of parallel algorithms;distributed computing;parallel algorithm;cluster analysis;cost efficiency	Embedded	-4.764059564180948	46.63800261032247	113188
1987e0559eed282304dc5056ea04443a5031829a	self-adaptive gossip policies for distributed population-based algorithms	cluster computing;agent modeling;p2p;parallel systems;experimental evaluation;p2p networks	Gossipping has demonstrate to be an efficient mechanism for spreading information among P2P networks. Within the context of P2P computing, we propose the so-called Evolvable Agent Model for distributed population-based algorithms which uses gossipping as communication policy, and represents every individual as a self-scheduled single thread. The model avoids obsolete nodes in the population by defining a self-adaptive refresh rate which depends on the latency and bandwidth of the network. Such a mechanism balances the migration rate to the congestion of the links pursuing global population coherence. We perform an experimental evaluation of this model on a real parallel system and observe how solution quality and algorithm speed scale with the number of processors with this seamless approach.	algorithm;central processing unit;experiment;intelligent agent;network congestion;peer-to-peer;refresh rate;scalability;seamless3d	Juan Luis Jiménez Laredo;A. E. Eiben;Marc Schoenauer;Pedro A. Castillo;Antonio Mora García;Francisco Fernández de Vega;Juan Julián Merelo Guervós	2007	CoRR		parallel computing;real-time computing;computer cluster;computer science;theoretical computer science;operating system;peer-to-peer;distributed computing	Metrics	-15.767374656998939	58.50175144153788	113480
53d02c08122d7a708f6a57298e0fe244fc7f2fa1	two-level main memory co-design: multi-threaded algorithmic primitives, analysis, and simulation	k;high bandwidth memory;two level memory;sorting	A fundamental challenge for supercomputer architecture is that processors cannot be fed data from DRAM as fast as CPUs can consume it. Therefore, many applications are memory-bandwidth bound. As the number of cores per chip increases, and traditional DDR DRAM speeds stagnate, the problem is only getting worse. A variety of non-DDR 3D memory technologies (Wide I/O 2, HBM) offer higher bandwidth and lower power by stacking DRAM chips on the processor or nearby on a silicon interposer. However, such a packaging scheme cannot contain sufficient memory capacity for a node. It seems likely that future systems will require at least two levels of main memory: high-bandwidth, low-power memory near the processor and low-bandwidth high-capacity memory further away. This near memory will probably not have significantly faster latency than the far memory. This, combined with the large size of the near memory (multiple GB) and power constraints, may make it difficult to treat it as a standard cache. In this paper, we explore some of the design space for a user-controlled multi-level main memory. We present algorithms designed for the heterogeneous bandwidth, using streaming to exploit data locality. We consider algorithms for the fundamental application of sorting. Our algorithms asymptotically reduce memory-block transfers under certain architectural parameter settings. We use and extend Sandia National Laboratories' SST simulation capability to demonstrate the relationship between increased bandwidth and improved algorithmic performance. Memory access counts from simulations corroborate predicted performance. This co-design effort suggests implementing two-level main memory systems may improve memory performance in fundamental applications.	algorithm;cpu cache;central processing unit;cluster analysis;computer data storage;direct memory access;double data rate;dynamic random-access memory;high bandwidth memory;ibm websphere extreme scale;input/output;interposer;k-means clustering;locality of reference;low-power broadcasting;multi-core processor;scratchpad memory;simulation;sorting;sorting algorithm;stacking;supercomputer architecture	Michael A. Bender;Jonathan W. Berry;Simon D. Hammond;Karl S. Hemmert;Samuel McCauley;Branden Moore;Benjamin Moseley;Cynthia A. Phillips;David S. Resnick;Arun Rodrigues	2015	2015 IEEE International Parallel and Distributed Processing Symposium	10.1016/j.jpdc.2016.12.009	uniform memory access;shared memory;interleaved memory;parallel computing;real-time computing;distributed memory;memory rank;computer hardware;telecommunications;computer science;sorting;operating system;distributed computing;overlay;redundant array of independent memory;universal memory;extended memory;flat memory model;programming language;registered memory;algorithm;computing with memory;cache-only memory architecture;memory map;memory management	Arch	-8.16568350633158	53.226733357806154	113588
8a2c288357e5813ece9a960c3557e5b1c02fb335	shedding the shackles of time-division multiplexing		Multi-core architectures pose many challenges in real-time systems, which arise from contention between concurrent accesses to shared memory. Among the available memory arbitration policies, Time Division Multiplexing (TDM) ensures a predictable behavior by bounding access latencies and guaranteed bandwidth to tasks independently from the other tasks. To do so, TDM guarantees exclusive access to the shared memory in a fixed time window. TDM, however, provides a low resource utilization as it is non-work-conserving. Besides, it is very inefficient for resources having highly variable latencies, such as sharing the access to a DRAM memory. The constant length of a TDM slot is, hence, highly pessimistic and causes an underutilization of the memory. To address these limitations, we present dynamic arbitration schemes that are based on TDM. However, instead of arbitrating at the level of TDM slots, our approach operates at the granularity of clock cycles by exploiting slack time accumulated from preceding requests. This allows the arbiter to reorder memory requests, exploit the actual access latencies of requests, and thus improve memory utilization. We demonstrate that our policies are analyzable as they preserve the guarantees of TDM in the worst case, while our experiments show an improved memory utilization on average.		Farouk Hebbache;Mathieu Jan;Florian Brandner;Laurent Pautet	2018	2018 IEEE Real-Time Systems Symposium (RTSS)	10.1109/RTSS.2018.00059	distributed computing;least slack time scheduling;time-division multiplexing;exploit;schedule;computer science;dram;arbiter;bandwidth (signal processing);shared memory	Embedded	-10.4714243408858	58.31304347436162	113774
f0fc68ff6fdf32e9a314880c54e3b765cc5886bc	the effects of parameter tuning in software thread-level speculation in javascript engines	web applications;virtual machines;speculative execution;datavetenskap datalogi;computer science;programvaruteknik;dynamic parallelization;multithreading	JavaScript is a sequential programming language that has a large potential for parallel execution in Web applications. Thread-level speculation can take advantage of this, but it has a large memory overhead. In this article, we evaluate the effects of adjusting various parameters for thread-level speculation. Our results clearly show that thread-level speculation is a useful technique for taking advantage of multicore architectures for JavaScript in Web applications, that nested speculation is required in thread-level speculation, and that the execution characteristics of Web applications significantly reduce the needed memory, the number of threads, and the depth of our speculation.	concurrent computing;javascript engine;multi-core processor;overhead (computing);parallel computing;programming language;speculative execution;speculative multithreading	Jan Kasper Martinsen;Håkan Grahn;Anders Isberg	2014	TACO	10.1145/2686036	parallel computing;web application;real-time computing;multithreading;computer science;virtual machine;operating system;speculative multithreading;speculative execution	Arch	-7.275013637826669	46.38895386629546	113821
e9bf8f75d115fa2dc08082c9730995b5563e7279	approaches to balancing data load of shared-nothing clusters and their performance comparison	performance evaluation;resource allocation;performance comparison;databases personal communication networks workstations scalability parallel processing degradation system performance finance process control hospitals;system performance;general solution;process control;distributed databases;workstation clusters;distributed databases resource allocation workstation clusters performance evaluation;parallel processing;reading and writing;data skew load balancing shared nothing clusters performance comparison workstation cluster database server high scalability parallel processing	Using a cluster of PCs or workstations or the like (called nodes) to implement the database server can bring us two great benefits: high scalability and parallel processing capability. Before such a database server can be put into actual use, however two problems have to be solved. The one is how we cope with the data-skew since it can degrade the system performance significantly. The other is how a node is connected to or disconnected from a database server without affecting the users. One general solution to both problems is to redistribute the data. Unfortunately, this would take the data offline for a long time. In fact, numerous applications such as that for reservations, finance, process control, hospitals, police, and armed forces cannot afford the offline data for any significant amount of time. We address the subject of balancing data load online, i.e., balancing data load concurrently with users' reading and writing of the database. The main contributions are an effective approach for this purpose and a comprehensive performance study of the possible alternatives.		Jiahong Wang;Yuuhei Tsutaya;Norihisa Segawa;Shinji Yamane;Yuko Murayama;Masatoshi Miyazaki;Hironobu Suzuki	2002		10.1109/ICPADS.2002.1183414	parallel processing;parallel computing;real-time computing;resource allocation;computer science;operating system;process control;database;distributed computing;computer network	HPC	-16.1748533566938	55.67643816956845	113826
45baa37d513061bf616f68d521779c2cffabbe9f	disk scheduling at compile time	disk scheduling;scheduling algorithm;solo operating system;disk allocation;program loading;rotational delay	Abstract#R##N##R##N#This paper describes a simple algorithm* for allocating program files on a disk with a moving head. The algorithm tries to place a file on consecutive pages (but will scatter them somewhat if necessary). It then rearranges these pages to minimize the rotational delay during a sequential scan of the file. This method combines the best features of consecutive and non-consecutive allocation: fast sequential access and fast allocation. It is used in the Solo operating system to reduce program loading time by a factor of 3.	compile time;i/o scheduling;schedule (project management)	Per Brinch Hansen	1976	Softw., Pract. Exper.	10.1002/spe.4380060205	parallel computing;real-time computing;computer science;operating system;scheduling;i/o scheduling	HPC	-12.820459579070212	52.14718804615252	113909
4e419b76230f3663fac3798407680c3afee812f2	effect of replacement algorithms on a paged buffer database system	database system	In a database system a buffer may be used to hold recently referenced pages. If this buffer is in virtual memory, the database paging system and the memory paging system affect its performance. The study of the effect of main memory replacement algorithms on the number of main memory page faults is the basic objective of this paper. We assume that the buffer replacement algorithm is least recently used (LRU), and page fault rates for LRU, random (R), and generalized least recently used (GLRU) main memory replacement algorithms are calculated and compared. A set of experiments validates these fault rate expressions and establishes some conditions for the practical application of the results. Introduction An important aspect of the performance analysis of a data management system (DMS) is the behavior of the buffer where previously referenced pages are held for possible future reference. If the DMS executes in a virtual memory system the buffer is in the virtual address space and, at a given time, some of the buffer pages are also in real storage. Two paging mechanisms participate in the process of accessing a database page, a DMS-controlled buffer paging and a virtual memory paging under the control of the operating system. This structure is of practical importance since it represents the operating environment for IMS and other commercial database systems, and several studies of its behavior have been made. In particular, the effect of the buffer size on performance has been evaluated analytically by using models that incorporate different assumptions [ 1-31, and also by actual measurement [ l , 2, 4, 51. While the performance of replacement algorithms has been studied analytically for virtual memory systems, for DMS only empirical evaluations have been made [ 2 , 4, 51. The databqse virtual buffer environment is different due both to the presence of two memory levels, and to the different characteristics of the reference strings found in these applications. We analyze here the effect of main memory replacement algorithms on the performance of the buffer system, when the buffer replacement algorithm is LRU (least recently used). In a demand paging environment this performance can be characterized by the number of main memory page faults. If the reference string is described by the least recently used (LRU) stack model [6], known probabilities of reference can be associated with specific positions of the stack. In non-database applications the stack probabilities usually have a decreasing characteristic with respect to stack distance, and for this case the LRU replacement algorithm has been shown to be optimal [7]. However, for database applications the stack probabilities could exhibit a different distribution, and it is shown in [8] that in general the optimal algorithm belongs to a class of which the LRU algorithm is a particular case. This optimal algorithm is denoted here as generalized LRU'(GLRU). Another important replacement algorithm is the random algorithm (R), where the page to be replaced is selected with a uniform probabili(y. These three algorithms (LRU, GLRU, and R) are considered here since they are of practical importance and possess tractable analytical characteristics. The next section presents a model of the buffer system and introduces basic concepts. Models are then developed for the three memory replacement algorithms (LRU, R, and GLRU), and expressions are presented for Copyright 1978 by International Business Machines Corporation. Copying is permitted without payment of royalty provided that (1) each reproduction is done without alteration and (2) theJournal reference and IBM copyright notice are included on the first page. The title and abstract may be used without further permission in computer-based and other information-service systems. Permission to republish other excerpts should be obtained from the Editor. IBM I . RES. DEVELOP. e VOL. 22 NO. 2 e MAR 1978 I 185 'ERNANDEZ, LANG, AND WOOD	address space;ccir system a;call stack;cobham's thesis;computer data storage;database;develop;experiment;generalized least squares;ibm i;information management system (ims);openvms;operating environment;operating system;page fault;page replacement algorithm;paging;profiling (computer programming);r language;randomized algorithm	Eduardo B. Fernández;Tomás Lang;Christopher Wood	1978	IBM Journal of Research and Development	10.1147/rd.222.0185	demand paging;parallel computing;real-time computing;page fault;memory management unit;page replacement algorithm;computer science;operating system;database;adaptive replacement cache;overlay;write buffer;flat memory model;paging	DB	-13.040940805449136	56.60691866553513	114049
4bff6f366abb3156bdecd1c26331dbb6a3f9ff74	performance of mpi parallel applications	analytical models;application software;communication model;computer networks;computational modeling;operating system;performance analysis;mathematical model;application specific processors;predictive models;user behavior;quality of service;parallel applications;hardware;analytical models mathematical model application software quality of service computational modeling computer networks hardware predictive models application specific processors performance analysis	The study of the performance of parallel applications may have different reasons. One of them is the planning of the execution architecture to guaranty a quality of service. In this case, it is also necessary to study the computer hardware, the operating system, the network and the users behavior. This paper proposes different models for those based on simulation. First results of this work show the impact of the network and the necessary precision of communication model in fine-grained parallel applications.	bottleneck (engineering);computer hardware;message passing interface;operating system;parallel computing;quality of service;run time (program lifecycle phase);simulation	Samuel Richard;Bernard Miegemolle;Jean-Marie Garcia;Thierry Monteil	2006	2006 International Conference on Software Engineering Advances (ICSEA'06)	10.1109/ICSEA.2006.58	computer architecture;application software;parallel computing;models of communication;quality of service;computer science;theoretical computer science;operating system;mathematical model;predictive modelling;computational model	HPC	-9.171841261769023	47.988239811383245	114056
facb62439709ef88f0ff4c76573a17d229941bc8	a multiprocessor with replicated shared memory	low priority;shared memory;real time;operating system;program development	A multiprocessor includes five 8086 microprocessors interconnected with replicated shared memory. Such a memory structure consists of a set of memories, one for each processor, with identical contents. This minimizes read interference since each processor simply accesses its own private copy of the shared memory. To ensure shared memory integrity, write requests transfer data over the MULTIBUS to all copies in parallel. Overall, replicated shared memory structures provide improved concurrency.  An HP 64000 Logic Development System serves as a host computer for program development and a bulk storage device. A power-on and restart monitor in shared PROM provides a run-time debug and method for down-loading the operating system and application programs. The real-time, multi-tasked operating system (called MPX) distributes a sequence of high and low priority tasks, with possible preemption, among the processors. MPX floats from processor to processor while balancing the system load for maximum concurrency and throughput.	central processing unit;concurrency (computer science);hp 64000;host (network);interference (communication);load (computing);microprocessor;multibus;multiprocessing;operating system;preemption (computing);programmable read-only memory;real-time clock;shared memory;throughput	Sigurd L. Lillevik;John L. Easterday	1983		10.1145/1500676.1500745	uniform memory access;distributed shared memory;shared memory;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;computer science;operating system;computer memory;conventional memory;extended memory;data diffusion machine;memory map;memory management	Arch	-12.225404938533744	47.73536065220917	114091
0069ca1eb7efb65c0e5eb39bb2b89855c2c59f52	static task partitioning for locked caches in multi-core real-time systems	multi core architectures;timing analysis;technical report;real time systems	Locking cache lines in hard real-time systems is a common means to ensure timing predictability of data references and to lower bounds on worst-case execution time, especially in a multi-tasking environment. Growing processing demand on multi-tasking real-time systems can be met by employing scalable multi-core architectures, like the recently introduced tile-based architectures. This paper studies the use of cache locking on massive multi-core architectures with private caches in the context of hard real-time systems. In shared cache architectures, a single resource is shared among {\em all} the tasks. However, in scalable cache architectures with private caches, conflicts exist only among the tasks scheduled on one core. This calls for a cache-aware allocation of tasks onto cores. Our work extends the cache-unaware First Fit Decreasing (FFD) algorithm with a Naive locked First Fit Decreasing (NFFD) policy. We further propose two cache-aware static scheduling schemes: (1) Greedy First Fit Decreasing (GFFD) and (2) Colored First Fit Decreasing (CoFFD). This work contributes an adaptation of these algorithms for conflict resolution of partially locked regions. Experiments indicate that NFFD is capable of scheduling high utilization task sets that FFD cannot schedule. Experiments also show that CoFFD consistently outperforms GFFD resulting in lower number of cores and lower system utilization. CoFFD reduces the number of core requirements from 30% to 60% compared to NFFD. With partial locking, the number of cores in some cases is reduced by almost 50% with an increase in system utilization of 10%. Overall, this work is unique in considering the challenges of future multi-core architectures for real-time systems and provides key insights into task partitioning with locked caches for architectures with private caches.	best, worst and average case;cpu cache;cache (computing);computer multitasking;experiment;free-form deformation;greedy algorithm;lock (computer science);multi-core processor;real-time clock;real-time computing;real-time transcription;requirement;run time (program lifecycle phase);scalability;scheduling (computing);task allocation and partitioning of social insects;worst-case execution time	Abhik Sarkar;Frank Mueller;Harini Ramaprasad	2012		10.1145/2380403.2380434	embedded system;parallel computing;real-time computing;computer science;technical report;operating system;distributed computing;static timing analysis	Embedded	-10.81280907349157	59.11797662350765	114193
5232bdf468e907010a0886a63343e54b448780c5	designing os for hpc applications: scheduling	super computers;multiprocessor scheduling;kernel;performance evaluation;publikationer;multiprocessor systems;high performance computing;processor scheduling;performance;konferensbidrag;high performance linux;hpc application;software performance;computer architecture;performance improvement;operating system;design and implementation;scheduling;high performance computer;artiklar;load management;rapporter;linux;linux kernel hardware noise load management computer architecture real time systems;multicore architecture;power consumption;operating system kernels;dual socket ibm power6;multiprocessor scheduling hpc application high performance computing multicore architecture power consumption high performance linux operating system performance optimization dual socket ibm power6;processor scheduling linux performance evaluation;high performance;performance optimization;parallel applications;multiprocessor systems operating system kernels scheduling performance super computers;noise;hardware;real time systems	Operating systems have historically been implemented as independent layers between hardware and applications. User programs communicate with the OS through a set of well defined system calls, and do not have direct access to the hardware. The OS, in turn, communicates with the underlying architecture via control registers. Except for these interfaces, the three layers are practically oblivious to each other. While this structure improves portability and transparency, it may not deliver optimal performance. This is especially true for High Performance Computing (HPC) systems, where modern parallel applications and multi-core architectures pose new challenges in terms of performance, power consumption, and system utilization. The hardware, the OS, and the applications can no longer remain isolated, and instead should cooperate to deliver high performance with minimal power consumption. In this paper we present our experience with the design and implementation of High Performance Linux (HPL), an operating system designed to optimize the performance of HPC applications running on a state-of-the-art compute cluster. We show how characterizing parallel applications through hardware and software performance counters drives the design of the OS and how including knowledge about the architecture improves performance and efficiency. We perform experiments on a dual-socket IBM POWER6 machine, showing performance improvements and stability (performance variation of 2.11% on average) for NAS, a widely used parallel benchmark suite.	benchmark (computing);control register;experiment;linux;multi-core processor;operating system;random access;scheduling (computing);software performance testing;system call	Roberto Gioiosa;Sally A. McKee;Mateo Valero	2010	2010 IEEE International Conference on Cluster Computing	10.1109/CLUSTER.2010.16	supercomputer;parallel computing;kernel;real-time computing;software performance testing;performance;computer science;noise;operating system;scheduling;multiprocessor scheduling;linux kernel	HPC	-8.936357491340031	49.43340275790416	114206
878a950afa09da335b1e53c549aff901c4c60d39	hatch: hash table caching in hardware for efficient relational join on fpga	hash table caching;accelerator fpga database hash join;xilinx ise;database;accelerator;fpga;hardware for efficient relational;conference report;pipeline processing benchmark testing dram chips field programmable gate arrays file organisation;pipeline optimized baseline hatch hash table caching fpga relational join hash join engine bram resources collision resolution ddr memory hash join operations tpc h benchmark queries;hash join;hatch;hardware field programmable gate arrays engines random access memory europe probes electronic mail	In this paper we present HATCH, a novel hash join engine. We follow a new design point which enables us to effectively cache the hash table entries in fast BRAM resources, meanwhile supporting collision resolution in hardware. HATCH enables us to have the best of two worlds: (i) to use the full capacity of the DDR memory to store complete hash tables, and (ii) by employing a cache, to exploit the high access speed of BRAMs. We demonstrate the usefulness of our approach by running hash join operations from 5 TPC-H benchmark queries and report speedups up to 2.8× over a pipeline-optimized baseline.	baseline (configuration management);benchmark (computing);cache (computing);field-programmable gate array;hash join;hash table;ibm tivoli storage productivity center;relational algebra	Behzad Salami;Oriol Arcas;Nehir Sönmez	2015	2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2015.28	hash join;embedded system;hash table;double hashing;parallel computing;real-time computing;hash function;merkle tree;sha-2;computer science;operating system;block nested loop;secure hash standard;database;hash list;hash array mapped trie;field-programmable gate array;cryptographic hash function;hash tree;hat-trie	Arch	-11.519060367903407	52.46008907506682	114275
ab75b6a4d27bee20eff85e74a9a604ac62908abd	timely recovery from task failures in non-preemptive, deadline-driven schedulers	resource constraint;preemptive scheduling;fault tolerant;real time control system timely recovery task failures deadline driven scheduler nonpreemptive scheduling resource constraint overrun detection recovery mechanism domino style manner optional execution recovery handler;meetings and proceedings;book chapter;system recovery;scheduling;transient faults;non preemptive deadline scheduling;fault tolerance;system recovery real time systems scheduling;transient fault;real time control system;scheduling problem;real time systems processor scheduling transient analysis process control context scheduling complexity theory;transient faults real time systems non preemptive deadline scheduling fault tolerance;real time systems	Although preemptive scheduling mostly dominates non-preemptive scheduling from a feasibility perspective, developers of systems with resource constraints may sometimes choose to implement the latter. Amongst the available techniques for scheduling these systems, non-preemptive EDF (npEDF) is known to be an attractive option. However as with most non-preemptive forms of scheduling, problems may still arise due to the single-tasking nature of its operation. In particular npEDF can be highly susceptible to complete system failures (‘timeline breaks’) due to errors affecting only a single task. This paper will present a simple Overrun Detection and Recovery Mechanism (ODRM) that may help to alleviate this problem, by detecting task failures in such a fashion that subsequent task deadlines are not missed in a ‘domino-style’ manner. It also allows for the optional execution of a recovery handler. The technique is applied to a case study consisting of a real-time control system for an unstable process; the paper describes initial results which indicate ODRM allows for an improved ability to tolerate task failures, and has a minimal impact on scheduling overhead.	control theory;earliest deadline first scheduling;embedded system;overhead (computing);preemption (computing);real-time control system;real-time clock;scheduling (computing);sensor;timeline	Michael Short;Imran Sheikh	2010	2010 10th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2010.319	fixed-priority pre-emptive scheduling;job shop scheduling;fault tolerance;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;distributed computing	Embedded	-9.71973938364826	59.74832297381534	114331
0bd703e6b6cab3c9bdfe03187a02bb528c3f80a6	xcalls: safe i/o in memory transactions	xcalls;system calls;concurrent programming;i o;process management;software transactional memory;concurrent programs;transactional memory	Memory transactions, similar to database transactions, allow a programmer to focus on the logic of their program and let the system ensure that transactions are atomic and isolated. Thus, programs using transactions do not suffer from deadlock. However, when a transaction performs I/O or accesses kernel resources, the atomicity and isolation guarantees from the TM system do not apply to the kernel.  The xCall interface is a new API that provides transactional semantics for system calls. With a combination of deferral and compensation, xCalls enable transactional memory programs to use common OS functionality within transactions.  We implement xCalls for the Intel Software Transactional Memory compiler, and found it straightforward to convert programs to use transactions and xCalls. In tests on a 16-core NUMA machine, we show that xCalls enable concurrent I/O and system calls within transactions. Despite the overhead of implementing transactions in software, transactions with xCalls improved the performance of two applications with poor locking behavior by 16 and 70%.	application programming interface;atomicity (database systems);compiler;database transaction;deadlock;input/output;kernel (operating system);lock (computer science);overhead (computing);programmer;software transactional memory;system call	Haris Volos;Andres Jaan Tack;Neelam Goyal;Michael M. Swift;Adam Welc	2009		10.1145/1519065.1519093	input/output;transactional memory;parallel computing;real-time computing;concurrent computing;database transaction;distributed transaction;computer science;operating system;software transactional memory;database;compensating transaction;programming language;serializability	OS	-14.746895512565153	47.720780159997524	114429
8759234f03499d4a5858d7c790b05b46d52cc369	fault-tolerant and energy-efficient communication in mixed-criticality networks-on-chips		We observe a tremendous trend towards mixed-criticality systems, where subsystems of different safety assurance level coexist and interact. In addition, embedded systems are demanded to be efficient in terms of energy consumption to achieve longer operation time with the same battery capacity. This paper introduces a novel architecture for an adaptive time-triggered communication at the chip-level, which addresses the above challenges. In the proposed architecture, time-triggered communication offers safety by establishing temporal and spatial segregation of the communication channels. In addition, adaptivity enables the communication backbone to adapt the injection time of message according to the real execution time of computational tasks, thereby decreasing the overall makespan of the application and increasing the sleep time. In addition to power saving, adaptivity helps to achieve fault recovery, as a faulty subsystem can be shut down and replaced by a backup subsystem. The proposed concept has been evaluated by an example scenario. The results exhibit that using the proposed concept, makespan of the processor and consequently the energy consumption are reduced. In addition to energy, the amount of the used memory for storing the communication schedules is also decreased.		Adele Maleki;Hamidreza Ahmadian;Roman Obermaisser	2018	2018 IEEE Nordic Circuits and Systems Conference (NORCAS): NORCHIP and International Symposium of System-on-Chip (SoC)	10.1109/NORCHIP.2018.8573469	real-time computing;architecture;energy consumption;efficient energy use;job shop scheduling;mixed criticality;fault tolerance;backup;computer science;schedule	Embedded	-6.2907580665327325	58.16724013660901	114772
1f53dbc2eef9e2277b1082f75e130fbfaf220b04	exploiting gray-box knowledge of buffer-cache management	cache replacement;cache management;replacement policy	The buffer-cache replacement policy of the OS can have a significant impact on the performance of I/Ointensive applications. In this paper, we introduce a simple fingerprinting tool, Dust, which uncovers the replacement policy of the OS. Specifically, we are able to identify how initial access order, recency of access, frequency of access, and long-term history are used to determine which blocks are replaced from the buffer cache. We show that our fingerprinting tool can identify popular replacement policies described in the literature (e.g., FIFO, LRU, LFU, Clock, Random, Segmented FIFO, 2Q, and LRU-K) as well as those found in current systems (e.g., NetBSD, Linux, and Solaris). We demonstrate the usefulness of fingerprinting the cache replacement policy by modifying a web server to use this knowledge; specifically, the web server infers the contents of the OS file cache by modeling the replacement policy under the given set of page requests. We show that by first servicing those web pages that are believed to be resident in the OS buffer cache, we can improve both average response time and throughput.	cpu cache;central processing unit;fifo (computing and electronics);fingerprint (computing);least frequently used;linux;locality of reference;netbsd;operating system;page cache;page replacement algorithm;response time (technology);scheduling (computing);server (computing);simulation;throughput;web page;web server;whole earth 'lectronic link	Nathan C. Burnett;John Bent;Andrea C. Arpaci-Dusseau;Remzi H. Arpaci-Dusseau	2002			parallel computing;database;adaptive replacement cache;cache algorithms	OS	-12.674545397327742	56.23847462391683	115110
c18115f37f1752f4232121ea2c615ff35eabc8ce	asymmetric clustering using a register cache	cluster;cache;computer;thesis;register file;power consumption;functional unit;architecture;processor;register	Conventional register files spread porting resources uniformly across all registers. This paper proposes a method called Asymmetric Clustering using a Register Cache (ACRC). ACRC utilizes a fast register cache that concentrates valuable register file ports to the most active registers thereby reducing the total register file area and power consumption. A cluster of functional units and a highly ported register cache execute the majority of instructions, while a second cluster with a full register file having fewer read ports processes instructions with source registers not found in the register cache. An ‘in-cache’ marking system tracks the contents of the register cache and routes instructions to the correct cluster. This system utilizes logic similar to the ‘ready’ bit system found in wake-up and select logic keeping the additional logic required to a minimum. When using a 256-entry register file, this design reduces the total register file area by an estimated 65% while exhibiting similar IPC performance compared to a non-clustered 8-way processor. As the feature size becomes smaller and processor clocks become faster, the number of clock cycles needed to access the register file will increase. Therefore, the smaller register file area requirement and subsequent smaller register file delay of ACRC will lead to better IPC performance than conventional processors.	algorithm;branch misprediction;central processing unit;clock signal;cluster analysis;http 404;item unique identification;load balancing (computing);overhead (computing);register file	Roger Morrison;Ben Lee;Shih-Lien Lu	2006	J. Instruction-Level Parallelism		computer architecture;parallel computing;register window;computer hardware;clock skew;control register;cache;computer science;memory buffer register;architecture;program status word;operating system;register renaming;stack register;instruction register;index register;processor register;flags register;register allocation;register file;status register;memory data register;memory address register;cluster	HPC	-6.951910075466357	53.14883359668202	115131
fefb34c47fad0a3358bbd159a35baaf23a76db56	enhanced index management for accelerating hybrid storage systems		The conventional hard disk has been the dominant database storage system for over 25 years. Recently, hybrid systems which incorporate the advantages of flash memory into the conventional hard disks are considered to be the next dominant storage systems to support databases for desktops and server computers. Their features are satisfying the requirements like enhanced data I/O, energy consumption and reduced boot time, and they are sufficient to hybrid storage systems as major database storages. However, we need to improve traditional index management schemes based on B-Tree due to the relatively slow characteristics of hard disk operations, as compared to flash memory. In order to achieve this goal, we propose a new index management scheme called HBTree. HBTree-based index management enhanced search and update performance by caching data objects in unused free area of flash leaf nodes to reduce slow hard disk I/Os in index access processes.	b-tree;booting;computer data storage;database storage structures;disk storage;flash memory;hard disk drive;hybrid system;input/output;requirement;server (computing);tree (data structure)	Siwoo Byun	2009	JCIT		computer science;operating system;computer data storage	DB	-13.280942535243714	54.22998126893806	115236
fc508898e16912aa263f82b19e935808d483a110	speeding up fpga placement: parallel algorithms and methods	parallel programming;field programmable gate arrays;multicore processor;parallel algorithms;parallel processing;transactional memory;instruction sets;tracking;parallel algorithm;simulated annealing;cpu;tls;thread level speculation;hardware;programming	Placement of a large FPGA design now commonly requires several hours, significantly hindering designer productivity. Furthermore, FPGA capacity is growing faster than CPU speed, which will further increase placement time unless new approaches are found. Multi-core processors are now ubiquitous, however, and some recent processors also have hardware support for transactional memory (TM), making parallelism an increasingly attractive approach for speeding up placement. We investigate methods to parallelize the simulated annealing placement algorithm in VPR, which is widely used in FPGA research. We explore both algorithmic changes and the use of different parallel programming paradigms and hardware, including TM, thread-level speculation (TLS) and lock-free techniques. We find that hardware TM enables large speedups (8.1x on average), but compromises “move fairness” and leads to an unacceptable quality loss. TLS scales poorly, with a maximum 2.2x speedup, but preserves quality. A new dependency checking parallel strategy achieves the best balance: the deterministic version achieves 5.9x speedup and no quality loss, while the non-deterministic, lock-free version can scale to a 34x speedup.	benchmark (computing);central processing unit;computer-aided design;elegant degradation;fairness measure;field-programmable gate array;file synchronization;image scaling;multi-core processor;non-blocking algorithm;parallel algorithm;parallel computing;programming paradigm;rollback (data management);scalability;simulated annealing;speculative execution;speculative multithreading;speedup;transactional memory	Matthew An;J. Gregory Steffan;Vaughn Betz	2014	2014 IEEE 22nd Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2014.60		Arch	-10.424739663340473	51.009863570919954	115273
70c9cb55fe499e674dc9100b28bd87a4200da67d	processor- and memory-based checkpoint and rollback recovery	virtual memory;hardware availability fault tolerant systems taxonomy nonvolatile memory databases file systems parity check codes fault detection computer crashes;storage management;virtual storage fault tolerant computing shared memory systems storage management system recovery;fault tolerant computing;shared memory systems;system recovery;transient fault;rollback recovery;memory hierarchy;processor techniques hardware based techniques rollback recovery hardware schemes uniprocessors shared memory multiprocessors distributed virtual memory systems taxonomy memory techniques memory hierarchy subtle differences transient faults processor based transparent rollback techniques;virtual storage;shared memory multiprocessor	Several hardware-based techniques that support checkpoint and rollback recovery are presented. The focus is on hardware schemes for uniprocessors, shared-memory multiprocessors, and distributed virtual-memory systems. A taxonomy for processor and memory techniques based on the memory hierarchy is presented. This provides a basis for understanding subtle differences among the various schemes. Processor-based schemes that handle transient faults by using processor-based transparent rollback techniques and memory-based schemes that roll back data instead of instructions and can be integrated with the processor techniques or can be exploited by higher levels of software are discussed.<<ETX>>	computer architecture;computer engineering;computer science;electrical engineering;fault tolerance;memory hierarchy;memory protection;operating system;parallel computing;performance evaluation;shared memory;taxonomy (general);thomas j. watson research center;transaction processing system;uniprocessor system;very-large-scale integration	Nicholas S. Bowen;Dhiraj K. Pradhan	1993	Computer	10.1109/2.191981	uniform memory access;distributed shared memory;shared memory;interleaved memory;computer architecture;semiconductor memory;parallel computing;real-time computing;page fault;distributed memory;computer science;virtual memory;operating system;memory protection;overlay;conventional memory;extended memory;flat memory model;data diffusion machine;computing with memory;cache-only memory architecture;memory map;non-uniform memory access;memory management	Arch	-11.181036433850842	48.85047035854079	115304
182e5bcf9c3152720ad0ad83f770b59f9d013f4c	alternative fetch and issue policies for the trace cache fetch mechanism	wide issue machines;logically contiguous instructions;trace cache;cache storage;instruction cache;fetch and issue policies;performance evaluation;performance comparison;physically contiguous storage;effective fetch rate;trace cache fetch mechanism;inactive issue;instruction cache fetch and issue policies trace cache fetch mechanism superscalar processors logically contiguous instructions physically contiguous storage multiple fetch blocks partial matching inactive issue effective fetch rate specint95 benchmarks trace segments performance comparison;parallel architectures;instruction sets parallel architectures cache storage performance evaluation;superscalar processors;cache storage bandwidth hardware;speculative execution;partial matching;high bandwidth fetch mechanisms;multiple fetch blocks;trace segments;bandwidth;superscalar processor;specint95 benchmarks;instruction sets;hardware	The increasing widths of superscalar processors are placing greater demands upon the fetch mechanism. The trace cache meets these demands by placing logically contiguous instructions in physically contiguous storage. It is capable of supplying multiple fetch blocks each cycle. In this paper we examine two fetch and issue techniques, partial matching and inactive issue, that improve the overall performance of the trace cache by improving the effective fetch rate. We show that for the SPECint95 benchmarks partial matching increases the overall performance by 12% and inactive issue by 15%. Furthermore we apply these two techniques to issue blocks from trace segments which contain multiple execution paths. We conclude with a performance comparison between a trace cache implementing partial matching and inactive issue and an aggressive single block fetch mechanism. The trace cache increases performance by an average of 25% over the instruction cache.	cpu cache;central processing unit;superscalar processor	Daniel H. Friendly;Sanjay J. Patel;Yale N. Patt	1997		10.1109/MICRO.1997.645794	computer architecture;parallel computing;real-time computing;computer science;operating system;instruction set;cache algorithms;bandwidth;speculative execution	Arch	-8.241774507814677	52.44144715303719	115607
086e3b3bdd58259c7b8076d059956400a559e8d9	automatic experimental analysis of communication patterns in virtual topologies	topology;virtual topology;yarn;experimental analysis;concurrent computing;pattern search;pattern analysis topology yarn visualization parallel algorithms feedback performance analysis lifting equipment concurrent computing hardware;performance tool;event tracing communication pattern virtual topology parallel wavefront scheme;power method;network topology parallel processing multiprocessing systems;performance tools;network topology;wavefront algorithms;visualization;feedback;event tracing;performance analysis;lifting equipment;pattern analysis;multiprocessing systems;virtual topologies;wavefront algorithms performance tools event tracing virtual topologies visualization;communication pattern;parallel processing;parallel wavefront scheme;hardware;parallel algorithms	Automatic pattern search in event traces is a powerful method to identify performance problems in parallel applications. We demonstrate that knowledge about the virtual topology, which defines logical adjacency relationships between processes, can be exploited to explain the occurrence of inefficiency patterns in terms of the parallelization strategy used in an application. We show correlations between higher-level events related to a parallel wavefront scheme and wait states identified by our pattern analysis. In addition, we visually expose relationships between pattern occurrences and the topological characteristics of the affected processes.	algorithm;blocking (computing);data model;instrumentation (computer programming);library (computing);parallel computing;pattern recognition;pattern search (optimization);pipeline (computing);scheme;tracing (software);wait state	Nikhil Bhatia;Fengguang Song;Felix Wolf;Jack J. Dongarra;Bernd Mohr;Shirley Moore	2005	2005 International Conference on Parallel Processing (ICPP'05)	10.1109/ICPP.2005.21	pattern search;parallel processing;parallel computing;visualization;concurrent computing;power iteration;computer science;theoretical computer science;operating system;feedback;distributed computing;parallel algorithm;network topology;experimental analysis of behavior;lifting equipment	HPC	-9.431041014096353	47.655914455338056	115626
42dfc067e5f503e48e69b9385bfcf00bdc2edc0d	performance evaluation of memory caches in multiprocessors	silicon;random access memory;shared memory;performance evaluation;multiprocessor systems;cache memory;delay large scale systems multiprocessing systems cache memory scalability nasa random access memory parallel processing silicon graphics;scalability;multiprocessing systems;nasa;parallel processing;graphics;large scale systems	Large-scale MIN-based shared-memory multiprocessor systems have long shared memory latency. Private caches can improve memory access latency but they may suf fer from the cache coherence problem and potentially lower data locality due to data sharing and multiproces sor scheduling. These two problems also increase shared memory load and may result in frequent memory stalls. In this paper, we evaluate the performance of memory caches, a cache memory placed in front of shared memory, in a large-scale multiprocessor system in the presence of pro cessor caches. The memory cache is shown to have good performance and scalability.	cas latency;cpu cache;cache (computing);cache coherence;crew scheduling;locality of reference;multiprocessing;performance evaluation;regular expression;scalability;scheduling (computing);shared memory	Yung-Chin Chen;Alexander V. Veidenbaum	1993	1993 International Conference on Parallel Processing - ICPP'93	10.1109/ICPP.1993.142	bus sniffing;uniform memory access;distributed shared memory;shared memory;parallel processing;cache coherence;interleaved memory;computer architecture;semiconductor memory;parallel computing;real-time computing;cache coloring;scalability;distributed memory;cpu cache;computer science;graphics;operating system;computer memory;conventional memory;flat memory model;silicon;registered memory;cache pollution;cache-only memory architecture;memory map;non-uniform memory access;memory management	HPC	-11.0357059364928	50.05329072817221	115768
548480beec15c9da90790ab45f9fdbf38b4d49b8	harvesting-aware power management for real-time systems with renewable energy	renewable energy sources energy conservation energy harvesting optimisation power supply circuits real time systems;energy conservation;optimisation;renewable energy;power supply circuits;renewable energy sources;energy efficient;real time;harvested energy profile harvesting aware power management renewable energy energy efficiency system performance energy harvesting real time system static scheduling technique adaptive scheduling technique dynamic voltage frequency selection timing constraint energy constraint optimization problem task slack waste minimization minimum storage capacity requirement zero deadline miss rate;dynamic voltage and frequency selection dvfs;energy dissipation;system performance;energy harvesting;embedded system;task scheduling dynamic voltage and frequency selection dvfs embedded system energy harvest power management real time;optimization problem;energy harvesting schedules real time systems timing batteries energy dissipation;storage capacity;batteries;power management;schedules;energy harvest;task scheduling;real time systems;timing	In this paper, we propose a harvesting-aware power management algorithm that targets at achieving good energy efficiency and system performance in energy harvesting real-time systems. The proposed algorithm utilizes static and adaptive scheduling techniques combined with dynamic voltage and frequency selection to achieve good system performance under timing and energy constraints. In our approach, we simplify the scheduling and optimization problem by separating constraints in timing and energy domains. The proposed algorithm achieves improved system performance by exploiting task slack with dynamic voltage and frequency selection and minimizing the waste on harvested energy. Experimental results show that the proposed algorithm improves the system performance in deadline miss rate and the minimum storage capacity requirement for zero deadline miss rate. Comparing to the existing algorithms, the proposed algorithm achieves better performance in terms of the deadline miss rate and the minimum storage capacity under various settings of workloads and harvested energy profiles.	algorithm;dynamic voltage scaling;mathematical optimization;optimization problem;power management;real-time clock;real-time computing;real-time transcription;scheduling (computing);selection algorithm;slack variable;speedup	Shaobo Liu;Jun Lu;Qing Wu;Qinru Qiu	2012	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2011.2159820	renewable energy;embedded system;real-time computing;computer science;engineering;computer performance	Embedded	-5.209007983500753	58.68364025133164	115879
aaf52a116479a0d4c8e9ab612fe9c4ff5516f3be	computing load aware and long-view load balancing for cluster storage systems	processor scheduling;storage management cloud computing resource allocation;data locality;data locality computing cluster data allocation load balancing;servers;computational modeling;computing cluster;data allocation;data block transmission method computing load aware load balancing long view load balancing cluster storage system large scale computing cluster trace analysis calv;load management;facebook;load balancing;servers load management delays facebook computational modeling load modeling processor scheduling;load modeling;delays	In large-scale computing clusters, when the server storing a task's input data does not have sufficient computing capacity, current job schedulers either schedule the task and transmit the input data to the closest server or let the task wait until the server has sufficient computing capacity, which generates network load or task delay. To handle this problem, load balancing methods are needed to reduce the number of overloaded servers due to computing workloads. However, current load balancing methods either do not consider the computing workload or assume that it is proportional to the number of data blocks in a server. Through trace analysis, we demonstrate the diversity of computing workloads of different tasks and the necessity of balancing the computing workloads among servers. Then, we propose a cost-efficient Computing load Aware and Long-View load balancing approach (CALV). In addition to the computing load awareness, CALV is also novel in that it achieves long-term load balance by migrating out data blocks from an overloaded server that contribute more computing workloads when the server is more overloaded and contribute less computing workloads when the server is more underloaded at different epochs during a time period. CALV also has a lazy data block transmission method to improve the load balanced state and avoid network load peak. Trace-driven experiments in simulation and a real computing cluster show that CALV outperforms other methods in terms of balancing the computing workloads and cost efficiency.	computer cluster;cost efficiency;epoch (reference date);experiment;ibm notes;job stream;lazy evaluation;load balancing (computing);locality of reference;microsoft research;overhead (computing);server (computing);simulation	Guoxin Liu;Haiying Shen;Haoyu Wang	2015	2015 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2015.7363754	network load balancing;round-robin dns;network load balancing services;parallel computing;real-time computing;computer science;load balancing;distributed computing;utility computing	HPC	-16.917204735866157	58.44322166526549	115964
88aafee3c59b7c25dfc708fbbf421caedbb6659a	how to speed up r code: an introduction		Most calculations performed by the average R user are unremarkable in the sense that nowadays, any computer can crush the related code in a matter of seconds. But more and more often, heavy calculations are also performed using R, something especially true in some fields such as statistics. The user then faces total execution times of his codes that are hard to work with: hours, days, even weeks. In this paper, how to reduce the total execution time of various codes will be shown and typical bottlenecks will be discussed. As a last resort, how to run your code on a cluster of computers (most workplaces have one) in order to make use of a larger processing power than the one available on an average computer will also be discussed through two examples.	code;computer;r language;run time (program lifecycle phase)	Nathan Uyttendaele	2015	CoRR		mathematical optimization;mathematics;real-time computing;speedup	Arch	-7.130801210579006	47.030998922833625	115979
26fe519e983f86f509d6103a049dbe1a62cb830e	paging on an object-oriented personal computer	priority queueing networks;virtual memory;personal computer;programming environment;mean value analysis;multiclass queueing networks;error analysis;phase transition;object oriented;approximate solutions;smalltalk;product form solutions;short period;high performance;steady state	A high-performance personal computing environment must avoid perceptible pauses resulting from many page faults within a short period of time. Our performance goals for a paged virtual memory system for the Smalltalk-80TM@@@@; programming environment are both to decrease the average page fault rate and to minimize the pauses caused by clusters of page faults. We have applied program restructuring techniques to the Smalltalk-80 object memory in order to improve the locality of reference. The analysis in this paper considers the clustering of page faults over time and distinguishes between steady-state behavior and phase transitions. We compare the effectiveness of different restructuring strategies in reducing the amount of main memory needed to obtain desired levels of performance.	cluster analysis;computer data storage;integrated development environment;locality of reference;openvms;page fault;paging;personal computer;smalltalk;steady state	Ricki Blau	1983		10.1145/800040.801394	phase transition;mean value analysis;demand paging;real-time computing;page fault;simulation;page replacement algorithm;computer science;virtual memory;operating system;distributed computing;object-oriented programming;steady state;statistics;computer network	HPC	-13.837131346361932	50.641621478769885	116099
892baa6d8b88ab99d81af0d0909b0da9df8aa6c2	managing performance analysis with dynamic statistical projection pursuit	project management performance analysis measurement instruments monitoring contracts computer applications application software runtime time varying systems	Computer systems and applications are growing more complex. Consequently, performance analysis has become more difficult due to the complex, transient interrelationships among runtime components. To diagnose these types of performance issues, developers must use detailed instrumentation to capture a large number of performance metrics. Unfortunately, this instrumentation may actually influence the performance analysis, leading the developer to an ambiguous conclusion. In this paper, we introduce a technique for focussing a performance analysis on interesting performance metrics. This technique, called dynamic statistical projection pursuit, identifies interesting performance metrics that the monitoring system should capture across some number of processors. By reducing the number of performance metrics, projection pursuit can limit the impact of instrumentation on the performance of the target system and can reduce the volume of performance data.	profiling (computer programming)	Jeffrey S. Vetter;Daniel A. Reed	1999		10.1109/SC.1999.10028	real-time computing;simulation;computer science;operations management	HPC	-17.76116448938093	47.98359534817266	116298
3e7787eaa3a405e50456f5686a310a88fbb13c8c	cam-based retention-aware dram (cra-dram) for refresh power reduction	cam;refresh;retention time;dram;power	As the main component for modern main memory system, DRAM stores data by capacitors, which must be refreshed periodically to keep the charges. As the size and speed of DRAM devices continue to increase, the overhead of refresh has caused a great power and performance dissipation. In this paper, we proposed a CAM (content-addressable memory)-based Retention-Aware DRAM (CRA-DRAM) system, a hardware implementation that uses CAM and RAM to locate and replace the leaky cells at the IO granularity. Then the entire DRAM is refreshed at a much lower rate. With IO-granularity address of leaky cells stored in CAM at the profiling stage, each access address to CRA-DRAM would be searched to determine where the data are read from or written to. We proved the IO-granularity data replacement technique is completely compatible with the JEDEC standard. The experimental results show that when the refresh period is increased by 6, CRA-DRAM has a 82.5% refresh reduction, an average DRAM energy reduction of 29.1% and an average system performance improvement of 8.3%. Without modification to memory controller, OS and DRAM devices, CRA-DRAM is quite promising to be applied in DIMM, HBM and HMC.	cell (microprocessor);computer data storage;credit bureau;dimm;dynamic random-access memory;heart rate variability;high bandwidth memory;hybrid memory cube;memory controller;memory refresh;operating system;overhead (computing);socket.io	Yong Ye;Yuan Du;Weiliang Jing;Xiaoyun Li;Zhitang Song;Bomy Chen	2017	IEICE Electronic Express	10.1587/elex.14.20170053	cam;power;dram;physics	Arch	-8.383682387890978	54.354304683414945	116328
f92da228829d779046fdd3ab7c0df285802466ea	transactions are back---but are they the same?	multi core processor;websearch;concurrency control;concurrent programs;transactional memory;bibliotheque numerique rero doc	Transactions are back in the spotlight! They are emerging in concurrent programming languages under the name of transactional memory (TM). Their new role? Concurrency control on new multi-core processors. From afar they look the same as good ol' database transactions. But are they really?  In this position paper, we reflect about the distinguishing features of these memory transactions with respect to their database cousins.  Disclaimer: By its very nature, this position paper does not try to avoid subjectivity.	central processing unit;concurrency control;concurrent computing;dbpedia;database transaction;multi-core processor;programming language;software transactional memory	Pascal Felber;Christof Fetzer;Rachid Guerraoui;Timothy L. Harris	2008	SIGACT News	10.1145/1360443.1360456	multi-core processor;optimistic concurrency control;transactional memory;parallel computing;real-time computing;computer science;concurrency control;software transactional memory;distributed computing;multiversion concurrency control;serializability	DB	-14.021167173986743	48.70906131173328	116409
61977858b3eea4f5a6d81393301e7298ade7a2d8	sdf: software-defined flash for web-scale internet storage systems	flash memory;data center;solid state drive ssd	In the last several years hundreds of thousands of SSDs have been deployed in the data centers of Baidu, China's largest Internet search company. Currently only 40\% or less of the raw bandwidth of the flash memory in the SSDs is delivered by the storage system to the applications. Moreover, because of space over-provisioning in the SSD to accommodate non-sequential or random writes, and additionally, parity coding across flash channels, typically only 50-70\% of the raw capacity of a commodity SSD can be used for user data. Given the large scale of Baidu's data center, making the most effective use of its SSDs is of great importance. Specifically, we seek to maximize both bandwidth and usable capacity.  To achieve this goal we propose {\em software-defined flash} (SDF), a hardware/software co-designed storage system to maximally exploit the performance characteristics of flash memory in the context of our workloads. SDF exposes individual flash channels to the host software and eliminates space over-provisioning. The host software, given direct access to the raw flash channels of the SSD, can effectively organize its data and schedule its data access to better realize the SSD's raw performance potential.  Currently more than 3000 SDFs have been deployed in Baidu's storage system that supports its web page and image repository services. Our measurements show that SDF can deliver approximately 95% of the raw flash bandwidth and provide 99% of the flash capacity for user data. SDF increases I/O bandwidth by 300\% and reduces per-GB hardware cost by 50% on average compared with the commodity-SSD-based system used at Baidu.	cloud storage;computer data storage;concurrency (computer science);data access;data center;database transaction;ftl: faster than light;flash memory;input/output;internet;leveldb;provisioning;random access;scheduling (computing);solid-state drive;thread (computing);web page	Jian Ouyang;Shiding Lin;Jiang Song;Zhenyu Hou;Yong Wang;Yuanzheng Wang	2014		10.1145/2541940.2541959	flash file system;data center;parallel computing;real-time computing;computer hardware;computer science;operating system	OS	-15.23193734704202	53.27480097938892	116435
d3f3455f644747de17401ac64737bfb1f5feb365	load balancing for minimizing the average response time of get operations in distributed key-value stores		We investigate the impact of an unevenly distributed load among nodes of a distributed key-value store on response times. We find that response times of “get” operations quickly degrade in the presence of power law distributions of load and identify the point, at which the system needs to apply a mitigation approach. The migration technique, which we propose, overcomes the long response times of consistent hashing placement techniques. Our technique is a hybrid approach that combines consistent hashing and a directory for exceptions. Our experimental results show an improvement in the average response times and an equal load among the nodes.	algorithm;attribute–value pair;cloud computing;consistent hashing;directory (computing);experiment;key-value database;load (computing);load balancing (computing);redis;requests;response time (technology);software deployment	Antonios Makris;Konstantinos Tserpes;Dimosthenis Anagnostopoulos;Jörn Altmann	2017	2017 IEEE 14th International Conference on Networking, Sensing and Control (ICNSC)	10.1109/ICNSC.2017.8000102	algorithm design;consistent hashing;power law;real-time computing;distributed database;response time;load balancing (computing);load management;directory;computer science;distributed computing	Embedded	-17.9870580466754	57.6758898139552	116464
17d3ab99d03039ff1d8b13e35ac1926e1bf41c6c	a figure of merit for describing the performance of scaling of parallelization		With the spread of multiand many-core processors more and more typical task is to re-implement some source code written originally for a single processor to run on more than one cores. Since it is a serious investment, it is important to decide how much efforts pays off, and whether the resulting implementation has as good performability as it could be. The Amdahl’s law provides some theoretical upper limits for the performance gain reachable through parallelizing the code, but it needs the detailed architectural knowledge of the program code, does not consider the housekeeping activity needed for parallelization and cannot tell how the actual stage of parallelization implementation performs. The present paper suggests a quantitative measure for that goal. This figure of merit is derived experimentally, from measured running times, and number of threads/cores. It can be used to quantify the used parallelization technology, the connection between the computing units, the acceleration technology under the given conditions, or the performance of the software team/compiler.	algorithm;amdahl's law;automatic parallelization;central processing unit;compiler;computer performance;experiment;fractal dimension;image scaling;manycore processor;multi-core processor;parallel computing;programmer;speedup;xfig	János Végh;Péter Molnár;József Vásárhelyi	2016	CoRR		parallel computing;real-time computing;computer science;theoretical computer science;operating system;programming language;automatic parallelization	HPC	-7.202197488672584	46.4474251795221	116506
c42b82959f79eed2c459ef27cbf7bfd9c390f54f	improving load balancing for mapreduce-based entity matching	mapreduce based implementation execution time map phase distributed entity matching task performance real cloud infrastructure optimization algorithm block slice strategy load balancing data distribution preprocessing mapreduce job entity matching search space reduction blocking techniques blockslicer big data context balanced workload distribution skewed data handling task reduction mapreduce based entity matching data assignment data intensive tasks;optimisation;load management indexes optimization data handling context programming;resource allocation;improvement mapreduce entity matching load balancing;parallel programming;big data;pattern matching;resource allocation big data cloud computing optimisation parallel programming pattern matching;cloud computing	The effectiveness and scalability of MapReduce-based implementations for data-intensive tasks depends on the data assignment made from map to reduce tasks. The robustness of this assignment strategy is crucial to achieve skewed data handling and balanced workload distribution among all reduce tasks. For the entity matching problem in the Big Data context, we propose BlockSlicer, a MapReduce-based approach that supports blocking techniques to reduce the entity matching search space. The approach utilizes a preprocessing MapReduce job to analyze the data distribution and provides an improved load balancing by applying an efficient block slice strategy as well as a well-known optimization algorithm to assign the generated match tasks. We evaluate the approach against an existing one that addresses the same problem on a real cloud infrastructure. The results show that our approach increases significantly the performance of distributed entity matching task by reducing the amount of data generated from the map phase and diminishing the overall execution time.	big data;blocking (computing);cloud computing;data-intensive computing;entity;greedy algorithm;load balancing (computing);mapreduce;mathematical optimization;microsoft outlook for mac;preprocessor;run time (program lifecycle phase);scalability	Demetrio Gomes Mestre;Carlos Eduardo S. Pires	2013	2013 IEEE Symposium on Computers and Communications (ISCC)	10.1109/ISCC.2013.6755016	real-time computing;big data;cloud computing;resource allocation;computer science;operating system;pattern matching;database;distributed computing	DB	-16.85983834337939	55.89924483450616	116537
b202ddba25f18e8baa7ce1c2f6229003c3b5edf6	hycache+: towards scalable high-performance caching middleware for parallel file systems	heterogeneous storage distributed caching parallel and distributed file systems;unix cache storage distributed databases input output programs middleware parallel machines processor scheduling storage management;throughput protocols distributed databases servers bandwidth encoding middleware;lru algorithm hycache high performance caching middleware parallel file systems computation i o gap high performance system storage resource shared networking infrastructure distributed storage middleware bisection bandwidth high speed interconnect parallel high end computing systems posix interface memory class i o throughput cached data high capacity networked attached storage caching performance global storage system 2 phase mechanism 2layer scheduling 2ls ibm blue gene p supercomputer gpfs parallel file system heuristic caching approach	The ever-growing gap between the computation and I/O is one of the fundamental challenges for future computing systems. This computation-I/O gap is even larger for modern large scale high-performance systems due to their state-of-the-art yet decades long architecture: the compute and storage resources form two cliques that are interconnected with shared networking infrastructure. This paper presents a distributed storage middleware, called HyCache+, right on the compute nodes, which allows I/O to effectively leverage the high bi-section bandwidth of the high-speed interconnect of massively parallel high-end computing systems. HyCache+ provides the POSIX interface to end users with the memory-class I/O throughput and latency, and transparently swap the cached data with the existing slow speed but high-capacity networked attached storage. HyCache+ has the potential to achieve both high performance and low cost large capacity, the best of both worlds. To further improve the caching performance from the perspective of the global storage system, we propose a 2-phase mechanism to cache the hot data for parallel applications, called 2-Layer Scheduling (2LS), which minimizes the file size to be transferred between compute nodes and heuristically replaces files in the cache. We deploy HyCache+ on the IBM Blue Gene/P supercomputer, and observe two orders of magnitude faster I/O throughput than the default GPFS parallel file system. Furthermore, the proposed heuristic caching approach shows 29X speedup over the traditional LRU algorithm.	algorithm;big data;blue gene;cache (computing);clustered file system;computation;computer data storage;data compression;heuristic;ibm gpfs;ibm websphere extreme scale;input/output;many-task computing;middleware;next-generation network;posix;paging;programming language;scalability;scheduling (computing);speedup;supercomputer;swift (programming language);task computing;testbed;throughput	Dongfang Zhao;Kan Qiao;Ioan Raicu	2014	2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing	10.1109/CCGrid.2014.11	middleware;parallel computing;real-time computing;computer science;operating system;middleware;database;distributed computing	HPC	-15.784083568357223	53.28792717598402	116711
8791454873127a5cb3f26f3e516d6539728796e1	wbsp: a novel synchronization mechanism for architecture parallel simulation	cluster parallel simulator wall clock based synchronization protocol wbsp parallel architecture simulation causality error lax synchronization many core parallel simulator;clocks;full system simulation;lax synchronization;lax synchronization cluster system parallel simulation full system simulation;computer architecture;accuracy;computational modeling;synchronization;cluster system;synchronization computational modeling accuracy computer architecture program processors delays;synchronisation causality clocks digital simulation parallel architectures protocols;program processors;parallel simulation;delays	Parallelization is an efficient approach to accelerate multi-core, multi-processor and cluster architecture simulators. Nevertheless, frequent synchronization can significantly hinder the performance of a parallel simulator. A common practice in alleviating synchronization cost is to relax synchronization using lengthened synchronous steps. However, as a side effect, simulation accuracy deteriorates considerably. Through analyzing various factors contributing to the causality error in lax synchronization, we observe that a coherent speed across all nodes is critical to achieve high accuracy. To this end, we propose wall-clock based synchronization (WBSP), a novel mechanism that uses wall-clock time to maintain a coherent running speed across the different nodes by periodically synchronizing simulated clocks with the wall clock within each lax step. Our proposed method only results in a modest precision loss while achieving performance close to lax synchronization. We implement WBSP in a many-core parallel simulator and a cluster parallel simulator. Experimental results show that at a scale of 32-host threads, it improves the performance of the many-core simulator by 4.3χ on average with less than a 5.5 percent accuracy loss compared to the conservative mechanism. On the cluster simulator with 64 nodes, our proposed scheme achieves an 8.3χ speedup compared to the conservative mechanism while yielding only a 1.7 percent accuracy loss. Meanwhile, WBSP outperforms the recent proposed adaptive mechanism on simulations that exhibit heavy traffic.	amortized analysis;automatic parallelization;benchmark (computing);causality;coherence (physics);computation;computer architecture;computer cluster;correctness (computer science);flaw hypothesis methodology;image scaling;manycore processor;mg (editor);multi-core processor;multiprocessing;nanoscale molecular dynamics;parallel computing;simulation;speedup;synchronization (computer science)	Junmin Wu;Xiaodong Zhu;Tao Li;Xiufeng Sui	2016	IEEE Transactions on Computers	10.1109/TC.2015.2439253	clock synchronization;embedded system;synchronization;parallel computing;real-time computing;telecommunications;computer science;operating system;distributed computing;accuracy and precision;data synchronization;synchronization;computational model;statistics	HPC	-15.509008885268095	48.42133794864363	116802
356d6033511ef98dacec924fe6ae1f1945b66a77	efficient management of huge data sets on cluster computers		In a cluster computer a parallel file system is encharged to spread one single parallel file on the different computeru0027s I/O nodes using a determined distribution function. In file I/O intensive parallel scientific applications with semi-random temporal parallel file I/O acess patterns, this file is accessed at different addresses at the sametime by a number of processes that may vary between two consecutive iterations. In this thesis a set of semi-random temporal parallel file I/O access generated by a phylogenetical application is categorized. For these patterns a partitioning function is proposed that guarantees at any time during execution access to the parallel file. This thesis shows the correlation existing between the type of I/O access patterns and the type and setting of two round robin based distribution functions so that the overall applicationu0027s execution time can be reduced.	computer cluster	Hipolito Vásquez Lucas	2012			parallel computing;torrent file;unix file types;versioning file system;indexed file;file system fragmentation;self-certifying file system;file control block;fork (file system);computer science	ML	-15.040208354326042	57.32566614863727	116981
7947f2fb9829f20d237682c9984adaf6d626d946	an energy-oriented evaluation of buffer cache algorithms using parallel i/o workloads	energy efficiency;cache storage;energy aware devices;memory management;high end data servers;energy efficient;power aware computing cache storage parallel machines;storage management;energy oriented evaluation;interconnections subsystems;storage hierarchies;indexing terms;i o and data communications hardware;software engineering;data communication;chip;power aware computing;clustering algorithms energy efficiency memory management energy consumption energy management supercomputers costs cooling performance gain cache storage;buffer allocation;operating system;cache replacement;cluster supercomputers;energy consumption;memory energy consumption;i o and data communications hardware main memory storage management operating systems software software engineering storage hierarchies parallel i o interconnections subsystems;buffer cache algorithms;memory energy efficiency;parallel i o workloads;performance gain;clustering algorithms;parallel machines;software software engineering;parallel i o;cluster storage;power consumption;cache replacement algorithms;buffer allocation energy oriented evaluation buffer cache algorithms parallel i o workloads power consumption cluster supercomputers memory energy efficiency high end data servers energy aware devices energy saving;free riding;supercomputers;main memory;energy saving;cooling;energy management;operating systems	Power consumption is an important issue for cluster supercomputers as it directly affects running cost and cooling requirements. This paper investigates the memory energy efficiency of high-end data servers used for supercomputers. Emerging memory technologies allow memory devices to dynamically adjust their power states and enable free rides by overlapping multiple DMA transfers from different I/O buses to the same memory device. To achieve maximum energy saving, the memory management on data servers needs to judiciously utilize these energy-aware devices. As we explore different management schemes under five real-world parallel I/O workloads, we find that the memory energy behavior is determined by a complex interaction among four important factors: (1) cache hit rates that may directly translate performance gain into energy saving, (2) cache populating schemes that perform buffer allocation and affect access locality at the chip level, (3) request clustering that aims to temporally align memory transfers from different buses into the same memory chips, and (4) access patterns in workloads that affect the first three factors.	algorithm;align (company);arc (programming language);cpu cache;cache (computing);cluster analysis;computer cooling;computer data storage;direct memory access;emulator;input/output;lirs caching algorithm;locality of reference;lászló bélády;memory management;memory-mapped i/o;page cache;parallel i/o;population;requirement;simulation;supercomputer;tracing (software)	Jianhui Yue;Yifeng Zhu;Zhao Cai	2008	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2008.109	cuda pinned memory;uniform memory access;shared memory;interleaved memory;parallel computing;real-time computing;cache coloring;computer hardware;computer science;physical address;operating system;static memory allocation;efficient energy use;extended memory;flat memory model;registered memory;cache pollution;cache-only memory architecture;non-uniform memory access;memory management	HPC	-10.436828814764661	53.74217582536561	117199
7413acead39e8292a82ce1271a01a22a18a582d0	implementation and analysis of path history in dynamic branch prediction schemes	message coalescing;branch prediction;latency tolerance;communication optimization;program slicing	In todays microprocessors, when chip density increases and more execution units with deeper pipelines are integrated into the processor, accurate branch prediction is essential for providing higher performance levels. Many recent branch predictors use branch execution history to identify repetitive branch behavior. By recording address information of recently executed branches, we can separate sequences of different branch instructions with identical execution history. The use of path history information has been recently addressed by Young and Smith [l] in static branch correlation, and by Young et el. [2] and Nair [3] in dynamic branch prediction methods. We present a method to implement path history in hardware-based branch prediction, and a comprehensive simulation study of branch prediction strategies integrating path history.	branch predictor;digital history;execution unit;microprocessor;moore's law;pipeline (computing);simulation	Shlomo Reches;Shlomo Weiss	1997		10.1145/263580.263813	program slicing;parallel computing;real-time computing;computer science;distributed computing;branch predictor	Arch	-7.557824772952269	51.570144051652406	117500
b41eabec6be27a05ef9fa71ea1b786104d200eb7	adaptive metadata rebalance in exascale file system	exascale;metadata;rebalance;distributed file system	This paper presents an effective method of metadata rebalance in exascale distributed file systems. Exponential data growth has led to the need for an adaptive and robust distributed file system whose typical architecture is composed of a large cluster of metadata servers and data servers. Though each metadata server can have an equally divided subset from the entire metadata set at first, there will eventually be a global imbalance in the placement of metadata among metadata servers, and this imbalance worsens over time. To ensure that disproportionate metadata placement will not have a negative effect on the intrinsic performance of a metadata server cluster, it is necessary to recover the balanced performance of the cluster periodically. However, this cannot be easily done because rebalancing seriously hampers the normal operation of a file system. This situation continues to get worse with both an ever-present heavy workload on the file system and frequent failures of server components at exascale. As one of the primary reasons for such a degraded performance, file system clients frequently fail to look up metadata from the metadata server cluster during the period of metadata rebalance; thus, metadata operations cannot proceed at their normal speed. We propose a metadata rebalance model that minimizes failures of metadata operations during the metadata rebalance period and validate the proposed model through a cost analysis. The analysis results demonstrate that our model supports the feasibility of online metadata rebalance without the normal operation obstruction and increases the chances of maintaining balance in a huge cluster of metadata servers.	cloud storage;clustered file system;coexist (image);computer cluster;dce distributed file system;effective method;experiment;petascale computing;robertson–seymour theorem;self-balancing binary search tree;server (computing);terminate (software);unbalanced circuit;x86 memory segmentation	Myung-Hoon Cha;Dong-Oh Kim;Hong-Yeon Kim;Young-Kyun Kim	2016	The Journal of Supercomputing	10.1007/s11227-016-1812-x	parallel computing;computer science;operating system;journaling file system;data mining;database;distributed computing;data file;world wide web;computer security;metadata repository	OS	-17.622434730054568	55.96007756558258	117646
66ede69aec0e37e0851464076e1719cd8036998e	pvfs over infiniband: design and performance evaluation	client cpu utilization;parallel virtual file system;file systems tcpip bandwidth scalability parallel processing information science supercomputers memory management software standards hardware;performance evaluation;network operating systems;memory deregistration;buffer management;buffer storage;parallel file system performance evaluation infiniband technology i o performance cluster file systems parallel virtual file system transport layer buffer management flow control buffer sharing memory registration memory deregistration client cpu utilization tcp ip;transport layer;buffer sharing;i o performance;transport protocols;memory registration;parallel architectures;file system;tcp ip;parallel file system;cluster file systems;parallel i o;workstation clusters;infiniband technology;workstation clusters buffer storage network operating systems parallel architectures performance evaluation transport protocols;flow control;buffer memories;data transfer	I/O is quickly emerging as the main bottleneck limiting performance in modern day clusters. The need for scalable parallel I/O and file systems is becoming more and more urgent. In this paper, we examine the feasibility of leveraging InfiniBand technology to improve I/O performance and scalability of cluster file systems. We use Parallel Virtual File System (PVFS) as a basis for exploring these features. In this paper, we design and implement a PVFS version on InfiniBand by taking advantage of InfiniBand features and resolving many challenging issues. We design the following: a transport layer customized for PVFS by trading transparency and generality for performance; buffer management for flow control, dynamic and fair buffer sharing, and efficient memory registration and deregistration. Compared to a PVFS implementation over standard TCP/IP on the same InfiniBand network, our implementation offers three times the bandwidth if workloads are not disk-bound and 40% improvement in bandwidth in the diskbound case. Client CPU utilization is reduced to 1.5% from 91% on TCP/IP. To the best of our knowledge, this is the first design, implementation and evaluation of PVFS over InfiniBand. The research results demonstrate how to design high performance parallel file systems on next generation clusters with InfiniBand.	central processing unit;infiniband;input/output;internet protocol suite;parallel i/o;parallel virtual file system;performance evaluation;scalability	Jiesheng Wu;Pete Wyckoff;Dhabaleswar K. Panda	2003		10.1109/ICPP.2003.1240573	parallel computing;real-time computing;computer science;operating system;transport layer;computer network	HPC	-16.857099188026133	52.18717974921545	117651
0270cc32ac9e86f8009b8a83c8f08f51f46b87ac	towards adaptive hierarchical scheduling of real-time systems	timing constraint handling real time systems resource allocation scheduling shared memory systems;resource allocation;adaptive cpu allocation method adaptive hierarchical scheduling timing constraint integration timing constraint scheduling timing constraint guarantee compositional real time systems shared cpu dynamic systems cpu utilization;computer and information science;shared memory systems;scheduling;constraint handling;real time systems feedback loop analytical models adaptation models dynamic scheduling timing;data och informationsvetenskap;real time systems;timing	Hierarchical scheduling provides a modular framework for integrating, scheduling and guaranteeing timing constraints of compositional real-time systems. In such a scheduling framework, all modules should receive a sufficient portion of the shared CPU to be able to guarantee timing constraints of their internal parts. In dynamic systems i.e., systems where the execution time of tasks are subjected to sudden and drastic changes during run-time, assigning fixed CPU portions to the modules is conducive to either low CPU utilization or numerous task deadline misses. In this paper, in order to address this problem, we propose an adaptive CPU allocation method which dynamically assigns CPU portions to the modules during runtime based on their current CPU demand. Besides, the presented approach is evaluated using a series of different simulations. In addition, we present a method for scheduling modules in situations when the CPU resource is not sufficient for scheduling all modules. We introduce the notion of module (subsystem) criticality, and in an overload situation we distribute the CPU resource based on the criticality of modules.	central processing unit;criticality matrix;dynamical system;mathematical model;multi-core processor;overhead (computing);real-time clock;real-time computing;run time (program lifecycle phase);sampling (signal processing);scheduling (computing);self-organized criticality;simulation	Nima Moghaddami Khalilzad;Thomas Nolte;Moris Behnam;Mikael Asberg	2011	ETFA2011	10.1109/ETFA.2011.6059019	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;resource allocation;computer science;rate-monotonic scheduling;operating system;two-level scheduling;stride scheduling;distributed computing;round-robin scheduling;scheduling	Embedded	-9.47885288011014	60.404911208628775	117762
c5ef7419567216fb54285a084a53445374da85cd	cache sharing management for performance fairness in chip multiprocessors	cache miss penalty;analytical models;cache storage;data intensive application;performance fairness;shared resources;fairness metrics;last level cache;chip multiprocessor;data mining;system on a chip;shared last level cache;fairness issue;delay performance analysis resource management throughput hardware parallel architectures conference management technology management computer science application software;chip;concurrently executing application;cmp systems;cache sharing management;multiprocessing systems cache storage microprocessor chips;chip multiprocessors;pipelines;resource sharing;mathematical model;cmp systems cache sharing management performance fairness chip multiprocessor resource sharing concurrently executing application chip multiprocessors shared last level cache shared resources off chip request latency data intensive application fairness issue fairness metrics cache miss penalty;multiprocessing systems;off chip request latency;microprocessor chips;hardware	Resource sharing can cause unfair and unpredictable performance of concurrently executing applications in Chip-Multiprocessors (CMP). The shared last-level cache is one of the most important shared resources because off-chip request latency may take a significant part of total execution cycles for data intensive applications. Instead of enforcing performance fairness directly, prior work addressing fairness issue of cache sharing mainly focuses on the fairness metrics of cache miss numbers or miss rates. However, because of the variation of cache miss penalty, fairness on cache miss cannot guarantee performance fairness. Cache sharing management which directly addresses performance fairness is needed for CMP systems. This paper introduces a model to analyze the performance impact of cache sharing, and proposes a mechanism of cache sharing management to provide performance fairness for concurrently executing applications. The proposed mechanism monitors the actual penalty of all cache misses and dynamically estimates the cache misses with dedicated caches when the applications are actually running with a shared cache. The estimated relative slowdown for each core from dedicated environment to shared environment is used to guide cache sharing in order to guarantee performance fairness. The experiment results show that the proposed mechanism always improves the performance fairness metric, and can provide no worse throughput than the scenario without any management mechanism.	cpu cache;cache (computing);data-intensive computing;fairness measure;operating system;priority inversion;scheduling (computing);starvation (computer science);throughput;uncontrolled format string	Xing Zhou;Wenguang Chen;Weimin Zheng	2009	2009 18th International Conference on Parallel Architectures and Compilation Techniques	10.1109/PACT.2009.40	fairness measure;chip;bus sniffing;system on a chip;shared resource;pipeline burst cache;parallel computing;real-time computing;cache coloring;cache;computer science;write-once;cache invalidation;operating system;mathematical model;pipeline transport;smart cache;cache algorithms;cache pollution	HPC	-9.347486649724303	57.481734755355966	117785
856f98b071993851e8c9a227d755600c1784582a	optimization opportunities created by global data reordering	global data access mechanism;memory reference;optimization opportunity;entire global data;global data locality;improving memory locality;global data reordering;global data area;memory access;data reordering;global data;global variable;data access;data handling;registers;feedback;interference;instruction scheduling;global variables;software pipelining;sun;operating system;resource utilization;reduced instruction set computing;frequency	Memory access has proven to be one of the bottlenecks in modern architectures. Improving memory locality and eliminating the amount of memory access can help release this bottleneck. We present a method for link-time profile-based optimization by reordering the global data of the program and modifying its code accordingly. The proposed optimization reorders the entire global data of the program, according to a representative execution rate of each instruction (or basic block) in the code. The data reordering is done in a way that enables the replacement of frequently-executed Load instructions, which reference the global data, with fast Add Immediate instructions. In addition, it tries to improve the global data locality and to reduce the total size of the global data area. The optimization was implemented into FDPR (Feedback Directed Program Restructuring), a post-link optimizer, which is part of the IBM AIX operating system for the IBM pSeries servers. Our results on SPECint2000 show a significant improvement of up to 11% (average 3%) in execution time, along with up to 97.9% (average 83%) reduction in memory references to the global variables via the global data access mechanism of the program.	aix;basic block;bottleneck (software);cooley–tukey fft algorithm;data access;global variable;linker (computing);locality of reference;mathematical optimization;operating system;run time (program lifecycle phase)	Gadi Haber;Moshe Klausner;Vadim Eisenberg;Bilha Mendelson;Maxim Gurevich	2003			data access;software pipelining;global variable;reduced instruction set computing;computer architecture;in situ resource utilization;parallel computing;real-time computing;computer science;operating system;frequency;group method of data handling;feedback;interference;processor register;instruction scheduling;programming language	HPC	-6.548958244389835	52.310247904995066	117886
3bb84c2843f0ac31c45a1589a9a5702e58490e7d	on using an automatic, autonomous and non-intrusive approach for rewriting sql queries		Database applications have become very complex, dealing with a huge volume of data and database objects. Concurrently, low query response time and high transaction throughput have emerged as mandatory requirements to be ensured by database management systems (DBMSs). Among other possible interventions regarding database performance, SQL query rewriting has been shown quite efficient. The idea is to rewrite a new SQL statement equivalent to the statement initially formulated, where the new SQL statement provides performance gains w.r.t. query response time. In this paper, we propose an automatic and non-intrusive approach for rewriting SQL queries. The proposed approach has been implemented and its behavior was evaluated in three different DBMSs using TPC-H benchmark. The results show that the proposed approach is quite efficient and can be effectively considered by database professionals.	autonomous robot;benchmark (computing);database;heuristic (computer science);ibm tivoli storage productivity center;mathematical optimization;query optimization;query plan;requirement;response time (technology);rewrite (programming);rewriting;sql;select (sql);tpc-w;throughput	Arlino H. M. de Araújo;José Maria Monteiro;José Antônio Fernandes de Macêdo;Júlio A. Tavares;Angelo Brayner;Sérgio Lifschitz	2013			programming language;sql;database;sql injection;rewriting;computer science	DB	-17.926114427416838	54.75521309360647	118091
e6a1a8582958f9b568183d9c99d1870c0e8f2027	"""correction to """"dynamic improvement of locality in virtual memory systems"""""""			locality of reference	Jean-Loup Baer;Gary R. Sager	1976	IEEE Trans. Software Eng.		data diffusion machine;flat memory model;parallel computing;real-time computing;computer science;memory management;memory map;extended memory;overlay;interleaved memory;virtual memory	Arch	-11.170485037871037	48.93563098515957	118245
577435aef6eea043987a5cb6c49266f2704cb520	implementation of a fault-tolerant real-time network-attached storage device	disk scheduling;real time;fault tolerant;quality of service;service quality;best effort	Phoenix is a fault-tolerant real-time network-attachedst orage device (NASD). Like other NASD architectures, Phoenix provides an object-base d int rface to data stored on network-attached disks. In addition, it features many fu ctionalities not available in other NASDs. Phoenix supports both best-effort reads/writ es and real-time disk read accesses required to support real-time multimedia applica tions. A standard cyclebased scan-order disk scheduling algorithm is used to provi de guaranteed disk I/O performance. Phoenix ensures data availability through a R AID5-like parity mechanism, and supports service availability by maintaining th e same level of quality of service (QoS) in event of single disk failures. Given a spare disk, Phoenix automatically reconstructs the failed disk data onto the spare disk while servicing on-going real-time clients without degradation in service quality. Phoenix speeds up this reconstruction process by dynamically maintaining addition al redundancy beyond the RAID5-style parity on the unused space left on the disks. Pho enix attempts to improve the reliability of the disk subsystem by reducing its o verall power consumption, using active prefetching techniques in conjunction with di sk low-power modes. This paper describes the design and implementation details of th e first Phoenix prototype.	algorithm;application programming interface;best-effort delivery;cpu cache;disk space;elegant degradation;execution unit;fault tolerance;i/o scheduling;input/output;linux;low-power broadcasting;network-attached secure disks;network-attached storage;object-based language;online and offline;prototype;quality of service;real-time clock;real-time computing;real-time transcription;scheduling (computing)	Ashish Raniwala;Srikant Sharma;Anindya Neogi;Tzi-cker Chiueh	2000			redundancy (engineering);network-attached storage;quality of service;spare part;data availability;real-time computing;fault tolerance;phoenix;i/o scheduling;engineering	OS	-14.00457983124052	55.91935652197306	118269
7a751b85edc7dcf279568cc5d68c5d06909cbeb6	an exploratory study on assessing the energy impact of logging on android applications	logging;energy consumption;android	Execution logs are debug statements that developers insert into their code. Execution logs are used widely to monitor and diagnose the health of software applications. However, logging comes with costs, as it uses computing resources and can have an impact on an application’s performance. Compared with desktop applications, one additional critical computing resource for mobile applications is battery power. Mobile application developers want to deploy energy efficient applications to end users while still maintaining the ability to monitor. Unfortunately, there is no previous work that study the energy impact of logging within mobile applications. This exploratory study investigates the energy cost of logging in Android applications using GreenMiner, an automated energy test-bed for mobile applications. Around 1000 versions from 24 Android applications (e.g., Calculator, FeedEx, Firefox, and VLC) were tested with logging enabled and disabled. To further investigate the energy impacting factors for logging, controlled experiments on a synthetic application were performed. Each test was conducted multiple times to ensure rigorous measurement. Our study found that although there is little to no energy impact when logging is enabled for most versions of the studied applications, about 79% (19/24) of the studied applications have at least one version that exhibit medium to large effect sizes in energy consumption when enabling and disabling logging. To further assess the energy impact of logging, we have conducted a controlled experiment with a synthetic application. We found that the rate of logging and the number of disk flushes are significant factors of energy consumption attributable to logging. Finally, we have examined the relation between the generated OS level execution logs and mobile energy consumption. In addition to the common cross-application log events relevant to garbage collection and graphics systems, some mobile applications also have workload-specific log events that are highly correlated with energy consumption. The regression models built with common log events show mixed performance. Mobile application developers do not need to worry about conservative logging (e.g., logs generated at rates of ≤ 1 message per second), as they are not likely to impact energy consumption. Logging has a negligible effect on energy consumption for most of the mobile applications tested. Although logs have been used effectively to diagnose and debug functional problems, it is still an open problem on how to leverage software instrumentation to debug energy problems.	android;desktop computer;experiment;firefox;garbage collection (computer science);graphics;login;mobile app;operating system;server log;synthetic intelligence;testbed;vlc media player	Shaiful Alam Chowdhury;Silvia Di Nardo;Abram Hindle;Zhen Ming Jiang	2017	Empirical Software Engineering	10.1007/s10664-017-9545-x	debugging;logging;computer science;efficient energy use;energy consumption;android (operating system);real-time computing;software;login;instrumentation (computer programming);embedded system	SE	-11.989866170121488	56.8401094009717	118290
ad5e6df2a4d112767a42e81b90d152d3534046ec	agile paging for efficient memory virtualization		Virtualization provides benefits for many workloads, but the overheads of virtualizing memory are still high. The cost comes from managing two levels of address translation--one in the guest virtual machine (VM) and the other in the host virtual machine monitor (VMM)--with either nested or shadow paging. This article introduces agile paging, which combines the best of both nested and shadow paging within a page walk and exceeds the performance of both techniques.	agile software development;hardware virtualization;hypervisor;memory virtualization;network switch;page table;second level address translation;shadow paging;virtual 8086 mode;virtual machine manager	Jayneel Gandhi;Mark D. Hill;Michael M. Swift	2017	IEEE Micro	10.1109/MM.2017.67	real-time computing;page replacement algorithm;flat memory model;full virtualization;demand paging;parallel computing;computer science;memory virtualization;virtual machine;virtual memory;paging	Arch	-12.037740769304767	51.719498202634476	118305
cdf6afe5dd52b8a1caf7224d94e857c7f0cf0f1d	a dynamic load balancing strategy for massively parallel computers	distributed system;dynamic load balancing;massively parallel computer;load balance	This paper describes a new load balancing algorithm, the probabilistic strategy with neighbourhood synchronization (PNS), for massively parallel computers. The proposed strategy differs from fully distributed approaches which require a high interprocessor communication overhead when the number of processors becomes large. This load balancing strategy uses only local information and takes into account the information lags in distributed systems for estimating the system load.	load balancing (computing)	Mario Cannataro;Yaroslav D. Sergeyev;Giandomenico Spezzano;Domenico Talia	1993		10.1007/3-540-56891-3_54	massively parallel;probabilistic logic;synchronization;dynamic load testing;load balancing (computing);neighbourhood (mathematics);distributed computing;computer science	HPC	-15.698932417228695	58.4897701091317	118317
123d2a0bdac0a6824785c89b031ea3153687c582	constructing application-specific memory hierarchies on fpgas	cosic;chip;technology and engineering;memory hierarchy;external memory;high performance	The high performance potential of an FPGA is not fully exploited if a design suffers a memory bottleneck. Therefore, a memory hierarchy is needed to reuse data in on-chip buffer memories and minimize the number of accesses to off-chip memory. Buffer memories not only hide the external memory latency, but can also be used to remap data and augment the on-chip bandwidth through parallel access of multiple buffers. This paper discusses the differences and similarities of memory hierarchies on processorand on FPGA-based systems and presents a step-by-step methodology to construct a memory hierarchy on an FPGA.	cas latency;computer memory;fifo (computing and electronics);field-programmable gate array;level of detail;memory hierarchy;modular design;parallel computing;von neumann architecture	Harald Devos;Jan M. Van Campenhout;Ingrid Verbauwhede;Dirk Stroobandt	2011	Trans. HiPEAC	10.1007/978-3-642-19448-1_11	chip;uniform memory access;distributed shared memory;shared memory;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;sense amplifier;memory refresh;computer hardware;telecommunications;computer science;computer memory;overlay;conventional memory;extended memory;flat memory model;registered memory;computing with memory;cache-only memory architecture;memory map;memory management	EDA	-7.220805764884285	52.42330363181807	118399
1af504778cb3aa66feb6cd418cee4cacb6314248	an experimental study of redundant array of independent ssds and filesystems	performance evaluation;media;bandwidth;linux;parallel processing;flash memories;file systems	Solid state disks (SSDs) become more and more popular in personal devices and data centers. Flash chips can be packaged in Hard disk drive (HDD) form factors and provide the same interface as HDDs. This character makes SSDs easily replace HDDs in existing storage systems. PCIe-based SSD can provide a higher I/O performance, but it is still a little expensive. This paper studies the feasibility of Redundant Arrays of Independent SSDs (RAIS) with different filesystems. We comprehensively analyze the performance of RAIS constructed by SATA SSD and PCIe SSD individually. We investigate different RAIS configurations (RAIS0, 5, 6) and filesystems under various I/O access patterns. Finally, we illustrate our serval key findings and recommendations for building RAIS.	baseline (configuration management);benchmark (computing);computer form factor;data center;disk storage;garbage collection (computer science);hard disk drive;input/output;pci express;serial ata;solid-state drive	Yuxuan Xing;Ya Feng;Songping Yu;Zhengguo Chen;Fang Liu;Nong Xiao	2016	2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2016.0017	embedded system;parallel processing;parallel computing;media;computer hardware;computer science;operating system;linux kernel;bandwidth	HPC	-14.068772321268845	52.849113955290015	118426
99723365fc9fe6960201bf9d246a90ccbb6396fa	a study of application performance with non-volatile main memory	random access memory;kernel;servers;cpu caches non volatile main memory non volatile memories storage intensive workloads fine grain tuning nvmm flash based ssd flash based hdd dram;synchronization;nonvolatile memory;bandwidth;optimization;random access storage dram chips flash memories;nonvolatile memory random access memory bandwidth synchronization servers kernel optimization	Attaching next-generation non-volatile memories (NVMs) to the main memory bus provides low-latency, byte-addressable access to persistent data that should significantly improve performance for a wide range of storage-intensive workloads. We present an analysis of storage application performance with non-volatile main memory (NVMM) using a hardware NVMM emulator that allows fine-grain tuning of NVMM performance parameters. Our evaluation results show that NVMM improves storage application performance significantly over flash-based SSDs and HDDs. We also compare the performance of applications running on realistic NVMM with the performance of the same applications running on idealized NVMM with the same performance as DRAM. We find that although NVMM is projected to have higher latency and lower bandwidth than DRAM, these difference have only a modest impact on application performance. A much larger drag on NVMM performance is the cost of ensuring data resides safely in the NVMM (rather than the volatile caches) so that applications can make strong guarantees about persistence and consistency. In response, we propose an optimized approach to flushing data from CPU caches that minimizes this cost. Our evaluation shows that this technique significantly improves performance for applications that require strict durability and consistency guarantees over large regions of memory.	bandwidth (signal processing);byte addressing;cpu cache;computer data storage;durability (database systems);dynamic random-access memory;emulator;memory bus;non-volatile memory;persistence (computer science);solid-state drive	Yiying Zhang;Steven Swanson	2015	2015 31st Symposium on Mass Storage Systems and Technologies (MSST)	10.1109/MSST.2015.7208275	parallel computing;real-time computing;computer hardware;computer science	Arch	-12.102073331644855	53.221039255891114	118522
4f2e2b9bb4f989377c5b7c4839cc6c201b8077a2	moving database systems to multicore: an auto-tuning approach	database system;multi threading;pipelines multicore processing parallel processing instruction sets optimization database systems;query processing;query run time feedback database systems auto tuning approach query parallelization multicore platform query optimizers fine granular parallelism approach query execution plans sequential optimizers multithreading qjetpack feedback directed auto tuner;multicore;database systems;query processing multiprocessing systems multi threading;query processing multicore database systems;multiprocessing systems	In the multicore era, database systems are facing new challenges to exploit parallelism and scale query performance on new processors. Taking advantage of multicore, however, is not trivial and goes far beyond inserting parallel constructs into available database system code. Varying hardware characteristics require different query parallelization strategies on each multicore platform. Query optimizers at the heart of each database system have to be reengineered, but the problem is that these optimizers are complex. In addition, optimization best practices evolved during a long-term process of research and experimentation. This paper presents a successful modular technique that does not require a major rewrite of database code from scratch. We discuss the implementation details of new fine-granular parallelism approach that can be used as an add-on to existing systems and other query optimizations. We start with query execution plans that are generated by sequential optimizers. Using multithreading, we exploit parallelism within queries and within join operators, which leverages the new performance opportunities in modern multicore hardware. Our query performance optimization is adaptive and employs QJetpack, a feedback-directed auto-tuner, in a novel way. It iteratively partitions query execution plans by detecting performance patterns that are pre-benchmarked on each platform. Then, the auto-tuner steers the application of parallel transformations based on query run-time feedback. This paper focuses on difficult scenarios with I/O-intensive join queries and shows that we can speed up query execution despite significant I/O limitations. The performance of all benchmarked queries could be improved, with low tuning overhead, on all of our multicore platforms.	add-ons for firefox;benchmark (computing);best practice;central processing unit;computer performance;database;degree of parallelism;elegant degradation;experiment;exploit (computer security);input/output;mathematical optimization;multi-core processor;multithreading (computer architecture);numerical analysis;overhead (computing);parallel computing;pitch correction;platform-specific model;rewrite (programming);sensor;sequential consistency;tv tuner card;thread (computing)	Victor Pankratius;Martin Heneka	2011	2011 International Conference on Parallel Processing	10.1109/ICPP.2011.24	multi-core processor;online aggregation;sargable;query optimization;parallel computing;query expansion;real-time computing;multithreading;database tuning;computer science;query by example;operating system;database;programming language;view;alias	DB	-12.813255701274601	48.223419508268165	118604
803e648561b7fe019e627134afe4a482cd132bb8	analytical modelling of commit-time-locking algorithms for software transactional memories		We present an analytical performance modeling approach for concurrency control algorithms in the context of Software Transactional Memories (STMs). Unlike existing approaches, we consider a realistic execution pattern where each thread alternates the execution of transactional and non-transactional code portions. Also, our model captures dynamics related to the execution of both transactional read/write memory accesses and nontransactional operations, even when they occur within transactional contexts. Further, we rely on a detailed approach explicitly capturing key parameters, such as the execution cost of transactional and non-transactional operations, as well as the cost of begin, commit and abort operations. The proposed modeling methodology is general and extensible, lending itself to be easily specialized to capture the behavior of different STM concurrency control algorithms. In this work we specialize it to model the performance of Commit-Time-Locking algorithms, which are currently used by several STM systems.	algorithm;analytical performance modeling;benchmark (computing);concurrency (computer science);concurrency control;database;execution pattern;lock (computer science);performance prediction;read-write memory;simulation;software transactional memory;transaction processing	Pierangelo di Sanzo;Francesco Quaglia;Roberto Palmieri	2010				DB	-14.019322139198149	49.41019087126267	118644
11d58357294a5dc8dbf98d6d47b64aa5245e3420	energy-efficient task synchronization for real-time systems	dynamic voltage frequency scaling;energy efficiency;protocols;synchronization protocol;frequency synchronization;frequency locking;real system design;energy efficient real time task scheduling;energy efficient;job shop scheduling;real time;energy efficiency real time systems voltage energy consumption timing frequency synchronization protocols multimedia systems delay job shop scheduling;synchronization protocol dynamic voltage frequency scaling energy efficient real time system;multimedia systems;optimization problems;optimization problem;synchronisation;power aware computing;energy efficient real time task synchronization protocols;energy consumption;system design;scheduling;frequency switching;voltage;real time system;synchronisation power aware computing protocols real time systems scheduling;task scheduling;optimization problems real time systems energy efficient real time task scheduling energy efficient real time task synchronization protocols frequency switching real system design frequency locking energy consumption;real time systems;timing;time constraint	In the past decade, energy-efficient real-time task scheduling has been widely explored in the form of various optimization problems. This paper considers energy-efficient real-time task synchronization protocols and the overhead of frequency switching in real systems design. We propose the concept of frequency locking to better manage the cost in frequency switching. To minimize the energy consumption and meet the timing constraints, algorithms are presented to assign tasks base frequencies under existing synchronization protocols which are then extended with the frequency locking concept. Finally, a series of extensive simulations is performed and a real case study is presented to evaluate the proposed methodology and obtain comparison studies using different workloads and protocols.	algorithm;best, worst and average case;blocking (computing);central processing unit;deadlock;earliest deadline first scheduling;experiment;image-line fl studio;input/output;lock (computer science);mathematical optimization;optimization problem;overhead (computing);priority ceiling protocol;priority inversion;real-time clock;real-time operating system;real-time transcription;scsi rdma protocol;scheduling (computing);simulation;stack resource policy;systems design	Ya-Shu Chen;Chuan-Yue Yang;Tei-Wei Kuo	2010	IEEE Transactions on Industrial Informatics	10.1109/TII.2010.2052056	optimization problem;embedded system;job shop scheduling;mathematical optimization;real-time computing;computer science;operating system;distributed computing;efficient energy use	Embedded	-6.6310010999933775	59.79212377038892	118701
3c840cb64b6e911db133b8ddc7992aa5a4bd854f	preemption control for energy-efficient task scheduling in systems with a dvs processor and non-dvs devices	energy efficient;earliest deadline first;real time;dynamic voltage scaling;scheduling power aware computing real time systems;power aware computing;scheduling algorithm;real time system preemption control energy efficient periodic real time task scheduling algorithm dynamic voltage scaling processor multiple nondvs system device power consumption earliest deadline first scheduling;energy consumption;scheduling;control systems energy efficiency processor scheduling voltage control energy consumption scheduling algorithm real time systems dynamic scheduling dynamic voltage scaling energy management;real time system;preemption control;earliest deadline first scheduling;power consumption;task scheduling;multiple nondvs system device;device simulation;dynamic voltage scaling processor;energy efficient periodic real time task scheduling algorithm;real time systems	In reality, peripheral devices often make a significant contribution to the power consumption of the entire system. An effective energy-efficient scheduling algorithm should consider not only the energy consumption of the processor but also the usages of devices. In this paper, we explore energy-efficient scheduling of periodic real-time tasks in a system with a dynamic-voltage-scaling (DVS) processor and multiple non-DVS system devices. We consider systems that any device used by a task remains operating while the task is active. We propose scheduling algorithms in the management of task preemption to reduce the energy consumption of devices. Simulation results show that our proposed algorithms could not only reduce the number of task preemption significantly but also minimize the energy consumption, compared to earliest-deadline-first scheduling.	activation function;algorithm;dynamic priority scheduling;dynamic voltage scaling;earliest deadline first scheduling;fixed-priority pre-emptive scheduling;image scaling;overhead (computing);peripheral;preemption (computing);real-time clock;scheduling (computing);simulation;time complexity	Chuan-Yue Yang;Jian-Jia Chen;Tei-Wei Kuo	2007	13th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA 2007)	10.1109/RTCSA.2007.56	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;parallel computing;real-time computing;earliest deadline first scheduling;real-time operating system;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;scheduling	Embedded	-5.338686780487961	58.778555916398375	118922
44122a9bfc3bd8a131f2e5028e9abc83fbc860a7	destage algorithms for disk arrays with non-volatile caches	scheduling algorithm permission disk drives maintenance availability concurrent computing jacobian matrices logic arrays protection parallel processing;nonvolatile write cache;cache storage;logic arrays;disk arrays;approximate algorithm;concurrent computing;algorithm performance;read requests;maintenance;availability;processor scheduling;raid 5 system;storage management;software performance evaluation;disk drives;instantaneous write cache occupancy;read performance;destage algorithms;protection;scheduling algorithm;redundancy;permission;high low mark;scheduling;read request servicing;read performance disk arrays nonvolatile write cache destage algorithms read requests destage scheduling raid 5 system scheduling algorithm linear threshold scheduling instantaneous write cache occupancy algorithm performance least cost scheduling high low mark transparent destages read request servicing disk utilization workload burst tolerance;disk utilization;least cost scheduling;jacobian matrices;storage management cache storage processor scheduling scheduling software performance evaluation redundancy;linear threshold scheduling;parallel processing;destage scheduling;disk array;workload burst tolerance;transparent destages	In a disk array with a nonvolatile write cache, destages from the cache to the disk are performed in the background asynchronously while read requests from the host system are serviced in the foreground. In this paper, we study a number of algorithms for scheduling destages in a RAID-5 system. We introduce a new scheduling algorithm, called linear threshold scheduling, that adaptively varies the rate of destages to disks based on the instantaneous occupancy of the write cache. The performance of the algorithm is compared with that of a number of alternative scheduling approaches such as least-cost scheduling and high/low mark. The algorithms are evaluated in terms of their effectiveness in making destages transparent to the servicing of read requests from the host, disk utilization, and their ability to tolerate bursts in the workload without causing an overflow of the write cache. Our results show that linear threshold scheduling provides the best read performance of all the algorithms compared, while still maintaining a high degree of burst tolerance. An approximate implementation of the linear-threshold scheduling algorithm is also described. The approximate algorithm can be implemented with much lower overhead, yet its performance is virtually identical to that of the ideal algorithm.	approximation algorithm;cpu cache;disk array;non-volatile memory;overhead (computing);scheduling (computing);standard raid levels	Anujan Varma;Quinn Jacobson	1995		10.1145/223982.224042	fair-share scheduling;parallel processing;cache-oblivious algorithm;parallel computing;real-time computing;earliest deadline first scheduling;page cache;disk array;concurrent computing;dynamic priority scheduling;elevator algorithm;computer science;shortest seek first;rate-monotonic scheduling;operating system;foreground-background;distributed computing;round-robin scheduling;cache algorithms;scheduling;i/o scheduling	Arch	-12.683990759300507	52.54976937086246	118975
a772a64f745c8af57ece4998827e2ea08859c417	a ram architecture for concurrent access and on-chip testing	test neighborhoods;random access memory;addressed memory cells;memory chip;chips memory devices;concurrent computing read write memory sorting distributed computing parallel processing buffer storage vehicle crash testing computer errors parallel algorithms computer architecture;integrated memory circuits;indexing terms;chip;memory access;single cell;external request;memory architecture;fault detection;random access storage integrated circuit testing integrated memory circuits memory architecture;integrated circuit testing;concurrent memory access;random access storage;hardware overhead;buffer;on chip testing;cmat;architecture computers;hardware overhead ram architecture concurrent memory access on chip testing cmat memory chip test neighborhoods buffer external request addressed memory cells performance penalty;memory computers;ram architecture;electronic equipment tests;analytical model;performance penalty	A novel RAM architecture supporting concurrent memory access and on chip testing (CMAT) is proposed. A large-capacity memory chip is decomposed into test neighborhoods (TNDs), each of which is tested independently. When there are data stored in a TND, the data are saved into a buffer before testing the TND, and the TND's contents are restored using buffered data after testing the TND. If an external request is not made to the TND, the request can be directed to the addressed memory cells. Otherwise, the buffered data can be loaded back into the TND, or the request is detoured to a corresponding buffer. By deriving an analytical model, the performance penalty and hardware overhead of the CMAT architecture are shown to be very small. >	random-access memory	Jyh-Charn Liu;Kang G. Shin	1991	IEEE Trans. Computers	10.1109/12.93748	chip;embedded system;parallel computing;index term;buffer;computer hardware;telecommunications;computer science;operating system;fault detection and isolation	Arch	-8.333102152095378	51.77839154774696	119032
b9113bcba4e11ed31e605ad9a0983ca5d4360f31	a thermal-friendly load-balancing technique for multi-core processors	thermal behavior;thermal hot spot;power gating;thermal-friendly load-balancing technique;uneven temperature distribution;multi-core processors;load-balancing techniques result;smooth temperature distribution close;load-balancing technique;different power;highest performance case;multi-core processor;hot spots;multi core processor;integrated circuit design;cost function;load balance;multicore processors;design optimization;hot spot;multicore processing	In multi-core processors there are several ways to pair a thread to a particular core. These load-balancing techniques result in a quite different power, performance and thermal behavior of the processor, specially when low- power techniques like power gating are applied to the individual cores. In this work, a load-balancing technique that provides low overhead in performance and energy with respect to the highest performance case, yet featuring a smooth temperature distribution close to the optimal scenario is presented. An uneven temperature distribution leads to thermal hot spots which affect both the reliability of the processor (by stressing some parts of the die more than others), and the cost of the processor (since the package has to be designed to handle the worst hot spot).	central processing unit;die (integrated circuit);load balancing (computing);multi-core processor;overhead (computing);power gating;round-robin dns;semiconductor intellectual property core	Enric Musoll	2008	9th International Symposium on Quality Electronic Design (isqed 2008)	10.1109/ISQED.2008.41	multi-core processor;embedded system;electronic engineering;real-time computing;computer science;engineering;operating system	Arch	-5.896233695724528	54.708704525364155	119122
62fd398f97308a043d1d5a412c0a7f47999b53ac	sprat: runtime processor selection for energy-aware computing	kernel;computer languages;system configuration;personal computer;energy efficient;data stream;power efficiency;kernel runtime engines programming switches computer languages energy consumption;runtime environment;indexing terms;runtime;hybrid computing system;runtime processor selection;power aware computing;runtime environment sprat runtime processor selection energy aware computing commodity personal computer hybrid computing system system configuration;commodity personal computer;engines;sprat;energy consumption;automatic performance tuning;graphic processing unit;energy aware computing;switches;programming;microcomputers;power aware computing microcomputers microprocessor chips;microprocessor chips	A commodity personal computer (PC) can be seen as a hybrid computing system equipped with two different kinds of processors, i.e. CPU and a graphics processing unit (GPU). Since the superiorities of GPUs in the performance and the power efficiency strongly depend on the system configuration and the data size determined at the runtime, a programmer cannot always know which processor should be used to execute a certain kernel. Therefore, this paper presents a runtime environment that dynamically selects an appropriate processor so as to improve the energy efficiency. The evaluation results clearly indicate that the runtime processor selection at executing each kernel with given data streams is promising for energy-aware computing on a hybrid computing system.	approximation;c++;cuda;central processing unit;compiler;computer graphics;graphics processing unit;loss function;mathematical optimization;norm (social);optimization problem;performance per watt;performance prediction;personal computer;program optimization;programmer;runtime system;system configuration	Hiroyuki Takizawa;Katsuto Sato;Hiroaki Kobayashi	2008	2008 IEEE International Conference on Cluster Computing	10.1109/CLUSTR.2008.4663799	embedded system;programming;parallel computing;kernel;real-time computing;index term;electrical efficiency;network switch;computer science;operating system;microcomputer;runtime verification;efficient energy use;programming language	HPC	-5.406998758976563	47.854156588223354	119139
bf8db7a66abefe6b354534ac0c46fd4a69085c4a	using moldability to improve scheduling performance of parallel jobs on computational grid	computational grid;heterogeneous computing;moldability;system performance;load sharing;parallel job;job scheduling	In a computational grid environment, a common practice is try to allocate an entire parallel job onto a single participating site. Sometimes a parallel job, upon its submission, cannot fit in any single site due to the occupation of some resources by running jobs. How the job scheduler handles such situations is an important issue which has the potential to further improve the utilization of grid resources as well as the performance of parallel jobs. This paper develops adaptive processor allocation methods based on the moldable property of parallel jobs to deal with such situations in a heterogeneous computational grid environment. The proposed methods are evaluated through a series of simulations using real workload traces. The results indicate that adaptive processor allocation methods can further improve the system performance of a load sharing computational grid.	grid computing;integrated development environment;job scheduler;job stream;scheduling (computing);simulation;tracing (software)	Kuo-Chan Huang;Po-Chi Shih;Yeh-Ching Chung	2008		10.1007/978-3-540-68083-3_14	parallel computing;real-time computing;computer science;job scheduler;operating system;distributed computing;computer performance;symmetric multiprocessor system	HPC	-16.271390682742545	59.76909596640004	119279
47d30261b6a1b4b017123f9a1b72b2da838ec529	a consensus-based fault-tolerant event logger for high performance applications		High-performance computing (HPC) systems traditionally employ rollback-recovery techniques to allow faulttolerant executions of parallel applications. Rollback-recovery based on message logging is an attractive strategy that avoids the drawbacks of coordinated checkpointing in systems with low mean-time between failures (MTBF). Most message logging protocols rely on a centralized event logger to store information (i.e., the determinants) to allow the recovery of an application process. This centralized approach, besides the obvious single point of failure problem, represents a bottleneck for the efficiency of message logging protocols. In this work, we present a fault-tolerant distributed event logger based on consensus that outperforms the centralized approach. We implemented the event logger of MPI determinants using Paxos, a prominent consensus algorithm. Our event logger inherits the Paxos properties: safety is guaranteed even if the system is asynchronous and liveness is guaranteed despite processes failures. Experimental results are reported for the performance of the distributed event logger based both on classic Paxos and parallel Paxos applied to AMG (Algebraic MultiGrid) and NAS Parallel Benchmark applications.	application checkpointing;benchmark (computing);causal filter;centralized computing;chandra–toueg consensus algorithm;data logger;experiment;fault tolerance;finite-state machine;gossip protocol;keystroke logging;liveness;mean time between failures;mg (editor);multigrid method;parallel coordinates;paxos (computer science);reliability engineering;run time (program lifecycle phase);server log;single point of failure;state machine replication;throughput	Edson Tavares De Camargo;Elias Procópio Duarte;Fernando Pedone	2017		10.1007/978-3-319-64203-1_30	single point of failure;fault tolerance;multigrid method;paxos;distributed computing;bottleneck;asynchronous communication;computer science;liveness	HPC	-18.797123982286546	49.38358812786338	119372
144e5294c5481bf83986308ddd4682d1e685647a	analysis of multithreaded architectures for parallel computing	empirical study;multithreaded architecture;parallel computer;analytical model;switching cost;markov chain	Multithreading has been proposed as an architectural strategy for tolerating latency in multiprocessors and, through limited empirical studies, shown to offer promise. This paper develops an analytical model of multithreaded processor behavior based on a small set of architectural and program parameters. The model gives rise to a large Markov chain, which is solved to obtain a formula for processor efficiency in terms of the number of threads per processor, the remote reference rate, the latency, and the cost of switching between threads. It is shown that a multithreaded processor exhibits three operating regimes: linear (efficiency is proportional to the number of threads), transition, and saturation (efficiency depends only on the remote reference rate and switch cost). Formulae for regime boundaries are derived. The model is embellished to reflect cache degradation due to multithreading, using an analytical model of cache behavior, demonstrating that returns diminish as the number threads becomes large. Predictions from the embellished model correlate well with published empirical measurements. Prescriptive use of the model under various scenarios indicates that multithreading is effective, but the number of useful threads per processor is fairly small.	cas latency;cpu cache;context switch;elegant degradation;markov chain;multithreading (computer architecture);overhead (computing);parallel computing;run-length encoding;simultaneous multithreading;thread (computing)	Rafael H. Saavedra;David E. Culler;Thorsten von Eicken	1990		10.1145/97444.97683	markov chain;parallel computing;real-time computing;computer science;operating system;distributed computing;temporal multithreading;programming language;empirical research;statistics	Arch	-9.642695181839196	49.932651094638715	119406
7622c969e598a20c4bad564c8ed98ee55de2ef3e	time-predictable memory arbitration for a java chip-multiprocessor	java bytecode;shared memory;chip multiprocessor;upper bound;memory access;worst case execution time;real world application;timing analysis;worst case ex ecution time;multiple access;java	In this paper, we propose an approach to calculate worst-case execution times (WCET) of tasks running on a homogeneous Java multiprocessor. These processors access a shared main memory. Hence, the tasks running on different CPUs may influence the execution times of each other. Therefore, we implemented a time division multiple access arbiter that divides the memory access time into equal time slots, one time slot for each CPU. This memory arbitration allows calculating upper bounds for the execution time of Java bytecodes depending on the number of CPUs, the size of the time slot, and the memory access time. A WCET analysis tool can utilize these results and generate temporal, upper bounds for application tasks. We further explore how the size of the time slot and the number of CPUs in the system influence the WCET results. Furthermore, a real-world application task is used to compare the analyzed results with measured execution times. This paper describes the timing analysis of a time-predictable Java multiprocessor with shared memory.	access time;arbiter (electronics);best, worst and average case;cas latency;central processing unit;computer data storage;java bytecode;multiprocessing;run time (program lifecycle phase);shared memory;static timing analysis;worst-case execution time	Christof Pitter	2008		10.1145/1434790.1434808	uniform memory access;distributed shared memory;shared memory;memory model;embedded system;parallel computing;real-time computing;distributed memory;java concurrency;computer science;operating system;upper and lower bounds;programming language;java;static timing analysis;worst-case execution time	Embedded	-7.577881553086515	57.601207968566904	119419
06a5b1770adc469bb5bd1098631a74753c1bc63e	delayed interrupt processing technique for reducing latency of timer interrupt in embedded linux	delayed interrupt processing technique;interrupt handling;kernel;emergency job execution delayed interrupt processing technique latency reduction embedded linux real time operating system critical section kernel source structural design interrupt management technique locking mechanism timer interrupt handler;timer interrupt handler;linux real time os interrupt handling critical section;real time;kernel source;interrupt management technique;real time os;real time operating system;arrays;embedded systems;registers;locking mechanism;operating system kernels embedded systems interrupts linux;embedded linux;interrupts;linux;operating system kernels;emergency job execution;critical section;latency reduction;structural design;real time systems;delay linux real time systems operating systems kernel embedded computing information science job shop scheduling switches disaster management	In real-time operating systems, timer interrupts are usually used for indicating when a real-time task should be started. Critical sections with interrupts disabled can, however, cause an unacceptable delay in the execution of these tasks. Existing approaches for overcoming this issue either require modifications to many places in the kernel source or introduce a new structural design. In this paper, we propose a simple interrupt management technique for reducing the latency of timer interrupts. The locking mechanism and the timer interrupt handler have been modified so that timer interrupts are not disabled during critical sections, allowing the execution of emergency jobs requested by real-time tasks. We implemented a prototype system in Linux 2.6.20. Experimental results showed that the timer interrupt latency is reduced to 2.2\% of the original Linux kernel and the latency time for executing an emergency real-time job is 27.9$\mu$s in the worst case. Comparing to existing real-time systems, the results showed that proposed technique is efficient enough for a future real-time operating system.	best, worst and average case;critical section;interrupt handler;interrupt latency;job stream;linux on embedded systems;lock (computer science);prototype;real-time clock;real-time computing;real-time operating system;real-time transcription;timer	Maobing Dai;Yutaka Ishikawa	2009	2009 International Conference on Computational Science and Engineering	10.1109/CSE.2009.221	interrupt vector table;reentrancy;embedded system;kernel;real-time computing;real-time operating system;interrupt request;interrupt handler;interrupt priority level;computer science;operating system;vectored interrupt;interrupt;watchdog timer;critical section;processor register;interrupt latency;programmable interrupt controller;linux kernel	Embedded	-9.812174781111223	59.03702046816561	119470
3882c8df2c684666436d723f5fb1a4c609022f30	performance driven cooperation between kernel and auto-tuning multi-threaded interval b&b applications	appropriate number;asynchronous multiple pool;global optimization algorithm;single program;task_uninterruptible block time;line information;dynamic number;linux operating system;multiple data;multi-threaded application;multi-threaded interval	Dynamically determining the appropriate number of threads for a multi-threaded application may lead to a higher efficiency than predetermining the number of threads beforehand. Interval branch-andbound (B&B) global optimization algorithms are typically irregular algorithms that may benefit from the use of a dynamic number of threads. The question is how to obtain the necessary on line information to decide on the number of threads. We experiment with a scheme following a SPMD (Single Program, Multiple Data) and AMP (Asynchronous Multiple Pool) model. This means that all threads execute the same code and they are consequently affected by the same types of blocked time. There exist several methods to measure the blocked time of an application. The basis for the data to be obtained is the information provided by the Linux Operating System (O.S.) for tasks: task interruptible and task uninterruptible block time. We elaborate on this, to determine new metrics allowing kernel and applications to collaborate through system calls in order to decide on the number of threads for an application.	algorithm;auto-tune;global optimization;kernel (operating system);linux;mathematical optimization;operating system;spmd;system call;thread (computing)	Juan Francisco Sanjuan-Estrada;Leocadio G. Casado;Inmaculada García;Eligius M. T. Hendrix	2012		10.1007/978-3-642-31125-3_5	thread;real-time computing;computer science;artificial intelligence;theoretical computer science;operating system;machine learning;database;distributed computing;green threads;readers–writers problem;programming language;algorithm;process	Embedded	-14.253350305392352	59.361350943062405	119526
689d55a1c048fdf037a69165d9bc1a1d7d483e68	copy rate synchronization with performance guarantees for work consolidation in storage clusters	performance guarantee;jsq;power saving;data center;majorization;high power;stochastic convexity	As storage in data centers is increasing rapidly, it has become critical to find ways to operate efficiently this important component of a data center. Often, it has been proposed to consolidate the storage workload into a subset of storage devices and shutdown the unused ones with the purpose of preserving power. In many cases storage workload consolidation requires some amount of data to be copied from one device or set to the next. While storage workload consolidation techniques focus on extending power savings with minimal penalty in the performance of a data center, less attention is paid to the process of seamlessly integrating the data copy phase into the overall storage workload consolidation technique. Specifically, in this paper, we propose an analytic framework that synchronizes the pace of copying data between two storage devices (or nodes) such that performance is maintained within predefined targets. As such, we avoid either undesired performance degradation caused by aggressively scheduling the data copy task or a slow copy process caused by conservative scheduling. We show with extensive experimentation that the framework is robust and that it provides an important step toward automating storage consolidation and high power savings.	data center;elegant degradation;robustness (computer science);scheduling (computing);semiconductor consolidation;shutdown (computing)	Feng Yan;Xenia Mountrouidou;Alma Riska;Evgenia Smirni	2011	SIGMETRICS Performance Evaluation Review	10.1145/2160803.2160866	embedded system;data center;parallel computing;real-time computing;converged storage;computer science;operating system	OS	-6.735998024089403	56.66850840826936	120022
3fc392b0ce0390826ea7c470d5e898857af8df27	dynamic command scheduling for real-time memory controllers	memory management;bismuth;timing dynamic scheduling real time systems sdram memory management bismuth heuristic algorithms;heuristic algorithms;storage management embedded systems scheduling;average case execution time dynamic command scheduling memory controller design realtime embedded systems worst case execution time wcet memory transactions average execution time transaction size back end architecture;sdram;dynamic scheduling;real time systems;timing	Memory controller design is challenging as real-time embedded systems feature an increasing diversity of real-time and non-real-time applications with variable transaction sizes. To satisfy the requirements of the applications, tight bounds on the worst-case execution time (WCET) of memory transactions must be provided to real-time applications, while the lowest possible average execution time must be given to the rest. Existing real-time memory controllers cannot efficiently achieve this goal as they either bound the WCET by sacrificing the average execution time, or are not scalable to directly support variable transaction sizes, or both. In this paper, we propose to use dynamic command scheduling, which is capable of efficiently dealing with transactions with variable sizes. The three main contributions of this paper are: 1) a back-end architecture for a real-time memory controller with a dynamic command scheduling algorithm, 2) a formalization of the timings of the memory transactions for the proposed architecture and algorithm, and 3) two techniques to bound the WCET of transactions with both fixed and variable sizes, respectively. We experimentally evaluate the proposed memory controller and compare both the worst-case and average-case execution times of transactions to a state-of-the-art semi-static approach. The results demonstrate that dynamic command scheduling outperforms the semi-static approach by 33.4% in the average case and performs at least equally well in the worst case. We also show the WCET is tight for transactions with fixed and variable sizes, respectively.	algorithm;best, worst and average case;embedded system;experiment;memory controller;real-time clock;real-time transcription;requirement;run time (program lifecycle phase);scalability;scheduling (computing);semiconductor industry;software transactional memory;worst-case execution time	Yonghui Li;Benny Akesson;Kees G. W. Goossens	2014	2014 26th Euromicro Conference on Real-Time Systems	10.1109/ECRTS.2014.18	embedded system;interleaved memory;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;two-level scheduling;bismuth;memory management	Embedded	-9.800949112013102	60.185990090468664	120025
422908c0560729373c638b5535dc266c5c7378f1	a study on asymmetric operating systems on symmetric multiprocessors	virtual machine;multiple oses environment;symmetric multiprocessors;operating system;interprocessor interrupts;interos communications;operating systems	This paper proposes a technique to achieve asymmetric multiple OSes environment for symmetric multiprocessors. The system has a host OS and guest OSes: L4 microkernel and their servers run as the host OS, and modified Linux runs as the guest OS. OS manager which is one of the servers on the host OS manages the guest OSes. Our approach avoids a lot of execution overheads and modification costs of the guest OSes because the each OS can control hardware directly without any virtualization. The results of the evaluation show that our system is much better than the existing virtual machine systems in point of the performance. In addition, a guest OS in our system requires a few amount of modification costs. Consequently, the experiments prove that our system is a practical approach for both performance and engineering cost sensitive systems.	central processing unit;elegant degradation;embedded system;experiment;hardware virtualization;l4 microkernel family;linux;operating system;server (computing);symmetric multiprocessing;virtual machine	Yu Murata;Wataru Kanda;Kensuke Hanaoka;Hiroo Ishikawa;Tatsuo Nakajima	2007		10.1007/978-3-540-77092-3_17	embedded system;real-time computing;computer science;virtual machine;operating system;distributed computing	OS	-17.61044545296367	51.684729309374625	120066
b3815cc1129a60d6d0e84380f3ed18df8cd2cca5	system software for the sciences: taming the single-user supercomputer	trademarks;system software;application software;computer graphics;graphics single user supercomputer system software operating system compilers;systems software;operating system;displays;parallel machines;program compilers;systems software computer graphics operating systems computers parallel machines program compilers;operating systems computers;supercomputers;graphics;system software supercomputers costs hardware application software graphics operating systems trademarks file systems displays;file systems;operating systems;hardware	To date, supercomputers have required considerable programming effort to achieve significant fractions of peak speed. The authors highlight the design aspects of the system software in the Titan, a single-user supercomputer, and describe how that software interacts with the hardware to provide efficient supercomputer performance without undue effort. They discuss the operating system, compilers, and graphics.<<ETX>>	compiler;multi-user;operating system;supercomputer;titan	Randy Allen;Bruce Borden;Steve Johnson;Michael Kaplan;Way Ting;Charles Wetherell	1988	Digest of Papers. COMPCON Spring 88 Thirty-Third IEEE Computer Society International Conference	10.1109/CMPCON.1988.4911	computer architecture;parallel computing;computer science;operating system;supercomputer operating systems	HPC	-8.89547195868931	46.76845315204088	120322
bd70c95866754cfdc6ef643b2efb6da4fa3c704d	online maintenance of very large random samples on flash storage	energy efficient;random sampling;data stream;data management;spectrum;sensor network;mobile phone;data storage;flash storage;random sample;sensor networks;energy consumption;biased sampling;semi random writes	"""Recent advances in flash media have made it an attractive alternative for data storage in a wide spectrum of computing devices, such as embedded sensors, mobile phones, PDA's, laptops, and even servers. However, flash media has many unique characteristics that make existing data management/analytics algorithms designed for magnetic disks perform poorly with flash storage. For example, while random (page) reads are as fast as sequential reads, random (page) writes and in-place data updates are orders of magnitude slower than sequential writes. In this paper, we consider an important fundamental problem that would seem to be particularly challenging for flash storage: efficiently maintaining a very large (100 MBs or more) random sample of a data stream (e.g., of sensor readings). First, we show that previous algorithms such as reservoir sampling and geometric file are not readily adapted to flash. Second, we propose B-FILE, an energy-efficient abstraction for flash media to store self-expiring items, and show how a B-FILE can be used to efficiently maintain a large sample in flash. Our solution is simple, has a small (RAM) memory footprint, and is designed to cope with flash constraints in order to reduce latency and energy consumption. Third, we provide techniques to maintain biased samples with a B-FILE and to query the large sample stored in a B-FILE for a subsample of an arbitrary size. Finally, we present an evaluation with flash media that shows our techniques are several orders of magnitude faster and more energy-efficient than (flash-friendly versions of) reservoir sampling and geometric file. A key finding of our study, of potential use to many flash algorithms beyond sampling, is that """"semi-random"""" writes (as defined in the paper) on flash cards are over two orders of magnitude faster and more energy-efficient than random writes."""	flash memory	Suman Nath;Phillip B. Gibbons	2008	PVLDB	10.14778/1453856.1453961	sampling;parallel computing;real-time computing;wireless sensor network;computer hardware;data management;computer science;database	DB	-13.210246988988706	54.60477154791382	120456
a90b8a73effbd0bc376ad94d530005af119cbe9e	how to allocate tasks asynchronously	asynchrony;distributed algorithms;tree data structures computational complexity distributed algorithms randomised algorithms resource allocation scheduling;randomized algorithms;deterministic algorithms distributed computing task allocation do all randomized algorithms;coin flip processing asynchronous task allocation distributed computing asynchronous process write all do all distributed algorithms to do tree concurrent data structure randomized upper bounds deterministic upper bounds randomized to do tree algorithm work complexity adaptive scheduler;resource allocation;distributed computing;randomised algorithms;tree data structures;concurrent data structures;deterministic algorithms;computational complexity;scheduling;algorithms;resource management registers complexity theory bismuth data structures measurement radiation detectors;do all;task allocation	Asynchronous task allocation is a fundamental problem in distributed computing in which p asynchronous processes must execute a set of m tasks. Also known as write-all or do-all, this problem been studied extensively, both independently and as a key building block for various distributed algorithms. In this paper, we break new ground on this classic problem: we introduce the To-Do Tree concurrent data structure, which improves on the best known randomized and deterministic upper bounds. In the presence of an adaptive adversary, the randomized To-Do Tree algorithm has O(m+p log p log2 m) work complexity. We then show that there exists a deterministic variant of the To-Do Tree algorithm with work complexity O(m+p log5 m log2 max(m, p)). For all values of m and p, our algorithms are within log factors of the O(m + p log p) lower bound for this problem. The key technical ingredient in our results is a new approach for analyzing concurrent executions against a strong adaptive scheduler. This technique allows us to handle the complex dependencies between the processes' coin flips and their scheduling, and to tightly bound the work needed to perform subsets of the tasks.	adversary (cryptography);aggregate data;binary logarithm;concurrent data structure;distributed algorithm;distributed computing;randomized algorithm;scheduling (computing);shared memory	Dan Alistarh;Michael A. Bender;Seth Gilbert;Rachid Guerraoui	2012	2012 IEEE 53rd Annual Symposium on Foundations of Computer Science	10.1109/FOCS.2012.41	asynchrony;distributed algorithm;real-time computing;resource allocation;computer science;theoretical computer science;distributed computing;tree;concurrent data structure;randomized algorithm;computational complexity theory;scheduling;algorithm	Theory	-15.889340975598907	46.58767794733598	120506
8b05280f21297c235917137a81f773b8819aa8fe	mpi+threads: runtime contention and remedies	runtime contention;mpi;threads;critical section	Hybrid MPI+Threads programming has emerged as an alternative model to the “MPI everywhere” model to better handle the increasing core density in cluster nodes. While the MPI standard allows multithreaded concurrent communication, such flexibility comes with the cost of maintaining thread safety within the MPI implementation, typically implemented using critical sections. In contrast to previous works that studied the importance of critical-section granularity in MPI implementations, in this paper we investigate the implication of critical-section arbitration on communication performance. We first analyze the MPI runtime when multithreaded concurrent communication takes place on hierarchical memory systems. Our results indicate that the mutex-based approach that most MPI implementations use today can incur performance penalties due to unfair arbitration. We then present methods to mitigate these penalties with a first-come, first-served arbitration and a priority locking scheme that favors threads doing useful work. Through evaluations using several benchmarks and applications, we demonstrate up to 5-fold improvement in performance.	benchmark (computing);critical section;lock (computer science);message passing interface;multithreading (computer architecture);mutual exclusion;thread (computing);thread safety;throughput;two-phase locking	Abdelhalim Amer;Huiwei Lu;Yanjie Wei;Pavan Balaji;Satoshi Matsuoka	2015		10.1145/2688500.2688522	thread;parallel computing;real-time computing;gang scheduling;computer science;message passing interface;operating system;distributed computing;critical section	HPC	-13.676231179796867	48.632634882472004	120523
0e0fb6a3ccbd9da9dc216913ef77d346515936c6	an efficient compiler framework for cache bypassing on gpus	kernel;instruments;performance;gpu;compiler;cache bypassing;computer architecture;performance gpu compiler cache bypassing;graphics processing units instruction sets computer architecture optimization system on chip kernel instruments;system on chip;graphics processing units;program compilers cache storage graphics processing units instruction sets parallel memories;optimization;bypass all solution cache bypassing graphics processing unit general purpose application computing power scratchpad memory on chip memory irregular memory access gpu vendor cache access global load instruction cache performance configurable cache general purpose gpu application gpu cache utilization performance metrics memory traffic shared memory design space automatic compiler framework parallel thread execution instruction set architecture nvidia gtx680 cache all solution;instruction sets	Graphics Processing Units (GPUs) have become ubiquitous for general purpose applications due to their tremendous computing power. Initially, GPUs only employ scratchpad memory as on-chip memory. Though scratchpad memory benefits many applications, it is not ideal for those general purpose applications with irregular memory accesses. Hence, GPU vendors have introduced caches in conjunction with scratchpad memory in the recent generations of GPUs. The caches on GPUs are highly-configurable. The programmer or the compiler can explicitly control cache access or bypass for global load instructions. This highly-configurable feature of GPU caches opens up the opportunities for optimizing the cache performance. In this paper, we propose an efficient compiler framework for cache bypassing on GPUs. Our objective is to efficiently utilize the configurable cache and improve the overall performance for general purpose GPU applications. In order to achieve this goal, we first characterize GPU cache utilization and develop performance metrics to estimate the cache reuses and memory traffic. Next, we present efficient algorithms that judiciously select global load instructions for cache access or bypass. Finally, we integrate our techniques into an automatic compiler framework that leverages PTX instruction set architecture. Experiments evaluation demonstrates that compared to cache-all and bypass-all solutions, our techniques can achieve considerable performance improvement.	algorithm;cpu cache;compiler;general-purpose computing on graphics processing units;graphics processing unit;programmer;scratchpad memory	Xiaolong Xie;Yun Liang;Guangyu Sun;Deming Chen	2013	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2015.2424962	bus sniffing;cuda pinned memory;system on a chip;uniform memory access;shared memory;pipeline burst cache;computer architecture;compiler;cache-oblivious algorithm;snoopy cache;parallel computing;kernel;cache coloring;page cache;cpu cache;computer hardware;performance;cache;computer science;write-once;cache invalidation;operating system;instruction set;smart cache;programming language;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;non-uniform memory access	EDA	-6.701090680336086	48.79382581975375	120938
7838b642a34664fd0a46f40129769f4d8ec0a640	an efficient, nonintrusive, log-based i/o mechanism for scientific simulations on clusters	network resources;computational modeling supercomputers libraries computer simulation rockets file systems throughput aggregates yarn neck;nonintrusive library;scientific simulations;data storage;workstation clusters parallel machines;log based i o mechanism;high i o bandwidth;parallel machines;workstation clusters;supercomputers;nonintrusive library log based i o mechanism scientific simulations high i o bandwidth data storage supercomputers workstation clusters server resources network resources;server resources	Scientific simulations are often very I/O intensive, requiring high I/O bandwidth to store the data generated by the simulation. Traditional supercomputers have specialized I/O systems with multiple I/O nodes and specialized interconnects to handle such high I/O loads. However, with the increased availability of inexpensive clusters of workstations, more and more simulations are now run on clusters. Unfortunately, cluster supercomputers are usually not very well equipped for I/O, making I/O a serious bottleneck for such applications. To address this problem, we propose log-based I/O (LBIO), an approach that can substantially increase the I/O performance of simulations on clusters by utilizing free space on the cluster's local disks to stage data on its way to remote storage. LBIO uses local disks to create a log of all I/O calls, and uses a background thread to replay the log at the rate that best utilizes the server and network resources. LBIO is implemented as an easy-to-use, non-intrusive library - a user can turn on LBIO by adding a single initialization call to the simulation code. LBIO also works with existing scientific I/O libraries like HDF, as well as collective libraries like ROMIO. Our performance studies on microbenchmarks and a real-world scientific simulation code show that LBIO can provide upto 35% improvement in I/O performance for raw I/O and over 50% for I/O through libraries like ROMIO or HDF	computer simulation;electrical connection;hierarchical data format;input/output;library (computing);server (computing);supercomputer;workstation	Soumyadeb Mitra;Rishi Rakesh Sinha;Marianne Winslett;Xiangmin Jiao	2005	2005 IEEE International Conference on Cluster Computing	10.1109/CLUSTR.2005.347041	parallel computing;computer hardware;computer science;theoretical computer science;operating system;resource;computer data storage	HPC	-15.907305500427132	52.26780954043111	121124
00caa4dea9216bec01b465f8a69d0e1becc07b7a	application crash consistency and performance with ccfs		Recent research has shown that applications often incorrectly implement crash consistency. We present the Crash-Consistent File System (ccfs), a file system that improves the correctness of application-level crash consistency protocols while maintaining high performance. A key idea in ccfs is the abstraction of a stream. Within a stream, updates are committed in program order, improving correctness; across streams, there are no ordering restrictions, enabling scheduling flexibility and high performance. We empirically demonstrate that applications running atop ccfs achieve high levels of crash consistency. Further, we show that ccfs performance under standard file-system benchmarks is excellent, in the worst case on par with the highest performing modes of Linux ext4, and in some cases notably better. Overall, we demonstrate that both application correctness and high performance can be realized in a modern file system.	benchmark (computing);best, worst and average case;correctness (computer science);crash (computing);hypervisor;linux;linux;overhead (computing);relational database;scheduling (computing);software incompatibility	Thanumalayan Sankaranarayana Pillai;Ramnatthan Alagappan;Lanyue Lu;Vijay Chidambaram;Andrea C. Arpaci-Dusseau;Remzi H. Arpaci-Dusseau	2017	TOS	10.1145/3119897	real-time computing;parallel computing;ext4;streams;computer science;correctness;file system;scheduling (computing);crash	OS	-14.500527671746969	49.357503150676486	121341
1c17c330f5ae5689c12cc0a7ee0f9f0994af635f	unification of relative time frames for digital forensics	digital forensics;computer time stamps;computer forensics;model of computer clocks;digital events;model of computation	As digital investigations begin to involve more independent sources of digital timing information, it is becoming increasingly difficult to compare and correlate time stamps that reference these different time sources. Unless the time sources are synchronised to a reliable time source, then at any one moment, each time source can give a different time reading due to a number of different factors. A clock model is presented that can account for these factors and can simulate the behaviour of each independent clock. The clock models can then be used to remove the predicted clock errors from the time stamps to get a more realistic indication of the actual time at which the events occurred. All the time stamps from different sources can then be unified onto a single time-line.	han unification	Malcolm W. Stevens	2004	Digital Investigation	10.1016/j.diin.2004.07.003	model of computation;simulation;computer science;theoretical computer science;digital forensics;computer security;computer forensics;computer graphics (images)	HCI	-7.785619667492869	52.26469867976156	121453
ec5ceaeaeea24a7ba8a57cf2410bd49ad7e1e368	internet nuggets		"""It is my belief that the problem is not shared cache-coherent memory (bear with me for a second), but the """"universality"""" of shared cachecoherent memory. Everything that is cacheable is by mandate coherent, and this, in turn squashes out the parallelism at several hierarchies. There should be some mechanism that allows data to reside in a cache where that data is not coherent, and likewise, some mechanism that explicitly states that this data is shared and must be coherent. The lack of mechanisms to subscribe-to and subscribe-against coherence results in massive cache coherent traffic in the system."""	cpu cache;cache coherence;coherence (physics);memory coherence;parallel computing;universal turing machine	Mark Thorson	2008	SIGARCH Computer Architecture News	10.1145/1556444.1556459		Arch	-12.445789348042094	47.99134080592057	121499
48f7cfc2634144282358d745730cfd90b74aa0e3	dynamic fine-grain scheduling of pipeline parallelism	data parallel;multicore chips dynamic fine grain scheduling pipeline parallelism pipeline parallel program scheduling predictable execution times dynamic fine grain load balancing task stealing scheduler complex graphs gramps programming model stream programming models per stage queues queuing policies backpressure mechanism buffer management scheme packet stealing queue data locality aware dynamic allocation multicore smp canonical gpgpu scheduler general purpose multicore fine grain architecture variability;runtime programming parallel processing dynamic scheduling schedules instruction sets;dynamic load balancing;paper;resource allocation;queueing theory;performance;buffer management;parallel programming;runtime;programming model;cuda;programming techniques;scheduling;schedules;nvidia;load balance;scheduling multiprocessing systems parallel programming queueing theory resource allocation;multiprocessing systems;computer science;parallel programs;programming;parallel processing;dynamic scheduling;instruction sets	Scheduling pipeline-parallel programs, defined as a graph of stages that communicate explicitly through queues, is challenging. When the application is regular and the underlying architecture can guarantee predictable execution times, several techniques exist to compute highly optimized static schedules. However, these schedules do not admit run-time load balancing, so variability introduced by the application or the underlying hardware causes load imbalance, hindering performance. On the other hand, existing schemes for dynamic fine-grain load balancing (such as task-stealing) do not work well on pipeline-parallel programs: they cannot guarantee memory footprint bounds, and do not adequately schedule complex graphs or graphs with ordered queues. We present a scheduler implementation for pipeline-parallel programs that performs fine-grain dynamic load balancing efficiently. Specifically, we implement the first real runtime for GRAMPS, a recently proposed programming model that focuses on supporting irregular pipeline and data-parallel applications (in contrast to classical stream programming models and schedulers, which require programs to be regular). Task-stealing with per-stage queues and queuing policies, coupled with a backpressure mechanism, allow us to maintain strict footprint bounds, and a buffer management scheme based on packet-stealing allows low-overhead and locality-aware dynamic allocation of queue data. We evaluate our runtime on a multi-core SMP and find that it provides low-overhead scheduling of irregular workloads while maintaining locality. We also show that the GRAMPS scheduler outperforms several other commonly used scheduling approaches. Specifically, while a typical task-stealing scheduler performs on par with GRAMPS on simple graphs, it does significantly worse on complex ones, a canonical GPGPU scheduler cannot exploit pipeline parallelism and suffers from large memory footprints, and a typical static, streaming scheduler achieves somewhat better locality, but suffers significant load imbalance on a general-purpose multi-core due to fine-grain architecture variability (e.g., cache misses and SMT).	cpu cache;general-purpose computing on graphics processing units;general-purpose modeling;gramps;heart rate variability;load balancing (computing);locality of reference;memory footprint;memory management;multi-core processor;network packet;overhead (computing);parallel computing;pipeline (computing);programming model;schedule (computer science);scheduling (computing);stream processing;symmetric multiprocessing	Daniel Sánchez;David Lo;Richard M. Yoo;Jeremy Sugerman;Christoforos E. Kozyrakis	2011	2011 International Conference on Parallel Architectures and Compilation Techniques	10.1109/PACT.2011.9	parallel processing;programming;parallel computing;real-time computing;performance;dynamic priority scheduling;schedule;resource allocation;computer science;load balancing;operating system;instruction set;distributed computing;programming paradigm;queueing theory;scheduling	Arch	-9.943270490461813	50.9686233389205	121508
5bed61d183c993d339500372e6cdfecb646f6c19	optimizing mapreduce based on locality of k-v pairs and overlap between shuffle and local reduce	mlsr algorithm mapreduce optimization k v pair locality programming model open source implementation hadoop load balancing network traffic network communication performance degradation locality enhanced load balance algorithm execution flow map local reduce shuffle and final reduce cpu resources i o resources lelb algorithm;computers;google;degradation;cloud computing mapreduce hadoop locality overlap;locality;overlap;algorithm design and analysis manganese big data degradation cloud computing google computers;manganese;resource allocation cloud computing data handling parallel programming;big data;mapreduce;hadoop;algorithm design and analysis;cloud computing	At present, MapReduce is the most popular programming model for Big Data processing. As a typical open source implementation of MapReduce, Hadoop is divided into map, shuffle, and reduce. In the mapping phase, according to the principle moving computation towards data, the load is basically balanced and network traffic is relatively small. However, shuffle is likely to result in the outburst of network communication. At the same time, reduce without considering data skew will lead to an imbalanced load, and then performance degradation. This paper proposes a Locality-Enhanced Load Balance (LELB) algorithm, and then extends the execution flow of MapReduce to Map, Local reduce, Shuffle and final Reduce (MLSR), and proposes a corresponding MLSR algorithm. Use of the novel algorithms can share the computation of reduce and overlap with shuffle in order to take full advantage of CPU and I/O resources. The actual test results demonstrate that the execution performance using the LELB algorithm and the MLSR algorithm outperforms the execution performance using hadoop by up to 9.2% (for Merge Sort) and 14.4% (for Word Count).	algorithm;apache hadoop;big data;central processing unit;computation;elegant degradation;input/output;load balancing (computing);locality of reference;mapreduce;merge sort;network packet;open-source software;optimizing compiler;programming model	Jianjiang Li;Jie Wu;Xiaolei Yang;Shiqi Zhong	2015	2015 44th International Conference on Parallel Processing	10.1109/ICPP.2015.103	parallel computing;big data;degradation;cloud computing;computer science;manganese;operating system;database;distributed computing	HPC	-16.60663823027797	56.79128402109147	121531
40f127a92c07a51c03209b2858e19978517c7818	an fpga-based coprocessor for real-time fieldbus traffic scheduling - architecture and implementation	computacion informatica;performance evaluation;real time;real time traffic;traffic control;fieldbus;schedulability analysis;coprocessors;worst case execution time;community networks;ciencias basicas y experimentales;scheduling;scheduling problem;real time communication;grupo a;distributed computer control system;dynamic scheduling;admission control	Distributed computer control systems used nowadays in the industry need often to meet requirements of on-line reconfigurability so they can adjust dynamically to changes in the application environment or to evolving specifications. The communication network connecting the computer nodes, commonly a fieldbus system, must use therefore dynamic scheduling strategies, together with on-line admission control procedures that test the validity of all changes in order to guarantee the satisfaction of real-time constraints. These are both very computationally demanding tasks, something that has precluded their wide adoption. However, these algorithms also embed sufficient levels of parallelism to grant them benefits from implementations in dedicated hardware.This paper presents a scheduling coprocessor that executes dynamic real-time traffic scheduling and schedulability analysis. The FPGA-based implementation described here supports multiple scheduling policies and was tailored for the FTT-CAN protocol, but it can be used also in other fieldbuses relying on centralized scheduling. The coprocessor generates schedules in about two orders of magnitude less time than any practical network elementary cycle duration. The time to execute a schedulability test is deterministic. An evaluation based on the SAE benchmark yielded a worst-case execution time of 1.4 ms.The paper starts by discussing the scheduling problem being addressed. It describes then the coprocessor functionality and architecture, highlighting important design decisions, and its latest implementation. Finally the coprocessor performance evaluation is presented.	coprocessor;field-programmable gate array;fieldbus;real-time clock;scheduling (computing)	Ernesto Martins;Luis Fernando de Almeida;José Alberto Fonseca	2005	Journal of Systems Architecture	10.1016/j.sysarc.2004.06.003	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;job shop scheduling;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;distributed computing;round-robin scheduling;scheduling;coprocessor;foundation fieldbus;computer network;worst-case execution time	EDA	-8.984914109616694	59.041392770544036	121538
6f18b74247f8e5aca8efc3c8ddb1b63926a97641	microarchitecture support for dynamic scheduling of acyclic task graphs	dependence graph;computer architecture;task graphs;control dependence;dynamic scheduling	Any program can be broken into its loop structure, plus acyclic dependence graphs representing the body of each loop or subroutine. The parallelism inherent in these acyclic graphs augments the looplevel parallelism available in the program. This paper presents two algorithms for dynamic scheduling of acyclic task graphs containing both data and control dependences, and describes a microarchitecture which implements these algorithms efficiently.	algorithm;directed acyclic graph;microarchitecture;parallel computing;scheduling (computing);subroutine	Carl J. Beckmann;Constantine D. Polychronopoulos	1992		10.1145/144953.145791	open-shop scheduling;dynamic priority scheduling;two-level scheduling	Arch	-5.739028342881949	47.3847657897548	121541
3bf23f74bf33ed52f7c28587fab315610b27221a	zsim: fast and accurate microarchitectural simulation of thousand-core systems	drift tolerance;error correction;phase change memory	Architectural simulation is time-consuming, and the trend towards hundreds of cores is making sequential simulation even slower. Existing parallel simulation techniques either scale poorly due to excessive synchronization, or sacrifice accuracy by allowing event reordering and using simplistic contention models. As a result, most researchers use sequential simulators and model small-scale systems with 16-32 cores. With 100-core chips already available, developing simulators that scale to thousands of cores is crucial.  We present three novel techniques that, together, make thousand-core simulation practical. First, we speed up detailed core models (including OOO cores) with instruction-driven timing models that leverage dynamic binary translation. Second, we introduce bound-weave, a two-phase parallelization technique that scales parallel simulation on multicore hosts efficiently with minimal loss of accuracy. Third, we implement lightweight user-level virtualization to support complex workloads, including multiprogrammed, client-server, and managed-runtime applications, without the need for full-system simulation, sidestepping the lack of scalable OSs and ISAs that support thousands of cores.  We use these techniques to build zsim, a fast, scalable, and accurate simulator. On a 16-core host, zsim models a 1024-core chip at speeds of up to 1,500 MIPS using simple cores and up to 300 MIPS using detailed OOO cores, 2-3 orders of magnitude faster than existing parallel simulators. Simulator performance scales well with both the number of modeled cores and the number of host cores. We validate zsim against a real Westmere system on a wide variety of workloads, and find performance and microarchitectural events to be within a narrow range of the real system.	binary translation;client–server model;microarchitecture;multi-core processor;open sound system;parallel computing;scalability;server (computing);simulation;two-phase locking;user space;westmere (microarchitecture)	Daniel Sánchez;Christoforos E. Kozyrakis	2013		10.1145/2485922.2485963	computer architecture;parallel computing;real-time computing;error detection and correction;phase-change memory;computer science;operating system	Arch	-13.861928029715664	49.657673102379164	121594
3889ca0d42d71fa0251d0b8bda167de0b9dd63d2	a performance comparison of multi-micro and mainframe database architectures	queueing network;distributed database;on line transaction processing;performance comparison;multi user;relational database;performance analysis;performance model;data access;memory hierarchy;transaction processing;database management system;system management	Database machine architectures consisting of multiple microprocessors or mini-computers are attracting wide attention. There have been several proposals and prototypes (see, e.g., DeWitt, Gerber, Graefe, Heytens, Kumar and Muralikrishna (1986), Fishman, Lai and Wilkinson (1984), Hsiao (1983), or the 1983 and 1985 Proceedings of the International Workshop on Database Machines). There is also a commercially available system based on multiple microprocessors (Teradata (1984)). With these architectures it is possible to exploit parallelism at three levels: within a single query, within a single transaction, and by simultaneously executing multiple independent transactions. The rationale behind these multiple microprocessor architectures is primarily to take advantage of the potential lower cost per MIPS (Millions of Instructions per Second, a measure of processing power) of microprocessors as opposed to mainframes. In addition, database machines may offer incremental capacity growth as well as improved performance for large queries by exploiting parallelism within a single query. However, it is not clear if database machines made of multiple microprocessors indeed have any cost/performance advantage over a more conventional mainframe based database management systems. Several papers on the performance analysis of database machines can be found in the literature (e.g., Salza, Terranova and Velardi (1983) or Bit and Hartman (1985)). Most of these studies have focused on determining the execution time of a single query in a particular database machine architecture. Few studies have dealt with the response time of single queries in a multi-user environment. We are not aware of any papers that systematically study the performance trade-offs between a multi-microprocessor database machine and a large mainframe system. This paper presents such a systematic study. We examine a hypothetical database machine that uses standard microprocessors and disks; database machines that use special purpose hardware are not considered here (e.g., Sakai, Kamiya, Iwata, Abe, Tanaka, Shibayama and Murakami (1984)). However, we do not limit our studies to the components available today; we also consider processors and disks projected to be available in the future. We assume that both the database machine and the mainframe provide relational database functions (e.g., Date (1986)). While there are several applications for relational database (on-line transaction processing, ad-hoc queries, etc.), we limit our attention to one specific application domain; namely high volume on-line transaction processing. In this domain, we consider a range of transactions and investigate the sensitivity of the two architectures to various transaction related parameters. Dias, Iyer and Yu (1986), in a similar study, have investigated the issue of coupling many small systems to obtain comparable performance of a few (coupled) large systems. Their study is limited to a specific workload with no parametric or sensitivity study with respect to transaction characteristics and the architectures they compared are quite different from the database machine considered in this paper. For high volume transaction processing environments, there appears to be only a limited potential to exploit parallelism within a single transaction. It is therefore expected that since the database machine is made of slower processors and since the functions are distributed across several processors, it would require more aggregate processing capacity, or MIPS, than the mainframe to sustain a given throughput and a response time. Thus there is a trade-off between the cheaper cost per MIPS of microprocessors as opposed to mainframes and the increase in aggregate MIPS required by the database machine to achieve a given performance level. This paper addresses this trade-off through the use of queueing network performance models of the two architectures. Assuming that the MIPS ratings of the microprocessor and mainframe are equivalent, our models indicate that with today's processor technology, the performance of the database machine is sensitive to the transaction complexity, the amount of skew in the data access pattern, the amount of overhead required to implement the distributed database function and the buffer miss ratio. Furthermore, there is only a narrow range of transaction processing workloads for which the database machine can meet a prespecified response time objective with only a moderate increase in aggregate processing capacity over that of the mainframe. However, using the technology projected for the early 1990's, our models predict that the performance of the hypothetical database machine is less sensitive to the above factors. Assuming that the level of lock contention is low, the memory hierarchies of the two architectures are equivalent (in the sense of achieving equal buffer miss ratios), and the performance of disks are equivalent in the two architectures, the models predict that the performance objective can be met with only a moderate increase in aggregate capacity for a broader range of transaction workloads. The workloads considered in this paper consist of relatively short transactions based on primary key retrievals and updates. It is therefore difficult to make general conclusions about the overall superiority of one architecture against the other when a mixed set of workloads is expected (our study assumes that all transactions have the same expected pathlength and I/O activity). This study focused on performance issues and specifically does not address such issues as MIPS flexibility (general purpose versus special purpose architectures), security, recovery and system management.	aggregate data;application domain;attribute-based encryption;central processing unit;computer security;data access;database machine;design rationale;distributed database;gerber format;hoc (programming language);input/output;lock (computer science);mainframe computer;memory hierarchy;microprocessor;minicomputer;multi-user;network performance;online and offline;online transaction processing;overhead (computing);parallel computing;processor technology;queueing theory;relational database;response time (technology);run time (program lifecycle phase);sakai project;systems management;the legend of zelda: the wind waker;throughput;translation lookaside buffer;unique key;user interface	Philip Heidelberger;M. Seetha Lakshmi	1987		10.1145/29903.29906	data access;parallel computing;real-time computing;systems management;transaction processing;database tuning;distributed transaction;relational database;computer science;operating system;database;online transaction processing;distributed database;alias;transaction processing system;database testing;computer network	DB	-18.066702471055034	53.4257588828242	121666
86642ee8173a266834af7147188f4aa22b51b319	energy-efficiency potential of a phase-based cache resizing scheme for embedded systems	cache storage;cache;energy efficient;perforation;reconfigurable architectures;reconfigurable architectures cache storage memory architecture embedded systems program compilers low power electronics;embedded system energy consumption cache memory proposals energy management program processors computer architecture degradation power system management power dissipation;embedded system;embedded systems;energy performance;configurable architecture;low power;energy consumption;memory architecture;energy consumption phase based cache resizing embedded systems cache hierarchy integration density configurable caches embedded compilers program dynamic phase;low power electronics;program compilers	"""Managing the energy-performance tradeoff has become a major challenge with embedded systems. The cache hierarchy is a typical example where this tradeoff plays a central role. With the increasing level of integration density, a cache can feature millions of transistors, consuming a significant portion of the energy. At the same time however, a cache also permits to significantly improve performance. Configurable caches are becoming """"de-facto"""" solution to deal efficiently with these issues. Such caches are equipped with artifacts that enable one size to resize it dynamically. With regard to embedded systems, however, many of these artifacts restrict the configurability at the application level. We propose in this paper to modify the structure of a configurable cache to offer embedded compilers the opportunity to reconfigure it according to a program dynamic phase, rather than on a per-application basis. We show in our experimental results that the proposed scheme has a potential for improving the compiler effectiveness to reduce the energy consumption, while not excessively degrading the performance."""	access control;artifact (software development);cpu cache;compiler;embedded system;simulation;transistor;working set size	Gilles Pokam;François Bodin	2004	Eighth Workshop on Interaction between Compilers and Computer Architectures, 2004. INTERACT-8 2004.	10.1109/INTERA.2004.1299510	bus sniffing;computer architecture;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;cache;computer science;cache invalidation;operating system;efficient energy use;smart cache;cache algorithms;cache pollution;low-power electronics	EDA	-4.611056080026511	54.93172688123323	121668
745282b5601efdc937199cf99b05607b76c1c8d9	storage aware resource allocation for grid data streaming pipelines	scheduling data handling grid computing pipeline processing resource allocation;data overflow storage aware resource allocation grid data streaming pipelines data pipeline form genetic algorithm gray model pipelines task scheduling repertory strategies on demand data streaming;resource allocation;data stream;information technology;resource management;on demand data streaming;data communication;grid data streaming pipelines;storage aware resource allocation;scheduling;pipelines throughput scheduling resource management parallel processing gallium genetic algorithms;pipelines;storage aware;genetic algorithm;predictive models;genetic algorithms;resource allocation grid computing data streaming storage aware;data handling;task scheduling;data pipeline form;storage automation;grid computing;parallel processing;pipelines task scheduling;data streaming;pipeline processing;gray model;data overflow;repertory strategies;gallium;throughput	Data streaming applications, usually composed with sequential/parallel tasks in a data pipeline form, bring new challenges to task scheduling and resource allocation in grid environments. Due to high volumes of data and relatively limit storage capability, resource allocation and data streaming have to be storage aware. In this paper, genetic algorithm (GA) is adopted for task scheduling of pipelines, based on on-line measurement and prediction with gray model (GM). On-demand data streaming is introduced to avoid data overflow using repertory strategies. Experimental results show that balance among task executions with on-demand data streaming is required to improve overall performance, avoid system bottlenecks and backlogs of intermediate data, and increase data throughput of pipelines as a whole.	genetic algorithm;online and offline;pipeline (computing);scheduling (computing);software release life cycle;throughput	Wen Zhang;Junwei Cao;Yisheng Zhong;Lianchen Liu;Cheng Wu	2008	2008 International Conference on Networking, Architecture, and Storage	10.1109/NAS.2008.24	parallel processing;parallel computing;real-time computing;genetic algorithm;computer science;resource management;database;information technology	HPC	-17.467399775978336	59.05389479848229	121679
bf80df2690dfee0b08caf6f0437352d8cfaa06c5	an online data access prediction and optimization approach for distributed systems	selection model;analytical models;scientific application;distributed system;data intensive application;optimisation;predictive distribution;distributed database;stochastic process;prediction distributed computing distributed file system data access optimization time series analysis;lhc cern project online data access prediction optimization approach distributed systems current scientific applications large scale computing infrastructures data intensive applications distributed storage systems scientific community optorsim simulator;distributed processing;distributed computing;autoregressive process;distributed storage system;data replication;time series;time series analysis distributed databases optimization predictive models autoregressive processes analytical models stochastic processes;optimisation data handling distributed processing;large scale;stochastic processes;autoregressive processes;time series analysis;data access optimization;data access;distributed databases;distributed file system;performance prediction;predictive models;scientific communication;optimization;data handling;optimal prediction;prediction;analytical model	Current scientific applications have been producing large amounts of data. The processing, handling and analysis of such data require large-scale computing infrastructures such as clusters and grids. In this area, studies aim at improving the performance of data-intensive applications by optimizing data accesses. In order to achieve this goal, distributed storage systems have been considering techniques of data replication, migration, distribution, and access parallelism. However, the main drawback of those studies is that they do not take into account application behavior to perform data access optimization. This limitation motivated this paper which applies strategies to support the online prediction of application behavior in order to optimize data access operations on distributed systems, without requiring any information on past executions. In order to accomplish such a goal, this approach organizes application behaviors as time series and, then, analyzes and classifies those series according to their properties. By knowing properties, the approach selects modeling techniques to represent series and perform predictions, which are, later on, used to optimize data access operations. This new approach was implemented and evaluated using the OptorSim simulator, sponsored by the LHC-CERN project and widely employed by the scientific community. Experiments confirm this new approach reduces application execution time in about 50 percent, specially when handling large amounts of data.	clustered file system;computer cluster;data access;data-intensive computing;distributed computing;grid computing;large hadron collider;mathematical optimization;parallel computing;replication (computing);run time (program lifecycle phase);system migration;time series	Renato Porfirio Ishii;Rodrigo Fernandes de Mello	2012	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2011.256	stochastic process;parallel computing;computer science;theoretical computer science;operating system;time series;data mining;database;distributed computing;distributed database;statistics	HPC	-17.615019575922542	58.830495951519104	121803
feffac96ae3c2b6493892c4e446df77a7c2ae5a6	performance prediction for reconfigurable processor	analytical models;microprocessors;cache storage;performance evaluation;reconfigurable architectures cache storage learning artificial intelligence microprocessor chips performance evaluation processor scheduling;processor scheduling;reconfigurable architectures;computer architecture;accuracy;machine learning;predictive models;learning artificial intelligence;predictive models architecture computer architecture analytical models machine learning accuracy microprocessors;scheduling process optimization reconfigurable processor performance prediction model integrated circuit process ic process cache prefetch mechanism microprocessor frequency processor architecture parameters machine learning base learner ensemble learner prediction error m5p learner program architecture independent characteristics processor architecture optimization;architecture;microprocessor chips	As the Integrated Circuit (IC) process improves, the microprocessors become more and more complicated. Most microprocessors allow part of their important parameters to be reconfigured, such as the frequency, cache prefetch mechanism, and so on. Predicting the performance of reconfigurable processor is still an open question since the performance model needs to consider not only program characteristic, but also processor architecture parameters. In this paper, we propose a new performance prediction model for reconfigurable processor based on machine learning. We employ the M5P as base learners to gain better performance and custom an ensemble learner to get better accuracy. The experiment results show that the ensemble learner reduce the prediction error to 3% from 8%, which is the prediction of a single M5P learner. Furthermore, benefited from considering both program architecture independent characteristics and the architecture parameters, our model can not only predict the performance of the program under specific architecture, but also help to optimize processor's architecture and scheduling processes.	integrated circuit;learner-generated context;machine learning;microarchitecture;microprocessor;performance prediction;reconfigurable computing;run time (program lifecycle phase);scalability;scheduling (computing);software architecture;on-line system	Dao-Fu Liu;Qi Guo;Tianshi Chen;Ling Fei Li;Yunji Chen	2012	2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems	10.1109/HPCC.2012.199	computer architecture;parallel computing;real-time computing;computer science;architecture;operating system	EDA	-4.7885105407817665	52.41196637356756	121859
57ecb6e637c61f3f89f1d6cfdf10f7aaa7b2a184	probabilistic adaptive load balancing for parallel queries	databases;load management query processing decision making costs current distribution parallel processing bismuth proposals databases history;parallel queries;history;query processing;resource allocation;bismuth;processor load assignments;probabilistic approach;adaptive load balancing;current distribution;monitoring;adaptive query processing;processor load assignments probabilistic adaptive load balancing parallel queries adaptive query processing;load management;distributed databases;resource allocation parallel processing query processing;probabilistic adaptive load balancing;boron;dynamic adaptation;indium;proposals;parallel processing;iodine;arsenic;beryllium;nitrogen	In the context of adaptive query processing (AQP), several techniques have been proposed for dynamically adapting/redistributing processor load assignments throughout a computation to take account of varying resource capabilities. The effectiveness of these techniques depends heavily on when and to what they adapt processor load assignments, particularly in the presence of varying load imbalance. This paper presents a probabilistic approach to decide when and to what to adapt processor load assignments. Using a simulation based evaluation, it is compared to two other approaches already reported. These two approaches are simpler in their decision making than the probabilistic approach, but the latter performs better under several scenarios of load imbalance.	algorithm;analysis of algorithms;computation;control theory;database;load balancing (computing);mimo;memory management;memory pool;parallel computing;probabilistic automaton;self-tuning;simulation;state (computer science);window function	Daniel M. Yellin;Jorge Buenabad Chávez;Norman W. Paton	2008	2008 IEEE 24th International Conference on Data Engineering Workshop	10.1109/ICDEW.2008.4498280	boron;arsenic;beryllium;parallel processing;real-time computing;resource allocation;computer science;bismuth;nitrogen;database;distributed computing;iodine;indium;distributed database	DB	-15.272791708313896	58.18696719516685	121935
72131b897a55d14ffa8fb5dc174ada690d009a46	coordinated load management in peer-to-peer coupled federated grid systems	grid scheduling;peer to peer grids;grid computing	This paper proposes a coordinated load management protocol for Peer-to-Peer (P2P) coupled federated Grid systems. The participants in the system, such as the resource providers and the consumers who belong to multiple control domains, work together to enable a coordinated federation. The coordinated load management protocol embeds a logical spatial index over a Distributed Hash Table (DHT) space for efficient management of the coordination objects; the DHT-based space serves as a kind of decentralized blackboard system. We show that our coordination protocol has a message complexity that is logarithmic to the number of nodes in the system, which is significantly better than existing broadcast based coordination protocols. The proposed load management protocol can be applied for efficiently coordinating resource brokering services of distributed computing systems such as grids and PlanetLab. Resource brokering services are the main components that control the way applications are scheduled, managed and allocated in a distributed, heterogeneous, and dynamic Grid computing environments. Existing Grid resource brokers, e-Science application work-flow schedulers, operate in tandem but still lack a coordination mechanism that can lead to efficient application schedules across distributed resources. Further, lack of coordination exacerbates the utilization of various resources (such as computing cycles and network bandwidth). The feasibility of the proposed coordinated load management protocol is studied through extensive simulations.	blackboard system;control flow;distributed computing;distributed hash table;e-science;grid systems corporation;grid computing;iteration;load balancing (computing);load management;open-shop scheduling;peer-to-peer;planetlab;regular grid;scheduling (computing);simulation;spatial database	Rajiv Ranjan;Aaron Harwood;Rajkumar Buyya	2010	The Journal of Supercomputing	10.1007/s11227-010-0426-y	real-time computing;computer science;distributed computing;grid computing;computer network	HPC	-18.965628113107204	59.285456580707155	122248
4a56b6e5cf59d94d1f74159f1e6d93fa3454fa63	hybrid-comp: a criticality-aware compressed last-level cache		Cache compression is a promising technique to increase on-chip cache capacity and to decrease off-chip bandwidth usage. While prior compression techniques always consider a trade-off between compression ratio and decompression latency, they are oblivious to the variation in criticality of different cache blocks. In multi-core processors, last-level cache (LLC) is logically shared but physically distributed among cores. In this work, we demonstrate that, cache blocks within such nonuniform architecture exhibit different sensitivity to the access latency. Owing to this behavior, we propose a criticality-aware compressed LLC that favors lower latency over higher capacity based on the criticality of the data blocks. Based on our studies on a 16-core processor with 4MB LLC, our proposed criticality-aware mechanism improves the system performance comparable to that of with an 8MB uncompressed LLC.	cpu cache;central processing unit;criticality matrix;data compression;lunar lander challenge;megabyte;multi-core processor;self-organized criticality	Amin Jadidi;Mohammad Arjomand;Mahmut T. Kandemir;Chita R. Das	2018	2018 19th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2018.8357260	real-time computing;latency (engineering);compression ratio;cache;criticality;architecture;compression (physics);computer science;uncompressed video;bandwidth (signal processing)	Arch	-8.000529194720544	53.78629739131671	122270
92541a94b3e2a93e8c3f7335ca334cc638bb4608	real-time dram throughput guarantees for latency sensitive mixed qos mpsocs	system on chip dram chips quality of service real time systems;latency reduction real time dram throughput guarantees latency sensitive mixed qos mpsoc low latency best effort service low latency be service real time memory controllers traffic classes gt requestors be requestors formal timing analysis tight guarantees;privatization;real time systems dram chips throughput delays privatization;dram chips;delays;throughput;real time systems	The trend towards integration is leading to the design of multi- and many-core platforms that accommodate processing tiles (requestors) with different memory requirements. Such platforms require a memory controller capable of providing low-latency best-effort (BE) service for some requestors and guaranteed throughput (GT) for others. Although there are realtime controllers that support the concept of different traffic classes, they do not efficiently handle scenarios with multiple BE and GT requestors. We propose a memory controller that tackles this problem, providing low latency for BE requestors and real-time guarantees for GT ones. We support the guarantees with a formal timing analysis. Our experiments confirm that our approach enforces tight guarantees for GT requestors, while simultaneously reducing the latency of BE ones by up to 67%, when compared with a baseline memory controller.	baseline (configuration management);best-effort delivery;dynamic random-access memory;experiment;field-programmable gate array;manycore processor;memory controller;noise shaping;prototype;quality of service;real-time clock;real-time transcription;requirement;static timing analysis;throughput;traffic shaping	Leonardo Ecco;Selma Saidi;Adam Kostrzewa;Rolf Ernst	2015	10th IEEE International Symposium on Industrial Embedded Systems (SIES)	10.1109/SIES.2015.7185038	embedded system;throughput;parallel computing;real-time computing;computer science;operating system;computer network	Embedded	-8.143808912296194	58.2403905201808	122303
fa826359ae1c4f04cd35b85e2831720753807fb2	design and evaluation of a multi-threaded architecture for parallel graph reduction	distributed memory machine;side effect	Main limitations of distributed memory machines involving thousands of processors, deal with network latencies for remote data and/or programs accesses. In this paper we present multithreading techniques for parallel graph reduction model that can tolerate latencies of thousands of cycles by dynamically creating a set of threads. Efficiency is achieved by introducing fast context switch, non preemptive threads and comparative long run-lenght threads (thousand of cycles). Conventional multithreaded techniques deals with statically defined threads (at compile time) and dynamically or statically scheduling of the work to be performed. Parallel graph reduction is an attractive model because of its simplicity and inherently distributed nature: parallelism is introduced by parallel evaluation of function parameters corresponding to dynamically created set of threads. The single assignment feature and the absence of side effect, since the internal representation of programs remains purely functional, overcomes the difficulties of synchronized accesses to shared data and scheduling of parallel activities.	graph reduction	Francis Caudal;Bernard Lécussan	1995		10.1007/3-540-60222-4_129	parallel computing;computer science;theoretical computer science;distributed computing;side effect	Arch	-14.235972493776606	47.90754013188418	122418
5cd21682278c5979839dccc083654d5d087f8fd3	refinement of workload models for engine controllers by state space partitioning		We study an engine control application where the behavior of engine controllers depends on the engine’s rotational speed. For efficient and precise timing analysis, we use the Digraph Real-Time (DRT) task model to specify the workload of control tasks where we employ optimal control theory to faithfully calculate the respective minimum inter-release times. We show how DRT models can be refined by finer grained partitioning of the state space of the engine up to a model which enables an exact timing analysis. Compared to previously proposed methods which are either unsafe or pessimistic, our work provides both abstract and tight characterizations of the corresponding workload. 1998 ACM Subject Classification C.3 Real-Time and Embedded Systems	control theory;embedded system;optimal control;real-time transcription;space partitioning;state space;static timing analysis	Morteza Mohaqeqi;Syed Md Jakaria Abdullah;Pontus Ekberg;Wang Yi	2017		10.4230/LIPIcs.ECRTS.2017.11	workload;computer science;real-time computing;state space;optimal control;rotational speed;static timing analysis	Embedded	-8.442607130401269	60.3897358947508	122679
1fa33c5ed22c3f3078abb435897f880b32efcc33	hat: history-based auto-tuning mapreduce in heterogeneous environments	heterogeneous environments;scheduling algorithm;history based auto tuning;mapreduce	In MapReduce model, a job is divided into a series of map tasks and reduce tasks. The execution time of the job is prolonged by some slow tasks seriously, especially in heterogeneous environments. To finish the slow tasks as soon as possible, current MapReduce schedulers launch a backup task on other nodes for each of the slow tasks. However, traditional MapReduce schedulers cannot detect slow tasks correctly since they cannot estimate the progress of tasks accurately (Hadoop home page http://hadoop.apache.org/ , 2011; Zaharia et al. in 8th USENIX symposium on operating systems design and implementation, ACM, New York, pp. 29–42, 2008). To solve this problem, this paper proposes a History-based Auto-Tuning (HAT) MapReduce scheduler, which calculates the progress of tasks accurately and adapts to the continuously varying environment automatically. HAT tunes the weight of each phase of a map task and a reduce task according to the value of them in history tasks and uses the accurate weights of the phases to calculate the progress of current tasks. Based on the accurate-calculated progress of tasks, HAT estimates the remaining time of tasks accurately and further launches backup tasks for the tasks that have the longest remaining time. Experimental results show that HAT can significantly improve the performance of MapReduce applications up to 37% compared with Hadoop and up to 16% compared with LATE scheduler.	apache hadoop;backup;cell (microprocessor);cloud computing;home page;locality of reference;map;mapreduce;norm (social);operating system;run time (program lifecycle phase);scalability;scheduling (computing);self-tuning;smart environment;systems design	Quan Chen;Minyi Guo;Qianni Deng;Long Tai Zheng;Song Guo;Yao Shen	2011	The Journal of Supercomputing	10.1007/s11227-011-0682-5	parallel computing;real-time computing;computer science;operating system;distributed computing;scheduling	OS	-17.485135530928368	59.40080700530505	122773
56514c2adbe546faf4c9d80187c742fb7ffe501f	low-cost epoch-based correlation prefetching for commercial applications	real estate;storage management;on line transaction processing;data misses;chip;data misses epoch based correlation prefetching commercial application correlation table access;complex data;prefetching delay bandwidth file servers databases hardware yarn microarchitecture microelectronics sun;control flow;correlation table access;data access;cost effectiveness;commercial application;epoch based correlation prefetching	The performance of many important commercial workloads, such as on-line transaction processing, is limited by the frequent stalls due to off-chip instruction and data accesses. These applica- tions are characterized by irregular control flow and complex data access patterns that render many low-cost prefetching schemes, such as stream-based and stride-based prefetching, ineffective. For such applications, correlation-based prefetching, which is ca- pable of capturing complex data access patterns, has been shown to be a more promising approach. However, the large instruction and data working sets of these applications require extremely large correlation tables, making these tables impractical to be im- plemented on-chip. This paper proposes the epoch-based correla- tion prefetcher, which cost-effectively stores its correlation table in main memory and exploits the concept of epochs to hide the long latency of its correlation table access, and which attempts to elim- inate entire epochs instead of individual instruction and data miss- es. Experimental results demonstrate that the epoch-based correlation prefetcher, which requires minimal on-chip real estate to implement, improves the performance of a suite of important commercial benchmarks by 13% to 31% and significantly outper- forms previously proposed correlation prefetchers.	cpu cache;computer data storage;control flow;data access;epoch (reference date);link prefetching;online and offline;online transaction processing;prefetcher;table (database)	Yuan Chou	2007	40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007)	10.1109/MICRO.2007.23	chip;data access;parallel computing;real-time computing;cost-effectiveness analysis;computer science;operating system;programming language;control flow;real estate;complex data type	Arch	-8.908117622304943	53.52865619530647	122824
9cb89f865eaf5f6dbb759c03e0671d0e306425ef	opportunities for data base reorganization	data base reorganization	If the contents of the data base are volatile, automating the process does not eliminate the need for reorganization. In fact, the minor inefficiencies which could be tolerated with small files are a costly burden in the massive data bases which are increasingly common. There are numerous techniques for improving data base performance; this paper is concerned with the process known as reorganization. Reorganization is the rewriting of fields of data in such a way that the logical and physical accessing algorithms are unaffected (although the access paths may be altered) except that they now operate more economically. Economics include saving of storage space or execution time or both. In on-line systems there is the additional motivation of reducing the response time.	algorithm;database;online and offline;response time (technology);rewriting;run time (program lifecycle phase);volatile memory	Ben Shneiderman	1974	FDT - Bulletin of ACM SIGMOD	10.1145/983082.983083	computer science;data mining	DB	-15.37376516880941	54.51679740298267	123225
b51cec88ed6512236406c0c0b42e6456cf35c601	enhancing the performance of tiled loop execution onto clusters using memory mapped network interfaces and pipelined schedules	processor scheduling;nested loops;systems engineering and theory;computer networks;network interfaces;low latency;shape;pc cluster;linear programming;tiles;tiles shape processor scheduling delay computer networks linear programming network interfaces computer interfaces systems engineering and theory laboratories;network interface;computer interfaces	This paper describes the performance benefits attained using enhanced network interfaces to achieve low latency communication. Our experimental testbed concerns the parallel execution of tiled nested loops onto a Linux PC cluster with PCI-SCI NICs (Dolphin D330). Tiles are necessarily exchanging data and should also have large computational grain, so that their parallel execution becomes beneficial. We schedule tiles much more efficiently by exploiting the inherent overlapping between communication and computation phases among successive, atomic tile executions. The applied nonblocking schedule resembles a pipelined datapath where computation phases are overlapped with communication ones, instead of being interleaved with them. We are using DMA communication mode, to remote write (send) data to other nodes, while the host CPU is computing all iterations within each tile. We achieve zero-copy communication through pinned-down physical memory regions for DMA (PCI exported segments to SCI global space). Results illustrate that when using enhanced communication features such as DMA transfers, memorymapped interfaces and zero-copy mechanisms, overall performance is considerably enhanced than when typically using conventional, CPU and kernel bounded, communication primitives.	central processing unit;computation;computer data storage;datapath;direct memory access;dolphin;downstream (software development);for loop;iteration;linux;network interface controller;overhead (computing);paging;parallel computing;programmed input/output;random access;run time (program lifecycle phase);system call;testbed;zero-copy	Aristidis Sotiropoulos;Georgios Tsoukalas;Nectarios Koziris	2002		10.1109/IPDPS.2002.1016567	embedded system;parallel computing;real-time computing;computer science;linear programming;network interface;operating system;distributed computing;programming language;algorithm;computer network	HPC	-11.812756460814667	46.62537243304662	123226
78928f81d6fec720c34b53f2c37a38ac09c90ed4	incorporating temperature-leakage interdependency into dynamic voltage scaling for real-time systems	energy conservation;voltage control;minimization;mathematical model voltage control temperature sensors equations real time systems minimization;temperature sensors;power aware computing;scheduling;mathematical model;scheduling energy conservation power aware computing real time systems;leakage aware dvs approach temperature leakage interdependency dynamic voltage scaling real time systems energy efficiency analytic temperature leakage model temperature aware dvs energy minimization online dvs algorithm multiple task scheduling energy saving;real time systems	Energy efficiency is critical for many application specific real-time systems. Dynamic voltage scaling (DVS) is one of the most effective and well-studied techniques. In this paper, we study the interdependency of temperature and leakage and how it influences DVS. We derive an analytic temperature-leakage model, which has an average error of 0.5°K from the accurate numerical result. This temperature-leakage model enables us to perform temperature aware DVS for total energy minimization without using on-chip temperature sensors. We find that the most energy efficient way to complete a single task is, unlike the existing approaches that use high voltage to save leakage, to scale voltage down to the lowest level without missing the task's deadline. Based on this new finding, we propose an online DVS algorithm to schedule multiple tasks on real-time system. Simulation results show that our algorithm can achieve total energy saving over a state-of-the-art leakage aware DVS approach by as high as 14% and more than 9% on average.	algorithm;dynamic voltage scaling;energy minimization;image scaling;interdependence;numerical analysis;real-time clock;real-time computing;sensor;simulation;spectral leakage	Junjun Gu;Gang Qu	2013	2013 IEEE 24th International Conference on Application-Specific Systems, Architectures and Processors	10.1109/ASAP.2013.6567592	embedded system;parallel computing;real-time computing;energy conservation;computer science;operating system;mathematical model;scheduling	EDA	-5.187768621339121	58.57938742135723	123363
7ccab28ceb295bf1785a2db05862b3b1450b447e	ranking and new database architectures	top k queries;olap;aggregate queries	Database platform support for efficient ranking can have positive performance implications for a number of rank-aware applications, including data exploration, social network analysis, and keyword search. This talk highlights two pieces of work where ranking appears in the research area of database architectures for new hardware. First, we highlight the Bw-tree, a new high-performance B+-tree supporting sorted key-sequential access. The Bw-tree is re-architected to run efficiently on new hardware: its in-memory operations are completely latch-free, removing blocking behavior while also improving multi-core cache behavior, while its storage layer implements a novel log-structured flash storage layer for that exploits fast sequential writes and mitigates adverse performance impact of random writes. Second, we highlight a new classification technique for identifying âĂIJcoldâĂİ (infrequently accessed) data in main-memory database systems. Using a log of sampled record accesses, our technique estimates record access frequencies using exponential smoothing. This classification approach is very efficient: it is able to accurately identify hot and cold records among 1M records in sub-second time from a log of 1B record accesses on a workstation class machine.	b+ tree;blocking (computing);computer architecture;flash memory;in-memory database;multi-core processor;search algorithm;sequential access;smoothing;social network analysis;time complexity;workstation	Justin J. Levandoski	2013		10.1145/2524828.2524833	computer science;data mining;database;world wide web	DB	-13.515539380513456	52.35742148912234	123407
9f43e29603c3a9a4d711af8bf4e62894d20bacad	peak temperature minimization for embedded systems with dvs transition overhead consideration	voltage control;temperature on chip;kernel linux;rotation scheduling and voltage assignment algorithm;dvs transition overhead consideration;real time constraint;lss peak temperature minimization embedded systems dvs transition overhead consideration chip multiprocessor architecture design cmp architecture design dynamic voltage scaling power consumption reduction peak temperature voltage transition rotation scheduling and voltage assignment algorithm rosa algorithm voltage transition kernel linux 4 intel core quad cpu temperature on chip list scheduling solution;processor scheduling;peak temperature;dynamic voltage scaling;chip multiprocessor architecture design;rosa algorithm;voltage transition;embedded systems;cmp architecture design;power aware computing;energy consumption;list scheduling solution;scheduling;processor scheduling embedded systems linux microprocessor chips multiprocessing systems power aware computing;thermal aware;schedules;linux;peak temperature minimization;multiprocessing systems;task scheduling;program processors;voltage transition thermal aware peak temperature task scheduling real time constraint;power consumption reduction;4 intel core quad cpu;lss;microprocessor chips;schedules program processors energy consumption voltage control scheduling processor scheduling equations	Peak temperature is a critical issue in chip multiprocessor (CMP) architecture design. Dynamic Voltage Scaling (DVS) is used nowadays to reduce power consumption and peak temperature. However, traditional DVS in literature usually fails to take into consideration of task dependencies and voltage transition overhead such as energy consumption. In this paper, we propose the ROtation Scheduling and voltage Assignment (ROSA) algorithm, to minimize peak temperature under energy consumption constraints in voltage transition. We conduct experiments on Kernel Linux equipped with 4 Intel Core Quad CPUs. The experimental results demonstrate that our algorithm can reduce the peak temperature on chip with 4.9°C more comparing with the List Scheduling Solution (LSS).	algorithm;central processing unit;dynamic voltage scaling;embedded system;experiment;kernel (operating system);linux;list scheduling;multi-core processor;multiprocessing;overhead (computing);scheduling (computing)	Meikang Qiu;Jianwei Niu;Fei Pan;Yu Chen;Yongxin Zhu	2012	2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems	10.1109/HPCC.2012.71	embedded system;parallel computing;real-time computing;schedule;computer science;operating system;scheduling;linux kernel	EDA	-5.1039877213795055	57.90501937265733	123566
2607c3284c2f80b42b28ed9a174f6b413f6b0a73	a case for micro-cellstores: energy-efficient data management on recycled smartphones	performance measure;persistence;energy efficient;data management;micro data;recovery;low power;database engines;solid state storage;energy cost	Increased energy costs and concerns for sustainability make the following question more relevant than ever: can we turn old or unused computing equipment into cost- and energy-efficient modules that can be readily repurposed? We believe the answer is yes, and our proposal is to turn unused smartphones into micro-data center composable modules. In this paper, we introduce the concept of a Micro-Cellstore (MCS), a stand-alone data-appliance housing dozens of recycled smartphones. Through detailed power and performance measurements on a Linux-based current-generation smartphone, we assess the potential of MCSs as a data management platform. In this paper we focus on scan-based partitionable workloads. We show that smartphones are overall more energy efficient than recently proposed low-power alternatives, based on an initial evaluation over a wide range of single-node database scan workloads, and that the gains become more significant when operating on narrow tuples (i.e., column-stores, or compressed row-stores). Our initial results are very encouraging, showing efficiency gains of up to 6×, and indicate several promising future directions.	data center;linux;low-power broadcasting;smartphone	Stavros Harizopoulos;Spiros Papadimitriou	2011		10.1145/1995441.1995448	microdata;persistence;embedded system;parallel computing;real-time computing;recovery;simulation;data management;computer science;operating system;data mining;database;efficient energy use	Metrics	-14.317742277573338	53.720912315013074	123567
017b1df3346c7156b6cd375169d0a9de38c9cdbb	fault tolerance and recovery of scientific workflows on computational grids	control systems;computational grid;probability;real time constraint;fault tolerant;fault tolerance grid computing fault tolerant systems middleware processor scheduling computer networks usa councils control systems software maintenance weather forecasting;scheduling computational grids fault tolerance and recovery resilient scientific workflows;software maintenance;processor scheduling;scientific workflow;reliability modeling;weather forecasting;usa councils;queue wait time;soft real time;resilient scientific workflows;computer networks;fault tolerance recovery service;design and implementation;fault tolerant systems;scheduling;waiting time;fault tolerance;performance model;natural sciences computing fault tolerance grid computing;failure rate;middleware;application performance model;computational grids;resource reliability model;natural sciences computing;network latency;linked environments for atmospheric discovery;grid computing;linked environments for atmospheric discovery fault tolerance recovery service scientific workflow grid computing application performance model resource reliability model network latency queue wait time probability real time constraint;fault tolerance and recovery	In this paper, we describe the design and implementation of two mechanisms for fault-tolerance and recovery for complex scientific workflows on computational grids. We present our algorithms for over-provisioning and migration, which are our primary strategies for fault-tolerance. We consider application performance models, resource reliability models, network latency and bandwidth and queue wait times for batch-queues on compute resources for determining the correct fault-tolerance strategy. Our goal is to balance reliability and performance in the presence of soft real-time constraints like deadlines and expected success probabilities, and to do it in a way that is transparent to scientists. We have evaluated our strategies by developing a Fault-Tolerance and Recovery (FTR) service and deploying it as a part of the Linked Environments for Atmospheric Discovery (LEAD) production infrastructure. Results from real usage scenarios in LEAD show that the failure rate of individual steps in workflows decreases from about 30% to 5% by using our fault-tolerance strategies.	algorithm;bioinformatics;british informatics olympiad;byzantine fault tolerance;computation;emoticon;failure rate;ibm notes;job queue;provisioning;real-time clock;real-time computing;teragrid	Gopi Kandaswamy;Anirban Mandal;Daniel A. Reed	2008	2008 Eighth IEEE International Symposium on Cluster Computing and the Grid (CCGRID)	10.1109/CCGRID.2008.79	fault tolerance;parallel computing;real-time computing;computer science;control system;operating system;database;distributed computing	HPC	-17.29521311099777	59.85855621761951	123600
1ed9091150a8857c634995aed8b946be29f2a28b	crono: a configurable and easy to maintain resource manager optimized for small and mid-size gnu/linux cluster	optimisation;management system;crono management system;resource allocation;resource manager;resource management;linux cluster;resource manager crono management system gnu linux cluster installations management services;workstation clusters configuration management operating systems computers optimisation resource allocation;design and implementation;management services;workstation clusters;configuration management;operating systems computers;resource management linux power system management scheduling carbon capture and storage design optimization high performance computing computer science costs industrial relations;gnu;linux cluster installations;operating systems;optimization methods	Description: CLOUDs Lab, formerly GRIDS Lab, is a research group engaged in the design and development of next-generation computing systems and applications that aggregate or lease services of distributed resources depending on their availability, capability, performance, cost , and users' quality-of-service requirements. During this period, I worked with the design and development of resource co-allocation policies for parallel applications in distributed systems and development of a scheduling system prototype for executions in both simulated and real-time modes.	aggregate data;distributed computing;gnu;linux;prototype;quality of service;real-time clock;requirement;scheduling (computing)	Marco Aurélio Stelmar Netto;César A. F. De Rose	2003		10.1109/ICPP.2003.1240623	embedded system;parallel computing;real-time computing;computer cluster;resource allocation;computer science;resource management;operating system;management system;configuration management	HPC	-17.451047675434157	60.42040604409438	123618
73befa24e53baa18d0440e02bd3227c65f5b2f79	partitioning and dynamic mapping evaluation for energy consumption minimization on noc-based mpsoc	network on chip;power aware computing multiprocessing systems network on chip;software complexity;partitioning;noc partitioning mapping mpsoc;program processors tiles energy consumption computer architecture partitioning algorithms mathematical model equations;power aware computing;energy consumption;dynamic task mapping dynamic mapping evaluation energy consumption minimization noc based mpsoc software complexity static task partitioning;mapping;multiprocessing systems;noc;mpsoc	Software complexity has increased considerably over recent years, needing special target architectures as NoC-based MPSoCs to fulfill the heavy storage, communication and computation requirements. The design of these systems requires efficient methodologies aggregating partitioning and mapping. In these sense, this paper explores partitioning and mapping influence on energy consumption of homogeneous NoC-Based MPSoC. In addition, it compares two strategies to achieve efficient dynamic mappings: one that map tasks directly onto processors and another one that applies a previous static task-partitioning and uses this information to choose the dynamic task mapping. Experiments with various synthetic and four embedded applications show the efficiency of the second strategy that minimizes an average of 23.5% on energy consumption.	central processing unit;computation;embedded system;experiment;mpsoc;network on a chip;programming complexity;requirement;synthetic intelligence	Eduardo Antunes;Matheus Soares;Alexandra Aguiar;Sergio Johann Filho;Marcos Sartori;Fabiano Hessel;César A. M. Marcon	2012	Thirteenth International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2012.6187532	computer architecture;parallel computing;real-time computing;telecommunications;computer science;network operations center;network on a chip;programming complexity	Embedded	-5.509874260499364	57.24534424665167	124105
36c78dcaf820d7c91e5880f9656fe5cd872db2bf	dynamically reducing pressure on the physical register file through simple register sharing	instruction level parallel;microprocessors;cache storage;register sharing;multi threading;register renaming;simultaneous multi threaded;hardware complexity;clocks;physical registers;perforation;processor scheduling;physical register file;frequent values;false data dependence elimination;register pressure reduction;data engineering;physical register storage sharing;surface mount technology;physics computing;general solution;performance improvement;parallel architectures;registers;data dependence;instruction set reuse;pipelines;register file;modems;simultaneous multithreading;smt architecture;logical register;instruction level parallelism;register usage reduction;registers surface mount technology modems hardware physics computing data engineering parallel processing multithreading clocks pipelines;processor scheduling cache storage multi threading parallel architectures;logical registers;parallel processing;frequent values dynamical pressure reduction physical register file register sharing register renaming physical registers microprocessors false data dependence elimination instruction set reuse logical registers pipelines instruction level parallelism simultaneous multithreading register usage reduction logical register physical register storage sharing hardware complexity register pressure reduction smt architecture;hardware;multithreading;dynamical pressure reduction	Using register renaming and physical registers, modern microprocessors eliminate false data dependences from reuse of the instruction set defined registers (logical registers). High performance processors that have longer pipelines and a greater capacity to exploit instruction-level parallelism have more instructions in-flight and require more physical registers. Simultaneous multithreading architectures further exacerbate this register pressure. This paper evaluates two register sharing techniques for reducing register usage. The first technique dynamically combines physical registers having the same value the second technique combines the demand of several instructions updating the same logical register and share physical register storage among them. While similar techniques have been proposed previously, an important contribution of this paper is to exploit only special cases that provide most of the benefits of more general solutions but at a very low hardware complexity. Despite the simplicity, our design reduces the required number of physical registers by more than 10% on some applications, and provides almost half of the total benefits of an aggressive (complex) scheme. More importantly, we show the simpler design to reduce register pressure has significant performance effects in a simultaneous multithreaded (SMT) architecture where register availability can be a bottleneck. Our results show an average of 25.6% performance improvement for an SMT architecture with 160 registers or, equivalently, similar performance as an SMT with 200 registers (25% more) but no register sharing.	central processing unit;instruction-level parallelism;late binding;microprocessor;multithreading (computer architecture);parallel computing;pipeline (computing);processor register;register allocation;register file;register renaming;simultaneous multithreading	Liem Tran;Nicholas Nelson;Fung Ngai;Steven G. Dropsho;Michael C. Huang	2004	IEEE International Symposium on - ISPASS Performance Analysis of Systems and Software, 2004	10.1109/ISPASS.2004.1291358	x86 debug register;parallel processing;computer architecture;parallel computing;real-time computing;memory type range register;multithreading;control register;computer science;operating system;register renaming;stack register;instruction register;processor register;register allocation;memory address register	Arch	-10.206336946170508	50.69049514662669	124144
01aacd49cba035ab80e6c855ae7c2ffd774e110f	analysis of techniques to improve protocol processing latency	memory system;protocol processing behavior;memory cycle;protocol processing latency;dec alpha processor;improve protocol processing latency;instruction cache effectiveness;memory access;protocol stack;protocol processing overhead;key factor;protocol latency;cycles per instruction	This paper describes several techniques designed to improve protocol latency, and reports on their effectiveness when measured on a modern RISC machine employing the DEC Alpha processor. We found that the memory system---which has long been known to dominate network throughput---is also a key factor in protocol latency. As a result, improving instruction cache effectiveness can greatly reduce protocol processing overheads. An important metric in this context is the memory cycles per instructions (mCPI), which is the average number of cycles that an instruction stalls waiting for a memory access to complete. The techniques presented in this paper reduce the mCPI by a factor of 1.35 to 5.8. In analyzing the effectiveness of the techniques, we also present a detailed study of the protocol processing behavior of two protocol stacks---TCP/IP and RPC---on a modern RISC processor.	cpu cache;dec alpha;internet protocol suite;interrupt latency;throughput	David Mosberger;Larry L. Peterson;Patrick G. Bridges;Sean W. O'Malley	1996		10.1145/248156.248164	parallel computing;real-time computing;computer hardware;msi protocol;computer science;operating system;cycles per instruction;mesi protocol;mesif protocol;non-uniform memory access	Networks	-8.245294670188404	52.222892381002104	124173
91391b542e7506300f4ef594bd0ac1fb8c28ae7e	data backup optimization for nonvolatile sram in energy harvesting sensor nodes	sensor nodes energy harvesting systems nonvolatile sram cache memories dead blocks	Nonvolatile static random access memory (nvSRAM) has been widely investigated as a promising on-chip memory architecture in energy harvesting sensor nodes, due to zero standby power, resilience to power failures, and fast read/write operations. However, conventional approaches back up all data from static random access memory into nonvolatile memory when power failures happen. It leads to significant energy overhead and peak inrush current, which has a negative impact on the system performance and circuit reliability. This paper proposes a holistic data backup optimization to mitigate these problems in nvSRAM, consisting of a partial backup algorithm and a run-time adaptive write policy. A statistic dead-block predictor is employed to achieve dead block identification with trivial hardware overhead. An adaptive policy is used to switch between write-back and write-through strategy to reduce the rollback induced by backup failures. Experimental results show that the proposed scheme improves the performance by 4.6% on average while the backup power consumption and the inrush current are reduced by 38.1% and 54% on average compared to the full backup scheme. What is more, the backup capacitor size for energy buffer can be reduced by 40% on average under the same performance constraint.	algorithm;cache (computing);emergency power system;glossary of backup terms;holism;kerrison predictor;mathematical optimization;mobile phone;non-volatile memory;overhead (computing);random access;sensor;static random-access memory	Yongpan Liu;Jinshan Yue;Hehe Li;Qinghang Zhao;Mengying Zhao;Chun Jason Xue;Guangyu Sun;Meng-Fan Chang;Huazhong Yang	2017	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2017.2648841	rollback;memory architecture;parallel computing;real-time computing;computer science;static random-access memory;standby power;nvsram;inrush current;backup;non-volatile memory	Arch	-8.025985661051203	55.26179604314539	124187
30451d1c332282acaa2aca85be34470f1d45266d	leakage-aware dynamic scheduling for real-time adaptive applications on multiprocessor systems	jpeg2000 decoder leakage aware dynamic scheduling real time adaptive application multiprocessor system performance adaptable application embedded system runtime adaptive application scheduling scheme dynamic leakage energy constraint two stage receiver task selection method guided search heuristics;generators;multiprocessor systems;processor scheduling;real time;multimedia application;runtime;embedded system;adaptive applications dynamic scheduling;receivers;embedded systems;power aware computing;leakage power;heuristic algorithms;adaptive applications;dynamic scheduling real time systems multiprocessing systems runtime adaptive scheduling embedded system multimedia systems timing frequency decoding;scheduling problem;processor scheduling embedded systems multiprocessing systems power aware computing;multiprocessing systems;task graphs;algorithm design and analysis;dynamic scheduling	While performance-adaptable applications are gaining increased popularity on embedded systems (especially multimedia applications), efficient scheduling methods are necessary to explore such feature to achieve the most performance outcome. In addition to conventional scheduling requirements such as real-time and dynamic power, emerging challenges such as leakage power and multiprocessors further complicate the formulation and solution of adaptive application scheduling problems. In this paper, we propose a runtime adaptive application scheduling scheme that efficiently distributes the runtime slack in a task graph, to achieve maximized performance under timing and dynamic/leakage energy constraints. A guided-search heuristics is proposed to select the best-fit frequency levels that maximize the additional program cycles of adaptive tasks. Moreover, we devise a two-stage receiver task selection method that runs efficiently at runtime, in order to quickly find the slack distribution targets. Experiments on synthesized tasks and a JPEG2000 decoder are conducted to justify our approach. Results show that our method achieves at least 25% runtime performance increase compared to contemporary approaches, incurring negligible runtime overhead.	curve fitting;embedded system;heuristic (computer science);jpeg 2000;multiprocessing;overhead (computing);performance;real-time clock;requirement;run time (program lifecycle phase);scheduling (computing);slack variable;spectral leakage	Heng Yu;Bharadwaj Veeravalli;Yajun Ha	2010	Design Automation Conference	10.1145/1837274.1837396	fair-share scheduling;embedded system;job shop scheduling;algorithm design;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;distributed computing;least slack time scheduling	EDA	-4.968074879510343	58.86193906062769	124510
66a2208c4ef622e09509d60c09a4e8f6b4a5fb75	analysis of cache-coherence bottlenecks with hybrid hardware/software techniques	coherence protocols;program instrumentation;cache analysis;hardware performance monitors;binary rewriting;hardware performance monitoring;cache coherence;shared memory system;high performance;dynamic binary rewriting;smps	Application performance on high-performance shared-memory systems is often limited by sharing patterns resulting in cache-coherence bottlenecks. Current approaches to identify coherence bottlenecks incur considerable run-time overhead and do not scale. We present two novel hardware-assisted coherence-analysis techniques that reduce trace sizes by two orders of magnitude over full traces. First, hardware performance monitoring is combined with capturing stores in software to provide a lossy-trace mechanism, which is an order of magnitude faster than software-instrumentation-based full-tracing and retains accuracy. Second, selected long-latency loads are instrumented via binary rewriting, which provides even higher accuracy and control over tracing, but requires additional overhead.	binary recompiler;cache coherence;lossy compression;overhead (computing);rewriting;shared memory;tracing (software)	Jaydeep Marathe;Frank Mueller;Bronis R. de Supinski	2006	TACO	10.1145/1187976.1187978	cache coherence;computer architecture;parallel computing;real-time computing;computer science;operating system;switched-mode power supply	Arch	-10.227490411416353	50.72177783871142	124599
4744ae1dc901260de4ed7fc546589d6d79716cc1	cbm: a cooperative buffer management for ssd	flash memories nonvolatile memory random access memory radiation detectors power line communications buffer storage algorithm design and analysis;write sequentiality flash memory cooperative buffer management buffer hit ratio;input output programs cache storage flash memories;garbage collection overhead reduction cbm cooperative buffer management solid state drive openssd flash memory random write reduction i o intensive workload buffer management algorithms buffer hit rate write sequentiality page region block region block granularity performance improvement	Random writes significantly limit the application of Solid State Drive (SSD) in the I/O intensive applications such as scientific computing, Web services, and database. While several buffer management algorithms are proposed to reduce random writes, their ability to deal with workloads mixed with sequential and random accesses is limited. In this paper, we propose a cooperative buffer management scheme referred to as CBM, which coordinates write buffer and read cache to fully exploit temporal and spatial localities among I/O intensive workload. To improve both buffer hit rate and destage sequentiality, CBM divides write buffer space into Page Region and Block Region. Randomly written data is put in the Page Region at page granularity, while sequentially written data is stored in the Block Region at block granularity. CBM leverages threshold-based migration to dynamically classify random write from sequential writes. When a block is evicted from write buffer, CBM merges the dirty pages in write buffer and the clean pages in read cache belonging to the evicted block to maximize the possibility of forming full block write. CBM has been extensively evaluated with simulation and real implementation on OpenSSD. Our testing results conclusively demonstrate that CBM can achieve up to 84% performance improvement and 85% garbage collection overhead reduction compared to existing buffer management schemes.	algorithm;benchmark (computing);computational science;database;garbage collection (computer science);input/output;locality of reference;overhead (computing);page view;random access;randomness;simulation;system migration;web service;write buffer	Qingsong Wei;Cheng Chen;Jun Yang	2014	2014 30th Symposium on Mass Storage Systems and Technologies (MSST)	10.1109/MSST.2014.6855545	parallel computing;real-time computing;computer hardware;computer science;buffer underrun;write buffer;write combining	OS	-12.212874460735192	54.3960413478937	124620
8ec36832acbcf627981ad13c0270f3ae6b687b72	making mapreduce scheduling effective in erasure-coded storage clusters	storage management business data processing encoding parallel processing processor scheduling;runtime;performance gain erasure coded storage clusters mapreduce scheduling storage space performance overhead degraded first scheduling mapreduce performance default locality first scheduling;encoding runtime bandwidth scheduling schedules switches performance gain;scheduling;storage systems erasure coding mapreduce;schedules;performance gain;bandwidth;switches;encoding	With the explosive growth of data, enterprises increasingly adopt erasure coding on storage clusters to save storage space. On the other hand, erasure coding incurs higher performance overhead, especially during recovery. This motivates us to study the feasibility of alleviating performance overhead of erasure coding, while maintaining its storage efficiency advantage. In this paper, we study the performance issue of MapReduce when it runs on erasure-coded storage. We first review our previously proposed degraded-first scheduling, which avoids network bandwidth competition among degraded map tasks in failure mode, and hence improves the MapReduce performance over the default locality-first scheduling in MapReduce. We then show that the basic degraded-first scheduling may not work effectively when there are multiple running MapReduce jobs, and hence we propose heuristics to enhance the degraded-first scheduling design. Simulations demonstrate the performance gain of our enhanced degraded-first scheduling in a multi-job scenario. Our work makes a case that a new design of MapReduce scheduling is critical when we move to erasure-coded storage.	algorithm;apache hadoop;computer simulation;erasure code;experiment;failure cause;heuristic (computer science);job stream;locality of reference;mapreduce;overhead (computing);scheduling (computing);storage efficiency;testbed	Runhui Li;Patrick P. C. Lee	2015	The 21st IEEE International Workshop on Local and Metropolitan Area Networks	10.1109/LANMAN.2015.7114730	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;two-level scheduling;distributed computing;round-robin scheduling	HPC	-15.543408254148652	54.04904911114701	124828
063a0d9169f5f63a55cb2b0eff1c0ae768f45574	flexible desktop application management and its influence on green computing	application crash and restore;virtualization;computer crashes;green products;application virtualization;virtual execution environment;home appliances;servers;power consumption;green computing;operating systems	Environmentally sustainable computing and green computing have been studied addressing the need of reducing power consumption mainly in area of high performance computing systems(HPC), mostly on the server side. However, desktop applications in wide-spread use of large organizations and companies are given relatively less focus. In this paper, we study the application fast deployment and fast crash restore through lightweight application virtualization, which can actually conserve energy by significantly reducing application installation/start up time and application crash restore time. We demonstrate that the proposed lightweight application virtualization method has little performance degradation with a noticeable energy saving by carrying out experiments on lots of commonly used real-world work applications. In addition, we implement the prototype system to help users to continue normal work quickly without losing data after crashing. Several novel mechanisms are utilized to make the process fast. We evaluate the system from aspects of start up time, runtime performance and application compatibility, and using a power evaluation model to measure the effectiveness of the system.	application lifecycle management;application virtualization;crash (computing);desktop computer;elegant degradation;experiment;microsoft windows;operating system;point of view (computer hardware company);prototype;requirement;run time (program lifecycle phase);server (computing);server-side;software deployment;supercomputer;uptime;workspace	Wenlei Zhu;Yongwei Wu;Kang Chen	2015	2015 44th International Conference on Parallel Processing Workshops	10.1109/ICPPW.2015.32	green computing;embedded system;parallel computing;real-time computing;virtualization;application virtualization;computer science;operating system;server;computer network	HPC	-16.255820060564407	53.55035370594122	124837
828247fe3e802ce79f151a8b1e0f18db29183379	layermover: fast virtual machine migration over wan with three-layer image structure		Abstract Live Virtual Machine (VM) migration across data centers is an important technology to facilitate cloud management and deepen the cooperation between cloud providers. Without the support of a shared storage system between data centers, migrating the storage data (i.e. virtual disk) of a VM becomes the bottleneck of live VM migration over Wide Area Network (WAN) due to the contradiction between the low bandwidth of the Internet and the comparatively large size of VM storage data. According to previous studies, many inter- and intra-VM duplicated blocks exist between VM disk images. Therefore, data deduplication is widely used for accelerating VM storage data migration. However, it must make a trade-off between computation cost and transmission benefit. Existing approaches are fragile as they explore only the static data feature of image files without much consideration on data semantics. They may adversely influence on migration performance when the benefit resulting from data deduplication cannot remedy its computation overhead. In this paper, we propose a new space-efficient VM image structure—three-layer structure. According to different functions and features, the data of a VM are separated into an Operating System (OS) layer, a Working Environment (WE) layer, and a User Data (UD) layer. Based on this structure, we design a novel VM migration system—LayerMover. It mainly focuses on improving migration performance through optimizing the data deduplication technique. Our experimental results show that three-layer image structure can improve data sharing between VMs, and the similarity ratio between WE images can reach 70%. The tests for LayerMover indicate that it can be significantly beneficial to VM migration across data centers, especially when multiple VMs which share base images are migrated.	algorithm;cloud management;computation;computer data storage;data center;data deduplication;disk image;interrupt;mathematical optimization;multitier architecture;openvms;operating system;overhead (computing);semantic data model;urban dictionary;virtual machine;z/vm	Fei Zhang;Xiaoming Fu;Ramin Yahyapour	2018	Future Generation Comp. Syst.	10.1016/j.future.2018.01.017	real-time computing;cloud management;data migration;distributed computing;computer science;virtual machine;wide area network;cloud computing;data deduplication;computer data storage;bottleneck	OS	-14.430334322006468	54.649536834839786	124927
fbae94f5e2997d30f99d9327549d2e4e121639e4	data mining in intelligent ssd: simulation-based evaluation	itemsets;random access memory;electronic mail;performance evaluation;data mining;bandwidth;simulation based evaluation data mining algorithms data intensive handling algorithms big data processing hdd hard disk drive ssd solid state device internet applications intelligent ssd;data mining random access memory itemsets instruction sets electronic mail bandwidth performance evaluation;internet big data data mining;instruction sets	Due to an explosive growth of Internet applications, the amount of data has increased enormously. In order to store and process this big data more efficiently, a solid-state device (SSD) has replaced a hard disk drive (HDD) as a primary storage media. In spite of high internal bandwidth, SSD has its performance bottleneck on the host interface whose bandwidth is relatively low. To overcome the problem of performance bottleneck in big data processing, the notion of intelligent SSD (iSSD) was proposed to give computing power to SSD. However, its real implementation has not been provided to the public yet. In this paper, we are going to verify the potential of iSSD in handling data-intensive algorithms. To the end, we first develop an iSSD simulator and then evaluate the performance of data mining algorithms inside iSSD on the top of it in comparison with that by the host CPU. The results reveal that data mining with iSSD outperforms that with host CPUs up to around 300%.	algorithm;big data;central processing unit;computer data storage;data mining;data-intensive computing;disk storage;hard disk drive;internet;simulation;solid-state drive;solid-state electronics	Yong-Yeon Jo;Sang Wook Kim;Moonjun Chung;Hyunok Oh	2016	2016 International Conference on Big Data and Smart Computing (BigComp)	10.1109/BIGCOMP.2016.7425810	computer science;operating system;data mining;database	DB	-13.907922322614056	52.721709904847785	124968
56da22a4c6c72435092976b48726aed6efcbd9fb	compacting register file via 2-level renaming and bit-partitioning	register renaming;register allocation;dynamic superscalar processor;superscalar processor;register file;simultaneous multithreading;power consumption	A large multi-ported rename register file (RRF) is indispensable for simultaneous multithreaded (SMT) processors to hold more intermediate results of in-flight instructions from multiple threads running simultaneously. However, enlarging the RRF incurs longer access delays and more power consumption, both of which are critical to the overall performance and are becoming a bottleneck due to the ever-increasing pipeline depth and issue width in future SMT processors. We propose a novel register renaming scheme called Multi-usable Rename Register with 2-Level renaming and allocating (2L-MuRR), which focuses on more efficient utilization of a fewer number of rename registers. Based on the fact that the effective bit-width of most operands is narrower than the full-bit width of a register entry, 2LMuRR partitions each rename register into several fields of different widths. Either a single field or a field combination can hold an operand, thus making each rename register multi-usable. In addition, 2L-MuRR postpones the register allocation to the write-back stage, which is similar to the formerly proposed virtual–physical-register (VPR) scheme, further reducing the meaningless RRF occupancy. The simulations show that 2L-MuRR improves the efficiency of the RRF significantly, achieving higher performance with much fewer rename registers. For example, when the RRF size is 60, 2L-MuRR outperforms Trad (traditional register renaming approach) and VPR in terms of IPC by 38% and 11%, respectively, while decreases the RRF occupancy by 37% and 15%, respectively. 2006 Elsevier B.V. All rights reserved.	cache (computing);central processing unit;ibm notes;multithreading (computer architecture);operand;register allocation;register file;register renaming;rename (relational algebra);simulation;simultaneous multithreading;task parallelism;thread (computing);wearable computer	Hua Yang;Gang Cui;Hongwei Liu;Xiao-Zong Yang	2007	Microprocessors and Microsystems	10.1016/j.micpro.2006.08.004	computer architecture;parallel computing;real-time computing;control register;computer science;operating system;register renaming;stack register;processor register;simultaneous multithreading;register allocation;register file;status register	Arch	-8.100960412640164	53.50885546587164	125059
b585f83bfb6f26d375ceca124a091a8eead14d75	bandwidth adaptive write-update optimizations for chip multiprocessors	bandwidth adaptive;protocols;bandwidth allocation;bandwidth adaptive cache coherence multi core;protocols coherence bandwidth principal component analysis radiation detectors system on a chip optimization;protocols bandwidth allocation microprocessor chips multiprocessing systems;interconnection utilization bandwidth adaptive write update optimization chip multiprocessor system coherence misses scalable cache coherence protocols cmp adaptive hybrid protocol write invalidate based protocols write invalidate scheme producer consumer sharing pattern adaptive protocol interconnection resource aware mechanism full system simulation dynamic hybrid protocols performance analysis splash 2 nas parallel benchmark suites cache to cache sharing misses;cache coherence;multiprocessing systems;multi core;microprocessor chips	Chip Multiprocessors (CMPs) have different technological parameters and physical constraints than earlier multi-processor systems, which should be taken into consideration when designing cache coherence protocols. Also, contemporary cache coherence protocols use invalidate schemes that are known to generate a high number of coherence misses. This is especially true under producer-consumer sharing patterns that can become a performance bottleneck as the number of cores increases. This paper presents two mechanisms to design efficient and scalable cache coherence protocols for CMPs. First, we propose an adaptive hybrid protocol to reduce coherence misses observed in write-invalidate based protocols. The proposed protocol is based on a write-invalidate scheme. However, adaptively, it can push updates to potential consumers based on observed producer- consumer sharing patterns. Secondly, we extend this adaptive protocol with an interconnection resource aware mechanism. Experimental evaluations, conducted on a tiled-CMP via full- system simulation, were used to assess the performance from our proposed dynamic hybrid protocols. Performance analysis is presented on a set of scientific applications from the SPLASH- 2 and NAS parallel benchmark suites. Results showed that the proposed mechanisms reduce cache-to-cache sharing misses up to 48% and in return speed up application performance up to 25%. In addition, the proposed interconnection resource aware mechanism is proven to perform well under varying interconnection utilizations.	benchmark (computing);cpu cache;cache coherence;central processing unit;computer architecture simulator;first-order predicate;interconnection;kilobyte;manycore processor;multi-core processor;multiprocessing;network-attached storage;producer–consumer problem;profiling (computer programming);scalability;simulation;speedup	Abdullah Kayi;Olivier Serres;Tarek A. El-Ghazawi	2012	2012 IEEE 10th International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2012.34	bus sniffing;multi-core processor;communications protocol;cache coherence;parallel computing;real-time computing;computer science;write-once;cache invalidation;operating system;distributed computing;mesi protocol;mesif protocol;computer network;bandwidth allocation	HPC	-9.173419138587425	51.30156410135299	125188
72722e7602138e3896e5576d3f3ef730e7b7c4b4	the multi-streamed solid-state drive		This paper makes a case for the multi-streamed solidstate drive (SSD). It offers an intuitive storage interface for the host system to inform the SSD about the expected lifetime of data being written. We show through experimentation with a real multi-streamed SSD prototype that the worst-case update throughput of a Cassandra NoSQL DB system can be improved by nearly 56%. We discuss powerful use cases of the proposed SSD interface.	best, worst and average case;oracle nosql db;prototype;solid-state drive;streaming media;throughput	Jeong-Uk Kang;Jeeseok Hyun;Hyunjoo Maeng;Sangyeun Cho	2014			computer hardware;computer science;operating system;database	OS	-14.386149460831081	53.55778560502637	125231
e7cebf9cc8d2af11d0be908371924290aa8888a0	using cache-coloring to mitigate inter-set write variation in non-volatile caches		In recent years, researchers have explored use of non-volat ile devices such as STT-RAM (spin torque transfer RAM) for designing on-chip caches, since they provide high density a nd consume low leakage power. A common limitation of all nonvolatile devices is their limited write endurance. Further , since existing cache management policies are write-varia tion unaware, excessive writes to a few blocks may lead to a quick failure of the whole cache. We propose an architectural technique for w ea leveling of non-volatile last level caches (LLCs). Our tech nique uses cache-coloring approach which adds a software-c ontrolled mapping layer between groups of physical pages and cache set s. Periodically the mapping is altered to ensure that writet affic can be spread uniformly to different sets of the cache to achi eve wear-leveling. Simulations performed with an x86-64 si mulator and SPEC2006 benchmarks show that our technique reduces the worst-case writes to cache blocks and thus improves the cach e lifetime by 4.07×.	best, worst and average case;cache coloring;computer simulation;eve;graph coloring;non-volatile memory;random-access memory;spectral leakage;wear leveling;x86;x86-64	Sparsh Mittal	2013	CoRR		bus sniffing;embedded system;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cpu cache;tag ram;computer hardware;cache;computer science;write-once;cache invalidation;operating system;smart cache;cache algorithms;cache pollution	Arch	-7.360346977707179	55.255747961753954	125321
14dc489a1773afd67adec0e93ba0942c6ef84cc0	characterization of tpc-h queries for a column-oriented database on a dual-core amd athlon processor	tpc h;performance profiling;decision support system;monetdb;column oriented databases	In this paper, we characterize the performance of the TPC-H benchmark for a popular column-oriented database called MonetDB running on a dual-core AMD Athlon machine. Specifically, we measure the performance of key microarchitectural components and analyze in detail the nature of various stalls namely cache stalls, branch misprediction stalls and resource stalls. We compare our results with published results on the characterization of TPC-H for row-oriented databases. As opposed to the previous approaches, we use thread-level monitoring of database threads to study the performance of the database in isolation from the rest of the system.	athlon;benchmark (computing);branch misprediction;cpu cache;column-oriented dbms;database;ibm tivoli storage productivity center;microarchitecture;monetdb;multi-core processor	Pranav Vaidya;Jaehwan John Lee	2008		10.1145/1458082.1458306	real-time computing;decision support system;computer science;operating system;data mining;database;world wide web	DB	-12.066890469014337	51.62921988971084	125537
b6259c4fddc79f8fc9817f7a224ca93283ae2585	area-aware optimizations for resource constrained branch predictors exploited in embedded processors	optimising compilers;front end;optimization technique;branch prediction;power aware computing embedded systems instruction sets microprocessor chips optimising compilers;low complexity;embedded systems;power aware computing;front end gating techniques area aware optimizations resource constrained branch predictors embedded processors low complexity branch prediction technique;constraint optimization history pipelines costs counting circuits runtime performance evaluation switches hardware embedded computing;embedded processor;microprocessor chips;instruction sets	Modern embedded processors (e.g., Intel's XScale) use small and simple branch predictors to improve performance. Such predictors impose little area and power overhead but may offer low accuracy. As a result, branch misprediction rate could be high. Such mispredictions result in longer program runtime and wasted activity. To address this inefficiency, we introduce two optimization techniques: first, we introduce an adaptive and low-complexity branch prediction technique. Our branch predictor removes up to a maximum of 50% of the branch mispredictions of a bimodal predictor. This results in improving performance by up to 16%. Second, we present front-end gating techniques and reduce wasted activity up to a maximum of 32%	branch misprediction;branch predictor;central processing unit;computer performance;embedded system;kerrison predictor;mathematical optimization;overhead (computing);xscale	Babak Salamat;Amirali Baniasadi;Kaveh Jokar Deris	2006	2006 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation	10.1109/ICSAMOS.2006.300808	computer architecture;parallel computing;real-time computing;computer science;branch predictor	Arch	-5.290289498023316	53.31176286011317	125573
abdc1fb4634b5a39f45ce5f35ee76dac1e345109	system noise revisited: enabling application scalability and reproducibility with smt	instruction sets hardware scalability linux interference benchmark testing;parallel performance;scientific information systems hydrodynamics linux multi threading;interference;system noise;parallel performance system noise jitter simultaneous multithreading smt scalability reproducibility;linux;smt;scalability;simultaneous multithreading;jitter;high order finite element shock hydrodynamics application scientific application scalability scientific application reproducibility system noise production commodity clusters linux ecosystem simultaneous multithreading smt pervasive architectural feature operating system u s department of energy;benchmark testing;instruction sets;hardware;reproducibility	Despite significant advances in reducing system noise, the scalability and performance of scientific applications running on production commodity clusters today continue to suffer from the effects of noise. Unlike custom and expensive leadership systems, the Linux ecosystem provides a rich set of services that application developers utilize to increase productivity and to ease porting. The cost is the overhead that these services impose on a running application, negatively impacting its scalability and performance reproducibility. In this work, we propose and evaluate a simple yet effective way to isolate an application from system processes by leveraging Simultaneous Multi-Threading (SMT), a pervasive architectural feature on current systems. Our method requires no changes to the operating system or to the application. We quantify its effectiveness on a diverse set of scientific applications of interest to the U. S. Department of Energy showing performance improvements of up to 2.4 times at 16,384 tasks for a high-order finite elements shock hydrodynamics application. Finally, we provide guidance to system and application developers on how to best leverage SMT under different application characteristics and scales.	critical path method;ecosystem;finite element method;linux;operating system;overhead (computing);scalability	Edgar A. León;Ian Karlin;Adam Moody	2016	2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)	10.1109/IPDPS.2016.48	benchmark;parallel computing;real-time computing;scalability;jitter;computer science;operating system;reproducibility;instruction set;distributed computing;interference;programming language;simultaneous multithreading;linux kernel	HPC	-10.588907480125691	47.716939310068724	125661
deaefd1bbbb873bc1c9f6504acc75095d9c65f57	performance evaluation of ssd-index maintenance schemes in ir applications	performance evaluation;ssd solid state drive;information retrieval;index maintenance	— With the advent of flash memory based new storage device (SSD), there is considerable interest within the computer industry in using flash memory based storage devices for many different types of application. The dynamic index structure of large text collections has been a primary issue in the Information Retrieval Applications among them. Previous studies have proven the three approaches to be effective: In-Place, merge-based index structure and a combination of both. The above-mentioned strategies have been researched with the traditional storage device (HDD) which has a constraint on how keep the contiguity of dynamic data. However, in case of the new storage device, we don't have any constraint contiguity problems due to its low access latency time. But, although the new storage device has superiority such as low access latency and improved I/O throughput speeds, it is still not well suited for traditional dynamic index structures because of the poor random write throughput in practical systems. Therefore, using the experimental performance evaluation of various index maintenance schemes on the new storage device, we propose an efficient index structure for new storage device that improves significantly the index maintenance speed without degradation of query performance.	database index;dynamic data;elegant degradation;flash memory;hard disk drive;information retrieval;input/output;mpeg-21;performance evaluation;random access;semantic web;solid-state drive;stable storage;throughput	Du-Seok Jin;Hoe-Kyung Jung	2010	J. Inform. and Commun. Convergence Engineering	10.6109/jicce.2010.8.4.377	real-time computing;computer hardware;computer science;database	OS	-13.663894112405805	54.410519222008446	125741
e4f022e459786247969d8337bab3e686f268d63c	benefits and drawbacks of redundant batch requests	job performance;waiting time;parallel computer;middleware;job scheduling	Most parallel computing platforms are controlled by batch schedulers that place requests for computation in a queue until access to compute nodes is granted. Queue waiting times are notoriously hard to predict, making it difficult for users not only to estimate when their applications may start, but also to pick among multiple batch-scheduled platforms the one that will produce the shortest turnaround time. As a result, an increasing number of users resort to “redundant requests”: several requests are simultaneously submitted to multiple batch schedulers on behalf of a single job; once one of these requests is granted access to compute nodes, the others are canceled. Using simulation as well as experiments with a production batch scheduler we evaluate the impact of redundant requests on (1) average job performance, (2) schedule fairness, (3) system load, and (4) system predictability. We find that some of the popularly held beliefs about the harmfulness of redundant batch requests are unfounded. We also find that the two most critical issues with redundant requests are the additional load on current middleware infrastructures and unfairness towards users who do not use redundant requests. Using our experimental results we quantify both impacts in terms of the number of users who use redundant requests and of the amount of request redundancy these users employ.	batch processing;computation;experiment;fairness measure;job scheduler;load (computing);middleware;parallel computing;scheduling (computing);simulation	Henri Casanova	2007	Journal of Grid Computing	10.1007/s10723-007-9068-6	real-time computing;fscan;computer science;job scheduler;operating system;job performance;middleware;database;distributed computing	HPC	-16.653689098408886	59.67609273469222	125768
6d28f2ebef08c94f91b549cc10868086d19c8bbb	phase-guided scheduling on single-isa heterogeneous multicore processors	asymmetric multiprocessors;single isa heterogeneous multicore processors;phase identification;multiprocessor scheduling;power efficient cores phase guided scheduling single isa heterogeneous multicore processors;scheduling multiprocessing systems;multicore processing processor scheduling out of order hardware;processor scheduling;out of order;scheduling;multicore processing;phase identification single isa heterogeneous multicore processors asymmetric multiprocessors scheduling;multicore processors;multiprocessing systems;hardware	Single-ISA heterogeneous (also known as asymmetric) multicore processors offer significant advantages over homogenous multicores in terms of both power and performance. Power-efficient cores can be paired with higher-performance cores to achieve advantageous power/performance tradeoffs. Unfortunately, such processors also create unique challenges in effective mapping of processes to cores. The greater the diversity of cores, the more complex this problem becomes. Previous scheduling approaches sample performance while permuting the schedule across each type of core each time a change in application behavior is detected. However, approaches that require frequent sampling of the performance of threads (or combinations of threads) on each core may be impractical. We propose scheduling threads on a heterogeneous multicore processor using not just the detection of a change in program behavior or phase, but instead an identification and recording of these phase behaviors. We highlight the correlation between the execution phases of an application and the performance of those phases on any particular core type. We present mechanisms that exploit this correlation between program phases and appropriate scheduling decisions and demonstrate near optimal mapping of thread segments to processor cores can be done without frequently sampling the performance of each thread on each processor core type.	central processing unit;computer multitasking;gibbs sampling;instructions per cycle;machine learning;multi-core processor;multithreading (computer architecture);performance evaluation;sampling (signal processing);scheduling (computing);speedup;thread (computing);throughput	Lina Sawalha;Sonya R. Wolff;Monte P. Tull;Ronald D. Barnes	2011	2011 14th Euromicro Conference on Digital System Design	10.1109/DSD.2011.98	multi-core processor;fair-share scheduling;fixed-priority pre-emptive scheduling;computer architecture;parallel computing;real-time computing;gang scheduling;computer science;operating system;two-level scheduling	Arch	-5.617764993336476	51.77766365277049	125818
d568766a841d862b45a8bdbf9f846fb6bacd96f6	improving reliability for real-time systems through dynamic recovery		Technology scaling has increased concerns about transient faults due to soft errors and permanent faults due to lifetime wear processes. Although researchers have investigated related problems, they have either considered only one of the two reliability concerns or presented simple recovery allocation algorithms that cannot effectively use available time slack to improve soft-error reliability. This paper introduces a framework for improving soft-error reliability while satisfying lifetime reliability and real-time constraints. We present a dynamic recovery allocation technique that guarantees to recover any failed task if the remaining slack is adequate. Based on this technique, we propose two scheduling algorithms for task sets with different characteristics to improve system-level soft-error reliability. Lifetime reliability requirements are satisfied by reducing core frequencies for appropriate tasks, thereby reducing wear due to temperature and thermal cycling. Simulation results show that the proposed framework reduces the probability of failure by at least 8% and 73% on average compared to existing approaches.	algorithm;image scaling;memory management;real-time clock;real-time computing;real-time transcription;requirement;scheduling (computing);simulation;slack variable;soft error	Yue Ma;Thidapat Chantem;Robert P. Dick;Xiaobo Sharon Hu	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342062	computer science;real-time computing;task analysis;scheduling (computing);dynamic priority scheduling;scaling;schedule;temperature cycling	Embedded	-5.91872511303882	58.32373962937122	125969
b4cb920a01213cf73e54ea27d519ed493cd48879	improving service time with a multicore aware middleware		One of the major advantages of communication middleware is its independence from the underlying hardware platform. This improves portability and interoperability, whereas following the mainstream trend of favoring abstraction over performance or execution optimization. However, for time sensitive applications, this lack of integration with the hardware may fall short as performance is lowered and attention to priority requests is not sufficiently differentiated. In this paper, we propose a middleware that has a higher degree of integration with the underlying hardware platform; it uses the mechanisms of the operating system to control the use of the processing cores, reserving them as needed for supporting differentiated service to higher priority invoking nodes or clients. Results show that our middleware improves the service time of high priority clients and it offers stable communication times.	differentiated service;interoperability;mathematical optimization;middleware;multi-core processor;operating system	Marisol García-Valls;Christian Calva-Urrego	2017		10.1145/3019612.3019741	parallel computing;real-time computing;operating system;middleware	OS	-17.54010606988077	55.98175938690397	126045
ae0a1d4a0566727d7f265232e6fcf2144b3beddb	performance tradeoffs in multithreaded processors	multiprocessor interconnection networks;data sharing;index termsmultithreaded processors;network bandwidth;performance evaluation;caches;multiprocessingsystems;storage management;buffer storage;parallel programming;caches multithreaded processors cache interference network contention context switching overhead data sharing network bandwidth;cache interference;switching theory buffer storage multiprocessing systems multiprocessor interconnection networks parallel algorithms parallel programming performance evaluation storage management;context switchingoverhead;network contention;multithreading delay switches performance analysis bandwidth parallel processing synchronization process design pipelines;switching theory;performance model;multiprocessing systems;parallelprogramming;parallel algorithms	An analytical performance model for multithreaded processors that includes cache interference, network contention, context-switching overhead, and data-sharing effects is presented. The model is validated through the author's simulations and by comparison with previously published simulation results. The results indicate that processors can substantially benefit from multithreading, even in systems with small caches, provided sufficient network bandwidth exists. Caches that are much larger than the working-set sizes of individual processes yield close to full processor utilization with as few as two to four contexts. Smaller caches require more contexts to keep the processor busy, while caches that are comparable in size to the working-sets of individual processes cannot achieve a high utilization regardless of the number of contexts. Increased network contention due to multithreading has a major effect on performance. The available network bandwidth and the context-switching overhead limits the best possible utilization. >	thread (computing)	Anant Agarwal	1992	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.159037	parallel computing;real-time computing;computer science;distributed computing;parallel algorithm	Arch	-10.296710835961981	50.14181055782459	126321
88e3bef94da79147b828aaad339201d0b5bbe78b	an enhanced scheduler for mc2		Mixed Criticality on MultiCore (MC2) is an ongoing project that explores the issue of providing operating system support for implementing mixed-criticality workloads upon multicore CPUs. Thus far, this project has primarily focused on implementation concerns - can mixed criticality be implemented upon multicores without incurring significant run-time implementation overhead? In this paper, we focus upon the workload model assumed in the MC2 framework: we propose several generalizations to the workload model and devise Integer Linear Programming (ILP) based scheduling and schedulability-analysis algorithms for the resulting generalized model.	algorithm;central processing unit;computation;computational complexity theory;criticality matrix;experiment;integer programming;linear programming;mixed criticality;multi-core processor;operating system;overhead (computing);scheduling (computing);scheduling analysis real-time systems;self-organized criticality	Sanjoy K. Baruah	2017		10.1145/3139258.3139271	distributed computing;workload;scheduling (computing);multi-core processor;mixed criticality;computer science;integer programming	Embedded	-8.525864978998353	59.19232314956622	126344
18eb097dad1ae567ccf42131e33918acc9a818f9	high performance computing systems for autonomous spaceborne missions	real time;mission planning;chip;processor in memory;graceful degradation;solar system;high performance computer;next generation;space missions;commodity off the shelf;system architecture;memory bandwidth;jet propulsion laboratory	Future generation space missions across the solar system to the planets, moons, asteroids, and comets may someday incorporate supercomputers both to expand the range of missions being conducted and to significantly reduce their cost. By performing science computation directly on the spacecraft itself, the amount of data required to be downlinked may be reduced by many orders of magnitude, thus greatly reducing the mass of the resources needed for communication while increasing the quality and quantity of the science achieved. By performing the mission planning in real time directly on the spacecraft, complex and highly responsive missions can be conducted out of range of direct human intervention and the cost of mission management can be reduced. Through highly replicated computing structures, continued operation can be maintained in the presence of faults by means of graceful degradation. Two classes of systems, reflecting very different strategies of computer system architecture, are actively being pursued by the NASA Jet Propulsion Laboratory to take advantage of the opportunity of embedded high performance computing on spacecraft for deep space missions. Commodity off-theshelf (COTS) Clusters may permit the direct application of commercial computing hardware in loosely coupled ensembles to benefit from the enormous investment of industry in mass-market components. New Processor-in-Memory (PIM) architectures combine multiple nodes on a single chip of processor-memory pairs exposing the full memory bandwidth. This paper examines the driving issues motivating the use of supercomputing for future deep space missions and describes two active research projects at NASA JPL that are pursuing both the COTS and PIM strategies for next generation spaceborne computing.	computation;computer architecture;computer hardware;elegant degradation;embedded system;fault tolerance;loose coupling;memory bandwidth;next-generation network;supercomputer;systems architecture	Thomas L. Sterling;Daniel S. Katz;Larry A. Bergman	2001	IJHPCA	10.1177/109434200101500306	chip;embedded system;fault tolerance;parallel computing;real-time computing;simulation;computer science;space exploration;operating system;solar system;memory bandwidth;systems architecture	HPC	-9.463302812474607	46.77917543529079	126355
03d687fd3fc4d2d00f03f434d2c531ffff79580b	a quantitative performance evaluation of sci memory hierarchies		"""The Scalable Coherent Interface (SCI) is an IEEE standard that defines a hardware platform for scalable shared-memory multiprocessors. SCI consists of three parts. The first is a set of physical interfaces that defines board sizes, wiring and network clock rates. The second is a communication protocol based on unidirectional point to point links. The third defines a cache coherence protocol based on a full directory that is distributed amongst the cache and memory modules. The cache controllers keep track of the copies of a given datum by maintaining them in a doubly linked list. SCI can scale up to 65520 nodes. This dissertation contains a quantitative performance evaluation of an SCIconnected multiprocessor that assesses both the communication and cache coherence subsystems. The simulator is driven by reference streams generated as a by-product of the execution of """"real"""" programs. The workload consists of three programs from the SPLASH suite and three parallel loops. The simplest topology supported by SCI is the ring. It was found that, for the hardware and software simulated, the largest efficient ring size is between eight and sixteen nodes and that raw network bandwidth seen by processing elements is limited at about 80Mbytes/s. This is because the network saturates when link traffic reaches 600-700Mbytes/s. These levels of link traffic only occur for two poorly designed programs. The other four programs generate low traffic and their execution speed is not limited by interconnect nor cache coherence protocol. An analytical model of the multiprocessor is used to assess the cost of some frequently occurring cache coherence protocol operations. In order to build large systems, networks more sophisticated than rings must be used. The performance of SCI meshes and cubes is evaluated for systems of up to 64 nodes. As with rings, processor throughput is also limited by link traffic for the same two poorly designed programs. Cubes are 10-15% faster than meshes for programs that generate high levels of network traffic. Otherwise, the differences are negligible. No significant relationship between cache size and network dimensionality was found."""	cache coherence;clock rate;coherent;communications protocol;cubes;dimm;directory (computing);doubly linked list;geodetic datum;memory hierarchy;multiprocessing;network packet;olap cube;performance evaluation;scalability;shared memory;simulation;throughput;wiring	Roberto A. Hexsel	1994				Arch	-11.384264774422569	46.990238569453666	126476
a0796edf402715d2f0aae5168c1554ab5ebc26f2	reducing dram refresh overheads with refresh-access parallelism		This article summarizes the idea of “refresh–access parallelism,” which was published in HPCA 2014 [17], and examines the work’s signi cance and future potential. The overarching objective of our HPCA 2014 paper is to reduce the signi cant negative performance impact of DRAM refresh with intelligent memory controller mechanisms. To mitigate the negative performance impact of DRAM refresh, our HPCA 2014 paper proposes two complementary mechanisms, DARP (Dynamic Access Refresh Parallelization) and SARP (Subarray Access Refresh Parallelization). The goal is to address the drawbacks of state-of-the-art per-bank refresh mechanism by building more e cient techniques to parallelize refreshes and accesses within DRAM. First, instead of issuing per-bank refreshes in a round-robin order, as it is done today, DARP issues per-bank refreshes to idle banks in an out-of-order manner. Furthermore, DARP proactively schedules refreshes during intervals when a batch of writes are draining to DRAM. Second, SARP exploits the existence of mostly-independent subarrays within a bank. With minor modi cations to DRAM organization, it allows a bank to serve memory accesses to an idle subarray while another subarray is being refreshed. Our extensive evaluations on a wide variety of workloads and systems show that our mechanisms improve system performance (and energy e ciency) compared to three state-of-the-art refresh policies, and their performance bene ts increase as DRAM density increases.		Kevin K. Chang;Donghyuk Lee;Zeshan Chishti;Alaa R. Alameldeen;Chris Wilkerson;Yoongu Kim;Onur Mutlu	2018	CoRR		parallel computing;real-time computing;overhead (business);memory controller;schedule;computer science;dram;exploit;idle	Arch	-9.287188406229578	54.00335893251524	126517
bdd823bea4a51a09c2fd860621e83e378fc1644b	seer-mcache: a prefetchable memory object caching system for iot real-time data processing		Memory object caching systems, such as Memcached and Redis, have been proved to be a simple and high-efficient middleware for improving the performance of Internet of Things (IoT) devices querying the database in cloud. However, its performance guarantee is built on the fact that the target data, queried by the IoT device, will be accessed many times and hit in the caching system. Therefore, when database system is handling the unrepeated IoT queries, it usually presents the suboptimal performance, which greatly impairs the efficiency of real-time data processing on IoT devices. To improve this issue, we propose Seer-MCache, the memory object caching system with a smart prefetching (read-ahead) function, to fill up the caching system with the desired data before the intensive IoT queries arriving. Seer-MCache includes a set of rules to launch the specific behaviors of read-head. These rules are able to be customized according to the workload characteristics and system load. We implement a prototype system in Redis (caching layer) and MySQL server (database system). Extensive experiments are conducted to verify the effectiveness of Seer-MCache, the results show that Seer-MCache can improve the performance of read-intensive workload up to 61% (39.5% in average). Meanwhile, the cost of the read-ahead behavior is moderate and controllable.	big data;cpu cache;cache (computing);computation;cyberspace;data infrastructure;database;disk buffer;experiment;facial recognition system;internet of things;load (computing);locality of reference;memcached;middleware;mysql;principle of locality;prototype;real-time data;real-time locating system;redis;requirement;seer-sem;server (computing);web cache	Dingding Li;Mianxiong Dong;Yanting Yuan;J. Y. Chen;Kaoru Ota;Yong Tang	2018	IEEE Internet of Things Journal	10.1109/JIOT.2018.2868334	workload;distributed computing;real-time data;cloud computing;real-time computing;feature extraction;computer science;middleware;data processing;internet of things	DB	-17.94613911592089	56.14740608603621	126595
c93161d6ea3eb5020a46caf861d88832641375d8	compressed differential erasure codes for efficient archival of versioned data		In this paper, we study the problem of storing an archive of versioned data in a reliable and efficient manner in distributed storage systems. We propose a new storage technique called differential erasure coding (DEC) where the differences (deltas) between subsequent versions are stored rather than the whole objects, akin to a typical delta encoding technique. However, unlike delta encoding techniques, DEC opportunistically exploits the sparsity (i.e., when the differences between two successive versions have few non-zero entries) in the updates to store the deltas using compressed sensing techniques applied with erasure coding. We first show that DEC provides significant savings in the storage size for versioned data whenever the update patterns are characterized by in-place alterations. Subsequently, we propose a practical DEC framework so as to reap storage size benefits against not just in-place alterations but also real-world update patterns such as insertions and deletions that alter the overall data sizes. We conduct experiments with several synthetic workloads to demonstrate that the practical variant of DEC provides significant reductions in storage overhead (up to 60% depending on the workload) compared to baseline storage system which incorporates concepts from Rsync, a delta encoding technique to store and synchronize data across a network.	archive;baseline (configuration management);clustered file system;compressed sensing;computer data storage;delta encoding;dhrystone;erasure code;experiment;in-place algorithm;overhead (computing);software versioning;sparse matrix;rsync	J. Harshan;Anwitaman Datta;Frédérique E. Oggier	2015	CoRR		real-time computing;telecommunications;computer science;theoretical computer science;database	OS	-13.861413957251164	54.826759412110526	126597
0fde2fdef5670131475d6f4fc5b96e132ebdf82f	memory based metadata server for cluster file systems	cluster file system;file servers;storage management distributed databases file servers meta data network operating systems;reliability;cluster file system metadata server;metadata management;high performance computing;network operating systems;storage management;matrix organization;test bed;file servers file systems high performance computing computer architecture system performance scalability costs distributed computing grid computing design methodology;system performance;static scalability method;servers;memory based metadata server;file system;high performance computer;distributed databases;distributed file system;meta data;scalability;organizations;metadata server;high performance;storage management memory based metadata server cluster file system high performance computing distributed file system metadata management system matrix organization nonoverhead reliable mechanism static scalability method;metadata management system;benchmark testing;file systems;throughput;nonoverhead reliable mechanism	In high performance computing environment, the metadata servers of distributed file system become critical to impact overall system performance. An approach of memory based metadata server is proposed, instead of the disk based approach. We present a metadata management system with matrix organization, non-overhead reliable mechanism and static scalability method, which is design to efficiently utilize large memory and provide high performance. We examine and demonstrate the performance, overhead of reliability and scalability in a test bed environment of 28 machines. The result shows that the performance of our system is higher than other traditional distributed file system, the reliability can be achieved with little overhead and the metadata servers can be linear scaling.	ab initio quantum chemistry methods;backup;clustered file system;dspace;emulator;experiment;image scaling;lustre;overhead (computing);parallel virtual file system;proxy server;redo log;scalability;server (computing);supercomputer;testbed	Jing Xing;Jin Xiong;Jie Ma;Ninghui Sun	2008	2008 Seventh International Conference on Grid and Cooperative Computing	10.1109/GCC.2008.76	file server;benchmark;throughput;scalability;computer science;organization;operating system;matrix management;journaling file system;reliability;database;computer performance;distributed file system;data file;metadata;world wide web;distributed database;server;testbed	HPC	-18.203863660474273	52.20632785578709	126654
168b208bb836153d3e8dfa557a3593a0ffbf4c4b	memory footprint reduction with quasi-static shared libraries in mmu-less embedded systems	flash memory;random access memory;dsl;software libraries;real time operating system;embedded system;runtime library;home network;solid state circuits;registers;joining processes;embedded system solid state circuits joining processes registers software libraries runtime library dsl home automation random access memory flash memory;asymmetric digital subscriber line;home automation	Despite a rapid decrease in the price of solid state memory devices, system memory is still a very precious resource in embedded systems. The use of shared libraries is known to be effective in significantly reducing memory usage. Unfortunately, many resource-constrained embedded systems lack MMU, making it extremely difficult to support this technique. To address this problem, we propose a novel shared library scheme called the quasi-static shared library. In quasi-static shared libraries, global symbols are bound to pseudo-addresses at linking time and the actual physical addresses are bound at loading time. This scheme is made possible by emulating MMU’s memory mapping feature with a Data Section Base Register (DSBR) and a Data Section Base Table (DSBT). Quasi-static shared libraries do not require symbol tables which take up time and space at runtime. We have implemented the proposed scheme in a commercial ADSL (Asymmetric Digital Subscriber Line) home network gateway and conducted a series of experiments measuring its memory usage and performance overhead. The result is drastic: a 35% reduction in flash memory usage and a 10% reduction in RAM usage. These results were achieved with only a negligible performance penalty of less than 4%. Even though this scheme was applied to uClinux-based embedded systems, it can be used for any MMU-less real-time operating system.	asymmetric digital subscriber line;embedded system;emulator;experiment;flash memory;gateway (telecommunications);library (computing);memory footprint;memory management unit;overhead (computing);random-access memory;real-time clock;real-time operating system;run time (program lifecycle phase);solid-state drive;symbol table;uptime;μclinux	Jaesoo Lee;Jiyong Park;Seongsoo Hong	2006	12th IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS'06)	10.1109/RTAS.2006.26	uniform memory access;distributed shared memory;memory footprint;shared memory;embedded system;home automation;interleaved memory;semiconductor memory;parallel computing;real-time computing;real-time operating system;digital subscriber line;distributed memory;memory refresh;computer science;physical address;virtual memory;operating system;computer data storage;computer memory;overlay;conventional memory;extended memory;processor register;flat memory model;asymmetric digital subscriber line;programming language;registered memory;data diffusion machine;memory map;memory management	Embedded	-15.709805641398361	51.02941130194124	126737
3ab4c9eb4e64eb40431bf392f1b00619056c2380	evaluation of simple causal message logging for large-scale fault tolerant hpc systems	mean time between failure;program processors protocols computer crashes fault tolerance fault tolerant systems supercomputers receivers;system monitoring checkpointing fault tolerant computing multiprocessing systems parallel machines;communications;fault tolerant;collective communication;performance;system monitoring;checkpointing;mathematics computing and information science;large scale;fault tolerant computing;low latency;benchmarks;next generation;parallel machines;evaluation;tolerance;multiprocessing systems;exascale supercomputers simple causal message logging protocol large scale fault tolerant hpc systems petascale computing;productivity;nas parallel benchmarks;supercomputers;parallel applications;message logging	The era of petascale computing brought machines with hundreds of thousands of processors. The next generation of exascale supercomputers will make available clusters with millions of processors. In those machines, mean time between failures will range from a few minutes to few tens of minutes, making the crash of a processor the common case, instead of a rarity. Parallel applications running on those large machines will need to simultaneously survive crashes and maintain high productivity. To achieve that, fault tolerance techniques will have to go beyond checkpoint/restart, which requires all processors to roll back in case of a failure. Incorporating some form of message logging will provide a framework where only a subset of processors are rolled back after a crash. In this paper, we discuss why a simple causal message logging protocol seems a promising alternative to provide fault tolerance in large supercomputers. As opposed to pessimistic message logging, it has low latency overhead, especially in collective communication operations. Besides, it saves messages when more than one thread is running per processor. Finally, we demonstrate that a simple causal message logging protocol has a faster recovery and a low performance penalty when compared to checkpoint/restart. Running NAS Parallel Benchmarks (CG, MG, BT and DT) on 1024 processors, simple causal message logging has a latency overhead below 5%.	application checkpointing;causal filter;central processing unit;crash (computing);experiment;fault tolerance;image scaling;mean time between failures;multi-core processor;nas parallel benchmarks;overhead (computing);petascale computing;scalability;supercomputer;transaction processing system	Esteban Meneses;Greg Bronevetsky;Laxmikant V. Kalé	2011	2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum	10.1109/IPDPS.2011.307	system monitoring;fault tolerance;productivity;parallel computing;real-time computing;mean time between failures;performance;computer science;evaluation;operating system;distributed computing;low latency	Arch	-17.762478706965105	49.785271601367214	126792
0ef81c293055f4502826cc645b262876b3ffc750	blast: off-the-shelf hardware for building an efficient hash-based cluster storage system	off the shelf hardware;kernel;storage system;performance evaluation;peer to peer network;cost effective hash based cluster storage system;distributed processing;distributed storage;operating systems computers distributed processing file organisation;hardware file systems costs throughput operating systems file servers parallel processing power generation economics environmental economics informatics;data mining;structured peer to peer network;linux block device;servers;operating system;blast;block level operating system semantics;driver circuits;cost effectiveness;linux block device distributed storage dht structured peer to peer network;experimental evaluation;qa0075 electronic computers computer science;dht off the shelf hardware cost effective hash based cluster storage system enterprise environment maintenance block level operating system semantics experimental evaluation block level hash table based storage system blast;block level hash table based storage system;dht;off the shelf;operating systems computers;enterprise environment maintenance;file systems;hardware;file organisation	During the past few years, large, reliable and efficient storage systems have become increasingly important in enterprise environments. Additional requirements for these environments include low installation, maintenance and administration costs. In this paper we propose a hash-based storage approach, combined with block-level operating system semantics. The experimental evaluation confirms that the proposed approach is viable and can offer a cost-effective storage solution.	blast;computer cluster;computer data storage;desktop computer;distributed hash table;java;operating system;overlay network;requirement	George Parisis;George Xylomenos;Dimitris Gritzalis	2009	2009 Sixth IFIP International Conference on Network and Parallel Computing	10.1109/NPC.2009.27	parallel computing;kernel;real-time computing;cost-effectiveness analysis;converged storage;distributed data store;computer science;operating system;database;distributed computing;information repository;computer security;server;computer network	HPC	-18.365656598908156	51.85902692178745	126793
e288fdcc9b9e8d2bbb2bd72abea043f7dbfb3602	a coded shared atomic memory algorithm for message passing architectures	protocols;shared memory emulation;atomic memory;distributed storage;emulation;distributed storage shared memory emulation erasure coding atomic memory;servers encoding emulation protocols vectors algorithm design and analysis context;servers;vectors;erasure coding;storage management message passing shared memory systems;atomicity coded shared atomic memory algorithm message passing architectures communication cost storage costs atomic linearizable multiwriter multireader shared memory distributed message passing systems atomic shared memory emulation algorithm coded atomic storage erasure coding method storage system cas with garbage collection casgc algorithm write operations read operation;encoding;context;algorithm design and analysis	This paper considers the communication and storage costs of emulating atomic (linearizable) multi-writer multi-reader shared memory in distributed message-passing systems. The paper contains two main contributions: 1) We present an atomic shared-memory emulation algorithm that we call Coded Atomic Storage (CAS). This algorithm uses erasure coding methods. In a storage system with 'N' servers that is resilient to 'f' server failures, we show that the communication cost of CAS is N/(N-2f). The storage cost of CAS is unbounded. 2) We present a variant of CAS known as CAS with Garbage Collection (CASGC). The CASGC algorithm is parametrized by an integer 'd' and has a bounded storage cost. We show that in every execution where the number of write operations that are concurrent with a read operation is no bigger than d, the CASGC algorithm with parameter d satisfies atomicity and liveness. We explicitly characterize the storage cost of CASGC, and show that it has the same communication cost as CAS.	algorithm;message passing	Viveck R. Cadambe;Nancy A. Lynch;Muriel Médard;Peter M. Musial	2014		10.1109/NCA.2014.44	erasure code;distributed shared memory;shared memory;communications protocol;algorithm design;emulation;parallel computing;real-time computing;distributed data store;computer science;operating system;machine learning;distributed computing;server;encoding;memory management	Vision	-16.544247007498626	50.871727195031006	126907
2e9bdeb796e8e2e5609535558fff2456cf9c8952	evaluating the scalability of java event-driven web servers	java platform;nio-based server;java event-driven web servers;commercial server;high-performance web server;nio api;event-driven architecture;experimental server;commercial native-compiled web server;event-driven java server;standard api;internet;java;multi threading;file servers	The two major strategies used to construct high-performance Web servers are thread pools and event-driven architectures. The Java platform is commonly used in Web environments but up to the moment it did not provide any standard API to implement event-driven architectures efficiently. The new 1.4 release of the J2SE introduces the NIO (New I/O) API to help in the development of event-driven I/O intensive applications. We evaluate the scalability that this API provides to the Java platform in the field of Web servers, bringing together the majorly used commercial server (Apache) and one experimental server developed using the NIO API. We study the scalability of the NIO-based server as well as of its rival in a number of different scenarios, including uniprocessor, multiprocessor, bandwidth-bounded and CPU-bounded environments. The study concludes that the NIO API can be successfully used to create event-driven Java servers that can scale as well as the best of the commercial native-compiled Web server, at a fraction of its complexity and using only one or two worker threads.	application programming interface;application server;asynchronous i/o;blocking (computing);business models for open-source software;central processing unit;comparison of application servers;compiler;event-driven architecture;event-driven programming;input/output;java;middleware;multiprocessing;neurotechnology industry organization;non-blocking algorithm;processor affinity;scalability;server (computing);thread pool;uniprocessor system;user interface;web server	Vicenç Beltran;David Carrera;Jordi Torres;Eduard Ayguadé	2004	International Conference on Parallel Processing, 2004. ICPP 2004.	10.1109/ICPP.2004.1327913	web service;embedded system;file server;java api for xml-based rpc;parallel computing;real-time computing;the internet;multithreading;jsr 94;computer science;web api;operating system;real time java;programming language;java;application server;client–server model;server;java applet;java annotation;server farm;non-blocking i/o	HPC	-15.455124858193022	47.567097123409944	126946
30edbdb50e4bbc8d5847b1d010d39d40b6f5929d	assessing fault sensitivity in mpi applications	parallel performance;personal communication networks;software measurement;application software;performance;software systems;application software personal communication networks software standards hardware software systems large scale systems registers payloads software measurement fault detection;gyrokinetic;large scale;registers;theory;fault detection;eulerian;high performance computer;payloads;software standards;register file;fault injection;large scale systems;hardware;gyro;turbulence	Today, clusters built from commodity PCs dominate high-performance computing, with systems containing thousands of processors now being deployed. As node counts for multi-teraflop systems grow to thousands and with proposed petaflop system likely to contain tens of thousands of nodes, the standard assumption that system hardware and software are fully reliable becomes much less credible. Concomitantly, understanding application sensitivity to system failures is critical to establishing confidence in the outputs of large-scale applications. Using software fault injection, we simulated single bit memory errors, register file upsets and MPI message payload corruption and measured the behavioral responses for a suite of MPI applications. These experiments showed that most applications are very sensitive to even single errors. Perhaps most worrisome, the errors were often undetected, yielding erroneous output with no user indicators. Encouragingly, even minimal internal application error checking and program assertions can detect some of the faults we injected.	assertion (software development);central processing unit;experiment;flops;fault injection;register file;software bug;supercomputer	Charng-Da Lu;Daniel A. Reed	2004	Proceedings of the ACM/IEEE SC2004 Conference	10.1109/SC.2004.12	turbulence;payload;application software;parallel computing;real-time computing;performance;computer science;operating system;distributed computing;processor register;programming language;software measurement;register file;theory;fault detection and isolation;computer network;eulerian path;software system	HPC	-17.73618990916107	49.05160034452565	126961
60796534c63d408724bc8545d7e44c5c9b73df72	psnow: a tool to evaluate architectural issues for now environments	architectural design;conflict misses;xor based placement functions;performance evaluation;cache memory;high performance networks;system evaluation;execution driven simulation;network interface	Performance evaluation plays a crucial role in the design of any system. Evaluation tools should clearly identify,isolate andquantify the bottlenecks in the execution to help restructure the application for better performance, as well as suggest enhancements to the existing design. While there has been significant progress recently in novel network interface designs and system software solutions to lower the communication overheads on emerging high performance Network of Workstations environments, performance evaluation tools for these environments have not kept pace with this progress. In this research, we present an execution-driven simulation tool called pSNOW that provides us a unified framework to model different system software and architectural designs, and evaluate these designs using real applications. Using this tool, we model three network interfaces and three communication software substrates, and evaluate their relative merits and demerits.	computer cluster;network interface;performance evaluation;playstation now;simulation;unified framework;workstation	Mangesh Kasbekar;Shailabh Nagar;Anand Sivasubramaniam	1997		10.1145/263580.263606	computer architecture;parallel computing;real-time computing;simulation;cpu cache;computer science;network interface;operating system	HPC	-8.902579775662579	49.62155394170337	127036
cad6b74b99282f2596ba45ffec1f655f4bfb1308	optimized multiple platforms for big data analysis	databases;google;sparks big data file systems databases data warehouses scalability google;sql like query;big data;high performance cache system optimized multiple platforms big data analysis multiple big data processing optimization apache hive cloudera impala bdas spark sql sql queries;distributed memory storage;sparks;sql like query multiple big data processing platform data warehouse distributed memory storage distributed file system;distributed file system;multiple big data processing platform;scalability;data warehouses;sql big data cache storage cloud computing optimisation query processing;data warehouse;file systems	The objective of this study is to optimize a multiple big data processing platform with high performance and high availability. The optimization to the integration of Apache Hive, Cloudera Impala and BDAS Spark SQL enables the platform to support SQL queries in a big data environment, automatically select the best performing big data warehouse platform for computing, and receive the same result far more rapidly from the high-performance cache system. The proposed approach significantly improves overall performance, especially in terms of the application of multiple repeated SQL commands in multi-user mode, thus dramatically reducing the query/response time in such scenarios.	apache hive;big data;draw a secret;high availability;mathematical optimization;multi-user;response time (technology);sql;user space	Bao Rong Chang;Hsiu Fen Tsai;Yo-Ai Wang	2016	2016 IEEE Second International Conference on Multimedia Big Data (BigMM)	10.1109/BigMM.2016.61	computer science;data mining;database;world wide web	DB	-15.200522500105672	54.620000570505844	127046
04f48cf4a9083d16e5f57732690e1b048fda84f8	measuring the energy consumption of massive data insertions: an energy consumption assessment of the pl/sql for loop and forall methods		In today's age of global climate change, the energy consumption of IT is becoming a major issue. The power efficiency of algorithms has already been the subject of many works and Green Design Patterns have been proposed to promote the reduction of the energy consumed by programs. However, with the emergence of Big Data and data intensive applications, database storage management should also be studied with attention. Indeed, reducing the energy consumption of common data management operations could, multiplied by the number of data centers in the world, result in a significant reduction of the global energy consumption of IT. In this work we study and compare the use of two PL/SQL statements (the FOR LOOP and the FORALL constructs) used in the case of massive data insertions from an energy consumption view point. The comparison of these two statements has already been the subject of many works in the field of query optimization in order to minimize the response times of massive data insertions. However, to the best of our knowledge, their energy consumptions need yet to be compared. The main objective of our work is to compare the energy consumption of the massive insertion of data using these two constructs. In this paper we implement two massive data insertion algorithms, one using the FOR LOOP, the other using the FORALL statement and compare both approaches with regards to the global energy consumption of the insertion. Two measurements of the consumption are made: the DBMS's global energy consumption and the energy needed, at the process level, to carry out the insertions. Both a physical measurement scheme is used to determine the global energy consumption and a software library is used to investigate the energy consumption of the process carrying out the insertion.	algorithm;big data;data center;data-intensive computing;database storage structures;emergence;for loop;global variable;library (computing);mathematical optimization;pl/sql;performance per watt;query optimization;sql;software design pattern	Nicolas Gutowski;Olivier Camp;Eric Chauveau	2017	2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)	10.1109/iThings-GreenCom-CPSCom-SmartData.2017.73	real-time computing;sql;energy consumption;database storage structures;big data;pl/sql;query optimization;data management;computer science;server	DB	-15.117222696899212	56.61732967936515	127065
1412cf15e0db44401d9656675dc15f5cdf1bda9f	the evolution of the sperry univac 1100 series: a history, analysis, and projection	executive control software;1100 computer series;programming language;multiprocessing;computer model;executive control;multiprogramming;hardware architecture;computer architecture;large scale;operating system;end user facilities;data management systems;data management system;high speed;programming languages	The 1100 series systems are Sperry Univac's large-scale mainframe computer systems. Beginning with the 1107 in 1962, the 1100 series has progressed through a succession of eight compatible computer models to the latest system, the 1100/80, introduced in 1977. The 1100 series hardware architecture Is based on a 36-bit word, ones complement structure which obtains one operand from storage and one from a high-speed register, or two operands from high-speed registers. The 1100 Operating System is designed to support a symmetrical multiprocessor configuration simultaneously providing multiprogrammed batch, timesharing, and transaction environments.	36-bit;compiler;computer multitasking;computer simulation;forward compatibility;mainframe computer;microsoft forefront;multiprocessing;ones' complement;operand;operating system;real-time clock;real-time computing;succession;time-sharing;univac	Barry R. Borgerson;Merlin L. Hanson;P. A. Hartley	1978	Commun. ACM	10.1145/359327.359334	computer architecture;parallel computing;real-time computing;multiprocessing;computer multitasking;computer science;operating system;hardware architecture;programming language	OS	-12.068219594595869	46.97973526454524	127124
0cc75415bd3aafa448130b199796a2713f9a23d2	exposing speculative thread parallelism in spec2000	feedback driven optimization;thread level speculation;manual parallel programming;thread level parallelism;chip multiprocessor;spec cpu2000;chip multiprocessors;floating point;parallel programs;high performance;parallel applications;multithreading	As increasing the performance of single-threaded processors becomes increasingly difficult, consumer desktop processors are moving toward multi-core designs. One way to enhance the performance of chip multiprocessors that has received considerable attention is the use of thread-level speculation (TLS). As a case study, we manually parallelized several of the SPEC CPU2000 floating point and integer applications using TLS. The use of manual parallelization enabled us to apply techniques and programmer expertise that are beyond the current capabilities of automated parallelizers. With the experience gained from this, we provide insight into ways to aggressively apply TLS to parallelize applications for high performance. This information can help guide future advanced TLS compiler design.For each application, we discuss how and where parallelism was located within the application, the impediments to extracting this parallelism using TLS, and the code transformations that were required to overcome these impediments. We also generalize these experiences to a discussion of common hindrances to TLS parallelization, and describe methods of programming that help expose application parallelism to TLS systems. These guidelines can assist developers of uniprocessor programs to create applications that can easily port to TLS systems and yield good performance. By using manual parallelization on SPEC2000, we provide guidance on where thread-level parallelism exists in these well known benchmarks, what limits its extraction, how to reduce these limitations and what performance can be expected on these applications from a chip multiprocessor system with TLS.	central processing unit;compiler;desktop computer;multi-core processor;multiprocessing;parallel computing;programmer;speculative execution;speculative multithreading;task parallelism;thread (computing);uniprocessor system	Manohar K. Prabhu;Kunle Olukotun	2005		10.1145/1065944.1065964	computer architecture;parallel computing;real-time computing;multithreading;computer science;floating point;operating system;data parallelism;programming language;task parallelism	HPC	-6.477972093650086	46.58972338137531	127222
5f1aa488e73aa52ec8b3d7a2ab78c306b1e6957a	a dynamic load distribution strategy for systems under high task variation and heavy traffic	dynamic load balancing;measurement;performance;pareto distribution;theory;heavy traffic;load sharing;heavy tailed distribution;load distribution;design;task assignment;dynamic loading	Several approaches have been proposed to deal with the issue of load distribution, however they all have similar limitations, such as: (i) tasks are executed in an arbitrary order (which may cause large tasks to be delayed), (ii) the task dispatcher does not take into consideration the server processing capacity (which may cause a large task to be assigned to a server with low processing power) or (iii) they do not consider task deadlines (which if not met, may cause task starvation). This paper proposes an extension of LFF (Least Flow-time First) task assignment policy [9], called LFF-PRIORITY, to deal with these limitations. LFF-PRIORITY dynamically computes two priorities, namely task size and task size priorities, and put them in a priority based multi-section queue. The testing results clearly show that LFF-PRIORITY out performs existing load distribution strategies (that are based on heavy tailed distribution). The testing results also show that more than 80% of tasks meet their task deadline under LFF-PRIORITY.	load balancing (computing);server (computing)	Bin Fu;Zahir Tari	2003		10.1145/952532.952734	design;real-time computing;performance;computer science;pareto distribution;distributed computing;theory;measurement;statistics	HPC	-15.762713304741249	60.31490124875055	127250
efc9af6d346e8765750cc42ef94333650fbcd85b	garbage collection: 50+ years later	memory management;modern programming language;john mccarthy;interesting research direction;garbage collection algorithm;garbage collection;real-world experience;garbage collection issue;overall success	It's been more than 50 years since John McCarthy invented garbage collection. The last 15 years or so have been the most exciting as garbage collection became mainstream and is now considered the norm for all/most modern programming languages. However, despite its overall success, programmers still face challenges when relying on garbage collection for their memory management, especially when their applications push the limits in terms of latency, predictability, and scalability. In this talk I'll describe some of these challenges, share some of my real-world experiences with helping customers solve their garbage collection issues, go over which garbage collection algorithms and techniques are, in my opinion, effective and which are not, and suggest some interesting research directions.	algorithm;garbage collection (computer science);memory management;programmer;programming language;scalability	Tony Printezis	2013		10.1145/2491404.2491405	manual memory management;real-time computing;computer science;garbage in, garbage out;database	OS	-13.932124582216991	48.68110143067574	127272
5d7db1483e4e423d64a093dd6cc6ee5152413704	a scenario-based run-time task mapping algorithm for mpsocs	kpn;heuristic algorithms algorithm design and analysis computer architecture energy consumption program processors clustering algorithms embedded systems;simulation;universiteitsbibliotheek;simulation embedded systems kpn mpsoc task mapping;system on chip embedded systems;embedded systems;system on chip;task mapping;energy saving scenario based run time task mapping algorithm mpsoc based embedded systems static mapping strategy dynamic mapping strategy;mpsoc	The application workloads in modern MPSoC-based embedded systems are becoming increasingly dynamic. Different applications concurrently execute and contend for resources in such systems which could cause serious changes in the intensity and nature of the workload demands over time. To cope with the dynamism of application workloads at run time and improve the efficiency of the underlying system architecture, this paper presents a novel scenario-based run-time task mapping algorithm. This algorithm combines a static mapping strategy based on workload scenarios and a dynamic mapping strategy to achieve an overall improvement of system efficiency. We evaluated our algorithm using a homogeneous MPSoC system with three real applications. From the results, we found that our algorithm achieves an 11.3% performance improvement and a 13.9% energy saving compared to running the applications without using any run-time mapping algorithm. When comparing our algorithm to three other, well-known run-time mapping algorithms, it is superior to these algorithms in terms of quality of the mappings found while also reducing the overheads compared to most of these algorithms.	algorithm;embedded system;mpsoc;run time (program lifecycle phase);systems architecture	Wei Quan;Andy D. Pimentel	2013	2013 50th ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2463209.2488895	system on a chip;embedded system;computer architecture;parallel computing;real-time computing;computer science	EDA	-5.747569363200519	57.61381428218845	127281
19bf188b46eb51cb9ff89fbfe0dade38ab5d0ae3	the split-phase synchronisation technique: reducing the pessimism in the wcet analysis of parallelised hard real-time programs	real time systems multiprocessing systems parallel programming;buffer storage;parallel programming;synchronization multicore processing buffer storage hardware instruction sets real time systems;synchronization;multicore processing;parallelised hrt programs split phase synchronisation technique wcet analysis parallelised hard real time programs embedded multicore processors memory controller;multiprocessing systems;instruction sets;hardware;real time systems	In this paper we present the split-phase synchronisation technique to reduce the pessimism in the WCET analysis of parallelised hard real-time (HRT) programs on embedded multi-core processors. We implemented the split-phase synchronisation technique in the memory controller of the HRT capable MERASA multi-core processor. The split-phase synchronisation technique allows reordering memory requests and splitting of atomic RMW operations, while preserving atomicity, consistency and timing predictability. We determine the improvement of worst-case guarantees, that is the estimated upper bounds, for two parallelised HRT programs. We achieve a WCET improvement of up to 1.26 with the split-phase synchronisation technique, and an overall WCET improvement of up to 2.9 for parallel HRT programs with different software synchronisations.	atomicity (database systems);best, worst and average case;central processing unit;correctness (computer science);data structure;design pattern;embedded system;fast fourier transform;memory controller;multi-core processor;network on a chip;non-blocking algorithm;parallel programming model;programmer;read-modify-write;real-time clock;real-time computing;requirement;worst-case execution time	Mike Gerdes;Florian Kluge;Theo Ungerer;Christine Rochange	2012	2012 IEEE International Conference on Embedded and Real-Time Computing Systems and Applications	10.1109/RTCSA.2012.11	multi-core processor;synchronization;computer architecture;parallel computing;real-time computing;computer science;operating system;instruction set	Embedded	-8.429448179383401	58.255357092976034	127294
1eb95955759ce16d02f44684a51dca2682b7b148	performance of memory configurations for parallel-pipelined computers	reconfiguration;error recovery;capability machine;data stream;computer system;domain;protection;fault tolerant system;operating system;synchronization;error confinement;monitor;rollback	The performance of various memory configurations for parallel-pipelined computer which execute multiple instruction streams on multiple data streams is investigated.  For a parallel-pipelined processor of order (s,p), which consists of p parallel processors each of which is a pipelined processor with s degrees of multiprogramming, there can be up to s p memory requests in each instruction cycle. The memory, which consists of N(&equil;2n) identical memory modules, is organized such that there are l(&equil;2i) lines and m(&equil;2n−i) modules per line, where each module is characterized by the address cycle (address hold time)and memory cycle of a and c time units respectively.  The performance which is affected by the memory interference problem is evaluated as a function of the memory configuration, (l, m), the module characteristics (a, c) and the processor order (s, p). Design considerations are discussed and an example given to illustrate possible design options.	central processing unit;computer multitasking;dimm;instruction cycle;instruction pipelining;interference (communication);parallel computing	Faye A. Briggs	1978		10.1145/800094.803049	memory address;uniform memory access;distributed shared memory;shared memory;embedded system;synchronization;monitor;interleaved memory;fault tolerance;computer architecture;semiconductor memory;parallel computing;real-time computing;distributed memory;rollback;memory refresh;domain;telecommunications;computer science;physical address;control reconfiguration;operating system;memory protection;overlay;conventional memory;extended memory;flat memory model;memory segmentation;registered memory;computing with memory;memory map;memory management	Arch	-11.889408487829902	47.91792948003069	127540
0b2c11c551e47c1727b4ebe935300297db16c4ba	using performance reflection in systems software	perforation;real time;system performance;memory hierarchy	We argue that systems software can exploit hardware instrumentation mechanisms, such as performance monitoring counters in modern processors, along with general system statistics to reactively modify its behavior to achieve better performance. In this paper we outline our approach of using these instrumentation mechanisms to estimate productivity and overhead metrics while running user applications. At the kernel level, we speculate that the scheduler can exploit these metrics to improve system performance. At the application level, we show that applications can use these metrics as well as application-specific productivity metrics to reactively tune their performance. We give several examples of using reflection at the kernel level (e.g., scheduling to improve memory hierarchy performance) and at the application level (e.g., server throttling).	central processing unit;chipset;concurrency (computer science);kernel (operating system);linux;memory hierarchy;operating system;overhead (computing);scheduling (computing);server (computing);thrashing (computer science)	Robert J. Fowler;Alan L. Cox;Sameh Elnikety;Willy Zwaenepoel	2003			embedded system;real-time computing;computer science;operating system;computer performance	OS	-8.234430304339627	48.959548957567726	127578
d2f99b1e624ef4130aaad7a5251530a34810cd09	lazy instruction scheduling: keeping performance, reducing power	lazy instruction scheduling;microarchitecture;clocks;scheduling instruction sets low power electronics power consumption;scheduling algorithm;dead instruction elimination lazy instruction scheduling power dissipation reduction spec cpu 2000 benchmarks dynamic instruction scheduling;registers;scheduling;power dissipation;pipelines;low power electronics;spec cpu 2000 benchmarks;dead instruction elimination;power consumption;dynamic instruction scheduling;instruction scheduling;microarchitecture dead instruction elimination dynamic instruction scheduling;power dissipation scheduling algorithm pipelines computer aided instruction power engineering computing power engineering and energy hardware processor scheduling yarn permission;benchmark testing;instruction sets;power dissipation reduction	An important approach to reduce power dissipation is reducing the number of instructions executed by the processor. To achieve this goal, this paper introduces a novel instruction scheduling algorithm that executes an instruction only when its result is required by another instruction. In this manner, it not only does not execute useless instructions, but also reduces the number of instructions executed after a mispredicted branch. The cost of the extra hardware is 161 bytes for 128 instruction window size. Measurements done using SPEC CPU 2000 benchmarks show that the average number of executed instructions is reduced by 13.5% while the average IPC is not affected.	algorithm;byte;cpu cache;cpu power dissipation;central processing unit;instruction scheduling;instruction window;lazy evaluation;scheduling (computing)	Ali Mahjur;Mahmud Taghizadeh;Amir-Hossein Jahangir	2008	Proceeding of the 13th international symposium on Low power electronics and design (ISLPED '08)	10.1145/1393921.1394020	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;instruction register;cycles per instruction;scheduling;indirect branch;instructions per cycle	Arch	-6.662781174639252	52.79866896945036	127718
4f8196cdf7e494ba44f9668f49eaeb759043f22c	dynamic btb resizing for variable stages superscalar architecture	power aware computing energy conservation parallel processing;low energy processor architecture;energy conservation;pipelines accuracy clocks energy consumption degradation registers computer architecture;superscalar;variable stages pipeline branch target buffer superscalar low energy processor architecture;leakage control technique dynamic btb resizing variable stages superscalar architecture ilp instruction level parallelism tlp thread level parallelism high performance computers variable stage pipeline vsp architecture energy reduction energy consumption deeply pipelined processor branch target buffer resizing technique performance loss;branch target buffer;power aware computing;parallel processing;variable stages pipeline	To extract instruction level parallelism (ILP) and thread level parallelism (TLP), super scalar architecture has become commonly used for high-performance computers. While a deeper super scalar pipeline achieves a higher performance, it consumes a larger energy consumption. For the energy reduction of a deeply-pipelined processor, we have proposed a variable stage pipeline (VSP) architecture which reduces the energy consumption by dynamically unifying the pipeline stages according to behavior in a program. Because the pipeline structure alters after pipeline unification, hardware for extracting ILP and TLP also should be resized to balance the energy-performance trade-off. In this paper, we propose a dynamic branch target buffer (BTB) resizing technique into VSP implemented on a super scalar processor to reduce further energy consumption when the VSP unifies the pipeline stages. The proposed technique resizes the size of the BTB along with pipeline scaling. Our evaluation results show that using the proposed technique can reduce the BTB size to one-eight after pipeline unification with only 0.02% prediction accuracy degradation on the average compared with the baseline BTB. This results in 9.2% dynamic energy reduction of the processor core with a trivial performance loss. Furthermore, our technique reduces the leakage energy consumption in the BTB by 87.5% with a practical leakage control technique.	baseline (configuration management);branch target predictor;computer;elegant degradation;graphics pipeline;image scaling;instruction pipelining;instruction-level parallelism;multi-core processor;parallel computing;performance evaluation;pipeline (computing);re-order buffer;register file;scalar processor;seam carving;spectral leakage;supercomputer;superscalar processor;task parallelism;unification (computer science);virtual storage platform	Tomoyuki Nakabayashi;Takahiro Sasaki;Toshio Kondo	2013	2013 First International Symposium on Computing and Networking	10.1109/CANDAR.2013.63	pipeline burst cache;computer architecture;parallel computing;real-time computing;computer science;pipeline	Arch	-5.1974483228139	54.816324269687755	127724
1a0a5dd1565085701c0a1e707296127f39abeb1a	memory scalability evaluation of the next-generation intel bensley platform with infiniband	performance evaluation;infiniband host channel adapters memory scalability evaluation next generation intel bensley platform infiniband memory performance degradation fully buffered dimm next generation multicore intel platform multirail infiniband ddr configurations current generation intel lindenhurst platform inter node throughput bi directional bandwidth;memory performance;reference point;memory access;memory architecture;next generation;scalability bandwidth throughput computer networks next generation networking power engineering computing aggregates bidirectional control hardware sun;microprocessor chips memory architecture;microprocessor chips	As multi-core systems gain popularity for their increased computing power at low-cost, the rest of the architecture must be kept in balance, such as the memory subsystem. Many existing memory subsystems can suffer from scalability issues and show memory performance degradation with more than one process running. To address these scalability issues, fully-buffered DIMMs have recently been introduced. In this paper we present an initial performance evaluation of the next-generation multi-core Intel platform by evaluating the FB-DIMM-based memory subsystem and the associated InfiniBand performance. To the best of our knowledge this is the first such study of Intel multi-core platforms with multi-rail InfiniBand DDR configurations. We provide an evaluation of the current-generation Intel Lindenhurst platform as a reference point. We find that the Intel Bensley platform can provide memory scalability to support memory accesses by multiple processes on the same machine as well as drastically improved inter-node throughput over InfiniBand. On the Bensley platform we observe a 1.85 times increase in aggregate write bandwidth over the Lindenhurst platform. For inter-node MPI-level benchmarks we show bi-directional bandwidth of over 4.55 GB/sec for the Bensley platform using 2 DDR InfiniBand host channel adapters (HCAs), an improvement of 77% over the current generation Lindenhurst platform. The Bensley system is also able to achieve a throughput of 3.12 million MPI messages/sec in the above configuration	aggregate data;dimm;elegant degradation;infiniband;multi-core processor;performance evaluation;scalability;throughput	Matthew J. Koop;Wei Huang;Abhinav Vishnu;Dhabaleswar K. Panda	2006	14th IEEE Symposium on High-Performance Interconnects (HOTI'06)	10.1109/HOTI.2006.19	embedded system;parallel computing;computer hardware;computer science;operating system;conventional memory;multi-channel memory architecture	HPC	-10.23158941181432	46.97617629142043	127828
bdc97d4982782a2b636b367708f0ce9a17a5f28b	pgx.d: a fast distributed graph processing engine	kernel;graph data processing;distributed computing;computational modeling;clustering algorithms;bandwidth;programming;algorithm design and analysis;data models	Graph analysis is a powerful method in data analysis. Although several frameworks have been proposed for processing large graph instances in distributed environments, their performance is much lower than using efficient single-machine implementations provided with enough memory. In this paper, we present a fast distributed graph processing system, namely PGX.D. We show that PGX.D outperforms other distributed graph systems like GraphLab significantly (3x -- 90x). Furthermore, PGX.D on 4 to 16 machines is also faster than an implementation optimized for single-machine execution. Using a fast cooperative context-switching mechanism, we implement PGX.D as a low-overhead, bandwidth-efficient communication framework that supports remote data-pulling patterns. Moreover, PGX.D achieves large traffic reduction and good workload balance by applying selective ghost nodes, edge partitioning, and edge chunking transparently to the user. Our analysis confirms that each of these features is indeed crucial for overall performance of certain kinds of graph algorithms. Finally, we advocate the use of balanced beefy clusters where the sustained random DRAM-access bandwidth in aggregate is matched with the bandwidth of the underlying interconnection fabric.	aggregate data;algorithm;dynamic random-access memory;graph (abstract data type);graph theory;interconnection;overhead (computing);shallow parsing	Sungpack Hong;Siegfried Depner;Thomas Manhardt;Jan Van Der Lugt;Merijn Verstraaten;Hassan Chafi	2015	SC15: International Conference for High Performance Computing, Networking, Storage and Analysis	10.1145/2807591.2807620	data modeling;algorithm design;programming;parallel computing;kernel;power graph analysis;computer science;graph partition;theoretical computer science;operating system;machine learning;distributed computing;cluster analysis;computational model;bandwidth	HPC	-16.77124670597499	54.59088324279849	127837
7d242dfa559c24caed396e421f62199bf5c70530	processor frequency assignment in three-dimensional mpsocs under thermal constraints by polynomial programming	silicon;system reliability;power density;frequency polynomials temperature integrated circuit modeling reliability cooling costs power system modeling dynamic voltage scaling constraint optimization;conference_paper;processor scheduling;frequency assignment;system performance;three dimensional;maximum temperature constraint processor frequency assignment three dimensional mpsoc thermal constraints polynomial programming multi processor systems on chips system reliability mpsoc thermal optimization tools two dimensional planar ic thermal modeling dynamic voltage frequency scaling;3d thermal model;system on chip mathematical programming multiprocessing systems processor scheduling;performance improvement;mathematical programming;system on chip;three dimensional displays;multi processor system on chip;polynomial programming;thermal model;thermal conductivity;optimization;multiprocessing systems;active layer;magnetic cores;power demand;time frequency analysis;frequency assignment multi processor systems on chips 3d thermal model polynomial programming;multi processor systems on chips;dynamic voltage and frequency scaling	The operating frequency and the number of cores and active layers in multi-processor systems-on-chips (MPSoC) continue to increase, resulting in rising power density and operating temperature on the die. The increasing temperature reduces the system reliability and performance and increases the cooling cost. Most traditional MPSoC thermal optimization tools are based on two-dimensional planar IC thermal modeling, which is insufficient to capture the different thermal characteristics of three-dimensional (3D) MPSoCs and the behavior subjected to dynamic voltage and frequency scaling (DVFS). The recently proposed 3D optimization approach still ignores the mutual thermal impact between cores in the same layer, which reduces the precision of frequency assignment and may cause violation in the maximum temperature constraint. In this paper, we propose an approach based on polynomial programming that addresses the problem of processor frequency assignment in 3D MPSoC system, such that the total system performance can be maximized as well as the temperature and power constraints are met for all time instances. We also compare the performance improvement of DVFS in 3D MPSoC with that in 2D MPSoC, studying the importance of intra-layer temperature correlation in 3D MPSoC.	clock rate;computer cooling;dynamic frequency scaling;dynamic voltage scaling;image scaling;mpsoc;mathematical optimization;multi-core processor;multiprocessing;polynomial;system on a chip	Guangyao Zhao;Hing-Kit Kwan;Chi-Un Lei;Ngai Wong	2008	APCCAS 2008 - 2008 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2008.4746358	system on a chip;embedded system;three-dimensional space;electronic engineering;real-time computing;time–frequency analysis;computer science;engineering;operating system;power density;computer performance;silicon;thermal conductivity	EDA	-4.845074815824227	57.81611209927245	127883
93303233c425e5e0c19eee4e431d69264c55976f	evaluating memory compression and deduplication	memory data;storage management;deduplication memory data compression;deduplication;system performance memory deduplication ram random access memory memory traces memory block compression fixed size partition fsp content defined chunking cdc sliding block sb memory page memory resources;random access storage;storage management random access storage;compression;random access memory memory management dictionaries encoding computers compression algorithms data compression	Many programs require more RAM to hold their data than a typical computer has. Theoretically, both the compression and deduplication can trade the rich computing capacity for more available RAM space. This paper comprehensively evaluates the performance behaviour of memory compression and memory deduplication by using seven real memory traces. The experimental results give two implications: (1) Memory deduplication greatly outperforms memory block compression. (2) Fixed-size partition (FSP) achieves the best performance in contrast to Content-defined Chunking (CDC) and Sliding Block (SB). The optimal chunking size of FSP is equal to the size of a memory page. The analysis results in this paper should be able to provide useful insights for designing or implementing systems that require abundant memory resources to enhance the system performance.	computer;customer relationship management;data compression;data deduplication;page (computer memory);paging;random-access memory;sandy bridge;shallow parsing;tracing (software);von neumann architecture	Yuhui Deng;Liangshan Song;Xinyu Huang	2013	2013 IEEE Eighth International Conference on Networking, Architecture and Storage	10.1109/NAS.2013.45	uniform memory access;shared memory;interleaved memory;semiconductor memory;parallel computing;data deduplication;computer hardware;computer science;theoretical computer science;operating system;computer data storage;computer memory;overlay;conventional memory;extended memory;flat memory model;registered memory;sequential access memory;compression;cache-only memory architecture;memory map;memory management	HPC	-8.976049406540211	53.46260501218933	127977
72ab1ad461d2f2af267e4eecfbddda70d3e7a97e	exporting kernel page caching for efficient user-level i/o	libraries;cache storage;page faulting;kernel;storage system;shared memory;memory management;incache system workload kernel page caching user level i o memory management kernel development page faulting zero copy caching write ordering synchronous flush kernel page write back thread secure shared memory user level object store storage stack user level storage system kernel file system design out of cache random lookup;charge carrier processes;storage stack;real time;kernel page write back thread;write ordering;user level i o;synchronous flush;incache system workload;out of cache random lookup;indexes;user level object store;zero copy caching;kernel page caching;synchronization;file system;kernel file system design;linux;user level storage system;kernel development;secure shared memory;kernel file systems databases memory management indexing libraries protection computer science computer architecture tagging	The modern file system is still implemented in the kernel, and is statically linked with other kernel components. This architecture has brought performance and efficient integration with memory management. However kernel development is slow and modern storage systems must support an array of features, including distribution across a network, tagging, searching, deduplication, checksumming, snap-shotting, file pre-allocation, real time I/O guarantees for media, and more. To move complex components into user-level however will require an efficient mechanism for handling page faulting and zero-copy caching, write ordering, synchronous flushes, interaction with the kernel page write-back thread, and secure shared memory. We implement such a system, and experiment with a user-level object store built on top. Our object store is a complete re-design of the traditional storage stack and demonstrates the efficiency of our technique, and the flexibility it grants to user-level storage systems. Our current prototype file system incurs between a 1% and 6% overhead on the default native file system Ext3 for in-cache system workloads. Where the native kernel file system design has traditionally found its primary motivation. For update and insert intensive metadata workloads that are out-of-cache, we perform 39 times better than the native Ext3 file system, while still performing only 2 times worse on out-of-cache random lookups.	benchmark (computing);cpu cache;cache (computing);cache-oblivious algorithm;clustered file system;dce distributed file system;data deduplication;emoticon;fits;google file system;high-throughput computing;input/output;kernel (operating system);linear separability;memory management;operating system;overhead (computing);posix;page cache;page fault;prototype;random-access memory;scalability;shared memory;static build;static library;systems design;throughput;user space;zero-copy	Richard P. Spillane;Sagar Dixit;Shrikar Archak;Saumitra Bhanage;Erez Zadok	2010	2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)	10.1109/MSST.2010.5496973	sysfs;fork;self-certifying file system;parallel computing;real-time computing;memory-mapped file;device file;computer file;computer science;stub file;versioning file system;operating system;unix file types;journaling file system;epoll;open;file control block;configfs;virtual file system	OS	-13.352342534874817	52.18465503753998	128062
affca1f0bdf706bc053823db5f31be399da4d433	energy efficient storage management cooperated with data intensive applications in virtual machines	dbms power consumption storage virtualized environmen;standards;performance evaluation;storage management;virtual machining;layout;hdd access interval energy efficient storage management data intensive application virtual machines data center storage device power consumption virtualized environment cloud computing environment power effective storage management storage placing method asymmetric placing;virtualized environmen;dbms;power consumption;virtualisation cloud computing energy conservation energy consumption storage management virtual machines;virtual machining power demand time frequency analysis storage management performance evaluation layout standards;power demand;storage;time frequency analysis	In data centers, huge amount of computes are running. These computes consume enormous energy. For this issue, an energy efficient storage management method cooperated with data intensive applications was proposed. With this method, data and storage devices are managed with application supports and power consumption of storage devices is significantly decreased. However, the work does not take account of virtualized environment. Recently, many data intensive applications run on virtualized environment, such as cloud computing environment. Thus, we think discussion on power effective storage management in virtualized environment is also important in addition to this work. In this paper, we focus on virtualized environment in which plural virtual machines run on a physical computer and a data intensive application runs on each virtual machine. We apply this storage management method for this environment, and evaluate performance and power consumption. Then, we proposed two storage placing methods, which are symmetric placing and asymmetric placing, for virtualized environment. Our experiments demonstrated that the proposed methods could create long HDD access intervals enough to save power consumption of storage devices. In addition, our results showed that asymmetric method could better performance than symmetric method.	cloud computing;data center;data-intensive computing;experiment;hard disk drive;virtual machine	Shunsuke Yagai;Saneyasu Yamaguchi	2014	2014 IEEE 33rd International Symposium on Reliable Distributed Systems Workshops	10.1109/SRDSW.2014.24	layout;real-time computing;time–frequency analysis;converged storage;computer hardware;computer science;operating system;database;information repository	HPC	-15.034579383603992	56.047577637087706	128203
c0f27461d13599466370cd74f21f76964e530cda	hard: host-level address remapping driver for solid-state disk		Recent SSDs use parallel architectures with multi-channel and multiway, and manages multiple pages in a group, called superpage, to reduce the size of address mapping information. Such a coavrse grained mapping provides a poor performance for small sized random write requests. To solve the problem, this paper proposes a novel host-level device driver, called HARD, which merges several small sized random write requests into a long sequential write requests. Experimental results showed the proposed HARD improved the random write performance by up to eight times.	device driver;hard disk drive;random access;sequential access;solid-state drive	Young-Joon Jang;Dongkun Shin	2011		10.1007/978-94-007-2911-7_14	solid-state;computer hardware;computer science	Arch	-11.710257328277983	52.63169624153679	128302
450bcc8e106176458b08d693ec81800fe149c5ce	low cost load balancing algorithms	perforation;resource allocation;distributed processing;costs load management workstations australia council delay system performance distributed algorithms decision making;virtual machines resource allocation operating systems computers distributed processing;virtual machines;network traffic;decision process;load balance;user behavior;distributed system low cost load balancing algorithms network traffic design making process user behavior bursty usage feature simulation job stream performance less overhead;operating systems computers	Two load-balancing algorithms that significantly reduce network traffic and the latency of the design-making process are presented in this paper. These algorithms take user behavior into account. Based on the intermittent behavior of a user using his/her computer, these algorithms adopt different measures to accelerate decision process and to reduce unnecessary communication. The measures used in these algorithms are beneficial to all type machines, especially to those machines that have bursty usage feature. The simulation shows that even without the intermittent feature on the job stream of each participant machine, these algorithms achieve better performance than other algorithms and have less overhead. >	algorithm;load balancing (computing)	Weiping Zhu;A. Goscinski	1994		10.1109/SIMSYM.1994.283093	real-time computing;simulation;computer science;distributed computing	Theory	-15.266464344789078	59.80106077773195	128331
00b2d117d78c1caca6baa7c4baa2c8925796e721	efficient nonblocking software transactional memory	perforation;software transactional memory;nonblocking;continuous improvement	Foundational transactional memory research grew out of research into nonblocking concurrent data structures, which aim to overcome the many well-known software engineering, performance, and robustness problems associated with lock-based implementations. Recently, many researchers have developed blocking STMs, recognising that they are much easier to design and that the software engineering benefits of STM can be delivered even by a blocking STM. But hiding blocking from the application programmer does not eliminate all of its disadvantages, and in some cases blocking is unacceptable, for example if STM is to be used to coordinate between an interrupt handler and the interrupted thread.  Recently, a common belief has emerged that blocking STMs are fundamentally faster than nonblocking ones largely based on Ennals's argument [2] that nonblocking STMs cannot store datain-place as most blocking STMs do. However, this argument is based only on intuition, not a formal proof. It misses the possibility of nonblocking STM designs that closely mimic blocking STMs in the common case, resorting to techniques such as displacing transactional data only when needed to avoid waiting for a thread that is delayed while modifying it.  We present a novel nonblocking word-based STM based on this approach. Our STM eliminates several significant sources of common-case overhead in the previous best nonblocking word based STM, and also performs comparably with the simple blocking STM on which it is based.	blocking (computing);data structure;dynamic data;formal proof;interrupt handler;non-blocking algorithm;overhead (computing);programmer;software engineering;software transactional memory	Virendra J. Marathe;Mark Moir	2007		10.1145/1229428.1229454	parallel computing;real-time computing;computer science;operating system;software transactional memory;distributed computing;programming language	PL	-14.562334247191322	48.91508188168538	128409
7c63a5c4972c57bbd38d29c9420be671d3243a2f	correlation and aliasing in dynamic branch predictors	correlation;pipelines;history;hardware;branch prediction	Previous branch prediction studies have relied primarily upon the SPECint89 and SPECint92 benchmarks for evaluation. Most of these benchmarks exercise a very small amount of code. As a consequence, the resources required by these schemes for accurate predictions of larger programs has not been clear. Moreover, many of these studies have simulated a very limited number of configurations. Here we report on simulations of a variety of branch prediction schemes using a set of relatively large benchmark programs that we believe to be more representative of likely system workloads. We have examined the sensitivity of these prediction schemes to variation in workload, in resources, and in design and configuration. We show that for predictors with small available resources, aliasing between distinct branches can have the dominant influence on prediction accuracy. As a result, the simple scheme of selecting a predictor using the branch address can be more effective than more elaborate correlating branch predictors. With larger resources, the designer may have more latitudes, but we show that past studies have sometimes overlooked the importance of correct allocation of these resources.	aliasing;benchmark (computing);branch predictor;kerrison predictor;simulation	Stuart Sechrest;Chih-Chieh Lee;Trevor N. Mudge	1996		10.1109/ISCA.1996.10003	parallel computing;real-time computing;simulation;computer science;theoretical computer science;operating system;pipeline transport;correlation;branch predictor	Arch	-7.1392201250774585	51.24273972607145	128499
adc340696af57d9f6093e390ddeb15daa60d1dd1	ipacs: power-aware covering sets for energy proportionality and performance in data parallel computing clusters	energy proportionality;data parallel computing;covering subset;mapreduce	Energy consumption in datacenters has recently become a major concern due to the rising operational costs and scalability issues. Recent solutions to this problem propose the principle of energy proportionality, i.e., the amount of energy consumed by the server nodes must be proportional to the amount of work performed. For data parallelism and fault tolerance purposes, most common file systems used inMapReduce-type clustersmaintain a set of replicas for each data block. A covering subset is a group of nodes that together contain at least one replica of the data blocks needed for performing computing tasks. In this work, we develop and analyze algorithms to maintain energy proportionality by discovering a covering subset that minimizes energy consumption while placing the remaining nodes in low-power standby mode in a data parallel computing cluster. Our algorithms can also discover covering subset in heterogeneous computing environments. In order to allow more data parallelism, we generalize our algorithms so that it can discover k-covering subset, i.e., a set of nodes that contain at least k replicas of the data blocks. Our experimental results show that we can achieve substantial energy saving without significant performance loss in diverse cluster configurations and working environments. © 2013 Elsevier Inc. All rights reserved.	algorithm;computer cluster;data parallelism;experiment;fault tolerance;heterogeneous computing;low-power broadcasting;mapreduce;parallel computing;power management;queueing theory;requirement;scalability;scheduling (computing);server (computing);sleep mode	Jinoh Kim;Jerry Chi-Yuan Chou;Doron Rotem	2014	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2013.09.006	parallel computing;real-time computing;computer science;operating system;distributed computing	HPC	-16.01816400166579	57.045693812421874	128556
462375120c0cb1535d0722dc1b1821708efd17af	predicting the performance of wide area data transfers	grid computing distributed computing computer science instruments mathematics laboratories joining processes testing physics computer architecture;processing;general and miscellaneous mathematics computing and information science;mathematics;instruments;predictive framework;performance evaluation;data collection;distributed processing;performance;data grids;distributed computing;large data sets;testing;information services;physics;physical characteristic;computer architecture;data delivery infrastructure data grids distributed data stores large data sets predictive framework data transfer prediction replica selection information services;replicated databases distributed databases distributed processing electronic data interchange performance evaluation;data transfer prediction;distributed databases;joining processes;computer science;information service;data delivery infrastructure;grid computing;replicated databases;parallel processing;electronic data interchange;data transfer;data grid;distributed data stores;replica selection	As Data Grids become more commonplace, large data sets are being replicated and distributed to multiple sites, leading to the problem of determining which replica can be accessed most efficiently. The answer to this question can depend on many factors, including physical characteristics of the resources and the load behavior on the CPUs, networks, and storage devices that are part of the end-to-end path linking possible sources and sinks. We develop a predictive framework that combines (1) integrated instrumentation that collects information about the end-to-end performance of past transfers, (2) predictors to estimate future transfer times, and (3) a data delivery infrastructure that provides users with access to both the raw data and our predictions. We evaluate the performance of our predictors by applying them to log data collected from a wide area testbed. These preliminary results provide insights into the effectiveness of using predictors in this situation.	central processing unit;end-to-end principle;extrapolation;file transfer;gridftp;replication (computing);selection algorithm;testbed;usability	Sudharshan S. Vazhkudai;Jennifer M. Schopf;Ian T. Foster	2002		10.1109/IPDPS.2002.1015510	parallel processing;parallel computing;performance;computer science;processing;theoretical computer science;operating system;electronic data interchange;data grid;data mining;database;distributed computing;software testing;information system;grid computing;data collection	HPC	-18.79172689623209	57.88514058155136	128689
3eab48dec1bc4f343134240bd5d5643d695e4a9e	correlating hardware performance events to cpu and dram power consumption	random access memory;stall cycle ratio hardware performance events dram power consumption cpu power consumption running average power limit linux perf_events subsystem performance metrics rapl registers dram memory consumption;energy measurement;random access memory power demand power measurement benchmark testing energy measurement time frequency analysis;power demand;time frequency analysis;benchmark testing;power measurement;power aware computing dram chips energy consumption linux microprocessor chips performance evaluation		central processing unit;dynamic random-access memory	Michael Giardino;Bonnie H. Ferri	2016		10.1109/NAS.2016.7549395	embedded system;benchmark;parallel computing;real-time computing;time–frequency analysis;computer science;operating system	HPC	-5.200429650886514	54.90670996932589	128746
77f826132cf09ac91ea9c859387a8d52221a019a	memory power management via dynamic voltage/frequency scaling	dynamic voltage frequency scaling;energy efficiency;control algorithm;energy efficient;power management;memory systems;energy cost;power reduction;dvfs;dram;memory bandwidth;memory;analytical model	Energy efficiency and energy-proportional computing have become a central focus in enterprise server architecture. As thermal and electrical constraints limit system power, and datacenter operators become more conscious of energy costs, energy efficiency becomes important across the whole system. There are many proposals to scale energy at the datacenter and server level. However, one significant component of server power, the memory system, remains largely unaddressed. We propose memory dynamic volt age/frequency scaling (DVFS) to address this problem, and evaluate a simple algorithm in a real system.  As we show, in a typical server platform, memory consumes 19% of system power on average while running SPEC CPU2006 workloads. While increasing core counts demand more bandwidth and drive the memory frequency upward, many workloads require much less than peak bandwidth. These workloads suffer minimal performance impact when memory frequency is reduced. When frequency reduces, voltage can be reduced as well. We demonstrate a large opportunity for memory power reduction with a simple control algorithm that adjusts memory voltage and frequency based on memory bandwidth utilization.  We evaluate memory DVFS in a real system, emulating reduced memory frequency by altering timing registers and using an analytical model to compute power reduction. With an average of 0.17% slowdown, we show 10.4% average (20.5% max) memory power reduction, yielding 2.4% average (5.2% max) whole-system energy improvement.	algorithm;data center;dynamic voltage scaling;emulator;frequency scaling;mac os x 10.4 tiger;memory bandwidth;power management;reduction (complexity);server (computing);spec#	Howard David;Chris Fallin;Eugene Gorbatov;Ulf R. Hanebutte;Onur Mutlu	2011		10.1145/1998582.1998590	interleaved memory;parallel computing;real-time computing;computer hardware;computer science;operating system;efficient energy use;memory bandwidth;computing with memory	Arch	-5.615903313392416	55.561671794754204	128752
1d64c04a6e9c98adc397672ab8d53c633972c987	data-flow prescheduling for large instruction windows in out-of-order processors	silicon;out of order processors;clocks;processor scheduling;logic;instruction window;complexity;data flow prescheduling prescheduling instruction windows out of order processors instruction window complexity time critical operation;out of order clocks delay logic proposals silicon process design registers accuracy processor scheduling;out of order;process design;accuracy;data flow prescheduling;registers;processor scheduling data flow computing;data flow computing;prescheduling;data flow;time critical operation;proposals;instruction windows	The performance of out-of-order processors increases with the instruction window size. In conventional processors, the effective instruction window cannot be larger than the issue buffer. Determining which instructions from the issue buffer can be launched to the execution units is a timecritical operation which complexity increases with the issue buffer size. We propose to relieve the issue stage by reordering instructions before they enter the issue buffer. This study introduces the general principle of data-flow prescheduling. Then we describe a possible implementation. Our preliminary results show that data-flow prescheduling makes it possible to enlarge the effective instruction window while keeping the issue buffer small.	cpu cache;cache (computing);central processing unit;centralized computing;dataflow;execution unit;hit-or-miss transform;instruction window;memory hierarchy;microsoft windows;parallel computing;simd;smoothing	Pierre Michaud;André Seznec	2001		10.1109/HPCA.2001.903249	process design;parallel computing;complexity;real-time computing;computer science;theoretical computer science;operating system;silicon;logic	Arch	-7.157995636863468	52.94361563009393	128832
77a4f67d156c400df1bcae344f1e64e9c1ca7613	quickrecall: a hw/sw approach for computing across power cycles in transiently powered computers	energy harvesting;embedded systems;nonvolatile memory;transiently powered computers;low power design;ferroelectric ram	Transiently Powered Computers (TPCs) are a new class of batteryless embedded systems that depend solely on energy harvested from external sources for performing computations. Enabling long-running computations on TPCs is a major challenge due to the highly intermittent nature of the power supply (often bursts of < 100ms), resulting in frequent system reboots. Prior work seeks to address this issue by frequently checkpointing system state in flash memory, preserving it across power cycles. However, this involves a substantial overhead due to the high erase/write times of flash memory. This article proposes the use of Ferroelectric RAM (FRAM), an emerging nonvolatile memory technology that combines the benefits of SRAM and flash, to seamlessly enable long-running computations in TPCs. We propose a lightweight, in-situ checkpointing technique for TPCs using FRAM that consumes only 30nJ while decreasing the time taken for saving and restoring a checkpoint to only 21.06μs, which is over two orders of magnitude lower than the corresponding overhead using flash. We have implemented and evaluated our technique, QuickRecall, using the TI MSP430FR5739 FRAM-enabled microcontroller. Experimental results show that our highly-efficient checkpointing translate to significant speedup (1.25x - 8.4x) in program execution time and reduction (∼3x) in application-level energy consumption.	adobe flash;application checkpointing;computation;embedded system;ferroelectric ram;flash memory;microcontroller;nonvolatile bios memory;overhead (computing);power cycling;power supply;reboot (computing);run time (program lifecycle phase);shattered world;speedup;static random-access memory;transaction processing system	Hrishikesh Jayakumar;Arnab Raha;Woo Suk Lee;Vijay Raghunathan	2015	JETC	10.1145/2700249	ferroelectric ram;embedded system;parallel computing;real-time computing;non-volatile memory;computer hardware;telecommunications;computer science;electrical engineering;operating system;energy harvesting	OS	-6.319101045460662	55.62329584367825	128919
ad3fbf3114398af74bdf69585b8c6f25affa68ee	a new parallelization model for detecting temporal bursts in large-scale document streams on a multi-core cpu	multi core cpu burst detection parallel processing document stream;i o latency parallelization model temporal burst detection large scale document streams online document topic detection kleinberg s temporal burst detection algorithm event related keyword topic related keyword social media burst detection processing big data hybrid parallelization model hidden i o thread parallel processing multicore cpu environment;social networking online big data information retrieval multiprocessing systems parallel processing;detection algorithms instruction sets parallel processing media message systems data models viterbi algorithm	Burstiness is the simplest but the most robust criterion for detecting topics and events in online documents. Online documents are referred to as document streams because they have a temporal order. Kleinberg's temporal burst detection algorithm is the most successful algorithm for detecting bursty periods related to a topic- or event-related keyword. Kleinberg's temporal burst detection algorithm aims to find certain time periods in which a keyword occurs at a high frequency. In recent times, large-scale online documents are increasingly common on social media. Therefore, speed-up of burst-detection processing is one of the most important issues in this era of big data. In this paper, we propose a novel parallelization model, called the hybrid parallelization model with a hidden I/O thread, to enable the parallel processing of Kleinberg's temporal burst detection algorithm on a multi-core CPU. In a multi-core CPU environment, I/O latency is a critical issue for improving the performance of a parallelization model. To automatically hide the I/O latency, the proposed parallelization model utilizes speculative I/Os. The results of experiments using actual large-scale document streams show that the proposed parallelization model performs well compared with a conventional parallelization model.	algorithm;big data;central processing unit;experiment;input/output;multi-core processor;parallel computing;sensor;social media;speculative execution	Keiichi Tamura;Hajime Kitakami	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6973960	parallel computing;real-time computing;computer science;theoretical computer science	DB	-16.69512179284782	56.5748849860704	128945
65c7be4eb60a369c3364d9ffe4f2c479a4b6058c	scheduling imprecise computations with wind-up parts	earliest deadline first;computer model	The imprecise computation model provides the ability to cope with unpredictable workloads. However, there is no consistent way on how to terminate the computation in its early stage. This paper describes a novel approach for safely terminating imprecise computations. First, a new logical part called wind-up part is added to the imprecise computation model. This wind-up part is used by application programmers to explicitly specify any operations required to be performed before its optional part is terminated. We have also developed an algorithm based on the mandatoryfirst earliest deadline first strategy to schedule computations based on the proposed model.	algorithm;divergence (computer science);earliest deadline first scheduling;model of computation;programmer;real-time clock;real-time computing;scheduling (computing);terminate (software);transmitter	Hidenori Kobayashi;Nobuyuki Yamasaki;Yuichiro Anzai	2003			distributed computing;computer science;parallel computing;real-time computing;computation;earliest deadline first scheduling;scheduling (computing);dynamic priority scheduling;normalization property	Embedded	-9.419607664688831	60.01492037284352	129173
9ed0954a29e2285f99e3ce154a6e22d0209d8fca	hardware trends: challenges and opportunities in distributed computing	reconfiguration;geometric graph;edge flip;colored point set;compatible graphs	"""This article is about three trends in computer hardware, and some of the challenges and opportunities that I think they provide for the distributed computing community. A common theme in all of these trends is that hardware is moving away from assumptions that have often been made about the relative performance of different operations (e.g., computation versus network communication), the reliability of operations (e.g., that memory accesses are reliable, but network communication is not), and even some of the basic properties of the system (e.g., that the contents of main memory are lost on power failure).  Section 1 introduces """"rack-scale"""" systems and the kinds of properties likely in their interconnect networks. Section 2 describes challenges in systems with shared physical memory but without hardware cache coherence. Section 3 discusses non-volatile byte-addressable memory. The article is based in part on my talk at the ACM PODC 2014 event in celebration of Maurice Herlihy's sixtieth birthday."""	byte addressing;cache (computing);cache coherence;computation;computer data storage;computer hardware;distributed computing;maurice herlihy;non-volatile memory;podc	Timothy L. Harris	2015	SIGACT News	10.1145/2789149.2789165	distributed shared memory;parallel computing;computer science;control reconfiguration;theoretical computer science;mathematics;distributed computing;programming language;algorithm	Arch	-8.21751339471258	53.80962210855952	129186
2b9c4d1d343f85851d87cbe1b6f982809afbc6eb	the cache injectionkofetch architecture: initial performance evaluation	64 bit cache injection cofetch architecture performance evaluation distributed shared memory read misses system latencies microprocessors 64 bit processors data prefetching cache injection cica;cache storage;distributed memory systems;shared memory;performance evaluation;samarium microprocessors prefetching costs delay modems cache memory stress estimation theory program processors;shared memory systems;data prefetching;performance evaluation distributed memory systems shared memory systems cache storage;distributed shared memory;off the shelf	One of t?ie major problems in. a n,umber of SM (Shared Memory) and DSM (Distributed Shared Memory) applications is the overall cost of read misses in conditions when.: (a) system latencies are relatively large, and (b) a shared data item is read relatively few times b.y each of the processors in the system; modern SM and DSM systerns are typically based on. off-the-shelf microprocessors which do not include an.y support for the described problem. Con.sequently, the major goal of our research is to come up with a new con.cept to be incorporated into the n.ext generation microprocessors, so they can become more eficient in the sense described above. Existing 64bit processors support only data prefetching (PF) as a method to fight against negative effects of the described problem. Our research introduces a mew concept referred to as cache injection (Cl), as well as the related cache injectionkofetch architecture (CICA). Initial performance evaluation is pegormed using a simulation methodology based on the set of synthetic benchmarks of interest for the research sponsor.	64-bit computing;cpu cache;central processing unit;data item;distributed shared memory;emoticon;microprocessor;performance evaluation;simulation;synthetic intelligence	Veljko M. Milutinovic;Aleksandar Milenkovic;Gad Sheaffer	1997		10.1109/MASCOT.1997.567582	bus sniffing;uniform memory access;distributed shared memory;shared memory;cache coherence;computer architecture;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;distributed memory;cpu cache;cache;computer science;write-once;operating system;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;memory map;non-uniform memory access;memory management	Arch	-11.662633637268772	50.55984275982422	129209
5c6283863462779e4f2c593c72c4cd2b738d9720	experiences with virtuoso cluster rdf column store		Virtuoso Column Store [185] introduces vectorized execution into the Virtuoso DBMS. Additionally, its scale-out version, that allows running the system on a cluster, has been significantly redesigned. This article discusses advances in scale-out support in Virtuoso and analyzes this on the Berlin SPARQL Benchmark (BSBM) [101]. To demonstrate the features of Virtuoso Cluster RDF Column Store, we first present micro-benchmarks on a small 2node cluster with 10 billion triples. In the full evaluation we show one can now scale-out to a BSBM database of 150 billion triples. The latter experiment is a 750 times increase over the previous largest BSBM report, and for the first time includes both its Explore and Business Intelligence workloads. The storage scheme used by Virtuoso for storing RDF Subject-PropertyObject triples pertaining to a Graph (hence we have quads, not triples) consists of five indexes: PSOG, POSG, SP, OP, GS. To be precise, PSOG is a B-tree with key (P,S,O,G), where P is a number identifying a property, S a subject, O an object and G the graph. Additionally, there is a B-tree holding URIs and a B-tree holding string literals, both of them used to encode string(-URI)s into numerical identifiers. Users may alter the indexing scheme of Virtuoso but this almost never happens. The three last indexes (SP, OP, GS) are projections of the first two covering indexes, containing only the unique combinations – hence these are much smaller. We note that Virtuoso Column Store Edition (V7) departs from the previous Virtuoso editions (V6) in that	aggregate data;b-tree;benchmark (computing);blocking (computing);central processing unit;column-oriented dbms;data model;degree of parallelism;encode;embarrassingly parallel;francis;identifier;infiniband;interrupt latency;locality of reference;lookup table;mathematical optimization;message passing;messaging pattern;nl (complexity);network switch;network traffic control;numerical analysis;optimization problem;parallel computing;quad data rate sram;query optimization;resource description framework;roland gs;sparql;scalability;throughput;time complexity;triplestore;whetstone (benchmark)	Peter A. Boncz;Orri Erling;Minh-Duc Pham	2014		10.1201/b16859-13	data mining;database;world wide web	DB	-14.815183905392805	54.38468648420122	129210
09a021ebbf2c980aa9c3d671be9170077cf3be3d	energy-efficient optimal real-time scheduling on multiprocessors	voltage and frequency scaling real time scheduling energy aware multiprocessor systems;multiprocessor systems;energy efficient;processor scheduling;real time;multiprocessors;energy efficient optimal real time scheduling;real time static voltage and frequency scaling techniques;voltage and frequency scaling;energy consumption;real time scheduling;real time static voltage and frequency scaling techniques energy efficient optimal real time scheduling multiprocessors;energy aware multiprocessor systems;energy efficiency frequency real time systems processor scheduling scheduling algorithm dynamic voltage scaling energy consumption partitioning algorithms embedded system voltage control;lower bound;real time systems	Optimal real-time scheduling is effective to not only schedulability improvement but also energy efficiency for real-time systems. In this paper, we propose real-time static voltage and frequency scaling (RT-SVFS) techniques based on an optimal real-time scheduling algorithm for multiprocessors. The techniques are theoretically optimal when the voltage and frequency can be controlled both uniformly and independently among processors. Simulation results show that the independent RT-SVFS technique closely approaches the lower bound on energy consumption if the voltage and frequency can be controlled minutely.	algorithm;central processing unit;dynamic frequency scaling;dynamic voltage scaling;image scaling;np-hardness;real-time clock;real-time computing;real-time operating system;real-time transcription;scheduling (computing);simulation;time complexity;windows rt	Kenji Funaoka;Shinpei Kato;Nobuyuki Yamasaki	2008	2008 11th IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing (ISORC)	10.1109/ISORC.2008.19	fair-share scheduling;embedded system;parallel computing;real-time computing;dynamic priority scheduling;computer science;efficient energy use;upper and lower bounds	Embedded	-5.443145789498859	58.81717059809121	129500
1a2c1f4b8321ccc59e3dc372eba283a519d3a76c	an optimization of dbn/gpu speech recognition on wireless network applications		With the development of wireless networks and mobile computing, using speech recognition with wireless networks in mobile terminals to process data has become a new trend in mobile computing and achieved great success. Therefore, how to improve the speed of training speech recognition is still a problem worth studying. Using GPU to accelerate the training of speech recognition based on Deep Belief Network (DBN) has achieved great success, but there exits some problems. Aiming the problems that single GPU can not store huge parameters of DBM at one time and the unreasonable usage of GPU’s memory model, we propose a new method in this paper. We divide the weight matrix into blocks, take the connections between visible units and hidden unit as threads and store the weight matrix into shared memory of GPU, establishing a reasonable memory model. Experimental results show that the optimized GPU implementation achieves 223 times and 1.5 times acceleration compared to single CPU and single GPU in Kaldi respectively, which demonstrate that our method can improve the DBN’s training speed in mobile computing without GPU memory limitation.	graphics processing unit;speech recognition	Weipeng Jing;Tao Jiang;Yaqiu Liu	2016		10.1007/978-3-319-72998-5_20	wireless network;computer science;memory model;distributed computing;dbm;deep belief network;speech recognition;mobile computing;thread (computing);shared memory	Mobile	-8.782623226780913	55.46593020235846	129588
52b039696df0b082a090bd2b3a0c82126711d6b7	tapas: temperature-aware adaptive placement for 3d stacked hybrid caches	non volatile memories;data migration;3d stacking;temperature	3D integration enables large last level caches (LLCs) to be stacked onto a die. In addition, emerging Non Volatile Memories (NVMs) such as Spin-Torque Transfer RAM (STT-RAM) have been explored as a replacement for traditional SRAM-based LLCs due to their higher density and lower leakage power.  In this paper, we aim to use the benefits of the integration of STT-RAM in a 3D multi-core environment. The main challenge we try to address is the high operating temperatures. The higher power density of 3D ICs might incur temperature-related problems in reliability, power consumption, and performance. Specifically, recent works have shown that elevated operating temperatures can adversely impact STT-RAM performance. To alleviate the temperature-induced problems, we propose TAPAS, a low-cost temperature-aware adaptive block placement and migration policy, for a hybrid LLC that includes STT-RAM and SRAM structures. This technique places cache blocks according to their temperature characteristics. Specifically, the cache blocks that heat up a hot bank are recognized and migrated to a cooler bank to 1) enable those blocks to get accessed in a cooler bank with lower read/write latency and 2) reduce the number of accesses to the hotter bank. We design and evaluate a novel flow control mechanism to assign priorities to those cache blocks to reach their destination.  Evaluation results reveal that TAPAS achieves, on average, 11.6% performance improvement, 6.5% power, and 5.6°C peak temperature reduction compared to a state-of-the art hybrid cache design.	computer performance;die (integrated circuit);flow control (data);multi-core processor;non-volatile memory;spectral leakage;static random-access memory	Majed Valad Beigi;Gokhan Memik	2016		10.1145/2989081.2989085	embedded system;parallel computing;real-time computing;engineering	Arch	-7.484537280636634	54.99743693860773	129709
5e08f66d0bca260458accdad741f6237199da091	lifetime management of flash-based ssds using recovery-aware dynamic throttling	lifetime management;static throttling technique;required ssd lifetime;required lifetime;recovery-aware dynamic throttling;floating-gate transistor;flash-based ssds;existing static throttling technique;proposed ready technique;self-recovery effect;proposed technique;dynamic throttling technique;ssd lifetime	NAND flash-based solid-state drives (SSDs) are increasingly popular in enterprise server systems because of their advantages over hard disk drives such as higher performance and lower power consumption. However, the limited and unpredictable lifetime of SSDs remains to be a serious obstacle to wider adoption of SSDs in enterprise systems. In this paper, we propose a novel recovery-aware dynamic throttling technique, called READY, which guarantees the SSD lifetime required by the enterprise market while exploiting the self-recovery effect of floating-gate transistors. Unlike a static throttling technique, the proposed technique makes throttling decisions dynamically based on the predicted future write demand of a workload so that the required SSD lifetime can be guaranteed with less performance degradation. The proposed READY technique also considers the self-recovery effect of floating-gate transistors which improves the endurance of SSDs, enabling to guarantee the required lifetime with less write throttling. Our experimental results show that the proposed READY technique can improvewrite performance by 4.4x with less variations on the write time over the existing static throttling technique while guaranteeing the required SSD lifetime.	elegant degradation;enterprise system;flash memory;hard disk drive;overhead (computing);recovery effect;response time (technology);server (computing);solid-state drive;transistor	Sungjin Lee;Taejin Kim;Kyungho Kim;Jihong Kim	2012			embedded system;real-time computing;computer hardware;operating system	OS	-11.472360541035016	55.19814900105004	129724
7daac59fe63e5771ef437549ece77fb91bdbfc67	lease/release: architectural support for scaling contended data structures	fast path slow path;non blocking queue;wait free	High memory contention is generally agreed to be a worst-case scenario for concurrent data structures. There has been a significant amount of research effort spent investigating designs which minimize contention, and several programming techniques have been proposed to mitigate its effects. However, there are currently few architectural mechanisms to allow scaling contended data structures at high thread counts.   In this paper, we investigate hardware support for scalable contended data structures. We propose Lease/Release, a simple addition to standard directory-based MSI cache coherence protocols, allowing participants to lease memory, at the granularity of cache lines, by delaying coherence messages for a  short, bounded  period of time. Our analysis shows that Lease/Release can significantly reduce the overheads of contention for both non-blocking (lock-free) and lock-based data structure implementations, while ensuring that no deadlocks are introduced. We validate Lease/Release empirically on the Graphite multiprocessor simulator, on a range of data structures, including queue, stack, and priority queue implementations, as well as on transactional applications. Results show that Lease/Release consistently improves both throughput and energy usage, by up to 5x, both for lock-free and lock-based data structure designs.	data structure;image scaling	Syed Kamran Haider;William Hasenplaugh;Dan Alistarh	2016		10.1145/2851141.2851155	parallel computing;real-time computing;computer science;operating system	Arch	-10.083302144295347	50.62052591224303	129908
d4651d9a2c12717e71c3aae86b13f1181749d05e	cobra: an adaptive runtime binary optimization framework for multithreaded applications	data sharing;multi threading;storage management multi threading;storage management;itanium 2 based smp;adaptive runtime binary optimization framework;openmp nas parallel benchmark;multithreaded applications;cobra;data prefetching;runtime program behavior;prefetch instructions;runtime prefetching yarn optimizing compilers design optimization application software scalability program processors monitoring system buses;continuous binary readaptation;nas parallel benchmarks;prefetch instructions cobra adaptive runtime binary optimization framework multithreaded applications continuous binary readaptation itanium 2 based smp cc numa systems openmp nas parallel benchmark runtime program behavior data prefetching;cc numa systems	This paper presents COBRA (continuous binary re-adaptation), a runtime binary optimization framework, for multithreaded applications. It is currently implemented on Itanium 2 based SMP and cc-NUMA systems. Using OpenMP NAS parallel benchmark, we show how COBRA can adoptively choose appropriate optimizations according to observed changing runtime program behavior. Coherent cache misses caused by true/false data sharing often limit the scalability of multithreaded applications. This paper shows that COBRA can significantly improve the performance of some applications parallelized with OpenMP, by reducing the aggressiveness of data prefetching and by using exclusive hints for prefetch instructions. For example, we show that COBRA can improve the performance of OpenMP NAS parallel benchmarks up to 68%, with an average of 17.5% on the SGI Altix cc-NUMA system.	benchmark (computing);cpu cache;cache coherence;coherent;ibm notes;itanium;mathematical optimization;nas parallel benchmarks;non-uniform memory access;openmp;parallel computing;performance per watt;prototype;scalability;speedup;supercomputer;symmetric multiprocessing;thread (computing);university of minnesota supercomputing institute	Jinpyo Kim;Wei-Chung Hsu;Pen-Chung Yew	2007	2007 International Conference on Parallel Processing (ICPP 2007)	10.1109/ICPP.2007.23	computer architecture;parallel computing;multithreading;computer science;operating system	HPC	-7.372640741443846	48.59518059718747	130042
45462c400fe7e88ce6ed0e44aa0aa782a2f7295b	evaluating the performance impact of multiple streams on the mic-based heterogeneous platform	benchmarking;pipelining performance evaluation multiple streams resource partitioning;kernel;paper;performance evaluation;heterogeneous systems;resource partitioning;performance;coprocessors;computational modeling;resource granularity performance impact multiple streams mic based heterogeneous platform gpu microbenchmarking data transfers kernel execution search space task granularity;graphics processing units;computer science;pipelining;kernel coprocessors performance evaluation benchmark testing data transfer graphics processing units computational modeling;intel xeon phi;benchmark testing;data transfer;task analysis benchmark testing graphics processing units operating system kernels;multiple streams	Using multiple streams can improve the overall system performance by mitigating the data transfer overhead on heterogeneous systems. Prior work focuses a lot on GPUs but little is known about the performance impact on (Intel Xeon) Phi. In this work, we apply multiple streams into six real-world applications on Phi. We then systematically evaluate the performance benefits of using multiple streams. The evaluation work is performed at two levels: the microbenchmarking level and the real-world application level. Our experimental results at the microbenchmark level show that data transfers and kernel execution can be overlapped on Phi, while data transfers in both directions are performed in a serial manner. At the real-world application level, we show that both overlappable and non-overlappable applications can benefit from using multiple streams (with an performance improvement of up to 24%). We also quantify how task granularity and resource granularity impact the overall performance. Finally, we present a set of heuristics to reduce the search space when determining a proper task granularity and resource granularity. To conclude, our evaluation work provides lots of insights for runtime and architecture designers when using multiple streams on Phi.	benchmark (computing);graphics processing unit;heuristic (computer science);kernel (operating system);machine learning;overhead (computing);performance evaluation;xeon phi	Zhaokui Li;Jianbin Fang;Tao Tang;Xuhao Chen;Cheng Chen;Canqun Yang	2016	2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2016.99	niche differentiation;benchmark;parallel computing;kernel;real-time computing;performance;computer science;operating system;distributed computing;xeon phi;computational model;pipeline;coprocessor;benchmarking	HPC	-6.129543855164961	47.5458711087246	130067
372b2d1627747a2edf3a1015a82fbb32d628d899	optimizing mechanisms for latency tolerance in remote memory access communication on clusters	general and miscellaneous mathematics computing and information science;performance evaluation;communications;implementation;performance evaluation workstation clusters message passing;performance;nonblocking operations optimizing mechanisms latency tolerance remote memory access communication clusters myrinet user level requirements network specific communication interfaces latency hiding overlapping communication microbenchmarks nas parallel benchmark suite;latency tolerance;high performance networks;computer networks;computer network;performance improvement;design and implementation;benchmarks;message passing;design;workstation clusters;remote memory access;nas parallel benchmarks;memory devices	paper describes the design and implementation of mechanisms for latency tolerance in the remote memory access communication on clusters equipped with high-performance networks such as Myrinet. It discusses strategies that bridge the gap between user-level requirements and network-specific communication interfaces while attempting to increase opportunities for latency hiding. Mechanisms for overlapping communication with computation and coalescing small messages (trading latency for bandwidth) are explored. The effectiveness of these techniques is evaluated using microbenchmarks and application kernels including the NAS parallel benchmark suite. The microbenchmark results showed a better degree of overlap for nonblocking operations in ARMCI as compared to MPI. Application results showed up 30% to 45% improvement over MPI on using nonblocking operations. The aggregation of small messages yielded performance improvement of up to 78% over non-aggregated communication.	benchmark (computing);computation;message passing interface;network-attached storage;optimizing compiler;requirement;user space	Jarek Nieplocha;Vinod Tipparaju;Manojkumar Krishnan;Gopalakrishnan Santhanaraman;Dhabaleswar K. Panda	2003		10.1109/CLUSTR.2003.1253309	design;parallel computing;message passing;real-time computing;performance;computer science;operating system;distributed computing;implementation;computer network	HPC	-10.632261397249781	46.53469727220206	130077
107378252f13e8a94819269fc706e5149672c7ec	efficient main memory deduplication through cross layer integration			computer data storage;data deduplication	Konrad Miller	2014				DB	-13.330247097892764	55.029670816664606	130116
b90a2a6114837d2a9186de8c47bb72df6181d210	a user friendly phase detection methodology for hpc systems' analysis	phase detection;phase identification;systeme d exploitation;architectures materielles;phase representation;systemes embarques;phase detection benchmark testing vectors radiation detectors hardware measurement algorithms;reseaux et telecommunications;user friendly phase detection methodology execution vector based phase detection ev based phase detection program phase detection high performance computing hpc system analysis;simulation point;execution vector;parallel processing;phase representation phase detection execution vector phase identification simulation point	A wide array of today's high performance computing (HPC) applications exhibits recurring behaviours or execution phases throughout their run-time. Accurate detection of program phases allows reconfiguring the system for a better power/performance trade off, and can reduce the simulation time of programs by identifying regions of code whose performance is critical to the entire program. Program phases are also reflected in different behaviours the system goes through or system phases, which can be used as an alternative means of program phase detection for users lacking expertise. In this paper, we present an execution vector based (EV-based) phase detection, which is an on-line methodology for detecting phases in the behaviour of a HPC system and determining execution points that correspond to these phases. We also present a methodology for defining a small set of EVs representative of the system's behaviour over a fixed period of time and show that EV-based phase detection identifies recurring phases. Our methodology is illustrated with benchmarks and a real life application.	benchmark (computing);extended validation certificate;online and offline;real life;sensor;simulation;supercomputer	Ghislain Landry Tsafack Chetsa;Laurent Lefèvre;Jean-Marc Pierson;Patricia Stolf;Georges Da Costa	2013	2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing	10.1109/GreenCom-iThings-CPSCom.2013.43	phase detector;embedded system;parallel processing;real-time computing;simulation;computer science	HPC	-5.392259479912424	51.18597818388314	130128
42e478fa66614ed7601be8ac9bfbfb7d728c2711	scope-based method cache analysis	real time systems cache analysis time predictable computer architecture;004	The quest for time-predictable systems has led to the exploration of new hardware architectures that simplify analysis and reasoning in the temporal domain, while still providing competitive performance. For the instruction memory, the method cache is a conceptually attractive solution, as it requests memory transfers at well-defined instructions only. In this article, we present a new cache analysis framework that generalizes and improves work on cache persistence analysis. The analysis demonstrates that a global view on the cache behavior permits the precise analyses of caches which are hard to analyze by inspecting cache state locally. 1998 ACM Subject Classification B.8.2 Performance Analysis and Design Aids	persistence (computer science)	Benedikt Huber;Stefan Hepp;Martin Schoeberl	2014		10.4230/OASIcs.WCET.2014.73	bus sniffing;pipeline burst cache;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;computer hardware;cache;computer science;write-once;cache invalidation;operating system;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;global assembly cache	Arch	-10.658877011181442	50.382641806838585	130166
0d6b6ea81c6db3536a6e716316570970209123fe	dynamic and adaptive spm management for a multi-task environment	virtual memory;memory management unit;preemptive scheduling;real time operating system;journal;memory access;management strategy;reference systems;energy consumption;multi task system;static analysis;scratchpad memory;virtual space;embedded processor	In this paper, we present a dynamic and adaptive scratchpad memory (SPM) management strategy targeting a multi-task environment. It can be applied to a contemporary embedded processor that maps the physically addressed SPM into a virtual space with the help of an integrated memory management unit (MMU). Based on mass-count disparity, we introduce a hardware memory reference sampling unit (MRSU) that samples the memory reference stream with very low probability. The captured address is considered as one of the memory addresses contained in a frequently referenced memory block. A hardware interruption is generated by the MRSU, and the identified frequently accessed memory block is placed into the SPM space by software. The software also modifies the page table so that the follow-up memory accesses to the memory block will be redirected to the SPM. With no dependence on compiler and profiling information, our proposed strategy is specifically adequate for SPM management in a multi-task environment. In such an environment, a real-time operating system (RTOS) is usually hosted, and the behavior of the memory accesses cannot be predicted by static analysis or profiling. We evaluate our SPM allocation strategy by running several tasks on a tiny RTOS with preemptive scheduling. Experimental results show that our approach can achieve 10% reduction in energy consumption on average, with 1% performance degradation at runtime compared with a cache-only reference system.	computer multitasking;super paper mario	Weixing Ji;Ning Deng;Feng Shi;Qi Zuo;Jiaxin Li	2011	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2010.11.002	uniform memory access;distributed shared memory;shared memory;embedded system;interleaved memory;parallel computing;real-time computing;page fault;real-time operating system;memory management unit;distributed memory;memory refresh;computer science;physical address;virtual memory;operating system;memory protection;static memory allocation;overlay;preemption;conventional memory;extended memory;flat memory model;memory segmentation;registered memory;static analysis;memory map;memory management	Embedded	-7.337570959305758	50.18174482036658	130226
3f8dbd4a5e05ab90f59442b77a5f6b8afe8ee561	garbage collection algorithms for nand flash memory devices -- an overview	sata;write amplification;performance evaluation;flash memories performance evaluation memory management solids power demand operating systems;storage management;storage management flash memories nand circuits performance evaluation;trim;wear leveling;garbage collection;ftl garbage collection trim hdd ssd sata write amplification wear leveling;ssd;nand circuits;ftl;hdd;flash memories;flash page write amplication garbage collection algorithms nand flash memory devices flash translation layer ftl sata ssd solid state devices ssd performance flash memory long term problems flash memory reliability threshold program limit p e cycles wear leveling operations wl operations gc operations flash management flash page wear out	This paper highlights the necessary garbage collection algorithms which is needed in flash translation layer (FTL) for NAND Flash memory devices such as SATA SSD's (Solid State Devices). Garbage collection is the process of freeing up partially filled blocks to make room for more data and the Garbage collection (GC) algorithms recommended here plays a key role in maintaining SSD's Performance and structuring the Flash memory's long term problems related to Reliability and early wearing out of cells due to repeated overwrites beyond the threshold program limit (P/E cycles). NAND Flash Memories require Garbage Collection (GC) and Wear Leveling (WL) operations to be carried out by Flash Translation Layers (FTLs) that oversee flash management. These Algorithms were implemented to reduce the number of read/write, erase cycles on the pages so that Wear Leveling, Write Amplification and Wear out of the Flash Pages can be reduced.	algorithm;ftl: faster than light;flash file system;flash memory controller;garbage collection (computer science);serial ata;solid-state drive;solid-state electronics;wear leveling	Raja Subramani;Haritima Swapnil;Niharika Thakur;Bharath Radhakrishnan;Krishnamurthy Puttaiah	2013	2013 European Modelling Symposium	10.1109/EMS.2013.14	flash file system;parallel computing;real-time computing;computer hardware;computer science;flash memory emulator	OS	-11.246445955429138	54.8841717257754	130236
a2c6610495d9c037e447d3faf996bfdee75d2e20	concise loads and stores: the case for an asymmetric compute-memory architecture for approximation		Cache capacity and memory bandwidth play critical roles in application performance, particularly for data-intensive applications from domains that include machine learning, numerical analysis, and data mining. Many of these applications are also tolerant to imprecise inputs and have loose constraints on the quality of output, making them ideal candidates for approximate computing. This paper introduces a novel approximate computing technique that decouples the format of data in the memory hierarchy from the format of data in the compute subsystem to significantly reduce the cost of storing and moving bits throughout the memory hierarchy and improve application performance. This asymmetric compute-memory extension to conventional architectures, ACME, adds two new instruction classes to the ISA - load-concise and store-concise - along with three small functional units to the micro-architecture to support these instructions. ACME does not affect exact execution of applications and comes into play only when concise memory operations are used. Through detailed experimentation we find that ACME is very effective at trading result accuracy for improved application performance. Our results show that ACME achieves a 1.3x speedup (up to 1.8x) while maintaining 99% accuracy, or a 1.1x speedup while maintaining 99.999% accuracy. Moreover, our approach incurs negligible area and power overheads, adding just 0.005% area and 0.1% power to a conventional modern architecture.	acme;approximate computing;approximation;data mining;data-intensive computing;machine learning;memory bandwidth;memory hierarchy;microarchitecture;numerical analysis;speedup	Animesh Jain;Parker Hill;Shih-Chieh Lin;Muneeb Khan;Md E. Haque;Michael Laurenzano;Scott A. Mahlke;Lingjia Tang;Jason Mars	2016	2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)		parallel computing;real-time computing;computer science;theoretical computer science;operating system;programming language;information and computer science	Arch	-5.52466131069712	50.88400775284078	130257
3fe7bcf79b1b880f0c3adea03c094ab23e4cd117	load balancing distributed inverted files	search engine;query processing;inverted files;parallel and distributed computing;scheduling algorithm;bulk synchronous parallel;cluster processor;load balance	This paper present a comparison of scheduling algorithms applied to the context of load balancing the query traffic on distributed inverted files. We implemented a number of algorithms taken from the literature. We propose a novel method to formulate the cost of query processing so that these algorithms can be used to schedule queries onto processors. We avoid measuring load balance at the search engine side because this can lead to imprecise evaluation. Our method is based on the simulation of a bulk-synchronous parallel computer at the broker machine side. This simulation determines an optimal way of processing the queries and provides a stable baseline upon which both the broker and search engine can tune their operation in accordance with the observed query traffic. We conclude that the simplest load balancing heuristics are good enough to achieve efficient performance. Our method can be used in practice by broker machines to schedule queries efficiently onto the cluster processors of search engines.	algorithm;baseline (configuration management);bulk synchronous parallel;central processing unit;database;heuristic (computer science);inverted index;load balancing (computing);parallel computing;principle of good enough;scheduling (computing);simulation;web search engine	Mauricio Marín;Carlos Gómez-Pantoja	2007		10.1145/1316902.1316912	real-time computing;computer science;load balancing;database;distributed computing;scheduling;world wide web;search engine;bulk synchronous parallel	Web+IR	-17.42235127249333	59.91030114549865	130391
f3a4ae84fa5b36eb8d47c5d6adfa42a925615887	a fair replica placement for parallel download on cluster grid	data intensive application;quality of transmission;numerical computation;data access;high speed;data grid	Grid technologies congregate numerous computers to provide powerful computing and massive storage. In data-intense applications, Data Grids are developed to cope with the efficiency of data access. Replication is one of the methods to elevate the access performance. When a file is replicated, it can be downloaded from all the nodes with that file in parallel, thus reducing the access latency. Therefore, a fair and adaptive replication strategy for high speed transmission is important. In this paper, we design such a strategy to duplicate popular files to beneficial nodes in the grid networks. All the users deserve the same quality of transmission. The contributions of our mechanism are to average the transmission cost and evenly distribute the workload for download. Simulation results are also given to demonstrate the performance of our replication strategy.	download	Chih-Ming Wang;Chu-Sing Yang;Ming-Chao Chiang	2007		10.1007/978-3-540-74573-0_28	data access;grid file;parallel computing;computer science;operating system;data grid;database;distributed computing;computer security;computer network	HPC	-15.857864405053213	52.926475615257125	130551
0d80792095683e980b5f2e0639e6d8eb000d2df7	plover: a proactive low-overhead file replication scheme for structured p2p systems	p2p system;communications society;peer to peer file sharing systems;proactive low overhead file replication scheme;file query redirection algorithm;high performance computing;resource allocation;time consistency;resource allocation file organisation peer to peer computing;intrusion detection;software engineering;computational modeling;internet;plover;load balancing plover proactive low overhead file replication scheme peer to peer file sharing systems consistency maintenance file query redirection algorithm;aggregates;peer to peer computing internet computer science communications society high performance computing load management computational modeling software engineering intrusion detection aggregates;load management;load balancing;load balance;file sharing;computer science;consistency maintenance;peer to peer computing;peer to peer;high performance;high efficiency;file organisation	File replication is a widely used technique for high performance in peer-to-peer file sharing systems. A file replication technique should be efficient and meanwhile facilitates efficient file consistency maintenance. However, most traditional methods don't consider node available capacity and physical location in file replication, leading to high overhead for both file replication and consistency maintenance. This paper presents a proactive low-overhead file replication scheme, namely Plover. By making file replication among physically close nodes based on node available capacities, Plover not only achieves high efficiency in file replication but also supports low-cost and timely consistency maintenance. It also includes an efficient file query redirection algorithm for load balancing between replica nodes. Simulation results demonstrate the effectiveness of Plover in comparison with other file replication schemes. It dramatically reduces the overhead of both file replication and consistency maintenance compared to other schemes. In addition, it yields significant improvements in reduction of overloaded nodes.	algorithm;consistency model;interdependence;load balancing (computing);locality of reference;overhead (computing);peer-to-peer file sharing;plover;proactive parallel suite;simulation	Haiying Shen;Yingwu Zhu	2008	2008 IEEE International Conference on Communications	10.1109/ICC.2008.1053	self-certifying file system;supercomputer;real-time computing;computer science;load balancing;operating system;database;distributed computing;file system fragmentation;replication;computer network	HPC	-19.08763792216789	53.21329108871719	130704
dc88240ccc9fbe5ced19cd1c4a5677dd9c733e36	paired robs: a cost-effective reorder buffer sharing strategy for smt processors	evaluation methodology;cost effectiveness	An important design issue of SMT processors is to find proper sharing strategies of resources among threads. This paper proposes a ROB sharing strategy, called paired ROB, that considers the fact that task parallelism is not always available to fully utilize resources of multithreaded processors. To this aim, an evaluation methodology is proposed and used for the experiments, which analyzes performance under different degrees of parallelism. Results show that paired ROBs are a cost-effective strategy that provides better performance than private ROBs for low task parallelism, whereas it incurs slight performance losses for high task parallelism.	blocking (computing);central processing unit;experiment;parallel computing;re-order buffer;task parallelism;thread (computing);vector processor	Rafael Ubal;Julio Sahuquillo;Salvador Petit;Pedro López	2009		10.1007/978-3-642-03869-3_31	computer architecture;parallel computing;real-time computing;cost-effectiveness analysis;computer science;operating system;distributed computing;data parallelism;instruction-level parallelism;task parallelism	HPC	-8.97788081042164	51.615440545323914	130758
6eed0a4f11ca34dfcb834bd309233b00606caeca	graceful degradation of low-criticality tasks in multiprocessor dual-criticality systems		According to the conventional mixed-criticality (MC) system model, low-criticality tasks are completely discarded in high-criticality system mode. Allowing such loss of low-criticality tasks is controversial and not obviously necessary. We study how to achieve graceful degradation of low-criticality tasks by continuing their executions with imprecise computing or even precise computing if there is sufficient utilization slack. Schedulability conditions under this Variable-Precision Mixed-Criticality (VPMC) system model are investigated for partitioned scheduling and fpEDF-VD scheduling. It is found that the two scheduling methods in VMPC retain the same speedup factors as in conventional MC systems. We develop a precision optimization approach that maximizes precise computing of low-criticality tasks through 0-1 knapsack formulation. Experiments are performed through both software simulations and Linux prototyping with consideration of overhead. The results show that schedulability degradation caused by continuing low-criticality task execution is often very small. The proposed precision optimization can largely reduce computing errors compared to constantly executing low-criticality tasks with imprecise computing in high-criticality mode. The prototyping results indicate that partitioned scheduling in VPMC outperforms the latest work based on fluid model.	criticality matrix;elegant degradation;fault tolerance;graceful exit;knapsack problem;linux;mathematical optimization;mixed criticality;multiprocessing;overhead (computing);scheduling (computing);self-organized criticality;simulation;slack variable;speedup;variably modified permutation composition	Lin Huang;I-Hong Hou;Sachin S. Sapatnekar;Jiang Hu	2018		10.1145/3273905.3273909	distributed computing;knapsack problem;scheduling (computing);software;fault tolerance;speedup;multiprocessing;system model;criticality;computer science	Embedded	-12.029709120116886	59.11839403840183	130791
d4f59859544799acb23895d95a033e5abc75bc1e	the named-state register file: implementation and performance	register file organization;storage allocation;cache storage;yarn;hardware techniques;named state register file;storage management;software techniques;software performance evaluation;parallel procedure activations;traffic control;sequential procedure activations;chip;registers switches yarn laboratories hardware milling machines artificial intelligence program processors costs traffic control;concurrent contexts;program compilers file organisation cache storage parallel architectures storage management storage allocation software performance evaluation;parallel architectures;registers;access time named state register file fine grain associative register file software techniques hardware techniques parallel procedure activations sequential procedure activations reload traffic concurrent contexts register file organization;milling machines;artificial intelligence;register file;program compilers;switches;parallel programs;fine grain associative register file;program processors;reload traffic;access time;hardware;file organisation	Context switches are slow in conventional processors because the entire processor state must be saved and restored, even if much of the state is not used before the next context switch. This paper introduces the NamedState Register File , a fine-grain associative register file. The NSF uses hardware and software techniques to efficiently manage registers among sequential or parallel procedure activations. The NSF holds more live data per register than conventional register files, and requires much less spill and reload traffic to switch between concurrent contexts. The NSF speeds execution of some sequential and parallel programs by 9% to 17% over alternative register file organizations. The NSF has access time comparable to a conventional register file and only adds 5% to the area of a typical processor chip.	access time;call stack;central processing unit;context switch;ibm notes;instruction cycle;interleaved memory;network switch;parallel computing;processor register;register allocation;register file;simulation;subroutine;thyristor	Peter R. Nuth;William J. Dally	1995		10.1109/HPCA.1995.386560	chip;computer architecture;parallel computing;real-time computing;register window;computer file;network switch;control register;access time;computer science;memory buffer register;operating system;register renaming;stack register;index register;processor register;flags register;programming language;register file;status register;memory data register;memory address register	Arch	-11.671062678007873	49.6509885602004	130794
3563d22f9c0aa426b37851b2642cf4bf6dfdbf9c	parallel game-tree search	tratamiento paralelo;alpha beta search;pediatrics;splitting method;traitement parallele;multiprocessor;chess game;probability density function;concurrent programming;search algorithm;delay effects;search strategy;graph and tree search strategies;message passing multiprocessors;data mining;tree graphs;ajedrez;tree decomposition alpha beta search chess programs concurrent programming graph and tree search strategies message passing multiprocessors;games;message passing;mathematical model;chess programs;concurrent programs;councils;communication delay;jeu echecs;delay systems;tree decomposition;parallel implementation;computer science;parallel processing message passing tree graphs algorithm design and analysis delay systems delay effects decision trees parallel algorithms councils computer science;game tree search;multiprocesador;decision trees;program processors;algorithm design and analysis;parallel processing;reactive power;parallel algorithms;multiprocesseur	The design issues affecting a parallel implementation of the alpha-beta search algorithm are discussed with emphasis on a tree decomposition scheme that is intended for use on well ordered trees. In particular, the principal variation splitting method has been implemented, and experimental results are presented which show how such refinements as progressive deepening, narrow window searching, and the use of memory tables affect the performance of multiprocessor based chess playing programs. When dealing with parallel processing systems, communication delays are perhaps the greatest source of lost time. Therefore, an implementation of our tree decomposition based algorithm is presented, one that operates with a modest amount of message passing within a network of processors. Since our system has low search overhead, the principal basis for comparison is the communication overhead, which in turn is shown to have two components.	allocation;alpha–beta pruning;central processing unit;data table;experiment;greater than;inter-process communication;memory management;message passing;minimax;multiprocessing;numerous;nut hypersensitivity;overhead (computing);parallel computing;progressive meshes;search algorithm;transposition table;tree (data structure);tree decomposition;trees (plant);variation (game tree)	T. Anthony Marsland;Fred Popowich	1985	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.1985.4767683	games;parallel processing;algorithm design;probability density function;parallel computing;message passing;multiprocessing;computer science;theoretical computer science;machine learning;decision tree;mathematical model;distributed computing;parallel algorithm;ac power;alpha–beta pruning;tree;statistics;tree decomposition;search algorithm	HPC	-16.51166964740237	46.87800238442188	130876
b31468d043fedcbaf1101d76a6e0dd863a01e5dc	effect of data access delays and system partitionability on the dynamic performance of a shared memory multiprocessor	delays;performance evaluation;shared memory systems;data access delays;delays;dynamic performance;global shared memory;low processor utilization;nondeterministic interconnection network delays;performance;processor requirements;shared memory accesses;shared memory multiprocessor;simulation model;system partitionability	Any architectural or operational inef~iencies in multiprocessors with thoasands of processors is likely to have a significant negative impact on the performance of the system. This paper examines two such issues in large scale shared memory multiprocessors — the delays involved in accessing global shared memory, and the low processor utilization resulting from inefficient partitioning of the system among several tasks. These studies are undertaken using a simulation model that directly integrates the non-de temtinistic interconnection network delays encountered during shared memory accesses into the execution of tasks on processors. The results of our study show the quantitative impact of interconnection network delay on system pe~ormance and point to potential ways of reducing this impact. Furthermore, when incoming tasks hnve large processor requirements, we find that the system’s inability to partition the processors among several tasks e~ciently has a signijcant detrimental effect on peflormance. We discuss the effectiveness of some techniques to solve this partitionability problem. The results from several experiments exploring these and other issues are presented.	central processing unit;data access;experiment;interconnection;multiprocessing;requirement;shared memory;simulation	Seth Abraham;Krishnan Padmanabhan	1992			uniform memory access;distributed shared memory;data access;shared memory;parallel computing;real-time computing;distributed memory;performance;computer science;operating system;simulation modeling;distributed computing;computer performance	Arch	-10.278175338350842	49.80227220273333	131092
21162a22b032f21774c9a9822b8b39d27bb1ac0b	instruction shuffle: achieving mimd-like performance on simd architectures	simd;kernel;resource management;mimd execution paradigm mimd like performance simd architecture multiple instruction multiple data single instruction multiple data diverse control flow behavior identical control flow behavior shuffle source instruction buffer array instruction shuffle unit simd execution paradigm;instruction sets process control resource management vectors scalability;arrays;vectors;instruction buffer array;process control;simd kernel process control resource management vectors scalability arrays data dependent control flow instruction shuffle instruction buffer array;data dependent control flow;scalability;instruction shuffle;parallel processing;instruction sets	SIMD architectures are less efficient for applications with the diverse control-flow behavior, which can be mainly attributed to the requirement of the identical control-flow. In this paper, we propose a novel instruction shuffle scheme that features an efficient control-flow handling mechanism. The cornerstones are composed of a shuffle source instruction buffer array and an instruction shuffle unit. The shuffle unit can concurrently deliver instructions of multiple distinct control-flows from the instruction buffer array to eligible SIMD lanes. Our instruction shuffle scheme combines the best attributes of both the SIMD and MIMD execution paradigms. Experimental results show that, an average performance improvement of 86% can be achieved, at a cost of only 5.8% area overhead.	best, worst and average case;control flow;iteration;mimd;overhead (computing);parallel computing;simd;task parallelism	Yaohua Wang;Shuming Chen;Kai Zhang;Jianghua Wan;Xiaowen Chen;Hu Chen;Haibo Wang	2012	IEEE Computer Architecture Letters	10.1109/L-CA.2011.34	parallel processing;computer architecture;parallel computing;kernel;real-time computing;scalability;simd;computer science;resource management;operating system;instruction set;process control;instruction register	Arch	-6.443661681237485	49.663205862567224	131209
6920c2d53727060e935179f6b9833716d644f11e	an intermediate data placement algorithm for load balancing in spark computing environment	data skew;load balancing;spark;mapreduce;data sampling	Since MapReduce became an effective and popular programming framework for parallel data processing, key skew in intermediate data has become one of the important system performance bottlenecks. For solving the load imbalance of bucket containers in the shuffle process of the Spark computing framework, this paper proposes a splitting and combination algorithm for skew intermediate data blocks (SCID), which can improve the load balancing for various reduce tasks. Because the number of keys cannot be counted out until the input data are processed by map tasks, this paper provides a sampling algorithm based on reservoir sampling to detect the distribution of the keys in intermediate data. Contrasting with the original mechanism for bucket data loading, SCID sorts the data clusters of key/value tuples from each map task according to their sizes, and fills them into the relevant buckets orderly. A data cluster will be split once it exceeds the residual volume of the current bucket. After filling this bucket, the remainder clusterwill be entered into the next iteration. Through this processing, the total size of data in each bucket is roughly scheduled equally. For each map task, each reduce task should fetch the intermediate results from a specific bucket, the quantity in all buckets for a map task will balance the load of the reduce tasks. We implement SCID in Spark 1.1.0 and evaluate its performance through three widely used benchmarks: Sort, Text Search, and Word Count. Experimental results show that our algorithms can not only achieve higher overall average balancing performance, but also reduce the execution time of a job with varying degrees of data skew. © 2016 Elsevier B.V. All rights reserved.	algorithm;apache spark;bucket (computing);iteration;job stream;load balancing (computing);mapreduce;reservoir sampling;run time (program lifecycle phase);sampling (signal processing);scid	Zhuo Tang;Xiangshen Zhang;Keqin Li;Keqin Li	2018	Future Generation Comp. Syst.	10.1016/j.future.2016.06.027	parallel computing;real-time computing;reservoir sampling;tuple;spark (mathematics);data cluster;remainder;computer science;load balancing (computing);algorithm;leaky bucket;distributed computing;token bucket	HPC	-15.999214808716848	55.679573397881235	131262
db00c8c29e2f1a8bc454c598b055f0c131481942	characterizing the sort operation on multithreaded architectures		The Sort operation is a core part of many critical applications. Despite the large efforts to parallelize it, the fact that it suffers from high data-dependencies vastly limits its performance. Multithreaded architectures are emerging as the most demanding technology in leading-edge processors. These architectures include Simultaneous Multithreading, Chip Multiprocessors and machines combining different multithreading technologies. In this paper, we analyze the memory behavior and improve the performance of the most recent parallel radix and quick integer sort algorithms on modern multithreaded architectures. We achieve speedups up to 4.69x for radix sort and up to 4.17x for quick sort on a machine with 4 multithreaded processors compared to single threaded versions, respectively. We find that since radix sort is CPU-intensive, it exhibits better results on Chip multiprocessors where multiple CPUs are available. While quick sort is accomplishing speedups on all types of multithreading processers due to its ability to overlap memory miss latencies with other useful processing.	central processing unit;data dependency;load balancing (computing);mathematical optimization;pentium 4;quicksort;radix sort;simultaneous multithreading;sorting algorithm	Layali K. Rashid;Wessam Hassanein;Moustafa A. Hammad	2008			computer science;parallel computing;particle;inlet;horizontal position representation;distributed computing;flow (psychology);kiln;duct (flow);damper;conical surface	HPC	-9.85551859947412	49.695696125068984	131471
52786bd74830dedb7a96cc61a6020510192b784c	conceptual: a network correctness and performance testing language	performance measure;design decision network correctness domain specific specification language conceptual performance testing language sophisticated communication benchmarks network validation tests debugging code bandwidth test round trip transmission back to back unidirectional messages benchmark preregisters buffer warm up messages;performance evaluation;performance test;performance comparison;performance evaluation specification languages message passing program debugging;lines of code;specification languages;benchmark testing specification languages programming profession writing debugging measurement reproducibility of results bandwidth clocks performance evaluation;message passing;source code;program debugging;high level language;domain specificity	"""Summary form only given. We introduce a new, domain-specific specification language called CONCEPTUAL. CONCEPTUAL enables the expression of sophisticated communication benchmarks and network validation tests in comparatively few lines of code. Besides helping programmers save time writing and debugging code, CONCEPTUAL addresses the important-but largely unrecognized-problem of benchmark opacity. Benchmark opacity refers to the current impracticality of presenting performance measurements in a manner that promotes reproducibility and independent evaluation of the results. For example, stating that a performance graph was produced by a """"bandwidth"""" test says nothing about whether that test measures the data rate during a round-trip transmission or the average data rate over a number of back-to-back unidirectional messages; whether the benchmark preregisters buffers, sends warm-up messages, and/or preposts asynchronous receives before starting the clock; how many runs were performed and whether these were aggregated by taking the mean, median, or maximum; or, even whether a data unit such as """"MB/s"""" indicates 10/sup 6/ or 2/sup 20/ bytes per second. Because CONCEPTUAL programs are terse, a benchmark's complete source code can be listed alongside performance results, making explicit all of the design decisions that went into the benchmark program. Because CONCEPTUAL's grammar is English-like, CONCEPTUAL programs can easily be understood by nonexperts. And because CONCEPTUAL is a high-level language, it can target a variety of messaging layers and networks, enabling fair and accurate performance comparisons."""	analytical performance modeling;benchmark (computing);byte;compiler;correctness (computer science);data logger;data rate units;debugging;domain-specific language;error detection and correction;high- and low-level;high-level programming language;human-readable medium;measuring network throughput;megabyte;network performance;networking hardware;performance prediction;programmer;runtime system;software performance testing;source lines of code;specification language	Scott Pakin	2004	18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.	10.1109/IPDPS.2004.1303014	parallel computing;message passing;real-time computing;computer science;theoretical computer science;operating system;distributed computing;programming language;source lines of code;high-level programming language;algorithm;source code	Arch	-16.87546680652811	47.599607295510225	131520
de8f972df6b7bfd32692db268ec54bb031b1ef3c	the tokufs streaming file system	block-level compression;write-optimized file system;read-optimized file system;large file;small unaligned writes;disk usage;fractal tree index;tokufs file system;small file	The TokuFS file system outperforms write-optimized file systems by an order of magnitude on microdata write workloads, and outperforms read-optimized file systems by an order of magnitude on read workloads. Microdata write workloads include creating and destroying many small files, performing small unaligned writes within large files, and updating metadata. TokuFS is implemented using Fractal Tree indexes, which are primarily used in databases. TokuFS employs block-level compression to reduce its disk usage.	append;b-tree;database;fractal;image scaling;microdata (html);supercomputer	John Esmet;Michael A. Bender;Martin Farach-Colton;Bradley C. Kuszmaul	2012			self-certifying file system;parallel computing;torrent file;indexed file;computer file;computer science;stub file;versioning file system;operating system;unix file types;journaling file system;database;open;data file;file system fragmentation;file control block	OS	-13.484045971905362	53.55245185858422	131528
add5c379c805b889fba749958182704ccce31da9	"""response to """"comment on 'a novel data distribution technique for host-client type parallel applications"""""""	libraries;data compression;heterogeneous cpu;finishing;client server systems;transform coding;host client type parallel applications;data distribution;books;data distribution technique;genetic algorithms;differential equations;scalability;parallel applications;parallel processing;australia	Our rebuttal statement is in two parts. A. Generic rebuttal: the area of scheduling is very active with more than 3,000 papers published over this decade alone in good journals. In addition, Krishna and Shin note, in 1997 itself, that the number of papers in the area seems to have increased exponentially since 1970. As a consequence, it is rather difficult to get a handle on all of them.. The records indicate that several aspects of our work appear to be very contemporary to the works of some of the other's work cited by the author. This goes to show that more than one person can have the similar approach to solving similar problems. B. Technical rebuttal: the work quoted by Sohn et al. (1998) deals with the analysis of heterogeneous CPUs. In particular, with the analysis of minimizing the cost (as in $) by selecting the lowest selection of CPUs available that will finish a job in a given time frame. They also analyze the minimum time as well for a given selection of CPUs. Their timeline shows all processes finishing at the same time and does not take into account applications that transfer result data, which our paper does.	central processing unit;scheduling (computing);timeline	Nicholas Comino;V. Lakshmi Narasimhan	2004	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2004.12	data compression;parallel processing;parallel computing;scalability;transform coding;genetic algorithm;computer science;theoretical computer science;operating system;data mining;database;distributed computing;computer security;differential equation;algorithm;statistics	Theory	-10.350096024848034	48.084947885063656	131647
4ed05cd45f236dc598948e8e93a9358c10fa6aa2	feedback control for providing qos in noc based multicores	multiprocessing systems;network-on-chip;quality of service;state feedback;three-term control;noc based multicores;pid controller;qos;formal feedback control theory;global controller architecture;network-on-chip based multicore;proportional integral derivative;virtual channels	In this paper, we employ formal feedback control theory to achieve desired communication throughput across a network-on-chip (NoC) based multicore. When the output of the system needs to follow a certain reference input over time, our controller regulates the system to obtain the desired effect on the output. In this work, targeting a multicore that executes multiple applications simultaneously, we demonstrate how to design and employ a PID (Proportional Integral Derivative) controller to obtain the desired throughput for communications by tuning the weights of the virtual channels of the routers in the NoC. We also propose a global controller architecture that implements policies to handle situations in which the network cannot provide the overlapping communications with sufficient resources or the throughputs of the communications can be enhanced (beyond their specified values) due to the availability of excess resources. Finally, we discuss how our novel control architecture works under different scenarios by presenting experimental results obtained using four embedded applications. These results show how the global controller adjusts the virtual channels weights to achieve the desired throughputs of different communications across the NoC, and as a result, the system output successfully tracks the specified input.	control theory;embedded system;feedback;multi-core processor;network on a chip;offset binary;pid;quality of service;throughput	Akbar Sharifi;Hui Zhao;Mahmut T. Kandemir	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)		pid controller;control engineering;embedded system;electronic engineering;real-time computing;quality of service;computer science;engineering;operating system;network on a chip	EDA	-8.119390762962423	58.70586711269732	131760
164dcd9b2c1437b7fcd8fae43a75c4f742c165d8	a cost estimation model for speculative thread partitioning	graph theory;execution probability flow graph;speculative multithreading;thread level parallelism;cost estimation model;multi threading;probability;instruction sets estimation probability computational modeling parallel processing flow graphs;flow graphs;computational modeling;thread level parallelism speculative multithreading cost estimation model;estimation;speculative multithreading technology;cost estimation;control dependence;speculative thread partitioning;probability graph theory multi threading;parallel processing;execution probability flow graph cost estimation model speculative thread partitioning speculative multithreading technology;instruction sets	Speculative Multithreading (SpMT) technology is an effective mechanism for parallelizing irregular programs which are hard by conventional approaches through allowing multiple threads to execute in the presence of ambiguous data and control dependences while the correctness of the programs is maintained by hardware support. Although speculative parallelization can potentially deliver significant speedup, several overheads associated with this technique can limit these speedups in practice. This paper proposes a novel cost estimation model for speculative thread partitioning which can be used to predict the resulting performance. Based on the analysis of the execution probability flow graph (EPFG) of each procedure, this model tries to divide the program’s execution time into sequential execution time and parallel execution time. Then, the model attempts to predict the theoretical speedup of the partitioned speculative procedures based on the estimation of the combined runtime effects of various overheads. Different from prior heuristics that only qualitatively estimate the benefits of speculative multithreaded execution, this model also produces a quantitative estimate of the speedup in theory. Experimental results show that the prediction accurately reflects the inherent parallelism of the thread partitioning results of the programs. Meanwhile, the predictive speedup also indicate the potential parallel performance of the thread partitioning results and then can assist to provide better guidance for thread partitioning.	automatic parallelization;correctness (computer science);heuristic (computer science);parallel computing;run time (program lifecycle phase);simultaneous multithreading;speculative execution;speculative multithreading;speedup;thread (computing)	Yuancheng Li;Yinliang Zhao;Yuanke Wei;Yanning Du	2010	International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2010.26	parallel processing;estimation;parallel computing;real-time computing;multithreading;computer science;graph theory;instruction set;probability;distributed computing;computational model;speculative multithreading;cost estimate;task parallelism	Arch	-7.354999193507474	49.82763333224478	131767
a3a2add0c48ef0e59d4090de2316f0f7d705723f	a2e: adaptively aggressive energy efficient dvfs scheduling for data intensive applications	energy efficiency;energy conservation;power aware 64 core cluster a2e adaptively aggressive energy efficient dvfs scheduling data intensive applications portability programmability dynamic voltage and frequency scaling high performance applications distributed memory architectures scheduling algorithms load imbalance network latency communication delay disk access stalls energy saving opportunities communication intensive applications energy saving blocks source code level esb boundary dvfs overhead;energy;resource allocation energy conservation memory architecture multiprocessing systems power aware computing processor scheduling;processor scheduling;resource allocation;performance;energy efficiency instruction sets writing energy consumption;power aware computing;adaptive;aggressive;energy consumption;memory architecture;data intensive;memory accesses;writing;disk accesses energy performance dvfs adaptive aggressive speculative data intensive memory accesses;multiprocessing systems;dvfs;speculative;disk accesses;instruction sets	Featured by high portability and programmability, Dynamic Voltage and Frequency Scaling (DVFS) has been widely employed to achieve energy efficiency for high performance applications on distributed-memory architectures nowadays through various scheduling algorithms. Generally, different forms of slack from load imbalance, network latency, communication delay, memory and disk access stalls, etc. are exploited as energy saving opportunities where peak CPU performance is not necessary, with little or limited performance loss. The deployment of DVFS for communication intensive applications is straightforward due to the explicit boundary between Energy Saving Blocks (ESBs) at source code level, while for data (e.g., memory and disk access) intensive applications it is difficult for applying DVFS since ESB boundary is implicit due to mixed types of workloads. We propose an adaptively aggressive DVFS scheduling strategy to achieve energy efficiency for data intensive applications, and further save energy via speculation to mitigate DVFS overhead for imbalanced branches. We implemented and evaluated our approach using five memory and disk access intensive benchmarks with imbalanced branches against another two energy saving approaches. The experimental results indicate an average of 32.6% energy savings were achieved with 6.2% average performance loss compared to the original executions on a power-aware 64-core cluster.	algorithm;benchmark (computing);best, worst and average case;cpu cache;cpu core voltage;central processing unit;data-intensive computing;distributed memory;dynamic frequency scaling;dynamic voltage scaling;image scaling;overhead (computing);scheduling (computing);slack variable;software deployment;software portability	Li Tan;Zizhong Chen;Ziliang Zong;Dong Li;Rong Ge	2013	2013 IEEE 32nd International Performance Computing and Communications Conference (IPCCC)	10.1109/PCCC.2013.6742766	parallel computing;real-time computing;energy;energy conservation;computer hardware;performance;resource allocation;computer science;operating system;adaptive behavior;instruction set;efficient energy use;writing	HPC	-5.369437484166185	53.45288593248632	132038
a803a3014b962810a820d92c1ba275ce79ca3335	partial redundancy in hpc systems with non-uniform node reliabilities		We study the usefulness of partial redundancy in HPC message passing systems where individual node failure distributions are not identical. Prior research works on fault tolerance have generally assumed identical failure distributions for the nodes of the system. In such settings, partial replication has never been shown to outperform the two extremes(full and no-replication) for any significant range of node counts. In this work, we argue that partial redundancy may provide the best performance under the more realistic assumption of non-identical node failure distributions. We provide theoretical results on arranging nodes with different reliability values among replicas such that system reliability is maximized. Moreover, using system reliability to compute MTTI (mean-time-to-interrupt) and expected completion time of a partially replicated system, we numerically determine the optimal partial replication degree. Our results indicate that partial replication can be a more efficient alternative to full replication at system scales where Checkpoint/Restart alone is not sufficient. Keywords—HPC, fault tolerance, resilience, replication, checkpoint.	application checkpointing;failure cause;fault tolerance;job stream;message passing;node (computer science);numerical analysis;transaction processing system	Zaeem Hussain;Taieb Znati;Rami G. Melhem	2018			message passing;redundancy (engineering);parallel computing;fault tolerance;computer science;reliability theory;distributed computing	HPC	-18.786106890241292	49.71724145376381	132051
7d68db3e705446da93c0c8397ea8836713ff1589	integration of resource synchronization and preemption-thresholds into edf-based mixed-criticality scheduling algorithm	mixed criticality systems resource synchronization integration preemption thresholds integration edf based mixed criticality scheduling algorithm earliest deadline first scheduling hardware platform mcs algorithms mcs analysis techniques synchronization mechanisms stack resource protocol mixed criticality srp stack space size schedulability enhancement resource constrained embedded systems schedulability analysis methods;protocols;resource allocation;synchronisation;embedded systems;synchronisation embedded systems protocols resource allocation scheduling;scheduling;artificial intelligence	In mixed-criticality systems, multiple subsystems with different levels of criticality may co-exist on the same hardware platform. Many scheduling algorithms have been proposed to achieve certification at multiple levels of criticality. However, current MCS algorithms and analysis techniques generally assume tasks are independent, i.e., they do not share data that need to be protected with synchronization mechanisms like mutexes or semaphores. In this paper, we address this limitation by presenting an extension to the Stack Resource Protocol (SRP), called Mixed-Criticality-SRP (MC-SRP). Moreover, preemption-threshold scheduling is a well-known technique for reducing stack space size and enhance schedulability in resource-constrained embedded systems.We also present the integration of preemption-thresholds into EDF-based mixed-criticality scheduling (MCS) algorithms, and develop the schedulability analysis methods to such systems.	algorithm;criticality matrix;earliest deadline first scheduling;embedded system;mixed criticality;mutual exclusion;preemption (computing);scheduling (computing);scheduling analysis real-time systems;self-organized criticality;semaphore (programming)	Qingling Zhao;Zonghua Gu;Haibo Zeng	2013	2013 IEEE 19th International Conference on Embedded and Real-Time Computing Systems and Applications	10.1109/RTCSA.2013.6732223	fair-share scheduling;communications protocol;synchronization;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;resource allocation;computer science;operating system;distributed computing;scheduling	Embedded	-9.559426048254558	59.916120747131615	132406
b3449dd9722b3a93238291a532280f8b133690a5	rate of change load balancing in distributed and parallel systems	distributed algorithms;dynamic load balancing;rate of change;resource allocation distributed processing parallel processing distributed algorithms;resource allocation;distributed processing;decision maker;load management microwave integrated circuits throughput runtime multiprocessing systems computer science reactive power resource management operating systems time sharing computer systems;parallel systems;parallel computer;load distribution;load balance;totally distributed algorithm rate of change load balancing parallel systems distributed systems dynamic load balancing parallel computer programs communication requirements efficient load distribution local rate of change observations global absolute load numbers;distributed algorithm;parallel processing	Dynamic Load Balancing is an important system function destined to distribute workload among available processors to improve throughput and/or execution times of parallel computer programs either uniform or non-uniform (jobs whose workload varies at run-time in unpredictable ways). Non-uniform computation and communication requirements may bog down a parallel computer if no efficient load distribution is effected. A novel distributed algorithm for load balancing is proposed and is based on local Rate of Change observations rather than on global absolute load numbers. It is a totally distributed algorithm and requires no centralized trigger and/or decision makers. The strategy is discussed and analysed by means of experimental simulation.	load balancing (computing)	Luís Miguel Campos;Isaac D. Scherson	1999		10.1109/IPPS.1999.760552	distributed algorithm;network load balancing services;parallel computing;real-time computing;computer science;load balancing;distributed computing;parallel algorithm	HPC	-15.907342706235665	59.49870507887702	132502
16cc02a182d934ee75ecdf2e10f762eff78ef023	the effectiveness of threshold-based scheduling policies in boinc projects	first come first serve;berkeley open infrastructure for network computing;availability;processor scheduling;distributed computing;computer networks;computational modeling;large scale simulation;virtual colonoscopy;availability processor scheduling computational modeling computer networks throughput predictive models virtual colonoscopy ip networks distributed computing computer science;ip networks;predictive models;computer science;throughput;discrete event simulation	Several scientific projects use BOINC (Berkeley Open Infrastructure for Network Computing) to perform largescale simulations using volunteers' computers (workers) across the Internet. In general, the scheduling of tasks in BOINC uses a First-Come-First-Serve policy and no attention is paid to workers' past performance, such as whether or not they have tended to perform tasks promptly and correctly. In this paper we use SimBA, a discrete-event Simulator of BOINC Applications, to study new threshold-based scheduling strategies for BOINC projects that use availability and reliability metrics to classify workers and distribute tasks according to this classification. We show that if availability and reliability thresholds are selected properly, then the workers' throughput of valid results increases significantly in BOINC projects.	boinc;charmm;computer;internet;reliability engineering;scheduling (computing);simulation;throughput;universal conductance fluctuations	Trilce Estrada;David A. Flores;Michela Taufer;Patricia J. Teller;Andre Kerstens;David P. Anderson	2006	2006 Second IEEE International Conference on e-Science and Grid Computing (e-Science'06)	10.1109/E-SCIENCE.2006.137	availability;throughput;real-time computing;simulation;computer science;boinc credit system;discrete event simulation;operating system;database;distributed computing;predictive modelling;computational model;computer network	HPC	-17.23470400703179	60.13969616734925	132569
7a33a181a38b62b53942e1c0be7f5c8ed4737f7e	a composable real-time architecture for replicated railway applications	composability;virtualization;certification;triple modular redundancy;scheduling;mixed criticality	Triple-modular-redundant applications are widely used for fault-tolerant safety-critical computation. They have strict timing requirements for correct operation. We present an architecture which provides composability and mixed-criticality to support integration and to ease certification of such safety-critical applications. In this architecture, an additional layer is required for the sharing/partitioning of resources. This potentially jeopardizes the synchronization necessary for the triple-modular-redundant applications.We investigate the effects of different (unsynchronized) scheduling methods for the resource-sharing layer in this architecture and conclude that an out-of-the-box solution, which guarantees the technical separation between applications with fast reaction time requirements is only feasible when executing at most one instance of a triple-modular-redundant application per CPU-core for single and multi-core CPUs. Only when accepting changes in the applications or the applications' synchronization mechanisms, are more flexible solutions with good performance and resource utilization available.	real-time transcription	Stefan Resch;Andreas Steininger;Christoph Scherrer	2015	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2015.04.003	triple modular redundancy;embedded system;parallel computing;real-time computing;virtualization;computer science;operating system;distributed computing;certification;scheduling;computer network	Embedded	-9.23510293812095	58.139739867314304	132683
6a52103d10f538816cf87533f66d49f8581d8c49	umr: a multi-round algorithm for scheduling divisible workloads	analytical models;distributed algorithms;application software;umr;processor scheduling;homogeneous platforms;simulation;parallel architectures processor scheduling distributed algorithms;heterogeneous platforms;approximately optimal number;parallel divisible workload applications;scheduling algorithm processor scheduling algorithm design and analysis partitioning algorithms computer science supercomputers application software computational modeling analytical models read write memory;scheduling algorithm;computational modeling;parallel architectures;scheduling;computer science;multi round algorithm;read write memory;simulation umr multi round algorithm scheduling parallel divisible workload applications approximately optimal number realistic platform models homogeneous platforms heterogeneous platforms;realistic platform models;algorithm design and analysis;supercomputers;partitioning algorithms	In this paper we present an algorithm for scheduling parallel applications that consist of a divisible workload. Ou r algorithm uses multiple rounds to overlap communication and computation between a master and several workers. Multi-round scheduling has been used for divisible workloads in previous work and our contribution is as follows. We use “uniform” rounds, i.e. a fixed amount of work is sent out to all workers at each round. This restriction makes it possible to compute an approximately optimal number of rounds, which was not possible for previously proposed algorithms. In addition, we use more realistic platform models than those used in previous works. We provide an analysis of our algorithm both for homogeneous and heterogeneous platforms and present simulation results to quantify the benefits of our approach.	central processing unit;computation;distributed computing;experiment;job shop scheduling;makespan;performance prediction;pollard's rho algorithm for logarithms;scheduling (computing);simulation	Yang Yang;Henri Casanova	2003		10.1109/IPDPS.2003.1213101	distributed algorithm;parallel computing;real-time computing;computer science;operating system;distributed computing;scheduling	Theory	-13.55486856441362	60.404677668776685	132753
1d901f01371dcfc6e2307fcfa72a850844d35749	cftl: a convertible flash translation layer adaptive to data access patterns	nand flash memory;flash memory;cftl;flash translation layer;ftl;data access	The flash translation layer (FTL) is a software/hardware in terface inside NAND flash memory. Since FTL has a critical impact on the performance of NAND flash-based devices, a variety of FTL schemes have been proposed to improve their performance. In this paper, we propose a novel hybrid FTL scheme named Convertible Flash Translation Layer (CFTL). Unlike other existing FTLs using static address mapping schemes, CFTL is adaptive to data access patterns so that it can dynamically switch its mapping scheme to either a read-optimized or a write-optimized mapping scheme. In addition to this convertible scheme, we propose an efficient caching strategy to further improve the CFTL performance with only a simple hint. Consequently, both the convertible feature and the caching strategy empower CFTL to achieve good read performance as well as good write performance.	cache (computing);data access;experiment;ftl: faster than light;flash file system;flash memory controller;random access;randomness	Dongchul Park;Biplob K. Debnath;David Hung-Chang Du	2010		10.1145/1811039.1811089	flash file system;data access;parallel computing;computer hardware;computer science;operating system	Embedded	-11.489989479537009	54.184172271783716	132766
ccaf8e9bcb1d16ddf6802d0451f2dbf8df06a3d3	the orek real-time micro kernel for fpga-based systems-on-chip	heterogeneous task sets;kernel;embedded processor architectures orek real time micro kernel fpga systems on chip heterogeneous task sets predictable shared resources synchronization task based interrupt servicing runtime policing;real time systems kernel processor scheduling timing field programmable gate arrays control systems resource management switches runtime operating systems;real time;design flow;embedded processor architectures;multimedia application;fpga;system on a chip;synchronisation;embedded systems;task based interrupt servicing;system on chip;memory architecture;synchronization;distributed shared memory systems;predictable shared resources synchronization;interrupts;runtime policing;field programmable gate arrays;systems on chip;embedded processor;orek real time micro kernel;system on chip distributed shared memory systems embedded systems field programmable gate arrays interrupts memory architecture synchronisation;hardware;real time systems;timing;time constraint	This paper presents the features, architecture, application design flow and evaluation of the OReK real-time micro kernel, for integration on FPGA-based SoCs for applications running on embedded softcore or hardwired processors. The features of the OReK kernel include the combined support for heterogeneous task sets, predictable shared resources synchronization, task-based interrupt servicing and runtime policing of the timing constraints, making it appropriate to manage multimedia application tasks with different types of timing properties and requirements. OReK is compact, highly configurable and supported on three distinct FPGA embedded processor architectures with different levels of performance and implementation technologies.	central processing unit;embedded system;field-programmable gate array;kernel (operating system);microkernel;real-time clock;requirement;system on a chip	Nelson Silva;Arnaldo S. R. Oliveira;Ricardo A. T. Santos;Luis Fernando de Almeida	2008	2008 IEEE/ACM/IFIP Workshop on Embedded Systems for Real-Time Multimedia	10.1109/ESTMED.2008.4697000	system on a chip;embedded system;synchronization;parallel computing;real-time computing;computer science;operating system;field-programmable gate array	Embedded	-8.361579475464307	58.40563713856128	132872
ffda941055d04c8348488f8292ad611e3e8cb329	using a segregation measure for the workload characterization of multi-class queuing networks	workload characterization;general and miscellaneous mathematics computing and information science;mathematical logic 990210 supercomputers 1987 1989;performance;network performance;computer networks;computer architecture;algorithms;queuing networks	When a queuing network model of a computer system is constructed, the workload is characterized by several parameters known as the device demands. The demand that every customer places upon every device must be specified. A complete workload characterization of a K device network with N different customers contains N * K parameters. Substantial savings in complexity result if the number of workload parameters is decreased. If, for example, only the average demands on the K devices are used in the workload characterization, the overhead of parameter collection is reduced, and the solution of the queuing network model is simplified. With this approach, however, the multi-class system is represented by a single-class model. A loss of accuracy results. It has been recently demonstrated that the performance of a multi-class network is bounded below by its single-class counterpart model and is bounded above by a simple function based upon the single-class model. In this paper, a new workload characterization technique is proposed which requires: the K average device demands for the single-class counterpart model and a segregation measure, a value which indicates the degree to which different customers tend to utilize different parts of the network. The segregation measure specifies themore » point between the two bounds where the multi-class model's performance lies. This measure is quite intuitive and is simple to calculate. The technique provides an accurate estimate of the performance of a multi-class network. 6 refs., 5 figs., 3 tabs.« less		Lawrence W. Dowdy;Alan T. Krantz;Michael R. Leuze	1990			real-time computing;simulation;computer science;theoretical computer science	Metrics	-13.535217547468896	57.30104244960138	132931
6c4f30386df5a2d8cc7587663d68a9147042561d	lifetime-based memory management for distributed data processing systems	cs dc	In-memory caching of intermediate data and eager combining of data in shuffle buffers have been shown to be very effective in minimizing the re-computation and I/O cost in distributed data processing systems like Spark and Flink. However, it has also been widely reported that these techniques would create a large amount of long-living data objects in the heap, which may quickly saturate the garbage collector, especially when handling a large dataset, and hence would limit the scalability of the system. To eliminate this problem, we propose a lifetime-based memory management framework, which, by automatically analyzing the userdefined functions and data types, obtains the expected lifetime of the data objects, and then allocates and releases memory space accordingly to minimize the garbage collection overhead. In particular, we present Deca, a concrete implementation of our proposal on top of Spark, which transparently decomposes and groups objects with similar lifetimes into byte arrays and releases their space altogether when their lifetimes come to an end. An extensive experimental study using both synthetic and real datasets shows that, in comparing to Spark, Deca is able to 1) reduce the garbage collection time by up to 99.9%, 2) to achieve up to 22.7x speed up in terms of execution time in cases without data spilling and 41.6x speedup in cases with data spilling, and 3) to consume up to 46.6% less memory.	byte;cache (computing);computation;dspace;distributed computing;experiment;garbage collection (computer science);input/output;mathematical optimization;memory management;overhead (computing);run time (program lifecycle phase);scalability;spatial variability;speedup;synthetic intelligence;time complexity	Lu Lu;Xuanhua Shi;Yongluan Zhou;Xiong Zhang;Hai Jin;Cheng Pei;Ligang He;Yuanzhen Geng	2016	PVLDB	10.14778/2994509.2994513	garbage;parallel computing;real-time computing;computer science;data mining;database	DB	-16.27823295220667	54.694866544358234	133321
5af3ac7d88730cd9752127dd16b85a8123da1b20	dynamic load balancing inspired by division of labour in ant colonies	dynamic load balancing;berkeley open infrastructure for network computing;personal computer;division of labour;resource allocation;computer model;ant colony;distributed processing;resource manager;resource management;distributed computing;software systems;resource allocation distributed processing;servers;computational modeling;distributed computing system;distributed computation dynamic load balancing algorithm ant colonies distributed computing systems idle processing time parallel problems equal duration process tasks;resource management bandwidth computational modeling servers distributed computing load management dynamic scheduling;load management;bandwidth;middleware;local area network;dynamic scheduling	Distributed computing systems may be constructed from various commodity computing resources in an organization in order to exploit otherwise idle processing time. When such a system is used to solve embarrassingly parallel problems, the performance of each computing resource in the system becomes a significant factor in overall processing performance. This paper shows that processing performance in a distributed computing system used to process tasks of equal duration declines as individual processing resources diverge with respect to performance. A new dynamic load balancing algorithm based on the division of labour in ant colonies is proposed and its viability tested in terms of the time taken to complete a distributed computation and the bandwidth utilized within the system.	algorithm;ant colony;commodity computing;computation;distributed computing;embarrassingly parallel;load balancing (computing)	Ronald Klazar;Andries Petrus Engelbrecht	2011	2011 IEEE Symposium on Swarm Intelligence	10.1109/SIS.2011.5952568	distributed algorithm;parallel computing;real-time computing;computer science;distributed computing	HPC	-16.182127940961497	59.99420163201609	133340
3a68faf7c52bd72625c0a68b7e3e8fdac167532d	b2p2: a scalable big bioacoustic processing platform	manuals;spectrogram;acoustics;big data;sparks;distributed databases;cloud computing	Many application areas such as biodiversity monitoring and speech recognition produce several gigabytes of audio data per day, think of weeks, months of accumulated data available to analyse. However, current methods and frameworks can only process a few megabytes at a time. Moreover, these methods require a lot of manual processing before they can give relevant results for biodiversity researchers. In this paper, we present a novel scalable framework called B2P2 for handling large audio files and processing them in an efficient manner. The proposed framework can process and handle large audio data that can indicate whether there are sounds of potential interest throughout large audio recordings. We implemented the proposed framework using BigData platforms such as Spark and HDFS. Experimental results demonstrate an increase in speed-up and performance of processing audio files when increasing nodes and data sizes on cloud computing.	apache hadoop;big data;cloud computing;gigabyte;megabyte;spark;scalability;speech recognition	Srikanth Thudumu;Saurabh Kumar Garg;James Montgomery	2016	2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2016.0169	parallel computing;real-time computing;big data;cloud computing;computer science;operating system;spectrogram;data mining;distributed computing;world wide web;computer security	DB	-17.807541395760808	54.8559776484277	133426
73bcdb7e7c042ae80d15be3fe88702bb36fc05cf	modeling interference for apache spark jobs	analytical models;interference;computational modeling;sparks;predictive models;cloud computing;data models	To maximize resource utilization and system throughput, hardware resources are often shared across multiple Apache Spark jobs through virtualization techniques in cloud platforms. However, while the performance of these jobs running in virtualized environment can be negatively affected due to interference caused by resource contention, it is nontrivial to predict the effect of interference on job performance in such settings, which is critical for efficient scheduling of such jobs and performance troubleshooting. To address this challenge, in this paper, we develop analytical models to estimate the effect of interference among multiple Apache Spark jobs running concurrently on job execution time in virtualized cloud environment. We evaluated the accuracy of our models using four real-life applications (e.g., Page rank, K-means, Logistic regression, and Word count) on a 6 node cluster while running up to four jobs concurrently. Our experimental results show that the model can achieve high prediction accuracy, and ranges between 86% to 99% when the number of concurrent jobs are four and all start simultaneously, and ranges between 71% to 99% when the number of concurrent jobs are four and start at different times.	ati radeon r300 series;algorithm;apache spark;interference (communication);job stream;k-means clustering;logistic regression;memory management;pagerank;real life;resource contention;run time (program lifecycle phase);scheduling (computing);throughput;vii	Kewen Wang;Mohammad Maifi Hasan Khan;Nhan Nguyen;Swapna S. Gokhale	2016	2016 IEEE 9th International Conference on Cloud Computing (CLOUD)	10.1109/CLOUD.2016.0063	data modeling;real-time computing;simulation;cloud computing;computer science;operating system;distributed computing;interference;predictive modelling;computational model	HPC	-17.28696137985385	58.024056158932154	133447
d99df8972ed69e629cfca79261c5fb9c83bb9bb4	towards pervasive and user satisfactory cnn across gpu microarchitectures	frequency modulation;support vector machines;convolution;computer architecture;feature extraction;entropy;real time systems	Accelerating Convolutional Neural Networks (CNNs) on GPUs usually involves two stages: training and inference. Traditionally, this two-stage process is deployed on high-end GPU-equipped servers. Driven by the increase in compute power of desktop and mobile GPUs, there is growing interest in performing inference on various kinds of platforms. In contrast to the requirements of high throughput and accuracy during the training stage, end-users will face diverse requirements related to inference tasks. To address this emerging trend and new requirements, we propose Pervasive CNN (P-CNN), a user satisfaction-aware CNN inference framework. P-CNN is composed of two phases: cross-platform offline compilation and run-time management. Based on users' requirements, offline compilation generates the optimal kernel using architecture-independent techniques, such as adaptive batch size selection and coordinated fine-tuning. The runtime management phase consists of accuracy tuning, execution, and calibration. First, accuracy tuning dynamically identifies the fastest kernels with acceptable accuracy. Next, the run-time kernel scheduler partitions the optimal computing resource for each layer and schedules the GPU thread blocks. If its accuracy is not acceptable to the end-user, the calibration stage selects a slower but more precise kernel to improve the accuracy. Finally, we design a user satisfaction metric for CNNs to evaluate ourPervasive deign. Our evaluation results show P-CNN can provide the best user satisfaction for different inference tasks.	compiler;convolutional neural network;desktop computer;fastest;graphics processing unit;kernel (operating system);online and offline;performance tuning;requirement;scheduling (computing);throughput	Mingcong Song;Yang Hu;Huixiang Chen;Tao Li	2017	2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2017.52	frequency modulation;embedded system;support vector machine;entropy;parallel computing;real-time computing;feature extraction;computer science;theoretical computer science;operating system;convolution;programming language	Arch	-4.871050955490424	53.7884593768006	133479
53ebb3c5ffcae6ed8c6c80a83c18a83313cac341	a dynamic scheduling logic for exploiting multiple functional units in single ship multithreaded architectures	threaded architectures;microprocessor;chip;computer architecture;multithreaded architecture;functional unit;tomasulo s algorithm;dynamic scheduling;multithreading	Abstmct4revious techniques used for out of order execution and speculative execution use modified versions of Tomosulo’s algorithm and re-order buffer. An extension to these algorithms for multi-threading is necessary in order to issue instructions from multiple streams of instructions dynamically and yet keep the consistency of state for the machine. We present an architecture which has a wide instruction issue rate and can issue from multiple threads at a time. This asynchronous architecture has a mechanism to dynamically resolve data dependencies and executes instructions out of order and speculatively without any special help from an optimizing compiler. Instructions from various threads are interleaved simultaneously to dynamically exploit ILP to the maximum that the architecture can provide. Consistency of state is maintained by precise interrupts and in-order commitment of instructions for all the threads in execution. cute concurrently and parallely on multiple logical pipelines, it allows User and Kernel threads to execute at the same time. Hence we named this architecture the Kernel-User Multithreaded Architecture (KUMA).	algorithm;data dependency;interrupt;multithreading (computer architecture);optimizing compiler;out-of-order execution;pipeline (computing);re-order buffer;scheduling (computing);speculative execution;thread (computing)	Prasad N. Golla;Eric C. Lin	1999		10.1145/298151.298422	chip;computer architecture;parallel computing;real-time computing;tomasulo algorithm;multithreading;dynamic priority scheduling;computer science	Arch	-8.139741267685118	50.91528987401679	133708
8b919605e6a2bf586ae7f25b9507fbb328e6ee1c	energy management in software-controlled multi-level memory hierarchies	empirical analysis;information extraction;software management;embedded system;embedded systems;energy consumption;data access;memory hierarchy;data transfer;software managed memory;energy management	Performance and energy consumption behavior of embedded applications are increasingly being dependent on their memory usage/access patterns. Focusing on a software-managed, application-specific multi-level memory hierarchy, this paper studies three different memory hierarchy management schemes from both energy and performance angles. The first scheme is pure performance-oriented and tuned for extracting the maximum performance possible from the software-managed multi-level memory hierarchy. The second scheme is built upon the first one but it also reduces leakage by turning-on and off memory modules (i.e., different memory levels) at appropriate program points during execution based on the data access pattern information extracted by the compiler. The last scheme evaluated is oriented towards further reducing leakage energy, as well as dynamic energy, by modifying the data transfer policy (data access pattern) of the performance-oriented scheme. Our empirical analysis indicates that it is possible to reduce leakage consumption of the application-specific multi-level memory hierarchy without seriously impacting its performance, and that one can achieve further savings by modifying data transfer pattern across the different levels of the memory hierarchy.	compiler;dimm;data access;embedded system;memory hierarchy;spectral leakage	Ozcan Ozturk;Mahmut T. Kandemir	2005		10.1145/1057661.1057727	uniform memory access;distributed shared memory;data access;shared memory;embedded system;interleaved memory;parallel computing;real-time computing;thrashing;distributed memory;computer science;physical address;operating system;database;overlay;extended memory;flat memory model;information extraction;computing with memory;memory map;memory management;energy management	HPC	-9.025809085337542	53.60625159572811	133949
d2b4457a2114c838801eb2811b4cc5d955fa3a77	design guidelines of storage class memory/flash hybrid solid-state drive considering system architecture, algorithm and workload characteristic	random access memory;memory management;metadata;nand circuits dram chips flash memories meta data;nand flash only ssd speed storage class memory flash hybrid solid state drive system architecture workload characteristic nand flash memories bandwidth gap dram flash solid storage performance file system metadata physical mapping tables write back nonvolatile memory based cache scm chip latency memory device circuit design data management scm interface capacity requirement;flash memories random access memory nonvolatile memory algorithm design and analysis file systems metadata memory management;data management algorithm storage class memory solid state drive nand flash memory;nonvolatile memory;algorithm design and analysis;flash memories;file systems	Solid-state drives (SSDs), composed of NAND flash memories, are replacing hard disk drives (HDDs) rapidly. In addition, storage class memories (SCMs) bridge the bandwidth gap between DRAM and NAND flash, thus introducing SCM to SSD further improves the solid storage performance. Different from schemes that use SCM to store file system metadata or logical to physical mapping tables, two architectures 1) use SCM as a write-back non-volatile memory (NVM) based cache, 2) use SCM as a storage device are presented in this paper. Since SCM chip latency varies due to memory device and circuit design, three SSD data management algorithms are evaluated under five SCM chip design scenarios to provide useful design guidelines of SCM/NAND flash hybrid SSD. SCM interface and capacity requirement are also analyzed. From the experimental results, less than 10% of the SCM/NAND flash capacity ratio is enough for SCM chips with 500 ns read and 5 μs write latency to boost NAND flash-only SSD speed by over 10 times when workloads own high IO skew1.	algorithm;associative entity;c syntax;cpu cache;cache (computing);circuit design;computer data storage;dynamic random-access memory;flash memory;hard disk drive;hybrid drive;microsoft write;non-volatile memory;requirement;solid-state drive;systems architecture;two-hybrid screening;volatile memory	Chao Sun;Shun Okamoto;Shogo Hachiya;Tomoaki Yamada;Ken Takeuchi	2016	IEEE Transactions on Consumer Electronics	10.1109/TCE.2016.7613193	racetrack memory;flash file system;algorithm design;parallel computing;real-time computing;non-volatile memory;computer hardware;computer science;operating system;flash memory emulator;computer data storage;computer memory;universal memory;non-volatile random-access memory;metadata;memory management	Embedded	-10.88557759055182	54.415450371207044	134151
054e4a4132a3463e820c548035d9abf73ffbda57	a performance comparison of contemporary dram architectures	performance evaluation;global correlation;extended data out;synchronous;performance comparison;presentation;memory system performance performance comparison contemporary dram architectures memory access time processor speed simulation based performance study small system organizations workstation class computers simulations;chip;recursive data structures;memory access;rambus;parallel architectures;load address prediction;random access memory delay bandwidth time measurement pins costs out of order capacitors computer architecture jacobian matrices;enhanced synchronous;memory systems;predictor implementation;direct rambus;synchronous link;dram;memory bandwidth;dram chips;fast page mode;digital simulation parallel architectures performance evaluation dram chips;digital simulation;context based predictor	In response to the growing gap between memory access time and processor speed, DRAM manufacturers have created several new DRAM architectures. This paper presents a simulation-based performance study of a representative group, each evaluated in a small system organization. These small-system organizations correspond to workstation-class computers and use on the order of 10 DRAM chips. The study covers Fast Page Mode, Extended Data Out, Synchronous, Enhanced Synchronous, Synchronous Link, Rambus, and Direct Rambus designs. Our simulations reveal several things: (a) current advanced DRAM technologies are attacking the memory bandwidth problem but not the latency problem; (b) bus transmission speed will soon become a primary factor limiting memory-system performance; (c) the post-L2 address stream still contains significant locality, though it varies from application to application; and (d) as we move to wider buses, row access time becomes more prominent, making it important to investigate techniques to exploit the available locality to decrease access time.	access time;cas latency;clock rate;computer;dynamic random-access memory;emoticon;graph bandwidth;locality of reference;memory bandwidth;simulation;workstation	Vinodh Cuppu;Bruce Jacob;Brian M Davis;Trevor N. Mudge	1999		10.1145/300979.300998	chip;embedded system;computer architecture;parallel computing;real-time computing;memory rank;cas latency;telecommunications;computer science;operating system;synchronous learning;dram;memory bandwidth	Arch	-7.89535816412939	52.701046152182315	134514
206901035b9759574dc2c91669389cd7f6bf3803	an approach to real-time scheduling - but is it really a problem for multimedia?	real time processing;multimedia systems;real time scheduling;priority ceiling protocol	This paper details work done on how to avoid bad behaviour caused by priority inversion and lock blocking in real-time processing in multimedia environments. We will show how they can be cured by more complex algorithms. (1) Priority inheritance protocol, (2) the priority ceiling protocol, (3) the semaphore dependency protocol deal with increasing complex situations. The question is why should there be complex situations? Our suggestion is that there should not be complex situations in multimedia systems, therefore and that simple priority inheritance ought to be good enough.	real-time transcription;scheduling (computing)	Roger M. Needham;Akira Nakamura	1992		10.1007/3-540-57183-3_4	fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;two-level scheduling;deadline-monotonic scheduling;distributed computing;least slack time scheduling;round-robin scheduling;priority ceiling protocol	Embedded	-10.616568355131934	60.23649063276379	134515
777c56442350547523654cef9f76e77bc4143e76	long-latency branches: how much do they matter?	branch prediction;upper bound;performance improvement;data cache;prediction accuracy;high performance	Dynamic branch prediction plays a key role in delivering high performance in the modern microprocessors. The cycles between the prediction of a branch and its execution constitute the branch misprediction penalty because a misprediction can be detected only after the branch executes. Branch misprediction penalty depends not only on the depth of the pipeline, but also on the availability of branch operands. Fetched branches belonging to the dependence chains of loads that miss in the L1 data cache exhibit very high misprediction penalty due to the delay in the execution resulting from unavailability of operands. We call these the long-latency branches. It has been speculated that predicting such branches accurately or identifying such mispredicted branches before they execute would be beneficial. In this paper, we show that in a traditional pipeline the frequency of mispredicted long-latency branches is extremely small. Therefore, predicting all these branches correctly does not offer any performance improvement. Architectures that allow checkpoint-assisted speculative load retirement fetch a large number of branches belonging to the dependence chains of the speculatively retired loads. Accurate prediction of these branches is extremely important for staying on the correct path. We show that even if all the branches belonging to the dependence chains of the loads that miss in the L1 data cache are predicted correctly, only four applications out of twelve control speculation-sensitive applications selected from the SPECInt2000 and BioBench suites exhibit visible performance improvement. This is an upper bound on the achievable performance improvement in these architectures. This article concludes that it may not be worth designing specialized hardware to improve the prediction accuracy of the long-latency branches.	branch misprediction;branch predictor;cpu cache;microprocessor;operand;speculative execution;transaction processing system;unavailability	Abhas Kumar;Nisheet Jain;Mainak Chaudhuri	2006	SIGARCH Computer Architecture News	10.1145/1152394.1152396	embedded system;parallel computing;real-time computing;computer science;operating system;upper and lower bounds;branch misprediction;branch predictor	Arch	-7.639487179421476	51.63756986029536	134696
33ceeeb891405bda29b49b2613a1977843411ba1	competitive snoopy caching	analytical models;optimized production technology;memory management;time measurement;multiprocessor systems;paging strategies;cache memory;additives;artificial neural networks;computational modeling;memory architecture;heuristic algorithms;merging;communication cost;writing;terminology;centralized control;multiprocessing systems;costs computer science broadcasting multiprocessing systems memory architecture cache memory time measurement writing;computer science;broadcasting;on line algorithm;algorithm design and analysis	In a snoopy cache multiprocessor system, each processor has a cache in which it stores blocks of data. Each cache is connected to a bus used to communicate with the other caches and with main memory. For several of the proposed models of snoopy caching, we present new on-line algorithms which decide, for each cache, which blocks to retain and which to drop in order to minimize communication over the bus. We prove that, for any sequence of operations, our algorithms' communication costs are within a constant factor of the minimum required for that sequence; for some of our algorithms we prove that no on-line algorithm has this property with a smaller constant.	bus (computing);cache (computing);computer data storage;multiprocessing;online algorithm;online and offline;snoopy cache	Anna R. Karlin;Mark S. Manasse;Larry Rudolph;Daniel Dominic Sleator	1986	27th Annual Symposium on Foundations of Computer Science (sfcs 1986)	10.1109/SFCS.1986.14	bus sniffing;algorithm design;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;cpu cache;cache;food additive;computer science;cache invalidation;distributed computing;smart cache;programming language;terminology;computational model;writing;cache algorithms;cache pollution;broadcasting;artificial neural network;algorithm;time;memory management	Theory	-12.422700992176141	58.68202861837535	134838
e87aba9c7b1d3fa23deac706691ba2a71b2d76c8	transitive closure: an experimental case study of three multithreaded database algorithms on a shared memory multiprocessor (synopsis)	shared memory;performance evaluation;query processing;limited primary memory transitive closure query execution multithreaded database algorithms shared memory multiprocessor performance evaluation dataflow patterns;shared memory systems;distributed databases;transitive closure;shared memory systems database theory distributed databases parallel algorithms performance evaluation query processing;database theory;computer aided software engineering yarn algorithm design and analysis iterative algorithms partitioning algorithms information systems hardware educational institutions database systems computational complexity;shared memory multiprocessor;parallel algorithms	An experimental performance evaluation of three multithreaded database transitive closure algorithms on a shared memory multiprocessor is described. The algorithms are digerentiated on the basis of the dataflow patterns among the threads, namely no dataflow, pipelined datajlow, and network dataflow. Close to linear speedup is achieved by the no dataflow algorithm when there is suflciently large workhad, and suflcient primary memory for processing. Even when communication is through shared memory, the no dataflow algorithm performs significanrry better than the algorithms with dataflow. When access to each page of memory is shared by multiple threads, the number of threads significantly agect the performance of the algorithms in limited primary memory. When each page is exclusively accessed by a single thread, performance remains constant when the number of threads is varied.	algorithm;computer data storage;dataflow;multiprocessing;performance evaluation;shared memory;speedup;thread (computing);transitive closure;video synopsis	Hélène Young-Myers;Louiqa Raschid	1993		10.1109/PDIS.1993.253085	dataflow architecture;uniform memory access;distributed shared memory;shared memory;memory model;parallel computing;distributed memory;computer science;theoretical computer science;distributed computing;data diffusion machine;memory map;memory management	Arch	-11.378948534215102	50.1062639847283	134883
2678699388c2951c8455d3791b36c12e39d37dc7	exploiting set-level non-uniformity of capacity demand to enhance cmp cooperative caching	capacity demand;cache storage;random access memory;set level non uniformity of capacity demand chip multiprocessors last level cache management cooperative caching;memory management;index bit flipping scheme;off chip memory access;cooperative caching;snug;cmp cooperative caching;radiation detectors;last level cache;resource management;chip multiprocessor;cmps;dynamic spill receive paradigm;data engineering;per set shadow tag array;set level non uniformity of capacity demand;chip;memory access;capacity sharing;interleaved codes;indexes;multiprocessing systems cache storage;monitoring;chip multiprocessors;fine grained nonuniformity;engineering management;indexation;core multiprocessors;last level cache management;cache performance;quad core cmp systems;execution driven simulation;quad core cmp systems capacity demand cmp cooperative caching chip multiprocessors off chip memory access capacity sharing cache performance dynamic spill receive paradigm set level nonuniformity identifier fine grained nonuniformity per set shadow tag array snug index bit flipping scheme;enhanced performance technique;bandwidth;multiprocessing systems;organizations;computer science;coarse grained;magnetic cores;set level nonuniformity identifier;cooperative caching delay random access memory memory management computer science data engineering bandwidth engineering management interleaved codes resource management;benchmark testing	As the Memory Wall remains a bottleneck for Chip Multiprocessors (CMP), the effective management of CMP last level caches becomes of paramount importance in minimizing expensive off-chip memory accesses. For the CMPs with private last level caches, Cooperative Caching (CC) has been proposed to enable capacity sharing among private caches by spilling an evicted block from one cache to another. But this eviction-driven CC does not necessarily promote the cache performance since it implicitly favors the applications full of block evictions regardless of their real capacity demand. The recent Dynamic Spill-Receive (DSR) paradigm improves CC by prioritizing applications with higher benefit from extra capacity in spilling blocks. However, the DSR paradigm only exploits the coarse-grained application-level difference in capacity demand, making it less effective as the non-uniformity exists at a much finer level. This paper (i) highlights the observation of cache set-level non-uniformity of capacity demand, and (ii) presents a novel L2 cache design, named SNUG (Set-level Non-Uniformity identifier and Grouper), to exploit the fine-grained non-uniformity to further enhance the effectiveness of cooperative caching. By utilizing a per-set shadow tag array and saturating counter, SNUG can identify whether a set should either spill or receive blocks; by using an index-bit flipping scheme, SNUG can group peer sets for spilling and receiving in an flexible way, capturing more opportunities for cooperative caching. We evaluate our design through extensive execution-driven simulations on Quad-core CMP systems. Our results show that for 6 classes of workload combinations our SNUG cache can improve the CMP throughput by up to 22.3%, with an average of 13.9% over the baseline configuration, while the state-of-the-art DSR scheme can only achieve an improvement by up to 14.5% and 8.4% on average.	ansi escape code;baseline (configuration management);cpu cache;cache (computing);circuit complexity;computer memory;cooperative mimo;cooperative multitasking;identifier;programming paradigm;random-access memory;simulation;throughput	Dongyuan Zhan;Hong Jiang;Sharad C. Seth	2010	2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS)	10.1109/IPDPS.2010.5470441	chip;database index;benchmark;parallel computing;real-time computing;computer hardware;computer science;organization;resource management;operating system;distributed computing;particle detector;bandwidth;computer network;memory management	Arch	-9.601737752564741	52.39780244749824	134931
b2592ef78055598f70492646e09d0c19ec669028	a coarse-grained pessimistic message logging scheme for improving rollback recovery efficiency		As a common technology for fault tolerance and load balance, rollback-recovery faces the challenges of scalability and inherent variability in those long-running large-scale applications with grids as the computing infrastructure. Among the rollback recovery schemes, pessimistic message logging protocols (PMLPs) and coordinated checkpointing protocols (CCPs) are the most popular in practice. Although PMLPs are good in scalability, their fault-free overhead sometimes is prohibitive. CCPs introduce relatively lower overhead, but they are poor in scalability. This work employs partition strategy and introduces the concept of pessimism grain to rollback recovery, striking a balance between good scalability and acceptable overhead. For a partitioned system, a coarse-grained pessimistic message-logging protocol is proposed to achieve scalability and asynchrony both in fault-free execution and in fault recovery. The impact of pessimism grain on the performance is evaluated theoretically. Experimental results show that the pessimism grain is one of the key configuration parameters to reach a desired performance level.	application checkpointing;asynchronous i/o;fault tolerance;load balancing (computing);overhead (computing);scalability;spatial variability	Jinmin Yang;Kin Fun Li;Dafang Zhang;Jing Cheng	2007	Third IEEE International Symposium on Dependable, Autonomic and Secure Computing (DASC 2007)	10.1109/DASC.2007.19	communications protocol;fault tolerance;parallel computing;real-time computing;computer science;load balancing;distributed computing;grid computing	HPC	-18.833789526623228	49.75177438656464	134976
a3ac155f60aae8ec21d504df3e0bb65cbdd83524	a rtrm proposal for multi/many-core platforms and reconfigurable applications	optimisation;resource allocation;reconfigurable architectures;embedded systems;parallel architectures;resource allocation distributed control embedded systems multiprocessing systems optimisation parallel architectures reconfigurable architectures;multiprocessing systems;optimization resource management computer architecture quality of service availability processor scheduling embedded systems;distributed control;overhead sustainability rtrm framework many core architecture reconfigurable applications multicore architecture high performance computing hpc mobile devices self adaptive systems computational resources run time resource manager framework optimal trade off identification quality of service qos requirements time varying resource availability hierarchical control distributed control design time information exploitation multiobjective optimization strategy portable design modular design open source project numa architecture parallel application execution multiobjective resource partitioning strategy	Emerging multi/many-core architectures, targeting both High Performance Computing (HPC) and mobile devices, increase the interest for self-adaptive systems, where both applications and computational resources could smoothly adapt to the changing of the working conditions. In these scenarios, an efficient Run-Time Resource Manager (RTRM) framework can provide a valuable support to identify the optimal tradeoff between the Quality-of-Service (QoS) requirements of the applications and the time varying resources availability. This paper introduces a new approach to the development of a system-wide RTRM featuring: a) a hierarchical and distributed control, b) the exploitation of design-time information, c) a rich multi-objective optimization strategy and d) a portable and modular design based on a set of tunable policies. The framework is already available as an Open Source project, targeting a NUMA architecture and a new generation multi/many-core research platform. First tests show benefits for the execution of parallel applications, the scalability of the proposed multi-objective resources partitioning strategy, and the sustainability of the overheads introduced by the framework.	adaptive system;computation;computational resource;distributed control system;emoticon;manycore processor;mathematical optimization;mobile device;modular design;multi-objective optimization;quality of service;requirement;run time (program lifecycle phase);scalability;smoothing	Patrick Bellasi;Giuseppe Massari;William Fornaciari	2012	7th International Workshop on Reconfigurable and Communication-Centric Systems-on-Chip (ReCoSoC)	10.1109/ReCoSoC.2012.6322885	embedded system;parallel computing;real-time computing;resource allocation;computer science;operating system;distributed computing	HPC	-6.1232130754151815	57.42434988826618	135092
f8db2bd0d00d91cb6086f371e5a67ff61e0c3511	a blocking bound for nested fifo spin locks	nesting;synchronization;blocking analysis;spin locks;real time systems	Bounding worst-case blocking delays due to lock contention is a fundamental problem in the analysis of multiprocessor real-time systems. However, virtually all fine-grained (i.e., non-asymptotic) analyses published to date make a simplifying (but impractical) assumption: critical sections must not be nested. This paper overcomes this fundamental limitation and presents the first fine-grained blocking bound for nested non-preemptive FIFO spin locks under partitioned fixed-priority scheduling. To this end, a new analysis method is introduced, based on a graph abstraction that reflects all possible resource conflicts and transitive delays.	autosar;best, worst and average case;blocking (computing);computation;critical section;encode;fifo (computing and electronics);fixed-priority pre-emptive scheduling;interaction;lock (computer science);maximal set;multiprocessing;real-time clock;real-time computing;scheduling (computing);semaphore (programming);spinlock;transitive reduction	Alessandro Biondi;Björn B. Brandenburg;Alexander Wieder	2016	2016 IEEE Real-Time Systems Symposium (RTSS)	10.1109/RTSS.2016.036	synchronization;parallel computing;real-time computing;spinlock;computer science;distributed computing	Embedded	-10.105939546409672	60.100823086514566	135324
f4a024633d96f28c306a5216b285ce49828a99b1	paratm: transparent embedding of hardware transactional memory for traditional applications		"""As the many-core processors become more prevalent, the parallelism degree of applications is rapidly increasing. It is well known that multi-thread approaches are an effective solution to improve performance by exploiting multiple cores. However, the synchronization problem that occurs between multiple threads can limit the concurrency and scalability of applications. Hardware transactional memory (HTM) has been studied to simplify the synchronization problem, and Intel adopted transactional synchronization extensions (TSX) for its processors in the year 2012. TSX can dynamically decide and perform instructions as an atomic transaction. In this paper, we evaluate and analyze the performance of TSX. It is expected that the latest technology implementing HTM to cope with synchronization scalability will be a nice solution for handling the high degree of parallelism. We found two major reasons that cause performance degradation and propose a novel approach to address these more effectively based on our analysis. We also introduced a mechanism, named ParaTM, to transparently adopt TSX for existing lock-based applications. By using ParaTM, one can apply TSX features without modification of the code. From our evaluation using a micro-benchmark and real-world applications, we confirmed ParaTM is highly effective for transparency and performance. ParaTM achieved 1.75<inline-formula> <tex-math notation=""""LaTeX"""">$\times$ </tex-math></inline-formula>, 4.76<inline-formula> <tex-math notation=""""LaTeX"""">$\times$ </tex-math></inline-formula>, and 1.53<inline-formula> <tex-math notation=""""LaTeX"""">$\times$ </tex-math></inline-formula> better performance compared to the traditional lock mechanism for LevelDB, RocksDB, and Memcached, respectively."""	atomicity (database systems);benchmark (computing);central processing unit;concurrency (computer science);degree of parallelism;elegant degradation;html;leveldb;manycore processor;memcached;parallel computing;rocksdb;scalability;transactional synchronization extensions;transactional memory	Kangmin Lee;Heeseung Jo	2018	IEEE Access	10.1109/ACCESS.2018.2864969	lock (computer science);parallel computing;atomicity;degree of parallelism;scalability;thread (computing);concurrency;distributed computing;transactional synchronization extensions;computer science;transactional memory	HPC	-14.456128277916331	49.60066625586254	135394
45c5a3d86082a8ea5ecca80655ca2df7ae94e4e1	combining dataflow applications and real-time task sets on multi-core platforms		Future real-time embedded systems will increasingly incorporate mixed application models with timing constraints running on the same multi-core platform. These application models are dataflow applications with timing constraints and traditional real-time applications modelled as independent arbitrary-deadline tasks. These systems require guarantees that all running applications execute satisfying their timing constraints. Also, to be cost-efficient in terms of design, they require efficient mapping strategies that maximize the use of system resources to reduce the overall cost.  This work proposes an approach to integrate mixed application models (dataflow and traditional real-time applications) with timing requirements on the same multi-core platform. It comprises three main algorithms: 1) Slack-Based Merging, 2) Timing Parameter Extraction, and 3) Communication-Aware Mapping. Together, these three algorithms play a part in allowing mapping and scheduling of mixed application models in embedded real-time systems. The complete approach and the three algorithms presented have been validated through proofs and experimental evaluation.	algorithm;cost efficiency;dataflow;embedded system;multi-core processor;real-time clock;real-time computing;real-time transcription;requirement;scheduling (computing);slack variable	Hazem Ismail Abdel Aziz Ali;Benny Akesson;Luís Miguel Pinho	2017		10.1145/3078659.3078671	real-time computing;merge (version control);computer science;parallel computing;multi-core processor;scheduling (computing);dataflow;distributed computing	Embedded	-7.852332106114141	58.84711287118617	135498
602b5eeee57c581c0eda722422760e0931ca0816	extending unix for scalable computing	unix multiprocessing systems;computer architecture concurrent computing packaging hardware application software computer aided instruction supercomputers microprocessors operating systems parallel programming;computer architecture;ncube 2 unix scalable computing multicomputer architecture system software;parallel computer;multiprocessing systems;parallel programs;unix	Because it retrieves all instructions and data from a single memory, the von Neumann computer architecture has a fundamental speed limit. The scalable multicomputer architecture, which uses many microprocessors together to solve a single problem and can run at teraflop speeds, may be a solution. While teraflop processor technology is known, the scalable operating and I/O system technology necessary for those speeds are not known. The authors describe how Unix can be extended to scalable computing, permitting teraflop speeds and offering parallel computing to users unfamiliar with parallel programming. They designed this technology into the system software of the Ncube-2, the predecessor to Ncube's announced teraflop parallel computer. The authors describe the system in detail and provide some performance results.<<ETX>>	computer architecture;flops;input/output;microprocessor;parallel computing;processor technology;scalability;unix;von neumann architecture	Erik DeBenedictis;Stephen C. Johnson	1993	Computer	10.1109/2.241425	unix architecture;computer architecture;parallel computing;computer science;operating system;unix filesystem;unix;programming language	Arch	-11.534980828601588	47.4887513343515	135572
171450cd7ed50d4c50955949c87df08bbb115549	collective i/o tuning using analytical and machine learning models	model based tuning;analytical models;collective i o tuning black box models storage operations communication operations hybrid model architecture performance modeling i o autotuning software stacks hardware stacks shared storage systems performance variability storage hierarchy parallel i o optimization machine learning models analytical models;interference;computer architecture;computational modeling;shared memory systems learning artificial intelligence optimisation parallel processing;tuning;statistical and analytical performance models;model based tuning i o performance modeling statistical and analytical performance models;i o performance modeling;predictive models;analytical models computational modeling optimization tuning predictive models computer architecture interference;optimization	The optimization of parallel I/O has become challenging because of the increasing storage hierarchy, performance variability of shared storage systems, and the number of factors in the hardware and software stacks that impact performance. In this paper, we perform an in-depth study of the complexity involved in I/O autotuning and performance modeling, including the architecture, software stack, and noise. We propose a novel hybrid model combining analytical models for communication and storage operations and black-box models for the performance of the individual operations. The experimental results show that the hybrid approach performs significantly better and shows a higher robustness to noise than state-of-the-art machine learning approaches, at the cost of a higher modeling complexity.	black box;heart rate variability;input/output;machine learning;mathematical optimization;memory hierarchy;parallel i/o;profiling (computer programming)	Florin Isaila;Prasanna Balaprakash;Stefan M. Wild;Dries Kimpe;Robert Latham;Robert B. Ross;Paul D. Hovland	2015	2015 IEEE International Conference on Cluster Computing	10.1109/CLUSTER.2015.29	simulation;computer science;theoretical computer science;machine learning;interference;predictive modelling;computational model	HPC	-8.517316753033091	48.405070465145236	135752
49a2c40631f90c0894cab24d7b5940f4e70975d5	ssdup: a traffic-aware ssd burst buffer for hpc systems		Many high performance computing (HPC) applications are highly data intensive. Current HPC storage systems still use hard disk drives (HDDs) as their dominant storage devices, which suffer from disk head thrashing when accessing random data. New storage devices such as solid state drives (SSDs), which can handle random data access much more efficiently, have been widely deployed as the buffer to HDDs in many production HPC systems. Burst buffer has also been proposed to manage the SSD buffering of bursty write requests. Although burst buffer can improve I/O performance in many cases, we find that it has some limitations such as requiring large SSD capacity and harmonious overlapping between computation phase and data flushing stage.  In this paper, we propose a scheme, called SSDUP (a traffic-aware SSD burst buffer), to improve the burst buffer by addressing the above limitations. In order to reduce the SSD capacity demand, we develop a novel traffic-detection method to detect the randomness in the write traffic. Based on this method, only the random writes are buffered to SSD and other writes are deemed sequential and propagated to HDDs directly. In order to overcome the difficulty of perfectly overlapping the computation phase and the flushing stage, we propose a pipeline mechanism for the SSD buffer, in which the data buffering and data flushing are performed in pipeline. Finally, in order to further improve the performance of buffering random writes in SSD, we covert the random writes to sequential writes in SSD by storing the data with a log structure. Further, we propose to use the AVL tree structure to store the sequence information of the data. We have implemented a prototype of SSDUP based on the OrangeFS and performed extensive experimental evaluation. The experimental results show that the proposed SSDUP scheme can improve the write performance by more than 50% on average.	avl tree;benchmark (computing);cpu cache;computation;data access;data-intensive computing;hard disk drive;input/output;interoperable object reference;lustre;orangefs;overhead (computing);prototype;randomness;solid-state drive;supercomputer;thrashing (computer science);translation lookaside buffer;tree structure	Xuanhua Shi;Ming Li;Wei Liu;Hai Jin;Chen Yu;Yong P Chen	2017		10.1145/3079079.3079087	solid-state drive;parallel computing;randomness;solid-state;thrashing;real-time computing;write buffer;avl tree;computer science;data access;supercomputer	HPC	-13.013262536715693	53.713907871953026	135875
926e1a8bc363cd9b8797ec66b0c641a69e7f7888	design and performance evaluation of an adaptive cache coherence protocol	cache storage;memory protocols;performance evaluation;system performance;telecommunication traffic;shared memory systems;cache coherence protocol;adaptive systems;discrete event simulation cache storage coherence memory protocols adaptive systems performance evaluation shared memory systems telecommunication traffic;coherence;spatial locality;execution time adaptive cache coherence protocol protocol design performance evaluation shared memory multiprocessor systems local caches processor memory performance gap bus transactions shared data coherency coherency misses data traffic spatial locality false sharing sectored cache block size block migration block invalidation transfer mode event driven simulation cache miss ratio;discrete event simulation;shared memory multiprocessor;parallel processing computer science electrical capacitance tomography delay access protocols system performance degradation	In shared-memory multiprocessor systems, the local caches which are used to tolerate the performance gap between processor and memory cause additional bus transactions to maintain the coherency of shared data. Especially, coherency misses and data traffic due to spatial locality and false sharing have a singificant effect on the system performance. In this approach, an adaptive cache coherence protocol based on the sectored cache is introduced. It determines the size of a block to be migrated or invalited dynamically depending on the transfer mode so that it can exploit the spatial locality and reduce useless data traffic due to false sharing at the same time. This protocol is evaluated via event driven simulation and its results show 58% decrease in the data traffic and 45% decrease in the cache miss ratio. Thus, the adaptive cache coherence protocol provides about 56% improvement in the execution time.	block size (cryptography);cpu cache;cache (computing);cache coherence;digraphs and trigraphs;event-driven programming;false sharing;locality of reference;multiprocessing;performance evaluation;principle of locality;run time (program lifecycle phase);shared memory;simulation	Won-Kee Hong;Nam-Hee Kim;Shin-Dug Kim	1998		10.1109/ICPADS.1998.741017	bus sniffing;uniform memory access;shared memory;cache coherence;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;coherence;cpu cache;cache;msi protocol;computer science;write-once;adaptive system;discrete event simulation;cache invalidation;distributed computing;computer performance;smart cache;mesi protocol;cache algorithms;cache pollution;moesi protocol;mesif protocol;cache-only memory architecture;non-uniform memory access	HPC	-12.21887740860071	49.48246282157821	135938
7fbf8b04b1d945fdb809d9414065c9706c4808eb	lrtm: life-time and reliability-aware task mapping approach for heterogeneous multi-core systems		Technology scaling, increasing number of components in a single chip, and aging effects have brought severe reliability challenges in multi-core platforms. They are more susceptible to faults, both permanent and transient. This paper proposes a Lifetime and Reliability-aware Task Mapping (LRTM) approach to many-core platforms with heterogeneous cores. It tries to confront both transient faults and wear-out failures. Our proposed approach maintains the predefined level of reliability for the task graph in presence of transient faults over the whole lifetime of the system. LRTM uses replication technique with minimum replica overhead, maximum achievable performance, and minimum temperature increase to confront transient faults while increasing the lifetime of the system. Besides, LRTM specifies task migration plans with the minimum overhead using a novel heuristic approach on the occurrence of permanent core failures due to wear-out mechanisms. Task migration scenarios are used during run-time to increase the lifetime of the system while maintaining reliability threshold of the system. Results show the effectiveness of LRTM improves for bigger mesh sizes and higher reliability thresholds. Simulation results obtained from real benchmarks show the proposed approach decreases design-time calculation up to 4371% compared to exhaustive exploration while achieving lifetime negligibly lower than the exhaustive solution (up to 5.83%). LRTM also increases lifetime about 3% compared to other heuristic approaches in the literature.	algorithm;heuristic;image scaling;manycore processor;multi-core processor;online and offline;overhead (computing);simulation;software bug;system migration	Alireza Namazi;Meisam Abdollahi;Saeed Safari;Siamak Mohammadi;Masoud Daneshtalab	2018	2018 11th International Workshop on Network on Chip Architectures (NoCArc)	10.1109/NOCARC.2018.8541223	chip;task analysis;real-time computing;scaling;distributed computing;computer science;multi-core processor;heuristic;graph	HPC	-5.939320825549483	58.02783908306203	135994
8651e7ccfd5b3a7a81430b18008e5bf80fe9ac87	the migrating tasks: an execution model for irregular codes	processing element;distributed memory;parallel applications	We introduce the use of the task migration paradigm for compiling and executing fine grain parallel applications on distributed memory computers. With this model, the execution of each iteration of a parallel loop is a task. When a task needs to access non local data, it migrates on the processing element which owns this data, and its execution resumes on this processor. The parallel data are never moved unlike the owner compute rule paradigm where the data is explicitly sent to the processor using it or in SVM systems where the data is implicitly requested. Our experiments show that good performance on irregular codes can be obtained on a distributed memory computer even when the application exhibits poor memory locality.		Yvon Jégou	1996		10.1007/3-540-61626-8_76	parallel computing;real-time computing;distributed memory;computer science;distributed computing	Theory	-13.738280342009674	46.55160449094663	136012
d8ddf09256538c0a593be23ac3104e3953330fe1	f-sefi: a fine-grained soft error fault injection tool for profiling application vulnerability	virtual machines fault tolerant computing operating systems computers parallel processing program compilers public domain software;high performance computing;high performance computing soft error fault injection resilience vulnerability;vulnerability;hardware benchmark testing virtual machine monitors probes registers virtual machining circuit faults;resilience;fault injection;soft error;data corruption f sefi fine grained soft error fault injection tool profiling application vulnerability high performance computing community exascale computing feature size operating voltage hpc applications software robustness profiling soft errors open source virtual machine hypervisor qemu design constraints application source code compilers operating systems	As the high performance computing (HPC) community continues to push towards exascale computing, resilience remains a serious challenge. With the expected decrease of both feature size and operating voltage, we expect a significant increase in hardware soft errors. HPC applications of today are only affected by soft errors to a small degree but we expect that this will become a more serious issue as HPC systems grow. We propose F-SEFI, a Fine-grained Soft Error Fault Injector, as a tool for profiling software robustness against soft errors. In this paper we utilize soft error injection to mimic the impact of errors on logic circuit behavior. Leveraging the open source virtual machine hypervisor QEMU, F-SEFI enables users to modify emulated machine instructions to introduce soft errors. F-SEFI can control what application, which sub-function, when and how to inject soft errors with different granularities, without interference to other applications that share the same environment. F-SEFI does this without requiring revisions to the application source code, compilers or operating systems. We discuss the design constraints for F-SEFI and the specifics of our implementation. We demonstrate use cases of F-SEFI on several benchmark applications to show how data corruption can propagate to incorrect results.	benchmark (computing);compiler;emulator;fault injection;fault model;hypervisor;interference (communication);logic gate;next-generation network;open-source software;operating system;programmer;robustness (computer science);soft error;supercomputer;virtual machine	Qiang Guan;Nathan DeBardeleben;Sean Blanchard;Song Fu	2014	2014 IEEE 28th International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2014.128	embedded system;parallel computing;real-time computing;soft error;vulnerability;computer science;operating system;distributed computing;psychological resilience;computer network	HPC	-17.478957984344476	49.30454099413143	136053
1b1cef95cfd5c9beda6739d3d5620571f79698b7	enhancing the performance of assisted execution runtime systems through hardware/software techniques	resource partitioning;resource allocation;performance;assisted execution;programming model;performance improvement;exascale;operating system;automatic detection;software transactional memory;runtime system;transactional memory;parallel programs	To meet the expected performance, future exascale systems will require programmers to increase the level of parallelism of their applications. Novel programming models simplify parallel programming at the cost of increasing runtime overheard. Assisted execution models have the potential of reducing this overhead but they generally also reduce processor utilization.  We propose an integrated hardware/software solution that automatically partition hardware resources between application and auxiliary threads. Each system level performs well-defined tasks efficiently: 1) the runtime system is enriched with a mechanism that automatically detects computing power requirements of running threads and drives the hardware actuators; 2) the hardware enforces dynamic resource partitioning; 3) the operating system provides an efficient interface between the runtime system and the hardware resource allocation mechanism. As a test case, we apply this adaptive approach to STM2, an software transactional memory system that implements the assisted execution model.  We evaluate the proposed adaptive solution on an IBM POWER7 system using Eigenbench and STAMP benchmark suite. Results show that our approach performs equal or better than the original STM2 and achieves up to 65% and 86% performance improvement for Eigenbench and STAMP applications, respectively.	basic stamp;benchmark (computing);operating system;overhead (computing);parallel computing;programmer;requirement;runtime system;software transactional memory;test case	Gokcen Kestor;Roberto Gioiosa;Osman S. Unsal;Adrián Cristal;Mateo Valero	2012		10.1145/2304576.2304598	niche differentiation;transactional memory;parallel computing;real-time computing;performance;resource allocation;computer science;operating system;software transactional memory;programming paradigm;programming language	HPC	-7.194691670159447	48.89893769511054	136397
744438f8f1479646dadeaafbc209fcf0e401e668	poster reception - network performance impact of a lightweight linux for cray xt3 compute nodes	network performance;operating system;scientific applications;overlapping communication and computation;performance modeling	This poster describes initial performance results comparing a tuned lightweight Linux environment to the standard Catamount lightweight kernel environment on compute nodes of the Cray XT3 system. We have created a lightweight Linux environment that consumes less memory and outperforms Catamount for the selfish micro-benchmark that measures operating system interference. In spite of this, Catamount significantly outperforms our lightweight Linux environment for all network performance micro-benchmarks. Latency and bandwidth performance are more than 20% worse for Linux and 16-byte allreduce performance is 2.5 times worse, even at small numbers of nodes. These results indicate that even a properly configured and tuned Linux environment can still suffer from performance and scalability issues on a highly balanced platform like the XT3. This poster provides a detailed description of our lightweight Linux environment, shows relevant performance results, and describes the important issues that allow Catamount to achieve superior performance.	benchmark (computing);catamount (operating system);cray xt3;interference (communication);interrupt latency;lightweight kernel operating system;linux;linux;network performance;scalability	Trammell Hudson;Ron Brightwell	2006		10.1145/1188455.1188636	embedded system;parallel computing;real-time computing;computer science;operating system;network performance;supercomputer operating systems	OS	-9.923871094231833	46.654895808727574	136465
