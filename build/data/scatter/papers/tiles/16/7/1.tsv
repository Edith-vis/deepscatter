id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
66216e4ee190daf69fc2c94144d59906ef57324f	cnn: a speaker recognition system using a cascaded neural network	unsupervised learning;system performance;speaker recognition;linear prediction coding;design and implementation;kohonen s self organising map;self organising map;neural network;cepstrum analysis	This work includes the design and implementation of both conventional, and neural network approaches to recognition of the speakers templates which are introduced to the system via a voice master card and preprocessed before extracting the features used in the recognltion. The conclusion is that the system performance in case of neural network is better than that of the conventional one, achieving a smooth degradation when dealing with nolsy patterns and higher performance when dealing with noise-free patterns.		M. Zaki;A. Ghalwash;A. A. Elkouny	1996	Multidim. Syst. Sign. Process.	10.1007/BF02106109	unsupervised learning;speaker recognition;speech recognition;computer science;machine learning;pattern recognition;time delay neural network	Vision	-16.337202649687974	-88.08010979023871	85113
e3541d118a6834ec8df7cfbca0dee68dae0daef5	improve multichannel speech recognition with temporal and spatial information			speech recognition	Yu Zhang;Pengyuan Zhang;Qingwei Zhao	2018	IEICE Transactions		computer science;computer vision;artificial intelligence;spatial analysis;speech recognition	Vision	-13.648789637457467	-87.46100216365255	85120
eb5f166f2586542eb38504fbf5bbdb7ebc510e18	word-conditioned hmm supervectors for speaker recognition	hidden markov model;speaker recognition;support vector machine;indexing terms	We improve upon the current Hidden Markov Model (HMM) techniques for speaker recognition by using the means of Gaussian mixture components of keyword HMM states in a support vector machine (SVM) classifier. We achieve an 11% improvement over the traditional keyword HMM approach on SRE06 for the 8 conversation task, using the original set of keywords. Using an expanded set of keywords, we achieve a 4.3% EER standalone on SRE06, and a 2.6% EER in combination with a word-conditioned phone N-grams system, a GMM-based system, and the traditional keyword HMM system on SRE05+06. The latter result improves on our previous best.	enhanced entity–relationship model;google map maker;grams;hidden markov model;markov chain;n-gram;speaker recognition;support vector machine	Howard Lei;Nikki Mirghafori	2007			artificial intelligence;speech recognition;speaker diarisation;hidden markov model;speaker recognition;support vector machine;pattern recognition;computer science	ML	-17.20094470314784	-89.31203760644811	85153
ab9de53fcbcd3cb966c9ea8a9ffe2dc9be867644	active learning for lf-mmi trained neural networks in asr		This paper investigates how active learning (AL) effects the training of neural network acoustic models based on Latticefree Maximum Mutual Information (LF-MMI) in automatic speech recognition (ASR). To fully exploit the most informative examples from fresh datasets, different data selection criterions based on the heterogeneous neural networks were studied. In particular, we examined the relationship among the transcription cost of human labeling, example informativeness and data selection criterions for active learning. As a comparison, we tried both semi-supervised training (SST) and active learning to improve the acoustic models. Experiments were performed for both the small-scale and large-scale ASR systems. Experimental results suggested that, our AL scheme can benefit much more from the fresh data than the SST in reducing the word error rate (WER). The AL yields 6∼13% relative WER reduction against the baseline trained on a 4000 hours transcribed dataset, by only selecting 1.2K hrs informative utterances for human labeling via active learning.	acoustic cryptanalysis;active learning (machine learning);automated system recovery;automatic system recovery;baseline (configuration management);experiment;mutual information;neural networks;semi-supervised learning;semiconductor industry;speech recognition;transcription (software);weatherstar;word error rate	Yanhua Long;Hong Ye;Yijie Li;Jiaen Liang	2018		10.21437/Interspeech.2018-1162	speech recognition;artificial neural network;active learning;computer science	ML	-18.631812013242826	-88.97666080118191	85169
f613e2fd290b88337264cd2f17e6a3f682bcf285	dance performance evaluation using hidden markov models	performance evaluation;hidden markov models;maximum likelihood linear regression;interactive systems;gesture recognition	We present in this paper a hidden Markov model-based system for real-time gesture recognition and performance evaluation. The system decodes performed gestures and outputs at the end of a recognized gesture, a likelihood value that is transformed into a score. This score is used to evaluate a performance comparing to a reference one. For the learning procedure, a set of relational features has been extracted from high-precision motion capture system and used to train hidden Markov models. At runtime, a low-cost sensor Microsoft Kinect is used to capture a learneru0027s movements. An intermediate step of model adaptation was hence requested to allow recognizing gestures captured by this low-cost sensor. We present one application of this gesture evaluation system in the context of traditional dance basics learning. The estimation of the log-likelihood allows giving a feedback to the learner as a score related to his performance. Copyright © 2016 John Wiley u0026 Sons, Ltd.	hidden markov model;markov chain;performance evaluation	Sohaib Laraba;Joëlle Tilmanne	2016	Journal of Visualization and Computer Animation	10.1002/cav.1715	speech recognition;computer science;machine learning;pattern recognition;gesture recognition;markov model	Visualization	-16.177013417648226	-83.16602168715387	85313
5fd3ed7924505b35f14dbc1bad99155ae97e8655	polyphonic audio matching for score following and intelligent audio editors	symbolic representation	Getting computers to understand and process audio recordings in terms of their musical content is a difficult challenge. We describe a method in which general, polyphonic audio recordings of music can be aligned to symbolic score information in standard MIDI files. Because of the difficulties of polyphonic transcription, we perform matching directly on acoustic features that we extract from MIDI and audio. Polyphonic audio matching can be used for polyphonic score following, building intelligent editors that understand the content of recorded audio, and the analysis of expressive performance.	acoustic cryptanalysis;align (company);audio editing software;audio signal processing;chroma feature;computer;dynamic time warping;image warping;information retrieval;midi;matching (graph theory);performance;real-time computing;transcription (software)	Roger B. Dannenberg;Ning Hu	2003			combinatorics;discrete mathematics;audio mining;pattern recognition	HCI	-6.903329528151867	-92.89753031336058	85318
49e13a1f4f5aba432a39374b5e834a242139320e	speaker diarization of meetings based on large tdoa feature vectors	model combination speaker diarization time delay of arrival features meetings recordings;microphones;nist;model combination;gaussian processes;acoustic information;large tdoa feature vectors;acoustics;speech;time of arrival estimation gaussian processes speaker recognition;gaussian mixture modeling;hidden markov models vectors microphones nist delay speech acoustics;reference channel;speaker recognition;time delay of arrival features;nist rt06 rt07 rt09 evaluation dataset speaker diarization meeting diarization large tdoa feature vectors acoustic information microphones pair reference channel gaussian mixture modeling information bottleneck;hidden markov models;meetings recordings;vectors;nist rt06 rt07 rt09 evaluation dataset;time of arrival estimation;speaker diarization;information bottleneck;microphones pair;meeting diarization	This paper investigates the use of large TDOA feature vectors together with acoustic information in speaker diarization of meetings. TDOAs are obtained by considering all possible microphones pairs and this approach is compared with conventional TDOA features extracted w.r.t. a reference channel. The study is carried using two systems, the first based on Gaussian Mixture Modeling and the second based on the Information Bottleneck approach. Results on NIST RT06/RT07/RT09 evaluation datasets show a large speaker error reduction of 30% relative going from 14.3% to 10.8% for the first and from 12.3% to 8.2% for the second whenever the feature weighting is properly handled. Furthermore results reveal that the IB system is more robust to different number of microphones even when all pairs large TDOA vectors are used thus outperforming the HMM/GMM by 25% relative (8.2% error compared to 10.8%).	acoustic cryptanalysis;feature vector;hidden markov model;microphone;multilateration;speaker diarisation	Deepu Vijayasenan;Fabio Valente	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288838	speaker recognition;speaker diarisation;speech recognition;information bottleneck method;nist;computer science;speech;pattern recognition;gaussian process	Robotics	-13.99268370296972	-90.46578617527763	85614
87a84d59a4434efd9886139aa59dd2a970aff2ea	triphone state-tying via deep canonical correlation analysis		Context-dependent phone models are used in modern speech recognition systems to account for co-articulation effects. Due to the vast number of possible context-dependent phones, statetying is typically used to reduce the number of target classes for acoustic modeling. We propose a novel approach for state-tying which is completely data dependent and requires no domain knowledge. Our method first learns low-dimensional embeddings of context-dependent phones using deep canonical correlation analysis. The learned embeddings capture similarity between triphones and are highly predictable from the acoustics. We then cluster the embeddings and use cluster IDs as tied states. The bottleneck features of a DNN predicting the tied states achieve competitive recognition accuracy on TIMIT.	acoustic cryptanalysis;acoustic model;biconnected component;context-sensitive language;speech recognition;timit;triphone	Weiran Wang;Hao Tang;Karen Livescu	2016		10.21437/Interspeech.2016-1300	speech recognition;canonical correlation;pattern recognition;artificial intelligence;triphone;computer science;tying	ML	-17.512508654014926	-88.26980916339664	85687
2cc417e1486d344ff11d4f093b5888219a672d4f	on the impact of labialization contexts on unit selection speech synthesis	unit selection;speech synthesis;coarticulatory labialization;unit selection coarticulatory labialization speech synthesis;labialization importance coarticulatory labialization contexts unit selection speech synthesis speech unit selection speech unit concatenation speech quality;speech context databases	This paper presents a study on coarticulatory labialization and the significance of its respecting/violation during selection and concatenation of speech units in the unit selection speech synthesis. The aim of this study is to improve the overall speech quality, especially to increase the perceptual inconspicuousness between concatenated units. The labialization importance was verified by two listening tests-for phonetic laymen and specialists. To suppress the influence of other factors, both tests contained utterances with specially selected phones in specific contexts with respected and violated labialization. The preference for items with correct labialization was evident, which confirms the benefit of considering coarticulatory labialization in a unit selection speech synthesis.	concatenation;experiment;netware file system;speech synthesis;synthetic data	Daniel Tihelka;Zdenek Hanzlícek;Pavel Machac;Radek Skarnitzl;Jindrich Matousek	2012	2012 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)	10.1109/ISSPIT.2012.6621284	speech recognition;computer science;speech synthesis	Arch	-10.7070886387169	-85.18976394819012	85704
3601540574c58e284b4d25298ff0d19452d14722	state-space model based labeling of speech signals	phoneme;speech processing;tratamiento palabra;traitement parole;fonema;phonem;backpropagation;etiquetage;etiquetaje;recurrent network;state space method;methode espace etat;labelling;recurrent neural network;reseau neuronal;state space model;red neuronal;dual heuristic programming;metodo espacio estado;neural network	The paper present an application of one method for obtaining derivatives used in training a recurrent neural network that combine the technique of backpropagation through time and dual heuristic programming. The recurrent network was used in contextual causal phoneme recognition for complete (phoneme segmented) training set labeling.		Dusan Krokavec;Anna Filasová	1999		10.1007/3-540-48239-3_47	speech recognition;computer science;artificial intelligence;state-space representation;recurrent neural network;backpropagation;machine learning;speech processing;time delay neural network;artificial neural network	NLP	-15.81839617286083	-87.53156477131438	86216
62929c9ad0a430e0f03f173ae59f1b12b48b1aef	joint estimation of reverberation time and early-to-late reverberation ratio from single-channel speech signals		The reverberation time (RT) and the early-to-late reverberation ratio (ELR) are two key parameters commonly used to characterize acoustic room environments. In contrast to conventional blind estimation methods that process the two parameters separately, we propose a model for joint estimation to predict the RT and the ELR simultaneously from single-channel speech signals from either full-band or sub-band frequency data, which is referred to as joint room parameter estimator (jROPE). An artificial neural network is employed to learn the mapping from acoustic observations to the RT and the ELR classes. Auditory-inspired acoustic features obtained by temporal modulation filtering of the speech time-frequency representations are used as input for the neural network. Based on an in-depth analysis of the dependency between the RT and the ELR, a two-dimensional (RT, ELR) distribution with constrained boundaries is derived, which is then exploited to evaluate four different configurations for jROPE. Experimental results show that-in comparison to the single-task ROPE system which individually estimates the RT or the ELR-jROPE provides improved results for both tasks in various reverberant and (diffuse) noisy environments. Among the four proposed joint types, the one incorporating multi-task learning with shared input and hidden layers yields the best estimation accuracies on average. When encountering extreme reverberant conditions with RTs and ELRs lying beyond the derived (RT, ELR) distribution, the type considering RT and ELR as a joint parameter performs robustly, in particular. From state-of-the-art algorithms that were tested in the acoustic characterization of environments challenge, jROPE achieves comparable results among the best for all individual tasks (RT and ELR estimation from full-band and sub-band signals).		Feifei Xiong;Stefan Goetze;Birger Kollmeier;Bernd T. Meyer	2019	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2018.2877894	estimator;speech recognition;filter (signal processing);computer science;pattern recognition;artificial intelligence;artificial neural network;modulation;reverberation;communication channel;rope	ML	-14.927122849429466	-90.63751015868135	86276
12b0c5e6f972e1ca9b4c266e3eb4ae64890caaca	focus detection by comparison of speech waveforms		For the e cient translation of speech by machine, the word sequence alone is not always su cient to convey the intended meaning. Prosodic information can be lost in the speech recognition process. This paper presents methods by which focus can be detected in the input speech using timing and pitch information. By comparing the prosodic characteristics of an input utterance against pro les generated by components of a speech synthesiser for a default rendition of the same sequence of words, we are able to detect areas in the signal where prominence has been added.	holomatix rendition;speech recognition;speech synthesis	Satoshi Kitagawa;Nick Campbell	1999			speech recognition;waveform;computer science;speech processing	NLP	-12.88903195742402	-84.06845262258746	86480
2557a28afa8ab01e57302c54e4d1b4a1ea45ab8f	improved chirp group delay based algorithms with applications to vocal tract estimation and speech recognition	group delay;asr;phase processing;vocal tract estimation	We pointed out important drawbacks in earlier works in chirp group delay processing.RRCGD method eliminates above shortcomings while retaining the CGDGCIs advantages.RRCGD-ICA gives good results for high pitch signals.Proposed 3-D nasal feature vector discriminates /m/, /n/, and /ng/ well.Comparative ASR results are given for magnitude and phase-based features. In this paper we propose two algorithms for estimating the vocal tract from the Fourier transform phase of a given speech segment. In the first approach, we find the zeros of the z-transform, reflect all outside-unit-circle zeros inside, and then compute the chirp group delay spectrum. This method eliminates many of the drawbacks in Bozkurt's CGDGCI method, and is able to model well the spectral valleys present. In the case of high pitch sounds, the vocal tract estimate in the proposed method is corrupted by source oscillations. In the second approach, by casting the problem within the framework of Independent Component Analysis, we propose a method wherein these effects are considerably suppressed. ASR results on the TIMIT database using features derived from the first method are comparable to those obtained using MFCC features. Further improvement in the recognition accuracy (compared with the baseline MFCC) was obtained by using lattice combining technique, resulting in a Phone Error Rate of 17%. Also, by using our abilities to model spectral valleys well, we propose additional features that are able to distinguish the nasals /m/, /n/, and /ng/, which in turn lead to an increase in their recognition accuracy.	algorithm;chirp;group delay and phase delay;speech recognition;tract (literature)	M. K. Jayesh;C. S. Ramalingam	2016	Speech Communication	10.1016/j.specom.2016.02.003	speech recognition;group delay and phase delay	EDA	-11.466586390263723	-90.83508267268758	86530
394fdfb600672b2ba8ebd2abd65f31c354b26458	changing the topic: how long does it take?	training data;speech processing;system testing;inspection;language model;databases;upper bound;frequency;speech;natural languages	The authors examine the frequency of topic change in Australian court dialogue with a view to automatically changing language models.	language model	Mary O'Kane;P. E. Kenne	1996			natural language processing;speech recognition;computer science;communication	NLP	-16.22680531542705	-84.36425027897104	86664
6ff3bed08c959684a97badbca825e81a17dd561f	data-driven clustering in emotional space for affect recognition using discriminatively trained lstm networks	indexing terms;support vector machine;long short term memory;short term memory;neural net	In today’s affective databases speech turns are often labelled on a continuous scale for emotional dimensions such as valence or arousal to better express the diversity of human affect. However, applications like virtual agents usually map the detected emotional user state to rough classes in order to reduce the multiplicity of emotion dependent system responses. Since these classes often do not optimally reflect emotions that typically occur in a given application, this paper investigates data-driven clustering of emotional space to find class divisions that better match the training data and the area of application. Thereby we consider the Belfast Sensitive Artificial Listener database and TV talkshow data from the VAM corpus. We show that a discriminatively trained Long Short-Term Memory (LSTM) recurrent neural net that explicitly learns clusters in emotional space and additionally models context information outperforms both, Support Vector Machines and a Regression-LSTM net.	artificial neural network;cluster analysis;database;discriminative model;long short-term memory;recurrent neural network;support vector machine	Martin Wöllmer;Florian Eyben;Björn W. Schuller;Ellen Douglas-Cowie;Roddy Cowie	2009			long short term memory;speech recognition;support vector machine;pattern recognition;artificial intelligence;artificial neural network;affect (psychology);cluster analysis;training set;emotion recognition;machine learning;computer science;data-driven	AI	-17.018917894423733	-87.30265002084211	86668
adf30fcde2096fbe4eec4a1cf9cef1ad5d37946b	influence of spectral cues on the perception of pitch height		This study aims to provide direct perceptual evidence of whether listeners integrate spectral cues in pitch-range perception. A force-choice pitch classification experiment with four spectral conditions was conducted to investigate whether spectral cues manipulation can affect pitch-height perception. The results show that the pitch classification function is significantly shifted under different spectral conditions. Listeners generally hear a higher pitch when the spectrum has higher high-frequency energy (i.e. tenser phonation). This study strongly supports the hypothesis that voice quality cues and F0 interact in pitch perception.	pitch (music);stellar classification	Jianjing Kuang;Mark Liberman	2015			acoustics;mathematics;perception	HCI	-9.577747179740955	-82.78283161262935	86784
49175789e273a3f2241c5c6f555e7a46fe7ca285	all for one: feature combination for highly channel-degraded speech activity detection		Speech activity detection (SAD) on channel transmissions is a critical preprocessing task for speech, speaker and language recognition or for further human analysis. This paper presents a feature combination approach to improve SAD on highly channel degraded speech as part of the Defense Advanced Research Projects Agency’s (DARPA) Robust Automatic Transcription of Speech (RATS) program. The key contribution is the feature combination exploration of different novel SAD features based on pitch and spectro-temporal processing and the standard Mel Frequency Cepstral Coefficients (MFCC) acoustic feature. The SAD features are: (1) a GABOR feature representation, followed by a multilayer perceptron (MLP); (2) a feature that combines multiple voicing features and spectral flux measures (Combo); (3) a feature based on subband autocorrelation (SAcC) and MLP postprocessing and (4) a multiband comb-filter F0 (MBCombF0) voicing measure. We present single, pairwise and all feature combinations, show high error reductions from pairwise feature level combination over the MFCC baseline and show that the best performance is achieved by the combination of all features.	acoustic cryptanalysis;activity recognition;autocorrelation;baseline (configuration management);coefficient;comb filter;it baseline protection;medical transcription;mel-frequency cepstrum;memory-level parallelism;multilayer perceptron;pitch (music);preprocessor;spectral flux	Martin Graciarena;Abeer Alwan;Daniel P. W. Ellis;Horacio Franco;Luciana Ferrer;John H. L. Hansen;Adam Janin;Byung Suk Lee;Yun Lei;Vikramjit Mitra;Nelson Morgan;Seyed Omid Sadjadi;T. J. Tsai;Nicolas Scheffer;Lee Ngee Tan;Benjamin Williams	2013			speech recognition;computer science;pattern recognition	NLP	-13.24768257868772	-90.48346293271283	87002
c0d9b6703126c0e8939bac80707645131c94b74b	biologically inspired continuous arabic speech recognition	institutional repository research archive oaister	Despite many years of research into speech recognition systems, there are limited research publications available covering Arabic speech recognition. Although statistical techniques have been the most applied techniques for such classification problems, Neural Networks have also recorded successful results in speech recognition. In this research three different biologically inspired Continuous Arabic Speech Recognition neural network system structures are presented. An Arabic phoneme database (APD) of six male speakers was constructed manually from the King Abdulaziz Arabic Phonetics Database (KAPD). The Mel-Frequency Cepstrum Coefficients (MFCCs) algorithm was used to extract the phoneme features from the speech signals of this database. The normalized dataset was used to train and test three different architectures of Multilayer Perceptron (MLP) neural network identification systems.	algorithm;artificial neural network;auditory processing disorder;mel-frequency cepstrum;memory-level parallelism;multilayer perceptron;speech recognition	Nadia Fathe Hmad;Tony Allen	2012		10.1007/978-1-4471-4739-8_20	natural language processing;speech recognition;engineering;data science	ML	-6.322523671422303	-90.11346110568466	87100
48cd52d669d9543a39acd8912ac3f2240c987d0f	reexamine the sandhi rules and the merging tones in hakka language		This paper tried to probe the merging tones and the application of sandhi rule for unchecked tones (/24/[33] and /33/[33]) in Hailu variety of Hakka language. Generally speaking, the sandhi rule was applied in both the younger (under 30) and the older (above 50) groups. Tone merging phenomenon (/24/[24] merging toward /33/[33]) was not found in the older group. Yet, the merging phenomenon was found in the younger group. The sandhi rule for /24/ and /33/ was not observed in most of the young speakers. It is proposed that the /24/ and /33/ tones were undergoing sound change by the younger generation.	http 404	Shao-Ren Lyu;Ho-hsien Pan	2013			merge (version control);speech recognition;sound change;computer science;sandhi	HCI	-11.005060054944073	-81.09589694712813	87136
37c45bb63695927734502b2aa5c99a78fa960541	speech coding and synthesis using parametric curves	speech coding;parametric curve	Accurate modeling of co-articulation, the contextsensitive merging of the boundaries between allophones in continuous speech, is vital for natural sounding speech synthesis. This paper describes initial research investigating the use of Bézier Curves to form models of co-articulation in human speech. A 12th order, pitch synchronous line spectral pair (LSP) [1] analysis is performed on a corpus of 239 phonetically balanced sentences of English speech. The resulting data are divided to form an inventory of the diphones occurring in the speech database. The trajectory of each line spectral pair parameter through each diphone can then be represented by a single cubic Bézier curve segment, found using the LevenbergMarquardt curve fitting method [2, 3]. Results are presented showing the accuracy of Bézier models of the coarticulation between different types of speech sounds.	automatic sounding;biconnected component;bézier curve;cubic function;curve fitting;line spectral pairs;speech coding;speech synthesis;text corpus	Luis Miguel Teixeira de Jesus;Gavin C. Cawley	1997			diphone;artificial intelligence;coarticulation;speech recognition;speech coding;pattern recognition;vector sum excited linear prediction;computer science;linear predictive coding;curve fitting;harmonic vector excitation coding;speech synthesis	ML	-14.406749038518619	-84.31754341358646	87208
0b9f57b0585e875602c768ce42d87c30a667060a	detection of replay attacks using single frequency filtering cepstral coefficients		Automatic speaker verification systems are more vulnerable to spoofing attacks. Recently, various countermeasures have been developed for detecting high technology attacks such as speech synthesis and voice conversion. However, there is a wide gap in dealing with replay attacks. In this paper, we propose a new feature for replay attack detection based on single frequency filtering (SFF), which provides high temporal and spectral resolution at each instant. Single frequency filtering cepstral coefficients (SFFCC) with Gaussian mixture model classifier is used for the experimentation on the standard BTAS-2016 corpus. The previously reported best result, which is based on constant Q cepstral coefficients (CQCC) has achieved a half total error rate of 0.67 % on this data-set. Our proposed method outperforms the state of the art (CQCC) with a half total error rate of 0.0002 %.	coefficient;mel-frequency cepstrum;mixture model;replay attack;sensor;speaker recognition;speech synthesis;spoofing attack	K. N. R. K. Raju Alluri;Sivanand Achanta;Sudarsana Reddy Kadiri;Suryakanth V. Gangashetty;Anil Kumar Vuppala	2017			replay attack;filter (signal processing);speech recognition;pattern recognition;artificial intelligence;computer science;mel-frequency cepstrum	AI	-11.30415444391391	-91.67851934338228	87218
0aed0a5cecc594748d978cad0802825b38e9bfcf	utilizing glottal source pulse library for generating improved excitation signal for hmm-based speech synthesis	databases;libraries;glottal source pulse library;speech synthesis;hidden markov model;source modeling;hidden markov models libraries speech speech synthesis databases feature extraction power harmonic filters;vocal tract;hmm;speech;pulse library;hmm based speech synthesis;hidden markov models;pulse library speech synthesis hmm source modeling glottal inverse filtering;power harmonic filters;feature extraction;excitation signal;glottal inverse filtering;glottal inverse filtering glottal source pulse library excitation signal hmm based speech synthesis hidden markov model vocal tract filter;speech synthesis feature extraction filtering theory;vocal tract filter;filtering theory	This paper describes a source modeling method for hidden Markov model (HMM) based speech synthesis for improved naturalness. A speech corpus is first decomposed into the glottal source signal and the model of the vocal tract filter using glottal inverse filtering, and parametrized into excitation and spectral features. Additionally, a library of glottal source pulses is extracted from the estimated voice source signal. In the synthesis stage, the excitation signal is generated by selecting appropriate pulses from the library according to the target cost of the excitation features and a concatenation cost between adjacent glottal source pulses. Finally, speech is synthesized by filtering the excitation signal by the vocal tract filter. Experiments show that the naturalness of the synthetic speech is better or equal, and speaker similarity is better, compared to a system using only single glottal source pulse.	concatenation;hidden markov model;inverse filter;markov chain;speaker recognition;speech corpus;speech synthesis;synthetic data;tract (literature)	Tuomo Raitio;Antti Suni;Hannu Pulakka;Martti Vainio;Paavo Alku	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947370	vocal tract;speech recognition;feature extraction;computer science;speech;machine learning;hidden markov model	Robotics	-9.898026618397717	-91.1615279526336	87634
2d14b6cfec69abbb5a05470c69f413bc0f73b5ca	towards realizing sign language-to-speech conversion by combining deep learning and statistical parametric speech synthesis		This paper realizes a sign language-to-speech conversion system to solve the communication problem between healthy people and speech disorders. 30 kinds of different static sign languages are firstly recognized by combining the support vector machine (SVM) with a restricted Boltzmann machine (RBM) based regulation and a feedback fine-tuning of the deep model. The text of sign language is then obtained from the recognition results. A context-dependent label is generated from the recognized text of sign language by a text analyzer. Meanwhile, a hidden Markov model (HMM) based Mandarin-Tibetan bilingual speech synthesis system is developed by using speaker adaptive training. The Mandarin speech or Tibetan speech is then naturally synthesized by using context-dependent label generated from the recognized sign language. Tests show that the static sign language recognition rate of the designed system achieves 93.6 %. Subjective evaluation demonstrates that synthesized speech can get 4.0 of the mean opinion score (MOS).		Xiaochun An;Hongwu Yang;Zhenye Gan	2016		10.1007/978-981-10-2053-7_61	computer science;hidden markov model;support vector machine;mean opinion score;sign language;deep learning;parametric statistics;speech recognition;artificial intelligence;restricted boltzmann machine;speech synthesis	NLP	-17.48255742536094	-87.49662612651606	87655
f9a7b0d0dea8dfaae6c3a4056ca1e3477869b4a3	some properties of formant frequencies of vowels uttered by deaf and hard of hearing children	deafness auditory system speech analysis frequency measurement electric variables measurement muscles;speech analysis;auditory system;frequency measurement;muscle contraction;deafness;electric variables measurement;muscles	Formant frequencies of the five Japanese vowels uttered by 35 deaf and hard of hearing children were measured utilizing sound spectrograph. The distribution of the formant frequencies on the F1-F2 plane for each kind of vowel and the relation between the formant frequencies of the five Japanese vowels of each of the defective children were compared with those of 8 normal children. Based on a muscle contraction model of speech organs, those results were analyzed in terms of the nature of imperfection in articulatory movement.		Shizuo Hiki;Ryuzaemon Kagami	1976		10.1109/ICASSP.1976.1169930	speech recognition	NLP	-8.381277147707271	-84.48482962402774	87662
994ffa83c84ae2a66258648577d0fbf437675db6	earwitness line-ups: effects of speech duration, retention interval and acoustic environment on identification accuracy		An experiment was conducted to investigate the effects of retention interval, exposure duration and acoustic environment on speaker identification accuracy in voice line-ups. In addition, the relation between confidence assessments by participants and test assistant and identification accuracy was explored. A total of 361 participants heard a single target voice in one of four exposure conditions (short or long speech sample, recorded only indoors or indoors and outdoors). Half the participants were tested immediately after exposure to the target voice and half one week later. The results show that the target was correctly identified in 42% of cases. In the targetabsent condition there were 51% false alarms. Acoustic environment did not affect identification accuracy. There was an interaction between speech duration and retention interval in the target-absent condition: after a one-week interval, listeners made fewer false identifications if the speech sample was long. No effects were found when participants were tested immediately. Only the confidence scores of the test assistant had predictive value. Taking the confidence score of the test assistant into account therefore increases the diagnostic value of the line-up.	acoustic cryptanalysis;speaker recognition;time-compressed speech	Jose H. Kerstholt;E. J. M. Jansen;A. G. van Amelsvoort;A. P. A. Broeders	2003			artificial intelligence;speech recognition;pattern recognition;computer science	HCI	-6.636675478793649	-88.02743741264649	87744
d41b39762d1edcf374e256ea5dd6a52a0a72d25c	the application of the wavelet transform for speech processing	speech processing;wavelet transform		speech processing;wavelet transform	Eliathamby Ambikairajah;M. Keane;Liam Kilmartin;Graham Tattersall	1993			constant q transform;stationary wavelet transform;wavelet;speech recognition;second-generation wavelet transform;computer science;discrete wavelet transform;wavelet packet decomposition;speech processing;harmonic wavelet transform	OS	-13.030861800838712	-87.17223443094248	87891
6ac247b1fda7a1a68cfed9fd83ef7afe41370ea5	comparison of hmm and tmdn methods for lip synchronisation	conference paper	This paper presents a comparison between a hidden Markov model (HMM) based method and a novel artificial neural network (ANN) based method for lip synchronisation. Both model types were trained on motion tracking data and a perceptual evaluation was carried out comparing the output of the models, both to each other and to the original tracked data. It was found that the ANN based method was judged significantly better than the HMM based method. Furthermore the original data was not judged significantly better than the output of the ANN method.	artificial neural network;hidden markov model;markov chain;newton's method	Gregor Hofer;Korin Richmond	2010			speech recognition;computer science;artificial intelligence;machine learning	NLP	-15.877090237200251	-83.06849761574681	87972
352329b95fd945d5a9374007cb9b852e068547f5	discrimination of melodic patterns in indian classical music	timbre;standards;pattern classification feature selection information retrieval music;speech;ear;shape;phrase selection melodic pattern discrimination indian classical music music retrieval musically consistent synthetic stimuli raga deshkar;speech shape timbre harmonic analysis standards ear;harmonic analysis	The melodic phrases of a raga are an important cue to its identity. Artists, however, incorporate considerable creative variation within a raga phrase during performance while still preserving its identity in the ears of the listeners. It is of interest therefore to explore the boundaries of this categorization of phrase identity, given the space of musical variations in the tonal interval and duration dimensions. Such an endeavor can help better model musical similarity for music retrieval and pedagogy applications. In this work, we carry out melodic shape manipulations on a selected prominent phrase of raga Deshkar to study the subjective responses of musicians in comparison with non-musicians in terms of perceived discrimination of the controlled variations. A method is presented for deriving musically consistent synthetic stimuli for listening. Subjective responses on the discrimination and identification tasks are presented along with a discussion on possible perceptual mechanisms at play.	categorization;synthetic intelligence	Kaustuv Kanti Ganguli;Preeti Rao	2015	2015 Twenty First National Conference on Communications (NCC)	10.1109/NCC.2015.7084866	psychology;speech recognition;acoustics;communication	Vision	-9.591517635272833	-84.83263904393816	88134
b92406098a7b3cf91299936205cfb6e5fd9ca2e1	discrimination method of synthetic speech using pitch frequency against synthetic speech falsification		We propose discrimination method of synthetic speech using pitch pattern of speech signal. By applying the proposed synthetic speech discrimination system as pre-process before the conventional HMM speaker verification system, we can improve the safety of conventional speaker verification system against imposture using synthetic speech. The proposed method distinguishes between synthetic speech and natural speech according to the pitch pattern which is distribution of value of normalized short-range autocorrelation function. We performed the experiment of user verification, and confirmed the validity of the proposed method. key words: speaker verification, biometrics, synthetic speech, pitch frequency	autocorrelation;biometrics;hidden markov model;natural language;preprocessor;speaker recognition;speech synthesis;synthetic data;synthetic intelligence	Akio Ogihara;Hitoshi Unno;Akira Shiozaki	2005	IEICE Transactions		speech recognition;biometrics	Vision	-10.890967911653124	-91.87474191292822	88155
cdc7ddf38d74542786ec0d5d0e0cfccad044f91d	improvement, evaluation and testing of a low cost multilingual portable speaking aid for the speech impaired				Géza Németh;Gábor Olaszy;Laszlo Pataki;Luis A. Hernández Gómez;Diamantino Freitas	1995			speech recognition;natural language processing;computer science;artificial intelligence	NLP	-15.847774121089396	-85.56893862367896	88244
93a2c97a0ef8e06a89aa455629024577bf8064e1	speech recognition under additive noise	databases;background noise;working environment noise;additive noise;speech enhancement;training data;continuous speech recognition;performance improvement;moving average;noise level;noise reduction;speech recognition additive noise signal processing algorithms background noise working environment noise noise level noise reduction training data databases speech enhancement;speaker dependent;error rate;speech recognition;signal processing algorithms;high performance	In this paper we present a study on the performance of a speaker-dependent continuous speech recognition algorithm in various background noise levels, including the mismatch tolerance of the algorithm. This mismatch exists in most applications where the user is trained in one noise level and does the recognition in different and highly variable noise levels. Finally, we introduce a pre-processing technique called 'twicing' and a simple 3-point moving average post-processor. The twicing algorithm, while still maintaining the high performance in the quiet background, reduces the error rate in the noisy environments by a significant amount. The results indicate that smoothing in addition to twicing provides significant performance improvement in high noise background.	additive white gaussian noise;speech recognition;utility functions on indivisible goods	Chin-Hui Lee;Kalyan Genesan	1984		10.1109/ICASSP.1984.1172752	gradient noise;computer vision;training set;speech recognition;word error rate;computer science;noise measurement;pattern recognition;noise reduction;background noise;moving average	Vision	-14.335053676108506	-91.70943243235253	88286
4019a2743e8a3dbdce883d3f184191cb738c3f6b	on predicting the unpleasantness level of a sound event.		This work presents a novel framework for the automatic assessment of the unpleasantness caused by audio events to a human listener which is a relatively new research problem. Melfrequency cepstral coefficients and temporal modulation parameters were employed to characterize 75 sound stimuli varying from animal calls to baby cries. The final assessment is made by means of a clustering scheme realized by Gaussian mixture models. The proposed framework leads to the best performance in terms of mean squared error and correlation between predicted and measured unpleasantness levels reported so far in the literature.	cepstrum;cluster analysis;coefficient;mean squared error;mixture model;modulation;pattern recognition	Stavros Ntalampiras;Ilyas Potamitis	2014			pattern recognition;artificial intelligence;computer science;speech recognition	ML	-10.453721938844296	-89.11213367432677	88299
2c4ff6a2888dcb4bcd335cfc8cd8eeb6c701e2f5	tone and pitch accent classification using auditory attention cues	mandarin chinese speech database tone accent classification auditory attention cues pitch accent modeling tone accent modeling fine grained pitch accent classification;human performance;computer model;biological system modeling;human auditory system;mandarin chinese;lexical tone;feature extraction;speech recognition;auditory gist pitch accent boundary tone lexical tone tone recognition auditory attention;spoken language processing;classification accuracy;feature extraction accuracy speech recognition speech biological system modeling computational modeling databases	A detailed description of tone and intonation is beneficial for many spoken language processing applications. In traditional methods for tone and pitch accent modeling, prosodic features, such as pitch, energy and duration, have been used. Here, a novel system that uses auditory attention cues is proposed for tone and fine grained pitch accent classification. The auditory attention cues are biologically inspired and hence extracted by mimicking the processing stages in the human auditory system. When tested on the Boston University Radio News Corpus, the proposed method achieves 64.6% pitch accent and 89.7% boundary tone classification accuracy. In addition, it is demonstrated that the model also successfully recognizes lexical tones in Mandarin with 79.0% accuracy when tested on a continuous Mandarin Chinese speech database. The results compare very well to the reported human performance on these tasks.	auditory processing disorder;human reliability;lexicon;pitch (music);super robot monkey team hyperforce go!	Ozlem Kalinli	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947531	computer simulation;natural language processing;human performance technology;speech recognition;mandarin chinese;feature extraction;computer science	Vision	-12.63841147869596	-86.96084033659437	88385
10f42bcf2d7c9189d64746471f2111a2f959acc7	developing and enhancing posterior based speech recognition systems	local likelihood;prior information;speech;prior knowledge;posterior probability;posterior distribution;viterbi decoder;speech recognition;gamma distribution;localized state	Local state or phone posterior probabilities are often investigated as local scores (e.g., hybrid HMM/ANN systems) or as transformed acoustic features (e.g., “Tandem”) to improve speech recognition systems. In this paper, we present initial results towards boosting these approaches by improving posterior estimates, using acoustic context (e.g., as available in the whole utterance), as well as possible prior information (such as topological constraints). In the present work, the enhanced posterior distribution is associated with the “gamma” distribution typically used in standard HMMs training, and estimated from local likelihoods (GMM) or local posteriors (ANN). This approach results in a family of new HMM based systems, where only posterior probabilities are used, while also providing a new, principled, approach towards a hierarchical use/integration of these posteriors, from the frame level up to the phone and word levels, and integrating the appropriate context and prior knowledge in each level. In the present work, we used the resulting posteriors as local scores in a Viterbi decoder. On the OGI Numbers’95 database, this resulted in improved recognition performance, compared to a state-of-the-art hybrid HMM/ANN system.	acoustic cryptanalysis;google map maker;hidden markov model;speech recognition;viterbi decoder	Hamed Ketabdar;Jithendra Vepa;Samy Bengio;Hervé Bourlard	2005			speech recognition;computer science;machine learning;pattern recognition;posterior probability;statistics	NLP	-18.030999211233137	-91.6299982243398	88430
e35001bd42fac65dda1a505341c74e96bb1a7258	acoustic monitoring of the great reed warbler using multiple microphone arrays and robot audition	acoustic monitoring;the great reed warbler;robot audition;microphone arrays;harkbird	This paper reports the results of our field test of HARKBird, a portable system that consists of robot audition, a laptop PC, and omnidirectional microphone arrays. We assessed its localization accuracy to monitor songs of the great reed warbler (Acrocephalus arundinaceus) in time and two-dimensional space by comparing locational and temporal data collected by human observers and HARKBird. Our analysis revealed that stationarity of the singing individual affected the spatial accuracy. Temporally, HARKBird successfully captured the exact song duration in seconds, which cannot be easily achieved by human observers. The data derived from HARKBird suggest that one of the warbler males dominated the sound space. Given the assumption that the cost of the singing activity is represented by song duration in relation to the total recording session, this particular male paid a higher cost of singing, possibly to win the territory of best quality. Overall, this study demonstrated the high potential of HARKBird as an effective alternative to the point count method to survey bird songs in the field.	acoustic cryptanalysis;laptop;microphone;robot;stationary process	Shiho Matsubayashi;Reiji Suzuki;Fumiyuki Saito;Tatsuyoshi Murate;Tomohisa Masuda;Koichi Yamamoto;Ryosuke Kojima;Kazuhiro Nakadai;Hiroshi G. Okuno	2017	JRM	10.20965/jrm.2017.p0224	speech recognition;acoustics;engineering;communication	Robotics	-5.1044417973022576	-83.16313124575338	88488
a0933498582eedfe9eb6e1d7cdcd4ffed2835af9	an articulatory analysis of apical syllables in standard chinese	standards;ultrasonic imaging;tongue ultrasonic imaging production standards shape ear yttrium;ear;shape;yttrium;tongue;speech processing natural language processing;production;jaw opening apical syllables ultrasound images ema tongue;articulatory analysis post alveolar consonant alveolar apical consonant mandarin chinese tongue and jaw movement ema data synchronous ultrasonic data standard chinese apical syllable	In the present paper, the synchronous Ultrasonic and EMA data of two participants were collected to observe the Tongue and Jaw movements during the production of zi and zhi in Mandarin Chinese. Results of data analysis partly verified the viewpoint that the performance of articulators in producing the two apical syllables are different, which in turn suggests that the vowels of the two apical syllables are totally different in nature, with the vowel after z as the voiced extension of that alveolar apical consonant, and the vowel after the post-alveolar consonant zh being a real apical vowel in Standard Chinese.	binary prefix;super robot monkey team hyperforce go!	Yu Chen;Jin Zhang;Yanting Chen;Licheng Liu;Jianwu Dang	2015	2015 International Conference Oriental COCOSDA held jointly with 2015 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)	10.1109/ICSDA.2015.7357877	speech recognition;acoustics;engineering;audiology	Robotics	-11.791286964225447	-82.8111755622256	88759
93676a9f913f00cbdd53def215ee6d987b200e8d	lombard speech impact on perceptual speaker recognition	speaker recognition	It is well known that stress and Lombard effect impact speech production. The goal in this study is to investigate how Lombard effect impacts perceptual speaker recognition. We report results from In-Set/Out-of-Set speaker identification (ID) tasks performed by human subjects with a comparison to automatic algorithms. The main trends show that mismatch in reference and test data causes a significant decrease in speaker ID accuracy. The results also indicate that Lombard speech contributes to higher accuracy for In-Set speaker ID, but interferes with correct detection of Out-of-Set speakers. In addition, it is observed that the mismatched conditions cause a higher false reject rate, and that the matched conditions result in higher false acceptance. We further discuss automated system performance in comparison to human performance. Overall observations suggest that deeper understanding of cognitive factors involved in perceptual speaker ID offers meaningful insights for further development of automatic systems and combined automatic-human based systems.	acceptance testing;algorithm;algorithmic trading;caller id;human reliability;speaker recognition;test data	Ayako Ikeno;John H. L. Hansen	2007			artificial intelligence;speech recognition;lombard effect;speaker recognition;speaker diarisation;pattern recognition;perception;computer science	HCI	-11.141619292587917	-84.96621977558729	88812
f141b115d6570dc661c141c5bbc3bd588133fbe6	acoustic analysis of whispered speech for phoneme and speaker dependency		Whisper is used by speakers in certain circumstances to protect personal information. Due to the differences in production mechanisms between neutral and whispered speech, there are considerable differences between the spectral structure of neutral and whispered speech, such as formant shifts and shifts in spectral slope. This study analyzes the dependency of these differences on speakers and phonemes by applying a Vector Taylor Series (VTS) approximation to a model of the transformation of neutral speech into whispered speech, and estimating the parameters of this model using an Expectation Maximization (EM) algorithm. The results from this study shed light on the speaker and phoneme dependency of the shifts of neutral to whisper speech, and suggest that similarly derived model adaptation or compensation schemes for whisper speech/speaker recognition will be highly speaker dependent.	acoustic cryptanalysis;approximation;expectation–maximization algorithm;personally identifiable information;speaker recognition;spectral slope;vehicle tracking system	Xing Fan;Keith W. Godin;John H. L. Hansen	2011			speech recognition	NLP	-10.9998492844012	-83.26672644108594	88813
169bb61af6c7ae2abe64a1d1ffefe0e2abec45d2	a preliminary study of child vocalization on a parallel corpus of us and shanghainese toddlers		This paper studies various aspects of child vocalization as captured in a newly established parallel corpus of sixteen 18–31 months old US and Shanghainese toddlers. The recordings were acquired in 16-hour sessions during an ‘ordinary’ day in the child’s natural environment and manually labeled. The vocalization characteristics are studied by means of phonotactic and prosodic analysis with emphasis on automatic processing. In the phonotactic domain, a Gaussian mixture model (GMM) tokenizer, a bank of phone recognizers, and formant tracking are used to analyze the movements in the acoustic-phonetic space. In the prosodic domain, pitch patterns, duration, and rhythm are analyzed. Besides strong individual-specific characteristics of the subjects in some of the domains considered, the two language groups show differences in the occupation of the F1–F2 formant space, choice of pitch pattern durations, and consistency in producing complex phonetic patterns.	acoustic cryptanalysis;finite-state machine;lexical analysis;mixture model;parallel text	Hynek Boril;Qian Zhang;Pongtep Angkititrakul;John H. L. Hansen;Dongxin Xu;Jill Gilkerson;Jeffrey A. Richards	2013			speech recognition	Web+IR	-11.66347695838086	-82.7139467198912	88896
3fe92b141bf4ee527c55702d6bfcc8e38a1e0b0a	single-channel speech enhancement in variable noise-level environment	speech enhancement working environment noise background noise speech processing time frequency analysis fuzzy neural networks inference algorithms noise level testing performance evaluation;noise estimation;word boundary detection;filter bank;time frequency;speech segmentation;speech enhancement;time frequency analysis single channel speech enhancement variable noise level environment subtractive type speech enhancement scheme refined time frequency parameter based recurrent self organizing neural fuzzy inference network algorithm rtf based rsonfin algorithm word boundaries detection speech pauses speech segments filter bank noise estimation;smoothing methods;single channel;recurrent network;self organising feature maps;smoothing methods speech enhancement time frequency analysis noise recurrent neural nets self organising feature maps;fuzzy inference;network algorithm;self organization;recurrent neural nets;article;time frequency analysis;noise	This paper discusses the problem of single-channel speech enhancement in variable noise-level environment. Commonly used, singlechannel subtractive-type speech enhancement algorithms always assume that the background noise level is fixed or slowly varying. In fact, the background noise level may vary quickly. This condition usually results in wrong speech/noise detection and wrong speech enhancement process. In order to solve this problem, we propose a new subtractive-type speech enhancement scheme in this paper. This new enhancement scheme uses the RTF (refined time-frequency parameter)-based RSONFIN (recurrent self-organizing neural fuzzy inference network) algorithm we developed previously to detect the word boundaries in the condition of variable background noise level. In addition, a new parameter (MiFre) is proposed to estimate the varying background noise level. Based on this parameter, the noise level information used for subtractive-type speech enhancement can be estimated not only during speech pauses, but also during speech segments. This new subtractive-type enhancement scheme has been tested and found to perform well, not only in variable background noise level condition, but also in fixed background noise level condition.	algorithm;bayesian network;noise (electronics);organizing (structure);self-organization;speech enhancement	Chin-Teng Lin	2003	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/TSMCA.2003.811115	speech recognition;time–frequency analysis;computer science;machine learning;pattern recognition	AI	-12.496133634912056	-92.54322839635907	88964
22505ca2210a39b0c96a74b069cd4a583c7985f6	challenges in adopting speech recognition	tecnologia electronica telecomunicaciones;computacion informatica;grupo de excelencia;ciencias basicas y experimentales;tecnologias	Although progress has been impressive, there are still several hurdles that speech recognition technology must clear before ubiquitous adoption can be realized. R&D in spontaneous and free-flowing speech style is critical to its success.	speech recognition;spontaneous order	Li Deng;Xuedong Huang	2004	Commun. ACM	10.1145/962081.962108	speech recognition	HCI	-17.627078011979172	-83.4658239213732	88977
025d2e8e4985b1a5de47879a32797f9fa1bf4570	onset detection in musical audio signals		This paper presents work on changepoint detection in musical audio signals, focusing on the case where there are note changes with low associated energy variation. Several methods are described and results of the best are presented.	onset (audio);sensor	Stephen W. Hainsworth;Malcolm D. Macleod	2003			musical;acoustics;audio signal;computer science	HCI	-11.573498468879155	-87.08564403824363	89054
bdfe2d1cb82f4b643763086b40c427e891cc9aab	multilingual exemplar-based acoustic model for the nist open kws 2015 evaluation	kernel;neural networks;acoustics;training;hidden markov models acoustics tuning speech recognition neural networks kernel training;speech recognition hidden markov models natural language processing neural nets probability;hidden markov models;tuning;speech recognition;asr system multilingual exemplar based acoustic model nist open kws 2015 evaluation kernel density model emission probability prediction hidden markov model hmm state neural network score tuning module automatic speech recognition	In this paper, we investigate the use of the proposed non-parametric exemplar-based acoustic modeling for the NIST Open Keyword Search 2015 Evaluation. Specifically, kernel-density model is used to replace GMM in HMM/GMM (Hidden Markov Model / Gaussian Mixture Model) or DNN in HMM/DNN (Hidden Markov Model / Deep Neural Network) acoustic model to predict the emission probability of HMM states. To get further improvement, likelihood score generated by the kernel-density model is discriminatively tuned by the score tuning module realized by a neural network. Various configurations for score tuning module have been examined to show that simple neural network with 1 hidden layer is sufficient to fine tune the likelihood score generated by the kernel-density model. With this architecture, our exemplar-based model outperforms the 9-layer-DNN acoustic model significantly for both the speech recognition and keyword search tasks. In addition, our proposed exemplar-based system provides complementary information to other systems and we can further benefit from system combination.	acoustic cryptanalysis;acoustic model;artificial neural network;deep learning;discriminative model;google map maker;hidden markov model;library (computing);markov chain;mixture model;performance tuning;search algorithm;speech recognition	Van Hai Do;Xiong Xiao;Haihua Xu;Chng Eng Siong;Haizhou Li	2015	2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)	10.1109/APSIPA.2015.7415338	speech recognition;computer science;machine learning;pattern recognition;markov model;hidden markov model	AI	-17.855377980212406	-89.42531995413658	89095
3b41e25f7b30b7e674797081a8e0277947231ebb	an empirical investigation of sparse log-linear models for improved dialogue act classification	dialogue act classification;sparse log linear modeling approach svm model nonsparse log linear model rule based baseline phone level asr hypotheses augmenting n best word elastic net ridge net lasso net element wise frequentist shrinkage model automatic speech recognition discriminative model dense generative model improved dialogue act classification;rule based;confusion network;feature space;elastic net;automatic speech recognition;sparsity;local features;speech recognition;hidden markov models speech support vector machines accuracy training computational modeling noise measurement;learning artificial intelligence;discriminative model dialogue act classification sparsity log linear model maximum entropy;maximum entropy;discriminative model;log linear model;speech recognition learning artificial intelligence	Previous work on dialogue act classification have primarily focused on dense generative and discriminative models. However, since the automatic speech recognition (ASR) outputs are often noisy, dense models might generate biased estimates and overfit to the training data. In this paper, we study sparse modeling approaches to improve dialogue act classification, since the sparse models maintain a compact feature space, which is robust to noise. To test this, we investigate various element-wise frequentist shrinkage models such as lasso, ridge, and elastic net, as well as structured sparsity models and a hierarchical sparsity model that embed the dependency structure and interaction among local features. In our experiments on a real-world dataset, when augmenting N-best word and phone level ASR hypotheses with confusion network features, our best sparse log-linear model obtains a relative improvement of 19.7% over a rule-based baseline, a 3.7% significant improvement over a traditional non-sparse log-linear model, and outperforms a state-of-the-art SVM model by 2.2%.	automated system recovery;baseline (configuration management);discriminative model;elastic map;elastic net regularization;experiment;feature vector;lasso;linear model;log-linear model;logic programming;overfitting;sparse matrix;speech recognition	Yun-Nung Chen;William Yang Wang;Alexander I. Rudnicky	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6639287	rule-based system;speech recognition;feature vector;computer science;principle of maximum entropy;machine learning;pattern recognition;sparsity-of-effects principle;elastic net regularization;discriminative model;statistics;log-linear model	NLP	-19.067210132600614	-89.94533117847138	89102
406c1b6294206d334a42dd78b58ff887d30b1abd	superpositional hmm-based intonation synthesis using a functional f0 model	speech processing hidden markov models natural language processing;intonation synthesis;hidden markov models registers speech speech synthesis training correlation frequency synthesizers;making focal prominence;hmm based speech synthesis;prosody intonation synthesis hmm based speech synthesis functional f0 model making focal prominence;functional f0 model;prosody;functional model parameters superpositional hmm based intonation synthesis functional f0 model fundamental frequency hmm based speech synthesis fujisaki model context dependent hmm japanese speech corpus intonation manipulation	This paper addresses intonation synthesis combining statistical and functional approach with manipulation of fundamental frequency (F 0 ) contours in HMM-based speech synthesis. An F 0  contour is represented as a sum of micro, accent, and register components at the logarithmic scale, which is rooted in the Fujisaki model. Separated context-dependent (CD) HMMs are trained for each type of components extracted from a speech corpus based on a functional F 0  model. At the phase of synthesis, CDHMM-generated micro, accent, and register components are superimposed to form F 0  contours for input text. Objective and subjective evaluations are carried out on a Japanese speech corpus. Compared with the conventional approach, this method not only demonstrates the improved performance in naturalness of synthetic speech by achieving better global F 0  behaviors but also shows its flexibility for intonation manipulation through modifying the functional model parameters.	hidden markov model	Jinfu Ni;Yoshinori Shiga;Chiori Hori	2014		10.1109/ISCSLP.2014.6936614	natural language processing;speech recognition;computer science;linguistics;prosody	Logic	-17.17584827299918	-84.43454336009052	89223
2ecf05b4c82e6561bc5aadc1e1a269850fa7cf4a	learning vocal tract variables with multi-task kernels	learning vt variable;kernel;automatic speech recognition accuracy;multitask kernel based method;vocal tract variables;automatic speech recognition robustness;task dynamics application model;acoustic to articulatory inversion;vocal tract;speech;e svr speech inversion technique learning vocal tract variable acoustic to articulatory speech inversion automatic speech recognition robustness automatic speech recognition accuracy multitask kernel based method melfrequency cepstral coefficient mfcc learning vt variable synthetic speech dataset task dynamics application model tada model;acoustic to articulatory speech inversion;speech recognition cepstral analysis;articulatory feature;hilbert space;mel frequency cepstral coefficient;melfrequency cepstral coefficient;automatic speech recognition;matrix valued ker nel;cepstral analysis;matrix valued kernel;machine learning;learning vocal tract variable;synthetic speech dataset;multi task learning;kernel speech hilbert space machine learning speech recognition mel frequency cepstral coefficient;speech recognition;e svr speech inversion technique;acoustic to articulatory inversion multi task learning matrix valued kernel vocal tract variables;tada model;mfcc	The problem of acoustic-to-articulatory speech inversion continues to be a challenging research problem which significantly impacts automatic speech recognition robustness and accuracy. This paper presents a multi-task kernel based method aimed at learning Vocal Tract (VT) variables from the Mel-Frequency Cepstral Coefficients (MFCCs). Unlike usual speech inversion techniques based on individual estimation of each tract variable, the key idea here is to consider all the target variables simultaneously to take advantage of the relationships among them and then improve learning performance. The proposed method is evaluated using synthetic speech dataset and corresponding tract variables created by the TAsk Dynamics Application (TADA) model and compared to the hierarchical ε-SVR speech inversion technique.	acoustic cryptanalysis;coefficient;computer multitasking;kernel (operating system);mel-frequency cepstrum;speech recognition;synthetic intelligence;tract (literature);voice inversion	Hachem Kadri;Emmanuel Duflos;Philippe Preux	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946917	speech recognition;computer science;pattern recognition;mel-frequency cepstrum	Robotics	-16.47097013724764	-92.05234418805556	89367
a7b65223735bb13fcba9a8db13ebede27a948e32	the production of english vowels by fluent early and late italian-english bilinguals	apprentissage precoce;bilingualism;anglais langue maternelle;228210;locuteur non natif;production de la parole;bilinguisme;italien langue maternelle;vol 59;phonetique experimentale;anglais langue seconde;second language acquisition;etude comparative;phonetica 2002;second language;early learning;comparative study;acquisition d une langue seconde;native speaker;voyelle;locuteur natif;native english;vowel;no 1;non native speaker;speech production;psycholinguistique;psycholinguistics;experimental phonetics	The primary aim of this study was to determine if fluent early bilinguals who are highly experienced in their second language (L2) can produce L2 vowels in a way that is indistinguishable from native speakers' vowels. The subjects were native speakers of Italian who began learning English when they immigrated to Canada as children or adults ('early' vs. 'late' bilinguals). The early bilinguals were subdivided into groups differing in amount of continued L1 use (early-low vs. early-high). In experiment 1, native English-speaking listeners rated 11 English vowels for goodness. As expected, the late bilinguals' vowels received significantly lower ratings than the early bilinguals' vowels did. Some of the early-high subjects' vowels received lower ratings than vowels spoken by a group of native English (NE) speakers, whereas none of the early-low subjects' vowels differed from the NE subjects' vowels. Most of the observed differences between the NE and early-high groups were for vowels spoken in a nonword condition. The results of experiment 2 suggested that some of these errors were due to the influence of orthography.	speaking (activity)	Thorsten Piske;James Emil Flege;Ian R. A. MacKay;Diane Meador	2002	Phonetica	10.1159/000056205	psychology;speech production;speech recognition;philosophy;second-language acquisition;comparative research;first language;linguistics;sociology;psycholinguistics;communication;experimental phonetics	NLP	-11.824887197124315	-81.57141711938175	89370
bfe57fc6ad68f46c695a8904013c5c18957760e9	prediction of vowel systems using a deductive approach	acoustic dispersion;inference mechanisms;pattern classification;prediction theory;speech;speech processing;acoustic tube;deductive approach;maximum acoustic dispersion criterion;phonology;vocalic systems;vowel classification;vowel system prediction	A deductive approach is developed to predict vocalic systems. First, vowels are proposed from an efficient and simple use of an acoustic tube. Then, a maximum acoustic dispersion criterion is applied to classify the obtained vowels.	acoustic cryptanalysis;williams tube	René Carré	1996			natural language processing;speech recognition;acoustics;computer science	Robotics	-12.889125403747464	-86.2415535822909	89371
86cf25fb5f4a025091ec0a07329359ca134523ea	analysis and selection of prosodic features for language identification	quantization;prosodic lid systems;normalization methhod;nist;phonotactic approach;training;speech;prosodic feature extraction prosodic feature selection language identification task speech recognition normalization methhod prosodic lid systems system fusion phonotactic approach;data mining;prosodic feature extraction;feature extraction;natural languages feature extraction speech analysis mutual information information analysis speech recognition support vector machines support vector machine classification performance analysis frequency;prosodic feature selection;feature analysis prosody mutual information language identification;language identification;speech recognition;mutual information;language identification task;feature selection;correlation;feature analysis;prosody;natural language processing;system fusion;speech recognition feature extraction natural language processing	Prosodic features are relatively simple in their structures and are believed to be effective in some speech recognition tasks. However, this kind of features is subject to undesirable bias factors, such as speaking styles. To cope with this, researchers have suggested various normalization and measure methods to the features, which makes the feature inventory very large. In this paper, we use a mutual information criterion to analyze and select a number of prosody-related features in a language identification (LID) task. Among twelve optimal features, eight of them are elaborated in this paper. The feature analysis metric, z-score, is shown to have a moderate to high correlation with LID accuracies. Feature selection proposed in this paper brings about the best performance among all prosodic LID systems to our knowledge. A further attempt in system fusion shows a 13% relative improvement the prosodic LID system brings to the conventional phonotactic approach to LID.	feature selection;language identification;mutual information;programming paradigm;replication (computing);semantic prosody;speech recognition	Raymond W. M. Ng;Tan Lee;Cheung-Chi Leung;Bin Ma;Haizhou Li	2009	2009 International Conference on Asian Language Processing	10.1109/IALP.2009.34	pattern recognition;natural language processing;language identification;speech recognition;nist;quantization;feature extraction;computer science;speech;pattern recognition;linguistics;prosody;mutual information;feature selection;correlation	NLP	-15.747399768209371	-91.36984121715092	89449
a0f9c057b7547e9c86141a5a07fcfa8302620881	enhanced video handling based on audio analysis	speech processing indexing multimedia computing data analysis music image segmentation audio visual systems;audio visual systems;image segmentation;frequency analysis;speech processing;video segmentation;multimedia computing;video indexing;data analysis;indexing;detection rate;indexing speech analysis data mining motion pictures feature extraction layout humans laboratories telegraphy telephony;audio analysis;detection rates enhanced video handling audio analysis video soundtracks content based information video indexing frequency analysis music voice video in time drama movies video segments time;music	Soundtracks of videos contain a rich source of content-based information. In this paper, we propose an audio-based approach to video indexing and handling. Audio data is analysed by means of frequency analysis, and music and voice are independently detected even if they occur together. The method is implemented on a system called Video in Time as an example of creating reasonable condensed versions of dramas or movies by excerpting meaningful video segments. Users can select the desired replaying time from several different levels, depending on how much time can be afforded for viewing. Detection rates for music and voice are evaluated and experiences with the system are mentioned.	sound card	Kenichi Minami;Akihito Akutsu;Hiroshi Hamada;Yoshinobu Tonomura	1997		10.1109/MMCS.1997.609596	video compression picture types;computer vision;audio mining;speech recognition;video;computer science;video quality;video capture;video tracking;music;speech processing;multimedia;video processing;image segmentation;smacker video;frequency analysis;data analysis;video denoising;statistics;multiview video coding	EDA	-5.964427429325226	-94.06960317627255	89475
76be035614fc3128124835e5cb5f0add8f1ee7c1	estimation of severity of speech disability through speech envelope	signal sampling	In this paper, envelope detection of speech is discussed to distinguish the pathological cases of speech disabled children. The speech signal samples of children of age between five to eight years are considered for the present study. These speech signals are digitized and are used to determine the speech envelope. The envelope is subjected to ratio mean analysis to estimate the disability. This analysis is conducted on ten speech signal samples which are related to both place of articulation and manner of articulation. Overall speech disability of a pathological subject is estimated based on the results of above analysis.	biconnected component;envelope detector	Anandthirtha B. Gudi;H. K. Shreedhar;H. C. Nagaraj	2011	CoRR	10.5121/sipij.2011.2203	psychology;speech recognition;communication;audiology	NLP	-6.36651351050327	-85.6048965426752	89853
6433fbb99abd4dce087235f2fb86ddcc89a450f7	on using intrinsic spectral analysis for low-resource languages	psi_speech;isa;manifold learning;presentation;automatic speech recognition;low resource languages;low resource speech recognition;intrinsic spectral analysis	This paper demonstrates the application of Intrinsic Spectral Analysis (ISA) for low-resource Automatic Speech Recognition (ASR). State-of-the-art speech recognition systems that require large amounts of task specific training data fail to reliably model feature distributions in resource impoverished settings. We address this issue by approaching the problem in the front-end, where we can learn an intrinsic subspace that can replace the traditional feature space like mel frequency cepstral coefficients (MFCC). We use ISA features for underresourced settings to model the acoustic feature distribution with less complexity. We also propose to combine intrinsic features with extrinsic ones to take advantage of both subspaces. Experimental results for a phone recognition task on the Afrikaans language show that a combination of the intrinsic subspace and extrinsic subspaces provides us with improved performance compared to conventional features.	acoustic cryptanalysis;coefficient;feature vector;mel-frequency cepstrum;spectral density estimation;speech recognition	Reza Sahraeian;Dirk Van Compernolle;Febe de Wet	2014			speech recognition;computer science;pattern recognition;communication	NLP	-15.758633364773733	-90.93832214306282	90056
8ee8d465e87846b2158b2273945662942a143f82	feature selection algorithms for the generation of multiple classifier systems and their application to handwritten word recognition	classifier combination;classifier ensemble;handwriting recognition;hidden markov model;ensemble creation method;pattern recognition;hidden markov model hmm;handwritten word recognition;feature selection;multiple classifier system	The study of multiple classifier systems has become an area of intensive research in pattern recognition recently. Also in handwriting recognition, systems combining several classifiers have been investigated. In this paper new methods for the creation of classifier ensembles based on feature selection algorithms are introduced. Those new methods are evaluated and compared to existing approaches in the context of handwritten word recognition, using a hidden Markov model recognizer as basic classifier. 2004 Published by Elsevier B.V.	best, worst and average case;ensemble learning;experiment;feature selection;finite-state machine;handwriting recognition;hidden markov model;identity management;information management;markov chain;multimodal interaction;pattern recognition;random subspace method;scheme;search algorithm;statistical classification	Simon Günter;Horst Bunke	2004	Pattern Recognition Letters	10.1016/j.patrec.2004.05.002	margin classifier;speech recognition;feature;quadratic classifier;intelligent character recognition;computer science;intelligent word recognition;machine learning;pattern recognition;handwriting recognition;feature selection;hidden markov model	AI	-4.84403535019538	-89.5742403307454	90090
0024a489de48f5b545e8ef34f86ff20eacc7e801	semi-supervised speaker identification under covariate shift	speaker identification;kernel logistic regression;semi supervised learning;covariate shift;importance estimation	In this paper, we propose a novel semi-supervised speaker identification method that can alleviate the influence of non-stationarity such as session dependent variation, the recording environment change, and physical conditions/emotions. We assume that the voice quality variants follow the covariate shift model, where only the voice feature distribution changes in the training and test phases. Our method consists of weighted versions of kernel logistic regression and cross validation and is theoretically shown to have the capability of alleviating the influence of covariate shift. We experimentally show through text-independent/dependent speaker identification simulations that the proposed method is promising in dealing with variations in voice quality.	control theory;cross-validation (statistics);expect;experiment;kullback–leibler divergence;logistic regression;machine learning;mixture model;multiclass classification;semi-supervised learning;semiconductor industry;simulation;speaker recognition;stationary process;supervised learning	Makoto Yamada;Masashi Sugiyama;Tomoko Matsui	2010	Signal Processing	10.1016/j.sigpro.2009.06.001	semi-supervised learning;speech recognition;computer science;machine learning;pattern recognition;statistics	AI	-16.076603635009178	-91.63373400186194	90102
2f1a8f669489e7cf687e98f5cb6b1c8455780431	enhanced vertical perception through head-related impulse response customization based on pinna response tuning in the median plane	tecnologia electronica telecomunicaciones;hrtf customization;normal hearing;interaural time difference;head related transfer function;pinna response tuning;principal components analysis;principal component analysis;virtual auditory display;impulse response;time domain;tecnologias;grupo a;hrir	Human’s ability to perceive elevation of a sound and distinguish whether a sound is coming from the front or rear strongly depends on the monaural spectral features of the pinnae. In order to realize an effective virtual auditory display by HRTF (head-related transfer function) customization, the pinna responses were isolated from the median HRIRs (head-related impulse responses) of 45 individual HRIRs in the CIPIC HRTF database and modeled as linear combinations of 4 or 5 basic temporal shapes (basis functions) per each elevation on the median plane by PCA (principal components analysis) in the time domain. By tuning the weight of each basis function computed for a specific height to replace the pinna response in the KEMAR HRIR at the same height with the resulting customized pinna response and listening to the filtered stimuli over headphones, 4 individuals with normal hearing sensitivity were able to create a set of HRIRs that outperformed the KEMAR HRIRs in producing vertical effects with reduced front/back ambiguity in the median plane. Since the monaural spectral features of the pinnae are almost independent of azimuthal variation of the source direction, similar vertical effects could also be generated at different azimuthal directions simply by varying the ITD (interaural time difference) according to the direction as well as the size of each individual’s own head. key words: HRTF customization, HRIR, pinna response tuning, principal components analysis	auditory display;basis function;head-related transfer function;headphones;principal component analysis	Ki Hoon Shin;Youngjin Park	2008	IEICE Transactions	10.1093/ietfec/e91-a.1.345	speech recognition;computer science;machine learning;principal component analysis	Visualization	-9.172170771505883	-86.2369813037272	90279
1374a98b836e787b6a753aa1b2ac158b165af449	multi-style prosodic model for french text-to-speech synthesis			speech synthesis	Valerie Pasdeloup	1990			speech recognition;speech synthesis;computer science	Logic	-15.215322257821489	-85.5335300523236	90615
0b82f7f8006d4765fd879e99e8cd4f6980110eeb	recognising 'real-life' speech with spem: a speech-based computational model of human speech recognition	human speech recognition;computer model;word recognition;article in monograph or in proceedings	In this paper, we present a novel computational model of human speech recognition called SpeM based on the theory underlying Shortlist. We will show that SpeM, in combination with an automatic phone recogniser (APR), is able to simulate the human speech recognition process from the acoustic signal to the ultimate recognition of words. This joint model takes an acoustic speech file as input and calculates the activation flows of candidate words on the basis of the degree of fit of the candidate words with the input. Experiments showed that SpeM outperforms Shortlist on the recognition of ‘real-life’ input. Furthermore, SpeM performs only slightly worse than an off-the-shelf full-blown automatic speech recogniser in which all words are equally probable, while it provides a transparent computationally elegant paradigm for modelling word activations in human word recognition.	acoustic cryptanalysis;computational model;programming paradigm;real life;simulation;speech recognition	Odette Scharenborg;Louis ten Bosch;Lou Boves	2003			computer simulation;natural language processing;speech technology;speaker recognition;trace;speech recognition;speech corpus;word recognition;computer science;speech;motor theory of speech perception;viseme;linguistics;logogen model;speech synthesis	NLP	-18.59386755674183	-85.86515074935866	90620
d7801b60049e59a043cc306ee8410d8b135c982b	performance improvement of rapid speaker adaptation based on eigenvoice and bias compensation		In this paper, we propose the bias compensation methods and the eigenvoice method using the mean of dimensional eigenvoice to improve the performance of rapid speaker adaptation based on eigenvoice. Experimental results for vocabulary-independent word recognition task shows the proposed method yields improvements for a small adaptation data. We obtained 22~30% relative improvement by the bias compensation methods, and obtained 41% relative improvement by the eigenvoice method using the mean of dimensional eigenvoice with only single adaptation word.	motion compensation;newton's method;vocabulary	Jong Se Park;Hwa Jeon Song;Hyung Soon Kim	2003			artificial intelligence;speech recognition;pattern recognition;performance improvement;computer science	Robotics	-18.628434438110382	-91.63122190874125	90688
c236c2f61833c64616ae4cca7c8c70bf21258ab4	structure-constrained basis pursuit for compressed sensing		In compressive sensing (CS) theory, as the number of samples is decreased below a minimum threshold, the average error of the recovery increases. Sufficient sampling is either required for quality reconstruction or the error is resignedly accepted. However, most CS work has not taken advantage of the inherent structure in a variety of signals relevant to engineering applications. Hence, this paper proposes a new method of recovery built on basis pursuit (BP), called Structure-Constrained Basis Pursuit (SCBP), that constrains signals based on known structure rather than through extra sampling. Preliminary assessments of this method on TIMIT recordings of the speech phoneme /A/ show a substantial decrease in error: with a fixed 5:1 compression ratio the average recovery error is 23.8% lower versus vanilla BP. More significantly, this method can be applied to any CS application that samples structured data, such as FSK waveforms, speech, and tones. In these cases, higher compression ratios can be reached with comparable error.	basis pursuit;compressed sensing;sampling (signal processing);timit	Miguel Domínguez;Behnaz Ghoraani	2015	CoRR		speech recognition;telecommunications;artificial intelligence;statistics	ML	-13.594081241184025	-92.40445232179708	90810
9b6d7102b4fefeecc5fcc44946e8af59170a71ab	unsupervised adaptation with domain separation networks for robust speech recognition		Unsupervised domain adaptation of speech signal aims at adapting a well-trained source-domain acoustic model to the unlabeled data from target domain. This can be achieved by adversarial training of deep neural network (DNN) acoustic models to learn an intermediate deep representation that is both senone-discriminative and domain-invariant. Specifically, the DNN is trained to jointly optimize the primary task of senone classification and the secondary task of domain classification with adversarial objective functions. In this work, instead of only focusing on learning a domain-invariant feature (i.e. the shared component between domains), we also characterize the difference between the source and target domain distributions by explicitly modeling the private component of each domain through a private component extractor DNN. The private component is trained to be orthogonal with the shared component and thus implicitly increases the degree of domain-invariance of the shared component. A reconstructor DNN is used to reconstruct the original speech feature from the private and shared components as a regularization. This domain separation framework is applied to the unsupervised environment adaptation task and achieved 11.08% relative WER reduction from the gradient reversal layer training, a representative adversarial training method, for automatic speech recognition on CHiME-3 dataset.		Zhong Meng;Zhuo Chen;Vadim Mazalov;Jinyu Li;Yifan Gong	2017	2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)	10.1109/ASRU.2017.8268938	artificial intelligence;machine learning;acoustic model;artificial neural network;domain adaptation;computer science;speech recognition;extractor;pattern recognition	NLP	-16.06857058717795	-90.3392807670069	90820
c356a90731f49d93ccba9e2fc008ca69960c6ec8	automatic age recommendation system for children's video content	visual databases feature extraction image classification recommender systems speech recognition support vector machines video signal processing;age category automatic age recommendation system children video content video database support vector machines multiple classifiers complex speech recognition signal processing techniques feature extraction methods noise jumps language complexity word rate syllable rate children cognitive capacity high level audio features;streaming media feature extraction accuracy noise kernel speech color	This paper presents a novel automatic method to determine the appropriate age of video content in a video database geared to children. When combined with classical features the system improves accuracy rate for more than 0.13 for the same type of classifier in determining the age category of content for children between the ages of three to six years old. The main novelty of the system is that it utilizes high level audio features related to the cognitive capacity of children. These novel features gage the cognitive ability of the intended audience by quantifying the structure of the language. These novel features include syllable rate, word rate, language complexity and noise jumps. The feature extraction methods are also novel in that we count the number of syllables and words using relatively computationally inexpensive signal processing techniques forgoing complex speech recognition. The presented new method is tested using multiple classifiers on a commercial video database.	algorithm;cognition;computational complexity theory;cross-validation (statistics);digital video;experiment;feature extraction;high-level programming language;recommender system;signal processing;speech recognition;statistical classification;syllable	Joseph Santarcangelo;Xiao-Ping Zhang	2014	2014 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2014.6865244	natural language processing;speech recognition;feature extraction;computer science;video quality;pattern recognition	AI	-5.653994559154723	-94.09594640794947	90860
d182a6b12171a7c7151614d8bd4670f9165aac83	singing pitch extraction from monaural polyphonic songs by contextual audio modeling and singing harmonic enhancement	hidden markov model;spectrum;feature extraction	This paper proposes a novel approach to extract the pitches of singing voices from monaural polyphonic songs. The hidden Markov model (HMM) is adopted to model the transition between adjacent singing pitches in time, and the relationships between melody and its chord, which is implicitly represented by features extracted from the spectrum. Moreover, another set of features which represents the energy distribution of the enhanced singing harmonic structure is proposed by applying a normalized sub-harmonic summation technique. By using these two feature sets with complementary characteristics, a 2stream HMM is constructed for singing pitch extraction. Quantitative evaluation shows that the proposed system outperforms the compared approaches for singing pitch extraction from polyphonic songs.	hidden markov model;information extraction;markov chain;pitch (music);quantum harmonic oscillator	Chao-Ling Hsu;Liang-Yu Chen;Jyh-Shing Roger Jang;Hsing-Ji Li	2009			speech recognition;hidden markov model;chord (music);feature extraction;computer science;monaural;singing;harmonic;polyphony	NLP	-9.970919070621303	-91.15028563572473	90911
fa768829be7e1ee8346e051e9f4bb6dd233d5b66	the training of the tone of mandarin two-syllable words based on pitch projection synthesis speech	speech synthesis;computer assisted mandarin tone learning system mandarin two syllable words tone training pitch projection synthesis speech teaching speech synthesis standard voice selection lexicon tones learner speech standard tones speech segments speech timbie complex speech signal variation japanese language speech perception speech production generalization control group statistical analysis selective attention mechanism human brain speech learning;tone;tone language learning speech synthesis pitch projection;language learning;speech training educational institutions standards production speech synthesis;teaching computer based training linguistics speech synthesis statistical analysis;pitch projection	Summary form only given. This paper uses pitch projection method to synthesize teaching speech by selecting the appropriate standard voice. To synthesize the teaching speech, lexicon tones in learners' speech is turned into standard tones, while keeping the segments and timbie unchanged. Thereby complex variation of speech signal is reduced except for tone. Then the paper carries tone training experiments for Japanese based on the synthesis speech of Mandarin two-syllable words. The training results show that the method of synthesis speech is superior to standard voice method in relative improvement of perception, production, as well as generalization of production. The method of synthesis speech is far better than the method of control group without training. Most of the results are statistically significant. Experimental results also prove the existence of selective attention mechanism of the human brain when learning speech. It provides an experimental and theoretical basis that speech synthesis method could be integrated into the computer-assisted Mandarin tone learning system.	experiment;lexicon;speech synthesis;super robot monkey team hyperforce go!;syllable	Yanlu Xie;Bei Zhang;Jinsong Zhang	2014	The 9th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2014.6936697	language acquisition;speech technology;speech production;cued speech;speech recognition;speech perception;lightness;speech corpus;computer science;speech;motor theory of speech perception;speech shadowing;speech processing;linguistics;chinese speech synthesis;speech synthesis;intelligibility	NLP	-12.57298101871582	-82.30143324206475	91103
1e9ea86588e6d681582aa120be5130bce941c1b9	compression of line spectral frequency parameters with asynchronous interpolation	line spectral frequency;interpolation;error reduction;network synthesis;codecs;speech synthesis;probability density function;data stream;acoustics;speech;natural languages;indexing terms;measurement uncertainty;data mining;speech synthesis interpolation;telephony;speech quality;speech features;vectors;temporal decomposition;tts;linear interpolation;feature extraction;acoustical engineering;text to speech;coarticulation effects;weight function;temporal decomposition line spectral frequency asynchronous interpolation speech synthesis text to speech speech quality acoustic inventory coarticulation effects speech features error reduction;compression;frequency;acoustic measurements;asynchronous interpolation;frequency interpolation speech synthesis acoustic measurements network synthesis telephony codecs vectors natural languages acoustical engineering;acoustic inventory;tts speech synthesis temporal decomposition compression acoustic inventory	TTS systems require a trade-off between size and speech quality. A larger acoustic inventory allows synthesis of speech that sounds more natural. The Asynchronous Interpolation Model improves the quality to size ratio, allowing better compression of large acoustic inventories, as well as better quality speech from a small system. At maximum compression, our method represents most phonemes by a single frame of data. Coarticulation effects are specified as context-specific non-linear interpolation functions. Dividing the speech features into multiple data streams allows asynchronous interpolation. In this study, AIM was applied to LSF parameters. Varying the number of streams allows for variable amount of compression. We used three different objective measures to investigate the effect of number and partitioning of streams. The first few weight functions (and the last one) seem to offer the most error reduction. Partitions separating the first 6 LSFs score well with all three measures.	aim alliance;acoustic cryptanalysis;asynchronous circuit;inventory;lsf;linear interpolation;log-structured file system;nonlinear system;speech synthesis	Rachel Moldover;Alexander Kain	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960452	network synthesis filters;probability density function;codec;weight function;speech recognition;index term;feature extraction;interpolation;computer science;speech;frequency;mathematics;natural language;telephony;linear interpolation;speech synthesis;compression;statistics;measurement uncertainty	Robotics	-10.512724854958128	-88.65816112854101	91112
577894b3cb79a4dab3a97625ff2976a742e93c66	a phoneme recognizer for the hearing impaired	normal hearing;computer and information science;hearing impaired;automatic speech recognition;system design;data och informationsvetenskap;hearing aid	This paper describes an automatic speech recognition system designed to investigate the use of phoneme recognition as a hearing aid in telephone communication. The system was tested in two experiments. The first involved 19 normal hearing subjects with a simulated severe hearing impairment. The second involved 5 hearing impaired subjects. In both studies we used a procedure called Speech Tracking, which measures the effective communication speed between two persons. A substantial improvement was found in both cases.	experiment;finite-state machine;speech recognition	Mathias Johansson;Mats Blomberg;Kjell Elenius;Lars-Erik Hoffsten;Anders Torberger	2002			speech recognition;computer science;systems design	HCI	-13.389252403159382	-89.61425574073762	91374
39110e2bbcfcd2b0d15ed2e4e28c06268bb2780b	混合聲音事件驗證在家庭自動化之應用 (home environmental sound recognition) [in chinese]				Chang-Hong Lin;Ernestasia Siahaan;Bo-Wei Chen;Hsiang-Lung Chuang;Wen-Chi Hsieh;Jia-Ching Wang	2013			acoustics;computer science;sound recognition	HCI	-14.250738188009977	-85.8388329820307	91536
dfc1a944bd1b3f109f93b651316fc72f0c7035e7	temporal filtering of visual speech for audio-visual speech recognition in acoustically and visually challenging environments	hidden markov model;temporal variation;audio visual speech recognition;noise robustness;computer vision;artificial intelligent;temporal filtering;band pass filter;feature extraction;visual features;speech recognition;late integration;natural language processing;visual speech recognition;neural network	The use of visual information of speech has been shown to be effective for compensating for performance degradation of acoustic speech recognition in noisy environments. However, visual noise is usually ignored in most of audio-visual speech recognition systems, while it can be included in visual speech signals during acquisition or transmission of the signals. In this paper, we present a new temporal filtering technique for extraction of noise-robust visual features. In the proposed method, a carefully designed band-pass filter is applied to the temporal pixel value sequences of lip region images in order to remove unwanted temporal variations due to visual noise, illumination conditions or speakers' appearances. We demonstrate that the method can improve not only visual speech recognition performance for clean and noisy images but also audio-visual speech recognition performance in both acoustically and visually noisy conditions.	acoustic cryptanalysis;audio-visual speech recognition;elegant degradation;image noise;pixel	Jong-Seok Lee;Cheol Hoon Park	2007		10.1145/1322192.1322231	voice activity detection;speaker recognition;computer vision;audio mining;speech recognition;feature extraction;computer science;machine learning;speech processing;acoustic model;band-pass filter;hidden markov model	Vision	-11.557839958868106	-90.16848483003906	91736
9a5d55dbbfe5329a600fdb4a01e7c97eac874660	definition of subword acoustic units for wordspotting			acoustic cryptanalysis;substring	Richard C. Rose	1993			artificial intelligence;speech recognition;pattern recognition;computer science	ML	-14.807639215806681	-86.30131701229566	91779
c112c091ce4efb6e305f7a6803e46a7065971f4e	reviewing human language identification	experimental design;language use;speech modification;human language identification;language identification;source filter model;prosody;white noise	This article overviews human language identification (LID) experiments, especially focusing on the modification methods of stimulus, mentioning the experimental designs and languages used. A variety of signals to represent prosody have been used as stimuli in perceptual experiments: lowpass-filtered speech, laryngograph output, triangular pulse trains or sinusoidal signals, LPC-resynthesized or residual signals, white-noise driven signals, resynthesized signals preserving or degrading broad phonotactics, syllabic rhythm, or intonation, and parameterized source component of speech signal. Although all of these experiments showed that “prosody” plays a role in LID, the stimuli differ from each other in the amount of information they carry. The article discusses the acoustic natures of these signals and some theoretical backgrounds, featuring the correspondence of the source, in terms of the sourcefilter theory, to prosody, from a linguistic perspective. It also reviews LID experiments using unmodified speech, research into infants, dialectology and sociophonetic research, and research into foreign accent.	acoustic cryptanalysis;design of experiments;experiment;language identification;low-pass filter;semantic prosody;white noise	Masahiko Komatsu	2007		10.1007/978-3-540-74122-0_17	speech recognition;computer science;speech processing;linguistics;prosody;communication	NLP	-10.220880624617882	-83.85381970890651	91856
286117fa89f62d51a3a040b9a8560c9a10876d11	synchrony in prosodic and linguistic features between backchannels and preceding utterances in attentive listening	pragmatics;complexity theory;employee welfare;speech;speaker recognition computational complexity computational linguistics;correlation employee welfare syntactics pragmatics timing complexity theory speech;syntactics;speaker utterances synchrony prosodic linguistic features attentive listening human human dialogue coordinated backchannels smooth communication reactive tokens morphological complexity syntactic complexity;correlation;prosody dialogue backchannel;timing	In human-human dialogue, especially in attentive listening such as counseling, backchannels play an important role. Appropriately coordinated backchannels will not only make smooth communication but also help establish rapport. By collecting counseling dialogue, we investigate whether and how synchrony is expressed by prosodic and linguistic features of backchannels with respect to the preceding speaker's utterances. First, we find out correlation patterns according to the type of backchannels and prosodic features; a larger correlation is observed for reactive tokens than acknowledging tokens and for the power features than the pitch features. Next, we investigate the relationship between the morphological complexity of backchannels and the syntactic complexity of the preceding clause/sentence unit. The result can be useful for generating a variety of backchannels adaptive to the speaker's utterances.	norm (social)	Tatsuya Kawahara;Takashi Yamaguchi;Miki Uesato;Koichiro Yoshino;Katsuya Takanashi	2015	2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)	10.1109/APSIPA.2015.7415301	psychology;natural language processing;linguistics;communication	NLP	-13.019280453555417	-82.40673933111863	91870
c8df655852c9f00b7bb8292af5352131a500230b	dealing with diverse data variances in factor analysis based methods	decomposition;plda;i vector;speaker recognition;factor analysis	Probabilistic Linear Discriminant Analysis PLDA and the concept of i-vectors are state-of-the-art methods used in the speaker recognition. They are based on Factor Analysis, in which a data covariance matrix is decomposed in order to find a low dimensional representation of given feature vectors. More precisely, the Factor Analysis based methods seek for directions/subspaces in which the projected overall/between/within variance is highest. In order to train models related to individual methods, development speech corpora comprising various acoustic conditions are utilized. The higher are the variations in some of these acoustic conditions, the more will the model tend to reflect them. Strong data variations in some of the development corpora may suppress conditions present in other corpora. This can lead to poor recognition when acoustic variations in test conditions significantly differ. In this paper techniques alleviating such effects are investigated. The idea is to use several background and i-vector models related to different parts of development data so that several i-vectors are extracted, processed and handed over to the PLDA modelling. PLDA model is then used to utilize all the extracted information and provide the verification result.	factor analysis	Lukás Machlica	2013		10.1007/978-3-319-01931-4_14	speech recognition;computer science;machine learning;pattern recognition	ML	-16.536651476979358	-91.67471696480729	91883
1a3c4a57b4ee90365fb714a24672c97083fc44d5	an evaluation of audio-visual person recognition on the xm2vts corpus using the lausanne protocols	speaker recognition architecture;audio visual systems;protocols;multimodal person recognition;verification protocols audio visual person recognition xm2vts corpus lausanne protocols multimodal person recognition architecture face recognition system speaker recognition architecture;audio visual person recognition;testing;speech enhancement;protocols speaker recognition face recognition speech enhancement testing engines face detection linear discriminant analysis feature extraction target recognition;speaker recognition;lausanne protocols;face recognition;face recognition system;engines;target recognition;xm2vts corpus;feature extraction;verification protocols;speaker recognition audio visual systems face recognition protocols;audio visual;multimodal person recognition architecture;face detection;linear discriminant analysis	A multimodal person recognition architecture has been developed for the purpose of improving overall recognition performance and for addressing channel-specific performance shortfalls. This multimodal architecture includes the fusion of a face recognition system with the MIT/LL GMM/UBM speaker recognition architecture. This architecture exploits the complementary and redundant nature of the face and speech modalities. The resulting multimodal architecture has been evaluated on the XM2VTS corpus using the Lausanne open set verification protocols, and demonstrates excellent recognition performance. The multimodal architecture also exhibits strong recognition performance gains over the performance of the individual modalities.	facial recognition system;ll parser;multimodal interaction;speaker recognition	Kevin Brady;Michael S. Brandstein;Thomas F. Quatieri;Robert B. Dunn	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366893	speaker recognition;communications protocol;computer vision;face detection;speech recognition;feature extraction;computer science;machine learning;software testing;linear discriminant analysis	Vision	-12.69178684201631	-92.32275597027152	91966
186a3098585663d8cb1a944d0cb8402358810b90	amplitude convergence in children²s conversational speech with animated personas.	conversational interface;text to speech;animated character	During interpersonal conversation, both children and adults adapt the basic acoustic-prosodic features of their speech to converge with those of their conversational partner. In this study, 7-to-10year-old children interacted with a conversational interface in which animated characters used text-to-speech output (TTS) to answer questions about marine biology. Analysis of children’s speech to different animated characters revealed a 29% average change in energy when they spoke to an extroverted loud software partner (E), compared with an introverted soft-spoken one (I). The majority, or 77% of children, adapted their amplitude toward their partner’s TTS voice. These adaptations were bi-directional, with increases in amplitude observed during I to E condition shifts, and ecreases during E to I shifts. Finally, these results generalized across different user groups and TTS voices. Implications are discussed for guiding children’s speech to remain within system processing bounds, and for the future development of robust and adaptive conversational interf ac s.	acoustic cryptanalysis;bi-directional text;converge;netware file system;speech synthesis	Rachel Coulston;Sharon L. Oviatt;Courtney Darves	2002			natural language processing;speech recognition;computer science;speech synthesis	HCI	-11.674204608864567	-83.35458902633827	92017
b78d9327c32017e364104700ae666a764d0272eb	adjusting dysarthric speech signals to be more intelligible	dysarthria;speech transformation;intelligibility	This paper presents a system that transforms the speech signals of speakers with physical speech disabilities into a more intelligible orm that can be more easily understood by listeners. These transformations are based on the correction of pronunciation errors y the removal of repeated sounds, the insertion of deleted sounds, the devoicing of unvoiced phonemes, the adjustment of the empo of speech by phase vocoding, and the adjustment of the frequency characteristics of speech by anchor-based morphing of he spectrum. These transformations are based on observations of disabled articulation including improper glottal voicing, lessened ongue movement, and lessened energy produced by the lungs. This system is a substantial step towards full automation in speech ransformation without the need for expert or clinical intervention. Among human listeners, recognition rates increased up to 191% (from 21.6% to 41.2%) relative to the original speech by using he module that corrects pronunciation errors. Several types of modified dysarthric speech signals are also supplied to a standard utomatic speech recognition system. In that study, the proportion of words correctly recognized increased up to 121% (from 72.7% o 87.9%) relative to the original speech, across various parameterizations of the recognizer. This represents a significant advance owards human-to-human assistive communication software and human–computer interaction. 2012 Elsevier Ltd. All rights reserved.	biconnected component;finite-state machine;human–computer interaction;morphing;speech recognition;speech synthesis;vocoder	Frank Rudzicz	2013	Computer Speech & Language	10.1016/j.csl.2012.11.001	voice activity detection;speech production;speech recognition;motor theory of speech perception;viseme;acoustic model;linguistics;speech error;intelligibility	HCI	-10.701029777342494	-85.26014633995604	92234
8ffc3196faa03ad7d4aa7f8019e0b35842c8ee1e	speaker identification using pseudo pitch synchronized phase information in noisy environments	speech mel frequency cepstral coefficient noise measurement noise speaker recognition synchronization databases;speaker recognition;cepstral analysis;speaker identification japanese newspaper article sentence jnas database mfcc mel frequency cepstral coefficients noisy environments pseudo pitch synchronized phase information;speaker recognition cepstral analysis	In conventional speaker identification methods based on mel-frequency cepstral coefficients (MFCCs), phase information is ignored. Recent studies have shown that phase information contains speaker dependent characteristics, and, pitch synchronous phase information is more suitable for speaker identification. In this paper, we verify the effectiveness of pitch synchronous phase information for speaker identification in noisy environments. Experiments were conducted using the JNAS (Japanese Newspaper Article Sentence) database. The pseudo pitch synchronized phase information based method achieved a relative speaker identification error reduction rate of 15.5% compared to the conventional phase information (that is pitch non-synchronized phase). By cutting frames with low power and combining phase information with MFCC, a furthermore improvement was obtained.	coefficient;mel-frequency cepstrum;speaker recognition	Yuta Kawakami;Longbiao Wang;Seiichi Nakagawa	2013	2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference	10.1109/APSIPA.2013.6694385	speaker recognition;speaker diarisation;speech recognition;acoustics;computer science;pattern recognition	AI	-13.231222424663258	-90.70742156654968	92258
28cd68a9baffa43617ce59c946e68861eb307118	identifying the main determinants of phonetic variation in the newcastle electronic corpus of tyneside english	north east	Abstract The Newcastle Electronic Corpus of Tyneside English is a corpus of dialect speech from North-East England. It includes phonetic transcriptions of 63 interviews together with social data relating to each interviewee, and offers an opportunity to study the sociophonetics of Tyneside speech of the late 1960s. In a previous paper we began that study with an exploratory multivariate analysis of the transcriptions. The results were that speakers fell into clearly defined groups on the basis of their phonetic usage, and that these groups correlated well with social characteristics associated with the speakers. The present paper develops these results by trying to identify the main phonetic determinants of the speaker groups.		Hermann Moisl;Warren Maguire	2008	Journal of Quantitative Linguistics	10.1080/09296170701794302	speech recognition;mathematics;linguistics	NLP	-12.346576704292989	-81.9396444507399	92308
54c746c904a8c3dae1db1da3e648710163ec599d	statistical models for speech dereverberation	ar model;reverberation;probability;clean speech signal;moving average power spectral density model;statistical model based approach;probability density function;speech processing;biological system modeling;room transmission channel;speech;dereverberation;speech processing speech enhancement acoustics parameter estimation reverberation noise reduction probability density function degradation microphones frequency;speech dereverberation;statistical model;autoregressive model;accuracy;large scale;power spectral density;moving average;autoregressive power spectral density model;probability density functions;moving average power spectral density model speech dereverberation statistical model based approach probability density functions room transmission channel clean speech signal autoregressive model autoregressive power spectral density model;speech processing probability reverberation;statistical model dereverberation	This paper discusses a statistical-model-based approach to speech dereverberation. With this approach, we first define parametric statistical models of probability density functions (pdfs) for a clean speech signal and a room transmission channel, then estimate the model parameters, and finally recover the clean speech signal by using the pdfs with the estimated parameter values. The key to the success of this approach lies in the definition of the models of the clean speech signal and room transmission channel pdfs. This paper presents several statistical models (including newly proposed ones) and compares them in a large-scale experiment. As regards the room transmission channel pdf, an autoregressive (AR) model, an autoregressive power spectral density (ARPSD) model, and a moving-average power spectral density (MAPSD) model are considered. A clean speech signal pdf model is selected according to the room transmission channel pdf model. The AR model exhibited the highest dereverberation accuracy when a reverberant speech signal of 2 sec or longer was available while the other two models outperformed the AR model when only a 1-sec reverberant speech signal was available.	autoregressive model;channel (communications);portable document format;spectral density;statistical model	Takuya Yoshioka;Hirokazu Kameoka;Tomohiro Nakatani;Hiroshi G. Okuno	2009	2009 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics	10.1109/ASPAA.2009.5346489	probability density function;speech recognition;acoustics;computer science;speech processing;mathematics;autoregressive model;statistics	ML	-13.872280280740215	-93.78999224258013	92309
69b7ad2f5d928f1f41936a04ed3de1af51e334fe	a speech and language database for speech translation research			database;machine translation	Tsuyoshi Morimoto;Noriyoshi Uratani;Toshiyuki Takezawa;Osamu Furuse;Yasuhiro Sobashima;Hitoshi Iida;Atsushi Nakamura;Yoshinori Sagisaka;Norio Higuchi;Yasuhiro Yamazaki	1994			speech recognition;cued speech;speech translation;voxforge;speech technology;speech corpus;chinese speech synthesis;computer science;telegraphic speech;speech synthesis	NLP	-15.753034342842307	-85.77567057151084	92480
f3e4ee5d6a400da2372bac3566a03c5847e48b66	speaker recognition with discriminative speaker vq models	speaker recognition		pc speaker;speaker recognition;vector quantization	Kai Tat Ng;Jian Su;Bingzheng Xu	1995			artificial intelligence;discriminative model;speech recognition;pattern recognition;speaker diarisation;computer science;speaker recognition	Vision	-14.68648269114837	-87.7203604698541	92547
5f9319835efced3d81d572960bf1425588c662ce	a continuous prominence score based on acoustic features	institutional repositories;fedora;serveur institutionnel;vital;archive institutionnelle;open access;archive ouverte unige;vtls;cybertheses;institutional repository;ils	Up to now, prominence detection has mainly been considered a binary matter, a syllable or a word being considered as prosodically prominent or not. This contribution aims at developing an automatic detection procedure of gradual prominence. Based on 4 prosodic parameters (relative duration, relative f0, f0 movement and pause duration), the system provides each syllable with a gradual score of prominence ranging from 0 (non-prominent syllable) to 4 (extra-prominent syllable). The automatic detection (ProsoProm) relies on a manually annotated corpus (18 minutes, or 3669 syllables, of speech annotated by three experts) and is cumulative (the relative weight of each parameter is taken into account in order to compute a global score for each syllable). The discussion of the results includes a qualitative analysis of misses and false detections. The agreement between automatic and (median) human annotation reaches a Kappa score of 0.8. GOLDMAN, Jean-Philippe, et al. A Continuous Prominence Score Based on Acoustic Features. In: INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication Association. 2012.		Jean-Philippe Goldman;Mathieu Avanzi;Anne-Catherine Simon;Antoine Auchlin	2012			natural language processing;speech recognition;computer science	NLP	-13.078616018908212	-81.75423125342833	92696
aed7be981eedc103e1a5e9fd112d0d7f7adca7ac	comparison of voice activity detectors for interview speech in nist speaker recognition evaluation.		Interview speech has become an important part of the NIST Speaker Recognition Evaluations (SREs). Unlike telephone speech, interview speech has substantially lower signal-to-noise ratio, which necessitates robust voice activity detection (VAD). This paper highlights the characteristics of interview speech files in NIST SREs and discusses the difficulties in performing speech/nonspeech segmentation in these files. To overcome these difficulties, this paper proposes using speech enhancement techniques as a pre-processing step for enhancing the reliability of energy-based and statistical-model-based VADs. It was found that spectral subtraction can make better use of the background spectrum than the likelihood-ratio tests in statisticalmodel-based VADs. A decision strategy is also proposed to overcome the undesirable effects caused by impulsive signals and sinusoidal background signals. Results on NIST 2010 SRE show that the proposed VAD outperforms the statistical-modelbased VAD, the ETSI-AMR speech coder, and the ASR transcripts provided by NIST SRE Workshop.	adaptive multi-rate audio codec;decision theory;preprocessor;sensor;signal-to-noise ratio;speaker recognition;speech coding;speech enhancement;statistical model;stellar classification;voice activity detection	Hon-Bill Yu;Man-Wai Mak	2011			natural language processing;speech recognition	AI	-13.342740984356114	-91.27942937088702	92776
70357f13b87fb7fe850412e18fd6c8aacc2633c6	maximum a posteriori eigenvoice speaker adaptation for korean connected digit recognition		In this paper, we present a maximum a posteriori (MAP) eigenvoice speaker adaptation approach to the self-adaptation system. The proposed MAP eigenvoice is developed by introducing a probability density model for the eigenvoice coefficients. And we make a self-adaptation system which is useful to public user, because user does not need to speak several sentences for adaptation. In self-adaptation system we use only one utterance that will be recognized, so we use eigenvoice adaptation with MAP criterion that is most robust adaptation algorithm for very small adaptation data. In a series of self-adaptation experiments on the Korean connected digit recognition task, we demonstrate that the proposed approach achieves a good performance for a very small amount of adaptation data.	algorithm;coefficient;experiment;pc speaker	Hyung Bae Jeon;Dong Kook Kim	2004			artificial intelligence;speech recognition;maximum a posteriori estimation;speaker diarisation;pattern recognition;computer science;numerical digit	ML	-17.964524533426236	-91.73908385697048	92845
060b7aea76b0ec12259ccb3bd0c0bf28ce18d5f7	time-frequency reassignment for music analysis		Time-frequencyreassignment maybe viewedas a refinement of theshort-timeFourier transform,in which phaseinformation is usedto reducethesmearingof energyassociatedwith thestandard spectrogram. However, evengiventheperceptibly clearer visualrepresentationyieldedby thereassignment methodin thecaseof musicalsignals,thetaskremainsof extractingusefulinformationfromit for further processing. To this endit is proposedthat timereassignment informationbe usedto help identify musicaltransients,and that frequency reassignment informationbesimilarly employedasa means of estimatingthe pitch of musicalsignal components.To illustratetheseideas,anexampleis shownin which reassigned timeandfrequencypointsareusedto segmenta monophonic piano melodyand locatethe partials of its individual notes. Lastly, thepotentialroleof reassignment in theoverall framework of musictranscriptionis described,and several areas aredetailedfor futurestudy.	refinement (computing);spectrogram	Stephen W. Hainsworth;Patrick J. Wolfe	2001			short-time fourier transform;music theory;time–frequency analysis;acoustics;speech recognition;computer science	ML	-9.418711771930242	-90.91585053142963	92866
8b956fc653b90f0e294efe98ffbebbecc5bc31b3	a low complexity time-scaling expansion algorithm of speech signals suitable for real time implementation	voiced segment;iterative refinement;time scale modification;time scale;variable rate;low complexity;pitch period;dsp implementation;mean opinion score;elderly people;digital signal processor;time domain;real time implementation;floating point;source code;frequency domain;foreign language;computer simulation	This paper presents the development and implementation of a variable rate time-scaling expansion system for speech signals, based on the pitch information, in which only the voiced segments are expanded, keeping the unvoiced and silence segments unchanged. The proposed system was first evaluated by computer simulation and then implemented on a digital signal processor (DSP). Time-domain, frequency-domain, mean opinion score (MOS) and diagnostic rhyme test (DRT) evaluations were done to test the actual performance of developed algorithm, which show that the proposed system allows improving the learning level of foreign language students as well as the understanding ability of elderly people. Objective tests also were carried out in order to probe similarity between the original and the expanded signals. Applying an iterative refinement of the C source code it was possible to obtain a real-time implementation. The current implemented algorithm requires 11 kwords program memory and about 9 million of floating point operations per second (MFLOPS).	algorithm;image scaling	Gonzalo Duchen-Sanchez;José Juan García-Hernández;Mariko Nakano-Miyatake;Héctor M. Pérez Meana	2009	Digital Signal Processing	10.1016/j.dsp.2008.07.010	computer simulation;mean opinion score;foreign language;digital signal processor;real-time computing;simulation;speech recognition;time domain;computer science;floating point;frequency domain;source code	Graphics	-8.183311733921325	-87.47549289550912	92935
0f9093fe3e4213010667d795f48d1ade05c70c92	bio-inspired auditory processing for speech feature enhancement		Mel-frequency cepstrum based features have been traditionally used for speech recognition in a number of applications, as they naturally provide a higher recognition accuracies. However, these features are not very robust in a noisy acoustic conditions. In this article, we investigate the use of bio-inspired auditory features emulating the processing performed by cochlea to improve the robustness, particularly to counter environmental reverberation. Our methodology first extracts robust noise resistant features by gammatone filtering, which emulate cochlea frequency resolution and then a long-term modulation spectral processing is performed which preserves speech intelligibility in the signal. We compare and discuss the features based upon the performance on Aurora5 meeting recorder digit task recorded with four different microphones in a hands-free mode at a real meeting room. The experimental results show that the proposed features provide considerable improvements with respect to the state of the art feature extraction techniques.	acoustic cryptanalysis;british informatics olympiad;emulator;feature extraction;intelligibility (philosophy);mel-frequency cepstrum;microphone;modulation;speech recognition	Hari Krishna Maganti;Marco Matassoni	2011			neurocomputational speech processing;speech processing;computational auditory scene analysis	ML	-10.732092366907517	-89.20788824317286	92993
2b6c46d95e2a7a22b075bf09d40148922c38dd99	a likelihood ratio-based forensic voice comparison in microphone vs. mobile mismatched conditions using japanese /ai/		This paper describes a likelihood ratio-based forensic voice comparison experiment in microphone versus mobile channel mismatched conditions using parametric representations of formant trajectories. Cubic polynomial coefficients of /ai/ from non-contemporaneous recordings of 30 Japanese male speakers are used to derive multivariate likelihood ratios. The results are evaluated separately for a matched and mismatched group to determine the effect of the mismatch on system performance. A calibrated cross-validated log-likelihood ratio cost (Cllr) of 0.93 is achieved for the F-pattern of /ai/ representing an 18% reduction in system validity relative to the matched group. Separate testing involving only F1 and F2 features evinces a smaller (10%) reduction; suggesting F3 may be more impacted by channel differences. Spectral analysis of F3 indicates this stems from formant tracking errors due to weak signal energy in transmission. As such, F3 in /ai/ should be excluded from analysis where it is poorly preserved. Given the relatively small percentage reductions in validity, it is concluded that /ai/ may be reasonably robust to the mismatch. However, poor performance in optimal conditions (Cllr = 0.77) suggests it may not be a particularly useful parameter in the first place. Limitations to the current study are also discussed.	coefficient;cubic function;microphone;polynomial;spectrum analyzer	Michael J. Carne	2015			speech recognition;forensic science;pattern recognition;artificial intelligence;microphone;computer science	AI	-9.913899389045898	-84.24569708891426	93216
7619271dbd49b2c3a08ef70bb19f1e21185aff23	a contrastive study of lexical stress placement in singapore english and british english		Singapore English (SE) and British English (BE) have been claimed to differ in lexical stress placement. Examples frequently cited in the literature involve polysyllabic words such as hopelessly and compounds such as blackboard. Such words are stressed word-initially in BE, but are said to be stressed word-finally in SE. In the present paper, we investigate the acoustic evidence for the suggested cross-varietal difference. Two observations lead us to explore the claim that SE and BE differ in lexical stress placement. Firstly, all observations about stress differences between SE and BE are based solely on auditory impressions by British English listeners. The acoustic evidence for the claim has remained unexplored. Secondly, it appears that the auditory evidence comes largely from realisations of test words in citation form, i.e. in nuclear, phrase-final position. In phrase-final position, however, we can expect phrase-final lengthening, and lengthening is a cue to stress, at least in British English. If Singapore English has more phrasefinal lengthening than British English, then this effect may account for the suggested differences in lexical stress placement.	acoustic cryptanalysis;american and british english spelling differences;auditory processing disorder;comparison of american and british english;stress ball	Ee Ling Low;Esther Grabe	1998			speech recognition;linguistics;natural language processing;computer science;singapore english;british english;artificial intelligence	NLP	-10.596633407924365	-80.70285243426652	93228
4af313b0cb31e835c1682f8f6041e929f9763685	degradation modeling and classification of mixed populations using segmental continuous hidden markov models			elegant degradation;hidden markov model;markov chain;population	Zhen Chen;Tangbin Xia;Yaping Li;Ershun Pan	2018	Quality and Reliability Eng. Int.	10.1002/qre.2292	econometrics;engineering;artificial intelligence;hidden markov model;pattern recognition	SE	-14.010809860901608	-87.61575678402771	93283
aeb2674ce3b99c65bef3c7859ddce84ceecf0d03	choosing best algorithm combinations for speech processing tasks in machine learning using marf	speech processing;machine learning	This work reports experimental results in various speech processing tasks using an application based on the Modular Audio Recognition Framework (MARF) in terms of the best of the available algorithm configurations for each particular task. This study focuses on the tasks of identification of speakers' as of their gender and accent vs. who they are through machine learning. This work significantly complements a preceding statistical study undertaken only for the text-independent speaker identification.	algorithm;machine learning;modular audio recognition framework (marf);speech processing	Serguei A. Mokhov	2008		10.1007/978-3-540-68825-9_21	natural language processing;multi-task learning;speech recognition;computer science;machine learning	ML	-15.062044979219918	-88.34997872172244	93464
c38dcb17ea333d1eda21d781e6fd26fbc48fcdfd	probabilistic speaker pronunciation adaptation for spontaneous speech synthesis using linguistic features	linguistic features;spontaneous speech synthesis;conditional random fields;pronunciation adaptation;feature selection	Pronunciation adaptation consists in predicting pronunciation variants of words and utterances based on their standard pronunciation and a target style. This is a key issue in text-to-speech as those variants bring expressiveness to synthetic speech, especially when considering a spontaneous style. This paper presents a new pronunciation adaptation method which adapts standard pronunciations to the style of individual speakers in a context of spontaneous speech. Its originality and strength are to solely rely on linguistic features and to consider a probabilistic machine learning framework, namely conditional random fields, to produce the adapted pronunciations. Features are first selected in a series of experiments, then combined to produce the final adaptation method. Backend experiments on the Buckeye conversational English speech corpus show that adapted pronunciations significantly better reflect spontaneous speech than standard ones, and that even better could be achieved if considering alternative predictions.	biconnected component;buckeye corpus;conditional random field;experiment;interpolation;machine learning;mean squared error;relevance;speech corpus;speech synthesis;spontaneous order;synthetic intelligence;word error rate	Raheel Qader;Gwénolé Lecorvé;Damien Lolive;Pascale Sébillot	2015		10.1007/978-3-319-25789-1_22	natural language processing;speech recognition;computer science;linguistics	NLP	-18.586845202610252	-85.35294150053508	93538
38502d87219cbf53d06b2cfa19ce47795b110a1f	significance of pitch synchronous analysis for speaker recognition using aann models	speaker recognition	For speaker recognition studies, it is necessary to process the speech signal suitably to capture the speaker-specific information. There is complementary speaker-specific information in the excitation source and vocal tract system characteristics. Therefore it is necessary to separate these components, even approximately, from the speech signal. We propose linear prediction (LP) residual and LP coefficients to represent these two components. Analysis is performed in a pitch synchronous manner in order to focus on the significant portion of the speech signal in each glottal cycle, and also to reduce the artifacts of digital signal processing on the extracted features. Finally, the speaker-specific information is captured from the excitation and the vocal tract system components using autoassociative neural networks (AANN) models. We show that the pitch synchronous extraction of information from the residual and vocal tract system bring out the speaker-specific information much better than using the pitch asynchronous analysis as in the traditional block processing using an analysis window of fixed size.	artificial neural network;coefficient;digital signal processing;speaker recognition;tract (literature)	Sri Harish Reddy Mallidi;Kishore Prahallad;Suryakanth V. Gangashetty;Bayya Yegnanarayana	2010			artificial intelligence;speech recognition;pattern recognition;speaker recognition;speaker diarisation;computer science	ML	-10.38401430272367	-90.40999613832865	93655
e92066986149561ab339c4743ae236008991c67a	assessing emotions in a cross-cultural context	databases;emotion encoding;emotion recognition cultural differences context databases training labeling magnetic heads;valence emotion encoding emotion recognition cross cultural context emotion database universality specificity;magnetic heads;training;emotion recognition;universality;emotional valence scale cross cultural context positive emotion recognition system negative emotion recognition system multimodal emotional expression emotional image;cross cultural;valence;specificity;context;labeling;cultural differences;emotion database	Emotion recognition systems could support professionals in a wide range of areas. Several work in emotion recognition has been carried out in the last decades, yet few attention has been payed to cross-culture context emotion recognition. Multimodal emotional expressions from 36 subjects with different cultural backgrounds were collected. In the experiment, participants observed and assessed emotional images in a 5 point positive and negative emotional valence scale. This information was used as ground truth for the recorded information. The dataset was segmented for all the participants and partially labeled for 8 of them, for a total of 160 segments. Recognition of positive and negative emotions was obtained from the dataset suggesting agreement points in expression of emotion between cultures.	emotion recognition;ground truth;multimodal interaction	Maria Alejandra Quiros-Ramirez;Takehisa Onisawa	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6378246	valence;labeling theory;universality;cultural diversity;cross-cultural	Robotics	-7.814853440716363	-80.79976244197255	93859
44001848d40df40db0da364fa081bdb6e9074364	self-learning speaker identification for enhanced speech recognition	speaker identification;speech recognition;speaker adaptation	A novel approach for joint speaker identification and speech recognition is presented in this article. Unsupervised speaker tracking and automatic adaptation of the human–computer interface is achieved by the interaction of speaker identification, speech recognition and speaker adaptation for a limited number of recurring users. Together with a technique for efficient information retrieval a compact modeling of speech and speaker characteristics is presented. Applying speaker specific profiles allows speech recognition to take individual speech characteristics into consideration to achieve higher recognition rates. Speaker profiles are initialized and continuously adapted by a balanced strategy of short-term and long-term speaker adaptation combined with robust speaker identification. Different users can be tracked by the resulting self-learning speech controlled system. Only a very short enrollment of each speaker is required. Subsequent utterances are used for unsupervised adaptation resulting in continuously improved speech recognition rates. Additionally, the detection of unknown speakers is examined under the objective to avoid the requirement to train new speaker profiles explicitly. The speech controlled system presented here is suitable for in-car applications, e.g. speech controlled navigation, hands-free telephony or infotainment systems, on embedded devices. Results are presented for a subset of the SPEECON database. The results validate the benefit of the speaker adaptation scheme and the unified modeling in terms of speaker identification and speech recognition rates. © 2011 Elsevier Ltd. All rights reserved.	approximation error;baseline (configuration management);codebook;complex system;computational complexity theory;embedded system;experiment;extended validation certificate;feature extraction;finite-state machine;human–computer interaction;information retrieval;overhead (computing);principal component analysis;speaker recognition;speech recognition;unsupervised learning	Tobias Herbig	2011	Computer Speech & Language	10.1016/j.csl.2011.11.002	voice activity detection;speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition;voice analysis;speech processing	NLP	-17.46445818850786	-90.281869128205	93896
781eddc595b86e8a366cb071da08bcaeea9713ff	punctuation has a point, so use it!		It is all too common for systems processing natural language, whether for input (automatic speech recognition, text queries, dialogue etc.) or output (text-to-speech), to ignore or strip out punctuation. The effect of prosodic factors, such as intonation and pausing, on language processing remains controversial. While there is an obvious relationship between punctuation and prosody it cannot be a simple mapping: grammatical rules prevent the inclusion of punctuation at points where a speaker might pause, and the set of punctuation is not rich enough to transcribe all the spoken features categorised as prosody. It is therefore important for any realistic text-to-speech (or speech-to-text) conversion to consider these important features of language. An experimental investigation showed that commas exert a consistently strong and direct rhetorical influence on sentences being read aloud. They result in the slower delivery of words preceding the comma and an increase in pauses in speech. While the lengthening effect is an uncontroversial feature found at the end of clauses, even in the absence of punctuation, there is evidence to suggest that the comma is particularly useful in acoustically segmenting text by stimulating a gap, or period of silence, between linguistic units. This is particularly salient at points where a break can convey disambiguating information. Somewhat surprisingly, commas do not induce shifts in the fundamental frequency of speech or alter intonational patterns. Any generation of naturalistic synthetic speech should therefore take these factors into consideration. BODY Too many TTS and ASR systems ignore punctuation. Mistake! Little things can make a difference for the better. Volume 2 of Tiny Transactions on Computer Science This content is released under the Creative Commons Attribution-NonCommercial ShareAlike License. Permission to make digital or hard copies of all or part of this work is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. CC BY-NC-SA 3.0: http://creativecommons.org/licenses/by-nc-sa/3.0/.	automatic system recovery;computer science;digital data;mcgurk effect;natural language;netware file system;semantic prosody;speech recognition;speech synthesis;synthetic intelligence;word-sense disambiguation	Robin L. Hill;Wayne S. Murray	2013	TinyToCS		communication;silence;mistake;natural language;punctuation;citation;prosody;permission;rhetorical question;computer science	NLP	-10.960990556996258	-81.05227450350758	94040
476b094e887837ae32d9241ee3b6b8ca4a5eb472	an llr-based technique for frame selection for gmm-based text-independent speaker identification			google map maker;lucas–lehmer–riesel test;speaker recognition	Pang Kuen Tsoi;Pascale Fung	2000			artificial intelligence;speech recognition;pattern recognition;speaker diarisation;computer science	HCI	-14.40755394911871	-88.62995500683185	94157
398d7abfe9efe146bd5c63b8d05aefd8e8f06f77	investigating syllabic prominence with conditional random fields and latent-dynamic conditional random fields		The present study performs an investigation on several issues concerning the automatic detection of prominences. Its aim is to offer a better understanding of the prominence phenomenon in order to be able to improve existent prominence detection systems. The study is threefold: first, the presence of hidden dynamics in the sequence of prominent and non-prominent syllables is tested by comparing results obtained with CRFs and LDCRFs. Second, the size of the context to be taken into account when determining prominence was examined and third, a new set of features was investigated. The obtained results show that LDCRFs systematically outperform CRFs, that a context of three syllables is generally sufficient for prominence detection and that syllable length is a useful feature to include. Also, new features concerning pitch movements we introduced can substitute adequately heuristic measures used in previous works.	conditional random field;heuristic;syllable	Francesco Cutugno;Enrico Leone;Bogdan Ludusan;Antonio Origlia	2012			artificial intelligence;pattern recognition;syllabic verse;computer science;conditional random field	NLP	-12.72322470114298	-81.34612395839014	94400
50f06ec5ba432166a9367e6a60e15aed1d2e3985	human listening experiments provide insight into cetacean auditory perception		Odontocete cetaceans produce an array of sounds including echolocation clicks that can be used for object recognition and whistles used primarily for communication. Echoes and whistles contain a multitude of acoustic features, and the salient features used by animals when identifying objects or in social contexts are often difficult to isolate. One method for detecting salient acoustic features is to compare the performance of cetaceans and humans on the same auditory perception tasks. Human listeners can be presented with echoes produced with simulated cetacean clicks or whistle-like stimuli and provide verbal feedback on discriminatory cues. An early human listening study was performed by Au and Martin (1989). Seven studies performed over the past 15 years show that humans perform as well or better than cetacean subjects in a variety of tasks: echoic discrimination of objects varying in size, shape, material, texture, or wall thickness; echoic recognition of objects from various aspect angles; echoic ob...	experiment	Caroline M. DeLong	2016	Proc. Meetings on Acoustics	10.1121/2.0000447	acoustics	ML	-7.973761037254954	-81.62056577951115	94463
d6948ecb974e021ab1c6b49e2ef7077171f62115	data sampling for improved speech recognizer training	sampling methods;spectrum	Proper data selection for training a speech recognizer can be important for reducing costs of developing systems on new tasks and exploratory experiments, but it is also useful for efficient leveraging of the increasingly large speech resources available for training large vocabulary systems. In this work, we investigate various sampling methods, comparing the likelihood criterion to new acoustic measures motivated by work in child language acquisition. The acoustic criteria can be used with or without pre-existing transcriptions or models. When applied to the problem of selecting a small training set, the best results are obtained using modulation spectrum features and a discriminant function trained on child vs. adult-directed speech. For large corpora, none of the methods outperforms random sampling, but reduced training costs are obtained by using multistage training and initializing with the small corpus.	acoustic cryptanalysis;discriminant;experiment;finite-state machine;modulation;monte carlo method;multistage amplifier;sampling (signal processing);speech recognition;test set;text corpus;vocabulary	Takahiro Shinozaki;Mari Ostendorf;Les E. Atlas	2005			speech recognition;pattern recognition;artificial intelligence;sampling (statistics);computer science	NLP	-18.66340493623597	-90.28603459700227	94493
f93b90a12a17eb2b88309f0f382f945e3d4d274e	cellular-phone based speech-to-speech translation system atr-matrix		We describe the implementation of a cellular-phone based speech translation system without telephone quality speech database or special CT hardware. The purpose is to quickly build a prototype service system that can be used for data collection with real users. To train the acoustic model for the speech recognition system, available high-quality databases were made usable by 1.) appropriate downsampling and ltering of high-quality databases, and 2.) by piping, similar to the NTIMIT and CTIMIT paradigms. An evaluation of acoustic models with ltered, piped and real cellular-phone data is given. Recognition rates are at same levels as for wideband speech.	acoustic cryptanalysis;acoustic model;answer to reset;database;decimation (signal processing);machine translation;mobile phone;open road tolling;pipeline (unix);prototype;speech recognition;timit	Rainer Gruhn;Harald Singer;Hajime Tsukada;Masaki Naito;Atsushi Nishino;Atsushi Nakamura;Yoshinori Sagisaka;Satoshi Nakamura	2000			speech translation;speech recognition;computer-assisted translation;phone;machine translation;matrix (mathematics);transfer-based machine translation;computer science;rule-based machine translation;example-based machine translation	OS	-16.267947196852884	-85.8228167455264	94500
874adf47b16cbd11cd5562d610217ef67feaad00	tones in tongluo - a phonetic analysis		This is an acoustic analysis of the citation tones on the monosyllables from the native Tongluo speakers. Tongluo is located in the south of Wujiang in Jiangsu Province in China and on the east of Jiaxing in Zhejiang Province in China. The purpose of the study is to obtain the citation tones in Tongluo and compare with the tones in other towns in Wujiang district. Results show that there are 7 citation tones in Tongluo. They are yinping[44], yangping[23], yinshang[51], yangshang[231], qusheng[213], yinru[5 ] and yangru[2 3 ].	acoustic cryptanalysis;towns	Yanhong Xu	2011				NLP	-12.259484366396691	-81.9237818913728	94622
851b18cc8816958e6a25224c96ef91289159ab6c	dynamic time warping based on modified alignment costs for evoked potentials averaging		Averaging of time-warped signal cycles is an important tool for suppressing noise of quasi-periodic or event related signals. However, in the paper we show that the operation of time warping introduces unfavorable correlation among the noise components of the summed cycles. Such correlation violates the requirements necessary for effective averaging and results in poor suppression of noise. To limit these effects, we redefine the matrix of the alignment costs. The proposed modifications result in significant increase of the noise reduction factor in the experiments on different types and levels of noise.	dynamic time warping	Marian Kotas;Jacek M. Leski;Tomasz Moron	2015		10.1007/978-3-319-23437-3_26	computer vision;speech recognition;machine learning	NLP	-9.826265422621592	-89.82740073068356	94769
48c8c0709308721696900e8b92fc740f4c324f2b	an empirical text transformation method for spontaneous speech synthesizers		Spontaneously spoken utterances are characterized by a number of lexical and non-lexical features. These features can also reflect speaker specific characteristics. A major factor that discriminates spontaneous speech from written text is the presence of these paralinguistic features such as filled pauses (fillers), false starts, laughter, disfluencies and discourse markers that are beyond the framework of formal grammars. The speech recognition community has dealt with these variabilities by making provisions for them in language models, to improve recognition accuracy for spoken language. In another scenario, the analysis of these features could also be used for language processing/generation for the overall improvement of synthesized speech or machine response. Such synthesized spontaneous speech could be used for computer avatars and Speech User Interfaces (SUIs) where lengthy interactions with machines occur, and it is generally desired to mimic a particular speaker or the speaking style. This problem of language generation involves capturing general characteristics of spontaneous speech and also speaker specific traits. The usefulness of conventional language processing tools is limited by the availability of training corpus. Hence and empirical text processing technique with ideas motivated from psycholinguistics is proposed. Such an empirical technique could be included in the text analysis stage of a TTS system. The proposed technique is adaptable: it can be extended to mimic different speakers based on an individual’s speaking style and filler preferences.	formal grammar;householder transformation;interaction;language model;lexicon;natural language generation;netware file system;speech recognition;speech synthesis;spontaneous order;star filler;text corpus;text mining	Shiva Sundaram;Shrikanth (Shri) Narayanan	2003			speech recognition;natural language processing;computer science;artificial intelligence	NLP	-13.648123295079548	-82.82595910870086	94854
6dfe9a20b734fb67eb9f2492c9762eb18ef94c98	detection of hypernasality using statistical pattern classifiers		Speakers with velopharyngeal incompetence produce hypernasal speech across voiced elements. Acoustical study [1] on hypernasal speech and nasalized vowels of normal speakers revealed the fact that there is an additional formant frequency introduced in the low-frequency region, close to the first formant of the phonations /a/, /i/, and /u/. Based on this observation, in the current study, the focus is given to the low-frequency region alone, by low-pass filtering the speech signal. From each frame of the given speech signal, a three dimensional feature vector, which comprises of the locations of first two highest frequency peaks in the group delay spectrum and the ratio of the group delay of these frequencies, is extracted. An Accumulated Minimum distance classifier and a Maximum likelihood classifier are trained for each of the phonations separately, and tested to make a decision between normal and hypernasal speakers. For the current study, phonations /a/, /i/, and /u/ uttered by 45 speakers with cleft palate who are expected to produce hypernasal speech, and phonations of 26 normal speakers are considered. Results show that the presence of hypernasality in speech can be detected with 85% of accuracy using the statistical classifiers which use the proposed three dimensional feature vector.	feature vector;group delay and phase delay;low-pass filter;statistical classification;statistical machine translation	P. Vijayalakshmi;M. Ramasubba Reddy	2005			formant;artificial intelligence;speech recognition;filter (signal processing);group delay and phase delay;pattern recognition;maximum likelihood;feature vector;hypernasal speech;classifier (linguistics);computer science;nasal vowel	HCI	-10.465512977509954	-89.82381348473034	94891
0b50fedca5f781054816f6e09166261e7226a4b4	coordination of eyebrow movement with speech acoustics and head movement		Studies on the relationship between eyebrow movement and other aspects of speech production have focused on large, discrete movements of the eyebrows. Using integrated optical and electromagnetic point tracking, we measured eyebrow movements relative to the skull with a high level of precision. These data in combination with a correlational analysis method that accommodates varying phasing between the signals enabled the investigation of the relationship between continuous eyebrow movements, speech acoustics, and head movements. Our results show that there was a correlation between eyebrow movements and speech acoustics, though there was notable variation within and across participants. The relationship between eyebrow and head movements was much closer, with a strong correlation between eyebrow movement and subsequent head movement, which held across participants. We discuss the implications for theories of gestural control in speech production.	high-level programming language;speech synthesis;theory	Kevin D. Roon;Mark K. Tiede;Katherine Dawson;Douglas H. Whalen	2015			speech recognition;eyebrow;psychology;speech acoustics	HCI	-7.739069672067616	-81.58226113383371	94898
460dcd3c648fd37a2b759b9e20c503295901b293	a new approach to designing a feature extractor in speaker identification based on discriminative feature extraction	second order;optimisation;speaker identification;optimizacion;identification locuteur;second order all pass system function;reconocimiento palabra;classification;discriminant analysis;analyse discriminante;analisis discriminante;cepstral analysis;cuantificacion vectorial;vector quantization;analyse cepstrale;discriminative feature extraction;feature extraction;estimacion parametro;mel cepstral estimation;speech recognition;optimization;reconnaissance parole;extraction caracteristique;parameter estimation;estimation parametre;minimum classification error;clasificacion;quantification vectorielle	This paper presents a new framework for designing a feature extractor in a speaker identification system based on the discriminative feature extraction (DFE) method. In order to find the frequency scale appropriate for accurate speaker identification, a mel-cepstral estimation technique using a second-order all-pass warping function is applied to the feature extractor; the frequency warping parameters and the text-independent speaker model parameters are jointly optimized based on a minimum classification error (MCE) criterion. Experimental results show that the frequency scale after optimization is different from traditional Linear/Mel scales and the proposed system outperforms conventional systems in which only the classifier is optimized with the MCE criterion.	feature extraction;randomness extractor;speaker recognition	Chiyomi Miyajima;Hideyuki Watanabe;Keiichi Tokuda;Tadashi Kitamura;Shigeru Katagiri	2001	Speech Communication	10.1016/S0167-6393(00)00079-0	speech recognition;feature extraction;biological classification;computer science;machine learning;pattern recognition;linear discriminant analysis;estimation theory;second-order logic;vector quantization	NLP	-15.063539136999465	-93.52454037402494	94914
0310d55b5c9ccb46463d91e0be9107b96138f01e	a study of the adapting schematics for text-independent online writer identification	training;training data;computational modeling;hidden markov models;feature extraction;adaptation models;data models	In the Gaussian mixture model based online writer identification system, the writer specific models are usually learned by adapting the universal background model. However, among all the possible adapting plans, which one performs best is still an unsolved problem, as well as the underlying principles. Towards finding the answer, this paper analyses all the combinations of the parameter adaptation. The conclusion is that the local and global adapting plan make use of different information for discrimination. Adapting both the local and global aspect is capable of use the both information for discrimination. In the experiments, a study of 5 specific adapting plans is performed in different available test data, relevance factor, number of mixtures and feature set, which shows the advantages of the plan adapting both aspects. On the point based feature set, the weight adapting plan performs better than the mean adapting plan, which is contradict with the current research. On the all point based feature set, the plan adapting both the mean and weight acquired competitive performance with the state-of-the-art.	experiment;mixture model;relevance;schematic;test data	Yabei Wu;Huan-zhang Lu;Zhi-Yong Zhang;Fei Zhao	2016	2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2016.7852856	data modeling;training set;feature extraction;computer science;artificial intelligence;machine learning;data mining;computational model;hidden markov model	ML	-18.846532094652606	-90.48146075571331	94971
3ae58ae095e3c64871bcc67471744dea8daa1658	evaluation framework for distant-talking speech recognition under reverberant environments: newest part of the censrec series -		Recently, speech recognition performance has been drastically improved by statistical methods and huge speech databases. Now performance improvement under such realistic environments as noisy conditions is being focused on. Since October 2001, we from the working group of the Information Processing Society in Japan have been working on evaluation methodologies and frameworks for Japanese noisy speech recognition. We have released frameworks including databases and evaluation tools called CENSREC-1 (Corpus and Environment for Noisy Speech RECognition 1; formerly AURORA-2J), CENSREC-2 (in-car connected digits recognition), CENSREC-3 (in-car isolated word recognition), and CENSREC-1-C (voice activity detection under noisy conditions). In this paper, we newly introduce a collection of databases and evaluation tools named CENSREC-4, which is an evaluation framework for distant-talking speech under hands-free conditions. Distant-talking speech recognition is crucial for a hands-free speech interface. Therefore, we measured room impulse responses to investigate reverberant speech recognition. The results of evaluation experiments proved that CENSREC-4 is an effective database suitable for evaluating the new dereverberation method because the traditional dereverberation process had difficulty sufficiently improving the recognition performance. The framework was released in March 2008, and many studies are being conducted with it in Japan.	database;experiment;information processing;speech recognition;voice activity detection	Takanobu Nishiura;Masato Nakayama;Yuki Denda;Norihide Kitaoka;Kazumasa Yamamoto;Takeshi Yamada;Satoru Tsuge;Chiyomi Miyajima;Masakiyo Fujimoto;Tetsuya Takiguchi;Satoshi Tamura;Shingo Kuroiwa;Kazuya Takeda;Satoshi Nakamura	2008			speech recognition;voice activity detection;artificial intelligence;natural language processing;voxforge;speech technology;information processing;speech processing;computer science;word recognition	NLP	-15.712536759253087	-88.61728829985076	94975
b7adc567faf206075b12e6fa3046363d1871958c	feature selection for robust automatic speech recognition: a temporal offset approach	noise robustness;delta features;automatic speech recognition;feature extraction	Automatic speech recognition relies on extracting features at fixed intervals. In order to enhance these features with dynamical (delta) components, discrete derivatives are usually computed and added as features. However, derivative operations tend to be susceptible to noise. Our proposed method alleviates this problem by replacing these derivatives with nearby features selected on a per-frequency basis. In particular, we noted that, at low frequency, consecutive samples are highly correlated and more information can be gathered by looking at features farther away in time. We thus propose a strategy to perform this frequency-based selection and evaluate it on the Aurora 2 continuousdigits and connected-digits tasks using MFCC, PLPCC and LPCC standard features. The results of our experimentations show that our strategy achieved an average relative improvement of 32.10% in accuracy, with most gains in very noisy environments where the traditional delta features have low recognition rates.	acoustic cryptanalysis;acoustic model;aurora;concatenation;deep learning;experiment;feature selection;image noise;speech recognition;team foundation server;triphone;vocabulary	Ludovic Trottier;Philippe Giguère;Brahim Chaib-draa	2015	I. J. Speech Technology	10.1007/s10772-015-9276-6	speech recognition;feature extraction;computer science;machine learning;pattern recognition	NLP	-13.157880534917052	-91.29489413589273	95341
7b6bb666747fade68cd755b8f95d3a4532e20492	modeling incompletion phenomenon in mandarin dialog prosody	speech synthesis;generic model;indexing terms;general methods	The paper proposes a prosody generation method for dialog speech synthesis in Mandarin. The method is an extension of a prosody model for read speech and also takes the essential characteristic of dialog speech into account. Besides the faster speaking rate and narrower pitch range in dialog speech, our method concentrates on the more underlying and essential characteristic: the incompletion of pitch contour within a syllable and its impacts on adjacent syllables. To simulate this phenomenon, a CART-based method is constructed to predict whether a syllable is incomplete or not. Based on that, a prosody generation model which focuses on the prosody constraint between adjacent syllables is constructed, and this method can simulate the influence of incomplete syllable on adjacent syllables. Experiments show that the synthesized results based on that prosody model sound much natural and colloquial.	experiment;semantic prosody;simulation;speech synthesis;super robot monkey team hyperforce go!;syllable;dialog	Jian Yu;Lixing Huang;Jianhua Tao;Xia Wang	2007			natural language processing;speech recognition;index term;computer science;prosody;speech synthesis	NLP	-13.99632437956372	-83.16016094259362	95475
81430515d901fc4b61b8183998d98d28ea0ae8d8	laryngeal-oral coordination in mixed-voicing clusters	consonant clusters;laryngeal transillumination;laryngeal oral coordination;german	Laryngeal–oral coordination was studied in German clusters of voiceless fricative or plosive plus /l/ or /r/ by means of videofiberendoscopy and transillumination. In all cases voice onset time (i.e. the time from release of C1 to onset of voicing) was longer in the clusters compared to the single fricative or plosive controls. However, the coordination patterns leading to this consistent acoustic effect were quite varied, ranging from a passive effect of aerodynamic conditions at release of C1, via shortening of C1 with constant glottal gesture, to enhancement of the glottal gesture. Active reorganization was particularly clear in the rhotic clusters. For the single consonants the duration of the glottal gesture was quite constant over place of articulation but occlusion duration varied systematically. Accordingly, for both clusters and singletons peak glottal opening did not keep a constant timing relationship to landmarks in the oral occlusion of C1. The above findings were robustly present over a range of prosodic conditions. Prosodic strengthening itself had a particularly clear influence on the magnitude of the devoicing gesture. & 2014 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;biconnected component;onset (audio)	Phil Hoole;Lasse Bombien	2014	J. Phonetics	10.1016/j.wocn.2014.02.004	speech recognition;acoustics;philosophy;german;linguistics;sociology;communication	AI	-10.38786736867805	-81.67090184534905	95549
09a76f55dfd08a5515410c1ef5782ece9795f28b	nature of contrast and coarticulation: evidence from mizo tones and assamese vowel harmony		Tonal coarticulation is universally found to be greater in extent in the carryover direction compared to the anticipatory direction ([1], [2], [3], [4], [5]) leading to assimilatory processes. In general, carryover coarticulation has been understood to be due to intertio-mechanical forces, and, anticipatory effects are seen to be a consequence of parallel activation of articulatory plans ([6]). In this paper, we report on results from a set of Artificial Neural Networks (ANN) trained to predict adjacent tones in disyllabic sequences. Our results confirm the universal pattern of greater carryover effects in Mizo leading to tonal assimilation. In addition, we report on results from single-layered ANNmodels and Support VectorMachines (SVM) that predict the identity of V2 from V1 (anticipatory) consistently better than V1 from V2 (carryover) in Assamese non-harmonic #...V1CV2...# sequences. The directionality in the performance of the V1 and V2 models, help us conclude that the directionality effect of coarticulation in Assamese non-harmonic sequences is greater in the anticipatory direction, which is the same direction as in the harmonic sequences. We argue that coarticulatory propensity exhibits a great deal of sensitivity to the nature of contrast in a language.	artificial neural network;data assimilation;dual-tone multi-frequency signaling;neural networks	Indranil Dutta;S. Irfan;Pamir Gogoi;Priyankoo Sarmah	2017			speech recognition;vowel harmony;coarticulation;linguistics;assamese;computer science	ML	-9.92832992471519	-81.72038954311375	95656
4f7c326294dfee680b73e9d7ddd997de867e6d10	aligning music audio with symbolic scores using a hybrid graphical model	variable cachee;dynamic programming;graphic method;alignement;hidden variable theory;ajustamiento modelo;programacion dinamica;graphical models score matching;steganographie;data interpretation;score following;continuous variable;musica;document musical;modelo hibrido;teoria variable escondida;en continu;acoustique musicale;en continuo;musical score;documento musical;dynamic program;intelligence artificielle;modele hybride;data model;hybrid model;score matching;latent variable model;musical acoustics;ajustement modele;modele variable latente;modelo variable latente;steganography;musique;esteganografia;graphical models;methode graphique;music score following;acustica musical;hidden variables;model matching;theorie variable cachee;programmation dynamique;alineamiento;graphical model;metodo grafico;artificial intelligence;modele donnee;inteligencia artificial;music;alignment;continuous process;data models	We present a new method for establishing an alignment between a polyphonic musical score and a corresponding sampled audio performance. The method uses a graphical model containing both latent discrete variables, corresponding to score position, as well as a latent continuous tempo process. We use a simple data model based only on the pitch content of the audio signal. The data interpretation is defined to be the most likely configuration of the hidden variables, given the data, and we develop computational methodology to identify or approximate this configuration using a variant of dynamic programming involving parametrically represented continuous variables. Experiments are presented on a 55-minute hand-marked orchestral test set.	approximation algorithm;data model;dynamic programming;essence;experiment;explicit modeling;forward–backward algorithm;graphical model;hidden markov model;hidden variable theory;language model;markov chain;online and offline;onset (audio);pattern matching;real-time clock;smoothing;springer (tank);test set	Christopher Raphael	2006	Machine Learning	10.1007/s10994-006-8415-3	speech recognition;computer science;artificial intelligence;machine learning;mathematics;graphical model;hidden variable theory	ML	-16.491594022228607	-83.84338478282932	95663
17c9a0f1a287c08bb2c1c1df47fa51ce1e428c4e	multi-microphone speech recognition integrating beamforming, robust feature extraction, and advanced dnn/rnn backend		This paper gives an in-depth presentation of the multi-microphone speech recognition system we submitted to the 3rd CHiME speech separation and recognition challenge (CHiME-3) and its extension. The proposed system takes advantage of recurrent neural networks (RNNs) throughout the model from the front-end speech enhancement to the language modeling. Three different types of beamforming are used to combine multi-microphone signals to obtain a single higher-quality signal. The beamformed signal is further processed by a single-channel long short-term memory (LSTM) enhancement network, which is used to extract stacked mel-frequency cepstral coefficients (MFCC) features. In addition, the beamformed signal is processed by two proposed noise-robust feature extraction methods. All features are used for decoding in speech recognition systems with deep neural network (DNN) based acoustic models and large-scale RNN language models to achieve high recognition accuracy in noisy environments. Our training methodology includes multi-channel noisy data training and speaker adaptive training, whereas at test time model combination is used to improve generalization. Results on the CHiME-3 benchmark show that the full set of techniques substantially reduced the word error rate (WER). Combining hypotheses from different beamforming and robust-feature systems ultimately achieved 5.05% WER for the real-test data, an 84.7% reduction relative to the baseline of 32.99% WER and a 44.5% reduction from our official CHiME-3 challenge result of 9.1% WER. Furthermore, this final result is better than the best result (5.8% WER) reported in the CHiME-3 challenge. 2017 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;artificial neural network;baseline (configuration management);beamforming;benchmark (computing);coefficient;deep learning;feature extraction;language model;long short-term memory;mel-frequency cepstrum;microphone;random neural network;recurrent neural network;signal-to-noise ratio;speech enhancement;speech recognition;test data;word error rate	Takaaki Hori;Zhuo Chen;Hakan Erdogan;John R. Hershey;Jonathan Le Roux;Vikramjit Mitra;Shinji Watanabe	2017	Computer Speech & Language	10.1016/j.csl.2017.01.013	machine learning;computer science;speech recognition;word error rate;artificial neural network;artificial intelligence;feature extraction;beamforming;mel-frequency cepstrum;speech enhancement;recurrent neural network;language model;pattern recognition	ML	-15.842880155863913	-89.61836777733399	96002
38e260d88e7ffc2a426d4032f67203b0370d7a09	multiple feature combination to improve speaker diarization of telephone conversations	broadcast news;pattern clustering;speaker identification;diarization error rate multiple feature combination speaker diarization process telephone conversations multistage segmentation clustering system broadcast news acoustic change point detection algorithm iterative viterbi re segmentation gender labeling agglomerative clustering bayesian information criterion state of the art speaker identification methods gaussian mixture models;gaussian processes;bayes methods;speaker recognition bayes methods error statistics feature extraction gaussian processes gender issues iterative methods pattern clustering;speaker recognition;iterative methods;sid clustering speaker diarization speaker segmentation and clustering bic clustering;gaussian mixture model;gender issues;telephony loudspeakers viterbi algorithm density estimation robust algorithm labeling error analysis testing broadcasting detection algorithms iterative methods;feature extraction;sid clustering;cluster system;speaker segmentation and clustering;error rate;error statistics;bic clustering;speaker diarization;bayesian information criterion;change point detection	We report results on speaker diarization of telephone conversations. This speaker diarization process is similar to the multistage segmentation and clustering system used in broadcast news. It consists of an initial acoustic change point detection algorithm, iterative Viterbi re-segmentation, gender labeling, agglomerative clustering using a Bayesian information criterion (BIC), followed by agglomerative clustering using state-of-the-art speaker identification methods (SID) and Viterbi re-segmentation using Gaussian mixture models (GMMs). The Viterbi re-segmentation using GMMs is new, and it reduces the diarization error rate (DER) by 10%. We repeat these multistage segmentation and clustering steps twice: once with MFCCs as feature parameters for the GMMs used in gender labeling, SID and Viterbi re-segmentation steps, and another time with Gaussianized MFCCs as feature parameters for the GMMs used in these three steps. The resulting clusters from the parallel runs are combined in a novel way that leads to a significant reduction in the DER. On a development set containing 30 telephone conversations, this combination step reduced the DER by 20%. On another test set containing 30 telephone conversations, this step reduced the DER by 13%. The best error rate we have achieved is 6.7% on the development set, and 9.0% on the test set.	acoustic cryptanalysis;algorithm;bayesian information criterion;cluster analysis;iterative method;mixture model;multistage amplifier;speaker diarisation;speaker recognition;test set	Vishwa Gupta;Patrick Kenny;Pierre Ouellet;Gilles Boulianne;Pierre Dumouchel	2007	2007 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)	10.1109/ASRU.2007.4430198	speaker recognition;speaker diarisation;speech recognition;feature extraction;word error rate;computer science;machine learning;pattern recognition;mixture model;gaussian process;iterative method;bayesian information criterion;change detection;statistics	NLP	-17.693648496963128	-93.43456793083938	96112
1266256d30de962208aa0b2208440f2c9622e74c	entropy of teager energy in wavelet-domain algorithm applied in note onset detection	adaptive sub band spectrum entropy;instruments;note segmentation;signal analysis;anti noise performance;note onset detection algorithm;spectrum;entropy wavelet transforms feature extraction instruments multiple signal classification noise algorithm design and analysis;wavelet domain algorithm;wavelet transforms;wavelet transforms entropy;entropy note segmentation wavelet transform teager energy;background music;multiple signal classification;wavelet transform;feature extraction;teager energy;teager energy operator;multiresolution;content based musical signal analysis;entropy;onset detection;information entropy;multi resolution;detection curve;background music wavelet domain algorithm note segmentation content based musical signal analysis multiresolution wavelet transform anti noise performance teager energy operator information entropy note onset detection algorithm adaptive sub band spectrum entropy detection curve;algorithm design;algorithm design and analysis;noise	Note segmentation is a crucial step in content-based musical signal analysis and processing. Considering the characters of multi-resolution of wavelet transform, anti-noise performance of TEO (Teager Energy Operator) and good statistical performance of information entropy, this paper combined this three features and proposed a novel note onset detection algorithm—Entropy of Teager Energy in Wavelet-domain (ETEW). Compared with the Adaptive Sub-band Spectrum Entropy (ASSE) which was a typical and effective note onset detection algorithm, the detection curve obtained from ETEW was smoother and the note boundaries were more obvious, which led to a 10% increase in the note segmentation accuracy. Especially for pieces played by percussion instruments, the results would be better. The experiment data set contained several groups played by 7 different kinds of instruments and had 2000 notes in total. Experiments indicated that the advantages of ETEW became much prominent when pieces were played by a variety of instruments or accompanied by background music. What's more, the anti-noise performance was improved in a great extent especially with lower SNR.	algorithm;entropy (information theory);experiment;onset (audio);signal processing;signal-to-noise ratio;universality probability;wavelet transform	Yanan Feng;Qiang Li;Xin Guan	2011	2011 Seventh International Conference on Natural Computation	10.1109/ICNC.2011.6022503	mathematical optimization;speech recognition;pattern recognition;mathematics	Robotics	-7.926695801990614	-90.79998661825523	96174
a3b3fe4adf7283c17511726bc80eb58ea1933ef8	applying unsupervised learning to support vector space model based speaking assessment		Vector Space Models (VSM) have been widely used in the language assessment field to provide measurements of students’ vocabulary choices and content relevancy. However, training reference vectors (RV) in a VSM requires a time-consuming and costly human scoring process. To address this limitation, we applied unsupervised learning methods to reduce or even eliminate the human scoring step required for training RVs. Our experiments conducted on data from a non-native English speaking test suggest that the unsupervised topic clustering is better at selecting responses to train RVs than random selection. In addition, we conducted an experiment to totally eliminate the need of human scoring. Instead of using human rated scores to train RVs, we used used the machine-predicted scores from an automated speaking assessment system for training RVs. We obtained VSM-derived features that show promisingly high correlations to human-holistic scores, indicating that the costly human scoring process can be eliminated.	cluster analysis;experiment;holism;relevance;synapomorphy;unsupervised learning;viable system model;vocabulary	Lei Chen	2013			speech recognition;machine learning;data mining;statistics	AI	-17.86423638485132	-80.43907305238051	96238
34465298945e3b204873c71460fac6060d15ae4a	variational bayesian model selection for gmm-speaker verification using universal background model		In this paper we propose to use Variational Bayesian Analysis (VBA) instead of Maximum Likelihood (ML) estimation for Universal Background Model (UBM) building in GMM text independent speaker verification systems. Using VBA estimation solves the problem of the optimal choice of the UBM mixture dimensionality for the training data set, as well as the problem of noise Gaussians which are typical for ML estimation. Experiments using the NIST 2006 and 2008 SRE datasets (cellular channels only) demonstrate superior efficiency of baseline verification systems with a UBM trained using the VBA method compared to standard ML training. Verification error was reduced by almost 8%, compared to a baseline system with standard ML training for the UBM.	baseline (configuration management);bayes factor;bayesian network;google map maker;model selection;speaker recognition;standard ml;test set;variational principle;visual basic for applications	Timur Pekhovsky;Alexandra Lokhanova	2011			maximum likelihood;artificial intelligence;curse of dimensionality;nist;training set;pattern recognition;computer science;machine learning;standard ml;bayesian probability;bayesian inference	NLP	-17.513366254515972	-91.52888069753799	96430
2fb55f35e6e46b5ff24920808252e3608dc4e999	a diagnostic and rehabilitation aid workstation for speech and voice pathologies			workstation	Bernard Teston;Benoit Galindo	1995			speech recognition;rehabilitation;computer science;workstation	NLP	-6.788541570446262	-84.26058227544063	96627
85ee5f950529d484894bae910f620fab5ef7f725	speaker verification without background speaker models	biometrics access control;databases speaker recognition information science cities and towns information security statistical analysis error analysis nist automatic speech recognition speech processing;speaker verification;speaker recognition;tolerance analysis;statistical analysis;telecommunication security;error rate;voice access control speaker verification background speaker models authorized speaker security applications tolerance interval analysis statistics error rates gmm gaussian mixture model benchmark databases tcc 300 timit nist 2000;learning artificial intelligence;security of data;interval analysis;biometrics access control speaker recognition security of data telecommunication security tolerance analysis statistical analysis learning artificial intelligence	Speaker verification concerns the problem of verifying whether a given utterance has been pronounced by a claimed authorized speaker. This problem is important because an accurate speaker verification system can be applied to many security applications. In this paper, we present a new algorithm for speaker verification called OSCILLO. By applying tolerance interval analysis in statistics, OSCILLO can verify a speaker’s ID without background speaker models. This greatly reduces the space requirement of the system and the time for both training and verification. Experimental results show that OSCILLO can achieve error rates comparable or better than the GMM-based system with background speaker models for three benchmark databases: TCC-300, TIMIT and NIST 2000.	algorithm;authorization;benchmark (computing);consistency model;database;google map maker;interval arithmetic;speaker recognition;timit;verification and validation	Chun-Nan Hsu;Hau-Chung Yu;Bo-Hou Yang	2003		10.1109/ICASSP.2003.1202337	natural language processing;speaker recognition;speaker diarisation;speech recognition;word error rate;computer science;pattern recognition	AI	-15.756180650270888	-92.78338501204257	96717
679eccec5830d0797bc9894d0ee295258f340d8c	evaluating the robustness of privacy-sensitive audio features for speech detection in personal audio log scenarios	zero crossing rate;speech nonspeech detection privacy sensitive features;front end;nist;kurtosis;spectral flatness;speech processing;speech nonspeech detection;speech;acoustic signal processing;speech processing acoustic signal processing audio acoustics;privacy sensitive audio features;feature combinations privacy sensitive audio features speech detection personal audio log scenarios robust front end processing speech nonspeech detection zero crossing rate spectral flatness kurtosis;feature extraction;robust front end processing;robustness;entropy;context modeling;robustness computer vision speech analysis context modeling microphones audio recording privacy speech processing pattern analysis face detection;privacy sensitive features;context;audio acoustics;feature combinations;speech detection;personal audio log scenarios	Personal audio logs are often recorded in multiple environments. This poses challenges for robust front-end processing, including speech/nonspeech detection (SND). Motivated by this, we investigate the robustness of four different privacy-sensitive features for SND, namely energy, zero crossing rate, spectral flatness, and kurtosis. We study early and late fusion of these features in conjunction with modeling temporal context. These combinations are evaluated in mismatched conditions on a dataset of nearly 450 hours. While both combinations yield improvements over individual features, generally feature combinations perform better. Comparisons with a state-of-the-art spectral based and a privacy-sensitive feature set are also provided.	privacy;spectral flatness;zero crossing;zero-crossing rate	Sree Hari Krishnan Parthasarathi;Mathew Magimai-Doss;Hervé Bourlard;Daniel Gatica-Perez	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495596	entropy;speech recognition;nist;feature extraction;computer science;speech;front and back ends;kurtosis;speech processing;context model;robustness;spectral flatness	Visualization	-12.26200720675591	-92.24143530189491	96739
269b7b2cfd21d1f3a76352056cb6a6c41d2025b2	waveguide physical modeling of vocal tract acoustics: flexible formant bandwidth control from increased model dimensionality	modelizacion;resonador;acoustic wave scattering;waveguide;impedancia acustica;speech synthesis;formant;propagation onde;bandwidth allocation;modele lineaire;vocal tract;diffusion onde;acoustical resonators digital waveguide physical modeling vocal tract acoustics flexible formant bandwidth control kelly lochbaum tract model wave scattering mechanisms two dimensional waveguide mesh model;sistema complejo;modelo lineal;difusion onda;vocal;acoustic waveguides acoustic waves bandwidth human voice speech synthesis acoustic scattering acoustic propagation waveguide components natural languages impedance;modelisation;vocal system acoustic resonators acoustic waveguides speech synthesis;impedance acoustique;canal vocal;vocal system;systeme complexe;conducto vocal;complex system;propagacion onda;modelo 2 dimensiones;guide onde;methode maille;mesh method;linear model;wave scattering;modele 2 dimensions;acoustic impedance;resonator;voyelle;metodo malla;sintesis palabra;bandwidth allocation acoustic waveguides acoustic resonators acoustic wave scattering speech synthesis;physical model;wave propagation;formante;guia onda;vowel;modeling;resonateur;acoustic waveguides;two dimensional model;synthese parole;acoustic resonators	Digital waveguide physical modeling is often used as an efficient representation of acoustical resonators such as the human vocal tract. Building on the basic one-dimensional (1-D) Kelly-Lochbaum tract model, various speech synthesis techniques demonstrate improvements to the wave scattering mechanisms in order to better approximate wave propagation in the complex vocal system. Some of these techniques are discussed in this paper, with particular reference to an alternative approach in the form of a two-dimensional waveguide mesh model. Emphasis is placed on its ability to produce vowel spectra similar to that which would be present in natural speech, and how it improves upon the 1-D model. Tract area function is accommodated as model width, rather than translated into acoustic impedance, and as such offers extra control as an additional bounding limit to the model. Results show that the two-dimensional (2-D) model introduces approximately linear control over formant bandwidths leading to attainable realistic values across a range of vowels. Similarly, the 2-D model allows for application of theoretical reflection values within the tract, which when applied to the 1-D model result in small formant bandwidths, and, hence, unnatural sounding synthesized vowels.	acoustic coupler;acoustic cryptanalysis;approximation algorithm;automatic sounding;bandwidth management;characteristic impedance;coefficient;control system;high-level programming language;kelly criterion;natural language;real life;simulation;software propagation;speech synthesis;tract (literature)	J. Mullen;David M. Howard;Damian T. Murphy	2006	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TSA.2005.858052	vocal tract;waveguide;acoustic impedance;speech recognition;systems modeling;formant;acoustics;physical model;wave propagation;linear model;linguistics;resonator;speech synthesis;bandwidth allocation	Graphics	-8.76111383224239	-86.21373277276956	96769
a77dc94dbc13b8277a2e48340a8025a497b6817f	automatic assessment of speech capability loss in disordered speech	goodness of pronunciation;traitement du signal et de l image;intelligence artificielle;vision par ordinateur et reconnaissance de formes;automatic assessment of pronunciation;disordered speech;traitement des images;synthese d image et realite virtuelle	In this article, we report on the use of an automatic technique to assess pronunciation in the context of several types of speech disorders. Even if such tools already exist, they are more widely used in a different context, namely, Computer-Assisted Language Learning, in which the objective is to assess nonnative pronunciation by detecting learners’ mispronunciations at segmental and/or suprasegmental levels. In our work, we sought to determine if the Goodness of Pronunciation (GOP) algorithm, which aims to detect phone-level mispronunciations by means of automatic speech recognition, could also detect segmental deviances in disordered speech. Our main experiment is an analysis of speech from people with unilateral facial palsy. This pathology may impact the realization of certain phonemes such as bilabial plosives and sibilants. Speech read by 32 speakers at four different clinical severity grades was automatically aligned and GOP scores were computed for each phone realization. The highest scores, which indicate large dissimilarities with standard phone realizations, were obtained for the most severely impaired speakers. The corresponding speech subset was manually transcribed at phone level; 8.3% of the phones differed from standard pronunciations extracted from our lexicon. The GOP technique allowed the detection of 70.2% of mispronunciations with an equal rate of about 30% of false rejections and false acceptances. Finally, to broaden the scope of the study, we explored the correlation between GOP values and speech comprehensibility scores on a second corpus, composed of sentences recorded by six people with speech impairments due to cancer surgery or neurological disorders. Strong correlations were achieved between GOP scores and subjective comprehensibility scores (about 0.7 absolute). Results from both experiments tend to validate the use of GOP to measure speech capability loss, a dimension that could be used as a complement to physiological measures in pathologies causing speech disorders.	algorithm;experiment;lexicon;sensor;speech recognition	Thomas Pellegrini;Lionel Fontan;Julie Mauclair;Jérôme Farinas;Charlotte Alazard-Guiu;Marina Robert;Peggy Gatignol	2015	TACCESS	10.1145/2739051	natural language processing;speech recognition;computer science;artificial intelligence;linguistics	NLP	-10.535479190206747	-85.20554423637603	96790
45ad537d4ea82a3d4b4200d32b4ce3b5ddd3d9cf	temporal structures for fast and slow speech rate	speech synthesis;speech	The rhythmic component in speech synthesis often remains rather rudimentary, despite recent major efforts in the modeling of prosodic models. The European COST Action 258 has identified this problem as one of the next challenges for speech synthesis. This paper is a contribution to a new, promising approach that was tested on a French temporal model.	speech synthesis	Brigitte Zellner	1998			speech recognition;acoustics;computer science;communication	NLP	-13.426930262210593	-84.00435062240699	96808
4c513e2526c9b03c2dbd808a76cc56f00d8dc61f	music genre classification using novel features and a weighted voting method	signal classification audio signal processing music;modulation spectral flatness measure;time varying;music genre classification;audio signal processing;weighted voting method;support vector machines;music support vector machine classification modulation support vector machines accuracy statistical analysis mel frequency cepstral coefficient;time window;statistical method;octave based modulation spectral contrast;mel frequency cepstral coefficient;accuracy;modulation spectral crest measure;statistical analysis;beat strength;consecutive time segments;majority voting music genre classification weighted voting method modulation spectral flatness measure modulation spectral crest measure time varying behavior beat strength consecutive time segments octave based modulation spectral contrast short feature vector statistical method;signal classification;time varying behavior;short feature vector;support vector machine classification;majority voting;music;modulation	This paper proposes a novel music genre classification system based on two novel features and a weighted voting. The proposed features, modulation spectral flatness measure (MSFM) and modulation spectral crest measure (MSCM), represent the time-varying behavior of a music and indicate the beat strength. The weighted voting method determines the music genre by summarizing the classification results of consecutive time segments. Experimental results show that the proposed features give more accurate classification results when combined with traditional features than the octave-based modulation spectral contrast (OMSC) does in spite of short feature vector and that the weighted voting is more effective than statistical method and majority voting.	feature vector;gnu octave;modulation;spectral efficiency;spectral flatness	Dalwon Jang;Minho Jin;Chang Dong Yoo	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607700	speech recognition;audio signal processing;computer science;machine learning;pattern recognition;music;mathematics;statistics;modulation	Robotics	-8.366004858398853	-91.15311078239928	96839
68fc7330aaf403f73098ebaf43f62279da1ff517	"""""""excuse meeee!!"""": (mis)coordination of lexical and paralinguistic prosody in l2 hyperarticulation"""		Abstract The present study investigates how first language (L1) and second language (L2) speakers coordinate lexical and paralinguistic prosody (lexically determined pitch accent and utterance-level intonation) in hyperarticulated speech. Modifications of F 0 and segmental durations in repeated utterances were analyzed jointly using a novel method, Functional Principal Component Analysis (FPCA). By testing Japanese and German participants as L1 and as L2 speakers of the respective languages, the task elicited a situation in which L2 speakers had to handle F 0 and segmental durations when these prosodic properties mainly carry a paralinguistic function in their L1, but carry a lexical one in their L2 or vice versa. Results show language-dependent and -independent modifications in hyperarticulated speech in both speaker groups. The language-dependent modifications were negatively transferred to the respective L2. The restriction of Japanese lexical pitch accent prevented Japanese participants from phonologically modifying F 0 contours in L1 and L2, and German L2 speakers applied the German pitch accent rules in L2 Japanese utterances. Lexical rules outweighed paralinguistic rules in modifying prosody. At the same time, both German and Japanese speakers hyperarticulated the utterances with utterance-final lengthening, larger pitch range and higher peak. Furthermore, we argue that there may be different dominance orders of prosodic properties in Japanese and German according to which prosodic property was more likely to be modified in hyperarticulated speech. L2 speakers were more prone to change a more dominant prosodic property in their L1 to convey a paralinguistic meaning, i.e., segmental durations for Japanese and F 0 for German. The findings are particularly noteworthy as the uttered words were highly frequent words of which L2 learners should have had sufficient L2 input before.	semantic prosody	Yuki Asano;Michele Gubian	2018	Speech Communication	10.1016/j.specom.2017.12.011	speech recognition;paralanguage;first language;computer science;versa;prosody;pitch accent;german	NLP	-10.753150101188028	-81.55484545849916	97110
bb742cfd57e924c92199eaaac97d58fab5318d2c	gated convolutional lstm for speech commands recognition		As the mobile device gaining increasing popularity, Acoustic Speech Recognition on it is becoming a leading application. Unfortunately, the limited battery and computational resources on a mobile device highly restrict the potential of Speech Recognition systems, most of which have to resort to a remote server for better performance. To improve the performance of local Speech Recognition, we propose C-1G-2-Blstm. This model shares Convolutional Neural Network’s ability of learning local feature and Recurrent Neural Network’s ability of learning sequence data’s long dependence. Furthermore, by adopting the Gated Convolutional Neural Network instead of a traditional CNN, we manage to greatly improve the models capacity. Our tests demonstrate that C-1G-2-Blstm can achieve a high accuracy at 90.6% on the Google Speech Commands data set, which is 6.4% higher than the state-of-art methods.	computational complexity theory;computational resource;convolution;convolutional neural network;experiment;hands-free computing;long short-term memory;mobile device;preprocessor;random neural network;recurrent neural network;server (computing);speech recognition	Shaohe Lv;Xiaodong Wang;Xinye Lin	2018		10.1007/978-3-319-93701-4_53	distributed computing;convolutional neural network;computer science;mobile device;machine learning;recurrent neural network;restrict;artificial intelligence	NLP	-18.173413348865065	-88.64581343538292	97193
0023affcf52ea5f175f7d759d5200cb191fe94f8	place of articulation cues for voiced and voiceless plosives and fricatives in syllable-initial position		In this paper, the acoustic correlates of the labial and alveolar place of articulation for both plosive and fricative consonants are investigated, and the results are analyzed in terms of vowel context, voicing and manner of articulation. Several measurements, including formant and noise measurements, are reported for CVs spoken by two male and two female talkers. It was found that the spectral amplitude of frication noise relative to F1 at vowel onset results in 84% or better correct classi cation for the fricatives in 3 vowel contexts. For plosives, a measure which quanti es the amplitude of noise at high frequencies relative to F1 at vowel onset (Av-Ahi [8]) resulted in 81 % or better correct classi cation in the three vowel contexts. Formant frequency cues, on the other hand, were not reliable measures for all vowel contexts.	ahi (amiga);av-test;acoustic cryptanalysis;biconnected component;concurrent versions system;onset (audio);stellar classification;syllable	Willa S. Chen;Abeer Alwan	2000			speech recognition;syllable;place of articulation;computer science	NLP	-10.275570416292789	-82.36149445101194	97212
f7e27ca381905568fc7d2a2fdc809976ac31a4a3	visual feedback applied to the learning of conscious pitch control in singing				David M. Howard;Graham F. Welch	1989			speech recognition;pitch control;computer science;singing	ML	-7.40266498191542	-83.4309085315621	97236
32f3b96e595c12290b181c54b88dbacb9ce53ea5	the beat spectrum: a new approach to rhythm analysis	music retrieval;rhythm;rhythm visualization vectors cepstral analysis laboratories html transform coding mel frequency cepstral coefficient psychology euclidean distance;spectrum;euclidean distance;psychology;transform coding;automatic generation;mel frequency cepstral coefficient;html;visualization;cepstral analysis;vectors;time lag	We introduce the beat spectrum, a new method of automatically characterizing the rhythm and tempo of music and audio. The beat spectrum is a measure of acoustic self-similarity as a function of time lag. Highly structured or repetitive music will have strong beat spectrum peaks at the repetition times. This reveals both tempo and the relative strength of particular beats, and therefore can distinguish between different kinds of rhythms at the same tempo. We also introduce the beat spectrogram which graphically illustrates rhythm variation over time. Unlike previous approaches to tempo analysis, the beat spectrum does not depend on particular attributes such as energy or frequency, and thus will work for any music or audio in any genre. We present tempo estimation results which are accurate to within 1% for a variety of musical genres. This approach has a variety of applications, including music retrieval by similarity and automatically generating music videos.	acoustic cryptanalysis;self-similarity;spectrogram	Jonathan Foote;Shingo Uchihashi	2001	IEEE International Conference on Multimedia and Expo, 2001. ICME 2001.	10.1109/ICME.2001.1237863	spectrum;counting;transform coding;speech recognition;visualization;html;computer science;rhythm;euclidean distance;mathematics;beat detection	Vision	-7.592949843920419	-92.16677977595681	97250
9367f49c85b537793917b4788794e5ded435d6d8	state-transition interpolation and map adaptation for hmm-based dysarthric speech recognition	transition probability matrix;speaker-adapted system;transition probabilities result;speaker-dependent system;map adaptation;higher word error rate;left-to-right structure;left-to-right hmms;dysarthria severity;hmm-based dysarthric speech recognition;state-transition interpolation;transition probability	This paper describes the results of our experiments in building speaker-adaptive recognizers for talkers with spastic dysarthria. We study two modifications – (a) MAP adaptation of speaker-independent systems trained on normal speech and, (b) using a transition probability matrix that is a linear interpolation between fully ergodic and (exclusively) leftto-right structures, for both speaker-dependent and speaker-adapted systems. The experiments indicate that (1) for speaker-dependent systems, left-to-right HMMs have lower word error rate than transition-interpolated HMMs, (2) adapting all parameters other than transition probabilities results in the highest recognition accuracy compared to adapting any subset of these parameters or adapting all parameters including transition probabilities, (3) performing both transition-interpolation and adaptation gives higher word error rate than performing adaptation alone and, (4) dysarthria severity is not a sufficient indicator of the relative performance of speakerdependent and speaker-adapted systems.	bit error rate;emoticon;ergodicity;experiment;finite-state machine;hidden markov model;intelligibility (philosophy);linear interpolation;markov chain;spastic;speech recognition;stochastic matrix;vocabulary;word error rate	Harsh Vardhan Sharma;Mark Hasegawa-Johnson	2010			speech recognition;computer science;pattern recognition	ML	-17.32032105287997	-85.33893935154025	97259
5d431ecbcb58be955c76268855f3a89916622cd5	error spotting using syllabic fillers in spontaneous conversational speech recognition	speech recognition	Spontaneous conversational phone-call speech databases are difficult to recognize because of the large variation of speech rates, of pronunciations as well as noises, of acoustic degradations from the telephone channel, and of an unpredictible non-grammatical language structure including many random phenomena. Each cause of misrecognition can be addressed separately; however there is still no satisfying solution. As a misrecognition is considered by the system as being a kind of new word, we propose to apply here our keyword spotting and new-word detection technology.. However because of the large variety of types of misrecognitions and of the lack of information on where, why and how they occur, we had to define a different language model from those used in previous work. Results show a noticeable recognition improvement, often associated with a decrease in the number of substitutions and a slight increase in the number of the deletions.	acoustic cryptanalysis;database;language model;protologism;speech recognition;spontaneous order	Rachida El Méliani;Douglas D. O'Shaughnessy	1999			speech recognition;syllabic verse;natural language processing;spotting;computer science;artificial intelligence	NLP	-13.07936521107863	-81.74148653677842	97373
093a55099c5a78e9a88b77ab02c7ffa171cfc725	contrast and covert contrast: the phonetic development of voiceless sibilant fricatives in english and japanese toddlers	acoustic analysis	This paper examines the acoustic characteristics of voiceless sibilant fricatives in English-and Japanese-speaking adults and the acquisition of contrasts involving these sounds in 2- and 3-year-old children. Both English and Japanese have a two-way contrast between an alveolar fricative (/s/), and a postalveolar fricative (/∫/ in English and /ɕ/ in Japanese). Acoustic analysis of the adult productions revealed cross-linguistic differences in what acoustic parameters were used to differentiate the two fricatives in the two languages and in how well the two fricatives were differentiated by the acoustic parameters that were investigated. For the children's data, the transcription results showed that English-speaking children generally produced the alveolar fricative more accurately than the postalveolar one, whereas the opposite was true for Japanese-speaking children. In addition, acoustic analysis revealed the presence of covert contrast in the productions of some English-speaking and some Japanese-speaking children. The different development patterns are discussed in terms of the differences in the fine phonetic detail of the contrast in the two languages.	acoustic cryptanalysis;bronchioloalveolar adenocarcinoma;languages;linguistics;speaking (activity);toddler (age group);transcription (software)	Fangfang Li;Jan Edwards;Mary E. Beckman	2009	Journal of phonetics	10.1016/j.wocn.2008.10.001	psychology;speech recognition;acoustics;philosophy;sociology;communication	NLP	-10.84532354105418	-81.86860200998545	97376
feec85c7b5b47cf57a748fce64ecdf6bc558e6d2	effects of intonational phrase boundaries on pitch-accented syllables in american english.	intonational phrase;american english	Recent studies of the acoustic correlates of various prosodic elements in American English, such as prominence (in the form of phrase-level pitch accents and word-level lexical stress) and boundaries (in the form of boundary-marking tones), have begun to clarify the nature of the acoustic cues to different types and levels of these prosodic markers. This study focuses on the importance of controlling for context in such investigations, illustrating the effects of adjacent context by examining the cues to H* and L* pitch accent in early and late position in the Intonational Phrase, and how these cues vary when the accented syllable is followed immediately by boundary tones. Results show that F0 peaks for H* accents occur significantly earlier in words that also carry boundary tones, and that energy patterns are also affected; some effects on voice quality measures were also noted. Such findings highlight the caveat that the context of a particular prosodic target may significantly influence its acoustic correlates.	acoustic cryptanalysis;item unique identification;pitch (music);syllable	Yen-Liang Shue;Stefanie Shattuck-Hufnagel;Markus Iseli;Sun-Ah Jun;Nanette Veilleux;Abeer Alwan	2008			computer science;linguistics	HCI	-10.931378104449365	-81.49183511114673	97396
e81fc623b668c953007b85a7ab0f7bb1f99e0df4	rapid speaker adaptation using model prediction	distributions;adaptation data;context dependent hmm;bayes methods;speech processing;vocabulary;resource manager;resource management;bayesian methods;regression based model prediction;speaker independent linear relationships;single adaptation sentence speaker independent linear relationships rapid speaker adaptation maximum information adaptation data context dependent hmm regression based model prediction training data distributions bayesian techniques parameter estimation arpa resource management corpus error rate reduction;training data;error analysis;adaptation model;single adaptation sentence;ear;hidden markov models;adaptive signal processing;statistical analysis;prediction theory;speaker independent;prediction theory speech recognition speech processing hidden markov models statistical analysis parameter estimation bayes methods adaptive signal processing;map estimation;error rate;speech recognition;arpa resource management corpus;context dependent;predictive models;parameter estimation;maximum information;rapid speaker adaptation;adaptation model predictive models hidden markov models parameter estimation vocabulary bayesian methods error analysis ear training data resource management;speaker adaptation;error rate reduction;bayesian techniques	A key issue in speaker adaptation is gaining the maximum information from a limited amount of adaptation data. In particular it is important that observations of parameters of (context-dependent) HMMs not occurring in the adaptation data can be updated. In the regression-based model prediction (RMP) approach, sets of speaker-independent linear relationships between different parameters in the HMM set are found from training data. During adaptation, distributions with sufficient adaptation data are used to update the parameters of poorly adapted models using these pre-computed regression-based relationships. The method used Bayesian techniques to combine parameter estimates from different sources. Evaluation on the ARPA Resource Management corpus gave a worthwhile reduction in error rate with just a single adaptation sentence, and that RMP consistently outperforms MAP estimation with the same amount of adaptation data.		S. M. Ahadi;Philip C. Woodland	1995		10.1109/ICASSP.1995.479786	adaptive filter;distribution;training set;speech recognition;bayesian probability;word error rate;computer science;resource management;machine learning;context-dependent memory;pattern recognition;predictive modelling;estimation theory;statistics	NLP	-18.642429239573808	-91.2941786866226	97508
72c10159e0bda462ffb806d98f820b183ea03048	acoustic analysis for automatic speech recognition	digital signal processing;reliability;speech processing information processing speech recognition pattern recognition spectral analysis automatic speech recognition time frequency analysis digital signal processing;acoustic analysis;speech processing;speech analysis;vocal cord vibration rate;speech perception;human speech production;asr;automatic speech recognition;time frequency representation automatic speech recognition digital signal processing pattern recognition spectral analysis speech analysis;time frequency representation;information processing;pattern recognition;speech recognition;speech recognition reliability;human speech communication;channel degradations;spectral analysis;time frequency analysis;speech information processing acoustic analysis automatic speech recognition pattern recognition application asr human speech production speech perception speech spectral envelopes reliability channel degradations human speech communication vocal cord vibration rate;pattern recognition application;speech information processing;speech spectral envelopes	As a pattern recognition application, automatic speech recognition (ASR) requires the extraction of useful features from its input signal, speech. To help determine relevance, human speech production and acoustic aspects of speech perception are reviewed, to identify acoustic elements likely to be most important for ASR. Common methods of estimating useful aspects of speech spectral envelopes are reviewed, from the point of view of efficiency and reliability in mismatched conditions. Because many speech inputs for ASR have noise and channel degradations, ways to improve robustness in speech parameterization are analyzed. While the main focus in ASR is to obtain spectral envelope measures, human speech communication efficiently exploits the manipulation of one's vocal-cord vibration rate [fundamental frequency (F0)], and so F0 extraction and its integration into ASR are also reviewed. For the acoustic analysis reviewed here for ASR, this work presents modern methods as well as future perspectives on important aspects of speech information processing.	acoustic cryptanalysis;automated system recovery;information processing;pattern recognition;relevance;speech recognition;speech synthesis	Douglas D. O'Shaughnessy	2013	Proceedings of the IEEE	10.1109/JPROC.2013.2251592	voice activity detection;speech recognition;speech perception;time–frequency analysis;acoustics;information processing;computer science;digital signal processing;time–frequency representation;reliability;speech processing;acoustic model	NLP	-9.302232827550304	-89.27068941296743	97651
75fbd97cd5aa324b2b6223b4c96bb89e9dc0d839	a fuzzy decision tree-based duration model for standard yorùbá text-to-speech synthesis	duration model;classification and regression tree;text to speech synthesis;root mean square error;text to speech;computational linguistics;fuzzy decision tree;linguistique informatique;qualitative evaluation	In this paper, we present syllable-based duration modelling in the context of a prosody model for Standard Yoruba (SY) text-to-speech (TTS) synthesis applications. Our prosody model is conceptualised around a modular holistic framework. This framework is implemented using the Relational Tree (R-Tree) techniques. An important feature of our R-Tree framework is its flexibility in that it facilitates the independent implementation of the different dimensions of prosody, i.e. duration, intonation, and intensity, using different techniques and their subsequent integration. We applied the Fuzzy Decision Tree (FDT) technique to model the duration dimension. In order to evaluate the effectiveness of FDT in duration modelling, we have also developed a Classification And Regression Tree (CART) based duration model using the same speech data. Each of these models was integrated into our R-Tree based prosody model. We performed both quantitative (i.e. Root Mean Square Error (RMSE) and Correlation (Corr)) and qualitative (i.e. intelligibility and naturalness) evaluations on the two duration models. The results show that CART models the training data more accurately than FDT. The FDT model, however, shows a better ability to extrapolate from the training data since it achieved a better accuracy for the test data set. Our qualitative evaluation results show that our FDT model produces synthesised speech that is perceived to be more natural than our CART model. In addition, we also observed that the expressiveness of FDT is much better than that of CART. That is because the representation in FDT is not restricted to a set of piece-wise or discrete constant approximation. We, therefore, conclude that the FDT approach is a practical approach for duration modelling in SY TTS applications.		Odétúnjí Àjàdí Odéjobí;Shun Ha Sylvia Wong;Anthony J. Beaumont	2007	Computer Speech & Language	10.1016/j.csl.2006.06.005	natural language processing;speech recognition;computer science;artificial intelligence;computational linguistics;machine learning;mean squared error;linguistics	Logic	-17.910800157250073	-85.01572870211537	98033
7f29960ed8e2d1b9639fcbd11eca07767472df6b	cluster-dependent modeling and confidence measure processing for in-set/out-of-set speaker identification	speaker identification;confidence measure	In this paper, we propose an approach to address the problem of text-independent open-set speaker identification. The in-set speakers are clustered into smaller subsets without merging speaker models. The Anti-Speaker or Background Model is then adapted for each subset which minimizes the identification errors of the pseudo impostors during the training stage. Score normalization is applied to align all the in-set speaker score distributions to share a single scale. Finally, confidence measure processing is used to identify in-set versus out-of-set speakers. Experiments with TIMIT and the CU-Accent corpora show an improvement in Equal Error Rate on the average of 20.28 and 8.35 over the baseline performance respectively. Finally, a probe experiment is also included that considers prosody for in-set speaker detection.	align (company);baseline (configuration management);consistency model;database normalization;semantic prosody;speaker recognition;timit;text corpus;tip (unix utility)	Pongtep Angkititrakul;Sepideh Baghaii;John H. L. Hansen	2004			speech recognition;computer science;pattern recognition	NLP	-14.945086691076074	-89.45743174995934	98169
92884107e3e92351a62dc2c3cd22eb4e80966009	acoustic features based word level dialect classification using svm and ensemble methods		In this paper, word based dialect classification system is proposed by using acoustic characteristics of the speech signal. Dialects mainly represent the different pronunciation patterns of any language. Dialectal cues can exist at various levels such as phoneme, syllable, word, sentence and phrase in an utterance. Word level dialectal traits are extracted to recognize dialects since every word exhibits significant dialect discriminating cues. Intonational Variations in English (IViE) speech corpus recorded in British English has been considered. The corpus includes nine dialects which cover nine distinct regions of British Isles. Acoustic properties such as spectral and prosodic features are derived from word level to construct the feature vector. Further, two different classification algorithms such as support vector machine (SVM) and tree-based extreme gradient boosting (XGB) ensemble algorithms are used to extract the prominent patterns that are used to discriminate the dialects. From the experiments, a better performance has been observed with word level traits using ensemble methods over the SVM classification method.	acoustic cryptanalysis;algorithm;boosting (machine learning);ensemble learning;experiment;feature vector;gradient boosting;phoneme;speech corpus;support vector machine;syllable;text corpus	Nagaratna B. Chittaragi;Shashidhar G. Koolagudi	2017	2017 Tenth International Conference on Contemporary Computing (IC3)	10.1109/IC3.2017.8284315	support vector machine;artificial intelligence;syllable;pattern recognition;feature vector;ensemble learning;pronunciation;computer science;statistical classification;speech corpus;utterance	NLP	-12.601464642482767	-87.55121590778214	98237
ba0693e465e39bbde6fc5832f8344817fb2da8fc	speaker-targeted audio-visual models for speech recognition in cocktail-party environments		Speech recognition in cocktail-party environments remains a significant challenge for state-of-the-art speech recognition systems, as it is extremely difficult to extract an acoustic signal of an individual speaker from a background of overlapping speech with similar frequency and temporal characteristics. We propose the use of speaker-targeted acoustic and audio-visual models for this task. We complement the acoustic features in a hybrid DNN-HMM model with information of the target speaker’s identity as well as visual features from the mouth region of the target speaker. Experimentation was performed using simulated cocktail-party data generated from the GRID audio-visual corpus by overlapping two speakers’s speech on a single acoustic channel. Our audio-only baseline achieved a WER of 26.3%. The audio-visual model improved the WER to 4.4%. Introducing speaker identity information had an even more pronounced effect, improving the WER to 3.6%. Combining both approaches, however, did not significantly improve performance further. Our work demonstrates that speaker-targeted models can significantly improve the speech recognition in cocktailparty environments.	acoustic cryptanalysis;baseline (configuration management);experiment;hidden markov model;speech recognition;visual modeling;word error rate	Guan-Lin Chao;William Chan;Ian Lane	2016		10.21437/Interspeech.2016-599	speaker recognition	NLP	-13.826366621429518	-89.71592151422286	98634
50c00c2128ba482743addf8f06ff7baaedcc1c1b	time dependent arma for automatic recognition of fear-type emotions in speech	time dependent arma models;speech emotion recognition;non stationary signals;continuous speech	The speech signals are non-stationary processes with changes in time and frequency. The structure of a speech signal is also affected by the presence of several paralinguistics phenomena such as emotions, pathologies, cognitive impairments, among others. Non-stationarity can be modeled using several parametric techniques. A novel approach based on time dependent auto-regressive moving average (TARMA) is proposed here to model the non-stationarity of speech signals. The model is tested in the recognition of “fear-type” emotions in speech. The proposed approach is applied to model syllables and unvoiced segments extracted from recordings of the Berlin and enterface05 databases. The results indicate that TARMA models can be used for the automatic recognition of emotions in speech.	database;stationary process	Juan Camilo Vásquez-Correa;Juan R. Orozco-Arroyave;Julián D. Arias-Londoño;Jesus Francisco Vargas Bonilla;L. D. Avendaño;Elmar Nöth	2015		10.1007/978-3-319-24033-6_11	natural language processing;speech recognition;speech processing;acoustic model	AI	-11.7669936434888	-86.80488954894942	98778
68806c41ec6a38bd5e3b40d3cdd840d636fcee33	detection of overlapped speech using lapel microphones in meeting	spectral subtraction;overlap speech detection;cosine distance	We propose an overlapped speech detection method for speech recognition and speaker diarization of meetings, where each speaker wears a lapel microphone. Two novel features are utilized as inputs for a GMM-based detector. One is speech power after cross-channel spectral subtraction which reduces the power from the other speakers. The other is an amplitude spectral cosine correlation coefficient which effectively extracts the correlation of spectral components in a rather quiet condition. We evaluated our method using a meeting speech corpus of four speakers. The accuracy of our proposed method, 75.7%, was significantly better than that of the conventional method, 66.8%, which uses raw speech power and power spectral Pearson's correlation coefficient.	microphone	Ryo Yokoyama;Yu Nasu;Koji Iwano;Koichi Shinoda	2013	Speech Communication	10.1016/j.specom.2013.06.013	speech recognition;computer science	NLP	-11.912228374733855	-91.00114673941334	98931
d8668faa54063d4c26b0fe4f076f2ec1a875b7b0	improving deep neural networks using softplus units	speech;timit softplus dropout deep neural networks;dropout strategy deep neural networks softplus units acoustic modeling speech recognition tasks sigmoid units backpropagation dnn data training unbounded activation functions relu context independent phoneme recognition tasks revised rbm pre training;speech recognition backpropagation neural nets	Recently, DNNs have achieved great improvement for acoustic modeling in speech recognition tasks. However, it is difficult to train the models well when the depth grows. One main reason is that when training DNNs with traditional sigmoid units, the derivatives damp sharply while back-propagating between layers, which restrict the depth of model especially with insufficient training data. To deal with this problem, some unbounded activation functions have been proposed to preserve sufficient gradients, including ReLU and softplus. Compared with ReLU, the smoothing and nonzero properties of the in gradient makes softplus-based DNNs perform better in both stabilization and performance. However, softplus-based DNNs have been rarely exploited for the phoneme recognition task. In this paper, we explore the use of softplus units for DNNs in acoustic modeling for context-independent phoneme recognition tasks. The revised RBM pre-training and dropout strategy are also applied to improve the performance of softplus units. Experiments show that, the DNNs with softplus units get significantly performance improvement and uses less epochs to get convergence compared to the DNNs trained with standard sigmoid units and ReLUs.	acoustic cryptanalysis;acoustic model;activation function;artificial neural network;deep learning;dropout (neural networks);experiment;gradient;logical volume management;mathematical optimization;rectifier (neural networks);restricted boltzmann machine;sigmoid function;smoothing;sparse matrix;speech recognition	Hao Zheng;Zhanlei Yang;Wenju Liu;Jizhong Liang;Yanpeng Li	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280459	speech recognition;computer science;speech;artificial intelligence;machine learning	AI	-18.140540447079335	-88.97912693465878	98980
9df0f7725eb0adc74c0df2d7c3edf6eeae9aa953	infants' native and nonnative tone perception		Infants’ native and nonnative tone perception was investigated in three studies. In Study 1, 6and 9-month-old English (non-tone) and Chinese (tone) infants were tested for discrimination of Thai tones in a training procedure. Study 2 examined English and French infants’ Thai tone perception in a more natural looking/listening task. Study 3 investigated English infants’ discrimination of Mandarin tones. The findings provide robust evidence for the reorganization of tone perception as a function of language experience. Agerelated decline in nonnative tone discrimination was found for English and French infants. Chinese infants showed stable tone perception across age.	jakobson's functions of language;super robot monkey team hyperforce go!	Karen Mattock	2008			speech recognition;communication;perception;computer science	HCI	-11.242299525832923	-81.91480909267673	99030
e25869e27af783fc2943cd34586edd5aa476a4fe	beyond the listening test: an interactive approach to tts evaluation		Traditionally, subjective text-to-speech (TTS) evaluation is performed through audio-only listening tests, where participants evaluate unrelated, context-free utterances. The ecological validity of these tests is questionable, as they do not represent real-world end-use scenarios. In this paper, we examine a novel approach to TTS evaluation in an imagined end-use, via a complex interaction with an avatar. 6 different voice conditions were tested: Natural speech, Unit Selection and Parametric Synthesis, in neutral and expressive realizations. Results were compared to a traditional audio-only evaluation baseline. Participants in both studies rated the voices for naturalness and expressivity. The baseline study showed canonical results for naturalness: Natural speech scored highest, followed by Unit Selection, then Parametric synthesis. Expressivity was clearly distinguishable in all conditions. In the avatar interaction study, participants rated naturalness in the same order as the baseline, though with smaller effect size; expressivity was not distinguishable. Further, no significant correlations were found between cognitive or affective responses and any voice conditions. This highlights 2 primary challenges in designing more valid TTS evaluations: in real-world use-cases involving interaction, listeners generally interact with a single voice, making comparative analysis unfeasible, and in complex interactions, the context and content may confound perception of voice quality.	baseline (configuration management);context-free language;expressive power (computer science);interaction;netware file system;qualitative comparative analysis;speech synthesis	Joseph Mendelson;Matthew P. Aylett	2017			speech recognition;active listening;computer science;ecological validity	HCI	-12.051863441421721	-84.57480844630729	99065
458cd7d1a87e41b581fd3ec68451e16d4abcde95	emotion-flow guided music accompaniment generation	perceived arousals emotion flow guided music accompaniment generation music piece emotion melody emotion flow chord progression accompaniment pattern valence data dynamic programming mathematical model system performance evaluation cross correlation coefficient expected arousals;decision support systems matlab indexes conferences creativity multimedia communication mood;emotion flow;music emotion;melody harmonization accompaniment emotion flow music emotion;music dynamic programming emotion recognition mathematical analysis;melody harmonization;accompaniment	The emotion of a music piece varies as it unrolls in time. We develop a system that takes a melody and an expected emotion flow as input and automatically generates an accompaniment. The accompaniment is composed of chord progression and accompaniment pattern. The former is generated from melody and valence data through dynamic programming, and the latter from arousal data. A mathematical model is developed to describe the relation between valence and chord progression. The performance of the system is evaluated subjectively. The cross-correlation coefficient between the expected arousals and the perceived ones is 0.84, and the cross-correlation coefficient between the expected valences and the perceived ones is 0.52. Both coefficients exceed 0.90 for musician subjects.	coefficient;color gradient;cross-correlation;dynamic programming;mathematical model	Yi-Chan Wu;Homer H. Chen	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7471740	melody;speech recognition;multimedia;music and emotion	Robotics	-8.94107306625666	-85.52327118993382	99211
2f54aeebac612f5bcc56130862975df4676632fa	alternative approaches to neural network based speaker verification		Just like in other areas of automatic speech processing, feature extraction based on bottleneck neural networks was recently found very effective for the speaker verification task. However, better results are usually reported with more complex neural network architectures (e.g. stacked bottlenecks), which are difficult to reproduce. In this work, we experiment with the so called deep features, which are based on a simple feed-forward neural network architecture. We study various forms of applying deep features to i-vector/PDA based speaker verification. With proper settings, better verification performance can be obtained by means of this simple architecture as compared to the more elaborate bottleneck features. Also, we further experiment with multi-task training, where the neural network is trained for both speaker recognition and senone recognition objectives. Results indicate that, with a careful weighting of the two objectives, multi-task training can result in significantly better performing deep features.	artificial neural network;bottleneck (software);computer multitasking;feature extraction;feedforward neural network;network architecture;personal digital assistant;speaker recognition;speech processing	Anna Silnova;Lukás Burget;Jan Cernocký	2017			artificial neural network;pattern recognition;artificial intelligence;time delay neural network;computer science	AI	-16.870447990112076	-88.73378190393933	99253
5916fedd2f635923796b2885904f99d16b0969ba	cascade classifiers trained on gammatonegrams for reliably detecting audio events	time frequency analysis audio streaming surveillance;surveillance applications gammatonegrams audio event detection audio analysis audio streams gammatone image time frequency distribution human auditory system adaboost cascade classifiers event detection stage;surveillance time frequency analysis auditory system training feature extraction streaming media glass	In this paper we propose a novel method for the detection of events of interest through audio analysis. The system that we propose is based on the representation of the audio streams through a Gammatone image, which describes the time-frequency distribution of the energy of the signal; this representation is inspired by the functioning of the human auditory system. A pool of AdaBoost cascade classifiers, one for each class of events of interest, is involved in the event detection stage. The performance of the proposed system has been evaluated on a large data set of audio events for surveillance applications and the achieved results, compared with two state of the art approaches, confirm its effectiveness.	adaboost;sensor;time–frequency representation	Pasquale Foggia;Alessia Saggese;Nicola Strisciuglio;Mario Vento	2014	2014 11th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2014.6918643	computer vision;speech recognition;computer science;pattern recognition	Robotics	-5.431769963957865	-89.19135254679668	99304
e97c806bcbf12123d9f20e0f133f487c56040791	measuring the dynamic encoding of speaker identity and dialect in prosodic parameters		This paper describes a methodology, and the results stemming from it, for analysing the dynamic encoding of speaker identity and dialect in prosodic parameters. A method based on employing properties of the well known Dynamic Time Warping (DTW) algorithm’s path of best match allows the separation of purely dynamic from static properties of acoustic parameters and hence their evaluation as to dynamic encoding of speaker characteristics. Nineteen adult speakers of Australian English were recorded uttering a set of four sentences on five separate occasions over a period of at least one week. The prosodic parameters F 0, horttime energy, zero crossing rate and voicing were extracted for all data and analysed as to their dynamic encoding of speaker identity and dialect. Discriminate analysis (for speaker identity) and correlation analysis (for speaker dialect) analysis showed higher dynamic encoding of identity (75%) and dialect (0.58) than static encoding (55% and 0.45 respectively). Normalisation of all parameters into the range 0—1 reduced discriminate and correlation scores to 70% and 0.54 respectively. Contrasting the warp path parameters with the more conventionally employed DTW distance showed that the warp path parameters better measured speaker identity (72% versus 54%) and speaker dialect (0.56 versus 0.31) encoding. Individual analysis of the prosodic parameters shows a far higher encoding of identity and dialect in F 0, though all four parameters encode dialect and identity.	acoustic cryptanalysis;algorithm;dynamic time warping;encode;identity management;online identity;stemming;whole earth 'lectronic link;zero crossing;zero-crossing rate	Michael Barlow;Michael Wagner	1998			speech recognition;zero-crossing rate;encoding (memory);voice;artificial intelligence;pattern recognition;dynamic time warping;speaker diarisation;linear discriminant analysis;computer science	HCI	-10.97720815266777	-82.8320243001985	99399
2b8543c8246eccab8d1580d7cded79da9130ca01	estimating prosodic weights in a syntactic-rhythmical prediction system		This paper concerns the study of information derived from the melodic, temporal and intensity characteristics of the material to be recognized in a speech recognition system, in French. More precisely, it describes experiments we achieved at the suprasegmental levels with a system that outperform automatic correlation between prosodic labels and linguistic organization of a message to decode. Firstly an overview of the system is described along with the results of experiments carried out to determine which prosodic indexes are bestsuited for syntactic and rhythmycal prediction.	database;experiment;relevance;scoring functions for docking;speech recognition	Philippe Langlais	1997			artificial intelligence;pattern recognition;syntax;computer science	NLP	-18.5478089108613	-83.72025190714679	99443
7b2d28ee77920fd9512a56f7da021a0bd06af06a	privacy protection for speech information	electronic equipments;microphones;speech synthesis;sensors;speech elimination privacy protection speech information ubiquitous network society electronic equipments virtual reality systems voice conversion voice characteristics linguistic privacy information speech recognition noisy speech;speech elimination;speech processing;virtual reality;speech;privacy protection;noise measurement;voice conversion;privacy protection loudspeakers speech recognition speech enhancement acoustic sensors microphones thermal sensors acoustic noise working environment noise;voice characteristics;virtual reality systems;personal infromation;data privacy;ubiquitous computing data privacy speech processing speech recognition speech synthesis;noisy speech;ubiquitous network society;speech recognition;ubiquitous computing;speech information;speech elimination privacy protection speech information personal infromation voice conversion speech recognition;linguistic privacy information;privacy	"""Ubiquitous network society will be achieved soon. In the society, all electronic equipments including """"sensors"""" are connected to the network and communicate each other to share information. Sensor information is very important for the network, especially for virtual reality systems which give feeling of begin there. Since the sensor information, however, includes a lot of privacy information, it is not preferred to send raw privacy information through the network. In this paper, we describe about privacy protection for speech information. We think that the privacy information in speech is """"voice characteristics"""" and """"linguistic privacy information."""" We try to protect these privacy information by using """"voice conversion"""" and """"deletion of privacy linguistic information for speech recognition result."""" Since, however, speech recognition technology is not robust in real environment still now, """"elimination of only speech in noisy speech"""" technique is also considered."""	privacy;sensor;speech processing;speech recognition;virtual reality	Kazumasa Yamamoto;Seiichi Nakagawa	2009	2009 Fifth International Conference on Information Assurance and Security	10.1109/IAS.2009.321	privacy software;speech recognition;acoustics;computer science;information quality;communication	Security	-6.619674397462552	-87.83318876853714	99568
87cebac24b0c93264b27682ae38e22f022d37c20	pauses and intonational phrasing: erp studies in 5-month-old german infants and adults	syntax;comparative analysis;cues;intonation;acoustics;neurology;diagnostic tests;language acquisition;physiology;phonology;adults;phrase structure;infants;contrastive linguistics;intonational phrase;english;language learning;german	In language learning, infants are faced with the challenge of decomposing continuous speech into relevant units, such as syntactic clauses and words. Within the framework of prosodic bootstrapping, behavioral studies suggest infants approach this segmentation problem by relying on prosodic information, especially on acoustically marked intonational phrase boundaries (IPBs). In the current ERP study, we investigate processing of IPBs in 5-month-old infants by varying the acoustic cues signaling the IPB. In an experiment in which pitch variation, vowel lengthening, and pause cues are present (Experiment 1), 5-month-old German infants show an ERP obligatory response. This obligatory response signals lower level perceptual processing of acoustic cues that, however, disappear when no pause cue is present (Experiment 2). This suggests that infants are sensitive to sentence internal pause, a cue that is relevant for the processing of IPBs. Given that German adults show both the obligatory components and the closure positive shift, a particular ERP component known to reflect the perception of IPBs, independent of the presence of a pause cue, the results of the current ERP study indicate clear developmental differences in intonational phrase processing. The comparison of our neurophysiological data from German-learning infants with behavioral data from English-learning infants furthermore suggests cross-linguistic differences in intonational phrase processing during infancy. These findings are discussed in the light of differences between the German and the English intonation systems.	acoustic cryptanalysis;closure;erp;high- and low-level;ips community suite;language development;linguistics;natural language processing;phrases;sensory process;biologic segmentation	Claudia Männel;Angela D. Friederici	2009	Journal of Cognitive Neuroscience	10.1162/jocn.2009.21221	language acquisition;neurology;german;english;linguistics;phonology	NLP	-9.950389988897934	-81.41920497093828	99586
1f6952db8190d2e146391c4d109dad26aad54521	the effectiveness of ica-based representation: application to speech feature extraction for noise robust speaker recognition	gaussian noise;speaker identification;filter bank;europe speech feature extraction abstracts databases dairy products method of moments;white noise independent component analysis ica based representation speech feature extraction speaker recognition mathematical derivation feature representation nongaussian signals gaussian noise logarithm filter bank energies gmm based speaker identification task timit database clean speech;independent component analysis;noise robustness;speaker recognition;feature extraction;white noise channel bank filters feature extraction gaussian noise independent component analysis mixture models speaker recognition;white noise	In this paper, we present a mathematical derivation demonstrating that feature representation obtained by using the Independent Component Analysis (ICA) is an effective representation for non-Gaussian signals when being both clean and corrupted by Gaussian noise. Our findings are experimentally demonstrated by employing the ICA for speech feature extraction; specifically, the ICA is used to transform the logarithm filter-bank-energies (instead of the DCT which provides MFCC features). The evaluation is presented for a GMM-based speaker identification task on the TIMIT database for clean speech and speech corrupted by white noise. The effectiveness of ICA is analysed individually for signals corresponding to each phoneme. The experimental results show that the ICA-based features can provide significantly better performance than traditional MFCCs and PCA-based features in both clean and noisy speech.	discrete cosine transform;expectation propagation;experiment;feature extraction;filter bank;google map maker;independent computing architecture;independent component analysis;principal component analysis;speaker recognition;timit;white noise	Xin Zou;Peter Jancovic;Ju Liu	2006	2006 14th European Signal Processing Conference		speaker recognition;speech recognition;computer science;machine learning;pattern recognition	ML	-15.688091707138554	-92.64293760297664	99727
2e13bfdd0df86e7021b260d727e47943bb096b5b	speech enhancement with reduction of noise components in the wavelet domain	white noise speech enhancement wavelet transforms;speech enhancement;wavelet transforms;wavelet transform;speech enhancement noise reduction wavelet domain wavelet analysis frequency discrete wavelet transforms acoustic noise signal processing speech processing continuous wavelet transforms;white noise spectral distance noise components reduction wavelet domain additive background noise removal semisoft thresholding wavelet coefficients noisy speech speech quality degradation unvoiced sounds denoising process unvoiced region classification experimental results speech enhancement algorithm;white noise	This paper describes a general problem of removing additive background noise from the noisy speech in the wavelet domain. A semisoft thresholding is used to remove noise components from the wavelet coefficients of noisy speech. To prevent the quality degradation of the unvoiced sounds during the denoising process, the unvoiced region is classified first and then thresholding is applied in a different way. Experimental results demonstrate that the speech enhancement algorithm using the wavelet transform is very promising.	speech enhancement;wavelet	Jong Won Seok;Keun-Sung Bae	1997		10.1109/ICASSP.1997.596190	wavelet noise;wavelet;speech recognition;second-generation wavelet transform;continuous wavelet transform;pattern recognition;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	EDA	-7.936225675651343	-89.66484254794459	99795
5048a9d2d8f17ec18f176e9fafbc780e44f2fe52	analysis of call-quality prediction performance for speech-only and audio-visual telephony	degradation predictive models databases biological system modeling instruments speech telecommunication standards;recency effect call quality episodic quality time varying quality temporal integration;itu t study group call quality prediction performance analysis audio visual telephony speech only telephone conversational structures individual conversation segments instrumental quality estimations call quality prediction models;telephony speech processing	In this paper, we will compare several approaches for predicting the quality of entire speech-only or audio-visual telephone calls from subjective judgments or predictions of the quality of individual segments of these calls. The comparison will be done using several databases which have been collected in a test paradigm to simulate conversational structures. The subjective judgments for individual conversation segments obtained in these tests, as well as instrumental quality estimations for these segments, are used as the basis for predicting episode-final quality ratings. Although different modeling approaches reach a similar performance, an optimum model is proposed which leads to comparably high prediction accuracy for speech-only and audio-visual telephony. Both the test paradigms as well as the call-quality prediction models are subject to standardization activities of ITU-T Study Group 12, for which this analysis is considered an input.	database;programming paradigm;simulation	Benjamin Weiss;Sebastian Möller;Benjamin Belmudez;Blazej Lewcio	2014	2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX)	10.1109/QoMEX.2014.6982329	speech recognition;computer science;multimedia	SE	-9.255012420448056	-88.4663981847424	99855
400c575198b1e006feb9b99268934b5b3db57f6e	prosodic timing analysis for articulatory re-synthesis using a bank of resonators with an adaptive oscillator.	oscillations;timing analysis	A method for the analysis of prosodic-level temporal structure is introduced. The method is based on measured phase angles of an oscillator as that oscillator is made to synchronize with reference points in a signal. Reference points are the predicted peaks of acoustic change as determined by the output of a bank of tuned resonators. A framework for articulatory resynthesis is then described. Jaw movements of a robotic vocal tract are made to replicate the mean phase portrait of an utterance with reference to a production oscillator. These jaw movements are modeled to inform the dynamics of withinsyllable phonemic articulations.	acoustic cryptanalysis;articulatory synthesis;reference implementation;robot;self-replicating machine;static timing analysis;tract (literature)	Michael C. Brady	2010			speech recognition;computer science;oscillation;static timing analysis	Robotics	-8.872645851550525	-83.89770214149151	99938
a497fe772d98e7539daeb5443bc8dba699cac345	characteristics of voice picked up from outer skin of larynx				Tomo-o Morohashi;Tetsuya Shimamura;Hiroyuki Yashima;Jouji Suzuki	1992			speech recognition;computer science;larynx	Vision	-7.159117783470838	-84.63913352177876	99949
b8195257ab839761d7ca6d287a78f6a51836a739	a new nonlinear prediction model based on the recurrent neural predictive hidden markov model for speech enhancement	learning algorithm;hidden markov chain;hidden markov model;time varying parameter;speech enhancement;baum welch;signal to noise ratio recurrent neural networks;back propagation algorithm;prediction model;recurrent neural networks;recurrent neural network;signal to noise ratio	In this paper, a new nonlinear prediction model based on the Recurrent Neural Predictive Hidden Markov Model (RNPHMM) is proposed for speech enhancement. Assuming that speech is an output of the RNPHMM combining RNN and HMM, the proposed nonlinear prediction model-based recurrent neural network (RNN) is used to present the nonlinear and nonstationary nature of speech. The RNPHMM is a nonlinear prediction process whose time-varying parameters are controlled by a hidden Markov chain. Given some speech data for training, the parameters of the RNPHMM are estimated by a learning algorithm based on the combination of Baum-Welch algorithm and RNN learning algorithm using the back-propagation algorithm. In our experiment, the proposed method achieved about 2–2.5 dB of improvement in SNR compared with both the NPHMM and the HFM at various input SNRs.	artificial neural network;backpropagation;baum–welch algorithm;decibel;hidden markov model;markov chain;nonlinear system;random neural network;recurrent neural network;signal-to-noise ratio;software propagation;speech enhancement;welch's method	Joohim Lee;Changwoo Seo;Ki Yong Lee	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5743972	forward algorithm;speech recognition;computer science;recurrent neural network;machine learning;pattern recognition;time delay neural network;markov model;hidden markov model	ML	-17.58629855972737	-88.47711241901433	99960
b1a92382d8b3fc221e88b89175272cb793bf430b	avoiding speaker variability in pronunciation verification of children' disordered speech	performance measure;pronunciation evaluation;speech disorders;system evaluation;speaker independent;children speech;speech disorder;equal error rate;young adult;speaker adaptation	This paper deals with the problematic of speaker variability in a task of pronunciation verification for the speech therapy of children and young adults in Computer-Aided Pronunciation Training (CAPT) tools. The baseline system evaluates two different score normalization techniques: Traditional Test normalization (T-norm), and a novel N-best based normalization that outperforms the first by normalizing to the log-likelihood score of the first alternative phoneme in an unconstrained N-best list. When performing speaker adaptation, the use of all the adaptation data from the speaker improves the performance measured in Equal Error Rate (EER) of these systems compared to the speaker independent systems; but this can be outperformed by more precise models that only adapt to the correctly pronounced phonetic units as labeled by a set of human experts. The best EER obtained in all experiments is 15.63% when using both elements: Score normalization and speaker adaptation. The possibility of automatizing a more precise adaptation without the human intervention is finally proposed and discussed.	baseline (configuration management);database normalization;enhanced entity–relationship model;experiment;heart rate variability;speaker recognition;t-norm	Oscar Saz-Torralba;Eduardo Lleida;William Ricardo Rodríguez-Dueñas	2009		10.1145/1640377.1640388	natural language processing;speaker recognition;speaker diarisation;speech recognition;computer science;communication	NLP	-18.214362408763265	-84.90707112302738	99981
bec109b7df84a43f9efced1c43f820586a181cd2	improvement of mask-based speech source separation using dnn	reliability;training;speech;testing;image color analysis;source separation;time frequency analysis	The speech mask is widely used to separate multiple speech sources, wherein the time-frequency bins are classified into clusters that correspond to each source. For each source, the separated signal consists of the components on TF bins that are dominated by this source, whereas the components on the remaining bins are completely masked. Most separation methods ignored the masked components. In fact, the masked components may contain some useful information, and the mask-based speech source separation can be improved by reconstructing the masked components. This paper proposes a post-processing method to reconstruct the masked frequency components through a deep neural network (DNN). We construct a regression from the reliable frequency components to the masked components. After the masked-based separation, the reliable components are kept unchanged, and the masked components are reconstructed by the outputs of DNN. Experimental results confirmed that the proposed method significantly improved the mask-based separation, and that the masked components are still useful to the speech quality.	artificial neural network;deep learning;source separation;video post-processing	Ge Zhan;Zhaoqiong Huang;Dongwen Ying;Jielin Pan;Yonghong Yan	2016	2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)	10.1109/ISCSLP.2016.7918461	speech recognition;time–frequency analysis;computer science;speech;pattern recognition;reliability;linguistics;software testing	Arch	-14.293767833939253	-92.38026633854136	99985
75aa75cb89f9d78fb6fbef760aed2381c9d66d8f	speech-driven lip motion generation with a trajectory hmm	indexing terms;hidden markov model;generic algorithm	Automatic speech animation remains a challenging problem that can be described as finding the optimal sequence of animation parameter configurations given some speech. In this paper we present a novel technique to automatically synthesise lip motion trajectories from a speech signal. The developed system predicts lip motion units from the speech signal and generates animation trajectories automatically employing a ”Trajectory Hidden Markov Model”. Using the MLE criterion, its parameter generation algorithm produces the optimal smooth motion trajectories that are used to drive control points on the lips directly. Additionally, experiments were carried out to find a suitable model unit that produces the most accurate results. Finally a perceptual evaluation was conducted, that showed that the developed motion units perform better than phonemes.	algorithm;experiment;hidden markov model;markov chain	Gregor Hofer;Junichi Yamagishi;Hiroshi Shimodaira	2008			speech recognition;computer science	Graphics	-15.702772105672556	-83.11776890109141	100018
90e63710732d39d5f38735f6a1c31356aa4d9ec3	construction and analysis of multiple paths in syllable models	hidden markov model;indexing terms;feature space;speech recognition;context dependent;article in monograph or in proceedings;kullback leibler distance	In this paper, we construct multi-path syllable models using phonetic knowledge for initialising the parallel paths, and a data-driven solution for their re-estimation. We hypothesise that the richer topology of multi-path syllable models would be better at accounting for pronunciation variation than context-dependent phone models that can only account for the effects of left and right neighbours. We show that parallel paths that are initialised with phonetic knowledge and then re­ estimated do indeed result in different trajectories in feature space. Yet, this does not result in better recognition performance. We suggest explanations for this finding, and provide the reader with important insights into the issues playing a role in pronunciation variation modelling with multi-path syllable models.	consistency model;context-sensitive language;feature vector;syllable	Annika Hämäläinen;Louis ten Bosch;Lou Boves	2007			speech recognition;index term;feature vector;computer science;machine learning;context-dependent memory;pattern recognition;kullback–leibler divergence;hidden markov model	ML	-13.080269307397653	-83.14070253218597	100103
ed3bdda84707ce2466f86e9941c6f52f0a31d551	linear histogram equalization in the acoustic feature domain for speech recognition over bluetooth™ channels	wireless channels;packet loss;bluetooth channel;bluetooth 8482;channel;mel frequency cepstral coefficient;linear histogram equalization;speech recognition;quantile;cumulant;density functional;histogram equalization	This paper studies the improvement of speech recognition over Bluetooth#8482; wireless channels. Speech recognition over Bluetooth#8482; suffers from the low SNR due to the position of the Bluetooth#8482; microphone, Bluetooth#8482; codec distortion, packet loss over the wireless channel, and Bluetooth#8482; channel distortion. By transforming the MFCCs (Mel-Frequency Cepstral Coefficients) to make the cumulative density functions of the MFCC values in recognition match the ones that were estimated on the training data, the recognition can be improved. The cumulative density functions are approximated using a small number of quantiles. Recognition tests on a Bluetooth#8482; speech database showed significant increase of recognition accuracy in noisy environments.	acoustic cryptanalysis;approximation algorithm;bluetooth;codec;distortion;histogram equalization;mel-frequency cepstrum;microphone;network packet;signal-to-noise ratio;speech corpus;speech recognition	Ke Peng;Hongbin Cai;Yaxin Zhang	2007		10.1145/1378063.1378130	quantile;speech recognition;computer science;pattern recognition;packet loss;histogram equalization;statistics;cumulant;channel	Mobile	-13.714243479286703	-91.1192666963672	100167
73bc4d014fac7e6a64161415ccd0f4b0e5db0dd1	estimating the intrinsic dimensionality of discrete utterances	signal estimation;isolated word utterances;vocabulary;filters;speech vocabulary signal processing algorithms time domain analysis psychology filters pattern recognition pattern analysis delta modulation loudspeakers;speech;psychology;delta modulation;isolated word utterances speech recognition intrinsic dimensionality discrete utterances;time domain analysis;loudspeakers;estimacion senal;pattern recognition;speech recognition;difference set;pattern analysis;intrinsic dimensionality;signal processing algorithms;estimation signal;discrete utterances	The Intrinsic Dimensionality (ID) of different sets of isolated word utterances is estimated through a method recently proposed by Pettis et al. The results show ID values ranging from 3 up to 13, which are consistent with the intuitive degree of difficulty associated to the sets considered. Also, some speculative applications of ID estimat-	speculative execution	Elvira Baydal;Gabriela Andreu;Enrique Vidal	1989	IEEE Trans. Acoustics, Speech, and Signal Processing	10.1109/29.17567	loudspeaker;natural language processing;delta modulation;speech recognition;computer science;speech;mathematics;difference set	ML	-9.954319082647533	-90.55072967543866	100186
dc0de95ed04101bb3473a1a61a5a57d42f4a6a38	robust speech recognition using model-based feature enhancement	robust speech recognition;time varying;psi_speech;additive noise;noise robustness;statistical model;feature vector;automatic speech recognition;vector taylor series	Maintaining a high level of robustness for Automatic Speech Recognition (ASR) systems is especially challenging when the background noise has a time-varying nature. We have implemented a Model-Based Feature Enhancement (MBFE) technique that not only can easily be embedded in the feature extraction module of a recogniser, but also is intrinsically suited for the removal of non-stationary additive noise. To this end we combine statistical models of the cepstral feature vectors of both clean speech and noise, using a Vector Taylor Series approximation in the power spectral domain. Based on this combined HMM, a global MMSE-estimate of the clean speech is then calculated. Because of the scalability of the applied models, MBFE is flexible and computationally feasible. Recognition experiments with this feature enhancement technique on the Aurora2 connected digit recognition task showed significant improvements on the noise robustness of the HTK recogniser.	additive white gaussian noise;approximation;cepstrum;embedded system;experiment;feature extraction;htk (software);hidden markov model;high-level programming language;scalability;speech recognition;stationary process;statistical model;utility functions on indivisible goods	Veronique Stouten;Hugo Van hamme;Kris Demuynck;Patrick Wambacq	2003			statistical model;speech recognition;feature vector;feature;computer science;machine learning;pattern recognition;statistics	ML	-16.330904533929793	-92.46305642052529	100259
597f0b50eebca85042645497ff2477c169213010	active perception: using a priori knowledge from clean speech models to ignore non-target features.	low energy;noisy data;noise robustness;a priori knowledge	Making ASR noise robust requires a form of data normalisation to ensure that the distributions of acoustic features i n the training and test condition look similar. Usually, it is att empted to compensate for the impact of noise by estimating the noise characteristics from the signal. In this paper we explore a n w method that builds on a priori knowledge stored in clean spee ch models. Using Mel bank log-energy features, classical clea n speech HMMs were replaced by models in which the model components corresponding to low energy are not considered during recognition. Application of the new method to clean matched data showed that recognition performance was equal or better compared to baseline when less than 45% of the model components were discarded. In the case of noisy data, the per formance gains were marginal for the model component selections studied so far. Analysis of the results suggests that f uture research should focus on combining the new model-driven approach with data-driven methods.	acoustic cryptanalysis;automated system recovery;baseline (configuration management);database normalization;image noise;marginal model;model-driven integration;signal-to-noise ratio	Bert Cranen;Johan de Veth	2004			a priori and a posteriori;speech recognition;computer science;machine learning;pattern recognition	NLP	-14.39736785811541	-91.64430222150231	100348
2e52b37600c76888537eed617d640fbfa1e2b9b8	modeling pronunciation, rhythm, and intonation for automatic assessment of speech quality in aphasia rehabilitation		Patients with aphasia often have impaired speech-language production skills, resulting in tremendous difficulties in tasks that require verbal communication. To facilitate rehabilitation outside of therapy, we are collaborating with the University of Michigan Aphasia Program (UMAP) to develop an automated system capable of providing feedback regarding the patient’s verbal output. In this paper we introduce a robust method for extracting rhythm and intonation features from aphasic speech based on template matching. These features, combined with Goodness of Pronunciation (GOP) scores and our previous feature set, help our system achieve human-level performance in classifying the quality of speech produced by patients attending UMAP. The results presented in this work demonstrate the efficacy of our technique and the potential of this system for handling natural speech data recorded in non-ideal conditions as well as the unpredictability in aphasic speech patterns.	natural language;template matching	Duc Le;Emily Mower Provost	2014			natural language processing;rehabilitation;nonverbal communication;template matching;speech perception;speech recognition;aphasia;rhythm;pronunciation;artificial intelligence;computer science	HCI	-17.143990839556903	-82.7838407031054	100480
4c7b62921c90c3088b9a0673301be3e21c1ca2ef	sound source separation for robot audition using deep learning	speech recognition feature extraction microphones speech training robots neural networks;microphones;neural networks;training;speech;speech recognition hearing human robot interaction independent component analysis learning artificial intelligence neural nets principal component analysis robot programming source separation;feature extraction;robots;speech recognition;sound source separation signal to noise ratio nr filter separation function dnn deep neural network noise reduction nonlinear principal component analysis independent component analysis sss human machine interaction noise robust speech recognition deep learning robot audition	Noise robust speech recognition is crucial for effective human-machine interaction in real-world environments. Sound source separation (SSS) is one of the most widely used approaches for addressing noise robust speech recognition by extracting a target speaker's speech signal while suppressing simultaneous unintended signals. However, conventional SSS algorithms, such as independent component analysis or nonlinear principal component analysis, are limited in modeling complex projections with scalability. Moreover, conventional systems required designing an independent subsystem for noise reduction (NR) in addition to the SSS. To overcome these issues, we propose a deep neural network (DNN) framework for modeling the separation function (SF) of an SSS system. By training a DNN to predict clean sound features of a target sound from corresponding multichannel deteriorated sound feature inputs, we enable the DNN to model the SF for extracting the target sound without prior knowledge regarding the acoustic properties of the surrounding environment. Moreover, the same DNN is trained to function simultaneously as a NR filter. Our proposed SSS system is evaluated using an isolated word recognition task and a large vocabulary continuous speech recognition task when either nondirectional or directional noise is accumulated in the target speech. Our evaluation results demonstrate that DNN performs noticeably better than the baseline approach, especially when directional noise is accumulated with a low signal-to-noise ratio.	acoustic cryptanalysis;algorithm;artificial neural network;baseline (configuration management);covox speech thing;deep learning;futures studies;human–computer interaction;independent component analysis;noise reduction;nonlinear system;principal component analysis;robot;scalability;signal-to-noise ratio;source separation;speech recognition;vocabulary	Kuniaki Noda;Naoya Hashimoto;Kazuhiro Nakadai;Tetsuya Ogata	2015	2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)	10.1109/HUMANOIDS.2015.7363579	robot;speech recognition;feature extraction;computer science;speech;speech processing;artificial neural network	ML	-14.711606258077609	-90.26146124735783	100652
c38840081bc52e02635dfae042e9f08ce892ae6a	on-line incremental adaptation based on reinforcement learning for robust speech recognition.	robust speech recognition;reinforcement learning	We propose an incremental unsupervised adaptation method based on reinforcement learning in order to achieve robust speech recognition in various noisy environments. Reinforcement learning is a training method based on rewards that represents correctness of outputs instead of supervised data. The training progresses gradually based on rewards given. Our method is able to perform environmental adaptation without priori knowledge about such things as speakers and noises in noisy environments. We conducted speech recognition experiments using a connected digit recognition database. We demonstrate that our method has higher recognition performance than the conventional adaptation method.	correctness (computer science);experiment;reinforcement learning;speech recognition;teaching method;unsupervised learning	Masafumi Nishida;Yoshitaka Mamiya;Yasuo Horiuchi;Akira Ichikawa	2004			unsupervised learning;speech recognition;computer science;machine learning;pattern recognition;learning classifier system	ML	-17.425690859518443	-87.47154994113905	100766
a2ee1c3d880d67bca388de449681eda3f195cca9	using prosody and phonotactics in arabic dialect identiﬁcation	speaker identification;speech processing;modern standard arabic;automatic speech recognition;computer science;everyday life	While Modern Standard Arabic is the formal spoken and written language of the Arab world, dialects are the major communication mode for everyday life; identifying a speaker’s dialect is thus critical to speech processing tasks such as automatic speech recognition, as well as speaker identification. We examine the role of prosodic features (intonation and rhythm) across four Arabic dialects: Gulf, Iraqi, Levantine, and Egyptian, for the purpose of automatic dialect identification. We show that prosodic features can significantly improve identification, over a purely phonotactic-based approach, with an identification accuracy of 86.33% for 2m utterances.	semantic prosody;speaker recognition;speech processing;speech recognition	Fadi Biadsy;Julia Hirschberg	2009		10.7916/D8HM5HRV	natural language processing;speech recognition;computer science;speech processing;linguistics	NLP	-14.242823652595932	-84.7372505531296	100782
061e990baec598f1a93a45b1d8cc340e1742752e	noise robust real world spoken dialogue system using gmm based rejection of unintended inputs		To realize a robust spoken dialogue system for use in a real environment, the robust rejection of unintended inputs such as laughter, coughing, background speech and other noise based on GMM is implemented and examined on the basis of actual utterances. All the triggered inputs to a speech-oriented guidance system from 125 days of field tests in a public space are collected, and the occurrence of unintended inputs is investigated. GMM classifiers for voice categories (adult speech and child speech) and non-voice categories (laughter, coughing and other noises) are trained on the basis of the analysis result. The rejection performance of unintended speech was experimented on actual uncontrolled real inputs, and an EER of 3.32% was achieved by the 5-class GMM, which outperforms simple 2class (voice / non-voice) GMM. The rejection of background speech using GMM is also investigated.	dialog system;enhanced entity–relationship model;google map maker;guidance system;rejection sampling;spoken dialog systems;uncontrolled format string	Akinobu Lee;Keisuke Nakamura;Ryuichi Nisimura;Hiroshi Saruwatari;Kiyohiro Shikano	2004			speech recognition;natural language processing;spoken language;artificial intelligence;computer science	NLP	-14.28001644451374	-89.45171722784457	100963
7d06fd77999a7fba9722e4e553c557c55a6859e2	multi-pitch detection and voice assignment for a cappella recordings of multiple singers		This paper presents a multi-pitch detection and voice assignment method applied to audio recordings containing a cappella performances with multiple singers. A novel approach combining an acoustic model for multi-pitch detection and a music language model for voice separation and assignment is proposed. The acoustic model is a spectrogram factorization process based on Probabilistic Latent Component Analysis (PLCA), driven by a 6-dimensional dictionary with pre-learned spectral templates. The voice separation component is based on hidden Markov models that use musicological assumptions. By integrating the models, the system can detect multiple concurrent pitches in vocal music and assign each detected pitch to a specific voice corresponding to a voice type such as soprano, alto, tenor or bass (SATB). This work focuses on four-part compositions, and evaluations on recordings of Bach Chorales and Barbershop quartets show that our integrated approach achieves an F-measure of over 70% for frame-based multipitch detection and over 45% for four-voice assignment.	acoustic cryptanalysis;acoustic model;beneath a steel sky;dictionary;hidden markov model;language model;markov chain;performance;pitch (music);pitch detection algorithm;spectrogram	Rodrigo Schramm;Andrew Mcleod;Mark Steedman;Emmanouil Benetos	2017			speech recognition;pitch detection algorithm;computer science;acoustics	SE	-11.869035597947093	-92.1383527499352	100997
0cdba4fbcd71140a3fa58f8b03b40474a53dc84d	streaming end-to-end speech recognition for mobile devices		End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recognizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.	artificial neural network;finite-state machine;long tail;recurrent neural network;speech recognition;transducer	Yanzhang He;Tara N. Sainath;Rohit Prabhavalkar;Ian McGraw;Raziel Alvarez;Ding Zhao;David Rybach;Arputharaj Kannan;Yonghui Wu;Ruoming Pang;Qiao Liang;Deepti Bhatia;Yuan Shangguan;Bo Li;Golan Pundak;Khe Chai Sim;Tom Bagby;Shuo-Yiin Chang;Kanishka Rao;Alexander Gruenstein	2018	CoRR		machine learning;long tail;artificial intelligence;end-to-end principle;latency (engineering);use case;recurrent neural network;speech recognition;mobile device;transducer;computer science	NLP	-18.211097517373982	-88.02998214654986	101161
d6ee0f197f4d4a91e4a06aa32df725cf651deee7	the relative importance of the factors affecting recogniser performance with telephone speech.				Peter J. Wyard	1993			speech recognition;artificial intelligence;pattern recognition;computer science	HCI	-14.287534959052165	-87.4182549910187	101167
833a0e67821838c507dd0fc0ac7c3dae3952501c	smartspelltm: a multipass recognition system for name retrieval over the telephone	filtering;optimisation;filtrage;optimizacion;reconocimiento palabra;information retrieval;telephone;speech processing;filtrado;tratamiento palabra;traitement parole;indexing terms;telephony;prediction theory optimisation speech recognition telephony spectral analysis;automatic recognition;prediction theory;recherche information;telephony dictionaries speech recognition filtering automatic speech recognition information retrieval telephone sets delay cepstral analysis navigation;rasta plp multipass recognition system name retrieval telephone smartspell sup tm continuously spelled name recognition optimization spectral parameter filtering data base field evaluation relative spectral perceptual linear predictive parameterization;speech recognition;optimization;recuperacion informacion;reconnaissance parole;spectral analysis;telefono;perceptual linear predictive;reconocimiento automatico;reconnaissance automatique	We present a multipass recognition approach for continuously spelled name recognition. After describing this approach and presenting an optimization of the first pass using spectral parameter filtering techniques, we report on the evaluation of the whole system, both on a data base and during a four month field evaluation on two different tasks. The optimization of the first pass showed that relative spectral perceptual linear predictive (RASTA-PLP) parameterization is a good compromise between performance and speed. Evaluation on a data base yielded more than 95% name retrieval for a dictionary of 3388 names, while a field evaluation on small dictionaries (50 and 78 names) showed that nearly 90% of the calls were completed successfully. At the end of the paper, we discuss some of the insights provided by the field evaluation and show how this system can be extended to tasks with large dictionaries (on the order of 100000 names).		Jean-Claude Junqua	1997	IEEE Trans. Speech and Audio Processing	10.1109/89.554779	filter;computer vision;speech recognition;index term;computer science;artificial intelligence;machine learning;speech processing;telephony	Vision	-8.640383292937479	-93.10947545893104	101183
521bc4a266fd1721838243cd61d79f746a63f5db	source normalization training for hmm applied to noisy telephone speech recognition	speech recognition	We refer to environment e as some combination of speaker, handset, transmission channel and noise background condition, and regard any practical situation of a speech recognizer as a mixture of environments. A speech recognizer may be trained on multi-environment data. It may also need to adapt the trained acoustic models to new conditions. How to train an HMM with multienvironment data and from what seed model to start an adaptation are two questions of great importance. We propose a new solution to speech recognition which is based on, for both training and adaptation, a separate modeling of phonetic variation and environment variations. This problem is formulated under hidden Markov process, where we assume, Speech x is generated by some canonical (independent of environmental factors) distributions, An unknown linear transformation We and a bias be, speci c to environment e, is applied to x with probability P(e), x cannot be observed, what we observe is the outcome of the transformation: o =Wex+ be. Under maximum-likelihood (ML) criterion, by application of EM algorithm and the extension of Baum's forward and backward variables and algorithm, we obtained a joint solution to the parameters of the canonical distributions, the transformations and the biases, which is novel. For special cases, on a noisy telephone speech database, the new formulation is compared to per-utterance cepstral mean normalization (CMN) technique and shows more than 20% word error rate improvement.	acoustic cryptanalysis;acoustic model;baum–welch algorithm;cepstrum;channel (communications);expectation–maximization algorithm;finite-state machine;hidden markov model;speech recognition;word error rate	Yifan Gong	1997			artificial intelligence;voice activity detection;speech recognition;speaker recognition;pattern recognition;normalization (statistics);hidden markov model;computer science;speech processing	ML	-18.393723757950024	-91.84552406215467	101284
e41e194453acf31160db412ef2de1e272bf721e8	spoken digit recognition in portuguese using line spectral frequencies		Recognition of isolated spoken digits is the core procedure for a large and important number of applications mainly in telephone based services, such as dialing, airline reservation, bank transaction and price quotation, only using speech. Spoken digit recognition is generally a challenging task since the signals last for short period of time and often some digits are acoustically very similar to each other. The objective of this paper is to investigate the use of machine learning algorithms for digit recognition. We focus on the recognition of digits spoken in Portuguese. However, we note that our techniques are applicable to any language. We believe that the most important task for successfully recognizing spoken digits is the attribute extraction. Audio data is composed by a huge amount of very weak features, and most machine learning algorithms will not be able to build accurate classifiers. We show that Line Spectral Frequencies (LSF) provides a set of highly predictive coefficients for digit recognition. The results are superior than those obtained with state-of-the-art methods using Mel-Frequency Cepstrum Coefficients (MFCC) for digit recognition. In particular, we show that the choice of the right attribute extraction method is more important than the specific classification paradigm, and that the right combination of classifier and attributes can provide almost perfect accuracy.	line spectral pairs	Diego Furtado Silva;Vinícius M. A. de Souza;Gustavo E. A. P. A. Batista;Rafael Giusti	2012		10.1007/978-3-642-34654-5_25	communication;audiology	Vision	-9.914615051000347	-91.61981561614473	101315
3cb0b6dba10c6b5be81744576949dd0126492375	the impact of musicianship on the cortical mechanisms related to separating speech from background noise		Musicians have enhanced auditory processing abilities. In some studies, these abilities are paralleled by an improved understanding of speech in noisy environments, partially due to more robust encoding of speech signals in noise at the level of the brainstem. Little is known about the impact of musicianship on attention-dependent cortical activity related to lexical access during a speech-in-noise task. To address this issue, we presented musicians and nonmusicians with single words mixed with three levels of background noise, across two conditions, while monitoring electrical brain activity. In the active condition, listeners repeated the words aloud, and in the passive condition, they ignored the words and watched a silent film. When background noise was most intense, musicians repeated more words correctly compared with nonmusicians. Auditory evoked responses were attenuated and delayed with the addition of background noise. In musicians, P1 amplitude was marginally enhanced during active listening and was related to task performance in the most difficult listening condition. By comparing ERPs from the active and passive conditions, we isolated an N400 related to lexical access. The amplitude of the N400 was not influenced by the level of background noise in musicians, whereas N400 amplitude increased with the level of background noise in nonmusicians. In nonmusicians, the increase in N400 amplitude was related to a reduction in task performance. In musicians only, there was a rightward shift of the sources contributing to the N400 as the level of background noise increased. This pattern of results supports the hypothesis that encoding of speech in noise is more robust in musicians and suggests that this facilitates lexical access. Moreover, the shift in sources suggests that musicians, to a greater extent than nonmusicians, may increasingly rely on acoustic cues to understand speech in noise.	acoustic cryptanalysis;auditory perception;brain stem;contribution;downstream (software development);electroencephalography;hearing problem;impacted tooth;lexicon;noise (electronics);probability amplitude;topography;electrical activity	Benjamin Rich Zendel;Charles-David Tremblay;Sylvie Belleville;Isabelle Peretz	2015	Journal of Cognitive Neuroscience	10.1162/jocn_a_00758	psychology;communication	ML	-9.108123496590474	-81.2062892575038	101795
d504a108f1ecf905ff64d7e32126f7ba93ec0fd8	continuous wavelet transform based speech emotion recognition	speech processing;emotion recognition;pattern recognition;support vector machine;continuous wavelet transform;morlet wavelet	Emotion recognition from speech is one of the most interesting topics in research community and has developed to a great extent in the last few years. The real challenge in speech emotion recognition (ER) lies in the extraction of features that efficiently encapsulate the emotional information in speech and also do not depend on the speaker. This paper deals with the challenging task of speaker independent ER based on feature selection and classification algorithms. Features are selected based on continuous wavelet transform (CWT) and prosodic coefficients, and are classified and compared using support vector machine (SVM).	algorithm;coefficient;complex wavelet transform;continuous wavelet;emotion recognition;feature selection;support vector machine	Pankaj Shegokar;Pradip Sircar	2016	2016 10th International Conference on Signal Processing and Communication Systems (ICSPCS)	10.1109/ICSPCS.2016.7843306	psychology;speaker recognition;speech recognition;feature;pattern recognition;communication	Robotics	-8.690875868097516	-91.21151181319169	101800
372d17878d0ff276f0b39d16972d14272dc0c812	assessment of speech denoising using a perceptual measure-pesq		In this research work, speech denoising algorithms are assessed and their performances are evaluated using Perceptual Evaluation of Speech Quality (PESQ) measure. The spatial domain algorithms Spectral Subspace algorithms, Statistical model based algorithm, Spectral Subtraction algorithm and Wiener algorithm are compared with Neighshrink algorithm (a wavelet thresholding technique). Ten speech sentences from NOIZEUS database are taken for the study. Eight real world noises at 4 noise levels are used for the assessment. It is observed that the performance of the wavelet thresholding technique is better compared to the spatial domain techniques.		R. Dhivya;Judith Justin	2016		10.1007/978-3-319-60618-7_27	artificial intelligence;wavelet;noise reduction;pattern recognition;pesq;computer science;subtraction;subspace topology;speech enhancement;thresholding;statistical model	HCI	-13.363678592641913	-93.0463033356722	101815
0c46abec4ab68817243c180041d036dd7b98e2dc	on the role of formants in cognition of vowels and place of articulation of plosives	human cognition;plosives;formants;formant transitions;machine recognition;vowels	The paper examines the general notion that place of articulation of vowels and plosives are primarily cognized by the formant frequencies and the transitions thereof respectively. A brief status report on the objective classification of vowels based on formant frequencies is presented including an experimental study of relevant parameters of formant frequencies for machine classification of plosives from correctly spoken VCV syllables by native speakers. The paper contains details of the test for cognitive assessment of the aforesaid general notion. For this normal speech signals are altered using digital tools. The test bed is Bangla isolated vowels and plosives in nonsense VCV syllables. 30 native educated listeners are used. The result of the listening test is reported which shows that, in general, neither the frequencies (for vowel cognition) nor the transitions of the first two formants (for plosives) are necessary or sufficient for the cognition of the places of articulation.	biconnected component;cognition	Ashoke Kumar Datta;Bhaswati Mukherjee	2011		10.1007/978-3-642-31980-8_17	psychology;speech recognition;acoustics;communication	ML	-10.077269076139213	-83.30613018153409	101910
33dc270bc5474650da4b18f71ec3bf65515c7d01	jitter in sustained vowels and isolated sentences produced by dysphonic speakers	jitter;dysophonic speakers	Abstract   Jitter measures are known to discriminate between normal and dysphonic speakers. We investigated the influence of (i) speech material type (i.e. sustained vowels vs. isolated sentences); (ii) phonetic vowel quality; (iii) preprocessing; and (iv) speaker sex on the discriminatory performance of two pitch perturbation measures. The aim was to learn about the influence of experimental conditions on the output of dysphonic voice analysis systems. Two comparative studies were carried out. The first showed that as far as inter-vowel quality differences were concerned, all significant differences could be related to the idiosyncratic behaviour of several preprocessing schemes with reference to vowel quality. Intrinsic differences were canceled out by normalizing absolute jitter by the average fundamental period. As a rule of thumb preprocessing routines were more successful, the further  F  0  and  F  1  were apart. With all other experimental factors held constant, significant differences persisted between several preprocessing schemes, e.g. analysis by linear prediction failed on female voices and low-pass filtering eliminated so much fine signal details that discrimination between normal and dysphonic voices became impossible. In a second experiment, jitter values extracted from connected speech did not discriminate between normal and dysphonic speakers any more efficiently than values calculated from sustained vowels. As far as our corpora were concerned, no intrinsic superiority in the discrimination performance of connected speech as opposed to sustained vowels could be found. In the case of running speech absolute microperturbation values appeared to be higher during inter-segment transitions and during voice onset and offset.		Jean Schoentgen	1989	Speech Communication	10.1016/0167-6393(89)90068-X	speech recognition	NLP	-10.120521716642921	-82.90711409455245	101960
10a605b4d2411285213bd55c780cfae4b8091f5b	application of the dypsa algorithm to segmented time scale modification of speech	perceptual quality;time scale;speech processing gaussian processes mixture models;speech abstracts;motion video dypsa algorithm speech time scale modification voiced speech pseudoperiodic speech speech periodicity glottal closure instants gsi gaussian mixture model based vus classifier voiced unvoiced silence classifier listening test itu t p800 average mean opinion score perceptual quality modification factor normal talking rate fast original talking rate high audio quality voicemail messages forensics lip synchronization;gaussian mixture model;mean opinion score;vu	This paper presents a method for speech time scale modification. Voiced speech is pseudo-periodic, allowing time scale modification by the repetition or removal of cycles as necessary. However, in the case of unvoiced speech and at the boundaries of voiced speech, no such periodicity exists so the speech should not be modified. To address this issue, the proposed approach is novel in its use of the DYPSA algorithm to derive speech periodicity from glottal closure instants (GCIs), followed by a Gaussian Mixture model-based voiced/unvoiced/silence (VUS) classifier. A listening test based on ITU-T P800 has been conducted and has shown that, by employing VUS detection, the average mean opinion score of the perceptual quality of processed speech exceeds that of a method without VUS detection by 0.61 over a range of modification factors. Results are presented as a function of modification factor for normal and fast original talking rate. Reliable time scale modification of high audio quality enables many applications, such as time scale compression for fast scanning of recorded voicemail messages, slowing talking rate for improved intelligibility in forensics and lip synchronization in motion video.	algorithm;cycle (graph theory);intelligibility (philosophy);mixture model;pitch (music);quasiperiodicity;semantic prosody;ut-vpn;video	Mark R. P. Thomas;Jón Guðnason;Patrick A. Naylor	2008	2008 16th European Signal Processing Conference		voice activity detection;speech recognition;acoustics;computer science;communication	ML	-7.50142148365695	-89.36221858522582	101969
75c9a32f44827da51ea071d5773db68f394f4126	perception of speech rate and naturalness in synthetic slow speech		This paper details two perception experiments based on synthetic British English obtained with CART models predicting phone durations in slow speech from normal speed speech. Speech rate and naturalness were assessed by 6 English natives. Synthetic slow speech was rated as both slower and less natural than natural slow speech; however, the insertion of the pauses produced in natural slow speech into the synthetic recordings suggests a better approximation of natural speech in terms of both naturalness and perceived speech rate.	approximation;experiment;natural language;synthetic intelligence	Cyril Auran;Caroline Bouzon	2011			speech recognition;slow speech;perception;psychology;naturalness	ML	-11.298122346396577	-83.2348562937492	102084
21940b00845e0c84de3cc41c55fa16c0c694c06d	robust estimation, interpretation and assessment of likelihood ratios in forensic speaker recognition	bayesian framework;linguistique judiciaire;likelihood ratio;speaker identification;identification du locuteur;traitement automatique de la parole;robust estimator;speech processing;forensic linguistics;speaker recognition;assessment tool;automatic recognition;conferenceobject;computational linguistics;article;linguistique informatique;reconnaissance automatique	In this contribution, the Bayesian framework for interpretation of evidence when applied to forensic speaker recognition is introduced. Different aspects of the use of voice as evidence in the court are addressed, as well as the use by the forensic expert of the likelihood ratio as the right way to express the strength of the evidence. Details on computation procedures of likelihood ratios (LR) are given, along with the assessment tools and methods to validate the performance of these Bayesian forensic systems. However, due to the practical scarcity of suspect data and the mismatched conditions between traces and reference populations common in daily casework, Preprint submitted to Computer Speech and Language 26 July 2005 significant errors appear in LR estimation if specific robust techniques are not applied. Original contributions for the robust estimation of likelihood ratios are fully described, including TDLRA (Target Dependent Likelihood Ratio Alignment), oriented to guarantee the presumption of innocence of suspected but non-perpetrators speakers. These algorithms are assessed with extensive Switchboard experiments but moreover through blind LR-based submissions to both NFI-TNO 2003 Forensic SRE and NIST 2004 SRE, where the strength of the evidence was successfully provided for every questioned speech-suspect recording pair in the respective evaluations.	algorithm;computation;experiment;lr parser;population;speaker recognition;telephone switchboard;tracing (software)	Joaquín González-Rodríguez;Andrzej Drygajlo;Daniel Ramos-Castro;Marta Garcia-Gomar;Javier Ortega-Garcia	2006	Computer Speech & Language	10.1016/j.csl.2005.08.005	forensic linguistics;speaker recognition;robust statistics;speech recognition;likelihood-ratio test;computer science;artificial intelligence;computational linguistics;pattern recognition;speech processing;statistics	NLP	-12.121249184456037	-84.38478022230028	102142
4a5cde821adf80dcd5f3db916d5e293977c55691	comparison of gender- and speaker-adaptive emotion recognition		Deriving the emotion of a human speaker is a hard task, especially if only the audio stream is taken into account. While state-of-the-art approaches already provide good results, adaptive methods have been proposed in order to further improve the recognition accuracy. A recent approach is to add characteristics of the speaker, e.g., the gender of the speaker. In this contribution, we argue that adding information unique for each speaker, i.e., by using speaker identification techniques, improves emotion recognition simply by adding this additional information to the feature vector of the statistical classification algorithm. Moreover, we compare this approach to emotion recognition adding only the speaker gender being a non-unique speaker attribute. We justify this by performing adaptive emotion recognition using both gender and speaker information on four different corpora of different languages containing acted and non-acted speech. The final results show that adding speaker information significantly outperforms both adding gender information and solely using a generic speaker-independent approach.	algorithm;emotion recognition;feature vector;speaker recognition;statistical classification;streaming media;text corpus	Maxim Sidorov;Stefan Ultes;Alexander Schmitt	2014			speech recognition;emotion recognition;computer science;statistical classification;feature vector	NLP	-12.341900337746182	-88.79928108299578	102274
69e2744a24bfee3b120f2e6529a3ef96f31b3cc4	labeling audio-visual speech corpora and training an ann/hmm audio-visual speech recognition system	artificial neural network;hidden markov model	We presenta method to label an audio-visualdatabaseand to setup a systemfor audio-visualspeechrecognition based on a hybrid Artificial Neural Network/HiddenMarkov Model (ANN/HMM) approach. The multi-stagelabeling processis presentedon a new audiovisual databaserecordedat the Institute de la Communication Parlée(ICP).Thedatabasewasgeneratedvia transpositionof the audiodatabaseNUMBERS95.For thelabelingfirst a largesubsetof NUMBERS95is usedto achieve abootstraptrainingof an ANN, which canthenbeemployedto labeltheaudiopartof the audio-visualdatabase.This initial labeling is further improved via readaptingthe ANN to the new databaseandreperforming the labeling. From theaudiolabelingthenthevideo labelingis derived. Testsat differentSignalto NoiseRatios(SNR)areperformedto demonstratetheefficiency of the labelingprocess.Furthermore waysto incorporateinformationfrom alargeaudiodatabaseinto thefinal audio-visualrecognitionsystemwereinvestigated.	artificial neural network;audio-visual speech recognition;hidden markov model;linear algebra;text corpus;time-compressed speech	Martin Heckmann;Frédéric Berthommier;Christophe Savario;Kristian Kroschel	2000			artificial intelligence;audio mining;audio-visual speech recognition;speech recognition;acoustic model;artificial neural network;pattern recognition;transposition (music);hidden markov model;time delay neural network;computer science;signal-to-noise ratio	ML	-15.669021747076703	-86.84023098338014	102393
15421f95f480fa91ead851df029ec5a0a1bd4ced	a serious mobile game with visual feedback for training sibilant consonants		The distortion of sibilant sounds is a common type of speech sound disorder (SSD) in Portuguese speaking children. Speech and language pathologists (SLP) frequently use the isolated sibilants exercise to assess and treat this type of speech errors.	mobile game	Ivo Anjos;Margarida Grilo;Mariana Ascensão;Isabel Guimarães;João Magalhães;Sofia Cavaco	2017		10.1007/978-3-319-76270-8_30	speech recognition;multimedia;computer science;distortion;speech sound disorder;sibilant	HCI	-7.72097504976282	-84.05434862634112	102694
28ef48369ebeee8e8a4841e4c402a23509bfb567	co-channel speech separation for robust automatic speech recognition: stability and efficiency	front end;architectural acoustics;signal to interference ratio;correlation methods;robustness stability hidden markov models speech recognition upper bound acceleration decorrelation adaptive filters filtering time varying channels;acoustic filters;upper bound;automatic speech recognition;continuous speech recognition;adaptive filters;hidden markov models;acoustic filters speech recognition correlation methods adaptive filters architectural acoustics time varying channels hidden markov models;accelerated adaptation gain sequence co channel speech separation robust automatic speech recognition stability efficiency signal separation front end adaptive decorrelation filtering hmm based speaker independent continuous speech recognition system adf adaptation gain adaptation rate system efficiency room acoustic conditions time invariant channels time varying channels signal to interference ratio recognition word accuracy upper bound;speaker independent;speech recognition;time varying channel;room acoustics;time varying channels	A signal-separation front-end based on adaptive decorrelation ltering (ADF) was integrated with an HMM based speaker independent continuous speech recognition system for co-channel speech recognition. The ADF is improved by addressing the adaptation gain for system stability and e ciency: an upper bound of adaptation rate is derived for system stability, and an accelerated sequence of adaptation gain is introduced for system e ciency. The system was evaluated under simulated room acoustic conditions with both time-invariant and time-varying channels. It is shown that the system signi cantly improved the signal-to-interference ratio and the recognition word accuracy, and that the combination of the derived upper bound for adaptation rate with the accelerated adaptation gain sequence achieved the best performance for system stability and e ciency.	acoustic cryptanalysis;adaptive filter;decorrelation;hidden markov model;interference (communication);speech recognition;time-invariant system	Kuan-Chieh Yen;Yunxin Zhao	1997		10.1109/ICASSP.1997.596071	adaptive filter;speech recognition;signal-to-interference ratio;room acoustics;computer science;front and back ends;upper and lower bounds;hidden markov model	ML	-15.394344944644006	-92.38775320935247	102727
4008bd5521f30e9dfcecbee80989b55abac2a2d8	adaptation for soft whisper recognition using a throat microphone	feature space;linear regression;multivariate regression;word error rate	This paper describes various adaptation methods applied to recognizing soft whisper recorded with a throat microphone. Since the amount of adaptation data is small and the testing data is very different from the training data, a series of adaptation methods is necessary. The adaptation methods include: maximum likelihood linear regression, feature-space adaptation, and re-training with downsampling, sigmoidal low-pass filter, or linear multivariate regression. With these adaptation methods, the word error rate improves from 99.3% to 32.9%.	bit error rate;decimation (signal processing);general linear model;low-pass filter;microphone;sigmoid function;word error rate	Szu-Chen Stan Jou;Tanja Schultz;Alexander H. Waibel	2004			speech recognition;word error rate;upsampling;statistics;artificial intelligence;test data;feature vector;multivariate statistics;sigmoid function;computer science;linear regression;pattern recognition;throat microphone	ML	-14.01507815831875	-88.66084156956211	102759
f491a68c723e1a9667bf0fc12a3fc6d85833b875	finding `lucy in disguise': the misheard lyric matching problem	approximate pattern matching;pattern search;edit distance;misheard lyric;mean reciprocal rank;matching problems;matching;music information retrieval;mean average precision	We investigated methods for music information retrieval systems where the search term is a portion of a misheard lyric. Lyric data presents its own unique challenges that are different to related problems such as name search. We compared three techniques, each configured for local rather than global matching: edit distance, Editex, and SAPSL — a technique derived from Syllable Alignment Pattern Searching. Each technique was selected based on effectiveness at approximate pattern matching in related fields. Local edit distance and Editex performed comparably as evaluated with mean average precision and mean reciprocal rank. SAPS-L’s effectiveness varied between measures.	approximation algorithm;graph edit distance;information retrieval;list of to heart series characters;pattern matching;syllable	Nicholas Ring;Alexandra L. Uitdenbogerd	2009		10.1007/978-3-642-04769-5_14	matching;pattern search;mean reciprocal rank;speech recognition;edit distance;computer science;machine learning;pattern recognition;information retrieval	Web+IR	-8.174752984732038	-93.57173038476016	103457
d8b6944e0f0804d22cc3acbcd4e5555e4d20fda0	language identification for the automatic grapheme-to-phoneme conversion of foreign words in a german text-to-speech system	language identification;text to speech		language identification;speech synthesis	Peter Henrich	1989			speech recognition;language transfer;natural language processing;language identification;computer science;artificial intelligence;speech synthesis;german;grapheme	NLP	-15.987290449768063	-85.50490271997121	103458
6447a858770fbd16b0bbb9c77652efdea4487597	mask estimation in non-stationary noise environments for missing feature based robust speech recognition.	robust speech recognition	In missing feature based automatic speech recognition (ASR), the role of the spectro-temporal mask in providing an accurate description of the relationship between target speech and environmental noise is critical for minimizing the degradation in ASR word accuracy (WAC) as the signal-to-noise ratio (SNR) decreases. This paper demonstrates the importance of accurate characterization of instantaneous acoustic background for mask estimation in data imputation approaches to missing feature based ASR, especially in the presence of non-stationary background noise. Mask estimation relies on a hypothesis test designed to detect the presence of speech in time-frequency spectral bins under rapidly varying noise conditions. Masked melfrequency filter bank energies are reconstructed using a minimum mean squared error (MMSE) based data imputation procedure. The impact of this mask estimation approach is evaluated in the context of MMSE based data imputation under multiple background conditions over a range of SNRs using the Aurora 2 speech corpus.	acoustic cryptanalysis;aurora;elegant degradation;filter bank;geo-imputation;mean squared error;signal-to-noise ratio;speech corpus;speech recognition;stationary process	Shirin Badiezadegan;Richard C. Rose	2010			speaker recognition;speech recognition;computer science;pattern recognition	ML	-13.455659111027371	-92.19497702596856	103484
6c4481eff8719016e835a48892b1cd1e295def45	bowed string sequence estimation of a violin based on adaptive audio signal classification and context-dependent error correction	bowed string classification stringed instruments music signal processing;audio signal processing;instruments;music signal processing;bowed string sequence estimation;estimation method;audio classification;recognition accuracy;pattern classification error correction fingers instruments music audio recording data mining video recording robustness timbre;electric violin;stringed instruments;data mining;musical instruments;estimation;bowed string classification;error correction;feature extraction;signal processing;signal classification;adaptive audio signal classification;acoustic violin;recognition accuracy adaptive audio signal classification context dependent error correction bowed string sequence estimation violin playing music recordings f0 dependent features electric violin acoustic violin recognition error;robustness;context dependent;music recordings;context dependent error correction;recognition error;music;f0 dependent features;violin playing;signal classification audio signal processing error correction musical instruments	he sequence of strings played on a bowed string instrument is essential to understanding of the fingering. Thus, its estimation is required for machine understanding of violin playing. Audio-based identification is the only viable way to realize this goal for existing music recordings. A naive implementation using audio classification alone, however, is inaccurate and is not robust against variations in string or instruments. We develop a bowed string sequence estimation method by combining audio-based bowed string classification and context-dependent error correction. The robustness against different setups of instruments improves by normalizing the F0-dependent features using the average feature of a recording. The performance of error correction is evaluated using an electric violin with two different brands of strings and an acoustic violin. By incorporating mean normalization, the recognition error of recognition accuracy due to changing the string alleviates by 8 points, and that due to change of instrument by 12 points. Error correction decreases the error due to change of string by 8 points and that due to different instrument by 9 points.	acoustic coupler;acoustic cryptanalysis;algorithm;artificial intelligence;context-sensitive language;dd (unix);error detection and correction;feature selection;qr code;robustness (computer science);statistical classification;string (computer science)	Akira Maezawa;Katsutoshi Itoyama;Toru Takahashi;Tetsuya Ogata;Hiroshi G. Okuno	2009	2009 11th IEEE International Symposium on Multimedia	10.1109/ISM.2009.30	computer vision;estimation;speech recognition;audio signal processing;feature extraction;computer science;signal processing;music;statistics;robustness	Embedded	-11.865093974986738	-89.32427957946021	103694
14488eff98cfd960ba7ab004b213ef1d1a76f17b	the reverb challenge: a common evaluation framework for dereverberation and recognition of reverberant speech	reverberation;speech recognition reverberation signal processing speech enhancement;speech enhancement;speech speech recognition speech enhancement reverberation training noise measurement;challenge reverberant speech dereverberation asr evaluation;signal processing;speech recognition;reverb challenge recognition benchmark reverberant voice enhancement speech enhancement automatic speech recognition multichannel dereverberation single channel dereverberation reverberant speech signal processing common evaluation framework	Recently, substantial progress has been made in the field of reverberant speech signal processing, including both single- and multichannel dereverberation techniques, and automatic speech recognition (ASR) techniques robust to reverberation. To evaluate state-of-the-art algorithms and obtain new insights regarding potential future research directions, we propose a common evaluation framework including datasets, tasks, and evaluation metrics for both speech enhancement and ASR techniques. The proposed framework will be used as a common basis for the REVERB (REverberant Voice Enhancement and Recognition Benchmark) challenge. This paper describes the rationale behind the challenge, and provides a detailed description of the evaluation framework and benchmark results.	algorithm;automated system recovery;benchmark (computing);design rationale;signal processing;speech enhancement;speech processing;speech recognition	Keisuke Kinoshita;Marc Delcroix;Takuya Yoshioka;Tomohiro Nakatani;Armin Sehr;Walter Kellermann;Roland Maas	2013	2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics	10.1109/WASPAA.2013.6701894	speech recognition;acoustics;reverberation;computer science;signal processing;speech processing;physics	Vision	-13.918831336862075	-90.38486929007692	103780
821436f4912e03d8531fdec611cab1b3b0b425cc	a perceptual study for modelling speaker-dependent intonation in tts and dialog systems		In general, most of the developed prosody and intonation models were obtained from a statistical analysis of F0 curves and resynthesis by TTS. But there is yet another chance improving quality and naturalness: effective results can also be obtained by analysing the listeners’ common sense about natural intonational behavior. Therefore, we use a digital process that generates signals representing only the melody of the original speech signal. Comprehensive listening experiments become possible to analyse and compare the perception of natural and synthetic intonation. Based on the results of some listening experiments a statistical analysis of the F0 curves was carried out, regarding that a speaker-individual intonation model needs more quantitative F0 information than traditional descriptions. The aim is an prosodical speaker-dependent model for synthetic speech and dialog systems. Furthermore, this flexible approach should not be limited to speaker-individual intonation.	dialog system;experiment;netware file system;semantic prosody;synthetic intelligence;yet another	Joachim Mersdorf;Thomas Domhover	1997			yet another;speech recognition;perception;prosody;natural language processing;dialog box;naturalness;computer science;active listening;artificial intelligence	NLP	-14.62363637869802	-83.87088257241231	103913
9aaa1942465af1ac089824f8f5d4f2d0e18a981b	speech recognition of the letter 'zha' in tamil language using hmm	language use;hidden markov model;sampling frequency;linear prediction coding;first order;speech recognition	Speech signals of the letter ‘zha’ (H) in Tamil language of 3 males and 3 females were coded using an improved version of Linear Predictive Coding (LPC). The sampling frequency was at 16 kHz and the bit rate was at 15450 bits per second, where the original bit rate was at 128000 bits per second with the help of wave surfer audio tool. The output LPC cepstrum is implemented in first order three state Hidden Markov Model(HMM) chain.	cepstrum;data rate units;hidden markov model;linear predictive coding;markov chain;sampling (signal processing);speech recognition;stagefright (bug);three-state logic;wavesurfer	A. Srinivasan;K. Srinivasa Rao;K. Kannan;D. Narasimhan	2010	CoRR		natural language processing;speech recognition;computer science;pattern recognition;first-order logic;sampling;hidden markov model		-13.695939122405628	-86.26222859454042	103918
2e4d6435c4262ea80ca07ae9d010bb0b3b6314db	the wavelet packet based cepstral features for open set speaker classification in marathi	wavelet packet	In this paper, a new method of feature extraction based on perceptually meaningful subband decomposition of speech signal has been described. Dialectal zone based speaker classification in Marathi language has been attempted in the open set mode using a polynomial classifier. The method consists of dividing the speech signal into nonuniform subbands in approximate Mel-scale using an admissible wavelet packet filterbank and modeling each dialectal zone with the 2nd and 3rd order polynomial expansions of feature vector.	cepstrum;wavelet	Hemant A. Patil;Pranab K. Dutta;Tapan Kumar Basu	2005		10.1007/3-540-31314-1_15	pattern recognition;wavelet packet decomposition	Vision	-8.97654654390025	-90.74288191546694	104014
2ac3afa3608d9294ee88a6b2fd8d35650617d5e7	dependency modeling with bayesian networks in a voicemail transcription system.	bayesian network;probabilistic model	In this paper we apply Bayesian networks to the problem of voicemail transcription. We use a Bayesian network system to test a variety of probabilistic models that model acoustic context in addition to phonetic state and acoustic observations. We use a context variable that has the ability to model contextual phenomena that are not implied by the linguistic sequence of phones (e.g. noise level or sp eech rate). In rescoring experiments, we are able to get a slight gain over a more standard system with a similar number of parameters. We obtained the best performance by conditioning the mixture coefficients on context, thus implementing a state-wise tied mixture system. In an utteranceclustering system, analysis of the learned parameters indicates that the context variable is highly correlated with C0 andC1.	acoustic cryptanalysis;bayesian network;coefficient;experiment;noise (electronics);transcription (software)	Geoffrey Zweig;Mukund Padmanabhan	1999			variable-order bayesian network;bayesian programming;pattern recognition;probabilistic logic;dynamic bayesian network;computer science;statistical model;conditioning;bayesian network;machine learning;artificial intelligence;graphical model	ML	-17.97747490042794	-90.88904312850467	104039
87697847062e7eefec99a8c3aec888e3eef5e5b9	unsupervised learning of spoken language with visual context		Humans learn to speak before they can read or write, so why can’t computers do the same? In this paper, we present a deep neural network model capable of rudimentary spoken language acquisition using untranscribed audio training data, whose only supervision comes in the form of contextually relevant visual images. We describe the collection of our data comprised of over 120,000 spoken audio captions for the Places image dataset and evaluate our model on an image search and annotation task. We also provide some visualizations which suggest that our model is learning to recognize meaningful words within the caption spectrograms.	acoustic cryptanalysis;affinity analysis;artificial neural network;cluster analysis;computer;deep learning;dynamic time warping;humans;image retrieval;lexicon;network architecture;network model;spectrogram;speech recognition;unsupervised learning;visual objects;yet another	David F. Harwath;Antonio Torralba;James R. Glass	2016			natural language processing;speech recognition;computer science;machine learning	ML	-17.705680940171522	-80.7887943655665	104107
e84677229466b4cc31cc969e607888735479fe54	ilmsaf based speech enhancement with dnn and noise classification		In order to improve the performance of speech enhancement algorithm in low Signal-to-Noise Ratio (SNR) complex noise environments, a novel Improved Least Mean Square Adaptive Filtering (ILMSAF) based speech enhancement algorithm with Deep Neural Network (DNN) and noise classification is proposed. An adaptive coefficient of filter's parameters is introduced into conventional Least Mean Square Adaptive Filtering (LMSAF). First, the adaptive coefficient of filter's parameters is estimated by Deep Belief Network (DBN). Then, the enhanced speech is obtained by ILMSAF. In addition, in order to make the presented approach suitable for various kinds of noise environments, a new noise classification method based on DNN is presented. According to the result of noise classification, the corresponding ILMSAF model is selected in the enhancement process. The performance test results under ITU-TG.160 show that, the performance of the proposed algorithm tends to achieve significant improvements in terms of various speech subjective and objective quality measures than the wiener filtering based speech enhancement approach with Weighted Denoising Auto-encoder and noise classification.	speech enhancement	Ru-wei Li;Yanan Liu;Yong-qiang Shi;Liang Dong;Weili Cui	2016	Speech Communication	10.1016/j.specom.2016.10.008	speech recognition;computer science;machine learning;pattern recognition	NLP	-13.60752796015743	-92.30828410994246	104240
af9b27e7b6936ec79126facd8d681cd3c7ce0ce8	syllable nuclei detection using perceptually significant features		Speech can be segmented into syllables by identifying the syllable nuclei, which are points of high sonority. The excitation peaks in the linear prediction (LP) residual and the formant peaks can be interpreted as perceptually significant point features which contribute to the loudness of speech. In this paper, the use of these two point features is described for the use of detecting syllable nuclei. Each of these evidences contain information about different aspects of speech production, namely the glottal vibrations and the time varying vocal tract system. Thus it is possible that they contain complementary information about the syllable nuclei. Performance of the proposed syllable nuclei detection algorithm is evaluated for the TIMIT, Switchboard and the NTIMIT corpus. The proposed method performs comparably against two other state of the art syllable nuclei detection methods, and is shown to perform better for conversational speech. It is very fast and requires no training.	algorithm;sensor;syllable;timit;telephone switchboard;tract (literature)	Apoorv Reddy Arrabothu;Nivedita Chennupati;Bayya Yegnanarayana	2013			pattern recognition;speech recognition;artificial intelligence;syllable;computer science	ML	-12.215981522867297	-89.65414041667539	104393
7549b7c35ea22d2f11d3b0545a809a4211b815d0	a new method for segmenting continuous speech	speech processing speech recognition telephony;error rate speech recognition systems telephone services segmentation input utterance speech portion nonspeech portion recognition performance speaker dependent models public switched telephone network algorithm;automatic segmentation;speech processing;telephony;public switched telephone network;speaker dependent;error rate;speech recognition;speech recognition databases vocabulary telephony detectors speech enhancement noise level instruments error analysis target recognition	Speech recognition systems are increasingly utilized in various applications like telephone services where a user places a call by uttering the digits or the name of the person. One of the main problems in this application is the segmentation of the input utterance into speech and non-speech portions. Current approaches typically suffer from two problems. They either incorporate noise as a part of the word to be enrolled or falsely classify a portion of a word as noise. As a result, recognition performance suffers. In this paper, we present another approach to automatically segment continuous speech and create speaker dependent models. To verify our hypothesis, we use a database of 30 speakers whose speech has been recorded over the public switched telephone network. With this database, we benchmark our algorithm against a state of the art approach and show a 4X reduction in the error rate of the recognition system.	algorithm;benchmark (computing);image noise;regular expression;speech recognition;web service	Basavaraj I. Pawate;Eric M. Dowling	1994		10.1109/ICASSP.1994.389357	voice activity detection;natural language processing;speaker recognition;audio mining;speech recognition;word error rate;computer science;speech coding;speech processing;acoustic model;psqm;telephony;speech synthesis;speech analytics	ML	-16.056667003219044	-86.6896401053959	104415
6ad648955cf166325f864a4134c2b69ef4334fda	robust speech/non-speech detection in adverse conditions based on noise and speech statistics	speech detection	Recognition performance decreases when recognition systems are used over the telephone network, especially wireless network and noisy environments. It appears that non efficient sp eech/ non-speech detection is a very important source of this degradation. Therefore, speech detector robustness to noise is a challenging problem to be examined, in order to improve recognition performance for the very noisy communications. Speech collected in GSM environment gives an example of such very noisy speech to be recognized. Several studies were conducted aiming to improve the robustness of sp eech/ non-speech detection used for speech recognition in adverse conditions. This paper introduces a robust word boundary detection algorithm reliable in the very noisy cellular network environment. The algorithm is based on the statistics of noise and speech in the observed signal. In order to decide on the binary hypotheses of noise only versus speech plus noise, we use a likelihood ratio criterion.	algorithm;elegant degradation;image noise;speech recognition	Lamia Karray;Jean Monné	1998			speech recognition;voice activity detection;artificial intelligence;pattern recognition;computer science	ML	-13.092468662633761	-91.87407793351478	104580
6b08db4aa42586d9e5accc9912af09aa9143125a	spoken language discrimination using speech fundamental frequency	fundamental frequency			Shuichi Itahashi;Jian Xiong Zhou;Kimihito Tanaka	1994			speech recognition;computer science;spoken language;fundamental frequency	NLP	-14.313227826675567	-86.58695755889117	104593
8e72aaf34e124738596d355d4ebd31423b734eee	speech error evidence on the role of the vowel in syllable structure		s Speakers produced errors on vowels less often than on consonants, and on nuclei less often than on codas. s Rates of errors on CV and VC pairs were above chance s Errors on Vs most often occur with errors on at least one contiguous C, but not vice versa. s The Articulatory Phonology model of syllable structure, with the additional feature of sequential activation, would best predict these observed asymmetries.	articulatory phonology;syllable	Erin Rusaw;Jennifer Cole	2011			semivowel;mathematics;articulatory phonology;syllable;vowel;speech error;artificial intelligence;mid vowel;pattern recognition	NLP	-10.162126609501412	-80.49157556591466	104841
2272708eca06e0caa0e8dc9a5b3363810c1c36f7	robust parameter estimation for audio declipping in noise		Contemporary audio declipping algorithms often ignore the possibility of the presence of additive channel noise. If and when noise is present, however, the efficacy of any declipping algorithm is critically dependent on the accuracy with which clipped portions of the signal can be detected. This paper introduces an effective technique for inferring the amplitude and percentile values of the clipping threshold, and develops a statistically-optimal classification algorithm for accurately differentiating between clipped and unclipped samples in a noisy speech signal. The overall effectiveness of the clipped sample estimation algorithm is evaluated by the degree to which automatic speech recognition performance is improved when decoding clipped speech that has been declipped with state-of-the-art declipping algorithms paired with the clipped sample estimation algorithm. Up to 35% relative improvements in word error rate have been observed. Beyond the accuracy of the developed techniques, this paper generally underscores the necessity of robust parameter estimation methods for declipping in noise.	algorithm;estimation theory;noise (electronics);speech recognition;utility functions on indivisible goods;word error rate	Mark Harvilla;Richard M. Stern	2015			artificial intelligence;pattern recognition;speech recognition;estimation theory;computer science	HCI	-12.857829564078418	-91.51656838157021	105041
4afeb96554b5c2f2152386fcd4425bed884fd484	maximum likelihood learning of auditory feature maps for stationary vowels	em algorithm;maximum likelihood estimation;maximum likelihood estimate;expectation maximization;gaussian distribution;training data;statistical analysis;matching pursuit;parameterization;automatic speech recognition;signal processing;feature extraction;pattern recognition;pattern matching;maximum likelihood;speech recognition	In this paper, a mathematical framework for learning the acoustic features from a central auditory representation is presented. We adopt a statistical approach that models the learning process as to achieve a maximum likelihood estimation of the signal distribution. An algorithm, calledstatistical matching pursuit (SMP) , is introduced to identify regions on the cortical surface where the features for each sound class are most prominent. We model the features with distributions of Gaussian mixture densities, and employ the expectation-maximization (EM) procedure to both improve the parameterization and refine iteratively the selection of cortical regions from which the features are extracted. The learning algorithm is applied to vowel classification on TIMIT database where all the vowels (excluding diphthongs, nine in total) are regarded as individual classes. Experimental results show that models trained under SMP/EM algorithm achieve a comparable recognition accuracy to that of conventional recognizers.	acoustic cryptanalysis;expectation–maximization algorithm;finite-state machine;map;matching pursuit;stationary process;symmetric multiprocessing;timit	Kuansan Wang;Chin-Hui Lee;Biing-Hwang Juang	1996			speech recognition;computer science;machine learning;pattern recognition	ML	-18.121887357669976	-91.96479054881934	105084
545318792a923627fa1504f88b6499ff3971bc30	"""""""you made me do it"""": classification of blame in married couples' interactions by fusing automatically derived speech and language information"""		One of the goals of behavioral signal processing is the automatic prediction of relevant high-level human behaviors from complex, realistic interactions. In this work, we analyze dyadic discussions of married couples and try to classify extreme instances (low/high) of blame expressed from one spouse to another. Since blame can be conveyed through various communicative channels (e.g., speech, language, gestures), we compare two different classification methods in this paper. The first classifier is trained with the conventional static acoustic features and models “how” the spouses spoke. The second is a novel automatic speech recognition-derived classifier, which models “what” the spouses said. We get the best classification performance (82% accuracy) by exploiting the complementarity of these acoustic and lexical information sources through scorelevel fusion of the two classification methods.	acoustic cryptanalysis;complementarity theory;dyadic transformation;high- and low-level;interaction;oracle fusion architecture;signal processing;speech recognition	Matthew Black;Panayiotis G. Georgiou;Athanasios Katsamanis;Brian R W Baucom;Shrikanth (Shri) Narayanan	2011			speech recognition;machine learning	NLP	-12.960434027313088	-84.81859379936229	105140
522b719daf420667c850131f18a1d10ad78aca91	pitch-based emphasis detection for segmenting speech recordings	hidden markov model;statistical model;interactive system;high speed	This paper describes a technique to automatically locate emphasized segments of a speech recording based on pitch. These salient portions can be used in a variety of applications, but were originally designed to be used in an interactive system that enables high-speed skimming and browsing of speech recordings. Previous techniques to detect emphasis have used Hidden Markov Models; emphasized regions in close temporal proximity were found to successfully create useful summaries of the recordings. The new research described herein presents a simpler technique to detect salient segments and summarize a recording without using statistical models that require large amounts of training data. The algorithm adapts to the pitch range of a speaker, then automatically selects the regions of highest pitch activity as a measure of emphasis.	acoustic cryptanalysis;algorithm;apple lisa;hidden markov model;high- and low-level;interactivity;magnetoencephalography;markov chain;michele mosca;pitch (music);semantic prosody;statistical model;user interface	Barry Arons	1994			statistical model;computer vision;speech recognition;computer science;pattern recognition;mathematics;hidden markov model;statistics	HCI	-17.91775124493506	-82.72302750385245	105170
c11170c830de3ea834d6d533f515240d6f060c53	virtual adversarial training and data augmentation for acoustic event detection with gated recurrent neural networks		In this paper, we use gated recurrent neural networks (GRNNs) for efficiently detecting environmental events of the IEEE Detection and Classification of Acoustic Scenes and Events challenge (DCASE2016). For this acoustic event detection task data is limited. Therefore, we propose data augmentation such as on-the-fly shuffling and virtual adversarial training for regularization of the GRNNs. Both improve the performance using GRNNs. We obtain a segment-based error rate of 0.59 and an F-score of 58.6%.	acoustic cryptanalysis;artificial neural network;convolutional neural network;neural networks;recurrent neural network;sensor	Matthias Zöhrer;Franz Pernkopf	2017			speech recognition;artificial intelligence;pattern recognition;machine learning;recurrent neural network;adversarial system;computer science	NLP	-15.715695345764335	-88.26860242192254	105278
4db30bd087a147a217c76b88e09358d4b996bb49	on the development of an automatic voice pleasantness classification and intensity estimation system	text to speech synthesis;subtle emotions;ciencia;voice pleasantness;projetos;investigacao;publicacoes;perceptual speech analysis;iscte iul;article	In the last few years, the number of systems and devices that use voice based interaction has grown significantly. For a continued se of these systems, the interface must be reliable and pleasant in order to provide an optimal user experience. However there are urrently very few studies that try to evaluate how pleasant is a voice from a perceptual point of view when the final application s a speech based interface. In this paper we present an objective definition for voice pleasantness based on the composition of a epresentative feature subset and a new automatic voice pleasantness classification and intensity estimation system. Our study is ased on a database composed by European Portuguese female voices but the methodology can be extended to male voices or to ther languages. In the objective performance evaluation the system achieved a 9.1% error rate for voice pleasantness classification nd a 15.7% error rate for voice pleasantness intensity estimation. 2012 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;database;difference quotient;feature vector;l (complexity);netware file system;performance evaluation;quasiperiodicity;real life;titcoin;triple modular redundancy;user experience;voice font	Luís Pinto Coelho;Daniela Braga;José Miguel Salles Dias;Carmen García-Mateo	2013	Computer Speech & Language	10.1016/j.csl.2012.01.006	speech recognition;voice analysis	Mobile	-11.75559685816375	-87.17321723993852	105302
ce1924914e7ca6553be3f8e07ec0bc9c36e5e2a9	the role of word-initial glottal stops in recognizing english words		English word-initial vowels in natural continuous speech are optionally preceded by glottal stops or functionally equivalent glottalizations. It may be claimed that these glottal elements disturb the smooth flow of speech. However, they clearly mark word boundaries, which may potentially facilitate speech processing in the brain of the listener. The present study utilizes the word-monitoring paradigm to determine whether listeners react faster to words with or without glottalizations. Three groups of subjects were compared: Czech and Spanish learners of English and native English speakers. The results indicate that perceptual use of glottalization for word segmentation is not entirely governed by universal rules and reflects the mother tongue of the listener as well as the status (L1/L2) of the target language.	compiler;programming paradigm;speech processing;text segmentation	Maria Paola Bissiri;Maria Luisa García Lecumberri;Martin Cooke;Jan Volín	2011			glottalization;speech recognition;first language;czech;perception;computer science;speech processing;text segmentation	NLP	-10.657662605764	-81.00621164979634	105327
5fc9f53d279a53a119a37566c1311309bf0e18bd	a fast and scalable hybrid fa/ppca-based framework for speaker recognition	fast i vector extraction;speaker recognition;i vectors;ppca	A text-independent speaker recognition system using a hybrid Probabilistic Principal Component Analysis (PPCA) and conventional i-vector modeling technique is proposed. In this framework, the total variability space (TVS) is estimated using PPCA while the i-vectors of target speakers and test utterances are extracted using the conventional method. This leads to appreciable decrease in development time, while the time required for training and testing remains unchanged. In this a paper, an algorithmic optimization to the PPCAu0027s EM algorithm is developed. This is observed to provide a speed up of 3.7x. To simplify the testing procedure, two different approximation procedures are proposed to be used in this framework. The first approximation assumes a covariance matrix computed based on the PPCA framework. The second approximation proposes an optimization to avoid inverting the precision matrix of the i-vector. The comparison of time taken by these approximations with the baseline i-vector extraction procedure shows speed gains with some deterioration in performance in terms of the Equal Error Rate (EER). Among the proposed techniques, a best case trade-off is obtained with a speed up of 81.2x with deterioration in performance by 0.7% in absolute terms. Speaker recognition performances are studied on the telephone conditions of the benchmark NIST SRE 2010 dataset with systems built on the Mel Frequency Cepstral Co-efficient (MFCC) feature. A trade-off in the performance is observed when the proposed approximations are used. The scalability of these trade-offs is tested on the Mel Filterbank Slope (MFS) feature. The trade-offs observed with the approximations are reduced when the two systems are fused.	approximation;benchmark (computing);expectation–maximization algorithm;fractional anisotropy;html5 in mobile devices;moose file system;scalability;speaker recognition	Srikanth R. Madikeri	2014	Digital Signal Processing	10.1016/j.dsp.2014.05.012	speaker recognition;speech recognition;computer science;machine learning;pattern recognition;statistics	Robotics	-17.31704847949551	-91.78004065564897	105422
7c677d3955f871c64c0dc089055b7369b4fd09bb	lmdt: a weakly-supervised large-margin-domain-transfer for handwritten digit recognition	domain adaptation;handwriting recognition;writer adaptation;transfer learning;discriminant learning	Performance of handwritten character recognition systems degrades significantly when they are trained and tested on different databases. In this paper, we propose a novel large margin domain transfer algorithm, which is able to jointly reduce the data distribution mismatch of training (source) and test (target) datasets, as well as learning a target classifier by relying on a set of pre-learned classifiers with the labeled source data in addition to a few available target labels. The proposed method optimizes the combination coefficients of pre-learned classifiers to obtain the minimum mismatch between results on the source and target datasets. Our method is applicable both in semi-supervised and unsupervised domain adaptation scenarios, while most of the previous competing domain adaptation methods work only in semi-supervised scenario. Experiments on adaptation to different handwritten digit datasets demonstrate that this method achieves superior classification accuracy on target sets, comparing to the state of the art methods. Quantitative evaluation shows that an unsupervised adaptation reduces the error rates by 40.2% comparing with the SVM classifier trained by the labeled samples from the source domain.		Hamidreza Hosseinzadeh;Farbod Razzazi	2016	Eng. Appl. of AI	10.1016/j.engappai.2016.02.014	speech recognition;transfer of learning;computer science;machine learning;pattern recognition;handwriting recognition	AI	-16.183394843348236	-90.21168299992948	105490
a5a5b7ffd28c10636c728431b4563a1bce298646	measuring children's phonemic awareness through blending tasks		In this paper, speech recognition techniques are applied to automatically evaluate children’s phonemic awareness through three blending tasks (phoneme blending, onset-rhyme blending and syllable blending). The system first applies disfluency detection to filter out disfluent phenomena such as false-starts, sounding out, self-repair and repetitions, and to localize the target answer. Since most of the children studied are Hispanic, accent detection is applied to detect possible Spanish accent. The accent information is then used to update the pronunciation dictionaries and duration models. For valid words, forced alignment is applied to generate sound segmentations and produce the corresponding HMM log likelihood scores. Normalized spectral likelihoods and duration ratio scores are combined to assess the overall quality of the children’s productions. Results show that the automatic system correlates well with teachers, and requires no human supervision.	alpha compositing;automatic sounding;dictionary;hidden markov model;onset (audio);speech recognition;syllable	Shizhen Wang;Patti Price;Yi-Hui Lee;Abeer Alwan	2009			phonemic awareness;speech recognition;computer science;normalization (statistics);hidden markov model;syllable;pronunciation	NLP	-14.569229422576543	-82.41671068866938	105532
8f241d037f86ac246398e886e27a9a0af404cadb	speaker verification using gaussian mixture models within changing real car environments	gaussian mixture model	Gaussian Mixture Model (GMM) based speaker verification has been widely used recently. However, little research has been performed using GMMs for actual in-vehicle speaker verification. In this paper, we propose to integrate speaker verification and localization techniques for an in-vehicle speech dialog system to locate the desired speaker. The proposed solution is able to locate both desired and undesired speakers who are talking from the same position. This problem cannot be addressed by a simply speaker localization technique only. We demonstrate that using speech data collected in real car environments, the Equal Error rate (EER) performance approaches 0 using gender dependent data, and 2.35% and 13.34% using randomly selected data under idle and city noise environments, respectively.	dialog system;enhanced entity–relationship model;google map maker;mixture model;randomness;speaker recognition	Xianxian Zhang;John H. L. Hansen;Pongtep Angkititrakul;Kazuya Takeda	2005			mixture model;speech recognition;pattern recognition;machine learning;computer science;artificial intelligence	AI	-14.368071734013357	-89.66949382478596	105774
a570f35d5cdd39223b9ad158bd989a4cd7fc57ee	music genre classification based on local feature selection using a self-adaptive harmony search algorithm	information retrieval;classification;feature selection;harmony search algorithm	This paper proposes an automatic music genre-classification system based on a local feature-selection strategy by using a self-adaptive harmony search (SAHS) algorithm. First, five acoustic characteristics (i.e., intensity, pitch, timbre, tonality, and rhythm) are extracted to generate an original feature set. A feature-selection model using the SAHS algorithm is then employed for each pair of genres, thereby deriving the corresponding local feature set. Finally, each one-against-one support vector machine (SVM) classifier is fed with the corresponding local feature set, and the majority voting method is used to classify each musical recording. Experiments on the GTZAN dataset were conducted, demonstrating that our method is effective. The results show that the local-selection strategies using wrapper and filter approaches ranked first and third in performance among all relevant methods.	feature selection;harmony search;search algorithm	Yin-Fu Huang;Sheng-Min Lin;Huan-Yu Wu;Yu-Siou Li	2014	Data Knowl. Eng.	10.1016/j.datak.2014.07.005	speech recognition;biological classification;harmony search;computer science;machine learning;pattern recognition;feature selection	ML	-8.53950604113873	-92.38545748878846	106120
f04048834821a507db9186806dd1a488c3f8312f	automatic rating of hoarseness by text-based cepstral and prosodic evaluation		The standard for the analysis of distorted voices is perceptual rating of read-out texts or spontaneous speech. Automatic voice evaluation, however, is usually done on stable sections of sustained vowels. In this paper, text-based and established vowel-based analysis are compared with respect to their ability to measure hoarseness and its subclasses. 73 hoarse patients (48.3 ± 16.8 years) uttered the vowel /e/ and read the German version of the text “The North Wind and the Sun”. Five speech therapists and physicians rated roughness, breathiness, and hoarseness according to the German RBH evaluation scheme. The best human-machine correlations were obtained for measures based on the Cepstral Peak Prominence (CPP; up to |r | = 0.73). Support Vector Regression (SVR) on CPP-based measures and prosodic features improved the results further to r ≈ 0.8 and confirmed that automatic voice evaluation should be performed on a text recording.	cepstrum;news/north;spontaneous order;support vector machine;text-based (computing)	Tino Haderlein;Cornelia Moers;Bernd Möbius;Elmar Nöth	2012		10.1007/978-3-642-32790-2_70	speech recognition	NLP	-12.162538826237508	-87.56113502710747	106191
942e8273d672ff608dbd01f3cc740347afd6a518	detecting changing emotions in natural speech	emotion recognition;automatic speech classification;natural human speech	The goal of this research, was to develop a system that will automatically measure changes in the emotional state of a speaker, by analyzing his/her voice. Natural (non-acted) human speech of 77 (Dutch) speakers was collected and manually splitted into speech units. Three recordings per speaker were collected, in which he/she was in a positive, neutral and negative state. For each recording, the speakers rated 16 emotional states on a 10-point Likert Scale. The Random Forest algorithm was applied to 207 speech features that were extracted from recordings to qualify (classification) and quantify (regression) the changes in speaker's emotional state. Results showed that predicting the direction of change of emotions and the change of intensity, measured by Mean Squared Error, can be done better than the baseline (the mean value of change). Moreover, it turned out that changes in negative emotions are more predictable than changes in positive emotions.	algorithm;baseline (configuration management);mean squared error;random forest;sensor	Wojtek Kowalczyk;C. Natalie van der Wal	2012		10.1007/978-3-642-31087-4_51	speaker recognition;speech recognition;pattern recognition	AI	-11.308745773942174	-87.90057159271474	106243
46f2f4403e33215ba8039790e90dfd286bd50b63	target speech gmm-based spectral compensation for noise robust speech recognition	noise robustness;speech recognition	To improve speech recognition performance in adverse conditions, a noise compensation method is proposed that applies a transformation in the spectral domain whose parameters are optimized based on likelihood of speech GMM modeled on the feature domain. The idea is that additive and convolutional noises have mathematically simple expression in the spectral domain while speech characteristics are better modeled in the feature domain such as MFCC. The proposed method works as a feature extraction front-end that is independent from decoding engine, and has ability to compensate for non-stationary additive and convolutional noises with a short time delay. It includes spectral subtraction as a special case when no parameter optimization is performed. Experiments were performed using the AURORA-2J database. It has been shown that significantly higher recognition performance is obtained by the proposed method than spectral subtraction.	broadcast delay;convolutional neural network;feature extraction;google map maker;mathematical optimization;monomial;spectral method;speech recognition;stationary process;utility functions on indivisible goods	Takahiro Shinozaki;Sadaoki Furui	2009			speech recognition;computer science;pattern recognition	Vision	-14.512493633159767	-91.37064908521975	106298
2f7150221e05938df319f18a76ec12a02773112e	music chord recommendation of self composed melodic lines for making instrumental sound		Music chords are important to enable musical instruments to be played harmonically. Therefore, appropriate chord determination is essential. However, automatic chord determination using only monotonic melodic information is very difficult because chords can be subjectively variable. In this paper, we propose a method for the appropriate basic chord recommendation of melodic lines. The proposed method functions by using the notes feature of each bar based on the theory of harmony. The feature is defined by the pitch and duration of each note. Also, two chords in a bar can be estimated by using bar segmentation policy based on statistical analysis. The experimental results showed that compared with the original songs, an overall accuracy of 92.04 % (81 recommended chords/88 bars) was achieved. Subsequently, the proposed method was applied to our self-composed songs for which chords had not been determined. The results confirmed that the chords derived by our proposed method were fairly harmonious with the songs.	experiment;hit (internet);pitch (music)	Eui Chul Lee;Min-Woo Park	2016	Multimedia Tools and Applications	10.1007/s11042-016-3984-z	speech recognition;root;key	AI	-10.205006040043697	-85.11492270587289	106373
184cf3dd170d50cafcd3e024c7b6c42dbd25769e	a method for automatically estimating f0 model parameters and a speech re-synthesis tool using f0 model and straight.		In this paper, we describe a speech re-synthesis tool using the fundamental frequency (F0) generation model proposed by Fujisaki et al. and STRAIGHT, designed by Kawahara, which can be used for listening experiments by modifying F0 model parameters. To create the tool, we first established a method for automatically estimating F0 model parameters by using genetic algorithms. Next, we combined the proposed method and STRAIGHT. We can change the prosody of input speech by manually modifying the F0 model parameters with the tool and evaluate the relation between human perception and F0 model parameters. We confirmed the ability of this tool to make natural speech data that have various prosodic parameters.	experiment;genetic algorithm;natural language;semantic prosody	Shota Sato;Taro Kimura;Yasuo Horiuchi;Masafumi Nishida;Shingo Kuroiwa;Akira Ichikawa	2008			natural language processing;speech recognition;machine learning	NLP	-15.831465581927722	-83.59671189166818	106659
471730bef0ac49af3b216304047ec62862017d66	clustering of foot-based pitch contours in expressive speech		Intonation generation is still one of the weak links in the textto-speech synthesis chain. It is a hard enough task to generate expressively neutral pitch contours, with accurate placement of accents and phrase boundaries, but to generate appropriate intonation for expressive speech is even more of a challenge. This paper is a first attempt at describing and categorizing the variation in pitch contours that occur in expressive speech, which is a necessary step in the development of a new intonation model for expressive speech. The analysis is performed in the framework of the Generalized Linear Alignment model [10]. A hierarchical clustering technique of foot-based pitch contours revealed some interesting phenomena. Apart from the standard declining phrase curve, we observed phrase curves consisting of an incline, an optional plateau and a decline. These phrase curves are often observed on the last two feet making up a minor or major phrase. In addition, the continuation rise that is associated with marking the end of a minor phrase, only occurred in about 10% of the cases.	categorization;cluster analysis;continuation;hierarchical clustering;item unique identification;pitch (music);speech synthesis	Esther Klabbers;Jan P. H. van Santen	2004			continuation;cluster analysis;pattern recognition;artificial intelligence;hierarchical clustering;mathematics;phrase	NLP	-11.523295485788747	-82.58444922381821	106749
5dd6b7796874ba9e2ca6dc260dc35e8c341fb645	language family relationship preserved in non-native english		Mother tongue interference is the phenomenon where linguistic systems of a mother tongue are transferred to another language. Recently, Nagata and Whittaker (2013) have shown that language family relationship among mother tongues is preserved in English written by IndoEuropean language speakers because of mother tongue interference. At the same time, their findings further introduce the following two research questions: (1) Does the preservation universally hold in non-native English other than in English of Indo-European language speakers? (2) Is the preservation independent of proficiency in English? In this paper, we address these research questions. We first explore the two research questions empirically by reconstructing language family trees from English texts written by speakers of Asian languages. We then discuss theoretical reasons for the empirical results. We finally introduce another hypothesis called the existence of a probabilistic module to explain why the preservation does or does not hold in particular situations.	case preservation;family tree;indo;interference (communication);linguistic systems	Ryo Nagata	2014			natural language processing;non-native pronunciations of english;speech recognition;first language;language transfer;linguistics	NLP	-11.436381261279696	-80.91987690649772	106808
82c2adf09f30d23ad6dd7c3f7cee98be982fcfc3	performance of a speech synthesis system	speech synthesis;artificial speech;speech education;speech;performance factors;phonetics;language;man machine systems;systems approach	A system for synthesizing speech from a phonetic input is described. A string of phonetic symbols representing the sentence to be uttered is transformed into the control signals required by a parametric speech synthesizer using a small digital computer. The performance of the system was investigated by listening tests. In the first set of experiments consonant-vowel syllables were synthesized, and presented to listeners for identification. The vowels were readily identified, but the fricatives less so. In the second set of experiments the intelligibility of synthesized sentences was examined. It was found that after about an hour of transcribing the sentences, listeners identified about 90% of the words correctly.	speech synthesis	William A. Ainsworth	1974	International Journal of Man-Machine Studies	10.1016/S0020-7373(74)80016-7	natural language processing;phonetics;speech technology;speech recognition;speech perception;speech corpus;computer science;speech;speech processing;linguistics;chinese speech synthesis;language;speech synthesis;intelligibility;systems thinking	EDA	-15.419768054351406	-84.21944783247845	106877
6b69078206adbc8bda31c9db07d9348138abfea6	effectiveness of voice quality features in detecting depression		Automatic assessment of depression from speech signals is affected by variabilities in acoustic content and speakers. In this study, we focused on addressing these variabilities. We used a database comprised of recordings of interviews from a large number of female speakers: 735 individuals suffering from depressive (dysthymia and major depression) and anxiety disorders (generalized anxiety disorder, panic disorder with or without agoraphobia) and 953 healthy individuals. Leveraging this unique and extensive database, we built an i-vector framework. In order to capture various aspects of speech signals, we used voice quality features in addition to conventional cepstral features. The features (F0, F1, F2, F3, H1-H2, H2-H4, H4-H2k, A1, A2, A3, and CPP) were inspired by a psychoacoustic model of voice quality [1]. An i-vector-based system using Mel Frequency Cepstral Coefficients (MFCCs) and another using voice quality features was developed. Voice quality features performed as well as MFCCs. A score-level fusion was then used to combine these two systems, resulting in a 6% relative improvement in accuracy in comparison with the i-vector system based on MFCCs alone. The system was robust even when the duration of the utterances was shortened to 10 seconds.	acoustic cryptanalysis;coefficient;database;mel-frequency cepstrum;psychoacoustics;sensor	Amber Afshan;Jinxi Guo;Soo Jin Park;Vijay Ravi;Jonathan Flint;Abeer Alwan	2018		10.21437/Interspeech.2018-1399	speech recognition;artificial intelligence;pattern recognition;computer science	Web+IR	-11.934195800193443	-88.45443632152022	107048
ef4a31fde907c4920fc224c50d53b58f6bb4430c	robust mandarin speech recognition in car environments for embedded navigation system	car environments;front end;speech recognition navigation noise robustness acoustic noise working environment noise databases estimation error background noise piecewise linear techniques taylor series;log spectral minimum mean square error estimation algorithm;traffic engineering computing audio databases computational complexity mean square error methods natural languages speech recognition;acoustics;acoustic modeling;training;speech;taylor expansion;natural languages;embedded navigation system;speech enhancement;indexing terms;estimation algorithm;noise robustness;robust mandarin speech recognition;estimation;piece wise linear;traditional taylor expansion;computational complexity;chinese dialects diversity;indexation;mean square error methods;speech recognition;traffic engineering computing;robustness;navigation system;chinese dialects diversity robust mandarin speech recognition car environments embedded navigation system log spectral minimum mean square error estimation algorithm background noise supression traditional taylor expansion computational complexity noise over reduction;audio databases;noise over reduction;minimum mean square error;noise;background noise supression	A low-cost robust Mandarin speech recognition system is investigated for embedded car navigation application. In the front-end, log-spectral minimum mean-square error (LogMMSE) estimation algorithm is applied to suppress the background noise, and a piece-wise linear function is used to approximate the traditional Taylor expansion in its gain function calculation to reduce the computational complexity. After speech enhancement, spectral smoothing is implemented in both time and frequency indexes with geometric sequence weights to further compensate the spectral components distorted by noise over-reduction. In acoustic model training, an immunity learning scheme is applied, in which pre-recorded car noise is artificially added to clean training utterances to simulate the in-car environment. In the context of Mandarin speech recognition, a special difficulty is the diversity of Chinese dialects, i.e. the pronunciation difference among accents degrades the recognition performance if the acoustic models are trained with a mismatched accented database. We propose to train the models with multiple accented Mandarin databases to deal with this problem. Evaluation results of isolated phrase recognition confirm the effectivity of the proposed technologies.	acoustic cryptanalysis;acoustic model;approximation algorithm;automotive navigation system;computational complexity theory;database;embedded system;linear function;simulation;smoothing;speech enhancement;speech recognition;super robot monkey team hyperforce go!	Pei Ding;Lei He;Xiang Yan;Rui Zhao;Jie Hao	2008	IEEE Transactions on Consumer Electronics	10.1109/TCE.2008.4560134	minimum mean square error;estimation;speech recognition;index term;computer science;noise;speech;taylor series;front and back ends;natural language;computational complexity theory;robustness	Vision	-15.301750000074193	-92.28722942864903	107131
6abd2f7259df76011bc56bb8a20cc756d5838590	brno university of technology system for interspeech 2009 emotion challenge.		This paper describes Brno University of Technology (BUT) system for the Interspeech 2009 Emotion Challenge. Our submitted system for the Open Performance Sub-Challenge uses acoustic frame based features as a front-end and Gaussian Mixture Models as a back-end. Different feature types and modeling approaches successfully applied in speakerand language recognition are investigated and we can achieve an 16% and 9% relative improvement over the best dynamic and static baseline system on the 5-class task, respectively.	acoustic cryptanalysis;baseline (configuration management);mixture model	Marcel Kockmann;Lukás Burget;Jan Cernocký	2009			speech recognition;mixture model;computer science	NLP	-15.097848895785699	-88.76137986248736	107146
395451ad4e0c6a9ac307c1f58fcea99fca6dc53d	zcr-aided neurocomputing: a study with applications	pattern recognition and knowledge based systems prkbs;image border extraction;zero crossing rates zcrs;speech segmentation;feature extraction fe;biomedical signal analysis	This paper covers a particular area of interest in pattern recognition and knowledge-based systems (PRKbS), being intended for both young researchers and academic professionals who are looking for a polished and refined material. Its aim, playing the role of a tutorial that introduces three feature extraction (FE) approaches based on zero-crossing rates (ZCRs), is to offer cutting-edge algorithms in which clarity and creativity are predominant. The theory, smoothly shown and accompanied by numerical examples, innovatively characterises ZCRs as being neurocomputing agents. Source-codes in C/C++ programming language and interesting applications on speech segmentation, image border extraction and biomedical signal analysis complement the text. © 2016 Elsevier B.V. All rights reserved.	algorithm;code;compatibility of c and c++;computational neuroscience;feature extraction;knowledge-based systems;numerical analysis;pattern recognition;programming language;signal processing;smoothing;speech segmentation;zero crossing;zero-crossing rate	Rodrigo Capobianco Guido	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.05.011	speech recognition;computer science;artificial intelligence;machine learning;speech segmentation	AI	-5.38945573991695	-91.15628888832225	107190
77275fbb2324a404d68fad9c22b067876b64ea58	quantitative analysis of backchannels uttered by an interviewer during neuropsychological tests	backchannels;lexical markers;prosody;neuropsychological test	This paper examines in detail the backchannels uttered by a French professional interviewer during a neuropsychological test of verbal memories. These backchannels are short utterances such as oui, d’accord, uhm, etc. They are mainly produced here to encourage subjects to retrieve a set of words after their controlled encoding. We show that the choice of lexical items, their production rates and their associated prosodic contours are influenced by the subject performance and conditioned by the protocol.		Gérard Bailly;Frédéric Elisei;Alexandra Juphard;Olivier Moreaud	2016		10.21437/Interspeech.2016-22	linguistics;prosody	NLP	-12.752816498908981	-81.15767356027601	107295
097e89a5435b5a7438384b9e5c53a9f3e7d70dd0	evaluation of external and internal articulator dynamics for pronunciation learning	external and internal articulator dynamics;pronunciation learning;x-ray film	In this paper we present a data-driven 3D talking head system using facial video and a X-ray film database for speech research. In order to construct a database recording the three dimensional positions of articulators at phoneme-level, the feature points of articulators were defined and labeled in facial and X-ray images for each English phoneme. Dynamic displacement based deformations were used in three modes to simulate the motions of both external and internal articulators. For continuous speech, the articulatory movements of each phoneme within an utterance were concatenated. A blending function was also employed to smooth the concatenation. In audio-visual test, a set of minimal pairs were used as the stimuli to access the realistic degree of articulatory motions of the 3D talking head. In the experiments where the subjects are native speakers and professional English teachers, a word identification accuracy of 91.1% among 156 tests was obtained.	basis function;concatenation;displacement mapping;experiment;simulation;visual test;x-ray detector	Lan Wang;Hui Chen;JianJun Ouyang	2009			speech recognition	Vision	-14.574224382217023	-82.54859518418829	107798
8abca5f56b12d209bbce5f8b04c03375da2f8567	a rhythmic-prosodic model of poetic speech		In this paper a new approach towards the analysis of speech rhythm is presented. In the speech rhythm literature it was often discussed that rhythmic phenomena are more transparent in the metrical structure of orally produced poetry. However, up to now only a few phoneticians have worked on this special speaking style. For analyzing the rhythmic and prosodic patterns of this kind of speech, a corpus of read German poetry, including four different meters, was recorded. This study gives a first sight on durational and intonational effects in the data. A final prosodic modeling and its perceptual evaluation is currently under development.	text corpus	Jörg Bröggelwirth	2005			poetry;speech recognition;rhythm;computer science	NLP	-11.670268393009778	-82.59354223691787	107901
e0ca55905a73d7d109a9751debb5dc98455d0468	detection of laughter-in-interaction in multichannel close-talk microphone recordings of meetings	human interaction	Laughter is a key element of human-human interaction, occurring surprisingly frequently in multi-party conversation. In meetings, laughter accounts for almost 10% of vocalization effort by time, and is known to be relevant for topic segmentation and the automatic characterization of affect. We present a system for the detection of laughter, and its attribution to specific participants, which relies on simultaneously decoding the vocal activity of all participants given multi-channel recordings. The proposed framework allows us to disambiguate laughter and speech not only acoustically, but also by constraining the number of simultaneous speakers and the number of simultaneous laughers independently, since participants tend to take turns speaking but laugh together. We present experiments on 57 hours of meeting data, containing almost 11000 unique instances of laughter.	baseline (configuration management);experiment;microphone;precision and recall;text segmentation	Kornel Laskowski;Tanja Schultz	2008		10.1007/978-3-540-85853-9_14	interpersonal relationship;speech recognition;computer science	NLP	-13.496239925982687	-82.34726441804453	107920
3a9fee9064a487c35f3b3d875e7a13782dda2158	sfc: a trainable prosodic model	analyse parole;modelizacion;learning;generacion automatica;analisis palabra;implementation;intonation;speech analysis;automatic generation of prosody;automatic generation;analyse discours;aprendizaje;modelisation;analisis de contenido;apprentissage;generation automatique;prosodie;entonacion;implementacion;prosody;discourse analysis;modeling;prosodic modelling;prosodia	This paper introduces a new model-constrained and data-driven system to generate prosody from metalinguistic information. This system considers the prosodic continuum as the superposition of multiple elementary overlapping multiparametric contours. These contours encode specific metalinguistic functions associated with various discourse units. We describe the phonological model underlying the system and the specific implementation made of that model by the trainable prosodic model described here. The way prosody is analyzed, decomposed and modelled is illustrated by experimental work. In particular, we describe the original training procedure that enables the system to identify the elementary contours and to separate out their contributions to the prosodic contours of the training data. 2005 Elsevier B.V. All rights reserved.	coherence (physics);constraint (mathematics);declarative programming;deconvolution;discourse types;encode;entity–relationship model;feedback;input/output;interaction;quantum superposition;semantic prosody;semiconductor industry;space-filling curve;speech coding;text segmentation;triune continuum paradigm;utility functions on indivisible goods	Gérard Bailly;Bleicke Holm	2005	Speech Communication	10.1016/j.specom.2005.04.008	speech recognition;systems modeling;computer science;discourse analysis;linguistics;prosody;implementation	NLP	-15.394779627077673	-84.15483925944525	107980
5ff709f0afc5d0a077c292f4e86cd3087cab0a97	tandem decoding of children's speech for keyword detection in a child-robot interaction scenario	psi_speech;hidden markov model;keyword spotting;children s speech;dynamic bayesian networks;fau;dynamic bayesian network;long short term memory	In this article, we focus on keyword detection in children's speech as it is needed in voice command systems. We use the FAU Aibo Emotion Corpus which contains emotionally colored spontaneous children's speech recorded in a child-robot interaction scenario and investigate various recent keyword spotting techniques. As the principle of bidirectional Long Short-Term Memory (BLSTM) is known to be well-suited for context-sensitive phoneme prediction, we incorporate a BLSTM network into a Tandem model for flexible coarticulation modeling in children's speech. Our experiments reveal that the Tandem model prevails over a triphone-based Hidden Markov Model approach.	aibo;context-sensitive grammar;experiment;hidden markov model;long short-term memory;markov chain;robot;speech recognition;spontaneous order;tandem computers;triphone	Martin Wöllmer;Björn W. Schuller;Anton Batliner;Stefan Steidl;Dino Seppi	2011	TSLP	10.1145/1998384.1998386	natural language processing;speech recognition;computer science;machine learning;hidden markov model;dynamic bayesian network	NLP	-16.98261971359909	-86.4272544015346	108086
64379fc3ad1c716f38020aea0d4fdf405196ea59	using missing feature theory to actively select features for robust speech recognition with interruptions, filtering and noise kn-37		Speech recognizers trained with quiet wide-band speech degrade dramatically with high-pass, low-pass, and notch filtering, with noise, and with interruptions of the speech input. A new and simple approach to compensate for these degradations is presented which uses mel-filter-bank (MFB) magnitudes as input features and missing feature theory to dynamically modify the probability computations performed in Hidden Markov Model recognizers. When the identity of features missing due to filtering or masking is provided, recognition accuracy on a large talker-independent digit recognition task often rises from below 50% to above 95%. These promising results suggest future work to continuously estimate SNR's within MFB bands for dynamic adaptation of speech recognizers.	computation;filter bank;finite-state machine;hidden markov model;low-pass filter;markov chain;signal-to-noise ratio;speech recognition;speech synthesis	Richard Lippmann;Beth A. Carlson	1997			filter (signal processing);speech recognition;pattern recognition;machine learning;computer science;feature (machine learning);artificial intelligence	ML	-13.192193316478951	-91.34626545020971	108113
89f13923d15d2b8db988a3319aa54a0df346c3bb	experience-dependent influence of music and language on lexical pitch learning is not additive		Research studies provide evidence for the facilitative effects of musical and linguistic experience on lexical pitch learning. However, the effect of interaction of linguistic and musical pitch experience on lexical pitch processing is a matter of ongoing research. In the current study, we sought to examine the effect of combination of musical and linguistic pitch experience on learning of novel lexical pitch. Using a 10session pseudoword-picture association training paradigm, we compared the learning performance of musicians and nonmusicians who either spoke a non-tone language, spoke one tone language, or spoke two tone languages. Among the nontone language speakers, we found that musicians showed enhanced learning of novel lexical pitch as compared to nonmusicians. In comparison, among the tone-language speakers, we found no significant difference in the learning performance of musicians and non-musicians no matter they spoke one or more tone languages. We conclude that though musical experience facilitates linguistic pitch learning, the effects of combination of musical and linguistic pitch experience are not additive i.e. possessing both types of pitch experience is no better than possessing either one of them and knowing two tone languages does not facilitate the learning of a new tone language beyond the knowledge of one.	additive model;pitch (music);programming paradigm;utility functions on indivisible goods	Akshay Raj Maggu;Patrick C. M. Wong;Hanjun Liu;Francis C. K. Wong	2018		10.21437/Interspeech.2018-2104	speech recognition;computer science	NLP	-9.40889388779828	-82.13744396584246	108210
52ad334756e7fcd8acabfe7b510182cd4dfa9783	a one-class classification approach to generalised speaker verification spoofing countermeasures using local binary patterns	support vector machines;artificial signals one class classification approach generalised speaker verification spoofing countermeasures local binary patterns automatic speaker verification systems spoofing attacks speech signals analysis spectro temporal texture spoofed speech genuine speech i vector speaker verification system probabilistic linear discriminant analysis intersession compensation support vector machine classifier converted voice synthesized speech;speech processing;speaker recognition;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;signal classification;speech standards speech synthesis hidden markov models feature extraction support vector machines face recognition;support vector machines signal classification speaker recognition speech processing	The vulnerability of automatic speaker verification systems to spoofing is now well accepted. While recent work has shown the potential to develop countermeasures capable of detecting spoofed speech signals, existing solutions typically function well only for specific attacks on which they are optimised. Since the exact nature of spoofing attacks can never be known in practice, there is thus a need for generalised countermeasures which can detect previously unseen spoofing attacks. This paper presents a novel countermeasure based on the analysis of speech signals using local binary patterns followed by a one-class classification approach. The new countermeasure captures differences in the spectro-temporal texture of genuine and spoofed speech, but relies only on a model of the former. We report experiments with three different approaches to spoofing and with a state-of-the-art i-vector speaker verification system which uses probabilistic linear discriminant analysis for intersession compensation. While a support vector machine classifier is tuned with examples of converted voice, it delivers reliable detection of spoofing attacks using synthesized speech and artificial signals, attacks for which it is not optimised.	ambiguous name resolution;biometrics;experiment;linear discriminant analysis;local binary patterns;one-class classification;sensor;speaker recognition;speech synthesis;spoofing attack;support vector machine;text corpus	Federico Alegre;Asmaa Amehraye;Nicholas W. D. Evans	2013	2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)	10.1109/BTAS.2013.6712706	voice activity detection;speaker recognition;support vector machine;speaker diarisation;speech recognition;computer science;machine learning;pattern recognition;speech processing	SE	-11.074216830461502	-91.96740193567881	108245
afa85a6ab1d9e2752412fe3beea56afed5de1147	using exact locality sensitive mapping to group and detect audio-based cover songs	databases;exact locality sensitive hashing mapping;sequence comparison;quantization;audio recording internet concatenated codes usa councils music information retrieval libraries information science educational institutions computer science algorithm design and analysis;audio signal processing;locality sensitive hashing;cover songs exact locality sensitive hashing mapping semantic audio summarization group and detect;regression analysis audio signal processing music;regression model;manganese;accuracy;distance measurement;internet;feature extraction;indexation;semantic audio summarization;regression analysis;group and detect;cover songs;soft hash values exact locality sensitive mapping audio based cover song detection personal music recordings internet musical audio sequences exhaustive pairwise comparisons regression modeling indexing based approximate techniques;music	Cover song detection is becoming a very hot research topic when plentiful personal music recordings or performance are released on the Internet. A nice cover song recognizer helps us group and detect cover songs to improve the searching experience. The traditional detection is to match two musical audio sequences by exhaustive pairwise comparisons. Different from the existing work, our aim is to generate a group of concatenated feature sets based on regression modeling and arrange them by indexing-based approximate techniques to avoid complicated audio sequence comparisons. We mainly focus on using exact locality sensitive mapping (ELSM) to join the concatenated feature sets and soft hash values. Similarity-invariance among audio sequence comparison is applied to define an optimal combination of several audio features. Soft hash values are pre-calculated to help locate searching range more accurately. Furthermore, we implement our algorithms in analyzing the real audio cover songs and grouping and detecting a batch of relevant cover songs embedded in large audio datasets.	approximation algorithm;bioinformatics;british informatics olympiad;concatenation;embedded system;experiment;finite-state machine;hash function;internet;k-nearest neighbors algorithm;locality of reference;sensor;lsh	Yi Yu;J. Stephen Downie;Fabian Mörchen;Lei Chen;Kazuki Joe	2008	2008 Tenth IEEE International Symposium on Multimedia	10.1109/ISM.2008.18	speech recognition;computer science;machine learning;pattern recognition;database;world wide web;regression analysis;statistics	DB	-7.628984193836433	-93.15199745017466	108379
dc2f4b39ee2d1a941adf219e96c742cbb008a79e	perceptual, acoustic and electroglottographic correlates of 3 aggressive attitudes in french: a pilot study		This paper presents an experimental study of 136 utterances of aggressive attitudes in French: sarcastic irony, cold anger, hot anger, and a neutral control condition. Two male actors following a predefined scenario produced utterances on a controlled corpus including logatomes. Perceptual ratings by 28 listeners of aggressiveness, dominancy, and control among a subset of 24 utterances are compared to measurements of F0 and open quotient from EGG, vowel duration and formants values. More extreme forms of anger are rated as less controlled, more aggressive and dominant. Results indicate that the gradient of activation (sarcastic irony<cold anger<hot anger) corresponds to an increase of F0 (mean, range) and F1 values, in line with the literature. However, variation in F2 and F3 values suggest that articulatory gestures are not systematically stronger in most activated attitudes. Opposite strategies of vocal tension variation for the production of hot anger by different speakers are revealed by open quotient analysis. The comparison of relative influences of attitude and consonantal context on production parameters extracted from 128 utterances indicates that F2 and F3 are equally affected by both factors. Finally, limitations in the design of highly controlled though ecological corpora of attitudinal and emotional speech expressions are discussed.	acoustic cryptanalysis;control theory;experiment;gradient;speech synthesis;text corpus	Charlotte Kouklia;Nicolas Audibert	2013			speech recognition;computer science;perception	HCI	-10.27934679142183	-82.1941575332216	108419
0d05b24e924dc2ca9ea46d56fd94e138c890ad2c	expressive speech recognition and synthesis as enabling technologies for affective robot-child communication	multimedia;speech synthesis;database;base dato;hombre;robotics;enfant;reconocimiento voz;emotion emotionality;nino;prosodie;human;base de donnees;child;robotica;speech recognition;emotion emotivite;synthetiseur;sintesis palabra;robotique;emocion emotividad;reconnaissance parole;prosody;sintetizador;synthese parole;prosodia;homme;synthesizer	This paper presents our recent and current work on expressive speech synthesis and recognition as enabling technologies for affective robot-child interaction. We show that current expression recognition systems could be used to discriminate between several archetypical emotions, but also that the old adage ”there’s no data like more data” is more than ever valid in this field. A new speech synthesizer was developed that is capable of high quality concatenative synthesis. This system will be used in the robot to synthesize expressive nonsense speech by using prosody transplantation and a recorded database with expressive speech examples. With these enabling components lining up, we are getting ready to start experiments towards hopefully effective child-machine communication of affect and emotion.	concatenative synthesis;display resolution;experiment;robot;semantic prosody;speech recognition;speech synthesis	Selma Yilmazyildiz;Wesley Mattheyses;Yorgos Patsis;Werner Verhelst	2006		10.1007/11922162_1	natural language processing;el niño;speech recognition;computer science;prosody;robotics;speech synthesis	AI	-14.35248891308416	-81.77419768902558	108508
dd4c71e1506ad50ab241eb3756209bc4223538c7	an articulation model for audiovisual speech synthesis - determination, adjustment, evaluation	modelizacion;evaluation performance;ajustamiento modelo;performance evaluation;speech synthesis;articulation;evaluacion prestacion;speech processing;auditory visual speech perception;tratamiento palabra;traitement parole;audiovisual speech synthesis;articulation model;speech perception;articulacion;ajustement modele;modelisation;audiovisual integration;percepcion visual;model matching;integracion audiovisual;word recognition;perception visuelle;talking head;visual perception;sintesis palabra;joint;modeling;auditory visual;synthese parole;integration audiovisuelle	The authors present a visual articulation model for speech synthesis and a method to obtain it from measured data. This visual articulation model is integrated into MASSY, the Modular Audiovisual Speech SYnthesizer, and used to control visible articulator movements described by six motion parameters: one for the up-down movement of the lower jaw, three for the lips and two for the tongue. The visual articulation model implements the dominance principle as suggested by Löfqvist (1990). The parameter values for the model derive from measured articulator positions. To obtain these data, the articulation movements of a female speaker were measured with the 2D-articulograph AG100 and simultaneously filmed. The visual articulation model is adjusted and evaluated by testing word recognition in noise.	biconnected component;confusion matrix;digital video;intelligibility (philosophy);jean;lucas sequence;mcgurk effect;preprocessor;speech synthesis;synthetic intelligence;visual basic[.net]	Sascha Fagel;Caroline Clemens	2004	Speech Communication	10.1016/j.specom.2004.10.006	joint;speech recognition;systems modeling;speech perception;visual perception;word recognition;computer science;speech processing;linguistics;speech synthesis	HCI	-10.276318262274213	-86.3812114092755	108768
c712ababe22ac7fdc79b2dc54bd2751507db1b94	adding expressiveness to musical messages	acoustic signal processing;multimedia systems;open architecture;multimedia systems music acoustic signal processing;music multimedia systems protocols synthesizers audio recording humans natural languages timing timbre context modeling;multimedia musical messages expressiveness acoustic analysis perceptual analysis acoustic parameters hierarchical segmentation self similarity curves open architecture physical models wavetable;physical model;music	A system to add expressiveness to musical messages has been developed, starting from the results of acoustic and perceptual analyses. The system allows to obtain different performances, by modifying the acoustic parameters of a given neutral performance. The modification of the input performance is performed by a model that use the hierarchical segmentation of the musical organization. For every hierarchical level, opportune curves are applied to the principal acoustic parameters. Level’s sevsimilarity is the main criteria to construct the curves. The modular structure of the system defines an open architecture, where the rendering steps can be realized both with synthesis and post-processing techniques. Different synthesis techniques, like FM, physical models or wavetable have been explored.	acoustic cryptanalysis;fm broadcasting;hierarchical clustering;open architecture;performance;video post-processing	Sergio Canazza;Giovanni De Poli;Antonio Rodà;Alvise Vidolin	1999		10.1109/MMCS.1999.778531	speech recognition;open architecture;physical model;computer science;music;multimedia	Graphics	-15.05229416788112	-84.56408209978287	108870
03c1bd0f4d1fd2108e3f756ba4fc4fc9f0df521e	single-channel speech separation using sparse non-negative matrix factorization	speech;indexing terms;non negative matrix factorization;machine learning	We apply machine learning techniques to the problem of separ ating multiple speech sources from a single microphone record ing. The method of choice is a sparse non-negative matrix factori zation algorithm, which in an unsupervised manner can learn sparse representations of the data. This is applied to the learning of p ersonalized dictionaries from a speech corpus, which in turn are u sed to separate the audio stream into its components. We show tha t computational savings can be achieved by segmenting the tra ining data on a phoneme level. To split the data, a conventional spe ech recognizer is used. The performance of the unsupervised and supervised adaptation schemes result in significant improvem ents in terms of the target-to-masker ratio.	algorithm;dictionary;finite-state machine;machine learning;microphone;non-negative matrix factorization;sed;sparse matrix;speech corpus;streaming media;unsupervised learning	Mikkel N. Schmidt;Rasmus Kongsgaard Olsson	2006			incomplete cholesky factorization;sparse matrix;incomplete lu factorization;sparse approximation	ML	-16.17557244545426	-92.33685995547216	108937
88ed4592beae4501bc2890c17ac9823fe362bea2	the usage of independent component analysis for robust speaker verification	fastica;robust speaker verification;independent component analysis;speaker verification;independent component analysis ica;egld ica and pearson ica	This study employs independent component analysis (ICA) subspace feature selection for the robust speaker verification (SV). ICA subspace provides statistically independent basis that spans the same space and preserves the Euclidean distance measurements. These independent components are applied to a vector quantizer (VQ) SV system. In the feature space modification stage, a batch-mode FastICA algorithm and two adaptive algorithms EGLD-ICA and Pearson-ICA are employed for two-microphone case. As a result, the feature space is modified by a choice of independent component basis to obtain a lower classification error and a better generalization in real environments. The performance of the approach is demonstrated with YOHO database in various noise cases.	independent component analysis;speaker recognition;verification and validation	Ahmet Sentürk;Fikret S. Gürgen	2006			speech recognition;machine learning;pattern recognition;mathematics	Logic	-13.708540313086422	-92.83960024441798	109108
3f1c369b0b01ee9a6f3c62dc1327a0a0aae6248c	segmentation of speech using speaker identification	gaussian processes speaker recognition speech processing viterbi decoding hidden markov models gaussian distribution;duration model;likelihood ratio;speaker identification;initialization conversational speech segmentation speaker identification speaker segmentation viterbi decoding hidden markov model network interconnected speaker sub networks baum welch training agglomerative clustering distance measure likelihood ratio gaussian distributions duration model segmentation accuracy speaker labeled data;distance measure;gaussian processes;speech hidden markov models viterbi algorithm iterative decoding gaussian distribution indexing iterative algorithms streaming media cepstral analysis statistical distributions;hidden markov model;speech processing;baum welch;speaker recognition;hidden markov models;viterbi decoder;viterbi decoding;gaussian distribution	This paper describes techniques for segmentation of conversational speech based on speaker identity. Speaker seg-mentation is performed using Viterbi decoding on a hidden Markov model network consisting of interconnected speaker sub-networks. Speaker sub-networks are initialized using Baum-Welch training on data labeled by speaker, and are iteratively retrained based on the previous segmentation. If data labeled by speaker is not available, agglomerative clustering is used to approximately segment the conversational speech according to speaker prior to Baum-Welch training. The distance measure for the clustering is a likelihood ratio in which speakers are modeled by Gaussian distributions. The distance between merged segments is recomputed at each stage of the clustering, and a duration model is used to bias the likelihood ratio. Segmentation accuracy using agglomorative clustering initialization matches accuracy using initialization with speaker labeled data.	baum–welch algorithm;cluster analysis;energy (psychological);hidden markov model;markov chain;speaker recognition;welch's method	Lynn Wilcox;Francine Chen;Don Kimber;Vijay Balasubramanian	1994		10.1109/ICASSP.1994.389330	speaker recognition;speaker diarisation;speech recognition;computer science;machine learning;pattern recognition;viterbi decoder;hidden markov model;statistics	ML	-17.644984286026123	-93.57836655247523	109127
6cec7bd4f6cd68239609a6e24cc423124685d483	psychoacoustically-motivated adaptive β-order generalized spectral subtraction for cochlear implant patients	background noise;noise estimation;ci processor;speech intelligibility;colored noise;bioacoustics;residual noise;psychology cochlear implants working environment noise speech enhancement signal to noise ratio background noise colored noise speech recognition noise reduction frequency;ss algorithm;word in sentence recognition;cochlear implant patient;working environment noise;speech;psychology;speech enhancement;noise measurement;interference suppression;spectral subtraction;psychoacoustically motivated adaptive beta order generalized spectral subtraction;speech intelligibility psychoacoustically motivated adaptive beta order generalized spectral subtraction cochlear implant patient speech recognition noise reduction ci processor noise estimation residual noise word in sentence recognition white noise speech babble noise ss algorithm;estimation;ieee;noise reduction;band importance function;speech recognition;speech babble noise;signal to noise ratio;frequency;white noise bioacoustics cochlear implants interference suppression medical signal processing speech intelligibility speech recognition;cochlear implant;medical signal processing;white noise;adaptive β order gss;cochlear implants;cochlear implant speech intelligibility band importance function adaptive β order gss;noise	Many cochlear implant (CI) users are able to understand speech in quiet listening conditions, however, CI users' speech recognition deteriorates rapidly as the level of background noise increases. To make CI more applicable in reallife environments, noise reduction is needed in CI processor. Recently, we presented a psychoacoustically-motivated adaptive β-order generalized spectral subtraction (GSS) which deals with the weakness of the traditional SS algorithms [9, 10]. To apply this adaptive β-order GSS into CI processor, in this paper, we investigate the effects of noise estimation approaches and residual noise components for the proposed adaptive β-order GSS. Word-in-sentence recognition in steady white noise and speech babble noise was measured in four CI users. Experimental results showed that 1) noise estimation significantly affected performance of the proposed algorithm, 2) the algorithm with the least residual noise components was preferred by CI subjects, and 3) the proposed psychoacoustically-motivated adaptive β-order GSS outperformed the traditional SS algorithms.	algorithm;cochlear implant;noise reduction;psychoacoustics;speech recognition;white noise	Junfeng Li;Qian-Jie Fu;Hui Jiang;Masato Akagi	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960671	estimation;speech recognition;colors of noise;noise measurement;noise;speech;frequency;noise reduction;background noise;white noise;signal-to-noise ratio;intelligibility;bioacoustics;statistics	Robotics	-7.884257324005307	-88.51723867462816	109203
1bd5b74d5a851f12e6804c78ad6d1059e9e2305b	learning representations of affect from speech		There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes, i.e. categorical and dimensional affective traits from speech. We show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative of activation intensity and at separating out negative valence (sadness and anger) from positive valence (happiness). We experiment with different input speech features (such as FFT and log-mel spectrograms with temporal context windows), and different autoencoder architectures (such as stacked and deep autoencoders). We also learn utterance specific representations by a combination of denoising autoencoders and BLSTM based recurrent autoencoders. Emotion classification is performed with the learnt temporal/dynamic representations to evaluate the quality of the representations. Experiments on a well-established real-life speech dataset (IEMOCAP) show that the learnt representations are comparable to state of the art feature extractors (such as voice quality features and MFCCs) and are competitive with state-of-the-art approaches at emotion and dimensional affect recognition.	autoencoder;fast fourier transform;feature learning;machine learning;microsoft windows;noise reduction;real life;recurrent neural network;sadness;spectrogram;speech recognition	Sayan Ghosh;Eugene Laksana;Louis-Philippe Morency;Stefan Scherer	2015	CoRR		natural language processing;speech recognition;machine learning	NLP	-4.556863700678375	-87.17776565526528	109480
349513aa6d5f6605c353b79e2d22159f28886716	multi-modal person verification system based on face profiles and speech	speech multi modal person verification system face profiles features extraction nonhomogeneous classifiers normalization performance combined system noisy conditions;biometrics access control;australia robustness speech analysis cepstral analysis testing nose hair signal processing microelectronics electronic mail;image classification;speaker recognition;face recognition;feature extraction;sensor fusion;sensor fusion face recognition speaker recognition feature extraction image classification biometrics access control	This paper describes a person verification system based on facial profile views and features extracted from speech. The system is comprised of two non-homogeneous classifiers whose outputs are fused after a normalization step. Experiments are reported which show that integration of the face profile and speech information results in superior performance to that of its subsystems. Additionally, the performance of the combined system in noisy conditions is shown to be more robust than the speech-based subsystem alone.	database normalization;experiment;modal logic;signal-to-noise ratio;speaker recognition	Conrad Sanderson;Kuldip K. Paliwal	1999		10.1109/ISSPA.1999.815828	speaker recognition;computer vision;contextual image classification;speech recognition;feature extraction;computer science;pattern recognition;three-dimensional face recognition;sensor fusion	NLP	-12.593614819472702	-92.45478412464963	109524
3279e01f551a22d7de75422139cc2885da9f203a	improved template based chord recognition using the crp feature		The task of chord recognition in music signals is often based upon pattern matching in chromagrams. Many variants of chroma exist and quality of chord recognition is related to the feature employed. Chroma Reduced Pitch (CRP) features are interesting in this context as they were designed to improve timbre invariance for the purpose of query retrieval. Their reapplication to chord recognition, however, has not been successful in previous studies. We consider that the default parametrisation of CRP attenuates some tonal information, as well as timbral, and consider alternatives to this default. We also provide a variant of a recently proposed compositional chroma feature, adapted for music pieces, rather than one instrument. Experiments described show improved results compared to existing features.		Ken O'Hanlon;Sebastian Ewert;Johan Pauwels;Mark B. Sandler	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952167	invariant (physics);artificial intelligence;pattern recognition;pattern matching;hidden markov model;computer science;speech recognition;chord (music);timbre	Robotics	-8.538320387494931	-92.46201392993855	109602
24e3e82033e54f4d08022b35531ae6b9d8e2bc86	characterizing in-car conversational speech of different dialogue modes	loudness;conversational utterance;linear regression;speech recognition driver information systems loudness regression analysis road vehicles;in car conversational speech recognition;speech recognition;linear regression analysis;regression analysis;speech analysis speech recognition microphones humans automatic speech recognition information analysis linear regression vocabulary navigation audio recording;driver information systems;dialogue mode;linear regression analysis in car conversational speech recognition dialogue mode conversational utterance;road vehicles	The dependency of conversational utterances on the mode of dialogue is analyzed. A speech corpus of 800 speakers collected under three different modes, i.e., talking to a human operator, a WOZ system and an ASR system, is used for analysis. Some characteristics such as sentence complexity and loudness of the voice are found to be significantly different among the dialogue modes. Linear regression analysis results also clarify the relative importance of those characteristics on speech recognition accuracy	integrated woz machine;speech corpus;speech recognition	Hiroshi Fujimura;Chiyomi Miyajima;Nobuo Kawaguchi;Katsunobu Itou;Kazuya Takeda;Fumitada Itakura	2006	First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)	10.1109/ICICIC.2006.249	audio mining;speech recognition;speech corpus;computer science;linear regression;machine learning;statistics;speech analytics	NLP	-13.419776347324584	-88.20988866055609	109881
34be03039461cc81c22ef22c36ceedd89a3fb812	an easy approach for the classification of children's voices based on the fundamental frequency estimation		Voice disorders, also called dysphonia, are qualitative and quantitative alterations of the voice. These pathologies, unfortunately, affect from 6% to 38% of children in the world. Voice disorders may have a negative impact on communication effectiveness, social development and self-esteem. The first weapon against the diffusion and the worsening of these pathologies is prevention. Acoustic analysis is one of the most important tools to appraise the state of health of a voice. It provides information about the possible presence of voice disorders by evaluating specific parameters like the Fundamental Frequency. In this paper we present an easy approach based on a mobile application for voice screening in children. The app provides a robust methodology for the fundamental frequency estimation of the voice signal by analysing in real time a child’s signal. It consists of a continuous vocalization of the vowel /a/ of five seconds in length. The methodology is also able to evaluate undesired noise that can alter the Fundamental Frequency estimation and the correct classification of the evaluated voice signal as pathological or healthy.	mobile app;spectral density estimation;state of health	Laura Verde;Giuseppe De Pietro;Giovanna Sannino	2016		10.5220/0005849005700577	pattern recognition	Mobile	-6.28814854572722	-85.7511539775998	110174
1d5f44a6df3e76cbfebd28d913a69508c7777b91	investigating automatic assessment of reading comprehension in young children	au tomatic speech recognition;literacy assessment;automatic assessment;young children;reading comprehension;automatic speech recognition educational institutions natural languages automatic testing system testing statistics investments decoding microphones;indexing terms;children s speech;automatic speech recognition;statistical analysis;literacy assessment children s speech reading comprehension automatic speech recognition;children speech;speech recognition;elementary school;young children reading comprehension;statistical analysis speech recognition;children speech young children reading comprehension automatic speech recognition system;automatic speech recognition system	This paper describes a preliminary investigation into automatic assessment of reading comprehension in young children. In particular we studied the feasibility of automatic scoring of answers to open-ended questions related to the contents of a passage read by a child. Data from 70 children in grades 1 and 2 were used in this work. An automatic speech recognition system, especially trained for children's speech, was used for tracking the read passage, and two methods for automatic assessment were tested and compared with scores assigned by elementary school teachers. Automatic assessment showed a high kappa statistics agreement with evaluation scores obtained from teachers' scores, K=0.62, comparable to the inter-teacher agreement, K=0.64.	kappa calculus;nonlinear gameplay;speech recognition	Matteo Gerosa;Shrikanth (Shri) Narayanan	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518795	natural language processing;speech recognition;index term;computer science	NLP	-17.188034434994673	-82.0030453664319	110252
ba7da402e81de0b8dee0e3ca5a36348d01774f4d	matching training and test data distributions for robust speech recognition	robust speech recognition;word error rate;feature transformation;signal analysis;feature space;noise robustness;data distribution;feature space rotation;feature extraction;normalization;histogram normalization	In this work normalization techniques in the acoustic feature space are studied that aim at reducing the mismatch between training and test by matching their distributions. Histogram normalization is the first technique explored in detail. The effect of normalization at different signal analysis stages as well as training and test data normalization are investigated. The basic normalization approach is improved by taking care of the variable silence fraction. Feature space rotation is the second technique that is introduced. It accounts for undesired variations in the acoustic signal that are correlated in the feature space dimensions. The interaction of rotation and histogram normalization is analyzed and it is shown that the recognition accuracy is significantly improved by both techniques on corpora with different complexity, acoustic conditions, and speaking styles. The word error rate is reduced from 24.6% to 21.8% on  VerbMobil II , a German large vocabulary conversational speech task, and from 16.5% to 15.5% on  EuTrans II , an Italian speech corpus of conversational speech over telephone. On the  CarNavigation  task, a German isolated-word corpus recorded partly in noisy car environments, the word error rate is reduced from 74.2% to 11.1% for heavy mismatch conditions between training and test.	speech recognition;test data	Sirko Molau;Daniel Keysers;Hermann Ney	2003	Speech Communication	10.1016/S0167-6393(03)00085-2	speech recognition;feature vector;feature extraction;word error rate;computer science;machine learning;normalization;signal processing;pattern recognition	ML	-14.01267072648426	-90.52260734843229	110302
08c08d86b040a010f6789b3aa69ac3dbfe28afec	who knows carl bildt? - and what if you don't?		One problem with using speaker identification by witnesses in legal settings is that high quality imitations can result in speaker misidentification. A recent series of experiments has looked at listener acceptance of an imitation of a well known Swedish politician. Results showed that listener expectation of the topic of an imitated passage impacts on the acceptance or rejection of the imitation. The strength of that impact varied according to various listener characteristics, including age of listener. It is likely that age reflected the degree of familiarity with the voice that was being imitated. The present study has reanalyzed the data from Swedish listeners in the previous studies to look at performance according to self reports of whether the listeners were familiar with the politician. Results showed that the acceptance of the imitation by those listeners who reported knowing the politician was more influenced by the topic of the imitated passage than by those who reported not knowing him. Implications of this finding in regard to listeners’ choice of alternate voices in the line up are discussed.	display resolution;experiment;rejection sampling;speaker recognition;what if	Elisabeth Zetterholm;Kirk P. H. Sullivan;James Green;Erik J. Eriksson;Jan van Doorn;Peter E. Czigler	2003				HCI	-10.476269133332387	-82.59247615047781	110348
cb956c31b9e9b67f2860402f6de9d42603f1175a	deep order statistic networks	training neural networks detectors annealing acoustics speech conferences;statistical analysis multilayer perceptrons speech recognition;multi layer perceptrons;automatic speech recognition systems deep order statistic networks maxout networks max nonlinearity order statistic nonlinearity sortout nonlinearity input activations osn interpolation weights network activation linear network model aggregation techniques dropout drop connect annealed dropout word error rate performance improvement wer performance improvement;deep neural networks;order statistic networks;maxout networks;rectified linear units;multi layer perceptrons order statistic networks maxout networks rectified linear units deep neural networks	Recently, Maxout networks have demonstrated state-of-the-art performance on several machine learning tasks, which has fueled aggressive research on Maxout networks and generalizations thereof. In this work, we propose the utilization of order statistics as a generalization of the max non-linearity. A particularly general example of an order-statistic non-linearity is the “sortout” non-linearity, which outputs all input activations, but in sorted order. Such Order-statistic networks (OSNs), in contrast with other recently proposed generalizations of Maxout networks, leave the determination of the interpolation weights on the activations to the network, and remain conditionally linear given the input, and so are well suited for powerful model aggregation techniques such as dropout, drop connect, and annealed dropout. Experimental results demonstrate that the use of order statistics rather than Maxout networks can lead to substantial improvements in the word error rate (WER) performance of automatic speech recognition systems.	dropout (neural networks);interpolation;machine learning;nonlinear system;speech recognition;word error rate	Steven J. Rennie;Vaibhava Goel;Samuel Thomas	2014	2014 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2014.7078561	speech recognition;computer science;artificial intelligence;machine learning;rectifier;statistics	ML	-18.377592376426325	-89.32647221706938	110400
4dcb213b015860fa35a11c20931384b2edb3fef3	toward musically-motivated audio fingerprints	audio signal processing;cover song identification fingerprinting spectral peaks music representations audio matching;query processing;signal representation audio recording audio signal processing music query processing;spectral peaks;audio recording;fingerprinting;lead;abstracts lead;abstracts;audio matching;signal representation;song identification musically motivated audio fingerprinting techniques audio recording musical variations spectrogram musical feature representations modified peak fingerprints query length audio matching;music representations;cover song identification;music	In this paper, we investigate to which extent well-known audio fingerprinting techniques, which aim at identifying a specific audio recording, can be modified to also deal with more musical variations. To this end, we replace the standard peak fingerprints based on a spectrogram by peak fingerprints based on other more “musical” feature representations. Our systematic experiments show that such modified peak fingerprints allow for a robust identification of different versions and performances of the same piece of music if the query length is at least 15 seconds. This indicates that highly efficient audio fingerprinting techniques can also be applied to accelerate tasks such as audio matching or cover song identification.	acoustic fingerprint;experiment;fingerprint (computing);performance;spectrogram	Peter Grosche;Meinard Müller	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6287825	lead;audio mining;speech recognition;audio signal processing;computer science;music	SE	-8.078161976367312	-92.19923162790711	110719
16f940b4b5da79072d64a77692a876627092d39c	a framework for automated measurement of the intensity of non-posed facial action units	kernel;pediatrics;high dimensionality;support vector machines;support vector machines face recognition regression analysis;facial image analysis;humans face detection image analysis psychology face recognition support vector machines support vector machine classification shape behavioral science image texture analysis;training;psychology;gold standard;lip corner puller;behavioral science;support vector machine classifier;face recognition;shape;facial action coding system;spectral regression technique;action unit;support vector machine classification;image analysis;regression analysis;face;humans;image texture analysis;infant expression;support vector machine;facial expression;face detection;face to face;support vector machine classifier nonposed facial action facial action coding system lip corner puller cheek raising infant expression facial image analysis spectral regression technique;videos;nonposed facial action;cheek raising	This paper presents a framework to automatically measure the intensity of naturally occurring facial actions. Naturalistic expressions are non-posed spontaneous actions. The facial action coding system (FACS) is the gold standard technique for describing facial expressions, which are parsed as comprehensive, nonoverlapping action units (Aus). AUs have intensities ranging from absent to maximal on a six-point metric (i.e., 0 to 5). Despite the efforts in recognizing the presence of non-posed action units, measuring their intensity has not been studied comprehensively. In this paper, we develop a framework to measure the intensity of AU12 (lip corner puller) and AU6 (cheek raising) in videos captured from infant-mother live face-to-face communications. The AU12 and AU6 are the most challenging case of infant's expressions (e.g., low facial texture in infant's face). One of the problems in facial image analysis is the large dimensionality of the visual data. Our approach for solving this problem is to utilize the spectral regression technique to project high dimensionality facial images into a low dimensionality space. Represented facial images in the low dimensional space are utilized to train support vector machine classifiers to predict the intensity of action units. Analysis of 18 minutes of captured video of non-posed facial expressions of several infants and mothers shows significant agreement between a human FACS coder and our approach, which makes it an efficient approach for automated measurement of the intensity of non-posed facial action units.	action potential;active appearance model;bcs-facs;coefficient;image analysis;locality of reference;maximal set;parsing;spontaneous order;support vector machine	Mohammad H. Mahoor;Steven Cadavid;Daniel S. Messinger;Jeffrey F. Cohn	2009	2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2009.5204259	facial recognition system;support vector machine;computer vision;image analysis;speech recognition;computer science;machine learning;pattern recognition;geometry	Vision	-11.63254872934288	-88.4323216277099	110905
1a34854a1904103a8330b8703e175e3ab59e59e7	context-dependent pattern simplification by extracting context-free floating qualifiers		Qualification may occur anywhere within a temporal utterance. To reduce the ensuing pattern complexity for context-dependent systems such as Enguage (^{mathrm{TM}}), it is necessary to remove the qualified value from the utterance; rendering the utterance atemporal and presenting the value as the contextual variable when. This is possible because a qualifier—at 7:30 or until today—is immediately recognisable as such if preceding a time value: when is context-free. This appropriation gives insight into the nature of the context-dependent processing of habitual natural language. While the difference between the resultant concepts—how many coffees do I have and how old am I—is perhaps not that great despite their differing origins, this work ensures the mediation system remains practical and effective. This research is informed by a prototype for the health-tech app Memrica Prompt in support of independent living for people with early stage dementia.	text simplification	Martin J. Wheatman	2016		10.1007/978-3-319-47175-4_14	rendering (computer graphics);natural language processing;time value of money;appropriation;natural language;utterance;mathematics;mediation (marxist theory and media studies);independent living;artificial intelligence	ML	-8.698538297972881	-80.28188373428414	111065
3303a6699b50617e04c6e86b82ee6b9967a53916	continuous speech recognition	spectrum;fast fourier transform;continuous speech recognition;group work;steady state	This group works towards automatic transcription of continuous speech with a vocabulary and syntax as unrestricted as possible. It is a long-term effort; however, an experimental system is operational. The acoustic processor contains a spectrum analyzer based on the Fast Fourier Transform and a phone segmenter/recognizer which makes use of transitional and steady-state information in its classification. The linguistic processor accepts an imperfect string of phones and produces an estimated transcription of the speech input.	acoustic cryptanalysis;experimental system;fast fourier transform;finite-state machine;spectrum analyzer;speech recognition;steady state;transcription (software);vocabulary	Frederick Jelinek	1977	SIGART Newsletter	10.1145/1045283.1045302	voice activity detection;natural language processing;spectrum;fast fourier transform;speech recognition;computer science;speech processing;steady state	NLP	-15.63908412649589	-84.65369912990397	111148
1c183d9fcce91ca47531f53be54d1e66bbe207ca	multi-stream asynchrony modeling for audio-visual speech recognition	word model;adbn model;phone level;visual stream;mm-adbn model;audio-visual speech recognition;phone model;multi-stream asynchrony modeling;ms-adbn model;network model;observation node level;hidden node;dynamic bayesian network;speech recognition	In this paper, two multi-stream asynchrony Dynamic Bayesian Network models (MS-ADBN model and MM-ADBN model) are proposed for audio-visual speech recognition (AVSR). The proposed models, with different topology structures, loose the asynchrony of audio and visual streams to word level. For MS-ADBN model, both in audio stream and in visual stream, each word is composed of its corresponding phones, and each phone is associated with observation vector. MM- ADBN model is an augmentation of MS-ADBN model, a level of hidden nodes--state level, is added between the phone level and the observation node level, to describe the dynamic process of phones. Essentially, MS-ADBN model is a word model, while MM-ADBN model is a phone model. Speech recognition experiments are done on a digit audio-visual (A-V) database, as well as on a continuous A-V database. The results demonstrate that the asynchrony description between audio and visual stream is important for AVSR system, and MM-ADBN model has the best performance for the task of continuous A-V speech recognition.	asynchronous i/o;asynchrony (computer programming);audio-visual speech recognition;dynamic bayesian network;experiment;microsoft windows;streaming media	Guoyun Lv;Dongmei Jiang;Rongchun Zhao;Yunshu Hou	2007	Ninth IEEE International Symposium on Multimedia (ISM 2007)	10.1109/ISM.2007.21	natural language processing;audio mining;speech recognition;computer science;acoustic model;hidden node problem;dynamic bayesian network	Vision	-17.13137665780456	-86.66630725120783	111301
c72be3b7ddeb5725ba92127942b81f710a2b5a5e	spatial audio cues based surveillance audio attention model	bottom up;surveillance;environment adaptive audio attention spatial audio;speech;surveillance adaptation model humans feature extraction computational modeling ear speech;event detection;computational modeling;adaptation model;ear;feature extraction;audio attention;humans;interaural level difference;environment adaptive;spatial audio	In this paper, we propose a bottom-up audio attention model based on spatial audio cues for unsupervised event detecting in stereo audio surveillance. Firstly, the spatial audio parameter Interaural Level Difference (ILD) is extracted to calculate and represent the attention events, which are caused by rapid moving sound source. Then an environment adaptive normalization is used to assess the normalized attention level. Experimental results demonstrate that our proposed audio attention model is effective for audio surveillance event detection.	bottom-up parsing;covox speech thing;internet listing display;sensor;surround sound	Bo Hang;Ruimin Hu	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495934	computer vision;speech recognition;feature extraction;computer science;speech;speech coding;top-down and bottom-up design;acoustic model;computational model	Robotics	-11.274845031502304	-89.79794118174767	111353
5c7d3fbf4ae99087356c255db61731f3e33f5f46	automatic extraction of drum tracks from polyphonic music signals	content management;databases;analysis by synthesis;electrical capacitance tomography;rhythm;audio signal processing;performance evaluation;content based retrieval music feature extraction audio signal processing signal synthesis;popular music automatic extraction drum tracks polyphonic music signals time indexes percussive sounds analysis by synthesis technique performance evaluation rhythmic similarity search european project cuidado audio signal;signal analysis;audio signal;rhythmic similarity search;automatic extraction;analysis by synthesis technique;system performance;failure analysis;european project cuidado;multiple signal classification;postal services;drum tracks;feature extraction;popular music;multiple signal classification rhythm signal synthesis content management postal services signal analysis databases failure analysis algorithm design and analysis electrical capacitance tomography;percussive sounds;signal synthesis;polyphonic music signals;content based retrieval;music;time indexes;algorithm design and analysis	We propose an approach for extracting automatically time indexes of occurrences of percussive sounds in an audio signal taken from the Popular music repertoire. The scheme is able to detect percussive sounds unknown a priori in a selective fashion. It is based on an analysis by synthesis technique, whereby the sound searched for is gradually synthesized from the signal itself. The possibility to extract different types of percussive sounds and their occurrences in the audio signal makes it possible to build a drum track representing the essential rhythmic component of a music piece. We present a systematic evaluation of the performance of our algorithm on a database of popular music titles. The system performs well on most of the cases (over 75%). We analyze the reasons for failure on the remaining cases, and propose solutions for yet improving the algorithm. The extracted percussive sounds and drum track serves as a basis for search by rhythmic similarity in the context of the European project Cuidado.	algorithm;speech coding	Aymeric Zils;François Pachet;Olivier Delerue;Fabien Gouyon	2002		10.1109/WDM.2002.1176209	speech recognition;acoustics;engineering;communication	ML	-7.854839560238206	-92.38264573924906	111602
80539db5000d17d222aa1fc60198ada29d17da4b	hybrid nearest-neighbor/cluster adaptive training for rapid speaker adaptation in statistical speech synthesis systems		Statistical speech synthesis (SSS) approach has become one of the most popular methods in the speech synthesis field. An advantage of the SSS approach is the ability to adapt to a target speaker with a couple of minutes of adaptation data. However, many applications, especially in consumer electronics, require adaptation with only a few seconds of data which can be done using eigenvoice adaptation techniques. Although such techniques work well in speech recognition, they are are known to generate perceptual artifacts in statistical speech synthesis. Here, we propose two methods to both alleviate those quality problems and improve the speaker similarity obtained with the baseline eigenvoice adaptation algorithm. Our first method is based on using a Bayesian approach for constraining the eigenvoice adaptation algorithm to move in realistic directions in the speaker space to reduce artifacts. Our second method is based on finding a reference speaker that is close to the target speaker, and using that reference speaker as the seed model in a second eigenvoice adaptation step. Both techniques performed significantly better than the baseline eigenvoice method in the subjective quality and similarity tests.	algorithm;baseline (configuration management);single-linkage clustering;speech recognition;speech synthesis	Amir Mohammadi;Cenk Demiroglu	2013			pattern recognition;speech recognition;artificial intelligence;k-nearest neighbors algorithm;computer science;speech synthesis	ML	-17.249277174480586	-92.81229037405274	111664
3ef0224202fdfbde74438d4b20a87b771e1ffd40	formant correlates in greek fricative-vowel sequences		The present study examined F1 and F2 onset, as well as locus equations in Greek fricative – vowel sequences as distinctive cues for the fricative place of articulation. The effects of voicing, speaker’s gender and post-fricative vowel on both formants’ onset values and locus equations were also investigated. The results showed that F2 onset values distinguished fricatives in terms of place of articulation, except for the labiodental and dental ones. F1 onset values differentiated voiceless from voiced fricatives with voiceless having significantly higher values than voiced ones. The locus equation coefficients i.e. slope and yintercept, also distinguished voiced from voiceless fricatives. Moreover, palatal fricatives were clearly differentiated from the other places of articulation in terms of the y-intercept. A discriminant analysis using slope and y-intercept as predictors successfully categorized Greek palatal fricatives with 83.8% accuracy, while the overall percent correct classification was 54.3%.	biconnected component;categorization;coefficient;locus;linear discriminant analysis;onset (audio);visual intercept	Elina Nirgianaki	2011			voice;formant;vowel;psychology;place of articulation;linear discriminant analysis;artificial intelligence;pattern recognition	HCI	-10.128882046006282	-82.30232635579256	111665
91a4078003689c0136a4326e919e832be8548d06	audio coding for representation in midi via pitch detection using harmonic dictionaries	digital music;audio coding;subset selection;musical instrument digital interface	The search for a flexible and concise alternate representation for digital musical sound leads to the proposal for the use of the MIDI (Musical Instrument Digital Interface) protocol. The problem becomes one of automating the conversion process from sound to MIDI. This requires processing musical sound and extracting the information necessary to represent the sound as MIDI data. We have conducted studies which have led to algorithms for segmentation of the sound and pitch detection of the individual notes. We describe a novel method for pitch detection using subset selection with dictionaries containing harmonic spectra from samples of musical sounds. Examples demonstrating applicability to monophonic sounds as well as signals with multiple sound sources are given, including detection of objects in a complex background scene.	dictionary;midi;pitch detection algorithm	Nicholas J. Sieger;Ahmed H. Tewfik	1998	VLSI Signal Processing	10.1023/A:1008074130468	computer vision;speech recognition;digital audio;telecommunications;computer science	ML	-7.252597087261666	-92.76723659734836	111898
60d50370a56c7a4b53b621a16b537aaf6c6eeea3	discourse structure and speech recognition problems	indexing terms;spoken dialogue system;discourse structure;automatic detection;state space;speech recognition	We study dependencies between discourse structure and speech recognition problems (SRP) in a corpus of speech-based computer tutoring dialogues. This analysis can inform us whether there are places in the discourse structure prone to more SRP. We automatically extract the discourse structure by taking advantage of how the tutoring information is encoded in our system. To quantify the discourse structure, we extract two features for each system turn: depth of the turn in the discourse structure and the type of transition from the previous turn to the current turn. The 2 test is used to find significant dependencies. We find several interesting interactions which suggest that the discourse structure can play an important role in several dialogue related tasks: automatic detection of SRP and analyzing spoken dialogues systems with a large state space from limited amounts of available data.	interaction;speech recognition;state space;text corpus	Mihai Rotaru;Diane J. Litman	2006			natural language processing;speech recognition;index term;computer science;state space;linguistics	NLP	-14.650512329769366	-82.74569716457138	112051
e21f7ee7d237c2b628c2d6cdeec4980fae5d7607	a novel method based on cross correlation maximization, for pattern matching by means of a single parameter. application to the human voice		This work develops a cross correlation maximization technique, based on statistical concepts, for pattern matching purposes in time series. The technique analytically quantifies the extent of similitude between a known signal within a group of data, by means of a single parameter. Specifically, the method was applied to voice recognition problem, by selecting samples from a given individual’s recordings of the 5 vowels, in Spanish. The frequency of acquisition of the data was 11.250 Hz. A certain distinctive interval was established from each vowel time series as a representative test function and it was compared both to itself and to the rest of the vowels by means of an algorithm, for a subsequent graphic illustration of the results. We conclude that for a minimum distinctive length, the method meets resemblance between every vowel with itself, and also an irrefutable difference with the rest of the vowels for an estimate length of 30 points (~2 10 s).	cross-correlation;distribution (mathematics);expectation–maximization algorithm;pattern matching;speech recognition;time series	Felipe Quiero;Fabian Quintana;Leonardo Bennun	2015	CoRR		speech recognition;pattern recognition;mathematics;statistics	ML	-7.859833179883668	-90.1058453356594	112121
1c820df441ccea3ee40b9b15a820d03d3fa733e7	audiovisual alignment in a face-to-face conversation translation framework	translating videophone;nonverbal communication;facial expressions;subjective evaluation methods;speech to speech translation;face to face	Recent improvements in audiovisual alignment for a translating videophone are presented. A method for audiovisual alignment in the target language is proposed and the process of audiovisual speech synthesis is described. The proposed method has been evaluated in the VideoTRAN translating videophone environment, where an H.323 software client translating videophone allows for the transmission and translation of a set of multimodal verbal and nonverbal clues in a multilingual face-to-face communication setting. An extension of subjective evaluation metrics of fluency and adequacy, which are commonly used in subjective machine translation evaluation tests, is proposed for usage in an audiovisual translation environment.		Jerneja Zganec-Gros;Ales Mihelic	2009		10.1007/978-3-642-04391-8_8	speech recognition;computer science;multimedia;communication	NLP	-14.953734638734824	-83.50385781780294	112348
432d6ee0477701fa45fb47706a70e4fb552eff4e	detecting structural events for assessing non-native speech	speech assessment;structural event;non-native speech data;human-annotated event;clause boundary;automated speech assessment research;automated structural event detection;spontaneous speech;important component;human speaking	Structural events, (i.e., the structure of clauses and disfluencies) in spontaneous speech, are important components of human speaking and have been used to measure language development. However, they have not been actively used in automated speech assessment research. Given the recent substantial progress on automated structural event detection on spontaneous speech, we investigated the detection of clause boundaries and interruption points of edit disfluencies on transcriptions of non-native speech data and extracted features from the detected events for speech assessment. Compared to features computed on human-annotated events, the features computed on machine-generated events show promising correlations to holistic scores that reflect speaking proficiency levels.	holism;interrupt;sensor;spontaneous order	Lei Chen;Su-Youn Yoon	2011			natural language processing;speech recognition;computer science;communication	NLP	-14.082649871027243	-82.21620289247242	112368
15e33c160c64307cd8353a1ac5ed8c6a547927af	crosslingual adaptation of semi-continuous hmms using maximum likelihood and maximum a posteriori convex regression	speech recognition acoustic signal processing convex programming hidden markov models maximum likelihood estimation natural language processing regression analysis;hidden markov models abstracts probabilistic logic databases monos devices bayes methods context;crosslingual model adaptation task automatic speech recognition technology slovenian hmm multilingual spanish english german hmm mlcr maximum likelihood convex regression mapcr plsa probabilistic latent semantic analysis source models acoustic regression scheme prototype weights mixture weight adjustment semicontinuous hidden markov models maximum a posteriori convex regression maximum likelihood estimation semicontinuous hmm	In this work we present a novel adaptation design for semicontinuous HMMs (SCHMM). The method, which is developed in the scope of a crosslingual model adaptation task, consists in adjusting the states' mixture weights associated to the prototype densities of the codebook. The mixture weights of the target language are modelled as convex combinations of prototype weights. They are defined by an acoustic regression scheme applied to the source models, followed by a refinement using probabilistic latent semantic analysis (PLSA). In order to find suitable combination weights for the convex combinations we present a maximum likelihood (ML) as well as a maximum a posteriori (MAP) estimate. Thus, we name them maximum likelihood convex regression (MLCR) and maximum a posteriori convex regression (MAPCR). Finally, a crosslingual model adaptation task transferring multilingual Spanish-English-German HMMs to Slovenian demonstrates the performance of the method.	acoustic cryptanalysis;codebook;compiler;convex function;probabilistic latent semantic analysis;prototype;refinement (computing);semi-continuity;semiconductor industry	Frank Diehl;Asunción Moreno;Enric Monte-Moreno	2006	2006 14th European Signal Processing Conference		speech recognition;expectation–maximization algorithm;computer science;machine learning;pattern recognition;maximum likelihood sequence estimation	ML	-18.997921233820318	-91.80910331686883	112456
93c86551720ebc2a43b584fc69a4a054c0f7bc80	acoustic feature analysis and discriminative modeling of filled pauses for spontaneous speech recognition	disfluency;linear discriminate analysis;automatic speech recognizer;gaussian mixture model;mixture model;filled pause;detection rate;guassian mixture model;speech recognition;spontaneous speech;feature analysis;spontaneous speech recognition;linear discriminant analysis;karhunen loeve transform;discriminative model;hypothesis test	Most automatic speech recognizers (ASRs) concentrate on read speech, which is different from spontaneous speech with disfluencies. ASRs cannot deal with speech with a high rate of disfluencies such as filled pauses, repetitions, lengthening, repairs, false starts and silence pauses. In this paper, we focus on the feature analysis and modeling of the filled pauses “ah,” “ung,” “um,” “em,” and “hem” in spontaneous speech. Karhunen-Loéve transform (KLT) and linear discriminant analysis (LDA) were adopted to select discriminant features for filled pause detection. In order to suitably determine the number of discriminant features, Bartlett hypothesis testing was adopted. Twenty-six features were selected using Bartlett hypothesis testing. Gaussian mixture models (GMMs), trained with a gradient decent algorithm, were used to improve the filled pause detection performance. The experimental results show that the filled pause detection rates using KLT and LDA were 84.4% and 86.8%, respectively. A significant improvement was obtained in the filled pause detection rate using the discriminative GMM with KLT and LDA. In addition, the LDA features outperformed the KLT features in the detection of filled pauses.	acoustic cryptanalysis;algorithm;bartlett's bisection theorem;finite-state machine;google map maker;gradient;iterative method;linear discriminant analysis;mixture model;speech recognition;speech synthesis;spontaneous order;unified model	Chung-Hsien Wu;Gwo-Lang Yan	2004	VLSI Signal Processing	10.1023/B:VLSI.0000015089.17975.f4	speech recognition;computer science;machine learning;pattern recognition;mixture model;linear discriminant analysis;statistics	ML	-15.132610703850862	-88.71465388164287	112499
35468dded19e15197784e35e4c11f16ad21c3053	a language-based generative model framework for behavioral analysis of couples' therapy	training;speech;psychology;speech intelligibility acoustic signal processing languages;hidden markov models;hard expectation maximization algorithms behavioral signal processing static behavioral models sbm dynamic behavioral model dbm;hidden markov models psychology training speech signal processing encoding mathematical model;session level negativity label language based generative model framework psychological evaluations multiple behavioral cues behavioral codes automated behavioral annotation human perception dynamic behavior modeling scheme constant session long behavioral state;signal processing;mathematical model;encoding	Observational studies for psychological evaluations rely on careful assessment of multiple behavioral cues. Recent studies have made good progress in automating the psychological evaluation, which often involved tedious manual annotation of a set of behavioral codes. However, the current methods impose strict and often unnatural assumptions for evaluation. In this work, we specifically investigate two goals: (1) Human behavior changes throughout an interaction and better models of this evolution can improve automated behavioral annotation and (2) Human perception of this evolution can be quite complex and non-linear and better techniques than averaging need to be investigated. For this purpose, we propose a Dynamic Behavior Modeling (DBM) scheme, which models a spouse as undergoing changes in behavioral state within a session, and contrast it against a Static Behavior Model (SBM) which allows only a constant session-long behavioral state. We use Negativity in a couples therapy task as our case study. We present results and analysis on both models for capturing the local behavior information and predicting the session level negativity label.	behavior model;code;dbm;generative model;negativity (quantum mechanics);nonlinear system;super bit mapping	Sandeep Nallan Chakravarthula;Rahul Gupta;Brian R W Baucom;Panayiotis G. Georgiou	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178339	speech recognition;computer science;speech;machine learning;signal processing;mathematical model;encoding;statistics	Robotics	-14.450573051786774	-82.87397516737839	112592
2225950d1d3e02bc0d88a0c78325d00e0122b576	end-to-end multi-speaker speech recognition		Current advances in deep learning have resulted in a convergence of methods across a wide range of tasks, opening the door for tighter integration of modules that were previously developed and optimized in isolation. Recent ground-breaking works have produced end-to-end deep network methods for both speech separation and end-to-end automatic speech recognition (ASR). Speech separation methods such as deep clustering address the challenging cocktail-party problem of distinguishing multiple simultaneous speech signals. This is an enabling technology for real-world human machine interaction (HMI). However, speech separation requires ASR to interpret the speech for any HMI task. Likewise, ASR requires speech separation to work in an unconstrained environment. Although these two components can be trained in isolation and connected after the fact, this paradigm is likely to be sub-optimal, since it relies on artificially mixed data. In this paper, we develop the first fully end-to-end, jointly trained deep learning system for separation and recognition of overlapping speech signals. The joint training framework synergistically adapts the separation and recognition to each other. As an additional benefit, it enables training on more realistic data that contains only mixed signals and their transcriptions, and thus is suited to large scale training on existing transcribed data.	automatic system recovery;cluster analysis;deep learning;end-to-end principle;human–computer interaction;programming paradigm;speech recognition;synergy	Shane Settle;Jonathan Le Roux;Takaaki Hori;Shinji Watanabe;John R. Hershey	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461893	task analysis;artificial neural network;transcription (linguistics);deep learning;decoding methods;cocktail party effect;computer science;speech recognition;cluster analysis;artificial intelligence;convergence (routing)	ML	-15.907728085320377	-90.01424583102293	112982
b8da1cb3abed89a63c97ccf89b27492563071f40	speech dimensionality analysis on hypercubical self-organizing maps	analyse parole;hypercube;self organizing maps;sequence projection;dimension reduction;reconocimiento palabra;multi dimensional scaling;analisis palabra;speech processing;speech analysis;tratamiento palabra;dimensional analysis;traitement parole;carte autoorganisatrice;dimension fractal;fractal dimension;reduction dimension;feature vector;cuantificacion vectorial;vector quantization;self organizing map;projective space;speech recognition;self organized map;vector quantizer;reconnaissance parole;dimension fractale;intrinsic dimension;quantification vectorielle;hipercubo	The problem of finding the intrinsic dimension of speech is addressed in this paper. Astructured vector quantization lattice, Self-Organizing Map (SOM), is used as a projection space for the data. The goal is to find a hypercubical SOM lattice where the sequences of projected speech feature vectors form continuous trajectories. The effect of varying the dimension of the lattice is investigated using feature vector sequences computed from the TIMIT database.	algorithm;cepstrum;codebook;coefficient;feature vector;fractal dimension;image scaling;intrinsic dimension;multidimensional scaling;organizing (structure);quantization (signal processing);scott continuity;self-organizing map;smoothing;timit;vector quantization	Panu Somervuo	2003	Neural Processing Letters	10.1023/A:1023646203167	computer vision;combinatorics;self-organizing map;computer science;machine learning;speech processing;mathematics	ML	-12.979562080000715	-87.08614853983647	113017
31e9b3359a3e0ed5a3a3bfa669e1d5aed7dea484	improving the acoustic modelling of speech using modular/ensemble combinations of heterogeneous neural networks			acoustic cryptanalysis;artificial neural network	Christos Andrea Antoniou	2000				NLP	-14.402721545128989	-87.04548535650757	113085
f09cc5f5b99f9c496aef697a164c7f5040164dc2	effect of language experience on the categorical perception of cantonese vowel duration		This study investigated the effect of language experience on the categorical perception of Cantonese vowel duration distinction. By comparing Cantonese and Mandarin listeners’ performances, we found that: (1) duration change elicited categorical perception in the performance of Cantonese listeners, but not in Mandarin listeners; (2) Cantonese listeners were affected by the vowel quality differences, whereas Mandarin subjects were generally unbiased towards the quality differences; (3) effect of duration was overridden by the vowel quality [a] condition in the performance of Cantonese listeners. Our findings suggested that vowel quality is incorporated as a phonological cue in Cantonese.	performance;super robot monkey team hyperforce go!	Caicai Zhang;Gang Peng;William S.-Y. Wang	2011			speech recognition;categorical perception;language experience approach;natural language processing;vowel;computer science;artificial intelligence	HCI	-10.845801232942371	-81.84975207236847	113150
2b666625b5522094514fb61b23442b4e1c703178	data-driven temporal filters based on multi-eigenvectors for robust features in speech recognition	eigenvalues and eigenfunctions;filtering;collision mitigation;testing environment;robust features;multi eigenvector approach;recognition performance;finite impulse response filter;testing;data engineering;eigenvalues;temporal filtering;training environment;cepstral analysis;principal component analysis;robustness speech recognition principal component analysis finite impulse response filter testing cepstral analysis collision mitigation filtering linear discriminant analysis data engineering;filtering theory principal component analysis speech recognition eigenvalues and eigenfunctions;multieigenvector filters;testing environment data driven temporal filters multi eigenvectors robust features speech recognition principal component analysis pca filter coefficients multi eigenvector approach eigenvalues multieigenvector filters recognition performance pca derived filters aurora2 database training environment;speech recognition;robustness;aurora2 database;pca derived filters;filter coefficients;linear discriminant analysis;data driven temporal filters;pca;filtering theory;multi eigenvectors;eigenvectors;principal component	It was previously proposed to use the principal component analysis (PCA) to derive the data-driven temporal filters for obtaining robust features in speech recognition, in which the first principal components are taken as the filter coefficients. In this paper, a multi-eigenvector approach is proposed instead, in which the first M eigenvectors obtained in PCA are weighted by their corresponding eigenvalues and summed to be used as the filter coefficients. Experimental results showed that the multi-eigenvector filters offer significant recognition performance as compared to the previously proposed PCA-derived filters under all different conditions tested with the AURORA2 database, especially when the training and testing environments are highly mismatched.	speech recognition	Ni-Chun Wang;Jeih-Weih Hung;Lin-Shan Lee	2003		10.1109/ICASSP.2003.1198802	speech recognition;computer science;machine learning;pattern recognition;linear discriminant analysis;principal component analysis	ML	-12.862991942513723	-92.74470139393736	113375
63f49858d0945844bd6246e132e1236cffc1f77e	a critical survey on the use of fuzzy sets in speech and natural language processing	speech processing;speech;speech processing fuzzy sets natural language processing;fuzzy set theory;speech processing fuzzy set theory natural language processing;fuzzy sets;hidden markov models;speech fuzzy sets speech recognition speech processing natural language processing computational linguistics hidden markov models;snlp fuzzy sets speech processing natural language processing fs;speech recognition;computational linguistics;natural language processing	This paper shows how the use and applications of Fuzzy Sets (FS) in Speech and Natural Language Processing (SNLP) have seen a steady decline to a point where FS are virtually unknown or unappealing for most of the researchers currently working in the SNLP field, tries to find the reasons behind this decline, and proposes some guidelines on what could be done to reverse it and make FS assume a relevant role in SNLP.	fuzzy set;machine learning;natural language processing;stochastic grammar	João Paulo Carvalho;Fernando Batista;Luísa Coheur	2012	2012 IEEE International Conference on Fuzzy Systems	10.1109/FUZZ-IEEE.2012.6250803	natural language processing;cache language model;speech recognition;computer science;computational linguistics;speech processing;fuzzy set	Robotics	-16.53860344296911	-84.21840766861558	113429
6d8905c99dad6bea8c3c44f7e00631772ee9b7fe	real-time recognition of broadcast radio speech	broadcast news;microphones;bbc radio 4;acoustic modelling;decoding;large vocabulary continuous speech recognition;real time;speech processing;vocabulary;acoustic signal processing real time systems speech recognition radio broadcasting decoding hidden markov models speech processing recurrent neural nets;acoustic signal processing;speech enhancement;system performance;cambridge university;error analysis;radio broadcasting;hidden markov models;acoustic modelling broadcast radio speech abbot real time speech recognition system hybrid connectionist hmm speech recognition large vocabulary continuous speech recognition cambridge university acoustic variability system performance spontaneous speech decoding system speed recognition results latency figures broadcast news segments bbc radio 4 recurrent neural network;latency figures;broadcast news segments;speech recognition radio broadcasting real time systems decoding microphones vocabulary error analysis speech enhancement buildings acoustical engineering;abbot;recognition results;acoustical engineering;real time speech recognition system;system speed;acoustic variability;speech recognition;system development;broadcast radio speech;recurrent neural nets;recurrent neural network;hybrid connectionist hmm speech recognition;spontaneous speech;spontaneous speech decoding;buildings;real time systems	G.D.Cooky J.D. Christiey P.R. Clarksony M.M. Hochberg B.T. Logany A.J. Robinsony C.W. Seymoury yCambridge University Engineering Department, Trumpington Street, Cambridge CB2 1PZ, UK. Nuance Communications, 333 Ravenswood Avenue, Building 110, Menlo Park, CA. USA. ABSTRACT This paper presents a real-time speech recognition system used to transcribe broadcast radio speech. The system is based on Abbot, the hybrid connectionist-HMM large vocabulary continuous speech recognition system developed at the Cambridge University Engineering Department [1]. Developments designed to make the system more robust to acoustic variability and to improve performance when decoding spontaneous speech are described. Modi cations necessary to increase the speed of the system so that it operates in real-time are also described. Recognition results and latency gures are presented for speech collected from broadcast news segments on BBC Radio 4.	acoustic cryptanalysis;connectionism;heart rate variability;hidden markov model;radio broadcasting;real-time computing;real-time transcription;speech recognition;spontaneous order;vocabulary	Gary D. Cook;James Christie;Philip Clarkson;Mike Hochberg;Beth Logan;Anthony J. Robinson;Carl W. Seymour	1996		10.1109/ICASSP.1996.540310	radio broadcasting;voice activity detection;speech technology;audio mining;speech recognition;acoustical engineering;computer science;recurrent neural network;speech processing;acoustic model;speech synthesis;speech analytics	Mobile	-16.02417652497933	-86.44108989774911	113477
cd13b00d459337f80585ca8dfa45e64b6524ddc5	the perception of a derived contrast in scottish english		This paper reports on ongoing experiments related to the perception of atypical phonological contrasts. It was hypothesized that the quasiphonemic status of a derived contrast in Scottish English would induce atypical responses, yielding behavioural correlates with intermediate values between phonemicity and allophony. The results show that some of the Scottish participants produce identification slopes that are quite similar to the French allophonic control group while other Scottish listeners show more phonemic responses.	experiment	Emmanuel Ferragne;Nathalie Bedoin;Véronique Boulenger;François Pellegrino	2011			scottish english;perception;social psychology;psychology	NLP	-10.809913395584099	-81.67570400773543	113506
6e7d23401aea619dc2ac777882c27c9fd8dbecb9	perceptual assessment of the degree of russian accent	article	This paper deals with the perceptual assessment of Russian-accented Estonian. Speech samples were recorded from 20 speakers with a Russian background; clips of about 20 seconds from each speaker were selected for this perceptual study. The accentedness was rated in two tests: first, 20 native Estonian speakers judged the samples and rated the degree of foreign accent on a six-point interval scale; secondly, two experienced phoneticians carried out a perceptual study of the same samples and compiled the list of pronunciations errors. The results of both listening tests were highly correlated – the higher the degree of accentedness given to a L2-speaker by naïve listeners, the more pronunciation errors were found by trained experts. The classification of most frequent pronunciation errors based on acoustic-phonetic features is given, as well.	acoustic cryptanalysis;compiler;level of measurement;naivety;time-compressed speech	Lya Meister	2007			natural language processing;speech recognition;communication	HCI	-12.20020652677229	-82.84340330981028	113630
e5734711f6ff06e835bbef705c42e91df9932955	speaker adaptation based on transfer vectors of multiple reference speakers				Kazumi Ohkura;Hiroki Ohnishi;Masayuki Iida	1994			speech recognition;artificial intelligence;speaker recognition;speaker diarisation;pattern recognition;computer science	Vision	-14.675769544335523	-87.79022171632373	113740
7adee7750d4f22272707450b4099e97a98b13e43	trial production of a module for speech synthesis by rule.	speech synthesis		speech synthesis	Mikio Yamaguchi	1990			natural language processing;speech recognition;computer science;speech synthesis	Logic	-15.56039434120238	-85.56746695673108	113993
2980b5c15bb2a765bfb331ea24c5d7401125d6ab	evoluzione dell'approccio mkl al relevance feedback per l'apprendimento a lungo termine			math kernel library;relevance feedback	Annalisa Franco;Alessandra Lumini	2005			speech recognition;psychology;relevance feedback	NLP	-14.591844603324343	-85.15821353647566	114075
10bb5968958ec5e2514d404c1fb8c7930ae1a3b5	f0 correlates of topic and subject in spontaneous japanese speech	data consistency;noun	This paper examines F0 correlates of morphologically marked grammatical functions, in particular topic and subject, in spontaneous Japanese speech. Our data consist of F0 measurements of 7,106 nouns in theCallHome Japanese corpus of telephone conversations [4]. We find that topics exhibit higher peakF0 than subjects, contradicting information-structure acco unts which predict that topics, which refer to ‘old’ information, shou ld be less prominent. However, we suggest that the style and genre of speech is an important factor in this regard.	spontaneous order	John Fry	2000				NLP	-11.145386065096048	-80.51356868347987	114161
b43fc052660b8ca1188e800315af446d458b2f58	speaking in time	analyse parole;analisis palabra;speech processing;speech analysis;tratamiento palabra;traitement parole;synchronisation;synchronization;production;pauses;sincronizacion;spontaneous speech;disfluencies;speaker;locutor;locuteur;timing	Most disfluencies, I argue, are not truly mistakes. Rather, speakers design them as signals for coordinating with their addressees on certain of their speech actions. At the lowest level, speakers try to synchronize their vocalizations with their addressees’ attention. At the next level up, they try to synchronize, or pace, the presentation of each expression with their addressees’ analysis of those expressions. Speakers have a variety of strategies for achieving synchronization, and many of these lead to the common forms of disfluencies.		Herbert H. Clark	2002	Speech Communication	10.1016/S0167-6393(01)00022-X	synchronization;speech recognition;computer science;speech processing;linguistics	HCI	-13.300205415497032	-81.87457255313264	114240
042c6c318190fb0a3e576c8184a4025f1003878c	a new adaptation method for speaker-model creation in high-level speaker verification	background modeling;system performance;speaker verification;adaptive method	Abstract. Research has shown that speaker verification based on highlevel speaker features requires long enrollment utterances to be reliable. However, in practical speaker verification, it is common to model speakers based a limited amount of enrollment data. To minimize the undesirable effect of insufficient enrollment data on system performance, this paper proposes a new adaptation method for creating speaker models based on high-level features. Different from conventional methods, the proposed adaptation method not only adapts the phoneme-dependent background model but also the phoneme-independent speaker model. The amount of adaptation in the latter is adjusted by a proportional factor derived from the phoneme-independent background models. The proposed method was compared with traditional MAP adaptation under the NIST2000 SRE framework. Experimental results show that the proposed method can solve the data-spareness problem effectively and achieves a better performance when compare with traditional MAP adaptation.	consistency model;high- and low-level;speaker recognition	Shi-Xiong Zhang;Man-Wai Mak	2007		10.1007/978-3-540-77255-2_35	speaker recognition;simulation;speech recognition;computer science;computer performance	AI	-17.70198993182563	-91.05505286997085	114287
a431585f2e60d0604a6b430ac21e806be02f6696	combining anns to improve phone recognition	neural nets;speech processing;backpropagation phone recognition training configurations mixed network configurations data space regions initial weights training parameters training data context dependent neural network phone estimator context independent neural network phone estimator ann biphones monophones neural networks committee;backpropagation;parameter estimation neural nets speech recognition backpropagation speech processing;speech recognition;neural networks speech recognition training data artificial neural networks natural languages interpolation computer networks;context dependent;parameter estimation;mix network;neural network	"""In applying neural networks to speech recognition, one often nds that slightly di erent training con gurations lead to signi cantly di erent networks. Thus di erent training sessions using di erent setups will likely end up in \mixed"""" network con gurations representing di erent solutions in di erent regions of the data space. This sensitivity to the initial weights assigned, the training parameters and the training data can be used to enhance performance, using a committee of neural networks. In this paper, we study various ways to combine context-dependent (CD) and context-independent (CI) neural network phone estimators to improve phone recognition. As a result, we obtain 6.3% and 2.2% increase in accuracy in phone recognition using monophones and biphones respectively."""	artificial neural network;context-sensitive language;dataspaces;naruto shippuden: clash of ninja revolution 3;speech recognition	Brian Kan-Wing Mak	1997		10.1109/ICASSP.1997.595487	neural gas;speech recognition;computer science;recurrent neural network;backpropagation;machine learning;context-dependent memory;pattern recognition;time delay neural network;deep learning;estimation theory;artificial neural network	ML	-18.362301700699838	-89.91571556773545	114335
029dbbba8dc84e189bf313a33aa4740414b9aae6	the “language filter” hypothesis: a feasibility study of language separation in infancy using unsupervised clustering of i-vectors	acoustics;training;speech;computational modeling;clustering algorithms;adaptation models;linear discriminant analysis	In order to avoid mixing up languages, infants immersed in a multilingual environment have to sort speech into language-homogeneous sets. To study the feasibility of this task, we use speech technology tools (Universal Background Models and i-vectors) in combination with unsupervised clustering to test language separation using speech from several speakers of two languages. We investigate the outcome of the clustering as a function of the variability of language experience (monolingual versus mixed background), and the availability of side information (speaker identity). Our findings show that in the absence of side information, language separation is relatively easier if the system has been pre-exposed to a single language (monolingual background), than it is for a system pre-exposed to both languages (mixed background). However, this initial disadvantage can be overcome by restricting the representation to the dimensions that most distinguish speakers using Linear Discriminant Analysis, suggesting that speaker identity side information may enhance language separation in a multilingual environment. The implications for language acquisition and computational modeling are discussed.	baseline (configuration management);cluster analysis;linear discriminant analysis;pure function;spatial variability;speech technology;text corpus	M. Julia Carbajal;Ahmad Dawud;Roland Thiollière;Emmanuel Dupoux	2016	2016 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)	10.1109/DEVLRN.2016.7846818	natural language processing;speech recognition;computer science;communication	NLP	-12.529516238818465	-84.77939093836049	114376
2cd0b7a58c39007be3e6a926da2a355fb3133698	tonal distinctions between emphatic stress and pretonic lengthening in quebec french	stress;natural languages;loudspeakers;speech;testing;speech processing;frequency;software performance;modulation	This study compares the tonal structures of stressed penultimate syllables in Quebec French and emphatic stress in the same spoken variety. Two main experiments have been conducted: the first was designed to highlight the tonal characteristics of emphatic stress in read and spontaneous sp eech. The sec ond was concerned with the phonetic and tonal description of stressed, penultimate syllables as a result of a possible stress shift. Our results do not confirm the common assumption that penultimate, stressed syllables in Quebec French are the result of emphatic prominence. Emphatic stress is characterized by a LH tone on the target syllable followed by a more or less abrupt fall covering a particular domain depending on the speech style. By contrast, penultimate stressed syllables are characterized by a falling F0 modulation covering the lengthened syllable. The hypothesis that the tonal anchoring happens on the penultimate syllable could explain the variety of tonal patterns observed on the final syllables in those precise cases.	experiment;lh (complexity);modulation;spontaneous order;stress ball;syllable;voice stress analysis	Linda Thibault;Marise Ouellet	1996			speech recognition;acoustics;communication	ML	-10.539771060494258	-82.02565963072168	114455
8f01d9d45e54ebe3995b897054ccb95f6b26479b	automatic tracking of 3d vocal tract features during speech production using mri			tract (literature)	Maria Susana Avila Garcia	2006				NLP	-7.788337713975913	-84.7922705780489	114462
73b1dca29f88a31b2a45dda339279d336eaf3df8	noise perturbation for supervised speech separation	speech separation;noise perturbation;supervised learning	Speech separation can be treated as a mask estimation problem, where interference-dominant portions are masked in a time-frequency representation of noisy speech. In supervised speech separation, a classifier is typically trained on a mixture set of speech and noise. It is important to efficiently utilize limited training data to make the classifier generalize well. When target speech is severely interfered by a nonstationary noise, a classifier tends to mistake noise patterns for speech patterns. Expansion of a noise through proper perturbation during training helps to expose the classifier to a broader variety of noisy conditions, and hence may lead to better separation performance. This study examines three noise perturbations on supervised speech separation: noise rate, vocal tract length, and frequency perturbation at low signal-to-noise ratios (SNRs). The speech separation performance is evaluated in terms of classification accuracy, hit minus false-alarm rate and short-time objective intelligibility (STOI). The experimental results show that frequency perturbation is the best among the three perturbations in terms of speech separation. In particular, the results show that frequency perturbation is effective in reducing the error of misclassifying a noise pattern as a speech pattern.	blinded;intelligibility (philosophy);interference (communication);noise-induced hearing loss;signal-to-noise ratio;statistical classification;thrombocytopenia;time–frequency representation;tract (literature);vocal cord structure	Jitong Chen;Yuxuan Wang;DeLiang Wang	2016	Speech communication	10.1016/j.specom.2015.12.006	speech recognition;computer science;machine learning;pattern recognition	ML	-11.983531812286397	-91.36160135256141	114465
caa8867b7d0ea5533504bf515e928eba06aebd49	a new approach for robust realtime voice activity detection using spectral pattern	microphones;vowel sound realtime voice activity detection spectral pattern voting algorithm speech audio signal;audio signal processing;spectral flatness;spectral flatness voice activity detection spectral peaks pattern;voting algorithm;speech processing;speech analysis;training;audio signal;spectral pattern;vowel sound;speech;robustness speech analysis context modeling microphones audio recording privacy speech processing computer vision pattern analysis face detection;audio recording;satisfiability;noise measurement;computer vision;acoustic signal detection;speech recognition;robustness;pattern analysis;voice activity detection;signal to noise ratio;face detection;realtime voice activity detection;context modeling;spectral peaks pattern;privacy;speech recognition acoustic signal detection audio signal processing	In this paper a Voice Activity Detection approach is proposed which applies a voting algorithm to decide on the existence of speech in audio signal. For this purpose, the proposed approach uses three different short time features along with the pattern of spectral peaks of every frame. Spectral peaks pattern is appropriate for determining vowel sounds in speech signal even in the presence of noise. Therefore this measure can be applicable in voice activity detection in which the vowels characterize the speech signal. Experiments show that incorporating this measure along with our recently proposed approach for VAD, will improve the results of the algorithm considerably while imposing little computational overhead. The proposed approach is evaluated on different datasets with various noises and SNR levels and satisfying results are achieved.	algorithm;computation;experiment;overhead (computing);signal-to-noise ratio;voice activity detection	Mohammad H. Moattar;Mohammad M. Homayounpour;Nima Khademi Kalantari	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495597	voice activity detection;face detection;speech recognition;audio signal processing;computer science;noise measurement;speech;audio signal;speech processing;context model;signal-to-noise ratio;privacy;robustness;satisfiability;spectral flatness	Robotics	-12.128593248942495	-92.47367787966732	114471
f3427ee9f4f1d4168ecc4cda063ef605a3c1d1b1	"""producing """"spoonerisms"""" on demand: lexical, phonological and orthographic factors in a new experimental paradigm"""	orthographe;ortografia;lateralidad;phonetique;systeme nerveux central;articulation;cerebral hemisphere;linguistic production;lateralite;orthography;articulacion;sistema nervioso central;language perception;percepcion verbal;laterality;hemisphere cerebral;fonetica;phonetics;lapsus;joint;central nervous system;hemisferio cerebral;production linguistique;perception verbale	Abstract   We report a new experimental approach to studying Spoonerisms, whereby subjects are required to deliberately exchange phonemes at specified positions in word pairs presented in list form, with speed and accuracy measurements. With CVC words, initial phoneme transpositions were performed best, and final phoneme transpositions worst. The ease with which medial phonemes were transposed depended upon whether or not the medial vowel was a dipthong, and also upon whether or not a Spoonerism exchange would result in a major orthographic change were the purely articulatory response to be actually transcribed. Thus orthographic variables appear to influence processing even in a task presumably performed at a purely acoustic/articulatory level. While performance was not affected by the word/nonword status of the response, the articulatory environment of surrounding phonemes did influence performance in exchanging a target medial phoneme. Finally sinistrals proved better able to produce such Spoonerisms on demand, possibly reflecting their superior ability at mirro reading and writing. The findings generated new hypotheses for the study of spontaneous Spoonerisms.	orthographic projection;programming paradigm	Roslyn E. Carter;John L. Bradshaw	1984	Speech Communication	10.1016/0167-6393(84)90028-1	phonetics;joint;speech recognition;orthography;central nervous system;linguistics;laterality	NLP	-9.750709920087328	-80.67449032285236	114582
d58bb26e06ef3ffdd495d8e1ddd651303b1a7e85	speech synthesis using hmm based diphone inventory encoding for low-resource devices	low resource device;speech synthesis;data compression;gaussian processes;hidden markov model;amr encoded inventory;acoustic synthesis;training;speech;speech coding;phoneme hmm;hmm based diphone inventory encoding;hidden markov models speech encoding training speech recognition indexes;indexes;hidden markov models;vectors;speaker independent;codebook;diphone inventory compression;indexation;gaussian mean vector;speaker dependent;speex encoded inventory;speech recognition;vectors data compression gaussian processes hidden markov models speech coding speech synthesis;concatenative synthesis system;amr encoded inventory speech synthesis hmm based diphone inventory encoding low resource device diphone inventory compression concatenative synthesis system acoustic synthesis codebook gaussian mean vector phoneme hmm speaker dependent speaker independent speex encoded inventory;encoding;hidden markov models speech synthesis speech coding	In this paper we describe the compression of diphone inventories used by the acoustic synthesis of a concatenative synthesis system. The inventory compression is based on a codebook drawn from the Gaussian mean vectors of phoneme HMMs. There are two encoding/synthesis schemes, a speaker dependent and a speaker independent one. The advantage of the latter is the potential common use of the HM-models by a recognizer and a synthesizer. We describe the steps to encode the inventories as well as the acoustic synthesis using them. Using the proposed method a diphone inventory with 1175 units can be compressed down to 19 kB. We will show that the synthesis quality with HMM-encoded inventories matches the quality of synthesis with AMR- or SPEEX-encoded inventories at noticeably smaller inventory sizes.	acoustic cryptanalysis;codebook;concatenative synthesis;consistency model;encode;finite-state machine;hidden markov model;inventory;speech synthesis	Guntram Strecha;Matthias Wolff	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947574	data compression;database index;speech recognition;computer science;speech;codebook;speech coding;pattern recognition;gaussian process;hidden markov model;encoding	Robotics	-18.319927719582118	-90.46674490231307	114600
83d9aa1b9d5880640509b6a26ac00086f0ab9c56	vibrat-o-matic: producing vocal vibrato using ems	vibrato;electrical muscle stimulation;musical performance	"""Vibrato is one of the most popular vocal techniques. Based on the fact that vibrato is caused by periodic pulsation of the cricothyroid and diaphragm muscles, we explore the possibility of reproducing vibrato by actuating these muscles with electrical muscle stimulation. We present """"Vibrat-o-matic"""", a system that enables users to interactively control their vocal vibrato while singing. We test the system on four participants, including two semi-professional singers, under two conditions of the electrode arrangement (stomach and neck). Although the muscles actuated by this system are not exactly the same as those used for natural vibrato, we succeeded in reproducing similar periodic modulation of volume, especially by stimulating the stomach. Moreover, although it is not suitable for training natural vibrato, it is suggested that this system could provide notion of when and where to use vibrato while singing."""	associate-o-matic;diaphragm (acoustics);interactivity;modulation;semiconductor industry	Ryohei Fushimi;Eisuke Fujinawa;Takuji Narumi;Tomohiro Tanikawa;Michitaka Hirose	2017		10.1145/3041164.3041193	speech recognition;acoustics;engineering;communication	HCI	-7.580698595588831	-84.73983453323135	114834
00ecab50c59fd77fbb9d740a0981b81778ce0b77	socialfx: studying a crowdsourced folksonomy of audio effects terms	reverberation;interfaces;vocabulary;equalization;signal processing;effects processing;audio engineering;compression;crowdsourcing	We present the analysis of crowdsourced studies into how a population of Amazon Mechanical Turk Workers describe three commonly used audio effects: equalization, reverberation, and dynamic range compression. We find three categories of words used to describe audio: ones that are generally used across effects, ones that tend towards a single effect, and ones that are exclusive to a single effect. We present select examples from these categories. We visualize and present an analysis of the shared descriptor space between audio effects. Data on the strength of association between words and effects is made available online for a set of 4297 words drawn from 1233 unique users for three effects (equalization, reverberation, compression). This dataset is an important step towards implementing of an end-to-end language-based audio production system, in which a user describes a creative goal, as they would to a professional audio engineer, and the system picks which audio effect to apply, as well as the setting of the audio effect.	amazon mechanical turk;audio engineer;audio signal processing;crowdsourcing;dynamic range;end-to-end principle;folksonomy;production system (computer science);professional audio;the turk;unique user;vocabulary	Taylor Zheng;Prem Seetharaman;Bryan Pardo	2016		10.1145/2964284.2967207	computer vision;audio mining;speech recognition;equalization;aes11;reverberation;computer science;machine learning;signal processing;interface;multimedia;world wide web;compression;crowdsourcing	HCI	-12.820653135413608	-85.64865324895226	114895
461a1cdc31439dd3e57cb5f9834a143a79076476	stream-weight optimization by lda and adaboost for multi-stream speaker verification	optimal method;linear discriminate analysis;noise robustness;speaker verification;adaptive method;white noise	This paper proposes an automatic stream-weight optimization method for noise-robust speaker verification using multi-stream HMMs integrating spectral and prosodic information. The paper first shows the effectiveness of the multi-stream technique in our speaker verification framework. Next, a stream-weight adaptation method combining the linear discriminant analysis (LDA) and Adaboost techniques is proposed. Experiments were conducted using four-connected-digit utterances of Japanese contaminated by white noise with various SNRs. Experimental results show that 1) the verification performance was improved in all SNR conditions by using stream weights estimated by the LDA and 2) the performance is further improved by using the Adaboost in 10 30dB SNR conditions.	adaboost;experiment;linear discriminant analysis;mathematical optimization;signal-to-noise ratio;speaker recognition;stream cipher;white noise	Taichi Asami;Koji Iwano;Sadaoki Furui	2005			speech recognition;computer science;machine learning;pattern recognition;white noise;statistics	AI	-14.241668130680468	-91.019998212817	115018
4a6100daecc0711936b4d97c641293e54a08d877	time-domain segmentation and labelling of speech with fuzzy-logic post-correction rules	methode domaine temps;phoneme;segmentation parole;raisonnement base sur cas;razonamiento fundado sobre caso;procesamiento informacion;image processing;reconocimiento palabra;fuzzy rules;logique floue;procesamiento imagen;logica difusa;fonema;phonem;algoritmo genetico;metodo dominio tiempo;traitement image;fuzzy logic;information processing;algorithme genetique;pattern recognition;speech recognition;genetic algorithm;time domain;time domain method;reconnaissance forme;reconnaissance parole;case based reasoning;reconocimiento patron;frequency domain;traitement information	In speech recognition, the procurement of accurate patterns that describe an input signal is a crucial task. Frequency-domain processing provides with rich information for such signal descriptions. However a first interpretation of the time-domain characteristics of the speech utterances may be enough for obtaining important information contained in the signal in a faster way. This paper shows that segmentation and labelling of speech may be performed using only time-domain information in an exact and accurate way. The method obtains syllable and phoneme level segmentation in two stages. The first identifies sonority decrease intervals for estimating transitions between syllables. The second, refines the placement of boundaries using a set of fuzzy-rules that compared current time-marks with previously computed syllable-transition values. The system was tested using an Italian language digit database. The reported results show that the accuracy of the inter-syllabic boundary placements get improved when using the fuzzy-correction method.	algorithm;database;experiment;fuzzy logic;pitch (music);procurement;speech recognition;syllable	Oscar Mayora-Ibarra;Francesco Curatelli	2002		10.1007/3-540-46016-0_15	fuzzy logic;case-based reasoning;speech recognition;genetic algorithm;information processing;image processing;time domain;computer science;artificial intelligence;frequency domain	NLP	-5.162342377299276	-92.86812042538281	115221
3f57017f61f3d1e7ed9ff1ef2bbc7a9ce6e9a050	a maximal figure-of-merit learning approach to maximizing mean average precision with deep neural network based classifiers	training neural networks support vector machines speech measurement speech processing acoustics;maximal figure of merit nonlinearity average precision deep neural networks;neural nets audio signal processing image classification learning artificial intelligence;audio event classification maximal figure of merit learning approach mean average precision deep neural network based classifiers mfom learning framework map metric multiclass classification task support vector machines automatic speech recognition image classification minimum cross entropy criterion objective function linear discriminant function ldf automatic image annotation	We propose a maximal figure-of-merit (MFoM) learning framework to directly maximize mean average precision (MAP) which is a key performance metric in many multi-class classification tasks. Conventional classifiers based on support vector machines cannot be easily adopted to optimize the MAP metric. On the other hand, classifiers based on deep neural networks (DNNs) have recently been shown to deliver a great discrimination capability in automatic speech recognition and image classification as well. However, DNNs are usually optimized with the minimum cross entropy criterion. In contrast to most conventional classification methods, our proposed approach can be formulated to embed DNNs and MAP into the objective function to be optimized during training. The combination of the proposed maximum MAP (MMAP) technique and DNNs introduces nonlinearity to the linear discriminant function (LDF) in order to increase the flexibility and discriminant power of the original MFoM-trained LDF based classifiers. Tested on both automatic image annotation and audio event classification, the experimental results show consistent improvements of MAP on both datasets when compared with other state-of-the-art classifiers without using MMAP.	artificial neural network;automatic image annotation;computer vision;cross entropy;deep learning;information retrieval;linear discriminant analysis;loss function;maximal set;mmap;multiclass classification;nonlinear system;optimization problem;speech recognition;support vector machine	Kehuang Li;Zhen Huang;You-Chi Cheng;Chin-Hui Lee	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854454	speech recognition;computer science;machine learning;pattern recognition	Vision	-15.805958650899688	-91.47050480333418	115222
1d9246da7fc999475ae5d5407fc6ecce6e6d1ef7	adapting speaking after evidence of misrecognition: local and global hyperarticulation	hyperarticulation;speaking rate;evaluation performance;performance evaluation;evaluacion prestacion;speech processing;rate adaptation;tratamiento palabra;traitement parole;conversacion;reconocimiento voz;conversation;speech recognition;reconnaissance parole;adaptation in speaking;spoken dialog;clear speech	In this paper we examine the two-way relationship between hyperarticulation and evidence of misrecognition of computer-directed speech. We report the results of an experiment in which speakers spoke to a simulated speech recognizer and received text feedback about what had been ‘‘recognized’’. At pre-determined points in the dialog, recognition errors were staged, and speakers made repairs. Each repair utterance was paired with the utterance preceding the staged recognition error and coded for adaptations associated with hyperarticulate speech: speaking rate and phonetically clear speech. Our results demonstrate that hyperarticulation is a targeted and flexible adaptation rather than a generalized and stable mode of speaking. Hyperarticulation increases after evidence of misrecognition and then decays gradually over several turns in the absence of further misrecognitions. When repairing misrecognized speech, speakers are more likely to clearly articulate constituents that were apparently misrecognized than those either before or after the troublesome constituents, and more likely to clearly articulate content words than function words. Finally, we found no negative impact of hyperarticulation on speech recognition performance. Published by Elsevier B.V.	computer;dialog system;emoticon;error message;finite-state machine;hirschberg's algorithm;interaction;kirchhoff's theorem;natural language processing;self-replication;serializability;speech recognition;spoken dialog systems;spontaneous order;text corpus;word lists by frequency;dialog	Amanda Stent;Marie K. Huffman;Susan Brennan	2008	Speech Communication	10.1016/j.specom.2007.07.005	speech recognition;computer science;speech processing;linguistics	NLP	-13.002929707976573	-81.81060301406305	115337
10be747c2ecad47ce8f478e3e2cb147f04c34c34	performance evaluation of the speaker-independent hmm-based speech synthesis system “hts 2007” for the blizzard challenge 2007	full covariance modeling;speaker independent hmm based speech synthesis system;modeling technique;covariance analysis;performance evaluation;speech synthesis;blizzard challenge hmm speech synthesis speaker adaptation hts;hmm;indexing terms;feature space;conference paper;hmm based speech synthesis;hidden markov models;blizzard challenge;speaker independent;covariance matrices;speech synthesis speech analysis hidden markov models high temperature superconductors covariance matrix context modeling training data loudspeakers robustness acoustic measurements;hts;speaker dependent;mixed gender modeling;speaker dependent approaches performance evaluation speaker independent hmm based speech synthesis system hts 2007 blizzard challenge 2007 speaker adaptation feature space adaptive training mixed gender modeling full covariance modeling;system development;speech synthesis covariance analysis hidden markov models;subjective evaluation;speaker adaptation;feature space adaptive training;hts 2007;blizzard challenge 2007;adaptive estimation;speaker dependent approaches	"""This paper describes a speaker-independent/adaptive HMM-based speech synthesis system developed for the Blizzard Challenge 2007. The new system, named """"HTS-2007"""", employs speaker adaptation (CSMAPLR+MAP), feature-space adaptive training, mixed-gender modeling, and full-covariance modeling using CSMAPLR transforms, in addition to several other techniques that have proved effective in our previous systems. Subjective evaluation results show that the new system generates significantly better quality synthetic speech than that of speaker-dependent approaches with realistic amounts of speech data, and that it bears comparison with speaker-dependent approaches even when large amounts of speech data are available."""	hidden markov model;high-throughput satellite;performance evaluation;speech synthesis;synthetic intelligence	Junichi Yamagishi;Takashi Nose;Heiga Zen;Tomoki Toda;Keiichi Tokuda	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518520	natural language processing;speech recognition;index term;feature vector;analysis of covariance;computer science;machine learning;hidden markov model	Robotics	-17.046535012622314	-91.81757908752327	115404
8935d8ed3e5d1872cbd8ef5b8106ef0635fc835b	spoken language identification based on i-vectors and conditional random fields		The task of an automatic language identification (LID) system is to automatically identify the language in a spoken utterance. Language identification can be applied as front-end to speech-to-speech translation systems, in speaker diarization, and at call centers to automatically route incoming calls to appropriate native speaker operators. In the current study, a method for automatic language identification based on i-vector paradigm and conditional random fields (CRF) is presented. CRF belong to discriminative classifiers and use an exponential distribution to model a sequence given the observation sequence. This allows non-independent observations, and allows also non-local dependencies between state and observation. When the proposed method is evaluated on the NIST 2015 i-vector Machine Learning Challenge task for the recognition of 50 in-set languages, a 3.7% equal error rate (EER) (i.e., miss probability equal to false alarms) is achieved. Using support vector machines (SVM) a 5.2% EER, using probabilistic linear discriminant analysis (PLDA) a 6.7% EER, and when using convolutional neural networks (CNN) a 4.2% EER are achieved.	artificial neural network;conditional random field;convolutional neural network;detection error tradeoff;discriminative model;enhanced entity–relationship model;film-type patterned retarder;language identification;linear discriminant analysis;machine learning;programming paradigm;speaker diarisation;support vector machine;time complexity	Panikos Heracleous;Yasser Mohammad;Kohichi Takai;Keiji Yasuda;Akio Yoneyama	2018	2018 14th International Wireless Communications & Mobile Computing Conference (IWCMC)	10.1109/IWCMC.2018.8450327	language identification;discriminative model;speech recognition;convolutional neural network;support vector machine;distributed computing;speaker diarisation;word error rate;spoken language;conditional random field;computer science	NLP	-16.60337705609105	-89.0327944800676	115423
48f1010efbcef20455a3599c2d4fc60c10aecfe6	discriminative training of wfst factors with application to pronunciation modeling		One of the most popular speech recognition architectures consists of multiple components (like the acoustic, pronunciation and language models) that are modeled as weighted finite state transducer (WFST) factors in a cascade. These factor WFSTs are typically trained in isolation and combined efficiently for decoding. Recent work has explored jointly estimating parameters for these models using considerable amounts of training data. We propose an alternative approach to selectively train factor WFSTs in such an architecture, while still leveraging information from the entire cascade. This technique allows us to effectively estimate parameters of a factor WFST using relatively small amounts of data, if the factor is small. Our approach involves an online training paradigm for linear models adapted for discriminatively training one or more WFSTs in a cascade. We apply this method to train a pronunciation model for recognition on conversational speech, resulting in significant improvements in recognition performance over the baseline model.	acoustic cryptanalysis;baseline (configuration management);discriminative model;estimation theory;finite-state transducer;language model;linear model;programming paradigm;speech recognition	Preethi Jyothi;Eric Fosler-Lussier;Karen Livescu	2013			pattern recognition;artificial intelligence;discriminative model;architecture;linear model;training set;pronunciation;decoding methods;language model;computer science;finite state transducer	NLP	-18.23091555106096	-89.30891562502849	115535
fa747460078286477078f8e48fd701e5686e4137	using voice-quality measurements with prosodic and spectral features for speaker diarization		Jitter and shimmer voice-quality measurements have been successfully used to detect voice pathologies and classify different speaking styles. In this paper, we investigate the usefulness of jitter and shimmer voice measurements in the framework of the speaker diarization task. The combination of jitter and shimmer voice-quality features with the long-term prosodic and shortterm spectral features is explored in a subset of the Augmented Multi-party Interaction (AMI) corpus, a multi-party and spontaneous speech set of recordings. The best results have been obtained by fusing the voice-quality features with the prosodic ones at the feature level, and then fusing them with the spectral features at the score level. Experimental results show more than 20% relative DER improvement compared to the spectral baseline system.	baseline (configuration management);speaker diarisation;spontaneous order;voice stress analysis	Abraham Woubie;Jordi Luque;Javier Hernando	2015			speaker diarisation;speech recognition;jitter;phonation;fusion;computer science;linguistics;prosody	NLP	-13.131188102329268	-89.12391673575617	115658
079290e57befac813540eeca0a22fbfe5aea3105	large vocabulary speaker-adaptive continuous speech recognition research overview at dragon systems			dragon naturallyspeaking;speech recognition;vocabulary	Janet M. Baker	1991			speech analytics;speech recognition;natural language processing;speech technology;speaker diarisation;speaker recognition;artificial intelligence;computer science;vocabulary	NLP	-15.717150582040894	-85.97499041578708	116015
8076917b4dc6510c7277c05bda3d63ada2261149	noise perturbation improves supervised speech separation	speech separation;supervised learning;noise perturbation	Speech separation can be treated as a mask estimation problem where interference-dominant portions are masked in a timefrequency representation of noisy speech. In supervised speech separation, a classifier is typically trained on a mixture set of speech and noise. Improving the generalization of a classifier is challenging, especially when interfering noise is strong and nonstationary. Expansion of a noise through proper perturbation during training exposes the classifier to more noise variations, and hence may improve separation performance. In this study, we examine the effects of three noise perturbations at low signal-to-noise ratios (SNRs). We evaluate speech separation performance in terms of hit minus false-alarm rate and short-time objective intelligibility (STOI). The experimental results show that frequency perturbation performs the best among the three perturbations. In particular, we find that frequency perturbation reduces the error of misclassifying a noise pattern as a speech pattern.	intelligibility (philosophy);interference (communication);signal-to-noise ratio;statistical classification	Jitong Chen;Yuxuan Wang;DeLiang Wang	2015		10.1007/978-3-319-22482-4_10	speech recognition;noise measurement;machine learning;pattern recognition;mathematics	NLP	-12.006620850285511	-91.34408210704487	116210
7de72cf0db88053a03c4a3182f2ecb24c85c6b18	the prosodic dimensions of emotion in speech: the relative weights of parameters		The emotional prosody is multi-dimensional. A debated question is to understand if some parameters are more specialized to convey some emotion dimensions [13]. Selected stimuli, mixed issued of authentic and acted French corpus, expressing anxiety, disappointment, disgust, disquiet, joy, resignation, satisfaction and sadness were used to synthesize artefactual stimuli integrating the emotional contour of each prosodic parameter separately. These stimuli were evaluated on a perception experiment. Results indicate that (1) no parameter alone is able to carry the whole emotion information, (2) F0 contours (not only the global F0 value) reveal to bring more information on positive expressions, (3) voice quality and duration convey more information on negative expressions, and (4) the intensity contours do not bring any significant information when used alone.	emotion markup language;sadness;semantic prosody;text corpus	Nicolas Audibert;Véronique Aubergé;Albert Rilliard	2005			speech recognition;natural language processing;artificial intelligence;computer science	HCI	-9.952167976666155	-82.14625765858005	116439
305bb1a70d1efd810b1293eed392e8da2118ba1b	music identification using chroma features		In this paper some specific issues related to Music Information Retrieval (MIR) are presented. First part is dedicated to introductive notions from this field in the field, and second part is dedicated to giving details about the system we built for MusiCLEF 2011. An important aspect related to our work is related to searching metrics able to allow us to identify an audio recording in a database with existing songs. Our system uses chroma features associated to a song and apply on it many types of metrics. Some of these metrics wants to find more accurate song of which part of that fragment belong to and other metrics are used to enable us to do this as quickly as possible.	acoustic fingerprint;information retrieval	Adrian Iftene;Andrei Rusu;Alexandra Leahu	2011			speech recognition;computer science;multimedia;communication	Web+IR	-7.330442707088143	-92.07439934956045	116606
cb7cf162fb44ef06abd6aa30026c99ded8cbcdf8	speech-driven 3d facial animation with implicit emotional awareness: a deep learning approach		We introduce a long short-term memory recurrent neural network (LSTM-RNN) approach for real-time facial animation, which automatically estimates head rotation and facial action unit activations of a speaker from just her speech. Specifically, the time-varying contextual non-linear mapping between audio stream and visual facial movements is realized by training a LSTM neural network on a large audio-visual data corpus. In this work, we extract a set of acoustic features from input audio, including Mel-scaled spectrogram, Mel frequency cepstral coefficients and chromagram that can effectively represent both contextual progression and emotional intensity of the speech. Output facial movements are characterized by 3D rotation and blending expression weights of a blendshape model, which can be used directly for animation. Thus, even though our model does not explicitly predict the affective states of the target speaker, her emotional manifestation is recreated via expression weights of the face model. Experiments on an evaluation dataset of different speakers across a wide range of affective states demonstrate promising results of our approach in real-time speech-driven facial animation.	acoustic cryptanalysis;alpha compositing;artificial neural network;baseline (configuration management);chroma feature;coefficient;color gradient;computer animation;deep learning;experiment;generative model;long short-term memory;mel-frequency cepstrum;nonlinear system;random neural network;real-time clock;real-time locating system;real-time transcription;recurrent neural network;spectrogram;streaming media;waveform	Hai Xuan Pham;Samuel Cheung;Vladimir Pavlovic	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.287	facial motion capture;computer vision;artificial intelligence;pattern recognition;computer facial animation;affect (psychology);animation;artificial neural network;deep learning;feature extraction;computer science;recurrent neural network	Vision	-14.522824304242938	-82.79084107424036	116700
92c84101e978cfde6c58d7093f5316c34de407fe	a data mining approach to objective speech quality measurement	mars;degradation;machine learning statistical data mining objective speech quality measurement subjective listening tests multivariate adaptive regression splines speech distortion speech quality estimation;speech processing;speech quality estimation;perceptual evaluation of speech quality;testing;distortion measurement;subjective listening tests;data mining;objective speech quality measurement;splines mathematics;speech distortion;performance improvement;machine learning;speech codecs;signal processing;learning artificial intelligence data mining regression analysis splines mathematics speech processing parameter estimation;regression analysis;data mining electric variables measurement testing mars speech codecs speech processing distortion measurement signal processing degradation optimization methods;statistical techniques;quality measures;statistical data mining;parameter estimation;multivariate adaptive regression splines;learning artificial intelligence;electric variables measurement;optimization methods	"""Existing objective speech quality measurement algorithms still fall short of the measurement accuracy that can be obtained from subjective listening tests. We propose an approach that uses statistical data mining techniques to improve the accuracy of auditory-model based quality measurement algorithms. We present the design of a novel measurement algorithm using the multivariate adaptive regression splines (MARS) method. A large set of speech distortion features is first created. MARS is used to find a small set of features that provide the best estimate (""""model"""") of speech quality. One appeal of the approach is that the model size can scale with the amount of speech data available for learning. In our simulations, the new algorithm furnishes significant performance improvement over PESQ (perceptual evaluation of speech quality)."""	algorithm;data mining;distortion;multivariate adaptive regression splines;pesq;scalability;simulation;smoothing spline	Wei Zha;Wai-Yip Geoffrey Chan	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326022	mars exploration program;speech recognition;degradation;multivariate adaptive regression splines;computer science;machine learning;signal processing;pattern recognition;speech processing;software testing;estimation theory;regression analysis	Robotics	-17.204207211201634	-93.47371052519568	116704
f6f1b157c2ab8b3d062da0791d9d3cacff3c3e4d	online hierarchical transformation of hidden markov models for speech recognition	bayes estimation;adaptive speech recognition;metodo adaptativo;adaptation data;hmm units;modelo markov;reconocimiento palabra;hidden markov model;current data block;mandarin syllables recognition;hmm parameters;bayes methods;efficiency;vocabulary;online hierarchical transformation;bayesian methods;methode adaptative;testing;trees mathematics;indexing terms;adaptation data hidden markov models speech recognition online hierarchical transformation hmm parameters adaptive speech recognition acoustical environment observed adaptation data hierarchical tree hmm units transformation parameters hmm mixture components approximate bayesian estimate refreshed prior statistics current data block speaker adaptation experiments mandarin syllables recognition efficiency;hierarchical tree;approximation theory;adaptive signal processing hidden markov models speech recognition approximation theory bayes methods statistical analysis trees mathematics;estimacion bayes;markov model;hidden markov models;adaptive signal processing;statistical analysis;adaptive method;speaker adaptation experiments;observed adaptation data;estimacion parametro;statistics;pattern recognition;algorithme em;transformation non lineaire;bayesian estimator;speech recognition;transformacion no lineal;robustness;algoritmo em;approximate bayesian estimate;reconnaissance parole;parameter estimation;hidden markov models speech recognition testing bayesian methods statistics vocabulary pattern recognition algorithm design and analysis robustness calibration;estimation parametre;modele markov;refreshed prior statistics;em algorithm;non linear transformation;speaker adaptation;calibration;transformation parameters;algorithm design and analysis;acoustical environment;estimation bayes;hmm mixture components	This paper proposes a novel framework of online hierarchical transformation of hidden Markov model (HMM) parameters for adaptive speech recognition. Our goal is to incrementally transform (or adapt) all the HMM parameters to a new acoustical environment even though most of HMM units are unseen in observed adaptation data. We establish a hierarchical tree of HMM units and apply the tree to dynamically search the transformation parameters for individual HMM mixture components. In this paper, the transformation framework is formulated according to the approximate Bayesian estimate, which the prior statistics and the transformation parameters can be jointly and incrementally refreshed after each consecutive adaptation data is presented. Using this formulation, only the refreshed prior statistics and the current block of data are needed for online transformation. In a series of speaker adaptation experiments on the recognition of 408 Mandarin syllables, we examine the effects on constructing various types of hierarchical trees. The efficiency and effectiveness of proposed method on incremental adaptation of overall HMM units are also confirmed. Besides, we demonstrate the superiority of proposed online transformation to Huo’s on-line adaptation [16] for a wide range of adaptation data.	approximation algorithm;batch processing;computation;emoticon;expectation–maximization algorithm;experiment;gradient;hidden markov model;iso 10303;markov chain;memory refresh;online and offline;quickbasic;recursion;relation (database);requirement;speech recognition;super robot monkey team hyperforce go!;tree structure;vocabulary	Jen-Tzung Chien	1999	IEEE Trans. Speech and Audio Processing	10.1109/89.799691	adaptive filter;algorithm design;calibration;speech recognition;index term;expectation–maximization algorithm;bayesian probability;computer science;machine learning;pattern recognition;software testing;efficiency;markov model;estimation theory;hidden markov model;statistics;robustness;approximation theory	ML	-18.892233209303086	-92.18121561221257	116779
0fdadcd0f1a46885f1d831333fc9afb4c157e8a4	depression detection &amp; emotion classification via data-driven glottal waveforms	speech processing emotion recognition medical disorders pattern classification psychology signal classification;speech processing;emotion recognition;psychology;affect classification automatic depression recognition emotion classification voice source waveform glottal waveform;medical disorders;glottal waveform;speech databases medical services speech recognition emotion recognition accuracy feature extraction;emotion classification;affect classification;signal classification;pattern classification;voice source waveform;depression diagnosis voice source waveform affective computing data driven glottal waveform representation depression detection accuracy improvement emotion recognition accuracy improvement depression severity classification idiosyncrasies mean shape measures ratio measures emotion classification automatic recognition systems objective tools;automatic depression recognition	This doctoral consortium paper outlines the author's proposed investigation into the use of the voice-source waveform for affective computing. A data-driven glottal waveform representation, previously examined in the authors earlier doctoral studies for its speaker discriminative abilities, is proposed to be studied for both depression detection and emotion recognition, including severity classification when considering depression. 'Data-driven' refers to a parameterisation focus on the small but consistent idiosyncrasies of the glottal wave rather than only the mean shape and ratio measures. A review of the literature is given covering existing studies of the glottal waveform for depression detection and emotion classification. The benefits of developing easily accessible automatic recognition systems is stressed. The value of developing objective tools for clinicians in diagnosing depression is also conveyed. Finally research questions are framed and experimental methodologies discussed in order to address these. The studies proposed here will expand the body of knowledge regarding the information content of the glottal waveform and aim to improve depression detection and emotion classification accuracies based on the voice-source alone.	affective computing;ampersand;emotion recognition;self-information;waveform	David Vandyke	2013	2013 Humaine Association Conference on Affective Computing and Intelligent Interaction	10.1109/ACII.2013.112	psychology;speech recognition;computer science;emotion classification;pattern recognition;speech processing;communication	AI	-8.579944243433982	-89.81168739632034	116912
8f2aa777f448aa8e546524bb868e9ed03747b0cb	a two-step technique for mri audio enhancement using dictionary learning and wavelet packet analysis		We present a method for speech enhancement of data collected in extremely noisy environments, such as those found during magnetic resonance imaging (MRI) scans. We propose a twostep algorithm to perform this noise suppression. First, we use probabilistic latent component analysis to learn dictionaries of the noise and speech+noise portions of the data and use these to factor the noisy spectrum into estimated speech and noise components. Second, we apply a wavelet packet analysis in conjunction with a wavelet threshold that minimizes the KL divergence between the estimated speech and noise to achieve further noise suppression. Based on both objective and subjective assessments, we find that our algorithm significantly outperforms traditional techniques such as nLMS, while not requiring prior knowledge or periodicity of the noise waveforms that current state-of-the-art algorithms require.	algorithm;dictionary;kullback–leibler divergence;machine learning;network packet;packet analyzer;quasiperiodicity;resonance;speech enhancement;wavelet;zero suppression	Colin Vaz;Vikram Ramanarayanan;Shrikanth (Shri) Narayanan	2013			gaussian noise;speech recognition;computer science;noise measurement;machine learning;pattern recognition	ML	-11.818145237994845	-93.22625691238834	116989
45a4e5d2d50202d8a87385a137884b848069876d	effect of anti-aliasing filtering on the quality of speech from an hmm-based synthesizer	anti aliasing;statistical parametric speech synthesis;down sampling;speech processing;filters;sampling frequency;speech quality;abstracts indexes organizing;hmm based speech synthesis;indexes;hidden markov models;organizing;abstracts;feature extraction;sampling frequency down sampling anti aliasing speech quality hmm based speech synthesis statistical parametric speech synthesis;cepstrum anti aliasing filtering effect hidden markov models hmm based synthesizer statistical parametric synthesis low pass filtering prerecorded speech sampling anti aliasing filters feature extraction nyquist frequency speech quality degradation;speech processing feature extraction filters hidden markov models	This paper investigates how the quality of speech produced through statistical parametric synthesis is affected by anti-aliasing filtering, i.e., low-pass filtering that is applied prior to (down-) sampling prerecorded speech at a desired rate. It has empirically been known that the frequency response of such anti-aliasing filters influences the quality of speech synthesized to a considerable degree. For the purpose of understanding such influence more clearly, in this paper we examine the spectral aspects of speech involved in the processes of HMM training and synthesis. We then propose a technique of feature extraction that can avoid producing the roll-off feature of the frequency response near the Nyquist frequency, which is found to be the major cause of speech quality degradation resulting from anti-aliasing filtering. In the technique, the spectrum is first computed from speech at a sampling rate higher than the desired rate, then it is truncated so that its frequency range above the target Nyquist frequency is discarded, and finally the truncated spectrum is converted directly into the cepstrum. Listening test results show that the proposed technique enables training HMMs efficiently with a limited number of model parameters and effectively with less artifacts in the speech synthesized at a desired sampling rate.	aliasing;anti-aliasing filter;cepstrum;elegant degradation;feature extraction;frequency band;frequency response;hidden markov model;low-pass filter;nyquist frequency;roll-off;sampling (signal processing);speech synthesis	Yoshinori Shiga	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288924	voice activity detection;database index;decimation;speech recognition;feature extraction;computer science;machine learning;pattern recognition;speech processing;sampling	Visualization	-9.79387568630056	-88.44465117127508	117034
6e15591459bd9342d6b78ea81160b497936b7263	phonological development in the home language among early polish-english bilinguals		The study involved the analysis of Polish speech samples of 59 Polish-English bilingual children of Polish immigrants to the UK. It aimed to explore differences in phonological performance between these early bilinguals and their Polish monolingual peers. The data collection procedure involved a sentence repetition task. 14 preselected sentences from this task were subsequently analysed auditorily by three phonetically trained raters. The measures of phonological performance included the number of speech errors made by children, and the assessment of cross-linguistic influence (CLI) in twelve areas (i.e. aspects of speech). Significant differences were found between the phonological performance measures of Polish-English bilinguals vs. 24 Polish monolingual controls. Bilinguals' speech was characterised by CLI from English, especially in the production of consonants and consonant clusters. As predicted, the phonology of the migrant Polish language in Polish-English bilingual children was found susceptible to the influence from English, the community language.	sentence extraction	Marta Marecka;Magdalena Wrembel;Agineszka Otwinowska-Kasztelanic	2015			phonological development;psychology;linguistics	NLP	-11.966073484077597	-81.42394968393961	117085
6aab5f5b05976542534f24796734935ee12e5031	early ganong effects		In the first of two experiments measuring Ganong effects, listeners were permitted to respond when they wished, while in the second, they were prompted to respond within a short interval at varying delays after stimulus onset. Both showed that lexical items were activated early and that their activation strength grew with response time and persisted after the effects of the stimuli’s acoustics began to fade. These results show that Ganong effects are not produced by late decision processes.	experiment;onset (audio);response time (technology)	Amanda Rysling;John Kingston;Adrian Staub;Andrew Cohen;Jeffrey Starns	2015			psychology	HCI	-8.625606756273624	-81.13213066396256	117176
bf5c6cbb0d9fb6f2d959813df88ac4700b6f376e	low-variance multitaper mel-frequency cepstral coefficient features for speech and speaker recognition systems	speech recognition;speaker verification;multitaper spectrum;aurora-2. nist 2010 sre;aurora-4	In this paper, we investigate low-variance multitaper spectrum estimation methods to compute the mel-frequency cepstral coefficient (MFCC) features for robust speech and speaker recognition systems. In speech and speaker recognition, MFCC features are usually computed from a single-tapered (e.g., Hamming window) direct spectrum estimate, that is, the squared magnitude of the Fourier transform of the observed signal. Compared with the periodogram, a power spectrum estimate that uses a smooth window function, such as Hamming window, can reduce spectral leakage. Windowing may help to reduce spectral bias, but variance often remains high. A multitaper spectrum estimation method that uses well-selected tapers can gain from the bias-variance trade-off, giving an estimate that has small bias compared with a single-taper spectrum estimate but substantially lower variance. Speech recognition and speaker verification experimental results on the AURORA-2 and AURORA-4 corpora and the NIST 2010 speaker recognition evaluation corpus (telephone as well as microphone speech), respectively, show that the multitaper methods perform better compared with the Hamming-windowed spectrum estimation method. In a speaker verification task, compared with the Hamming window technique, the sinusoidal weighted cepstrum estimator, multi-peak, and Thomson multitaper techniques provide a relative improvement of 20.25, 18.73, and 12.83 %, respectively, in equal error rate.	coefficient;computation;microphone;performance;speaker recognition;spectral density estimation;spectral leakage;text corpus;vocabulary	Md. Jahangir Alam;Patrick Kenny;Douglas D. O'Shaughnessy	2012	Cognitive Computation	10.1007/s12559-012-9197-5	speech recognition;pattern recognition	AI	-13.446229259029337	-91.17881199232691	117245
200b437f3025a68fb15e9c01c75fe2bc3bd8cf77	using audio-visual features for robust voice activity detection in clean and noisy speech	voice activity detector;etsi aurora vad audio speech information visual speech information robust voice activity detector statistical based audio only vad clean speech noisy speech mfcc vector visual only vad 2d discrete cosine transform 2d dct visual features audio visual vad signal to noise ratio snr av vad;discrete cosine transform;speech visualization accuracy signal to noise ratio support vector machine classification feature extraction;statistical analysis audio visual systems discrete cosine transforms signal denoising speech processing;visual features;audio visual;voice activity detection;signal to noise ratio;white noise	The aim of this work is to utilize both audio and visual speech information to create a robust voice activity detector (VAD) that operates in both clean and noisy speech. A statistical-based audio-only VAD is developed first using MFCC vectors as input. Secondly, a visual-only VAD is produced which uses 2-D discrete cosine transform (DCT) visual features. The two VADs are then integrated into an audio-visual VAD (AV-VAD). A weighting term is introduced to vary the contribution of the audio and visual components according to the input signal-to-noise ratio (SNR). Experimental results first establish the optimal configuration of the classifier and show that higher accuracy is obtained when temporal derivatives are included. Tests in white noise down to an SNR of -20dB show the AV-VAD to be highly robust with accuracy remaining above 97%. Comparison with the ETSI Aurora VAD shows the AV-VAD to be significantly more accurate.	av-test;aurora;discrete cosine transform;rca connector;signal-to-noise ratio;voice activity detection;white noise	Ibrahim Almajai;Ben P. Milner	2008	2008 16th European Signal Processing Conference		voice activity detection;speech recognition;acoustics;computer science;speech coding;communication	Vision	-12.624816383194913	-91.03756130860141	117350
4fa2aa3312aab008cad94e04c7c0015e27060d06	multi-task learning in deep neural networks for mandarin-english code-mixing speech recognition			deep learning;multi-task learning;speech recognition;super robot monkey team hyperforce go!	Mengzhe Chen;Jielin Pan;Qingwei Zhao;Yonghong Yan	2016	IEICE Transactions		natural language processing;multi-task learning;speech recognition;computer science;machine learning;time delay neural network;deep learning;artificial neural network	ML	-16.38826132215441	-87.7705101883623	117505
01e9be7570457d719e7b3683e6b77fcf32759593	syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese		Sequence-to-sequence attention-based models have recently rnshown very promising results on automatic speech recognition rn(ASR) tasks, which integrate an acoustic, pronunciation and rnlanguage model into a single neural network. In these models, rnthe Transformer, a new sequence-to-sequence attentionbased rnmodel relying entirely on self-attention without using rnRNNs or convolutions, achieves a new single-model state-ofthe- rnart BLEU on neural machine translation (NMT) tasks. Since rnthe outstanding performance of the Transformer, we extend rnit to speech and concentrate on it as the basic architecture of rnsequence-to-sequence attention-based model on Mandarin Chinese rnASR tasks. Furthermore, we investigate a comparison between rnsyllable based model and context-independent phoneme rn(CI-phoneme) based model with the Transformer in Mandarin rnChinese. Additionally, a greedy cascading decoder with the rnTransformer is proposed for mapping CI-phoneme sequences rnand syllable sequences into word sequences. Experiments on rnHKUST datasets demonstrate that syllable based model with rnthe Transformer performs better than CI-phoneme based counterpart, rnand achieves a character error rate (CER) of 28.77%, rnwhich is competitive to the state-of-the-art CER of 28.0% by rnthe joint CTC-attention based encoder-decoder network.	acoustic cryptanalysis;artificial neural network;automated system recovery;bleu;convolution;encoder;greedy algorithm;language model;neural machine translation;speech recognition;super robot monkey team hyperforce go!;syllable;transformer	Shiyu Zhou;Linhao Dong;Shuang Xu;Bo Xu	2018		10.21437/Interspeech.2018-1107	machine translation;architecture;syllable;speech recognition;bleu;word error rate;language model;artificial neural network;artificial intelligence;pronunciation;computer science;pattern recognition	NLP	-17.817989201909544	-87.52878269452566	117511
1bec6bfee0f920c7c95c4b14db93cf036c694fbb	automatic speech recognition in mandarin for embedded platforms.	automatic speech recognition	In this paper, we describe a real-time automatic speech recognition system for Mandarin for low-cost embedded platforms using fixed-point digital signal processors. The hands-free, speaker-independent speech recognition system employs 41 mono-phone models for representing the sounds in Mandarin Chinese and 11 whole-word models for connected digit recognition. The system achieves greater than 98% recognition accuracy on our hands-free test database of 46 distinct command phrases. The system achieves 95.9% digit accuracy on a 14 speaker, hands-free, connected digit recognition database. The analysis of the results shows that for speakers without dialect, the digit recognition accuracy is almost 98%. We present a detailed analysis of the digit recognition results and propose further improvements. A realtime platform based upon Lucent’s DSP1627 fixed-point digital signal processor has been developed.	central processing unit;digital signal processor;embedded system;fixed point (mathematics);fixed-point arithmetic;real-time clock;signal processing;speech recognition;super robot monkey team hyperforce go!	Fengguang Zhao;Prabhu Raghavan;Sunil K. Gupta;Ziyi Lu;Wentao Gu	2000			speech technology;speaker recognition;speech recognition;computer science;acoustic model	Mobile	-16.443193612795096	-86.27111307735707	117666
321ecf02a8c05b1b54bb2c7535f959161b4b645d	discourse prosody context - global f0 and tempo modulations	f0 height;topic change.;prosody context;global discourse prosody;paragraph association;tempo pattern;discourse association;duration modulations;statistical significance;speech communication	The present study is a corpus analysis of discourse prosodic information using two different types of fluent continuous Mandarin speech. Global F0 heights and duration patterns of withinand between-paragraph phrases were compared by discourse positions. Results showed that overall phrase-level F0 height was paragraph-initial>-medial>-final while the tempo pattern was paragraph-initial<-medial<-final. All of the differences were statistically significant across speakers and speech materials. The results suggest that discourse prosody context provides information of discourse planning, withinparagraph phrase association and between-paragraph topic change. We argue that global discourse prosody context is a crucial factor of speech communication, and can be applied to discourse segmentation, TTS synthesis naturalness and language pedagogy.	domain of discourse;semantic prosody;speech synthesis;super robot monkey team hyperforce go!;text corpus	Chiu-yu Tseng;Zhao-yu Su	2008			speech recognition;modulation (music);prosody;computer science;thrombus;blood vessel;lumen (unit)	NLP	-11.606706280130794	-81.0887208433244	117690
42141ad3e6b125c4377c7aa7722dae52c09263e0	improving markov chain classification using string transformations and evolutionary search	markov chain model;markov chain modeling;n gram model;bird song;natural language;genetic algorithm;similarity search;markov chain;genetic algorithm search	Markov chain classification or n-gram modeling, as it is sometimes called, is a very common and powerful tool for many problems that involve sequences of finite tokens. It has been used in a wide range of tasks, including natural language modeling, author identification, protein similarity searches, and even bird-song recognition. Clearly, an improvement in the Markov chain classification will have broad implications in many fields. Our new system, called SCS, improves upon Markov chain classification by introducing a preprocessing step in which an arbitrary set of transformation functions are performed on the input sequences. Since the space of possible transformations is unbounded, a genetic algorithm search is used to search for functions that improve classification. We show that GA is able to consistently find preprocessing functions that substantially improve the performance of the Markov chain model.	genetic algorithm;intrusion detection system;language model;markov chain;microelectronics and computer technology corporation;n-gram;natural language;preprocessor;software release life cycle	Timothy Meekhof;Terence Soule;Robert B. Heckendorn	2009		10.1145/1569901.1570070	forward algorithm;markov decision process;markov chain;maximum-entropy markov model;markov kernel;speech recognition;computer science;examples of markov chains;machine learning;pattern recognition;mathematics;markov algorithm;markov process;markov model;hidden markov model;statistics;variable-order markov model	ML	-4.834432946663487	-90.1897597814014	117705
2e4e4d58d77bd21ed12686fecbf48df8b91d7529	experience with speech recognition in automating telephone operator functions	speech recognition		speech recognition	Dina Yashchin;William C. G. Ortel	1991			voice activity detection;speech recognition;speech analytics;natural language processing;operator (computer programming);speech technology;speaker recognition;computer science;artificial intelligence;speech synthesis	ML	-15.561462969955818	-85.84529711530584	117787
1c4db65a3d3d154ce82fbe74efe6af525a1accc8	tone contour realization in sung cantonese		Twelve native speakers of Cantonese (6M, 6F) were recorded singing a tonal minimal set of words ([si]) in the context of a specially composed children’s song. Analysis of slope measurements of the vowels suggests that singers include a rising contour when singing a rising tone. They do not appear to include a falling contour when singing a falling tone. F0 appears to fulfill multiple demands in singing.	contour line	Murray Schellenberg	2011			speech recognition;tone contour;psychology	Vision	-10.747169884261623	-82.9689593945153	117913
1e03beb6416ac8cd90bf12fc35d254cd1a52e6fd	on the importance of pre-emphasis and window shape in phase-based speech recognition		This paper aims at investigating the potentials of the phase spectrum in automatic speech recognition (ASR). We show that speech phase spectrum could potentially provide features with high discriminability and robustness. Out of such belief and to realize a higher portion of the phase spectrum potentials, we propose two simple amendments in two common blocks in feature extraction, namely pre-emphasis and windowing, without changing the workflow of the algorithms. Recognition tests over Aurora 2 indicate up to 11.2% and 14.7% performance improvement in average in the presence of both additive and convolutional noises for phase-based MODGDF and CGDF features, respectively. It proves the high potentials of the phase spectrum in robust ASR.	algorithm;aurora;emphasis (telecommunications);feature extraction;robustness (computer science);spectral density;speech recognition;utility functions on indivisible goods	Erfan Loweimi;Seyed Mohammad Ahadi;Thomas Drugman;Samira Loveymi	2013		10.1007/978-3-642-38847-7_21	speech recognition;machine learning;pattern recognition	AI	-13.445342018457087	-91.19674878371525	118020
b438e7f86193cf8084d2a1ba329d3b41641025b9	orthographic vs. morphological incomplete neutralization effects		This study, following up on work on Dutch by Warner, Jongman, Sereno, and Kemps (2004. Journal of Phonetics, 32, 251–276), investigates the influence of orthographic distinctions and underlying morphological distinctions on the small sub-phonemic durational differences that have been called incomplete neutralization. One part of the previous work indicated that an orthographic geminate/ singleton distinction could cause speakers to produce an incomplete neutralization effect. However, one interpretation of the materials in that experiment is that they contain an underlying difference in the phoneme string at the level of concatenation of morphemes, rather than just an orthographic difference. Thus, the previous effect might simply be another example of incomplete neutralization of a phonemic distinction. The current experiment, also on Dutch, uses word pairs which have the same underlying morphological contrast, but do not differ in orthography. These new materials show no incomplete neutralization, and thus support the hypothesis that orthography, but not underlying morphological differences, can cause incomplete neutralization effects. r 2005 Elsevier Ltd. All rights reserved. see front matter r 2005 Elsevier Ltd. All rights reserved. wocn.2004.11.003 ding author. Department of Linguistics, University of Arizona, P.O. Box 210028, Tucson, 8, USA. Tel.: +1 520 626 5591; fax: +1 520 626 9014. ress: nwarner@u.arizona.edu (N. Warner).	concatenation;fax;like button;microsoft word for mac;orthographic projection	Natasha Warner;Erin Good;Allard Jongman;Joan A. Sereno	2006	J. Phonetics	10.1016/j.wocn.2004.11.003	speech recognition;mathematics;linguistics;communication	NLP	-10.803706297498355	-80.4350217574169	118027
5b6d4146b29841fef5bd36a17e759eaa818d700a	telephony based speaker-independent large vocabulary continuous mandarin speech recognition			speech recognition;super robot monkey team hyperforce go!;vocabulary	Jia-Lin Shen;Ying-chieh Tu;Po-yu Liang;Lin-Shan Lee	1999	IJCLCLP		mandarin chinese;speech recognition;telephony;vocabulary;computer science	NLP	-16.048280168372077	-86.02813710692055	118153
496f31a90d2e8e63fa08aead0926e32518020747	the importance of f0 tracking in query-by-singing-humming		In this paper, we present a comparative study of several state-of-the-art F0 trackers applied to the context of queryby-singing-humming (QBSH). This study has been carried out using the well known, freely available, MIR-QBSH dataset in different conditions of added pub-style noise and smartphone-style distortion. For audio-to-MIDI melodic matching, we have used two state-of-the-art systems and a simple, easily reproducible baseline method. For the evaluation, we measured the QBSH performance for 189 different combinations of F0 tracker, noise/distortion conditions and matcher. Additionally, the overall accuracy of the F0 transcriptions (as defined in MIREX) was also measured. In the results, we found that F0 tracking overall accuracy correlates with QBSH performance, but it does not totally measure the suitability of a pitch vector for QBSH. In addition, we also found clear differences in robustness to F0 transcription errors between different matchers.	baseline (configuration management);distortion;midi;query by humming;smartphone;transcription (software);whole earth 'lectronic link	Emilio Molina;Lorenzo J. Tardón;Isabel Barbancho;Ana M. Barbancho	2014			robustness (computer science);speech recognition;melody;bittorrent tracker;transcription (linguistics);singing;hum;distortion;computer science	NLP	-12.641304584734828	-89.6578602705661	118213
4078ef3a5f26a2d2bced99ffa600ff6192ac4fd2	strategies for using mlp based features with limited target-language training data	training data hidden markov models detectors speech acoustics feature extraction training;low resource scenario mlp based features target language training data lvcsr systems acoustic training data automatic speech attribute transcription asat conventional baseline model level methods feature level approaches multilingual scenario;articulatory features;articulatory feature;multi layer perceptrons;low resource asr;tandem features low resource asr multi layer perceptrons articulatory features;tandem features;target language;speech recognition;multi layer perceptron	Recently there has been some interest in the question of how to build LVCSR systems when there is only a limited amount of acoustic training data in the target language, but possibly more plentiful data in other languages. In this paper we investigate approaches using MLP based features. We experiment with two approaches: One is based on Automatic Speech Attribute Transcription (ASAT), in which we train classifiers to learn articulatory features. The other approach uses only the target-language data and relies on combination of multiple MLPs trained on different subsets. After system combination we get large improvements of more than 10% relative versus a conventional baseline. These feature-level approaches may also be combined with other, model-level methods for the multilingual or low-resource scenario.	acoustic cryptanalysis;baseline (configuration management);compiler;google map maker;hidden markov model;medical transcription;memory-level parallelism;pl/p;speech analytics;teaching method	Yanmin Qian;Ji Xu;Daniel Povey;Jia Liu	2011	2011 IEEE Workshop on Automatic Speech Recognition & Understanding	10.1109/ASRU.2011.6163957	speech recognition;computer science;machine learning;pattern recognition;multilayer perceptron	NLP	-18.45156907199251	-89.15506450132821	118279
5c91b8c00b489fb23d6e4a0ecc0d6060bcca2cef	microsegment synthesis - economic principles in a low-cost solution	natural languages;gold;time domain;speech recognition;microcomputers;speech intelligibility;speech synthesis;speech processing;german;linear predictive coding;intelligibility;economics	"""A low-cost concatenation based speech synthesis system for German is described which combines the advantage of minimal memory requirements with good intelligibility and high segmental and prosodic acceptability. This is achieved by the multiple use of """"microsegments"""", stretches of speech signal varying in length from demi-phone to phone size. All prosodic structuring is carried out in the time domain."""	concatenation;intelligibility (philosophy);requirement;speech synthesis	Ralf Benzmüller;William J. Barry	1996				EDA	-15.263528390982609	-85.43608383318058	118349
a389553deab6a3cd883e445786c7f0f8b0337168	hierarchical discriminant features for audio-visual lvcsr	word error rate;audio signal processing;word error rate hierarchical discriminant features audio visual speaker independent large vocabulary continuous speech recognition hierarchical two stage discriminant transformation automatic speech recognition linear discriminant analysis maximum likelihood linear transform discrete cosine transform single modality features hmm based speech recognizer ibm viavoice audio visual database feature fusion method clean audio conditions noisy audio conditions;large vocabulary continuous speech recognition;video signal processing;linear discriminate analysis;matrix algebra;maximum likelihood estimation;discrete cosine transform;automatic speech recognition;hidden markov models;statistical analysis;speaker independent;region of interest;linear discriminant analysis speech recognition automatic speech recognition discrete transforms mel frequency cepstral coefficient discrete cosine transforms hidden markov models spatial databases audio databases vocabulary;linear transformation;speech recognition;audio visual;feature fusion;sensor fusion;audio signal processing hidden markov models speech recognition matrix algebra video signal processing sensor fusion maximum likelihood estimation statistical analysis	We propose the use of a hierarchical, two-stage discriminant transformation for obtaining audio-visual features that improve automatic speech recognition. Linear discriminant analysis (LDA), followed by a maximum likelihood linear transform (MLLT) is first applied on MFCC based audio-only features, as well as on visualonly features, obtained by a discrete cosine transform of the video region of interest. Subsequently, a second stage of LDA and MLLT is applied on the concatenation of the resulting single modality features. The obtained audio-visual features are used to train a traditional HMM based speech recognizer. Experiments on the IBM ViaVoice audio-visual database demonstrate that the proposed feature fusion method improves speaker-independent, large vocabulary, continuous speech recognition for both clean and noisy audio conditions considered. A 24% relative word error rate reduction over an audio-only system is achieved in the latter case.	concatenation;discrete cosine transform;finite-state machine;hidden markov model;linear discriminant analysis;modality (human–computer interaction);region of interest;speech analytics;speech recognition;viavoice;vocabulary;word error rate	Gerasimos Potamianos;Juergen Luettin;Chalapathy Neti	2001		10.1109/ICASSP.2001.940793	speech recognition;audio signal processing;word error rate;computer science;machine learning;discrete cosine transform;pattern recognition;sensor fusion;linear map;maximum likelihood;hidden markov model;region of interest	Vision	-15.528434372399694	-92.61806828065822	118351
120eb316eaaca51e3eece12571f7616a1112f96a	low-variance multitaper mfcc features: a case study in robust speaker verification	mel frequency cepstral coefficient discrete fourier transforms speech processing speech frequency domain analysis analytical models robustness;estimation theory;time domain analysis autoregressive processes cepstral analysis discrete fourier transforms estimation theory frequency domain analysis gaussian processes speaker recognition speech processing statistical analysis support vector machines;support vector machines;small variance;gaussian processes;noncritical parameter selection low variance multitaper mfcc features robust speaker verification speech applications audio applications short term signal spectrum mel frequency cepstral coefficients windowed discrete fourier transform spectral leakage spectrum estimate multitaper method multiple time domain windows frequency domain averaging speech processing low variance features mfcc extraction statistical analysis mfcc bias mfcc variance autoregressive process simulations timit corpus speaker verification experiments nist 2002 sre corpora nist 2008 sre corpora gaussian mixture model universal background model gmm ubm support vector machine gmm svm joint factor analysis gmm jfa mindcf baseline windowed dft interview interview condition telephone data;frequency domain analysis;speech processing;multitaper;autoregressive process;spectrum;mel frequency cepstral coefficient mfcc;time domain analysis;speaker verification;speaker recognition;mel frequency cepstral coefficient;article letter to editor;gaussian mixture model;cepstral analysis;statistical analysis;estimation;parameter selection;autoregressive processes;discrete fourier transform;time domain;variance estimation;universal background model;support vector machine;frequency domain;discrete fourier transforms;joint factor analysis;speaker verification mel frequency cepstral coefficient mfcc multitaper small variance estimation;sannolikhetsteori och statistik;analytical model	In speech and audio applications, short-term signal spectrum is often represented using mel-frequency cepstral coefficients (MFCCs) computed from a windowed discrete Fourier transform (DFT). Windowing reduces spectral leakage but variance of the spectrum estimate remains high. An elegant extension to windowed DFT is the so-called multitaper method which uses multiple time-domain windows (tapers) with frequency-domain averaging. Multitapers have received little attention in speech processing even though they produce low-variance features. In this paper, we propose the multitaper method for MFCC extraction with a practical focus. We provide, first, detailed statistical analysis of MFCC bias and variance using autoregressive process simulations on the TIMIT corpus. For speaker verification experiments on the NIST 2002 and 2008 SRE corpora, we consider three Gaussian mixture model based classifiers with universal background model (GMM-UBM), support vector machine (GMM-SVM) and joint factor analysis (GMM-JFA). Multitapers improve MinDCF over the baseline windowed DFT by relative 20.4% (GMM-SVM) and 13.7% (GMM-JFA) on the interview-interview condition in NIST 2008. The GMM-JFA system further reduces MinDCF by 18.7% on the telephone data. With these improvements and generally noncritical parameter selection, multitaper MFCCs are a viable candidate for replacing the conventional MFCCs.	addendum;autoregressive model;baseline (configuration management);coefficient;discrete fourier transform;experiment;factor analysis;google map maker;matlab;mel-frequency cepstrum;microphone;microsoft windows;mixture model;reference implementation;signal processing;simulation;speaker recognition;spectral density estimation;spectral leakage;speech processing;support vector machine;timit;text corpus;variance reduction;www;window function	Tomi Kinnunen;Rahim Saeidi;Filip Sedlak;Kong-Aik Lee;Johan Sandberg;Maria Hansson;Haizhou Li	2012	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2012.2191960	speaker recognition;support vector machine;speech recognition;computer science;pattern recognition;speech processing;mathematics;frequency domain;statistics	ML	-14.935633850777572	-92.89238467609843	118389
17475418c35030e096f219555abb5e5a4ff84160	fitting new speakers based on a short untranscribed sample		Learning-based Text To Speech systems have the potential to generalize from one speaker to the next and thus require a relatively short sample of any new voice. However, this promise is currently largely unrealized. We present a method that is designed to capture a new speaker from a short untranscribed audio sample. This is done by employing an additional network that given an audio sample, places the speaker in the embedding space. This network is trained as part of the speech synthesis system using various consistency losses. Our results demonstrate a greatly improved performance on both the dataset speakers, and, more importantly, when fitting new voices, even from very short samples.	curve fitting;feed forward (control);mathematical optimization;netware file system;requirement;self-replicating machine;semi-supervised learning;semiconductor industry;speech synthesis;text corpus;uncontrolled format string	Eliya Nachmani;Adam Polyak;Yaniv Taigman;Lior Wolf	2018			machine learning;artificial intelligence;embedding;computer science;speech synthesis	ML	-18.031971474575144	-88.71906562725542	118391
08747a8dfec297b05deba2bf7b719373b001023b	genre-based classification of song using perceptual features		Genre-based classification of song is one of the major steps in the music retrieval system. In this work, we have presented perception-based song genre classification. Many of the past researchers have been using combination of perception-based features and other popular features such as zero-crossing, short-time energy. We have used three perceptual features that capture the ordering of sound in frequency scale (pitch-based features), the pace of a musical piece (tempo-based features), and repetition of a pattern in the audio signal. In order to capture the repeating pattern in a signal, we have used cooccurrence matrix. The experimental result using multilayer perceptron network as a classifier indicates the effectiveness of our proposed scheme.		Arijit Ghosal;Rudrasis Chakraborty;Bibhas Chandra Dhara;Sanjoy Kumar Saha	2013		10.1007/978-81-322-1665-0_26	perception;multilayer perceptron;artificial intelligence;classifier (linguistics);audio signal;pattern recognition;computer science	Vision	-8.549422207520342	-91.2213623165078	118830
ae01df892de2899f7e79798dc00c75222aeeb37d	the neutralization of the voice quality distinction in dinka songs: a production and perception study		The purpose of this study is to determine if the phonemic voice quality distinction of Dinka is also conveyed in songs. Acoustic measurements (formant tracking and spectral regression) were applied for the first time to a song data set. The results clearly pointed towards a neutralization of the voice quality distinction in songs. A perception experiment was performed to back up the results of the production study. Listeners were first trained to distinguish between modal and breathy vowels in speech, and then tested on speech and song data. The experimental results corroborate the claim that no voice quality distinction is conveyed in singing. Given the absence of any significant acoustic cue shown by the acoustic analysis or detected by the listeners, the author concludes that context seems to be the only means to disambiguate homophones in songs.	acoustic cryptanalysis;backup;modal logic	Luca Rognoni	2011			communication;perception;psychology	NLP	-10.262711467682973	-83.11299736643778	118882
7b88ec38d163bfd7ccf48c43538693c3890246b1	a study on adaptations of cepstral and delta cepstral coefficients for noisy speech recognition	speech recognition		cepstrum;coefficient;speech recognition	Lee-Min Lee;Hsiao-Chuan Wang	1994			artificial intelligence;speech recognition;mel-frequency cepstrum;computer science;pattern recognition;cepstrum	NLP	-13.861994976620647	-88.3701470966483	118918
160564a70d92f0fe06c08db3ab95338a7506f794	porting statistical parsers with data-defined kernels	statistical parsers;neural network probabilistic model;previous result;source domain;svm classifier;small amount;target domain;data-defined kernel;disappointing performance;large margin classifier;probabilistic model;neural network	Previous results have shown disappointing performance when porting a parser trained on one domain to another domain where only a small amount of data is available. We propose the use of data-defined kernels as a way to exploit statistics from a source domain while still specializing a parser to a target domain. A probabilistic model trained on the source domain (and possibly also the target domain) is used to define a kernel, which is then used in a large margin classifier trained only on the target domain. With a SVM classifier and a neural network probabilistic model, this method achieves improved performance over the probabilistic model alone.	algorithmic efficiency;artificial neural network;computation;experiment;kernel (operating system);margin classifier;parse tree;parsing;self-replication;software portability;statistical model;text corpus;treebank	Ivan Titov;James Henderson	2006			speech recognition;computer science;machine learning;pattern recognition	ML	-17.268089076539123	-89.47491021323546	118997
a076b9b70e61259d8f443e4d922d611b7c75b353	dynamic estimation of a noise over estimation factor for jacobian-based adaptation	reconnaissance automatique de la parole;testing;usa councils;jacobian matrices hidden markov models adaptation model testing usa councils;automatic speech recognition;parallel model combination;adaptation model;hidden markov models;computational complexity;adaptation jacobienne;jacobian adaptation;robustesse;robustness;jacobian matrices	In this paper we propose an enhancement of the Jacobian adaptation by estimating automatically a noise over estimation factor which yields to a closer approximation of Parallel model combination (PMC) than the traditional Jacobian adaptation. Noise over estimation factors are estimated at run-time for a set of clustered Gaussians obtained on the training set. Experiments conducted on a French natural number database show that similar performance as PMC can be obtained at the expense of a slight increase in computational complexity as compared to Jacobian adaptation.	approximation;computational complexity theory;experiment;jacobian matrix and determinant;test set	Christophe Cerisara;Jean-Claude Junqua;Luca Rigazio	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5743689	simulation;speech recognition;computer science;artificial intelligence;machine learning;software testing;computational complexity theory;hidden markov model;statistics;robustness	Robotics	-19.05212231969601	-91.67548522689265	119101
5d59d35730f7dd96111e84892d45c7191af6d1bc	a new acoustic measure for aspiration noise detection.	vocal tract;automatic detection	In this paper, we propose a new acoustic measure for detecting aspiration noise in vowels. The measure is an index of synchronization between frequency bands around the first and third formants. The measure is based on the principle that the vocal tract responses to the glottal excitation are synchronized between these frequency bands when aspiration noise is absent, and uncorrelated otherwise. Evaluation results show that the proposed measure can be used together with spectral slope measures for automatic detection of aspiration noise.	acoustic cryptanalysis;frequency band;sensor;spectral slope;tract (literature)	Carlos Toshinori Ishi	2004			vocal tract;computer science	NLP	-9.805677029037028	-89.24603829532577	119202
5b1d947a8896c4be7a93cb2b85736fc4e82b1d5e	single channel speech enhancement for mixed non-stationary noise environments		Speech enhancement is very important step for improving quality and intelligibility of noisy speech signal. In practical environment more than one noise sources are present, hence it is necessary to design a technique/ algorithm that can remove mixed noises or more than one noises from single-channel speech signals. In this paper, a single channel speech enhancement method is proposed for reduction of mixed non-stationary noises. The proposed method is based on wavelet packet and ideal binary mask thresholding function for speech enhancement. Db10 mother wavelet packet transform is used for decomposition of speech signal in three levels. After decomposition of speech signal a binary mask threshold function is used to threshold the noisy coefficients from the noisy speech signal coefficients. The performance of the proposed wavelet with ideal mask method is compared with Wiener, Spectral Subtraction, p-MMSE, log-MMSE, Ideal channel selection, Ideal binary mask, hard and soft wavelet thresholding function in terms of PESQ, SNR improvement, Cepstral Distance, and frequency weighted segmental SNR. The proposed method has shown improved performance over conventional speech enhancement methods.	speech enhancement;stationary process	Sachin Singh;Manoj Tripathy;R. S. Anand	2014		10.1007/978-3-319-04960-1_47	wavelet;artificial intelligence;pesq;mathematics;intelligibility (communication);wavelet packet decomposition;pattern recognition;cepstrum;thresholding;speech enhancement;communication channel	HPC	-13.096051537450075	-92.88299201399133	119233
0a213ea1efb2b615c2917ff3b6e0090139b851c6	acoustic synthesis of training data for speech recognition in living room environments	covariance matrices;filtering theory;hidden markov models;speech recognition;speech synthesis;transient response;white noise;acoustic mismatch;acoustic synthesis;clean speech;close talk;command recognition task;distant microphone;impulse responses;large speech collections;living room environments;natural number recognition;noise signals;recorded speech;robust automatic speech recognition;training data	Despitecontinuousprogressin robustautomaticspeech recognitionin recentyearsacousticmismatchbetweentraining and test conditionsis still a major problem. Consequently, largespeechcollectionsmustbeconductedin many environments.An alternati ve approachis to generatetraining datasyntheticallyby filtering cleanspeechwith impulse responsesand/oraddingnoisesignalsfrom the target domain. We comparetheperformanceof a speechrecognizer trainedon recordedspeechin thetargetdomainwith a systemtrainedon suitablytransformedcleanspeech.In order to obtaincomparableresults,our experimentsarebasedon two channelrecordingswith aclosetalkandadistantmicrophonewhichproducethecleansignalandthetargetdomain signalrespecti vely. By filtering andaddingnoiseweobtain error rateswhich areonly 10% higherfor naturalnumber recognitionand30%higherfor acommandrecognitiontask comparedto trainingwith targetdomaindata.	acoustic cryptanalysis;speech recognition	Volker Stahl;Alexander Fischer;Rolf-Dieter Bippus	2001		10.1109/ICASSP.2001.940757	voice activity detection;speech recognition;impulse response;word error rate;computer science;speech processing;acoustic model;white noise;speech synthesis;transient response	ML	-14.858514623391697	-92.16877784347962	119260
14bafac70ecca09e7eb09996d4bb669fa38c6a15	evaluation of wavelet filters for speech recognition	lpc;signal decomposition;flexible time frequency resolutions;image coding;filter bank;band pass filters;hidden markov model;time frequency;filters;hmm;filters speech recognition wavelet transforms hidden markov models linear predictive coding;wavelet decomposition;wavelet transforms;linear predictive coding;hidden markov models;mel cepstrums;channel bank filters;feature extraction;modified octave structured 5 level filter bank;wavelet filter evaluation;signal resolution;speech recognition;speech recognition filter bank wavelet transforms band pass filters channel bank filters signal resolution image coding time frequency analysis hidden markov models feature extraction;time frequency analysis;mel cepstrums wavelet filter evaluation speech recognition wavelet decomposition signal decomposition flexible time frequency resolutions feature set modified octave structured 5 level filter bank hmm hidden markov model lpc;feature set	Since wavelet decomposition of signals provides more flexible time-frequency resolutions, it can be utilized as a feature set for speech recognition. The authors explore the possibility of using wavelet decomposition for speech recognition. In particular, they investigate a modified octave structured 5-level filter bank and the HMM (hidden Markov model) is used as a recognizer. We present an analysis of various wavelet filters for speech recognition and compare the results with the conventional features that include LPC and mel-cepstrums.	speech recognition;wavelet	Kidae Kim;Dae Hee Youn;Chulhee Lee	2000		10.1109/ICSMC.2000.884438	wavelet;speech recognition;time–frequency analysis;second-generation wavelet transform;computer science;machine learning;pattern recognition;speech processing;wavelet packet decomposition;stationary wavelet transform;hidden markov model	ML	-9.656304313400849	-91.39050953693922	119446
4ed86d09f673a666525aeaa643f08027e0b75bef	topic transitions and durational prosody in reading aloud: production and modeling	analyse parole;modelizacion;lectura oral;analisis palabra;speech analysis;relationship to text structure;perception of prosody;modelisation;lecture orale;evaluation subjective;prosodic modeling;anglais;oral reading;prosodie;mathematical model;english;prosody;subjective evaluation;ingles;modeling;final lengthening;prosodia;american english;evaluacion subjetiva	The linguistic structure of an utterance is known to affect the durational prosody of sounds, words and phrases. There has been increasing interest in how discourse-level organization affects prosody, in part because modeling discourse-level effects could improve the comprehensibility of longer passages of synthesized text. The approach taken here is to look at how topics are sequenced in a text, and how this affects durational prosody when that text is read aloud. Two speakers of American English were recorded reading a set of text materials on 10 separate occasions. Measurements of these recordings indicated that the type of transition in topic between two successive sentences had a significant effect on the amount of sentence-final lengthening, the duration of the pause between sentences, and the speech rate at the end of a sentence and the beginning of the following sentence. These measurements were then used to create a mathematical model of one speaker, and to generate several versions of one of this speaker s original recordings, with each version incorporating different manipulations of the durational patterns and their variability. These versions were played to listeners, who preferred those where the manipulations included durational patterns reflecting the organization of topics in the text. 2003 Elsevier B.V. All rights reserved.	mathematical model;semantic prosody;spatial variability	Caroline L. Smith	2004	Speech Communication	10.1016/j.specom.2003.09.004	natural language processing;speech recognition;computer science;english;mathematical model;linguistics;prosody	NLP	-12.505573295082655	-81.48998169632138	119530
b2a75100f9c253674479ca767cee9b57f8b1b1fe	an expert system for the production of phoneme strings from unmarked english text using machine-induced rules	word level;two-tiered expert system;expert rule;expert system;cluster level;high quality speech output;diphone speech synthesis system;system maps english text;machine-induced rule;computer-based education research laboratory;phoneme string;unmarked english text;diphone template;speech synthesis group;speech synthesis;front end;short time fourier transform;decision tree	"""The speech synthesis group at the ComputerBased Education Research Laboratory (CERL) of the University of Illinois at Urbana-Champalgn is developing a diphone speech synthesis system based on pltch-adaptive short-tlme Fourier transforms. This system accepts the phonemic specification of an utterance along with pitch, time, and amplitude warping functions in order to produce high quality speech output from stored dlphone templates. This paper describes the operation of a program which operates as a front end for the dlphone speech synthesis system. The UTTER (for """"Unmarked Text Transcription by Expert Rule"""") system maps English text onto a phoneme string, which is then used as an input to the dlphone speech synthesis system. The program is a twotiered Expert System which operates first on the word level and then on the (vowel or consonant) cluster level. The system's knowledge about pronunciation is organized in two decision trees automatically generated by an induction algorithm on a dynamically specified """"training set"""" of examples. in that they are often unable to cope with a letter pattern that maps onto more than one phoneme pattern. Extreme cases are those words which, although differing in pronunciation, share orthographic representations (an analogous problem exists in speech recognition, where words which share phonemic representations differ in orthographic representation, and therefore possibly in semantic interpretation). A notable exception is the MIT speech synthesis system fAllen81] which is llngulstlcally-based, but not solely phoneme-based. A desirable feature in any rule-based system is the ability to automatically acquire or modify its own rules. Previous work [Oakey81] applies this automatic inference process to the text-tophoneme transcription problem. Unfortunately, Onkey's system is strlctly letter-based and suffers from the same deficiencies as other nonilnguistlcally-based systems. The UTTER system is an attempt to provide a llngulstlcally-based transcription system which has the ability to automatically acquire its own rule base."""	algorithm;decision tree;display resolution;event-driven programming;expert system;map;mathematical induction;medical transcription;orthographic projection;plato (computer system);pattern language;pitch (music);rule-based system;semantic interpretation;speech recognition;speech synthesis;test set;transcription (software)	Alberto Maria Segre;Bruce Arne Sherwood;Wayne B. Dickerson	1983			natural language processing;speech recognition;mbrola;short-time fourier transform;computer science;artificial intelligence;front and back ends;decision tree;linguistics;speech synthesis;expert system	NLP	-17.734716983147134	-83.50227928403756	119542
058ed2576a9af420cf9146e4b8427f9157e928a9	anti-models: - an alternative way to discriminative training		Traditional discriminative training methods modify Hidden Markov Model (HMM) parameters obtained via a Maximum Likelihood (ML) criterion based estimator. In this paper, anti-models are introduced instead. The antimodels are used in tandem with ML models to incorporate a discriminative information from training data set and modify the HMM output likelihood in a discriminative way. Traditional discriminative training methods are prone to over-fitting and require an extra stabilization. Also, convergence is not ensured and usually “a proper” number of iterations is done. In the proposed anti-models concept, two parts, positive model and anti-model, are trained via ML criterion. Therefore, the convergence and the stability are ensured.	computation;discriminative model;hidden markov model;iteration;markov chain;overfitting;test set	Jan Vanek;Josef Psutka	2014		10.1007/978-3-319-10816-2_54	discriminative model;computer science;speech recognition;estimator;maximum likelihood;training set;hidden markov model;artificial intelligence;convergence (routing);pattern recognition	Vision	-18.9520865831817	-92.14563949999565	119697
54a74105c5e68d3a28646eee5fe6def60bd6e168	automatic speech recognition with articulatory information and a unified dictionary for hindi, marathi, bengali and oriya		Despite the continuous progress of Automatic Speech recognition (ASR) technologies, these systems for Indian languages are still in infancy stage due to a multitude of challenges involved, including resource deficiency. This paper addressed this challenge with four Indian languages, Hindi, Marathi, Bengali, and Oriya by integrating articulatory information into acoustic features, thereby compensating the low resource property of these languages. Articulatory movements were recorded during speech production using an electromagnetic articulograph and trained together with acoustic features to build automatic speech recognizers for these languages. Both speaker-dependent and independent recognition experiments were conducted by adopting three ASR models: Gaussian Mixture Model (GMM)Hidden Markov Model (HMM), Deep Neural Network (DNN)HMM, and Long Short Term Memory recurrent neural network (LSTM)-HMM. A cross-language similarity was discerned in both acoustic and articulatory domains in the pairs of OriyaBengali and Hindi-Marathi. Based on these observations, a multi-lingual, multi-modal speech recognizer was built by constructing a unified dictionary consisting of common and unique phonemes of all the four languages, which significantly reduced the phoneme error rates.	acoustic cryptanalysis;angular defect;artificial neural network;deep learning;dictionary;experiment;finite-state machine;hidden markov model;long short-term memory;markov chain;mixture model;modal logic;recurrent neural network;speech recognition;speech synthesis	Debadatta Dash;Myung Jong Kim;Kristin Teplansky;Jun Wang	2018		10.21437/Interspeech.2018-2122	speech recognition;bengali;hindi;marathi;computer science;oriya	NLP	-17.777777598848218	-87.73036572410193	119974
ec2c383dcc20644454a79849fed167e5cc7760ec	irrelevant variability normalization based hmm training using map estimation of feature transforms for robust speech recognition	feature transforms;robust speech recognition;phonetic classification;hidden markov model robust speech recognition feature transformation map estimate;finnish aurora3 database;maximum likelihood;feature transformation;hidden markov model;frame dependent linear transformation;hmm training;indexing terms;maximum likelihood estimation;maximum likelihood training;automatic speech recognition;finnish aurora3 database irrelevant variability normalization hmm training hidden markov model map estimation feature transforms robust speech recognition phonetic classification maximum likelihood training frame dependent linear transformation maximum a posteriori;hidden markov models;map estimation;linear transformation;speech recognition;irrelevant variability normalization;maximum a posteriori;map estimate;hidden markov models robustness speech recognition automatic speech recognition training data support vector machines gaussian distribution asia electronic mail maximum likelihood estimation;speech recognition hidden markov models maximum likelihood estimation	"""In the past several years, we've been studying feature transformation (FT) approaches to robust automatic speech recognition (ASR) which can compensate for possible """"distortions"""" caused by factors irrelevant to phonetic classification in both training and recognition stages. Several FT functions with different degrees of flexibility have been studied and the corresponding maximum likelihood (ML) training techniques developed. In this paper, we study yet another new FT function which takes the most flexible form of frame-dependent linear transformation. Maximum a posteriori (MAP) estimation is used for estimating FT function parameters to deal with the possible problem of insufficient training data caused by the increased number of model parameters. The effectiveness of the proposed approach is confirmed by evaluation experiments on Finnish Aurora3 database."""	automated system recovery;distortion;experiment;financial times;hidden markov model;relevance;spatial variability;speech recognition;yet another	Donglai Zhu;Qiang Huo	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518710	speech recognition;computer science;maximum a posteriori estimation;machine learning;pattern recognition;maximum likelihood;hidden markov model;statistics	Vision	-18.029905297643815	-91.69637787218976	120104
06840fca60a25d008525e0095a9afc06e5bd74c1	acoustic cues for the auditory identification of the spanish fricative /f/		This study deals with the distinction of the fricative noises of the spanish fricatives / S and /f/. Previous studies revealed that fricative noises of both phonemes are perceptually similar, auditory identification being significantly dependent on contextual effects: /f/ in the /u/ context is well identified (about 85% correct identification rate), while in the /e/ context identification is much lower (about 60%). Identification of / S is low for every vocalic context (about 60%). These effects were identical for both Hypo and Hyper forms of speech for which perceptual experiments were performed separately. The objective of this paper is to determine which acoustic properties of /f/ in the /u/ context make it a well defined phoneme for the two different forms of speech, in relation to the fricative noises of / S in the /e,u/ contexts, and /f/ in the /e/ context. We conclude that the cues for the identification of the isolated fricative noise of /f/ seem to be in the low frequency region of the spectrum.	acoustic cryptanalysis;experiment;station hypo	Santiago Fernández;Sergio Feijóo;Ramón Balsa;Nieves Barros	1998			speech recognition;computer science	HCI	-10.206435414468261	-82.46862650038166	120154
b7b0e46f399c4f012978b1ee71e7625928c36634	open language interface for voice exploitation (olive)		We propose to demonstrate the Open Language Interface for Voice Exploitation (OLIVE) speech-processing system, which SRI International developed under the DARPA Robust Automatic Transcription of Speech (RATS) program. The technology underlying OLIVE was designed to achieve robustness to high levels of noise and distortion for speech activity detection (SAD), speaker identification (SID), language and dialect identification (LID), and keyword spotting (KWS). Our demonstration will show OLIVE performing those four tasks. We will also demonstrate SRI’s speaker recognition capability live on a mobile phone for visitors to interact with.	distortion;medical transcription;mobile phone;speaker recognition;speech processing	Aaron Lawson;Mitchell McLaren;Harry Bratt;Martin Graciarena;Horacio Franco;Christopher George;Allen R. Stauffer;Chris Bartels;Julien van Hout	2016			speech recognition;natural language processing;computer science;artificial intelligence	HCI	-15.512575300664743	-86.56299319665	120181
4a6872db7162d3f04a1e8a191548d088e0572f72	acoustic measures vs. phonetic features as predictors of audible discontinuity in concatenative speech synthesis	speech synthesis	Most concatenative speech synthesizers employ both acoustic measures and phonetic features to predict the perceptual damage caused by concatenating two waveform segments because no reliable acoustic measure has been found so far. This paper compares the predicting ability of the two kinds of predictor variables. We first conduct a perceptual experiment to measure the naturalness degradation due to signal discontinuity introduced by concatenating waveform segments. Secondly, we predict the score of naturalness degradation from acoustic measures derived from MFCC and/or phonetic features using statistical models such as a multiple regression model. Based on an investigation of the multiple regression coefficients, we found that (1) the phonetic features are more effective and that (2) the acoustic measures do not provide useful information in addition to the phonetic features.	acoustic cryptanalysis;coefficient;concatenation;elegant degradation;kerrison predictor;reflections of signals on conducting lines;speech synthesis;statistical model;waveform	Hisashi Kawai;Minoru Tsuzaki	2002			speech recognition;artificial intelligence;pattern recognition;discontinuity (linguistics);computer science;speech synthesis	ML	-10.042414608901414	-87.8424484770817	120318
d49053e0f3cc385dd6f44ea873151a920ba310d2	two sources of voicing neutralization in lithuanian		This study compares two processes that result in voicing neutralization in Lithuanian: regressive voicing assimilation in obstruent clusters and final devoicing of obstruents. Acoustic data is analyzed to assess the behaviour of three acoustic cues to obstruent voicing (i.e. closure and preceding vowel duration and voicing during closure) in both neutralizing environments. The results show that, although both processes result in incomplete voicing neutralization, they use the acoustic cues differently. This suggests that final devoicing and voicing assimilation have different acoustic realizations, supporting their analysis as two different processes.	acoustic cryptanalysis;data assimilation	Rebeka Campos-Astorkiza	2008			voice;obstruent;lithuanian;vowel;assimilation (phonology);speech recognition;engineering	NLP	-10.725453949677448	-82.7062727971162	120435
75df8d9b73f8f9df9fe1e229892f140fa79df1fa	unspeech: unsupervised speech context embeddings		We introduce ”Unspeech” embeddings, which are based on unsupervised learning of context feature representations for spoken language. The embeddings were trained on up to 9500 hours of crawled English speech data without transcriptions or speaker information, by using a straightforward learning objective based on context and non-context discrimination with negative sampling. We use a Siamese convolutional neural network architecture to train Unspeech embeddings and evaluate them on speaker comparison, utterance clustering and as a context feature in TDNN-HMM acoustic models trained on TEDLIUM, comparing it to i-vector baselines. Particularly decoding out-of-domain speech data from the recently released Common Voice corpus shows consistent WER reductions. We release our source code and pre-trained Unspeech models under a permissive open source license.	acoustic cryptanalysis;artificial neural network;cluster analysis;convolutional neural network;geforce 9 series;hidden markov model;network architecture;open-source license;open-source software;sampling (signal processing);time delay neural network;unsupervised learning;word error rate	Benjamin Milde;Christian Biemann	2018		10.21437/Interspeech.2018-2194	architecture;convolutional neural network;speech recognition;source code;cluster analysis;transcription (linguistics);artificial intelligence;unsupervised learning;spoken language;pattern recognition;utterance;computer science	NLP	-18.05089299341919	-87.2378384747932	120507
736347eb0a746798bb75a812f6e992d106f4de9c	front-end post-processing using histogram equalization combined with arma filtering for noise robust speech recognition	histograms;filtering;speech;cepstral analysis;speech recognition;robustness;noise	In this paper, we present a new feature set for robust speech recognition based on histogram equalization (HEQ) combined with auto regressive moving average (ARMA) filtering. Cepstral vectors extracted from the clean data, modified by Mean and Variance Normalization (MVN) have been used to generate a reference histogram for histogram equalization. The proposed post-processing module also consists of ARMA temporal filtering applied to normalized cepstral coefficients. HEQ compensates for nonlinear distortions caused by noise and ARMA filtering is used for smoothing the normalized feature vectors. The results on the AURORA2 task have shown noticeable improvements in the recognition of noisy speech. The proposed front-end achieved a relative error reduction of around 60% compared to the standard Mel-Cepstral front-end.	approximation error;coefficient;distortion;feature vector;histogram equalization;mel-frequency cepstrum;nonlinear system;smoothing;speech recognition;video post-processing	S. Saloomeh Shariati;Seyed Mohammad Ahadi;Karim Mohammadi	2007	2007 15th European Signal Processing Conference		speech recognition;computer science;histogram matching;machine learning;pattern recognition;adaptive histogram equalization	ML	-13.382832818839992	-92.12167767671474	120545
bbd3c16b859abce953448ff0ba561804c5a62ac0	a combined fbm and ppca based signal model for on-line recognition of pd signal	analisis componente principal;stochastic process;probabilistic principal component analysis;filtre reponse impulsion finie;gollete estrangulamiento;extraction forme;finite impulse response filter;random signal;espectro frecuencia;reconnaissance en ligne;time series;frequency spectrum;senal aleatoria;goulot etranglement;filtro respuesta impulsion acabada;senal debil;extraccion forma;bande frequence;frequency band;principal component analysis;fir filter;serie temporelle;partial discharge;long range dependent;analyse composante principale;serie temporal;small signal;parametric analysis;spectre frequence;rapport signal bruit;signal aleatoire;relacion senal ruido;signal to noise ratio;descarga parcial;bottleneck;banda frecuencia;pattern extraction;fractional brownian motion;signal faible;decharge partielle	The problem of on-line recognition and retrieval of relatively weak industrial signal such as Partial Discharges (PD), buried in excessive noise has been addressed in this paper. The major bottleneck being the recognition and suppression of stochastic pulsive interference (PI), due to, overlapping broad band frequency spectrum of PI and PD pulses. Therefore, on-line, on-site, PD measurement is hardly possible in conventional frequency based DSP techniques. We provide new methods to model and recognize the PD signal, on-line. The observed noisy PD signal is modeled as linear combination of systematic and random components employing probabilistic principal component analysis (PPCA). Being a natural signal, PD exhibits long-range dependencies. Therefore, we model the random part of the signal with fractional Brownian motion (fBm) process and pdf of the underlying stochastic process is obtained. The PD/PI pulses are assumed as the mean of the process and non-parametric analysis based on smooth FIR filter is undertaken. The method proposed by the Author found to be effective in recognizing and retrieving the PD pulses, automatically, without any user interference.		Pradeep Kumar Shetty	2005		10.1007/11590316_31	stochastic process;speech recognition;telecommunications;finite impulse response;mathematics;statistics	Robotics	-4.711215460012058	-92.94447640007695	120626
7f74fef553e559a51fc31fe6cfa5679778d7d014	f0 modeling in hmm-based speech synthesis system using deep belief network	speech synthesis;bengali;bengali f 0 modeling dbn speech synthesis;f 0 modeling;speech hidden markov models speech synthesis feature extraction training neural networks;statistical parametric speech synthesis hmm based speech synthesis system deep belief network deep neural network synthesized speech bengali language dbn dnn architectures clustering tree techniques;speech synthesis neural nets pattern clustering;dbn	In recent years multilayer perceptrons (MLPs) with many hidden layers Deep Neural Network (DNN) has performed surprisingly well in many speech tasks, i.e. speech recognition, speaker verification, speech synthesis etc. Although in the context of F0 modeling these techniques has not been exploited properly. In this paper, Deep Belief Network (DBN), a class of DNN family has been employed and applied to model the F0 contour of synthesized speech which was generated by HMM-based speech synthesis system. The experiment was done on Bengali language. Several DBN-DNN architectures ranging from four to seven hidden layers and up to 200 hidden units per hidden layer was presented and evaluated. The results were compared against clustering tree techniques popularly found in statistical parametric speech synthesis. We show that from textual inputs DBN-DNN learns a high level structure which in turn improves F0 contour in terms of objective and subjective tests.	artificial neural network;cluster analysis;deep belief network;deep learning;hidden markov model;high-level programming language;level structure;multilayer perceptron;randomness extractor;speaker recognition;speech recognition;speech synthesis;syllable	Sankar Mukherjee;Shyamal Kumar Das Mandal	2014	2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)	10.1109/ICSDA.2014.7051441	natural language processing;speech recognition;computer science;pattern recognition;speech synthesis;bengali	AI	-17.79455342815724	-87.7282063808621	120804
b8a51c3ac2d8c3991d083455d741039f3120578e	automatic speech recognition: an improved paradigm	unsupervised learning;autonomic system;language acquisition;automatic speech recognition;natural language processing;neural network	In this paper we present a short survey of automatic speech recognition systems underlining the current achievements and capabilities of current day solutions as well as their inherent limitations and shortcomings. In response to which we propose an improved paradigm and algorithm for building an automatic speech recognition system that actively adapts its recognition model in an unsupervised fashion by listening to continuous human speech. The paradigm relies on creating a semi-autonomous system that samples continuous human speech in order to record phonetic units. Then processes those phoneme sized samples to identify the degree of similarity of each sample that will allow the detection of the same phoneme across many samples. After a sufficiently large database of samples has been gathered the system clusters the samples based on their degree of similarity, creating a different cluster for each phoneme. After that the system trains one neural network for each cluster using the samples in that cluster. After a few iterations of sampling, processing, clustering and training the system should contain a neural network detector for each phoneme unit of the spoken language that the system has been exposed to, and be able to use these detectors to recognize phonemes from live speech. Finally we provide the structure and algorithms for this novel automatic speech recognition paradigm.	algorithm;artificial neural network;autonomous system (internet);cluster analysis;iteration;programming paradigm;sampling (signal processing);semiconductor industry;sensor;speech recognition;unsupervised learning	Tudor-Sabin Topoleanu;Gheorghe Mogan	2011		10.1007/978-3-642-19170-1_29	voice activity detection;language acquisition;natural language processing;unsupervised learning;audio mining;speech recognition;computer science;artificial intelligence;machine learning;speech processing;time delay neural network;acoustic model;speech analytics	ML	-17.43215318353284	-87.16687782900041	120833
1655ebc28fcb0b47a909678cc1bcc17894469ddd	conversational evaluation of artificial bandwidth extension of telephone speech using a mobile handset	telephone sets;conversational evaluation;bandwidth extension;speech enhancement;street noise simulation conversational evaluation mobile handset artificial bandwidth extension method narrowband telephone speech subjective listening only tests;indexes;radio frequency;abstracts;mobile handsets;telephone sets mobile handsets speech enhancement;abstracts indexes radio frequency;conversational evaluation speech enhancement bandwidth extension	Artificial bandwidth extension methods have been developed to improve the quality and intelligibility of narrowband telephone speech. Bandwidth extension methods have typically been evaluated with objective measures or subjective listening-only tests, whereas realistic conversational evaluations have been rare. This paper presents a conversational evaluation of two bandwidth extension methods together with narrowband and wideband speech. The evaluation was performed using a mobile handset with a wired earpiece and microphone both in silence and in simulated street noise. The results indicate that one of the evaluated bandwidth extension methods was significantly preferred over narrowband speech in silence. The results also suggest slight preference for this bandwidth extension method over narrowband speech in street noise. True wideband speech was considered superior to bandwidth-extended and narrowband speech especially in silence.	bandwidth extension;extension method;intelligibility (philosophy);microphone;mobile phone	Hannu Pulakka;Laura Laaksonen;Ville Myllylä;Santeri Yrttiaho;Paavo Alku	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288812	voice activity detection;database index;speech recognition;bandwidth extension;telecommunications;computer science;radio frequency	Robotics	-10.911654985324827	-84.69844914989442	121179
b14532ffef6828cbada33abc5e666dc07e768525	a comparison of linear prediction, fft, and zero-crossing analysis techniques for vowel recognition	working environment noise;speech analysis;filters;euclidean distance;linear predictive;fast fourier transform;automatic speech recognition;feature extraction;fast fourier transforms;speech recognition;frequency;algorithm design and analysis;speech recognition speech analysis fast fourier transforms automatic speech recognition feature extraction algorithm design and analysis frequency working environment noise filters euclidean distance	Two popular methods of feature extraction which have been applied to automatic speech recognition are I inear prediction and Fast Fourier Transform analysis. Recent work by the authors has indicated that zero—crossing analysis methods also have the potential to result in accurate speech recognition. In this paper two studies for determining the relative applicability of each of these three feature extraction methods for speech recognition are presented. One study is aimed at determining the relative discriminabi I ity of the methods for vowel recognition. The other study is aimed at determining the noise vulnerability of each method. Several Fast Fourier Transform and zero—crossing analysis algorithms perform well in the classifi— catior of vowels in a quiet environment. Exceptional classification results are obtained for several zero—crossing analysis algorithms applied to vowels in noise.	algorithm;fast fourier transform;feature extraction;speech recognition;vulnerability (computing);zero crossing	Patrick F. Castelaz;Russell J. Niederjohn	1978		10.1109/ICASSP.1978.1170489	fast fourier transform;speech recognition;feature;computer science;machine learning;pattern recognition;automatic target recognition	ML	-9.196830346753478	-91.2274370592271	121193
4751c576b0bba8832da870ebcee906fc596d759c	stream fusion for multi-stream automatic speech recognition	classifier ensemble creation and fusion;performance monitor;multi stream speech recognition	Multi-stream automatic speech recognition (MSASR) has been confirmed to boost the recognition performance in noisy conditions. In this system, the generation and the fusion of the streams are the essential parts and need to be designed in such a way to reduce the effect of noise on the final decision. This paper shows how to improve the performance of the MS-ASR by targeting two questions; (1) How many streams are to be combined, and (2) how to combine them. First, we propose a novel approach based on stream reliability to select the number of streams to be fused. Second, a fusion method based on Parallel Hidden Markov Models is introduced. Applying the method on two datasets (TIMIT and RATS) with different noises, we show an improvement of MS-ASR.	automated system recovery;bigram;hidden markov model;ms-dos;markov chain;run time (program lifecycle phase);speech recognition;statistical classification;timit	Hesam Sagha;Feipeng Li;Ehsan Variani;José del R. Millán;Ricardo Chavarriaga;Björn W. Schuller	2016	I. J. Speech Technology	10.1007/s10772-016-9357-1	speech recognition;computer science;machine learning;pattern recognition	NLP	-16.33148330238331	-88.92063626786518	121404
d8d469214ce6fc75aea01e8b87233d758d4924a5	a novel vector representation of stochastic signals based on adapted ergodic hmms	histograms;image recognition;distance metric learning;stochastic signal;hidden markov model;stochastic signals;speech;linear discriminate analysis;optimal distance metric learning;hidden markov models;image recognition adapted ergodic hmm hidden markov models stochastic signals pattern recognition vector representation optimal distance metric learning linear discriminant analysis;vectors;stochastic processes hidden markov models pattern recognition stochastic systems vectors speech signal processing image recognition histograms robustness;stochastic processes;image representation;signal processing;vector representation;pattern recognition;robustness;stochastic systems;stochastic systems hidden markov models image recognition image representation;linear discriminant analysis;vector representation distance metric learning hidden markov model pattern recognition stochastic signal;adapted ergodic hmm	In this letter, we propose a novel vector representation of stochastic signals for pattern recognition (PR) based on adapted ergodic hidden Markov models (HMMs). This vector representation is generic in nature and may be used with various types of stochastic signals (e.g., image, speech, etc.) and applied to a broad range of PR tasks (e.g., classification, regression, etc.). More importantly, by combining the vector representation with optimal distance metric learning (e.g., linear discriminant analysis) directly from the data, the performance of a PR system may be significantly improved. Our experiments on an image-based recognition task clearly demonstrate the effectiveness of the proposed vector representation of stochastic signals for potential use in many PR systems.	ergodic theory;ergodicity;experiment;hidden markov model;image;linear discriminant analysis;markov chain;pattern recognition;statistical classification	Hao Tang;Mark Hasegawa-Johnson;Thomas S. Huang	2010	IEEE Signal Processing Letters	10.1109/LSP.2010.2051945	speech recognition;computer science;speech;machine learning;pattern recognition;histogram;mathematics;hidden markov model;statistics;robustness	ML	-16.47633400888537	-92.92432561845054	121440
8455a3d519398a1a355b09db5453d3f82f143aa1	word spotting by csr through vector quantized background models			consistency model	Alessandro Falaschi;Alfredo Micozzi	1991			corporate social responsibility;artificial intelligence;speech recognition;pattern recognition;computer science;spotting	Vision	-13.450049192927926	-88.00527789674143	121547
520e4b21302b99e5f289075541f9fe4959a639a4	framewise approach in multimodal emotion recognition in omg challenge		In this report we described our approach achieves 53% of unweighted accuracy over 7 emotions and 0.05 and 0.09 mean squared errors for arousal and valence in OMG emotion recognition challenge. Our results were obtained with ensemble of single modality models trained on voice and face data from video separately. We consider each stream as a sequence of frames. Next we estimated features from frames and handle it with recurrent neural network. As audio frame we mean short 0.4 second spectrogram interval. For features estimation for face pictures we used own ResNet neural network pretrained on AffectNet database. Each short spectrogram was considered as a picture and processed by convolutional network too. As a base audio model we used ResNet pretrained in speaker recognition task. Predictions from both modalities were fused on decision level and improve single-channel approaches by a few percent.	artificial neural network;emotion recognition;frame (networking);modality (human–computer interaction);multimodal interaction;recurrent neural network;sms language;speaker recognition;spectrogram	Grigoriy Sterling;Andrey Belyaev;Maxim Ryabov	2018	CoRR		artificial intelligence;computer science;modalities;speech recognition;machine learning;artificial neural network;residual neural network;speaker recognition;spectrogram;emotion recognition;recurrent neural network	AI	-16.832684516598604	-87.3347185101426	121749
8dcdd0b95a26025f28dfa09314426c87fb6b0911	detection and compensation of undesirable discontinuities within the farsi/arabic subwords		In this paper, an unexplored subject in the domains of Farsi/Arabic handwritten word preprocessing is introduced. Subwords play a vital role in many applications such as cheque amount recognition, text recognition, lexicon reduction and subword-based word recognition. Correcting the faults occurred in subwords will improve the overall performance of these applications. A subword is a connected-component in the main body of a word. The occurrence of a discontinuity in a subword, divides the subword into two isolated parts. These parts are detected as two incorrect subwords. In our algorithm, before correcting these faults, the baseline of each subword is corrected using the proposed baseline correction method. Then, to limit the exploration area in matching stage, the dots are removed. Undesirable discontinuities in subwords are detected by using a template matching algorithm. Disconnected parts of a subword are joined together by using three different methods. Experiments show that the cubic polynomial-based compensation method causes the best results and 2.87 % improvement in the subword recognition rate.	algorithm;baseline (configuration management);cubic function;curve fitting;experiment;iranian.com;lexicon;optical character recognition;polynomial;preprocessor;reflections of signals on conducting lines;substring;template matching	Majid Ziaratban;Karim Faez	2011	Int. Arab J. Inf. Technol.		speech recognition	NLP	-14.694561813139023	-80.57913980541741	122065
138f20e76ef9ff4a8a0e8f3bd879ac1ff755c1d1	a comparative study of discriminative training using non-uniform criteria for cross-layer acoustic modeling	non uniform error cost;acoustics;training;cross layer acoustic modeling speech recognition discriminative training non uniform error cost;matrix algebra;training hidden markov models acoustics accuracy dynamic range speech recognition linear programming;accuracy;hidden markov models;dynamic range;linear programming;speech recognition;discriminative training;cross layer acoustic modeling;speech recognition nonuniform criteria comparative study cross layer acoustic modeling discriminative training frameworks minimum classification error like dt frameworks minimum phone error like dt frameworks cross layer confusion matrix large vocabulary task wsj0 nonuniform error cost embedded;speech recognition matrix algebra	This work focuses on a comparative study of discriminative training using non-uniform criteria for cross-layer acoustic modeling. Two kinds of discriminative training (DT) frameworks, minimum classification error like (MCE-like) and minimum phone error like (MPE-like) DT frameworks, are augmented to allow the error cost embedding at the phoneme (model) level respectively. To facilitate this comparative study, we implement both augmented DT frameworks under the same umbrella, using the error cost derived from the same cross-layer confusion matrix. Experiments on a large vocabulary task WSJ0 demonstrated the effectiveness of both DT frameworks with the formulated non-uniform error cost embedded. Several preliminary investigations on the effect of the dynamic range of error cost are also presented.	acoustic cryptanalysis;acoustic model;confusion matrix;discriminative model;dynamic range;embedded system;experiment;vocabulary	Chao Weng;Biing-Hwang Juang	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288817	dynamic range;speech recognition;computer science;linear programming;machine learning;pattern recognition;accuracy and precision	Robotics	-19.058939985378146	-91.29701993172218	122219
8a188579ea46745934cc75b28941deb5e528610e	on the relevance of high-level features for speaker independent emotion recognition of spontaneous speech	voice quality;emotion recognition;indexing terms;speaker independent;spontaneous speech	In this paper we study the relevance of so called high-level speech features for the application of speaker independent emotion recognition. After we give a brief definition of highlevel features, we discuss for which standard feature groups high-level features are conceivable. Two groups of high-level features are proposed within this paper: a feature set for the parametrization of phonation called voice quality parameters and a second feature set deduced from music theory called harmony features. Harmony features give information about the frequency interval and chord content of the pitch data of a spoken utterance. Finally, we study the gain in classification rate by combining the proposed high-level features with the standard low-level features. We show that both high-level feature sets improve the speaker independent classification performance for spontaneous emotional speech.	emotion recognition;high- and low-level;relevance;spontaneous order	Marko Lugger;Bin Yang	2009			speaker recognition;speech recognition;index term;phonation;feature;computer science;pattern recognition;linguistics;world wide web	ML	-12.300269230094644	-87.94167309543569	122229
37c3fed3e30a72b0f999dc94869f745c54226e62	shrinkage model adaptation in automatic speech recognition	word error rate;model adaptation;limit set;objective function;automatic speech recognition;maximum likelihood linear regression	We propose a parameter shrinkage adaptation framework to estimate models with only a limited set of adaptation data to improve accuracy for automatic speech recognition, by regularizing an objective function with a sum of parameterwise power q constraint. For the first attempt, we formulate ridge maximum likelihood linear regression (MLLR) and ridge constraint MLLR (CMLLR) with an element-wise square sum constraint to regularize the objective functions of the conventional MLLR and CMLLR, respectively. Tested on the 5k-WSJ0 task, the proposed ridge MLLR and ridge CMLLR algorithms give significant word error rate reduction from the errors obtained with standard MLLR and CMLLR in an utterance-by-utterance unsupervised adaptation scenario.	algorithm;loss function;optimization problem;speech recognition;word error rate	Jinyu Li;Yu Tsao;Chin-Hui Lee	2010			limit set;speech recognition;word error rate;computer science;machine learning;pattern recognition	AI	-18.396665691748453	-92.0894802493243	122293
13f373aefb0bf1a4b9c442f62fe4100090de0dbc	a statistical approach to mel-domain mask estimation for missing-feature asr	temporal correlation;statistical approach;front end;reliability;degree of freedom;time frequency;posterior probability;estimation algorithm;noise robustness;spectral subtraction;automatic speech recognition;statistical distributions;hidden markov models;random variable;speech recognition;spectral analysis	In this letter, we present a statistical approach to Mel-domain mask estimation for missing feature (MF)-based automatic speech recognition (ASR). Mel-domain time-frequency masks are of interest, since MF systems have been shown successful in that domain. Time- and channel-specific reliability measures are derived as posterior probabilities of active speech using a 2-state speech model. Since closed form distributions for Mel-domain spectra do not exist, they are instead modeled as χ2 processes with empirically-determined degrees of freedom. Additionally, we present HMM-based decoding to exploit temporal correlation of spectral speech data. The proposed mask estimation algorithm is integrated with an example MF-based ASR front-end from, and is shown to outperform the spectral subtraction (SS)-based method from in terms of word-accuracy, when applied to the Aurora-2 database.	algorithm;automated system recovery;baseline (configuration management);compressed sensing;hidden markov model;internationalized domain name;speech recognition	Bengt J. Borgstrom;Abeer Alwan	2010	IEEE Signal Processing Letters	10.1109/LSP.2010.2076348	probability distribution;random variable;speech recognition;time–frequency analysis;computer science;front and back ends;pattern recognition;reliability;mathematics;posterior probability;degrees of freedom;hidden markov model;statistics	Vision	-14.018036749823642	-93.20907603532962	122431
facbd0c4d23a9195eb2b8625e6a149b50b536346	a combined features approach for speaker segmentation using bic and artificial neural networks	artificial neural networks bayesian information criterion speaker segmentation combined features;feature extraction acoustics artificial neural networks speech training detection algorithms speech processing;speaker recognition acoustic signal processing audio signal processing bayes methods feature extraction learning artificial intelligence neural nets;audio signal processing;neural nets;bayes methods;acoustic signal processing;combined feature approach far false alarm rate mdr miss detection rate acoustic feature sets window growing based approach change point estimation δbic time index ann audio segment merging distance measure bayesian information criterion audio stream acoustic feature extraction speaker segmentation task artificial neural networks;speaker recognition;feature extraction;learning artificial intelligence	We present a combined features approach for speaker segmentation task. This approach utilizes different acoustic features extracted from audio stream. The Bayesian Information Criterion (BIC) is used for each acoustic feature as a distance measure to verify the merging of two audio segments. An Artificial Neural Network (ANN) combines the time index from each ?BIC with the highest value, and estimates the change point. In the experiments, a data set containing examples with several speakers is used to compare our approach with the Chen and Gopalakrishnan's window-growing-based approach, using different acoustic features sets. The results show an improvement in both the Miss Detection Rate (MDR) and the False Alarm Rate (FAR) compared to the window-growing-based approach.	acoustic cryptanalysis;acoustic fingerprint;algorithm;artificial neural network;bayesian information criterion;entity–relationship model;experiment;memory data register;neural networks;pc speaker;streaming media;synthetic data	Leonardo Valeriano Neri;Ing Ren Tsang;George D. C. Cavalcanti;Ing Jyh Tsang;Jan Sijbers	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.739	speaker recognition;speaker diarisation;speech recognition;audio signal processing;feature extraction;computer science;machine learning;pattern recognition;time delay neural network;artificial neural network	Robotics	-15.526770391518463	-90.30563608616035	122612
5481060a651d985ace068aa08bf4f6340bd731d7	experiments in continuous speech recognition with a 60, 000 word vocabulary			speech recognition;vocabulary	Patrick Kenny;Rene Hollan;Gilles Boulianne;Harinath Garudadri;Yan Ming Cheng;Matthew Lennig;Douglas D. O'Shaughnessy	1992			audio mining;speech recognition;speaker recognition;word error rate;speech corpus;acoustic model;computer science;logogen model;speech synthesis;speech production	ML	-15.41964283625008	-86.15882362865477	122614
23802c22911a4633f9cd8c7181e7f459d64d899b	where to associate stressed additive particles? evidence from speech prosody		Theoretical approaches mostly associate stressed additive particles (e.g., auch in German) with contrastive topics. Empirical data show that associated constituents are produced more prominently than unassociated ones but not that they are contrastive topics. This paper compares the prosodic realizations of associated constituents with contrastive and non-contrastive topics. We found no differences in accent types but later alignment for contrastive than non-contrastive topics; associated constituents lie in-between. An unrestricted sentence completion task tested whether listeners produce more additive particles upon hearing fragments with contrastive compared to noncontrastive topics. Completions containing additive particles were generally very infrequent (< 4%), but crucially more frequent in sentences with a contrastive topic compared to a noncontrastive topic. Stressed additive particles seem to associate with prominent accents, which may often be contrastive topics.	semantic prosody;utility functions on indivisible goods	Bettina Braun	2012				NLP	-11.187102870906608	-82.67404604621026	122751
5d30b956bd03bdc4029a5770e9247f47cf0a1e33	recent progress in prosodic speaker verification	analytical models;detection cost function;nist;probability;plda;support vector machines;acoustics;ivector;linear discriminate analysis;msm;speaker verification;speaker recognition;plda prosodic speaker verification snerfs msm ivector;feature extraction;equal error rate;prosodic speaker verification;nist prosodic speaker verification syllable based prosodic features modeling multinomial subspace model feature extraction within class covariance normalization probabilistic linear discriminant analysis plda session variability compensation speaker recognition acoustic baseline system detection cost function dcf;support vector machine;probabilistic logic;snerfs;speaker recognition feature extraction probability;analytical model;nist speaker recognition analytical models feature extraction acoustics support vector machines probabilistic logic	We describe recent progress in the field of prosodic modeling for speaker verification. In a previous paper, we proposed a technique for modeling syllable-based prosodic features that uses a multinomial subspace model for feature extraction and within-class covariance normalization or linear discriminant analysis for session variability compensation. In this paper, we show that performance can be significantly improved with the use of probabilistic linear discriminant analysis (PLDA) for session variability compensation. This system does not require score normalization. We report an equal error rate below 7% on a NIST 2008 task. To our knowledge, this is the best reported result to date for a prosodic system for speaker recognition. Fusion of this system with a state-of-the-art acoustic baseline system yields 10% relative improvement in the new detection cost function (DCF) as defined by NIST.	acoustic cryptanalysis;baseline (configuration management);database normalization;design rule for camera file system;feature extraction;heart rate variability;linear discriminant analysis;loss function;multinomial logistic regression;spatial variability;speaker recognition;syllable	Marcel Kockmann;Luciana Ferrer;Lukás Burget;Elizabeth Shriberg;Jan Cernocký	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947368	natural language processing;speaker recognition;support vector machine;speech recognition;computer science;machine learning;pattern recognition	Vision	-16.417223999234203	-91.28211968606338	122865
ea72e2d05f50b94b37feaa22096ba80fe83435df	evaluation of time compression for connected word recognition	rhythm;signal sampling;speech processing;testing;nonuniform sampling;nonuniform sampling testing speech recognition frequency noise reduction signal sampling rhythm speech processing signal processing spectral analysis;signal processing;noise reduction;linear time;time compression;word recognition;speech recognition;spectral analysis;frequency	Recently, several studies have shown the interesting aspects of nonuniform sampling of the filtered speech signal in the context of an isolated word recognizer. This paper investigates the effect of nonuniform sampling for connected word recognition. Three nonlinear time compression techniques are evaluated, one that brings all reference utterances down to one same length, and others, for which the utterance length is variable. The nonuniform sampling approach is compared to the uniform one by opposing the three non-linear methods to two linear time compression ones. The results show that the variable length trace segmentation technique gives the best scores under all conditions, and that the uniform sampling approach can therefore be advantageously used in connected word recognition processes.		Jean-Luc Gauvain;Joseph-Jean Mariani	1984		10.1109/ICASSP.1984.1172755	time complexity;nonuniform sampling;speech recognition;word recognition;computer science;rhythm;frequency;signal processing;noise reduction;speech processing;software testing	Vision	-11.142889938774347	-87.74220155816762	122987
91fb6d80672f1e99a82cf127e9c1b94965556525	supervised domain adaptation for emotion recognition from speech	databases;speech adaptation models support vector machines databases emotion recognition speech recognition training;support vector machines;training;speech;emotion recognition;supervised domain adaptation emotion recognition;speech recognition;speaker recognition emotion recognition pattern classification;adaptation models;multicorpus framework supervised domain adaptation speech emotion recognition system emotion classifier generalization mismatched training condition mismatched testing condition speaker diversity;supervised domain adaptation	One of the main barriers in the deployment of speech emotion recognition systems in real applications is the lack of generalization of the emotion classifiers. The recognition performance achieved in controlled recordings drops when the models are tested with different speakers, channels, environments and domain conditions. This paper explores supervised model adaptation, which can improve the performance of systems evaluated with mismatched training and testing conditions. We address the following key questions in the context of supervised adaptation for speech emotion recognition: (a) how much labeled data is needed for adaptation to achieve good performance? (b) how important is speaker diversity in the labeled set? (c) can spontaneous acted data provide similar performance than naturalistic non-acted recordings? and (d) what is the best approach to adapt the models (domain adaptation versus incremental/online training)? We address these problems by using a multi-corpus framework where the models are trained and tested with different databases. The results indicate that even small portion of data used for adaptation can significantly improve the performance. Increasing the speaker diversity in the labeled data used for adaptation does not provide significant gain in performance. Also, we observe similar performance when the classifiers are trained with naturalistic non-acted data and spontaneous acted data.	database;domain adaptation;emoticon;emotion recognition;software deployment;spontaneous order;supervised learning	Mohammed Abdel-Wahab;Carlos Busso	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178934	speaker recognition;support vector machine;speech recognition;computer science;speech;pattern recognition	Robotics	-16.874717747427137	-89.96812517365736	123209
3d6099ec78e709f2283f43f2bb870d9bfb80718b	speech enhancement using a wavelet thresholding method based on symmetric kullback-leibler divergence	wavelet thresholding;kullback leibler divergence;speech enhancement;probability distribution	Performance of wavelet thresholding methods for speech enhancement strongly depends on estimating an exact threshold value in the wavelet sub-bands. In this paper, we propose a new method for more exact estimation of the threshold value. Our proposed threshold value is firstly obtained based on the symmetric Kullback–Leibler divergence between the probability distributions of noisy speech and noise wavelet coefficients. In the next step, we improved this value using the segmental Signal-to-Noise Ratio (SNR). We used some TIMIT utterances to assess the performance of the proposed threshold. The algorithm is evaluated using the Perceptual Evaluation of Speech Quality (PESQ) score and the SNR improvement in ideal and real modes. In ideal and real modes, on average, we obtain respectively 2.25 dB and 1 dB SNR improvement and a PESQ score increase up to 1.1, 0.75 compared with the conventional wavelet thresholding approaches. In comparison to the adaptive thresholding approach, on average in ideal and real modes, we obtain respectively 1.6 dB and 0.9 dB SNR improvement. The PESQ value of the adaptive thresholding method, in the real and ideal modes, is 0.25 higher and 0.5 lower than that of our proposed method, respectively. & 2014 Elsevier B.V. All rights reserved.	algorithm;coefficient;decibel;kullback–leibler divergence;pesq;signal-to-noise ratio;speech enhancement;timit;wavelet	Shima Tabibian;Ahmad Akbari;Babak Nasersharif	2015	Signal Processing	10.1016/j.sigpro.2014.06.027	probability distribution;speech recognition;pattern recognition;mathematics;kullback–leibler divergence;statistics	AI	-13.138435832394167	-92.94199217423456	123353
90f060e3060710e86816fe13b3a83ca61bbca433	clustering speech utterances by speaker using eigenvoice-motivated vector space models	eigenvalues and eigenfunctions;speech loudspeakers acoustic measurements feedback indexing chaotic communication information science character recognition man machine systems biometrics;vector space model;automatic group;acoustic signal processing;acoustic signal processing speaker recognition relevance feedback eigenvalues and eigenfunctions vectors;automatic speaker recognition speech utterance clustering eigenvoice motivated vector space models associated speakers document retrieval research acoustic terms voice characteristics speaker clustering blind relevance feedback inter utterance similarity measure;speaker recognition;vectors;document retrieval;similarity measure;relevance feedback;eigenvectors	The paper investigates the problem of automatically grouping unknown speech utterances based on their associated speakers. The proposed method utilizes the vector space model, which was originally developed in document-retrieval research, to characterize each utterance as a tf-idf-based vector of acoustic terms, thereby deriving a reliable measurement of similarity between utterances. To define the required acoustic terms that are most representative in terms of voice characteristics, the Eigenvoice approach is applied to the utterances to be clustered, which creates a set of eigenvector-based terms. To further improve speaker-clustering performance, the proposed method encompasses a mechanism of blind relevance feedback for refining the inter-utterance similarity measure.	acoustic cryptanalysis;cluster analysis;relevance feedback;similarity measure;tf–idf	Wei-Ho Tsai;Shih-Sian Cheng;Yi-Hsiang Chao;Hsin-Min Wang	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415216	natural language processing;document retrieval;speaker recognition;speaker diarisation;speech recognition;eigenvalues and eigenvectors;computer science;pattern recognition;vector space model	Visualization	-10.136613917018467	-91.56153862303708	123408
4ce551971e296ff38f756cd1c685ca13acb61c13	evaluation of spoken language understanding by oxygenated hemoglobin concentration	oxygenated hemoglobin;optical topography;near infrared spectroscopy;subjective understanding;speech language understanding			Akio Nozawa;Tota Mizuno;Hirotoshi Asano;Hideto Ide	2010	JRM	10.20965/jrm.2010.p0003	computer vision;speech recognition;chemistry;communication	NLP	-7.8280440716144195	-83.56300282206142	123497
08bd3ee0d1d8ae0cce9f321eedba4d73c5cf0c62	neuro-classification of currency fatigue levels based on acoustic cepstrum patterns	fatigue level classification;currency classification;lvq;acoustic cepstrum patterns		acoustic cryptanalysis;cepstrum	Masaru Teranishi;Sigeru Omatu;Toshihisa Kosaka	2000	JACIII	10.20965/jaciii.2000.p0018	speech recognition;learning vector quantization;computer science;machine learning;pattern recognition	HCI	-14.00605572586344	-87.5286532167528	123605
c638784d6495dced1110a4d9189fc3251bc56935	length contrast and covarying features: whistled speech as a case study		The status of covarying features to sound contrasts is a longstanding issue in speech: are they deliberately controlled by the speakers, or are they contingent automatic effects required by the defining features? We address this question by drawing parallels between the way gemination is implemented in spoken language and the way it is rendered in whistled speech. Audio materials were collected with five Berber whistlers in Morocco. The spoken and whistled data were composed of pairs of words contrasting singletons to geminates in different word positions. Compared to spoken forms, whistling, while adapting to the specific constraints imposed by the medium, transposes the basic strategies used in normal speech. As in normal speech, the primary and most salient acoustic attribute differentiating whistled singletons and geminates is closure duration. But duration is not used alone. Covarying secondary attributes are conveyed which may serve to enhance the primary correlate by contributing additional properties increasing the distance between the two lexical categories. These enhancing correlates may take on distinctive function in cases where the primary correlate is not implemented. This is, for instance, the case of higher frequency values in word-initial position where duration differences cannot be acoustically implemented using whistled speech.	acoustic cryptanalysis;contingency (philosophy);parallels desktop for mac;speech synthesis	Rachid Ridouane;Giuseppina Turco;Julien Meyer	2018		10.21437/Interspeech.2018-1060	salient;part of speech;speech recognition;contrast (statistics);normal speech;parallels;gemination;spoken language;computer science	NLP	-10.23698821788176	-81.20170690053948	123753
dc291c80c63572d983a1eca2855403401942f27f	spectra transformed for model-testing and visual exploration	regression analysis pattern recognition;polyphonic music signal;spectra;visual exploration;interactive system;data visualization testing frequency animation spirals image segmentation educational institutions performance evaluation music information retrieval multiple signal classification;pattern recognition;model testing;i 5 5 pattern recognition implementation interactive systems;regression analysis;model test;dynamic regression plotting;polyphonic music signal spectra model testing visual exploration dynamic regression plotting	The presence of highly tangled patterns in spectra and other serial data exacerbates the difficulty of performing visual comparison between a test model for a particular pattern and the data. The use of a simple map that plants peaks in the data directly onto their corresponding position in a residual plot with respect to a chosen test model not only retrieves the advantages of dynamic regression plotting, but in practical cases also causes patterns in the data to congregate in meaningful ways with respect to more than one reference curve in the plane. The technique is demonstrated on a polyphonic music signal.	rca spectra 70;visual comparison	Palmyra Catravas	2007	2007 IEEE Symposium on Visual Analytics Science and Technology	10.1109/VAST.2007.4389024	computer vision;speech recognition;computer science;data mining;regression analysis;computer graphics (images)	Visualization	-7.092359030265214	-92.3172241896323	123817
c5594db022ef0cc3bc6d8f9123bec3ada285f990	analysis of language dependent front-end for speaker recognition		In Deep Neural Network (DNN) i-vector based speaker recognition systems, acoustic models trained for Automatic Speech Recognition are employed to estimate sufficient statistics for i-vector modeling. The DNN based acoustic model is typically trained on a wellresourced language like English. In evaluation conditions where enrollment and test data are not in English, as in the NIST SRE 2016 dataset, a DNN acoustic model generalizes poorly. In such conditions, a conventional Universal Background Model/Gaussian Mixture Model (UBM/GMM) based i-vector extractor performs better than the DNN based i-vector system. In this paper, we address the scenario in which one can develop a Automatic Speech Recognizer with limited resources for a language present in the evaluation condition, thus enabling the use of a DNN acoustic model instead of UBM/GMM. Experiments are performed on the Tagalog subset of the NIST SRE 2016 dataset assuming an open training condition. With a DNN i-vector system trained for Tagalog, a relative improvement of 12.1% is obtained over a baseline system trained for English.	acoustic cryptanalysis;acoustic model;baseline (configuration management);deep learning;extractor (mathematics);google map maker;mixture model;speaker recognition;speech recognition;test data	Srikanth R. Madikeri;Subhadeep Dey;Petr Motlícek	2018		10.21437/Interspeech.2018-2071	speech recognition;speaker recognition;artificial intelligence;front and back ends;pattern recognition;computer science	NLP	-17.6307318328855	-89.62981610370669	123922
4a1361ea4b46241a78559a4cda9228c922eed2ad	pattern recognition of non-speech audio	non speech audio;pattern recognition		pattern recognition	Jean-Julien Aucouturier;Laurent Daudet	2010	Pattern Recognition Letters	10.1016/j.patrec.2010.05.003	speaker recognition;audio mining;speech recognition;computer science;acoustic model	Vision	-13.993571649761432	-87.44547529495318	123988
a74f734ad1dcb545468d9d5903d1686b62ccd3b1	a comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge		The success of supervised deep neural networks (DNNs) in speech recognition cannot be transferred to zero-resource languages where the requisite transcriptions are unavailable. We investigate unsupervised neural network based methods for learning frame-level representations. Good frame representations eliminate differences in accent, gender, channel characteristics, and other factors to model subword units for withinand acrossspeaker phonetic discrimination. We enhance the correspondence autoencoder (cAE) and show that it can transform Mel Frequency Cepstral Coefficients (MFCCs) into more effective frame representations given a set of matched word pairs from an unsupervised term discovery (UTD) system. The cAE combines the feature extraction power of autoencoders with the weak supervision signal from UTD pairs to better approximate the extrinsic task’s objective during training. We use the Zero Resource Speech Challenge’s minimal triphone pair ABX discrimination task to evaluate our methods. Optimizing a cAE architecture on English and applying it to a zero-resource language, Xitsonga, we obtain a relative error rate reduction of 35% compared to the original MFCCs. We also show that Xitsonga frame representations extracted from the bottleneck layer of a supervised DNN trained on English can be further enhanced by the cAE, yielding a relative error rate reduction of 39%.	approximation algorithm;approximation error;artificial neural network;autoencoder;coefficient;deep learning;feature extraction;feature learning;machine learning;mel-frequency cepstrum;optimizing compiler;speech recognition;substring;triphone;umbrella term;uniform theory of diffraction	Daniel Renshaw;Herman Kamper;Aren Jansen;Sharon Goldwater	2015			natural language processing;unsupervised learning;computer science;machine learning;pattern recognition;time delay neural network;competitive learning	AI	-17.277412147950063	-88.51976753966407	124275
d159df63baccf4061d7ac94eb41b5a4cb742fc7e	an auditory display to convey urgency information in industrial control rooms		Auditory warning signals are common features in industrial control rooms. Finding sound signals that convey higher degrees of urgency while keeping the potential for annoyance low is challenging. In the present study, evaluations were performed on four different types of auditory displays. The displays were all designed to convey three levels of urgency. The examination focused on the following questions: (1) “How reliably can the operators identify the three levels of urgency?” and (2) “How annoying do the operators find the sound signals?”. Fourteen operators participated in the study. For every signal within each auditory display, the participants were asked to rate the level of urgency and annoyance. The results show that one can design auditory displays that employ appropriate urgency mapping while the perceived annoyance is kept at a low level. The work also suggests that involving the end users in the design process could be advantageous.	auditory display	Anna Sirkka;Johan Fagerlönn;Stefan Lindberg;Ronja Frimalm	2014		10.1007/978-3-319-07515-0_53	speech recognition;acoustics;engineering;communication	Robotics	-6.277117240467876	-82.50238697543143	124948
8ed3f519d7efaea73a58f083964196c29d41994a	effective speaker verification via dynamic mismatch compensation	test data degradation speaker verification dynamic mismatch compensation condition adjusted t norm mismatched noise conditions ct norm method mismatched data conditions data mismatch reduction multisignal to noise ratio universal background models snr ubm real world noise;real world noise;ct norm method;test data degradation;data mismatch reduction;snr ubm;speaker verification;speaker recognition;gmm ubm;multisignal to noise ratio universal background models;multi snr gmm;signal processing;test normalization;condition adjusted t norm;speaker recognition signal processing;mismatched data conditions;article;dynamic mismatch compensation;mismatched noise conditions	This paper presents a new approach to condition-adjusted T-norm (CT-Norm) for speaker verification under significant mismatched noise conditions. The study is motivated by the fact that, though the standard CT-Norm method offers enhanced accuracy under mismatched data conditions, its effectiveness reduces with the increased severity of such conditions. The proposed approach attempts to address this challenge by providing a more effective reduction of data mismatch through the incorporation of multi-signal-to-noise ratio (SNR) universal background models (UBMs). The effectiveness of the proposed approach is demonstrated through experiments based on examples of real-world noise. It is shown that the superiority of the approach over CT-Norm is particularly significant for such excessive levels of test data degradation considered in the study as 5 dB SNR and below. The paper provides a description of the characteristics of the proposed approach and details the experimental analysis of its effectiveness under different noise conditions.	ct scan;data degradation;elegant degradation;experiment;signal-to-noise ratio;speaker recognition;t-norm;test data	Surosh G. Pillay;Aladdin M. Ariyaeeinia;Perasiriyan Sivakumaran;M. Pawlewski	2012	IET Biometrics	10.1049/iet-bmt.2012.0001	speaker recognition;speech recognition;computer science;signal processing	AI	-13.984918406560412	-92.07292467949223	125024
17721c8b9d1a0c070ef268f3c6aae4a733e8beb2	a style control technique for hmm-based speech synthesis		This paper describes an approach to controlling style of synthetic speech in HMM-based speech synthesis. The style is defined as one of speaking styles and emotional expressions in speech. We model each speech synthesis unit by using a context-dependent HMM whose mean vector of the output distribution function is given by a function of a parameter vector called style control vector. We assume that the mean vector is modeled by multiple regression with the style control vector. The multiple regression matrices are estimated by EMalgorithm as well as other model parameters of HMMs. In the synthesis stage, the mean vectors are modified by transforming an arbitrarily given control vector which is associated with a desired style. The results of subjective tests show that we can control styles by choosing the style control vector appropriately.	context-sensitive language;hidden markov model;norm (social);speech synthesis;synthetic intelligence	Takashi Masuko;Takao Kobayashi;Keisuke Miyanaga	2004			control vector;speech recognition;artificial intelligence;pattern recognition;hidden markov model;matrix (mathematics);linear regression;emotional expression;computer science;speech synthesis	ML	-16.807107236387598	-84.57931899982297	125033
71473db1a8b4ae396be5eb7a6192c7cc5a69fe4f	automatic speech recognition and topic identification from speech for almost-zero-resource languages			speech recognition	Matthew Wiesner;Chunxi Liu;Lucas Ondel;Craig Harman;Vimal Manohar;Jan Trmal;Zhongqiang Huang;Najim Dehak;Sanjeev Khudanpur	2018		10.21437/Interspeech.2018-1836	speech recognition;computer science	NLP	-15.401710932798661	-86.06528331408263	125081
8e46a2e57ce37b846bef48d776aeafa16c411681	speaker adaptation of neural network acoustic models using i-vectors	neural nets;hessian free sequence training speaker adaptation i vectors deep neural network acoustic models dnn acoustic features asr switchboard 300 hours corpus speaker independent features word error rate wer speaker adapted features vtln fmllr;speech recognition learning artificial intelligence neural nets;training feature extraction neural networks acoustics hidden markov models vectors training data;speech recognition;learning artificial intelligence	We propose to adapt deep neural network (DNN) acoustic models to a target speaker by supplying speaker identity vectors (i-vectors) as input features to the network in parallel with the regular acoustic features for ASR. For both training and test, the i-vector for a given speaker is concatenated to every frame belonging to that speaker and changes across different speakers. Experimental results on a Switchboard 300 hours corpus show that DNNs trained on speaker independent features and i-vectors achieve a 10% relative improvement in word error rate (WER) over networks trained on speaker independent features only. These networks are comparable in performance to DNNs trained on speaker-adapted features (with VTLN and FMLLR) with the advantage that only one decoding pass is needed. Furthermore, networks trained on speaker-adapted features and i-vectors achieve a 5-6% relative improvement in WER after hessian-free sequence training over networks trained on speaker-adapted features only.	acoustic cryptanalysis;artificial neural network;concatenation;deep learning;fmllr;google map maker;hessian;telephone switchboard;test data;word error rate	George Saon;Hagen Soltau;David Nahamoo;Michael Picheny	2013	2013 IEEE Workshop on Automatic Speech Recognition and Understanding	10.1109/ASRU.2013.6707705	speaker recognition;speaker diarisation;speech recognition;computer science;machine learning;pattern recognition;artificial neural network	NLP	-17.690821983461912	-89.09123281898884	125162
5805a7ebfab3daf215df2c6f8c0eeeca2be642a1	an unsupervised deep domain adaptation approach for robust speech recognition		This paper addresses the robust speech recognition problem as a domain adaptation task. Specifically, we introduce an unsupervised deep domain adaptation (DDA) approach to acoustic modeling in order to eliminate the training–testing mismatch that is common in real-world use of speech recognition. Under a multi-task learning framework, the approach jointly learns two discriminative classifiers using one deep neural network (DNN). As the main task, a label predictor predicts phoneme labels and is used during training and at test time. As the second task, a domain classifier discriminates between the source and the target domains during training. The network is optimized by minimizing the loss of the label classifier and to maximize the loss of the domain classifier at the same time. The proposed approach is easy to implement by modifying a common feed-forward network. Moreover, this unsupervised approach only needs labeled training data from the source domain and some unlabeled raw data of the new domain. Speech recognition experiments on noise/channel distortion and domain shift confirm the effectiveness of the proposed approach. For instance, on the Aurora-4 corpus, compared with the acoustic model trained only using clean data, the DDA approach achieves relative 37.8% word error rate (WER) reduction. © 2017 Elsevier B.V. All rights reserved.	acoustic cryptanalysis;acoustic model;artificial neural network;computer multitasking;deep learning;discriminative model;distortion;domain adaptation;experiment;feedforward neural network;kerrison predictor;multi-task learning;speech recognition;unsupervised learning;word error rate	Sining Sun;Lei Xie;Yanning Zhang	2017	Neurocomputing	10.1016/j.neucom.2016.11.063	artificial intelligence;raw data;discriminative model;machine learning;artificial neural network;deep learning;distortion;word error rate;acoustic model;pattern recognition;speech recognition;computer science;communication channel	AI	-16.009116868953583	-90.13951736678726	125195
4ab6d8e8c37ebc00482b88cd68859d2d9f354c6a	improvements in connected digit recognition using linear discriminant analysis and mixture densities	nist;mixture densities;spectrum normalization;vocabulary;hmm;string error rate energy thresholding spectrum normalization hmm connected digit recognition linear discriminant analysis mixture densities error rate hidden markov model based speech recognizer spurious distortion accumulation random noise acoustic resolution feature selection class independent transformation matrix;acoustic resolution;spectrum;linear discriminate analysis;error analysis;random noise;hidden markov model based speech recognizer;hidden markov models;feature extraction;acoustic noise;error compensation;error rate;class independent transformation matrix;speech recognition;spurious distortion accumulation;connected digit recognition;string error rate;feature selection;energy thresholding;acoustic emission;speech recognition error compensation feature extraction hidden markov models random noise;frequency;continuous density hidden markov model;linear discriminant analysis;linear discriminant analysis error analysis hidden markov models speech recognition nist acoustic emission frequency acoustic noise vocabulary laboratories	Four methods were used to reduce the error rate of a continuous-density hidden Markov-model-based speech recognizer on the TI/NIST connected-digits recognition task. Energy thresholding sets a lower limit on the energy in each frequency channel to suppress spurious distortion accumulation caused by random noise. This led to an improvement in error rate by 15%. Spectrum normalization was used to compensate for across-speaker variations, resulting in an additional improvement by 20%. The acoustic resolution was increased up to 32 component densities per mixture. Each doubling of the number of component densities yielded a reduction in error rate by roughly 20%. Linear discriminant analysis was used for improved feature selection. A single class-independent transformation matrix was applied to a large input vector consisting of several adjacent frames, resulting in an improvement by 20% for high acoustic resolution. The final string error rate was 0.84%. >		Reinhold Häb-Umbach;Dieter Geller;Hermann Ney	1993		10.1109/ICASSP.1993.319279	spectrum;speech recognition;nist;feature extraction;word error rate;computer science;acoustic emission;machine learning;frequency;noise;pattern recognition;mathematics;hidden markov model	Vision	-14.567946742202473	-92.16522488713433	125204
2f97ee95cad6a1f13596b108072b846c6f747d4e	the microsoft 2016 conversational speech recognition system		We describe Microsoft's conversational speech recognition system, in which we combine recent developments in neural-network-based acoustic and language modeling to advance the state of the art on the Switchboard recognition task. Inspired by machine learning ensemble techniques, the system uses a range of convolutional and recurrent neural networks. I-vector modeling and lattice-free MMI training provide significant gains for all acoustic model architectures. Language model rescoring with multiple forward and backward running RNNLMs, and word posterior-based system combination provide a 20% boost. The best single system uses a ResNet architecture acoustic model with RNNLM rescoring, and achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The combined system has an error rate of 6.2%, representing an improvement over previously reported results on this benchmark task.	acoustic cryptanalysis;acoustic model;artificial neural network;benchmark (computing);bit error rate;convolutional neural network;ensemble learning;language model;machine learning;recurrent neural network;speech recognition;telephone switchboard;word error rate	Wayne Xiong;Jasha Droppo;Xuedong Huang;Frank Seide;Mike Seltzer;Andreas Stolcke;Dong Yu;Geoffrey Zweig	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953159	natural language processing;speech recognition;computer science;artificial intelligence;machine learning	Robotics	-17.38285586977166	-88.09462778511578	125220
ebe4caa7eef1fd59f1883b31d5f2a41ae6b6e501	music genre prediction by low-level and high-level characteristics		For music genre prediction typically low-level audio signal features from time, spectral or cepstral domains are taken into account. Another way is to use community-based statistics such as Last.FM tags. Whereas the first feature group often can not be clearly interpreted by listeners, the second one lacks in erroneous or not available data for less popular songs. We propose a two-level approach combining the specific advantages of the both groups: at first we create high-level descriptors which describe instrumental and harmonic characteristics of music content, some of them derived from low-level features by supervised classification or from analysis of extended chroma and chord features. The experiments show that each categorization task requires its own feature set.		Igor Vatolkin;Günther Rötter;Claus Weihs	2012		10.1007/978-3-319-01595-8_46	random forest;support vector machine;chord (music);categorization;cepstrum;audio signal;popular music;computer science;harmonic;artificial intelligence;pattern recognition	Arch	-11.619596536214644	-88.56481275835823	125226
d1206390a354c20850cf21c47fc02813cc80f09b	dynamic gesture recognition using echo state networks		In the last decade, training recurrent neural networks (RNN) using techniques from the area of reservoir computing (RC) became more attractive for learning sequential data due to the ease of network training. Although successfully applied in the language and speech domains, only little is known about using RC techniques for dynamic gesture recognition. We therefore conducted experiments on command gestures using Echo State Networks (ESN) to investigate both the effect of different gesture sequence representations and different parameter configurations. For recognition we employed the ensemble technique, i.e. using ESN as weak classifiers. Our results show that using ESN is a promising approach for dynamic gesture recognition and we give indications for future experiments.		Doreen Jirak;Pablo V. A. Barros;Stefan Wermter	2015			machine learning;gesture recognition;pattern recognition;artificial intelligence;reservoir computing;recurrent neural network;computer science;gesture	AI	-17.066101477601705	-87.61331618870106	125428
6b654915f3493786be1b69d917864f538a8232f9	hmm-based automatic eye-blink synthesis from speech		In this paper we present a novel technique to automatically synthesise eye blinking from a speech signal. Animating the eyes of a talking head is important as they are a major focus of attention during interaction. The developed system predicts eye blinks from the speech signal and generates animation trajectories automatically employing a ”Trajectory Hidden Markov Model”. The evaluation of the recognition performance showed that the timing of blinking can be predicted from speech with an F-score value upwards of 52%, which is well above chance. Additionally, a preliminary perceptual evaluation was conducted, that confirmed that adding eye blinking significantly improves the perception the character. Finally it showed that the speech synchronised synthesised blinks outperform random blinking in naturalness ratings.	hidden markov model;markov chain	Michal Dziemianko;Gregor Hofer;Hiroshi Shimodaira	2009			speech recognition;real-time computing;animation;hidden markov model;naturalness;computer science;perception	HCI	-15.238648440263585	-82.97519094607742	125649
b5771846b5bca03b427f4f640840a7ca1db3ed34	correct speech visemes as a root of total communication method for deaf people	slovak speech domain;lip reading;total communication method;speech triphones;main communication fiorm;deaf people;facial expression;design tool;speech visemes;developed tool;correct speech visemes	Many deaf people are using lip reading as a main communication fiorm. A viseme is a representational unit used to classify speech sounds in the visual domain and describes the particular facial and oral positions and movements that occur alongside the voicing of phonemes. A design tool for creating correct speech visemes is designed. It's composed of 5 modules; one module for creating phonemes, one module for creating 3D speech visemes, one module for facial expression and modul for synchronization between phonemes and visemes and lastly one module to generate speech triphones. We are testing the correctness of generated visemes on Slovak speech domains. The paper descriebes our developed tool.		Eva Pajorová;Ladislav Hluchý	2012		10.1007/978-3-642-30947-2_43	natural language processing;speech recognition;computer science;viseme	HCI	-15.827006996527714	-82.96592000976176	125675
b9326fb20b29a1d8f2353039e775016a16511f5a	vowel duration in pre-geminate contexts in polish		"""The study presents Polish experimental data on the variability of vowel duration in the context of following singleton and geminate consonants. The aim of the study is to explain the low vocalic variability values obtained from """"rhythm metrics"""" based analyses of speech rhythm. It also aims at contributing to the discussion about current dynamical models of speech rhythm that contain assumptions of the relative temporal stability of the vowel-to-vowel sequence. The results suggest that vowels in Polish co-vary with following consonant length in a roughly proportionate manner. An interpretation of the effect is offered where a fortition process overrides the possibility of temporal compensation."""	heart rate variability	Zofia Malisz	2009			speech recognition;mid vowel;computer science;vowel	ML	-10.225467125724709	-82.36860440163943	125773
0fd4658b59e3eb0c2b6972145b7639830445e492	sparse imputation for noise robust speech recognition using soft masks	background noise;noise robust speech recognition;probability;speech recognition decision making decision theory probability;probability noise robust speech recognition soft mask sparse missing data imputation method snr soft decision making;spectrogram;noise robustness speech recognition automatic speech recognition estimation error spectrogram speech enhancement background noise signal to noise ratio high definition video spatial databases;snr;speech;speech enhancement;indexing terms;noise robustness;noise measurement;sparse missing data imputation method;automatic speech recognition;accuracy;redundancy speech recognition robustness;redundancy;soft decision making;decision theory;spatial databases;high definition video;soft mask;speech recognition;robustness;missing data;estimation error;signal to noise ratio;article in monograph or in proceedings;noise	In previous work we introduced a new missing data imputation method for ASR, dubbed sparse imputation. We showed that the method is capable of maintaining good recognition accuracies even at very low SNRs provided the number of mask estimation errors is sufficiently low. Especially at low SNRs, however, mask estimation is difficult and errors are unavoidable. In this paper, we try to reduce the impact of mask estimation errors by making soft decisions, i.e., estimating the probability that a feature is reliable. Using an isolated digit recognition task (using the AURORA-2 database), we demonstrate that using soft masks in our sparse imputation approach yields a substantial increase in recognition accuracy, most notably at low SNRs.	geo-imputation;missing data;sparse matrix;speech recognition	Jort F. Gemmeke;Bert Cranen	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960666	speech recognition;computer science;machine learning;pattern recognition;mathematics;signal-to-noise ratio;statistics	Robotics	-14.631312606379788	-92.06119880992856	125885
e516f2fa994ad6125761a1eea991c45ef284a405	the effect of tone modeling in vietnamese lvcsr system		In this work, the tone modeling approaches are used manifest the tonal structure of Vietnamese and tonal feature is also used to build acoustic models. The results on LVCSR using deep bottleneck features (DBNFs) and different types of pronouncing dictionary, are also presented. The experiments are carried out on the dataset containing speeches on Voice of Vietnam channel (VOV). The results show that the performance of the system using tonal phoneme obtained relative improvements over the best non-tonal phoneme system by 19.25%. The DBNFs systems are applicable on tonal dictionary and adding tonal feature as input feature of the network reached around 18% relative recognition performance. c © 2016 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Organizing Committee of SLTU 2016.	acoustic cryptanalysis;acoustic model;dictionary;experiment;speech analytics	Quoc Bao Nguyen;Tat Thang Vu;Chi Mai Luong	2016		10.1016/j.procs.2016.04.046	speech recognition	Robotics	-18.5619686104734	-85.34601817453927	125892
a4268901af0801383e9621864a73d882b014f4c2	assignment of accent patterns to nonword items in a rapid reading task by japanese speakers of the kansai dialect				Yuuki Tanida;Yoko Higuchi;Yuri Yano;Satoru Saito	2012			cognitive psychology;psychology;linguistics	NLP	-12.840511512934516	-81.24652357402425	125941
24d1af3bf1da21500e9b84baf2dd12e6ba689d20	speaker independent voiced-unvoiced detection evaluated in different speaking styles	speaker independent	We propose a new algorithm for voiced/unvoiced classification of speech on a phoneme or sample level. The algorithm is inspired by auditory based approaches and combines two cues. One cue is based on the energy distribution of the signal and the other on the harmonicity. In order to extract the harmonicity of the signal we calculate a histogram of the zero crossings of the filter channels after applying a Gammatone filterbank to the signal. A measure similar to the variance of the zero crossings yields the harmonicity cue. The performance of the algorithm was measured on several minutes of read and spontaneous speech with various speakers. An algorithm proposed by Mustafa et al. [1] served as benchmark. The results show that our algorithm performs significantly better as well on read as on spontaneous speech and seems in particular be better able to to cope with different speaking styles. Index Terms: speech analysis, voiced/unvoiced detection, speaking style, zero crossings.		Martin Heckmann;Marco Moebus;Frank Joublin;Christian Goerick	2006			speaker recognition;speaker diarisation;computer science	NLP	-12.266780653823632	-90.44799308657068	125986
d1194556d01f7a49bf0501fd40c0423e095c4668	the influence of pitch and noise on the discriminability of filterbank features		Most features used for speech recognition are derived from the output of a filterbank inspired by the auditory system. The two most commonly used filter shapes are the triangular filters used in MFCC (mel-frequency cepstral coefficients) and the gammatone filters that model psychoacoustic critical bands. However, for both of these filterbanks there are free parameters that must be chosen by the system designer. In this paper, we explore the effect that different parameter settings have on the discriminability of speech sound classes. Specifically, we focus our attention on two primary parameters: the filter shape (triangular or gammatone) and the filter bandwidth. We use variations in the noise level and the pitch to explore the behavior of different filterbanks. We use the Fisher linear discriminant to give us insight about why some filterbanks perform better than others. We observe three things: 1) there are significant differences even among different implementations of the same filterbank, 2) wider filters help remove the non-informative pitch information, and 3) the Fisher criteria helps us understand why. We validate the Fisher measure with speech recognition experiments on the Aurora-4 speech corpus.	coefficient;common criteria;critical band;experiment;filter bank;information;linear discriminant analysis;mel-frequency cepstrum;noise (electronics);pitch (music);psychoacoustics;speech corpus;speech recognition;systems design	Malcolm Slaney;Michael L. Seltzer	2014			speech recognition	ML	-10.272050928233222	-88.65539620223265	125988
30026c93abd39c48e34b7d0e427d7cb860ae36f3	voice activity detection for best signal selection in air traffic management and control systems	speech intelligibility voice activity detection air traffic management air traffic control system best signal selection speech quality test voice activity detection algorithm vad score based method perceptual evaluation of speech quality pesq snr;speech intelligibility;speech signal to noise ratio correlation air traffic control signal processing algorithms standards noise measurement;speech processing air traffic control speech intelligibility;voice activity detection speech intelligibility;voice activity detection	The Best Signal Selection (BSS) in air traffic management and control systems has to decide among several signal instances of the same source which one offers the highest speech intelligibility. In these systems, the source signal is not available, thus, objective speech quality tests could not be used. However, information with regards to the speech quality could be obtained from the score of voice activity detection (VAD) algorithms. In this paper the correlation between speech quality and the score of VAD algorithms is analyzed. The results showed that the VAD score-based methods do not saturate for higher SNR, as the Perceptual Evaluation of Speech Quality (PESQ) does. A new VAD algorithm as a solution for the best signal selection problem is also proposed.	control system;intelligibility (philosophy);pesq;selection algorithm;signal-to-noise ratio;voice activity detection	Radu-Sebastian Marinescu;Corneliu Burileanu	2015	2015 38th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2015.7296395	voice activity detection;linear predictive coding;speech recognition;computer science;speech coding;speech processing;psqm;intelligibility	EDA	-9.27447936537061	-88.23240764124623	126159
9931fd86fb889ea413f6859eed332606749180f8	quality analysis of macroprosodic f0 dynamics in text-to-speech signals		We present a study on the relation between fundamental frequency (F0) and its perceptual effect in the context of text-tospeech (TTS) synthesis. Features that essentially capture the intonational (macro-prosodic) properties of spoken speech are introduced and analysed with regard to the following questions: (i) How does the prosodic variation of TTS signals differ from natural speech? (ii) Is there a functional relationship between the prosodic variation of TTS signals and its perceived quality? In answering these questions we present novel approaches for the construction of non-intrusive quality estimators. The results reveal a substantial degree of systematic influence of prosodic variation on TTS quality.	natural language;netware file system;speech synthesis	Christoph Norrenbrock;Florian Hinterleitner;Ulrich Heute;Sebastian Möller	2012			speech recognition;computer science;speech synthesis	NLP	-10.662760075971	-83.33988336305562	126194
c2684c9e9a13cc65e3705da9476f13454e2704bb	polish infant directed vs. adult directed speech: selected acoustic-phonetic differences		Infant-directed speech (IDS) is reported to differ significantly from adult-directed speech (ADS) in its acoustic-phonetic properties. In IDS, phonetic features of individual speech sounds tend to be intensified [6, 14, 20]. An example phenomenon documented for IDS in several languages is vowel hyperarticulation [25]. Thus, the formant frequency values (F1, F2) vary in the two target speaking styles. Other modifications can be observed in F0 levels (e.g., [5, 11]. Due to infants' preference towards IDS [5], laboratory-elicited IDS recordings are often used as stimuli in infant speech perception studies, aiming for example at the investigation of the effects of short-term exposure to foreign-language stimuli in early infancy and its potential contribution to the development of language learning skills (e.g., [15]). In the present study, we compare F0, F1, F2 values, and segmental duration in vowels produced by five female speakers of Polish, reading pseudoword lists in IDS and ADS.	acoustic cryptanalysis	Agnieszka Czoska;Katarzyna Klessa;Maciej Karpinski	2015			formant;language acquisition;speech perception;vowel;communication;speech recognition;pseudoword;phenomenon;psychology	NLP	-10.558909418356919	-82.13397560672003	126197
658c5faf7e9227ec17a02849bb6c3237aebfb082	phonetic convergence and imitation of speech by cochlear implant patients		Speech communication can be viewed as an interactive process involving a functional coupling between sensory and motor systems. One striking example comes from phonetic convergence, when speakers unconsciously tend to mimic their interlocutor’s speech during communicative interaction. In order to test whether deaf people w ith cochlear implantation did recover such perceptuomotor abilities, we measured online imitative changes on the fundamental frequency in relation to acoustic vowel targets in a non-interactive situati on of communication during both unintentional and voluntary imitative production tasks. We showed that cochlear implanted participants have the abili ty to converge to an acoustic target, both intentional ly and unintentionally, albeit with a lower degree tha n normal hearing participants. These results suggest that cochlear implanted patients recovered significant perceptuo-motor abilities less than two years following cochlear implantation.	acoustic cryptanalysis;cochlear implant;converge;interactivity;ion implantation	Lucie Scarbel;Denis Beautemps;Jean-Luc Schwartz;Sébastien Schmerber;Marc Sato	2015			communication;imitation;vowel;sensory system;fundamental frequency;psychology;cochlear implant;audiology	HCI	-7.421665827672877	-82.06731377976585	126326
896715117d586fbcb87c69a4c1f3591bbf8a8ca2	robust speech recognition in reverberant environments by using an optimal synthetic room impulse response model	artificial synthetic room impulse response;journal article;reverberant environment;speech recognition	This paper presents a practical technique for Automatic speech recognition (ASR) in multiple reverberant environment selection. Multiple ASR models are trained with artificial synthetic room impulse responses (IRs), i.e. simulated room IRs, with different reverberation time (T Model 60 s) and tested on real room IRs with varying T Room 60 s. To apply our method, the biggest challenge is to choose a proper artificial room IR model for training ASR models. In this paper, a generalised statistical IR model with attenuated reverberation after an early reflection period, named attenuated IR model, has been adopted based on three time-domain statistical IR models. Its optimal values of the reverberation-attenuation factor and the early reflection period on the recognition rate have been searched and determined. Extensive testing has been performed over four real room IR sets (63 IRs in total) with variant T Room 60 s and speaker microphone distances (SMDs). The optimised attenuated IR model had the best performance in terms of recognition rate over others. Specific considerations of the practical use of the method have been taken into account including: (i) the maximal training step of T Model 60 in order to get the minimal number of models with acceptable performance; (ii) the impact of selection errors on the ASR caused by the estimation error of T Room 60 ; and (iii) the performance over SMD and direct-to-reverberation energy Ratio (DRR). It is shown that recognition rates of over 80 90% are achieved in most cases. One important advantage of the method is that T Room 60 can be estimated either from reverberant sound directly (Takeda et al., 2009; Falk and Chan, 2010; Löllmann et al., 2010) or from an IR measured from any point of the room as it remains constant in the same room (Kuttruff, 2000), thus it is particularly suited to mobile applications. Compared to many classical dereverberation methods, the proposed method is more suited to ASR tasks in multiple reverberant environments, such as human-robot interaction. 2014 Elsevier B.V. All rights reserved.	automated system recovery;deficit round robin;human–robot interaction;maximal set;microphone;mobile app;service mapping description;speech recognition;synthetic intelligence	Jindong Liu;Guang-Zhong Yang	2015	Speech Communication	10.1016/j.specom.2014.11.004	speech recognition;computer science	AI	-13.288918439798202	-89.7552745643514	126382
637734db11aa0d90062f49685968c3e6aeef5661	synthesis for handwriting analysis	handwriting analysis;iterated function systems ifs;iterated function system;alphabet discrimination;pattern matching;pattern recognition;feature selection;fractal analysis	Recently a large number of studies has been published in the area of Fractal Analysis. In this paper we review briefly the IFS (Iterated Function System) theory, and we show how this theoretical tool leads to new applications in Pattern Recognition and more precisely in the analysis of handwritten texts. Among other results, this research work shows that fractal analysis is no longer only restricted to a descriptive role, but has entered in a functional phase. More precisely we developed a concrete application, based on automatic feature selection by fractal approach, and dedicated to alphabet discrimination. 2004 Elsevier B.V. All rights reserved.	feature selection;fractal analysis;fractal compression;graphology;iterated function system;optical character recognition;pattern recognition;personalization	Nicole Vincent;Audrey Seropian;Georges Stamon	2005	Pattern Recognition Letters	10.1016/j.patrec.2004.10.014	speech recognition;fractal analysis;computer science;machine learning;pattern matching;pattern recognition;mathematics;feature selection;iterated function system	AI	-5.138113498015863	-91.06432758135057	126423
8a8461ba1592f16b1805da2d7af4b4ec234da3a4	fbem: a filter bank em algorithm for the joint optimization of features and acoustic model parameters in bird call classification	optimisation;bird call classification em algorithm filter bank;filter bank;mel frequency cepstral coefficient abstracts feature extraction indexes filter banks discrete cosine transforms;acoustic signal processing;bird call classification;mel frequency cepstral coefficient;indexes;cepstral analysis;discrete cosine transforms;abstracts;channel bank filters;feature extraction;mel scaled filter bank filter bank em algorithm joint optimization bird call classification expectation maximization algorithm optimal center frequency optimal acoustic model parameter optimal center bandwidth cepstral feature extraction gradient ascent method antbird call classification error rate;filter banks;optimisation acoustic signal processing cepstral analysis channel bank filters feature extraction;em algorithm	This paper extends the expectation-maximization (EM) algorithm to estimate not only optimal acoustic model parameters, but also optimal center frequencies and bandwidths of the filter bank used in cepstral feature extraction for bird call classification. The search is done using the gradient ascent method. Filter bank and model parameters are optimized iteratively. Experiments are conducted on a large noisy corpus containing Antbird calls from 5 species. It is shown that features extracted using the optimized filter bank result in a lower classification error rate than those extracted using a Melscaled filter bank.	acoustic cryptanalysis;acoustic model;cepstrum;expectation–maximization algorithm;experiment;feature extraction;filter bank;gradient descent;mathematical optimization;text corpus;times ascent	Wei Chu;Abeer Alwan	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288298	database index;speech recognition;expectation–maximization algorithm;feature extraction;computer science;machine learning;pattern recognition;filter bank	Robotics	-18.23096705566941	-93.44104537428092	126482
25b42822c5a953e8c1b4498814a455aad853dc5b	data-driven generation of f0 contours using a superpositional model	text to speech synthesis;parametric model;regression tree;generic model;degree of freedom;f 0 contour generation;superpositional model;fundamental frequency;neural network;accentual phrase	This paper introduces a novel model-constrained, data-driven method to generate fundamental frequency contours for Japanese text-to-speech synthesis. In the training phase, the relationship between linguistic features and the parameters of a command–response F0 contour generation model is learned by a prediction module, which is represented by either a neural network or a set of binary regression trees. Input features consist of linguistic information related to accentual phrases that can be automatically derived from text, such as the position of the accentual phrase in the utterance, number of morae, accent type, and morphological information. In the synthesis phase, the prediction module is used to generate appropriate values of model parameters. The use of the parametric model restricts the degrees of freedom of the problem to facilitate the mapping between linguistic and prosodic features. Experimental results show that the method makes it possible to generate quite natural F0 contours with a relatively small training database. 2002 Elsevier Science B.V. All rights reserved.	artificial neural network;decision tree;logic programming;netware file system;parametric model;speech synthesis	Atsuhiro Sakurai;Keikichi Hirose	2003	Speech Communication	10.1016/S0167-6393(02)00177-2	natural language processing;speech recognition;parametric model;computer science;machine learning;decision tree;pattern recognition;fundamental frequency;degrees of freedom;artificial neural network	NLP	-17.95377839122594	-84.06367892621775	126527
215ddd893a796b6cc45aa0fc526204bea090ce98	sparse coding based features for speech units classification		In this paper a sparse representation based feature is proposed for the tasks in speech recognition. Dictionary plays an important role in order to get a good sparse representation. Therefore instead of using a single over complete dictionary, multiple signal adaptive dictionaries are used. A novel principal component analysis (PCA) based method is proposed to learn multiple dictionaries for each speech unit. For a given speech frame, first minimum distance criterion is employed to select appropriate dictionary and then a sparse solver is used to compute sparse feature for acoustic modeling. Experiments are performed using different datasets, which shows the proposed feature outperforms the existing features in recognition of isolated utterances.	acoustic cryptanalysis;acoustic model;dictionary;neural coding;principal component analysis;solver;sparse approximation;sparse matrix;speech recognition	Pulkit Sharma;Vinayak Abrol;Aroor Dinesh Dileep;Anil Kumar Sao	2015	Computer Speech & Language	10.1016/j.csl.2017.08.004	computer science;timit;machine learning;discriminative model;speech recognition;principal component analysis;feature extraction;hidden markov model;k-svd;mel-frequency cepstrum;sparse approximation;artificial intelligence;pattern recognition	AI	-16.11406767717994	-91.94258390581749	126571
77e12d42fa8f6e84697ba4961174f88edad27816	perceptual normalization of the vowels of a man and a child in various contexts	prononciation;repetition;produccion linguistica;hombre;linguistic production;universiteitsbibliotheek;vocal;article letter to editor;pronunciation;human;voyelle;repeticion;vowel;pronunciacion;production linguistique;homme	Abstract   The topic of this study is speaker normalization. Both a man and a child rapidly produced the Dutch sentences “Matroos p V t kaas” (“Sailor p V t eats cheese”): the man imitated the child's pitch. The test words p V t were presented to listeners in their original carrier sentence, in isolation, and in the other talker's carrier sentence. Especially in the latter condition considerably more vowel confusions occurred. It is argued that listeners match unknown vowels with template vowels of “average” men, women or children: the appropriate template is chosen on the basis of pitch and timbre of the unknown vowels. The vowel confusions in the present experiment can thus be explained by a confusion of templates. This explanation accounts better for the test results than the one suggested by Joos (1948) and others, that the acoustic context provides information about the formant frequencies of the talker's vowels with which a vowel space can be constructed that serves as a reference frame for the identification of the vowels in the test words.	database normalization	Dick R. van Bergem;Louis C. W. Pols;Florien J. van Beinum	1988	Speech Communication	10.1016/0167-6393(88)90018-0	speech recognition;mid vowel;human voice;linguistics	NLP	-10.352249352716333	-82.41438096417896	126584
b71e2abc746fdffd7db66c021a9c81753b554565	voice conversion based on mixtures of factor analyzers	gmm gaussian mixture model;indexing terms;voice conversion;gaussian mixture model;covariance matrices;mfa mixtures of factor analyzers;covariance matrix	This paper describes the voice conversion based on the Mixtures of Factor Analyzers (MFA) which can provide an efficient modeling with a limited amount of training data. As a typical spectral conversion method, a mapping algorithm based on the Gaussian Mixture Model (GMM) has been proposed. In this method two kinds of covariance matrix structures are often used : the diagonal and full covariance matrices. GMM with diagonal covariance matrices requires a large number of mixture components for accurately estimating spectral features. On the other hand, GMM with full covariance matrices needs sufficient training data to estimate model parameters. In order to cope with these problems, we apply MFA to voice conversion. MFA can be regarded as intermediate model between GMM with diagonal covariance and with full covariance. Experimental results show that MFA can improve the conversion accuracy compared with the conventional GMM.	algorithm;analog-to-digital converter;google map maker;mixture model	Yosuke Uto;Yoshihiko Nankaku;Tomoki Toda;Akinobu Lee;Keiichi Tokuda	2006			covariance matrix;speech recognition;index term;computer science;mixture model;statistics;covariance function	ML	-16.994149181363213	-92.3360445115256	126663
1a91509a668c61cd9105c6b9a53e996cf60c0172	noise robust exemplar matching for speech enhancement: applications to automatic speech recognition	psi_speech	We present a novel automatic speech recognition (ASR) scheme which uses the recently proposed noise robust exemplar matching framework for speech enhancement in the front-end. The proposed system employs a GMM-HMM back-end to recognize the enhanced speech signals unlike the prior work focusing on template matching only. Speech enhancement is achieved using multiple dictionaries containing speech exemplars representing a single speech unit and several noise exemplars of the same length. These combined dictionaries are used to approximate the noisy segments and the speech component is obtained as a linear combination of the speech exemplars in the combined dictionaries yielding the minimum total reconstruction error. The performance of the proposed system is evaluated on the small vocabulary track of the 2 CHiME Challenge and the AURORA-2 database and the results have shown the effectiveness of the proposed approach in improving the noise robustness of a conventional ASR system.	approximation algorithm;dictionary;google map maker;hidden markov model;speech enhancement;speech recognition;template matching;vocabulary	Emre Yilmaz;Deepak Baby;Hugo Van hamme	2015			voice activity detection;audio mining;linear predictive coding;speech recognition;computer science;speech coding;pattern recognition;speech processing;acoustic model	NLP	-15.271905744358575	-91.80577465651716	126805
7931c3c28e4b6c217555e0b27576e9652a38428b	a voice-to-midi system for singing melodies with lyrics	note segmentation;fft;voice to midi;satisfiability;digital music;tapping;melody input;pitch recognition;lyrics	"""In this paper, we propose a robust Voice-to-MIDI (V to M) system with which a user can input MIDI sequence data by naturally singing melodies with lyrics. A Voice-to-MIDI system translates singing voices into digital musical data, i.e., MIDI sequence data. Therefore, with such a system, users can input melodies intuitively, which releases them from manual translating memorized melodies into chromatic pitches. However, the quality of translation of ordinary Voice-to-MIDI systems is insufficient. One of the most significant problems is the poor accuracy of the segmentation of notes. We solve this problem by employing """"rhythmic tapping"""" concurrently with singing. We examined the proposed method by the accuracy of the numbers of segmented notes and their pitches. As a result, we confirmed that our system outperformed ordinary Voice-to-MIDI systems. Thus, this system satisfies both of easy and intuitive composition of MIDI sequence data and high accuracy of translation of sung data into MIDI sequence data."""	midi	Naoki Itou;Kazushi Nishimoto	2007		10.1145/1255047.1255085	midi tuning standard;speech recognition;art;acoustics;communication	DB	-6.730691982678024	-93.00174418980768	126882
d0a9a390b23c690fcad0ca6499088f220bb1bc7e	estimation of vocal tract front cavity resonance in unvoiced fricative speech	vocal tract		resonance;tract (literature)	Minkyu Lee;Donald G. Childers	1997			speech recognition;resonance;computer science;vocal tract	NLP	-8.48453672191362	-84.5908000419029	127092
4d2ba84aa70021c25779d2479871d62c5af70c5f	individualization of head related transfer functions based on radial basis function neural network		Head Related Transfer Functions (HRTFs) contain sound localization cues and are commonly used in 3D audio reproduction. Due to HRTFs are closely related to anthropometric parameters (head, pinna, torso), which means HRTFs vary with each individual, how to obtain a set of suitable HRTFs for each individual remains to be solved. In this paper, we investigated the complex relationship between HRTFs and anthropometric parameters through Radial Basis Function neural network (RBF), and proposed a method of generating individualized HRTFs with listener's anthropometric parameters. Objective experiments show that the estimated HRTFs have good consistency with measured ones, and the spectral distortion values have an average reduction of 0.59 dB compared with other methods. Subjective listening tests show that using estimated HRTFs enable accurate auditory localization.	anthropometry;artificial neural network;distortion;experiment;radial (radio);radial basis function	Lian Meng;Xiaodong Wang;Wei Chen;Chunling Ai;Ruimin Hu	2018	2018 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2018.8486494	artificial intelligence;computer vision;torso;pattern recognition;transfer function;computer science;artificial neural network;auditory localization;sound localization;radial basis function;distortion	Robotics	-9.130632781225417	-86.17401591045896	127220
7a39514678cbb5e70efedf600bb3ab2fa30d75a0	correlated-spaces regression for learning continuous emotion dimensions	output correlations;multi modal fusion;component analysis;valence;feature selection;arousal;continuous and dimensional emotion descriptions	Adopting continuous dimensional annotations for affective analysis has been gaining rising attention by researchers over the past years. Due to the idiosyncratic nature of this problem, many subproblems have been identified, spanning from the fusion of multiple continuous annotations to exploiting output-correlations amongst emotion dimensions. In this paper, we firstly empirically answer several important questions which have found partial or no answer at all so far in related literature. In more detail, we study the correlation of each emotion dimension (i) with respect to other emotion dimensions, (ii) to basic emotions (e.g., happiness, anger). As a measure for comparison, we use video and audio features. Interestingly enough, we find that (i) each emotion dimension is more correlated with other emotion dimensions rather than with face and audio features, and similarly (ii) that each basic emotion is more correlated with emotion dimensions than with audio and video features. A similar conclusion holds for discrete emotions which are found to be highly correlated to emotion dimensions as compared to audio and/or video features. Motivated by these findings, we present a novel regression algorithm (Correlated-Spaces Regression, CSR), inspired by Canonical Correlation Analysis (CCA) which learns output-correlations and performs supervised dimensionality reduction and multimodal fusion by (i) projecting features extracted from all modalities and labels onto a common space where their inter-correlation is maximised and (ii) learning mappings from the projected feature space onto the projected, uncorrelated label space.	algorithm;dimensionality reduction;feature vector;file spanning;multimodal interaction;oracle fusion architecture;spaces;video	Mihalis A. Nicolaou;Stefanos P. Zafeiriou;Maja Pantic	2013		10.1145/2502081.2502201	valence;computer vision;arousal;computer science;artificial intelligence;machine learning;pattern recognition;feature selection	AI	-4.578469687150312	-86.0982582296973	127243
6ec553ba62b8cfed86180ff59c219bf1b0e80dd7	familiarity effects in visual word recognition		"""This thesis is an investigation of two different aspects of familiarity processes involved in visual word recognition. The first is how capitalisation influences visual word recognition. The second is the role played by onset, nucleus and coda in nonword recognition. A familiar aspect of proper names in English, is that they are printed with an initial capital letter. Two experiments investigated the effects of the capitalisation of the initial letter of nonwords. It was found that subjects generate fewer pronunciations for initially capitalised nonwords than for those which were not capitalised. I suggest that in English initial capitalisation acts as a cue strong enough to prompt readers to perceive unfamiliar strings of letters as belonging to the category of proper names. As a result, the phonological domain used to retrieve the pronunciation of initially capitalised strings becomes more restricted than that used for the non-capitalised unfamiliar strings. These results extend the applicability of Brennen's theory for proper names, which is based on the size of the set of plausible phonologies of a word. In a third experiment, pairs of nonwords had their familiar visual appearance manipulated in terms of first and last letter capitalisation, in a same-different matching task. Faster response times were obtained for those nonword pairs that kept a more familiar aspect (e.g., pairs in which the first letter was capitalised as opposed to others in which the last letter was capitalised). These results are explained in terms of Besner and Jonhston (1989) """"orthographic familiarity route"""". I propose the transformation model as an explanation for the mechanisms by which this route operates. Nonwords are an important aspect of this thesis. A new algorithm was developed for the creation of monosyllabic nonwords in which the frequency of their onsets, nuclei and codas could be controlled carefully. This gave us the opportunity to study the influence of orthographic neighborhood in visual word recognition. The findings here are in agreement with previous studies which show the recognition of an item to be influenced by the presence of neighbours. It has been hypothesized that familiarity effects in visual word recognition can only be found in tasks where identification mechanisms are not implicated. Here, a new category of words, namely brand names, was used to test this hypothesis. There are many reasons why brand names are a more appropriate class of words than acronyms to be used in this type of investigation. The results obtained confirm the hypothesis above. Previously, acronyms had been the only class of words used to test this hypothesis."""		Possidonia de Freitas Drumond Gontijo	1998			visual word;speech recognition;computer science	AI	-10.410101254400908	-80.40545041563927	127468
d90b1c06fb616212f5d1a494f7fe282346ce7f9e	hybrid hmm-nn modeling of stationary-transitional units for continuous speech recognition	hidden markov model;continuous speech recognition;context dependent;neural network;generalization capability	This paper describes the bene®ts in recognition accuracy that can be achieved in a hybrid Hidden Markov Model ± Neural Network (HMM±NN) recognition framework by using context-dependent subword units named Stationary±Transitional Units. These units are made up of stationary parts of the context-independent phonemes plus all the admissible transitions between them; they have good generalization capability and capture a wide acoustic detail. These units are very suitable to be modeled with neural networks, can enhance the performances of hybrid HMM±NN systems, and represent a real alternative to the context-independent phonemes. The ecacy of Stationary± Transitional Units is veri®ed for the Italian language on isolated and continuous speech recognition tasks extracted from a real application employed for railway timetable telephonic vocal access. The results show that a relevant improvement is achieved with respect to the use of the context-independent phonemes. Ó 2000 Elsevier Science Inc. All rights reserved.	acoustic cryptanalysis;artificial neural network;context-sensitive language;embedded system;hidden markov model;markov chain;performance;schedule;speech recognition;stationary process;substring;on-line system	Dario Albesano;Roberto Gemello;Franco Mana	2000	Inf. Sci.	10.1016/S0020-0255(99)00106-1	speech recognition;computer science;machine learning;context-dependent memory;acoustic model;artificial neural network;hidden markov model	AI	-18.418469650546545	-87.59262160716904	127535
76e563e36e16bd6a67042528808e846b43c4e405	predictors of pause duration in read-aloud discourse		The research reported in this paper is an attempt to elucidate the predictors of pause duration in read-aloud discourse. Through simple linear regression analysis and stepwise multiple linear regression, we examined how different factors (namely, syntactic structure, discourse hierarchy, topic structure, preboundary length, and postboundary length) influenced pause duration both separately and jointly. Results from simple regression analysis showed that discourse hierarchy, syntactic structure. topic structure, and postboundary length had significant impacts on boundary pause duration. However, when these factors were tested in a stepwise regression analysis, only discourse hierarchy, syntactic structure, and postboundary length were found to have significant impacts on boundary pause duration. The regression model that best predicted boundary pause duration in discourse context was the one that first included syntactic structure, and then included discourse hierarchy and postboundary length. This model could account for about 80% of the variance of pause duration. Tests of mediation models showed that the effects of topic structure and discourse hierarchy were significantly mediated by syntactic structure, which was most closely correlated with pause duration. These results support an integrated model combining the influence of several factors and can be applied to text-to-speech systems.		Xiaohong Yang;Mingxing Xu;Yufang Yang	2014	IEICE Transactions		natural language processing;speech recognition;computer science	NLP	-11.706435569685903	-80.48086192337577	127695
91c72ea0b896a283565f3f0122c0b69db0f38e36	arabic isolated word recognition system using hybrid feature extraction techniques and neural network		In this paper, we implemented a speaker-dependent speech recognition system for 11 standard Arabic isolated words. During the feature extraction phase, several techniques were used such as Mel frequency cepstral coefficients, perceptual linear prediction, relative perceptual linear prediction and their first order temporal derivatives. Principal component analysis was adopted in order to reduce the feature dimension. The recognition phase is based on the feed forward back-propagation neural network using two learning algorithms: the Levenberg–Marquardt “Trainlm” and the scaled conjugate gradient “Trainscg”. Hybrid approaches were used and compared in terms of computational time and recognition rates and have produced very interesting performances.	artificial neural network;feature extraction	Lotfi Boussaid;Mohamed Hassine	2018	I. J. Speech Technology	10.1007/s10772-017-9480-7	artificial intelligence;linear prediction;speech recognition;artificial neural network;time delay neural network;pattern recognition;word recognition;feature (machine learning);feature extraction;feature dimension;mel-frequency cepstrum;computer science	NLP	-16.15757314896794	-88.30348238991945	127700
4e8c535a346720079df70efc00cc9c63396c1fba	a comparison of multi-layer perceptron and radial basis function neural network in the voice conversion framework	speech processing multilayer perceptrons radial basis function networks speaker recognition;mlp multilayer perceptron radial basis function neural network voice conversion framework speaker specific feature speech signal glottal excitation vocal tract line spectral frequencies linear predictive residual rbf prosodic features nonlinear mapping baseline residual selection method intergender voice conversion intragender voice conversion;voice conversion dynamic time warping line spectral frequencies multi layer perceptron radial basis function residual selection;speech training feature extraction vectors shape artificial neural networks	The voice conversion system modifies the speaker specific features of the source speaker so that it sounds like a target speaker speech. The voice individuality of the speech signal is characterized at various levels such as shape of the glottal excitation, shape of the vocal tract and the long term prosodic features. In this work, Line Spectral Frequencies (LSF) are used to represent the shape of the vocal tract and Linear Predictive (LP) residual represents the shape of the glottal excitation of a particular speaker. A Multi Layer Perceptron (MLP) and Radial Basis Function (RBF) based neural network are explored to formulate the nonlinear mapping for modifying the LSFs. The baseline residual selection method is used to modify the LP-residual of one speaker to that of another speaker. A relative comparison between MLP and RBF are carried out using various objective and subjective measures for inter-gender and intra-gender voice conversion. The results reveal that an optimized RBF performs slightly better than baseline MLP based voice conversion.	artificial neural network;baseline (configuration management);lsf;line spectral pairs;memory-level parallelism;multilayer perceptron;nonlinear system;quad flat no-leads package;radial (radio);radial basis function;tract (literature)	Ankita N. Chadha;Jagannath H. Nirmal;Mukesh A. Zaveri	2014	2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2014.6968405	speech recognition;engineering;machine learning;pattern recognition;multilayer perceptron;radial basis function network	Vision	-10.142206912729781	-87.50465109521157	127705
2c1244e8faad95640f011c2bfdf720e73a1bd3df	broadcast news segmentation with factor analysis system		This paper studies a novel audio segmentation-by-classification approach based on Factor Analysis (FA) with a channel compensation matrix for each class and scoring the fixed-length segments as the log-likelihood ratio between class/no-class. The system described here is designed to segment and classify the audio files coming from broadcast programs into five different classes: speech (SP), speech with noise (SN), speech with music (SM), music (MU) or others (OT). This task was proposed in the Albayzin 2010 evaluation campaign. The article presents a final system with no special features and no hierarchical structure. Finally, the system is compared with the winning system of the evaluation (the system use specific features with hierarchical structure) achieving a significant error reduction in SP and SN. These classes represent 3/4 of the total amount of the data. Therefore, the FA segmentation system gets a reduction in the average segmentation error rate that is able to be used in a generic task.	2.5d;factor analysis;speech recognition	Diego Castán;Alfonso Ortega Giménez;Antonio Miguel;Eduardo Lleida	2013			speech recognition;word error rate;artificial intelligence;pattern recognition;computer science;matrix (mathematics);segmentation;communication channel;broadcasting	ML	-18.467126047882676	-86.43070869058565	127789
d7222c52872704de608fd2053e7d1ca24b786f20	correlating text with prosody		The prediction of prosody from text information has long been recognised as a requirement for natural sounding speech synthesis. While an examination of the relationship between text information and prosody typically focuses on the role of accent, duration and phrasing both from a statistical and rulebased perspective, this paper investigates the correlation between the similarities calculated with respect to text information and those calculated with respect to prosody from an exemplarbased perspective. Two text features are examined, the syntactic tree and the dependency tree, along with two prosody features, pitch and intensity. The work in this paper investigates 1) the correlation between text information and prosody information 2) the conditional membership probability between text information and prosodic information, and 3) the effect of the number of exemplars on the conditional membership probability.	automatic sounding;pitch (music);semantic prosody;speech synthesis	Mohamed Abou-Zleikha;Julie Carson-Berndsen	2011			speech recognition;computer science;prosody	NLP	-13.721497185760194	-82.57126544437402	127862
e46952fea5890fe4ba03ffd15e3d0200ee8e027f	the influence of noise on the speaker recognition performance using the higher frequency band	additive noise;speaker recognition acoustic noise;energy spectrum;speaker recognition;wideband case noise speaker recognition performance higher frequency band speech energy spectrum fricative sounds additive noises performance;acoustic noise;speaker recognition frequency acoustic noise loudspeakers databases working environment noise noise level speech enhancement humans additive noise;high performance	In our previous studies, we have shown the effectiveness of using the information in the higher frequency band for speaker recognition. However, the energy spectrum of speech in the higher frequency band is weak, except for some fricative sounds. Therefore, it is important to investigate the speaker individual information in that region under noisy conditions. In this study, we examine the influence of additive noises on the performance of speaker recognition using the higher frequency band. Experimental results show that high performance is obtained in the wideband case under many typical noisy conditions. It is also shown that the higher frequency band is more stable against noises than the lower one. For that reason, the higher frequency band gives good performance even if the SNR of the higher frequency region is worse than the lower one.	frequency band;speaker recognition	Shoji Hayakawa;Fumitada Itakura	1995		10.1109/ICASSP.1995.479538	speaker recognition;speech recognition;computer science;noise;noise figure	Vision	-12.318671386936874	-90.8052695112736	127923
9764e26b56461276e2e8f4cd790f5cfc6c8fd588	advances in children's speech recognition within an interactive literacy tutor	cross-utterance word history information;speech recognition;error reduction;dynamic n-gram language modeling;interactive literacy tutor;word error rate;previous published result;speaker-adaptive training;previous work;vocal tract length normalization;language modeling;time-adaptive acoustic	In this paper we present recent advances in acoustic and language modeling that improve recognition performance when children read out loud within digital books. First we extend previous work by incorporating crossutterance word history information and dynamic n-gram language modeling. By additionally incorporating Vocal Tract Length Normalization (VTLN), Speaker-Adaptive Training (SAT) and iterative unsupervised structural maximum a posteriori linear regression (SMAPLR) adaptation we demonstrate a 54% reduction in word error rate. Next, we show how data from children’s read-aloud sessions can be utilized to improve accuracy in a spontaneous story summarization task. An error reduction of 15% over previous published results is shown. Finally we describe a novel real-time implementation of our research system that incorporates time-adaptive acoustic and language modeling.	acoustic cryptanalysis;bit error rate;book;e-book;iterative method;language model;n-gram;real-time clock;speech recognition;spontaneous order;think aloud protocol;tract (literature);word error rate	Andreas Hagen;Bryan L. Pellom;Sarel van Vuuren;Ronald A. Cole	2004			natural language processing;speech recognition;computer science;machine learning;linguistics	NLP	-16.88027306150276	-83.13717535330862	127965
97ddb59a2d0ce98d44315dd76f6bff2700df0432	techniques for crosslingual voice conversion	source speaker;timbre;speech synthesis;crosslingual voice conversion;speech processing;training;acoustical feature;speech;acoustic signal processing;acoustical feature crosslingual voice conversion source speaker timbre vocal identity recorded sentence source speaker target speaker time aligned recording;speech synthesis acoustic signal processing speaker recognition;speaker recognition;voice conversion;artificial neural networks;timbre transformation;time aligned recording;function approximation;vocal identity crosslingual voice conversion timbre transformation;recorded sentence;vocal identity;target speaker;source speaker timbre;training speech artificial neural networks timbre function approximation speech processing	The cross lingual voice conversion problem refers to the replacement of a speaker’s timbre or vocal identity in a recorded sentence, assuming that the source speaker and target speaker use different languages. This problem differs from typical voice conversion in the sense that the mapping of acoustical features cannot depend on time-aligned recordings of source and target speakers uttering the same sentences. This paper presents an overview of a general cross lingual voice conversion system and discusses the most important techniques used in each step of the conversion process.	benchmark (computing);systems design;tract (literature)	Anderson Fraiha Machado;Marcelo Queiroz	2010	2010 IEEE International Symposium on Multimedia	10.1109/ISM.2010.62	speaker recognition;speech recognition;function approximation;computer science;speech;machine learning;voice analysis;speech processing;speech synthesis	Embedded	-14.095261816978253	-88.16532915317885	128113
65e3e24a4d35f23c0c16fecc2c00545ab650e389	resistance is futile - the intonation between continuation rise and calling contour in german		German knows two plateau-based phrase-final intonation contours: the high level plateau of the continuation rise and the descending plateau sequence of the calling contour. They occur within a narrow scaling range of only a few semitones. The paper presents production and perception evidence for a third plateau-based phrase-final intonation contour inside this narrow scaling range. The new plateau contour shows a F0 decrease of between 1-3 st (in the form of a slightly declining plateau or a descending plateau sequence), involves additional lengthening of the vowels underneath the plateau, and occurs when resistance is futile, i.e. when speakers signal that they finally, but reluctantly, give in to a demand of the dialogue partner. Phonological implications are briefly outlined.	borg (star trek);continuation;contour line;high-level programming language;image scaling	Oliver Niebuhr	2013			speech recognition;linguistics	NLP	-9.848838838819	-81.57133910904375	128143
60cdf053dac67afee8d4219dc48af684186a28c1	study on tone classification of chinese continuous speech in speech recognition system	probabilistic method;gaussian mixture model;speech recognition;classification accuracy	In this paper, we first introduce the use of Gaussian mixture models (GMM) for Chinese tone classification in continuous speech. Then, we explain how to integrate it with the HMM-based speech recognition system. Finally, we provide the tone classification accuracy of this probabilistic method which is tested with Chinese continuous speech database of national “863” project.	hidden markov model;mixture model;speech recognition;statistical classification	Jian Liu;Xiaodong He;Fuyuan Mo;Tiecheng Yu	1999			natural language processing;speaker recognition;speech recognition;computer science;probabilistic method;pattern recognition;mixture model;acoustic model	ML	-14.886139999315477	-87.5711050028062	128151
25278716b1b9a2e28203aef3e924b2a012c1d237	a report on sound event detection with different binaural features		In this paper, we compare the performance of using binaural audio features in place of single channel features for sound event detection. Three different binaural features are studied and evaluated on the publicly available TUT Sound Events 2017 dataset of length 70 minutes. Sound event detection is performed separately with single channel and binaural features using stacked convolutional and recurrent neural network and the evaluation is reported using standard metrics of error rate and F-score. The studied binaural features are seen to consistently perform equal to or better than the single-channel features with respect to error rate metric.	artificial neural network;binaural beats;f1 score;recurrent neural network	Sharath Adavanne;Tuomas Virtanen	2017	CoRR		speech recognition;word error rate;computer science;binaural recording;recurrent neural network	NLP	-13.50975730446061	-89.43035136487909	128451
6000af86e8dbdbb8308b638b07ecb1b85d99856f	durational correlates of singleton-geminate contrast in hungarian voiceless stops		This paper presents the results of a durational analysis of singleton and geminate stop consonants from Hungarian spontaneous speech. The durational correlates of three types of geminates (i.e., underlying, derived true and fake geminates) are also examined and compared to one another. Results show that single voiceless stops are realized with significantly shorter total and closure duration than geminates. Research findings on rates of closure phase suggest that phonological lengthening targets certain portion of the internal structure of stops. VOT seemed to be invariant and therefore irrelevant parameter in the distinction of short and long consonants. We can evince differences among geminate types: fake geminates are produced with tendentiously longer durations than underlying and true derived geminates, which result suggests closer similarity between the two latter types.	invariant (computer science);neural correlates of consciousness;relevance;spontaneous order	Tilda Neuberger	2015			mathematics;singleton;artificial intelligence;closure phase;invariant (mathematics);pattern recognition	HCI	-10.095165467375857	-81.47051528281385	128605
021d628909c7d506a69d9098450ab39dfa4f8128	hmm-based mixed-language (mandarin-english) speech synthesis	speech synthesis natural languages hidden markov models loudspeakers asia testing acoustic measurements educational institutions vegetation mapping decision trees;decision tree;decent intelligibility hmm based mixed language speech synthesis mandarin utterances china hmm based bilingual tts bilingual state mapping monolingual speaker average mos score hidden markov models decision trees language state mapping monolingual source language recording cross language state mapping english word transcription accuracy;speech synthesis;english word transcription accuracy;training;college students;speech;language state mapping;natural languages;bilingual education;hmm based bilingual tts;or phrases;cross language state mapping;average mos score;hmm based mixed language;training data;speech synthesis decision trees hidden markov models linguistics natural languages;hidden markov models;target language;monolingual source language recording;source language;bilingual state mapping;decent intelligibility;china;switches;decision trees;monolingual speaker;mandarin utterances;linguistics	English words or short phrases embedded in Mandarin utterances have become more common among bilingually educated people like college students in China. Similarly, it becomes highly desirable that TTS systems can synthesize mixed- language speech properly. Recently, we proposed an HMM-based bilingual TTS to synthesize a target language when only monolingual source language recording from a speaker is available. In this paper, we extend it to synthesize mixed- language sentences. A cross-language state mapping is first established between decision trees built from the English and Mandarin recordings of a bilingual speaker. Via the mapping, English words or phrases embedded in Mandarin sentences can then be synthesized. The bilingual state-mapping is extended to monolingual speaker to perform mixed-language synthesis. Perceptual test results show: (1) decent intelligibility, confirmed by an English word transcription accuracy of 86%; (2) good speech quality with an average MOS score of 3.2.	compiler;context-sensitive language;decision tree;embedded system;hidden markov model;intelligibility (philosophy);netware file system;speech synthesis;super robot monkey team hyperforce go!;transcription (software)	Yao Qian;Houwei Cao;Frank K. Soong	2008	2008 6th International Symposium on Chinese Spoken Language Processing	10.1109/CHINSL.2008.ECP.15	natural language processing;speech recognition;computer science;decision tree;linguistics;speech synthesis;hidden markov model	NLP	-18.748371309680003	-83.69629921108039	128743
124641de338c7a79e9d15198ada8b7a01c13d470	does experience in talking facilitate speech repetition?	selected works;bepress	Speech is unique among highly skilled human behaviors in its ease of acquisition by virtually all individuals who have normal hearing and cognitive ability. Vocal imitation is essential for acquiring speech, and it is an important element of social communication. The extent to which age-related changes in cognitive and motor function affect the ability to imitate speech is poorly understood. We analyzed the distributions of response times (RT) for repeating real words and pseudowords during fMRI. The average RT for older and younger participants was not different. In contrast, detailed analysis of RT distributions revealed age-dependent differences that were associated with changes in the time course of the BOLD response and specific patterns of regional activation. RT-dependent activity was observed in the bilateral posterior cingulate, supplementary motor area, and corpus callosum. This approach provides unique insight into the mechanisms associated with changes in speech production with aging.	behavior;bilateral filter;body of uterus;cognition;corpus callosum;motor cortex;speech repetition;vocal cord paralysis;fmri	Linda I. Shuster;Donna R. Moore;Gang Chen;Dennis M. Ruscello;William F. Wonderlin	2014	NeuroImage	10.1016/j.neuroimage.2013.10.064	psychology;speech recognition;developmental psychology;communication	HCI	-7.812828683337284	-81.15490833128891	128747
8c2965a302d6e945e214bdc2c21820fab954e1e7	channel selection using n-best hypothesis for multi-microphone asr	conference report	If speech is captured by several arbitrarily-located microphones in a room, the degree of distortion by noise and reverberation may vary strongly from one channel to another. Channel selection for automatic speech recognition aims to rank the signals according to their quality, and, in particular, to select the best one for further processing in the recognition system. To create this ranking, we propose here to use posterior probabilities estimated from the N-best hypothesis of each channel. When evaluated experimentally, this new channel selection technique outperforms the methods published so far. We also propose the combination of different channel selection techniques to further increase the recognition accuracy and to reduce the computational load without significant performance loss.	distortion;experiment;microphone;speech recognition	Martin Wolf;Climent Nadeu	2013			speech recognition;computer science;machine learning;pattern recognition	ML	-13.764630501979768	-90.95012972489936	128972
02c522849922f1f4c86647dedf10e17814b36463	towards an efficient deep learning model for musical onset detection		In this paper, we propose an efficient and reproducible deep learning model for musical onset detection (MOD). We first review the state-of-the-art deep learning models for MOD, and identify their shortcomings and challenges: (i) the lack of hyper-parameter tuning details, (ii) the nonavailability of code for training models on other datasets, and (iii) ignoring the network capability when comparing different architectures. Taking the above issues into account, we experiment with seven deep learning architectures. The most efficient one achieves equivalent performance to our implementation of the state-of-the-art architecture. However, it has only 28.3% of the total number of trainable parameters compared to the state-of-the-art. Our experiments are conducted using two different datasets: one mainly consists of instrumental music excerpts, and another developed by ourselves includes only solo singing voice excerpts. Further, inter-dataset transfer learning experiments are conducted. The results show that the model pre-trained on one dataset fails to detect onsets on another dataset, which denotes the importance of providing the implementation code to enable re-training the model for a different dataset. Datasets, code and a Jupyter notebook running on Google Colab are publicly available to make this research understandable and easy to reproduce.	deep learning;experiment;hidden markov model;hyper-threading;ipython;network architecture;onset (audio);structure of observed learning outcome	Rong Gong;Xavier Serra	2018	CoRR		machine learning;transfer of learning;speech recognition;computer science;artificial intelligence;deep learning;architecture;mod;singing	NLP	-17.981712490527237	-86.93508990050073	129033
28cccf2326ee00ae9bbfcede34ed2ed49d8fc581	gaspeech: a framework for automatically estimating input parameters of klatt's speech synthesizer	stopping criteria;convergence;speech synthesis;speech processing;adaptive control;gaspeech;speech;synthesized speech sounding;utterance copy;evolution biology;distortion;low spectral distortion;natural target speech;speech synthesis distortion genetic algorithms parameter estimation;bandwidth;genetic algorithms;parameter estimation;adaptive control gaspeech parameter estimation klatt s speech synthesizer genetic algorithms speech imitation utterance copy synthesized speech sounding natural target speech low spectral distortion;parameter estimation speech synthesis synthesizers speech processing speech analysis genetic algorithms adaptive control neural networks signal processing laboratories;klatt s speech synthesizer;speech imitation;gallium	This work describes GASpeech: a framework centered on genetic algorithms for automatically estimating the input parameters of Klatt's speech synthesizer. GASpeech aims to speed up the process of speech imitation (or utterance copy), where one has to find the model parameters that lead to a synthesized speech sounding close enough to the natural target speech (i.e. low spectral distortion). The architecture of GASpeech is described, emphasizing the usage of adaptive control of probabilities and stopping criteria speeding up the convergence process. Though this paper does not present a closed solution for all Klatt's parameters, an enormous improvement in accuracy and methodology was reached when compared to previous works.	automatic sounding;distortion;genetic algorithm;speech repetition;speech synthesis	José Humberto DE Souza Borges;Igor Couto;Fabíola Oliveira;Tales Imbiriba;Aldebaro Klautau	2008	2008 10th Brazilian Symposium on Neural Networks	10.1109/SBRN.2008.23	speech recognition;genetic algorithm;convergence;distortion;adaptive control;computer science;speech;speech processing;estimation theory;speech synthesis;gallium;bandwidth	NLP	-8.923907249709915	-88.12905055224014	129090
dc9d9cb3c4a77b52944ca456f56aa8442f8bd9b4	assessing a speaker for fast speech in unit selection speech synthesis.	unit selection;speech synthesis	This paper describes work in progress concerning the adequate modeling of fast speech in unit selection speech synthesis systems, mostly having in mind blind and visually impaired users. Initially, a survey of the main characteristics of fast speech will be given. Subsequently, strategies for fast speech production will be discussed. Certain requirements concerning the ability of a speaker of a fast speech unit selection inventory are drawn. The following section deals with a perception study where a selected speaker's ability to speak fast is investigated. To conclude, a preliminary perceptual analysis of the recordings for the speech synthesis corpus is presented.	requirement;speech synthesis	Donata Moers;Petra Wagner	2009			voice activity detection;natural language processing;speech technology;speech recognition;speech corpus;computer science;motor theory of speech perception;speech processing;psqm;speech synthesis;intelligibility;speech analytics	NLP	-13.324609810881302	-85.18888935245235	129115
def89593ba1b636e7653b8ee0b81934ee00c4f91	attention-based end-to-end speech recognition on voice search		Recently, there has been a growing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. In this paper, we explore the use of attention-based encoder-decoder model for Mandarin speech recognition on a voice search task. Previous attempts have shown that applying attention-based encoder-decoder to Mandarin speech recognition was quite difficult due to the logographic orthography of Mandarin, the large vocabulary and the conditional dependency of the attention model. In this paper, we use character embedding to deal with the large vocabulary. Several tricks are used for effective model training, including L2 regularization, Gaussian weight noise and frame skipping. We compare two attention mechanisms and use attention smoothing to cover long context in the attention model. Taken together, these tricks allow us to finally achieve a character error rate (CER) of 3.58% and a sentence error rate (SER) of 7.43% on the MiTV voice search dataset. While together with a trigram language model, CER and SER reach 2.81% and 5.77%, respectively.	elastic net regularization;encoder;end-to-end principle;language model;smoothing;speech recognition;super robot monkey team hyperforce go!;trigram;vocabulary	Changhao Shan;Junbo Zhang;Yujun Wang;Lei Xie	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462492	voice activity detection;speech analytics;word error rate;acoustic model;speaker recognition;speech corpus;computer science;pattern recognition;artificial intelligence;language model;speech recognition;voice analysis	Vision	-18.614023425692398	-87.47717306882483	129193
0a017be3cbf9a11bea3fb75c42656ea79c9f678f	應用語音辨識技術於鳥鳴聲辨識 (bird species recognition based on speech processing techniques) [in chinese]			speech processing	Wei-En Liao;Hsin-Chieh Lee;Wei-Ho Tsai	2011			speech processing;speech recognition;computer science	AI	-14.428750311591031	-86.40619443207945	129308
5a91bbb483fb3def51fe50f11c38b6294e41e6c6	repeating segment detection in songs using audio fingerprint matching	histograms spectrogram time frequency analysis tree data structures databases fingerprint recognition pattern matching;search tree data structure repeating segment detection song audio fingerprint matching distance matrix audio fingerprint extraction sequence matching fingerprint codeword;tree data structures audio signal processing matrix algebra music pattern matching	We propose an efficient repeating segment detection approach that doesn't require computation of the distance matrix for the whole song. The proposed framework first extracts audio fingerprints for the whole song. Then,for each time step in the song we perform a query to match a sequence of M fingerprint codewords against the fingerprints of the rest of the song. In order to find a match for the first fingerprint query, a search tree data structure is built with the fingerprints of the rest of the song. For subsequent fingerprint queries for the rest of the song, the matching process dynamically updates the search tree data structure to exclude the M fingerprint codewords corresponding to each time step. For each matching segment, we record the time offset from the query segment. Following the matching process for the whole song, we compute the histogram of the number of matching segments for each offset. The peaks in this histogram correspond to offsets at which matches were found more often than others and can be used to pick out a set of repeating segments.	acoustic fingerprint;code word;computation;data structure;distance matrix;search tree;tree (data structure)	Regunathan Radhakrishnan;Wenyu Jiang	2012	Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference		speech recognition;computer science;machine learning;pattern recognition	DB	-7.058460708607578	-94.1888518893337	129367
3c7bbc3ca4cf3dda25d3e684a3005138aa8c5249	investigation of frame alignments for gmm-based text-prompted speaker verification		Frame alignments can be computed by different methods in GMM-based speaker verification. By incorporating a phonetic Gaussian mixture model (PGMM), we are able to compare the performance using alignments extracted from the deep neural networks (DNN) and the conventional hidden Markov model (HMM) in digit-prompted speaker verification. Based on the different characteristics of these two alignments, we present a novel content verification method to improve the system security without much computational overhead. Our experiments on the RSR2015 Part-3 digit-prompted task show that, the DNNbased alignment performs on par with the HMM alignment. The results also demonstrate the effectiveness of the proposed Kullback-Leibler (KL) divergence based scoring to reject speech with incorrect pass-phrases.	algorithm;artificial neural network;deep learning;experiment;frame language;google map maker;hidden markov model;kl-one;kullback–leibler divergence;markov chain;mixture model;overhead (computing);sequence alignment;speaker recognition;speech recognition;vocabulary	Yi Liu;Liang He;Jia Liu;Michael T. Johnson	2017	CoRR		mixture model;overhead (computing);speech recognition;computer science;artificial neural network;hidden markov model;numerical digit	NLP	-16.916646007071254	-90.15959687761722	129502
f07d8eadfcbf67b630a07849f74b5b71c42a9fd1	intoxicated speech detection by fusion of speaker normalized hierarchical features and gmm supervectors		Speaker state recognition is a challenging problem due to speaker and context variability. Intoxication detection is an important area of paralinguistic speech research with potential real-world applications. In this work, we build upon a base set of various static acoustic features by proposing the combination of several different methods for this learning task. The methods include extracting hierarchical acoustic features, performing iterative speaker normalization, and using a set of GMM supervectors. We obtain an optimal unweighted recall for intoxication recognition using score-level fusion of these subsystems. Unweighted average recall performance is 70.54% on the test set, an improvement of 4.64% absolute (7.04% relative) over the baseline model accuracy of 65.9%.	acoustic cryptanalysis;baseline (configuration management);database normalization;google map maker;heart rate variability;iterative method;test set	Daniel Bone;Matthew Black;Ming Li;Angeliki Metallinou;Sungbok Lee;Shrikanth (Shri) Narayanan	2011			speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition	NLP	-13.700158022065066	-89.56563752197346	129586
9b0caf9eb530e9b60c11fcf7929eb885e0ad476b	adapting to the speaker in automatic speech recognition	automatic speech recognition	Abstract   Many automatic speech recognisers work on the principle of matching incoming utterances to a library of stored voice templates. There are two main shortcomings of this approach, which can potentially be overcome by careful interface design. Firstly, the templates, collected under strictly controlled conditions, are not necessarily representative of the speaker's normal voice. Secondly, although the speaker's voice is likely to alter during the course of using the speech recogniser, the templates representing that voice will remain unchanged. This will result in a gradual lessening of the similarity of template and utterance.  In the context of an information-retrieval task using fully automatic speech recognition, attempts were made to overcome the above problems. It was found that a modified means of template formation, giving rise to more representative templates, could improve recognition figures, especially for female speakers. However, attempts at constantly updating the templates in accordance with drifts in the speaker's diction were ineffectual in this instance. This latter result conflicts with the results of earlier, comparable studies.		Mike Talbot	1987	International Journal of Man-Machine Studies	10.1016/S0020-7373(87)80008-1	voice activity detection;natural language processing;speaker recognition;speaker diarisation;speech recognition;computer science;voice analysis	Arch	-9.956094985473262	-80.98796779804645	129806
06fbce0205039aa9d0e14f92005ff408546817db	a feature-based approach to noise robust speech detection	speech feature extraction signal to noise ratio monitoring speech processing cepstral analysis			Dirk von Zeddelmann	2012			voice activity detection;linear predictive coding;speech recognition;acoustics;speech coding;pattern recognition;speech processing	NLP	-13.124828430700038	-90.22051618797032	130137
7d75349bfca9ad9ce81e5f51adc7a779da85c2dc	man-machine interaction using speech	man machine interaction	ed acoustic descriptions to an acoustically matching response. This is where the requirement for information at a higher level than the acoustic level arises, for out of all the pattern groupings that could be learned, only those that are meaningful are learned. Sutherland [145] argues the points involved in such a model of perception very cogently, for the visual case. We do not, at present, know enough about the process of speech perception to duplicate the feat. Sutherland’s model for visual perception, which is still highlv general at present, arose because, as a psychologist obtaining results from experiments on visual perception. he found that information from work on the neurophysiology of vision and on machine procedures for automatic pattern recognition (particularly Clowes [18, 19], but also well exemplified by Guzman [ 48, 49]) could be combined into a model of visual pattern recognition which offered great explanatory power in terms of the results obtained. A start on the explanation of speech perception may yet well involve a similar interdisciplinary approach, based on the same combination of psychology, machine methods, and neurophysiology. An appreciation of the need for such an explanation, on the part of those attempting machine perception of speech, and an appreciation of the source of relevant information, including related work in the field of visual pattern recognition, is probably essential to real progress in machine descriptions of speech patterns, of which the feature extraction problem is but part. (d) Models and Analysis The development within this subsection will follow, as far as possible, the lines of the section on models and synthesis [Section 4.2.2(b)] in an attempt to show the relationships, and illustrate more clearly some of the unexplored areas. The picture is complicated somewhat by the need to account for a time scale which can no longer be chosen arbitrarily, and which varies considerably; by the ingenious variations that have proved possible, in describing components of the models involved, in specific realizations; and by the fact that, whatever model is adopted, there ultimately must be some form of segmentation, which interferes with any attempt at clear distinctions between the approaches, in the former terms. Also physiologicallv based approaches are almost unrepresented. The very simplest model of speech, then, assumes a series of concatenated units. In the analysis case these may be interpreted as segments of the waveform which can he identified. Since segmentation at some stage is inevitable, this approach hardly differs from that of waveform matching, except that by trying to match shorter sections. the process should presumably be somewhat easier. An approach which identified phonemes on the basis of autocorrelation analysis of waveform segments would be in this category, but no significant devices appear in the literature. Such an approach would almost certainly require reliable direct segmentation. Other approaches to analysis attempt to retrieve the ingredients that would need to be used, by some particular model, in producing the unknown utterance. These ingredients may be used as descriptors of segments at particular levels in order to classify them. Almost always, segments compatible with phonemic analysis are used at some stage. although for convenience in computer analysis the raw data may initially consist of shorter segments arising from some fixed sampling DAVID R. HILL 204 MAN-MACHINE INTERACTION USING SPEECH 205 scheme. Early approaches to automatic speech recognition worked almost entirely in the domain of the acoustic analog, and the simplest version thereof. The approach is characterized by some form of spectrographic analysis, giving a two-dimensional array of data points representing energy intensity at some time and frequency. Time, frequency, and intensity are usually quantized to have a small number of discrete values—usually two for intensity, energy present or absent. Thus analysis produced a binary pattern which could be matched against stored patterns derived from known words, and a decision made as to which of the stored patterns most nearly resembled the unknown input, hence naming the input. This was the method of Sebesteyen [130], Uhr and Vossler [154, 155], Purton [120]—who actually uses multi-tap autocorrelation analysis rather than filter analysis, Shearme [137], Balandis [3]—who actually uses a mechanical filter system, and Denes and Matthews [24], as well as others. A rather similar kind of analysis may be obtained in terms of zero-crossing interval density or reciprocal zero-crossing interval density, the latter being closely related to frequency analysis [127, 128, 132, 139]. The major difficulties with either analysis lie in the time and frequency variability of speech cues. Time-normalization on a global basis (squashing or stretching the time scale to fit a standard measure) assumes, for example, that a longer utterance has parts that are all longer by the same percentage, which is not true. One simple way around the difficulty was adopted by Dudley and Balashek [28]. They integrated with respect to time for each of ten selected spectral patterns, and based their word decision on matching the ten-element “duration-of-occurrence” pattern, for an unknown input, against a master set. The more usual approach adopted has been to “segment” the input in some way, usually into phonemic segments, so that a series of (supposedly significant) spectral patterns, or spectrally based phonetic elements, results. Segmentation is either indirect—segments beginning when a named pattern is first detected and ending when it is no longer detected—or it is based on significant spectral change, much as suggested by Fant [see Section 5.2.2(b)]. Segmentation is frequently two-stage in computer-based approaches, the short (10 msec) raw data segments due to the sampling procedure being lumped to give the larger segments required. Vicens [158] describes one such scheme of great significance, using three stages, followed by “synchronization” of the input segments detected with the segments of “candidate” recognition possibilities stored in memory. His approach may be considered a “head-on” attack on the related problems of segmentation and time normalization and is the only scheme with a demonstrated ability to handle words in connected speech. The work is important for other reasons, as well. Other examples of the indirect approach include Olson and Belar [109], Bezdel [5], Fry [43], and Denes [22]—who also built in specific linguistic knowledge for error correction, and Scarr [133]—who incorporated an interesting scheme for amplitude normalization of the input speech. Examples of the direct approach include Gold [45]—who included other segmentation clues as well, Ross [125]—who included some adaptation, Traum and Torre [150], and Sakai and Doshita [128]. Needless to say either approach requires a further stage of recognition, to recogize words. Sakai and Doshita, and Bezdel used mainly zero-crossing measurements in their schemes. This may he significant in building machines entirely from digital components. At this stage. the different approaches become harder to disentangle. Individual segments. usually phonemic in character, may be described partly in terms of the kind of parameters used by a parametric resonance analog synthesizer, and partly in terms of measures derived from these parameters, or from spectral attributes. Otten [112] and Meeker [101] both proposed that time parameters in a formant vocoder system should be adequate for recognition, but the results of any such approach are not generally available. Forgie and Forgie approached vowel and fricative recognition on the basis of formant values, fricative spectra and transient cues, which are related to such parameters, and were quite successful [39, 40]. They reported up to 93% correct on vowel recognition, and “equal to humans” on fricative recognition. Frick [42] pointed out the advisability of “not putting all the eggs in one basket,” stating the M.I.T. Lincoln Laboratory aim at that time as being to define a set of cues which might be individually unreliable but, in combination, could lead to a reliable decision. This really indicates the general feeling for the decade that followed. Such a philosophy clearly leads to combined approaches. which are still in vogue. Here, really, is the nub of the “feature extraction problem.” which now centers around the question of which particular blend of which features is best suited to describing individual segments. The not too surprising conclusion, according to Reddy, is that it depends on what particular kind of segment one is trying to classify. Thus in Reddy’s scheme [121], segments are broadly classified on the basis of intensity and zerocrossing data, and finer discrimination within categories is accomplished on the basis of relevant cues. This seems an important idea, spilling over from research on the problems of search in artificial intelligence (and is the one developed by Vicens). He emphasizes the point that much automatic speech recognition research has been directed at seeking a structure, in terms of which the problems might he formulated, rather than seeking the refutation of a model or hypothesis. He further remarks that lack of adequate means for DAVID R. HILL 206 MAN-MACHINE INTERACTION USING SPEECH 207 collecting and interacting with suitable data has held up this aspect of the work. In 1951 Jakobson, Fant. and Halle published their Preliminaries to Speech Analysis. This work. recently reprinted for the eighth time [72] has a continuing importance for those working in fields of speech communication. The idea of distinctive features is based on the idea of “minimal distinctions,” a term coined by Daniel Jones [74] to indicate that any lesser distinction (between two sound sequences in a language) would be inadequate to dis	acoustic cryptanalysis;array data structure;artificial intelligence;autocorrelation;binary pattern (image generation);color vision;concatenation;data descriptor;data point;error detection and correction;experiment;feature extraction;frequency analysis;heart rate variability;humans;interaction;jones calculus;limbo;machine perception;mechanical filter;memory segmentation;mike lesser;multi-tap;parametric oscillator;pattern recognition;quantization (signal processing);resonance;sakai project;sampling (signal processing);speech recognition;synchronization (computer science);vocoder;voice analysis;waveform;zero crossing	David R. Hill	1971	Advances in Computers	10.1016/S0065-2458(08)60632-4	natural language processing;speech recognition;computer science;speech analytics	NLP	-9.82648214617465	-83.3293604030714	130153
5ad5b8e35a624b0e290d83f87bed74e4936b5bd7	speech segmentation in synthesized speech morphing using pitch shifting	speech segmentation	This paper discusses the speech morphing process showing some limitations of using the directly obtained LPC and excitation parameters of speech. The algorithm here depends on changing the pitch of the source to match that of the target based on analyzing the speech signals to its basic components. Different experiments for changing the female to female, male to male, male to female and female to male speech were performed. Interesting results were obtained while dealing with children's speech. Difficulties of obtaining the pitch period were overcome but the obtained results have some diversity in the quality of performance even though the pitch has been changed correctly. The method for obtaining LPC and excitation used could be improved which could provide better results for this application.	algorithm;experiment;lpc;morphing;pitch (music);pitch shift;speech segmentation;speech synthesis	Allam Mousa	2011	Int. Arab J. Inf. Technol.		speech recognition;computer science;speech segmentation	ML	-9.6954168310219	-85.9197479806823	130432
be1cb43bc0c414015e1b78d27871345b0389f3f5	audio-to-visual conversion using hidden markov models	modelo markov oculto;interfase usuario;man machine interaction;modele markov cache;user interface;signal audio;taux erreur;hidden markov model;relacion hombre maquina;audio signal;man machine relation;senal video;signal video;multimedia communication;viseme;error rate;video signal;interface utilisateur;relation homme machine;communication multimedia;indice error;conversion;senal audio	We describe audio-to-visual conversion techniques for efficient multimedia communications. The audio signals are automatically converted to visual images of mouth shape. The visual speech can be represented as a sequence of visemes, which are the generic face images corresponding to particular sounds. Visual images synchronized with audio signals can provide user-friendly interface for man machine interactions. Also, it can be used to help the people with impaired-hearing. We use HMMs (hidden Markov models) to convert audio signals to a sequence of visemes. In this paper, we compare two approaches in using HMMs. In the first approach, an HMM is trained for each viseme, and the audio signals are directly recognized as a sequence of visemes. In the second approach, each phoneme is modeled with an HMM, and a general phoneme recognizer is utilized to produce a phoneme sequence from the audio signals. The phoneme sequence is then converted to a viseme sequence. We implemented the two approaches and tested them on the TIMIT speech corpus. The viseme recognizer shows 33.9% error rate, and the phoneme-based approach exhibits 29.7% viseme recognition error rate. When similar viseme classes are merged, we have found that the error rates can be reduced to 20.5% and 13.9%, respectably.	hidden markov model;markov chain	Soonkyu Lee;Dongsuk Yook	2002		10.1007/3-540-45683-X_60	natural language processing;audio mining;speech recognition;word error rate;computer science;machine learning;audio signal;viseme;user interface;hidden markov model	NLP	-15.546817386704932	-82.96150988803153	130662
f4d5b05936b94785dd7a91606695d3697299c01d	whispered-to-voiced alaryngeal speech conversion with generative adversarial networks		Most methods of voice restoration for patients suffering from aphonia either produce whispered or monotone speech. Apart from intelligibility, this type of speech lacks expressiveness and naturalness due to the absence of pitch (whispered speech) or artificial generation of it (monotone speech). Existing techniques to restore prosodic information typically combine a vocoder, which parameterises the speech signal, with machine learning techniques that predict prosodic information. In contrast, this paper describes an end-to-end neural approach for estimating a fully-voiced speech waveform from whispered alaryngeal speech. By adapting our previous work in speech enhancement with generative adversarial networks, we develop a speakerdependent model to perform whispered-to-voiced speech conversion. Preliminary qualitative results show effectiveness in re-generating voiced speech, with the creation of realistic pitch contours.		Santiago Pascual;Antonio Bonafonte;Joan Serrà;José Alejandro González	2018	CoRR		computer science;speech recognition;generative grammar;adversarial system;alaryngeal speech;expressivity;speech enhancement;naturalness;monotone speech;intelligibility (communication)	NLP	-10.896860580333746	-86.90028964890168	130697
1c86b3f2123910b40bf4c55bcfffa3ab7ab6fb8d	speaker recognition using local models	speaker recognition	Many of the problems arising in speech processing are characterized by extremely large training and testing sets, constraining the kinds of models and algorithms that lead to tractable implementations. In particular, we would like the amount of processing associated with each test frame to be sublinear (i.e., logarithmic) in the number of training points. In this paper, we consider smoothed kernel regression models at each test frame, using only those training frames that are close to the desired test frame. The problem is made tractable via the use of approximate nearest neighbors techniques. The resulting system is conceptually simple, easy to implement, and fast, with performance comparable to more sophisticated methods. Preliminary results on a NIST speaker recognition task are presented, demonstrating the feasibility of the method.	approximation algorithm;cobham's thesis;linux;smoothing;speaker recognition;speech processing	Ryan Rifkin	2003			speaker recognition;speaker diarisation;speech recognition;computer science;machine learning;pattern recognition	ML	-17.102364526372632	-90.66059362090911	130735
0a80c06e82fcc9a3c0df1e92c7800a8b8f87d344	"""intelligibilities of mandarin chinese sentences with spectral """"holes"""""""		The speech intelligibility of Mandarin Chinese sentences of various spectral regions, regarding band-stop conditions (one or two “holes” in the spectrum), was investigated through subjective listening tests. Results demonstrated significant effects on Mandarin Chinese sentence intelligibilities when a single or a pair of spectral holes was introduced. Meanwhile, it revealed the importance of the first and second formant (F1, F2) frequencies for the comprehension of Mandarin sentences. More importantly, the first formant frequencies played a more primary role rather than those of the second formants. Sentence intelligibilities declined evidently with the lacking of F1 frequencies, but the effect became small when the spectrum holes covered more than 50% of F1 frequencies, and F2 frequencies came into a major play in the intelligibility of Mandarin sentence.	intelligibility (philosophy);super robot monkey team hyperforce go!	Yafan Chen;Yong Xu;Jun Yang	2017			speech recognition;mandarin chinese;computer science	NLP	-10.169937387203712	-83.62782904022079	130872
e6eae43fa78325e1686ae81f0672e300e3801135	cochlear implant-like processing of speech signal for speaker verification		In this paper, we investigate the cochlear implant-like processing of speech signal in speaker verification. This processing was applied on each speech utterance, in the temporal domain, to reduce spectral information in the original speech signal and synthesize a new one, called cochlear implant-like spectrally reduced speech (SRS), only from low-bandwidth subband temporal envelopes of the original speech. Spectral analyses, performed on voiced speech frames, showed that despite of the spectral and perceptual reduction induced by the cochlear implant-like signal processing, the global shape of the shortterm spectral envelopes of the SRS signal is rather similar to that of the original speech signal. Although the SRS is synthesized only from low-bandwidth subband temporal envelopes of original speech signal, its use in a baseline GMM-UBM speaker verification system, with cellular telephone conversational speech of the Switchboard corpus (used in NIST SRE 2002), did not alter substantially the minimal DCF (detection cost function) of the system. Furthermore, using appropriate SRS signals made it possible to reduce the minimal DCF (5.7% relative reduction) of the system. The linear combination at the score level, with equal weights, of the baseline and the SRS-based systems could also help in reducing the minimal DCF.	baseline (configuration management);cochlear implant;design rule for camera file system;loss function;mobile phone;nist rbac model;signal processing;speaker recognition;telephone switchboard	Cong-Thanh Do;Claude Barras	2012			speech recognition;pattern recognition;speech processing;signal processing;artificial intelligence;utterance;computer science;cochlear implant	NLP	-12.546957648366073	-90.7990687830848	131094
ae5c4527f4e587b6d99a09195d5fba906b62a0e6	a comparative study of speaker adaptation techniques		In previous work, we showed how to constrain the estimation of continuous mixture-density hidden Markov models (HMMs) when the amount of adaptation data is small. We used maximum-likelihood (ML) transformation-based approaches and Bayesian techniques to achieve near native performance when testing nonnative speakers of the recognizer language. In this paper, we study various ML-based techniques and compare experimental results on data sets with recordings from nonnative and native speakers of American English. We divide the transformation-based techniques into two groups. In feature space techniques, we hypothesize an underlying transformation in the feature-space that results in a transformation of the HMM parameters. In model-space techniques, we hypothesize a direct transformation of the HMM parameters. In the experimental section we show how the combination of the best ML and Bayesian adaptation techniques result in significant improvements in recognition accuracy. All the experiments were carried out with SRI's DECIPHER TM speech recognition system [1][2].	bayesian network;decipher;experiment;feature vector;finite-state machine;hidden markov model;markov chain;speech recognition	Leonardo Neumeyer;Ananth Sankar;Vassilios Digalakis	1995			speech recognition;computer science	NLP	-18.112787312102174	-91.60476103452955	131309
397ed2ebdc6958aced8cd002c9bc745f68da4e18	generating expressive speech for storytelling applications	analyse parole;basic emotions expression;expressive prosody;child directed speech;speech synthesis;analyse linguistique;application software;analisis palabra;implementation;speech processing;speech analysis;speech synthesis child directed speech expressive prosody expressive speech speech analysis;natural languages;indexing terms;storytelling speech generation system expressive speech generation storytelling approach expressive speech synthesis basic emotions expression prosodic rules text to speech system;linguistic analysis;storytelling approach;expressive speech;analisis linguistico;prosodie;performance analysis;intelligent agent;text to speech;storytelling speech generation system;humans;sintesis palabra;text to speech system;speech analysis speech synthesis humans natural languages application software intelligent agent performance analysis speech processing;prosodic rules;implementacion;prosody;expressive speech synthesis;synthese parole;prosodia;expressive speech generation	"""Work on expressive speech synthesis has long focused on the expression of basic emotions. In recent years, however, interest in other expressive styles has been increasing. The research presented in this paper aims at the generation of a storytelling speaking style, which is suitable for storytelling applications and more in general, for applications aimed at children. Based on an analysis of human storytellers' speech, we designed and implemented a set of prosodic rules for converting """"neutral"""" speech, as produced by a text-to-speech system, into storytelling speech. An evaluation of our storytelling speech generation system showed encouraging results"""	bang file;eigen (c++ library);emoticon;fear, uncertainty and doubt;gnu variants;haar wavelet;hall-effect thruster;semantic prosody;software quality;speech synthesis;the netherlands society for statistics and operations research	Mariët Theune;K. Meijs;Dirk Heylen;R. Ordelman	2006	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2006.876129	application software;speech recognition;index term;computer science;linguistics;prosody;natural language;implementation;speech synthesis	NLP	-16.01831593789991	-84.96341393781576	131339
3a33d36257a40d180bef5385c8586fb618fc1161	accurate and compact large vocabulary speech recognition on mobile devices		In this paper we describe the development of an accurate, smallfootprint, large vocabulary speech recognizer for mobile devices. To achieve the best recognition accuracy, state-of-the-art deep neural networks (DNNs) are adopted as acoustic models. A variety of speedup techniques for DNN score computation are used to enable real-time operation on mobile devices. To reduce the memory and disk usage, on-the-fly language model (LM) rescoring is performed with a compressed n-gram LM. We were able to build an accurate and compact system that runs well below real-time on a Nexus 4 Android phone.	acoustic cryptanalysis;acoustic model;artificial neural network;computation;deep learning;finite-state machine;language model;mobile device;n-gram;real-time clock;real-time transcription;speech recognition;speedup;vocabulary	Xin Lei;Andrew W. Senior;Alexander Gruenstein;Jeffrey S. Sorensen	2013			speech recognition;computer hardware;computer science;machine learning	Mobile	-18.378796624138005	-88.4557213323264	131642
ea01dc310a7ec8a037ab7e6ab0d19c0d0cb0f106	optimal speech recognition using phone recognition and lexical access	speech recognition		lexicon;speech recognition	Andrej Ljolje;Michael Riley	1992			artificial intelligence;speech recognition;voice activity detection;speech analytics;phone;pattern recognition;speaker recognition;computer science	NLP	-15.466283246372486	-86.38655740876445	131705
7ecd2bada46ea12a5d678c80dd41396aac7e4e5c	topic in dialogue: prosodic and syntactic features	languages and literatures	We investigate the relationship between phonetic phrasing, tonal pattern and phrase structure in left peripherical sentence topic. Our corpus consists of three task-oriented Italian dialogues. The results of prosodic analysis show that topics are usually associated to the highest pitch values in the Tone Unit, regardless to their actual syntactic position. Syntactic analysis shows that, while topic phrase structure is rather variable, topic function is quite stable, i.e., topics have mostly circumstantial-locative function, and less frequently subject function. Finally, phonetic phrasing, prominence placement and phrase structure shows clearly regular relationships.	phrase structure rules;text corpus	Claudia Crocco;Renata Savy	2007			natural language processing;speech recognition;computer science;linguistics	NLP	-11.876465579374232	-80.28786538932977	131768
f072971e2439fb998763bde0c0f51ce4479c08d6	on the importance of super-gaussian speech priors for pre-trained speech enhancement		For enhancing noisy signals, pre-trained singlechannel speech enhancement schemes exploit prior knowledge about the shape of typical speech structures. This knowledge is obtained from training data for which methods from machine learning are used, e.g., Mixtures of Gaussians, nonnegative matrix factorization, and deep neural networks. If only speech envelopes are employed as prior speech knowledge, e.g., to meet requirements in terms of computational complexity and memory consumption, Wiener-like enhancement filters will not be able to reduce noise components between speech spectral harmonics. In this paper, we highlight the role of clean speech estimators that employ super-Gaussian speech priors in particular for pretrained approaches when spectral envelope models are used. In the 2000s, such estimators have been considered by many researchers for improving non-trained enhancement schemes. However, while the benefit of super-Gaussian clean speech estimators in non-trained enhancement schemes is limited, we point out that these estimators make a much larger difference for enhancement schemes that employ pre-trained envelope models. We show that for such pre-trained enhancements schemes superGaussian estimators allow for a suppression of annoying residual noises which are not reduced using Gaussian filters such as the Wiener filter. As a consequence, considerable improvements in terms of Perceptual Evaluation of Speech Quality and segmental signal-to-noise ratios are achieved.	algorithm;artificial neural network;computational complexity theory;consistency model;deep learning;machine learning;mixture model;non-negative matrix factorization;pesq;requirement;signal-to-noise ratio;speech enhancement;video post-processing;wiener filter;zero suppression	Robert Rehr;Timo Gerkmann	2017	CoRR		speech recognition;computer science;pattern recognition	ML	-14.34602892835349	-91.42674265683311	131790
3ec8d19a983654749ce3bc9a730c5f4beb88536b	cost reduction of training mapping function based on multistep voice conversion	multistep voice conversion;speech synthesis;cost reduction;statistical method;voice conversion;training cost;cost function speech synthesis maximum likelihood estimation statistical analysis speech enhancement vector quantization;feature extraction;training cost reduction training mapping function multistep voice conversion statistical spectral mapping method correlation spectral features;speech synthesis feature extraction;multistep voice conversion voice conversion speech synthesis training cost	Several approaches based on a statistical method for voice conversion from one speaker to another have been developed. In a statistical spectral mapping method which is a typical one in these approaches, a mapping function which represents a correlation between different speakers is determined using spectral features. This technique has the problem that it is necessary to train the mapping function for each speaker pair. The training cost must become a serious issue in case that the number of speakers increases significantly. This paper describes a novel voice conversion method for reducing the training cost. This technique is easily implemented and can use conventional techniques directly. Experimental results demonstrate that the converted speech is almost maintaining the conventional quality despite the significant training cost reduction by the proposed method.		Tsuyoshi Masuda;Makoto Shozakai	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367007	voice activity detection;speech recognition;feature extraction;computer science;machine learning;speech synthesis	Robotics	-17.212635265271665	-91.87095151206404	132064
0797e3c1fe92d6766af23cf7c6e47ca05581d6d5	two-formant models of vowel perception: shortcomings and enhancement	formant;modele 2 formants;speech;voyelle;modele;parole;perception;vowel;models	The assessment of two-dimensional models of vowel perception is taken several stages further. Results are, prima facie, not encouraging: the formula proposed by Paliwal, Lindsay and Ainsworth [13] performs poorly at predicting the perceived upper formant (F2') values reported by Bladon and Fant [1]; and the formulae proposed by both these groups of authors are susceptible to an ambiguity error, whereby a calculated F2' can be shown to be associated with more than one vowel quality. However a key to unlock these (and other) difficulties may be found in the notion of auditory spectral integration of vowel formant energy within a broad bandwidth of approximately 3.5 Bark. On this assumption, nonlinearities in perceived vowel quality, of exactly the types observed, would be predicted as vowels fall under, or escape from, an integration skirt. If therefore the broad-band auditory integration hypothesis is built into existing two-formant perceptual models, their prognosis is greatly enhanced.		Anthony Bladon	1983	Speech Communication	10.1016/0167-6393(83)90047-X	speech recognition;formant;speech;linguistics;perception	NLP	-9.84840756021675	-83.51901782040589	132091
d46372ce7b5103235acf530bc19953277cda1ab1	gaussian process dynamical models for hand gesture interpretation in sign language	gaussian process dynamical model;gesture interpretation;hidden markov model;sign language;dynamic model;machine learning;gaussian process;artificial neural network;tk electrical engineering electronics nuclear engineering	Classifying human hand gestures in the context of a Sign Language has been historically dominated by Artificial Neural Networks and Hidden Markov Model with varying degrees of success. The main objective of this paper is to introduce Gaussian Process Dynamical Model as an alternative machine learning method for hand gesture interpretation in Sign Language. In support of this proposition, the paper presents the experimental results for Gaussian Process Dynamical Model against a database of 66 hand gestures from the Malaysian Sign Language. Furthermore, the Gaussian Process Dynamical Model is tested against established Hidden Markov Model for a comparative evaluation. A discussion on why Gaussian Process Dynamical Model is superior over existing methods in Sign Language interpretation task is then presented. 2011 Elsevier B.V. All rights reserved.	artificial neural network;dynamical system;gaussian process;gesture recognition;hidden markov model;jaishankar menon;matlab;machine learning;markov chain;neural networks;os-tan;ocean observatories initiative;parallel computing;problem domain;sunway	Nuwan Gamage;Ye Chow Kuang;Rini Akmeliawati;Serge N. Demidenko	2011	Pattern Recognition Letters	10.1016/j.patrec.2011.08.015	speech recognition;sign language;computer science;artificial intelligence;machine learning;pattern recognition;gaussian process;artificial neural network;hidden markov model	AI	-15.927570087325442	-87.74070593605074	132196
3954034e09bca89193a33f9b95b4313810b4b679	likelihood-based non-uniform allocation of gaussian kernels in scalar dimension for hmm compression	resource management database;gaussian kernels;kernel;gaussian mixture;non uniform allocation;scalar dimension;gaussian processes;speech recognition gaussian processes hidden markov models;kullback leibler divergence;resource manager;resource management;speech;kullback leibler divergence based allocation;hmm compression;training data;likelihood based nonuniform allocation;hidden markov models;gaussian kernel;speech recognition;hmm compressioin;kernel hidden markov models resource management training data speech recognition data models speech;speech recognition likelihood based nonuniform allocation gaussian kernels scalar dimension hmm compression kullback leibler divergence based allocation resource management database;likelihood speech recognition hmm compressioin gaussian kernels non uniform allocation;likelihood;data models	A new, likelihood-based non-uniform allocation of Gaussian kernels in scalar (feature) dimension is proposed to compress complex, Gaussian mixture-based, continuous density HMMs into computationally efficient, small footprint models. Different from the objective of the previously proposed Kullback-Leibler divergence-based (KLD-based) allocation (Li et al., 2005), which is to make a better representation of the original model, the objective of the likelihood-based approach is to make the current compressed model be a better representation of the training data. It is implemented based on the unequal likelihood contributions of different features with uniform representation resolutions. Our experiments on the resource management database show that likelihood-based allocation outperforms uniform allocation and KLD-based non-uniform allocation due to its better representation of the training data.	algorithmic efficiency;experiment;hidden markov model;kullback–leibler divergence	Xiao-Bing Li;Douglas D. O'Shaughnessy	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607544	data modeling;training set;kernel;speech recognition;computer science;speech;resource management;machine learning;pattern recognition;gaussian process;likelihood function;kullback–leibler divergence;gaussian function;statistics	Robotics	-18.458075172963976	-92.796837526219	132205
016b3d2611ba1c4f4185982846a66ba71630a75d	compact acoustic models for embedded speech recognition	signal image and speech processing;acoustics;acoustic modeling;mathematics in music;engineering acoustics;speech recognition	Speech recognition applications are known to require a significant amount of resources. However, embedded speech recognition only authorizes few KB of memory, few MIPS, and small amount of training data. In order to fit the resource constraints of embedded applications, an approach based on a semicontinuous HMM system using state-independent acoustic modelling is proposed. A transformation is computed and applied to the global model in order to obtain each HMM state-dependent probability density functions, authorizing to store only the transformation parameters. This approach is evaluated on two tasks: digit and voice-command recognition. A fast adaptation technique of acoustic models is also proposed. In order to significantly reduce computational costs, the adaptation is performed only on the global model (using related speaker recognition adaptation techniques) with no need for state-dependent data. The whole approach results in a relative gain of more than 20% compared to a basic HMM-based system fitting the constraints.	acoustic cryptanalysis;acoustic model;computation;embedded system;graph embedding;hidden markov model;semi-continuity;speaker recognition;speech recognition	Christophe Lévy;Georges Linarès;Jean-François Bonastre	2009	EURASIP J. Audio, Speech and Music Processing	10.1155/2009/806186	speaker recognition;speech recognition;acoustics;computer science;speech processing;acoustic model;physics	Mobile	-17.6535975846655	-91.0053867157578	132225
14ccdb3498742d6e02eb5b9da87d5b83467f53ea	a spectral lf model based approach to voice source parameterisation		This paper presents a new method of extracting LF model based parameters using a spectral model matching approach. Strategies are described for overcoming some of the known difficulties of this type of approach, in particular high frequency noise. The new method performed well compared to a typical time based method particularly in terms of robustness against distortions introduced by the recording system and in terms of the ability of parameters extracted in this manner to differentiate three discrete voice qualities. Results from this study are very promising for the new method and offer a way of extracting a set of non-redundant spectral parameters that may be very useful in both recognition and synthesis systems.	distortion	John Kane;Mark Kane;Christer Gobl	2010			robustness (computer science);artificial intelligence;speech recognition;pattern recognition;computer science	Robotics	-10.0782390243782	-89.74020298801193	132322
594246e58c77f1ed08419f0a37dee77ef6d18785	laryngoscopic analysis of pharyngeal articulations and larynx-height voice quality settings	voice quality	"""Using fibreoptic laryngoscopy to observe pharyngeal articulations, the aryepiglottic sphincter mechanism i s shown to be responsible for the production of speech sounds in the phonetic category """"pharyngeal."""" Major differences in auditory/acoustic quality are also produced when the larynx as a whole is raised or lowered during the production of pharyngeals. The voiceless pharyngeal fricative and voiced pharyngeal approximant are the result of increased sphincteric constriction of the laryngeal """"tube"""" in a continuum that begins with normal glottal stop and ventricular fold closure. A pharyngeal stop is produced when the aryepiglottic sphincter mechanism achieves complete closure, and trilling accompanying friction is evident at the pharyngeal place of articulation in both voiceless and voiced modes. It i s suggested that all five sounds share a common, pharyngeal place of articulation, but differ in manner of articulation. Raised larynx is the default setting for these articulations, but they may be produced with lowered larynx. 1. PHARYNGEAL SOUNDS Pharyngeals occur as discrete phonemes or as a secondary characteristic where a series of sounds is modified by the presence of a pharyngeal posture. Linguistic phonetic realizations of pharyngeal gestures include Semitic pharyngeals, pharyngeals in Caucasian languages, glottalization in North American languages (Salish and Wakashan), laryngealization in West African languages, implosives and ejectives, a feature of tone in Vietnamese and of segmental articulation in Danish, the [-ATR] vowel harmony series in West African languages, """"strident"""" vowels in Khoisan phonology, and the pharyngealized voice quality in a number of singing styles that have been analyzed phonetically. Laufer and Condax [1] and Laufer and Baer [2] have demonstrated that native-speakers of Arabic and of Oriental Hebrew produce a voiceless fricative and a voiced approximant from a stricture behind the epiglottis. Butcher and Ahmad [3] confirm that the voiceless Arabic pharyngeal is a fricative and that the voiced Arabic pharyngeal is an approximant, but that the latter is most often realized as a stop. Catford [4,5,6] uses the term """"epiglottopharyngeal"""" to refer to these sounds and to fricative, approximant, stop and """"possibly"""" trill sounds in the Caucasian languages investigated by Kodzasov [7,8]. Kodzasov also observes that the larynx is typically raised in the production of pharyngeals. This possibility is supported by El-Halees [9] and by Stephen Jones who found in early radiographic studies of Somali pharyngeals not only that the larynx was elevating but that there also appeared to be some sort of vibration around the epiglottis during some articulations [10]. There is additional evidence that larynx raising may be inherent in pharyngeal articulations. Esling, Heap, Snell and Dickson [11] demonstrate that there is no auditory perceptual distinction between pharyngealized voice and raised larynx voice at certain pitches, and that pharyngealized voice i s likely to be perceived when pitch is low while raised larynx voice is likely to be perceived when pitch is high. Esling [12] presents evidence that the pharyngeal articulator i s responsible for the production of both pharyngealized voice and what Laver terms raised larynx voice [13]. Negus [14], Gauffin [15], Roach [16] and Painter [17] present detailed insights into supraglottal strictures, and Yanagisawa, Estill, Kmucha, and Leder [18] and Honda, Hirai, Estill and Tohkura [19] provide evidence that the epilaryngeal tube is elevated during many singing styles. What is unanswered here is to demonstrate the phonetic relationship between these singing styles and pharyngealized voice, and between larynx raising and the various possible manners of pharyngeal articulation."""	acoustic cryptanalysis;apache continuum;biconnected component;el-fish;jones calculus;poor posture;radiography;thomas m. baer	John H. Esling	1998			phonology;glottalization;glottal stop;speech recognition;vowel harmony;manner of articulation;place of articulation;linguistics;pharyngeal stop;epiglottis;computer science	Web+IR	-9.94642880330752	-82.10444296391469	132376
3f1573a13fda35542c809586784a928cc6edf80f	comparing multilayer perceptron to deep belief network tandem features for robust asr	belief networks;training mel frequency cepstral coefficient speech recognition noise measurement accuracy signal to noise ratio;multilayer perceptrons;training;robust asr;multilayer perceptron;deep belief network;noise measurement;mel frequency cepstral coefficient;automatic speech recognition;accuracy;hidden markov models;multilayer perceptron automatic speech recognition deep belief network;mlp network;speech recognition;hmm system;speech recognition belief networks hidden markov models multilayer perceptrons;signal to noise ratio;mismatched noise conditions multilayer perceptron belief network tandem feature robust asr hmm system deep belief network dbn mlp network speech recognition;dbn;mismatched noise conditions;belief network;belief network tandem feature	In this paper, we extend the work done on integrating multilayer perceptron (MLP) networks with HMM systems via the Tandem approach. In particular, we explore whether the use of Deep Belief Networks (DBN) adds any substantial gain over MLPs on the Aurora2 speech recognition task under mismatched noise conditions. Our findings suggest that DBNs outperform single layer MLPs under the clean condition, but the gains diminish as the noise level is increased. Furthermore, using MFCCs in conjunction with the posteriors from DBNs outperforms merely using single DBNs in low to moderate noise conditions. MFCCs, however, do not help for the high noise settings.	bayesian network;deep belief network;hidden markov model;memory-level parallelism;multilayer perceptron;noise (electronics);speech recognition	Oriol Vinyals;Suman V. Ravuri	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947378	speech recognition;computer science;noise measurement;machine learning;pattern recognition;bayesian network;accuracy and precision;multilayer perceptron;deep belief network;signal-to-noise ratio	Robotics	-15.20352835035759	-90.81439389964854	132501
44f4c74fad0c401b042e7090e865aa90080a690c	korean traditional music genre classification using sample and midi phrases			midi	Jong-Seol Lee;MyeongChun Lee;Dalwon Jang;Kyoungro Yoon	2018	TIIS	10.3837/tiis.2018.04.026	distributed computing;speech recognition;midi;computer science	Vision	-15.431845632109379	-85.8625595807074	132532
d038bdd5505368ff3ae64dc23a8a3eed917e0580	cepstral features for classification of an impulse response with varying sample size dataset	feature extraction training mel frequency cepstral coefficient games signal processing polynomials;sample size;polynomial regression;statistical analysis pattern classification polynomials regression analysis speech processing;training;polynomials;feature vector;mel frequency cepstral coefficient;feature extraction;signal processing;games;retrieval system cepstral features impulse response varying sample size dataset speech characterisation audio characterisation feature vector cepstral polynomial regression recursive algorithm recursive formulation sequential learning framework isolate racket hits audio stream tennis video clip average normalised modified retrieval rank anmrr statistical properties;impulse response	Cepstrum-based features have proved useful in audio and speech characterisation. In this paper a feature vector of cepstral polynomial regression is introduced for the detection and classification of impulse responses. A recursive algorithm is proposed to compute the feature vector. This recursive formulation is appealing when used in a sequential learning framework. The discriminative power of these features to detect and isolate racket hits from the audio stream of a tennis video clip is discussed and compared with standard cepstrum-based features. Finally, a new formulation of the Average Normalised Modified Retrieval Rank (ANMRR) is proposed that exhibits relevant statistical properties for assessing the performance of a retrieval system.	algorithm;cepstrum;coefficient;computation;cyclic redundancy check;feature vector;pattern recognition;polynomial;racket;receiver operating characteristic;recursion (computer science);stationary process;streaming media;video clip	Cyril Hory;William J. Christmas	2007	2007 15th European Signal Processing Conference		speech recognition;machine learning;pattern recognition;mathematics	Vision	-9.664129157102451	-92.3547082494822	132557
51211a562b7fe797f852655fc2bd924177864ab4	hyperarticulation detection in repetitive voice queries using pairwise comparison for improved speech recognition		Automatic speech recognition systems can benefit from cues in user voice such as hyperarticulation. Traditional approaches typically attempt to define and detect an absolute state of hyperarticulation, which is very difficult, especially on short voice queries. We present a novel approach for hyperarticulation detection using pairwise comparisons and demonstrate its application in a real-world speech recognition system. Our approach uses delta features extracted from a pair of repetitive user utterances. Results show significant improvements in WER (word error rate) by using hyperarticulation information as a feature in a second pass N-best hypotheses rescoring setup.	speech recognition;word error rate	Ranjitha Gurunath Kulkarni;Ahmed El Kholy;Ziad Al Bawab;Noha Alon;Imed Zitouni;Umut Ozertem;Shuangyu Chang	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953105	voice activity detection;word error rate;artificial intelligence;computer science;feature extraction;pattern recognition;speech recognition;pairwise comparison	Vision	-13.010612780740152	-89.59976681714073	132558
84d9a879e9b4dded074bd2fecd8b24b1502a16e7	examination of the grooving patterns of the czech trill-fricative	tongue grooving;trill fricative;articulation;czech;ema	The Czech trill-fricative, /r/̝, is typologically rare among the world's languages. The present study used electromagnetic articulography (EMA) to examine the cross-sectional morphology during the production of the trill-fricative /r/̝ compared to the plain trill /r/ and sibilant fricatives /ʃ, ʒ, s, z/. Data collected from 5 native speakers of Czech show that the coronal shape of the tongue for the trill-fricative is flat, similar to that of the plain apical trill and the post-alveolar fricatives, but different from the highly grooved alveolar fricatives. However, toward the tip of the tongue, the trill-fricative is somewhat more grooved than the posterior region. This may help facilitate frication during trilling. The results also indirectly suggest that lateral tongue bracing is important for the articulation of trills. Furthermore, contrary to some previous descriptions in the literature, /r/̝ is more similar to post-alveolars than alveolars, and exhibits /ʒ/-like articulatory characteristics. & 2015 Elsevier Ltd. All rights reserved.	biconnected component;cross-sectional data;galaxy morphological classification;lateral thinking;matlab;mathematical morphology;technical support	Phil Howson;Alexei Kochetov;Pascal van Lieshout	2015	J. Phonetics	10.1016/j.wocn.2015.01.002	speech recognition;communication	AI	-9.293381098852675	-81.87425004720527	132802
ef8fdc4eb3a4717ef8dcf017388e4df515b1f514	a higher-dimensional expansion of affective norms for english terms for music tagging		The Valence, Arousal and Dominance (VAD) model for emotion representation is widely used in music analysis. The ANEW dataset is composed of more than 2000 emotion related descriptors annotated in the VAD space. However, due to the low number of dimensions of the VAD model, the distribution of terms of the ANEW dataset tends to be compact and cluttered. In this work, we aim at finding a possibly higher-dimensional transformation of the VAD space, where the terms of the ANEW dataset are better organised conceptually and bear more relevance to music tagging. Our approach involves the use of a kernel expansion of the ANEW dataset to exploit a higher number of dimensions, and the application of distance learning techniques to find a distance metric that is consistent with the semantic similarity among terms. In order to train the distance learning algorithms, we collect information on the semantic similarity from human annotation and editorial tags. We evaluate the quality of the method by clustering the terms in the found high-dimensional domain. Our approach exhibits promising results with objective and subjective performance metrics, showing that a higher dimensional space could be useful to model semantic similarity among terms of the ANEW dataset.	algorithm;cluster analysis;dominance drawing;machine learning;relevance;semantic similarity;voice activity detection	Michele Buccoli;Massimiliano Zanoni;György Fazekas;Augusto Sarti;Mark B. Sandler	2016			speech recognition;affect (psychology);multimedia;computer science	Web+IR	-7.413527587147284	-86.77165069392916	132969
0c7008602606fb86afcb9e6e14b997bc6cbf7e0c	sidechain harmonic enhancement of noise corrupted speech for hearing impaired listeners	gain;auditory system;speech;speech enhancement;noise measurement;signal to noise ratio;harmonic analysis	This work presents a single channel speech enhancement approach aimed at improving speech clarity for hearing impaired listeners under challenging listening conditions. The proposed method applies nonlinear distortions to speech components isolated from the observed noisy signal using aggressive speech enhancement. The enhanced components are then mixed back into the noisy signal. The results show that the proposed approach significantly improves speech clarity in noise.	distortion;nonlinear system;speech enhancement	Kamil K. Wójcicki;Kelly Fitz;Karrie Recker;Don Reynolds;Tao Zhang	2015	2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)	10.1109/WASPAA.2015.7336926	voice activity detection;speech recognition;acoustics;gain;noise measurement;speech;harmonic analysis;speech processing;mathematics;linguistics;signal-to-noise ratio;intelligibility;physics	ML	-9.803499513029566	-88.78075807452741	133288
166b1fb5614ebf317281be91060de198ec7bd113	automatic classification of singing voice quality	decision support systems signal classification pattern classification statistical analysis neural nets rough set theory;neural nets;rough set theory;voice quality;frequency statistical analysis speech analysis rough sets biomechanics intelligent systems signal analysis linear predictive coding paper technology multimedia systems;feature vector;statistical analysis;decision support systems;rough sets singing voice quality classification parameterization process voice source formant analysis fisher statistic feature vector decision systems neutral networks;signal classification;pattern classification;neutral network;rough set;automatic classification	In the paper problems related to the classification of singing voice quality are presented. For this purpose a database consisting of singers' sample recordings is constructed and parameters are extracted from recorded voice of trained and untrained singers. The parameterization process is based on both voice source and formant analysis of a singing voice. These parameters are explained as to their physical interpretation and analyzed statistically in order to diminish their number. The statistical analysis is based on the Fisher statistic. In such a way a feature vector of a singing voice is formed. Decision systems based on neutral networks and rough sets are utilized in the context of the voice type and voice quality classification. Results obtained in the automatic classification performed by both decision systems are compared. A possibility to classify automatically type/quality of voice is judged. The methodology proposed provides means for discerning trained and untrained singers.	feature vector;fisher information;physical symbol system;rough set	Bozena Kostek;Pawel Zwan	2005	5th International Conference on Intelligent Systems Design and Applications (ISDA'05)	10.1109/ISDA.2005.28	speech recognition;rough set;decision support system;computer science;machine learning;pattern recognition;artificial neural network	Robotics	-7.96044184892882	-90.01685053028808	133457
9ac7bd19d245ce740bccee31c0939725a014ed73	synthesis of breathy and rough voices with a view to validating perceptual and automatic glottal cycle pattern recognition		The framework of the presentation is the assessment of the ability of human raters or speechprocessing software to detect glottal cycles in speech sounds and measure their lengths in synthetic breathy and rough voices. The synthesis of hoarse voices designates the generation of speech sounds the timbre of which simulates the voice quality of dysphonic speakers. The added value of synthetically generated test stimuli is that the user may fix and know their properties exactly. The corpus comprises synthetic vowels [a] combining seven levels of frequency jitter and three levels of additive noise. The presentation is focused on the simulation of rough and breathy voices via frequency modulation of the glottal excitation model and addition of pulsatile noise at the glottis. Furthermore, the genuine glottal cycle lengths and glottal source to noise ratios are obtained to which lengths and ratios inferred via signal processing may be compared. The glottal cycle lengths are acquired by tracking the phase of the harmonic driving functions of the speech sound synthesizer. Actual glottal signal-to-noise ratios are measured by summing separately over the sound stimuli the squared clean volume velocity and pulsatile noise samples.	additive white gaussian noise;modulation;pattern recognition;signal processing;signal-to-noise ratio;simulation;synthetic intelligence;utility functions on indivisible goods;velocity (software development)	Samia Ben Elhadj Fraj;Francis Grenez;Jean Schoentgen	2011			computer vision;speech recognition;computer science;communication	EDA	-8.720688176325762	-86.6376765416474	133481
3a19a9d7cad8d78b7340e6d1bed1623562725d23	unit fusion for concatenative speech synthesis	speech synthesis	An important problem in concatenative synthesis is the occurence of spectral discontinuities or “concatenation mismatch” between sonorant speech units. In this paper, we present an approach to reduce concatenation mismatch by combining spectral information from two sequences of speech units selected in parallel. Concatenation units, on one hand, define initial spectral trajectories for a target utterance. Fusion units, on the other hand, define the desired transitions between concatenated units. The two unit sequences are “fused” by imposing dynamic constraints defined by the fusion units on the spectral trajectories of the concatenation units. To regenerate the modified speech units, we use a synthesis algorithm based on sinusoidal + all-pole analysis of speech, which overcomes the limitations of residual-excited LPC. Results from a perceptual test show that our method is highly successful at removing concatenation artifacts in speech generated from an inventory of diphones.	algorithm;concatenation;concatenative synthesis;speech synthesis	Johan Wouters;Michael W. Macon	2000			speech recognition;artificial intelligence;pattern recognition;fusion;computer science;speech synthesis	NLP	-10.802220246033686	-87.92463301632131	133576
45c86513c55cdca34328049040f67eb2296b5e8c	subband feature statistics normalization techniques based on a discrete wavelet transform for robust speech recognition	discrete wavelet transforms;histograms;robust speech recognition;discrete wavelet transform;frequency modulation;working environment noise;random variables;modulation spectral bands subband feature statistics normalization discrete wavelet transform speech recognition temporal domain feature sequence mean and variance normalization histogram equalization;noise robustness;higher order statistics;temporal domain feature sequence;wavelet transforms;cepstral analysis;modulation spectral bands;feature extraction;speech recognition;discrete wavelet transforms speech recognition cepstral analysis noise robustness higher order statistics frequency modulation histograms working environment noise random variables wavelet transforms;subband feature statistics normalization;mean and variance normalization;noise robust features;histogram equalization;speech recognition discrete wavelet transforms feature extraction;speech recognition discrete wavelet transform noise robust features	This letter proposes a novel scheme that applies feature statistics normalization techniques for robust speech recognition. In the proposed approach, the processed temporal-domain feature sequence is first decomposed into nonuniform subbands using the discrete wavelet transform (DWT), and then each subband stream is individually processed by well-known normalization methods, such as mean and variance normalization (MVN) and histogram equalization (HEQ). Finally, we reconstruct the feature stream with all of the modified subband streams using the inverse DWT. With this process, the components that correspond to more important modulation spectral bands in the feature sequence can be processed separately.	database normalization;discrete wavelet transform;histogram equalization;modulation;speech recognition	Jeih-Weih Hung;Hao-Teng Fan	2009	IEEE Signal Processing Letters	10.1109/LSP.2009.2024113	frequency modulation;random variable;computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;histogram;mathematics;discrete wavelet transform;histogram equalization;statistics;wavelet transform	ML	-12.62981547140751	-92.81611793725264	133599
d80aefaf90fd593d6eb1ef544fa8ebb84c76eb74	feature domain compensation of nonstationary noise for robust speech recognition	fixed interval smoothing;robust speech recognition;time varying;nonstationary noise;kalman filter;state estimation;speech recognition;state space model;fixed interval;interacting multiple model	One of the key issues in practical speech recognition is to achieve robustness against the environmental mismatches resulting from the background noises or different channels. Most of the conventional approaches have tried to compensate for the effects of such mismatches based on the assumption that the environmental characteristics are stationary, which, however, is far from the real observation. In this paper, we propose an approach to cope with time-varying environmental characteristics. With a direct modeling of the environment evolution process and the clean speech feature distribution, we construct a set of multiple linear state space models. Suboptimal state estimation under the given model structure can be efficiently performed with the interacting multiple model (IMM) algorithm. In addition to providing a comprehensive description of the compensation technique, we propose an adaptive Kalman filtering approach with which nonstationary noise evolution characteristics can be tracked. Moreover, we propose a novel way to do fixed-interval smoothing within the IMM framework. Performance of the presented compensation technique in both the slowly and rapidly varying noise conditions is evaluated through a number of continuous digit recognition experiments.	speech recognition	Nam Soo Kim	2002	Speech Communication	10.1016/S0167-6393(01)00013-9	kalman filter;speech recognition;computer science;state-space representation;machine learning;control theory	ML	-13.509043397114137	-93.85889241628291	133738
05f0ab1edb1313ab3633ef1602ac307a0117be75	efase: expressive facial animation synthesis and editing with phoneme-isomap controls	motion data;expressive facial animation synthesis;synthesized facial motion;expressive facial animation;emotion modifier;novel facial animation synthesis;phoneme-isomap control;realistic expressive facial animation;motion node;deleting motion node;processed facial motion node;finite elements;subdivision surfaces;facial animation;polar decomposition;cost function	"""This paper presents a novel data-driven system for expressive facial animation synthesis and editing. Given novel phoneme-aligned speech input and its emotion modifiers (specifications), this system automatically generates expressive facial animation by concatenating captured motion data while animators establish constraints and goals. A constrained dynamic programming algorithm is used to search for best-matched captured motion nodes by minimizing a cost function. Users optionally specify """"hard constraints"""" (motion-node constraints for expressing phoneme utterances) and """"soft constraints"""" (emotion modifiers) to guide the search process. Users can also edit the processed facial motion node database by inserting and deleting motion nodes via a novel phoneme-Isomap interface. Novel facial animation synthesis experiments and objective trajectory comparisons between synthesized facial motion and captured motion demonstrate that this system is effective for producing realistic expressive facial animations."""	algorithm;concatenation;dynamic programming;emotion markup language;experiment;isomap;loss function	Zhigang Deng;Ulrich Neumann	2006			computer vision;facial motion capture;real-time computing;simulation;computer facial animation;polar decomposition;computer science;artificial intelligence;finite element method;computer animation;multimedia;programming language;subdivision surface;computer graphics (images)	Graphics	-15.35330735865193	-81.80724750063759	133764
64b570a8451a1a0aac45f216177b1fd7f7b9ec96	czech language database of car speech and environmental noise		This paper will present new Czech language twochannel (stereo) speech database recorded in car environment. The created database was designed for experiments with speech enhancement for communication purposes and for the study and the design of a robust speech recognition systems. It respects car noise environment which is currently at the top of the interest. Tools for automated phoneme labelling based on Baum-Welch re-estimation were designed. The noise analysis of the car background environment was done.	autonomous car;baum–welch algorithm;experiment;image noise;maxima and minima;speech enhancement;speech recognition;telephone line;welch's method	Petr Pollák;Josef Vopièka;Pavel Sovka	1999			speech recognition;natural language processing;computer science;environmental noise;czech;artificial intelligence	NLP	-14.906145879250339	-86.84042748976493	133844
ec5bb1d9b38073c7894d362567da832e4b2ecee5	semi-blind source separation with multichannel variational autoencoder		This paper proposes a multichannel source separation technique called the multichannel variational autoencoder (MVAE) method, which uses a conditional VAE (CVAE) to model and estimate the power spectrograms of the sources in a mixture. By training the CVAE using the spectrograms of training examples with source-class labels, we can use the trained decoder distribution as a universal generative model capable of generating spectrograms conditioned on a specified class label. By treating the latent space variables and the class label as the unknown parameters of this generative model, we can develop a convergence-guaranteed semi-blind source separation algorithm that consists of iteratively estimating the power spectrograms of the underlying sources as well as the separation matrices. In experimental evaluations, our MVAE produced better separation performance than a baseline method.	algorithm;autoencoder;baseline (configuration management);blind signal separation;generative model;semiconductor industry;source separation;spectrogram;variational principle	Hirokazu Kameoka;Li Li;Shota Inoue;Shoji Makino	2018	CoRR		machine learning;artificial intelligence;autoencoder;mathematics;generative model;blind signal separation;source separation;spectrogram;matrix (mathematics)	ML	-16.795980341620577	-93.90081878199776	133865
47dbd770adf14ab9f28b3549f5d2912c87332bec	wake-up-word spotting using end-to-end deep neural network system	neural networks;training;computer architecture;computational modeling;hidden markov models;logic gates;speech recognition	Deep neural networks (DNNs) have tremendously improved the performance of automatic speech recognition (ASR). On the other hand, end-to-end speech recognition system can achieve state-of-the-art performance using Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) and Connectionist Temporal Classification (CTC) method for unsegmented sequence data. In this paper, we therefor propose a lightweight wake-up-word (WUW) spotting system based on end-to-end DNN architecture, which is intended to provide a great balance between decoding speed, accuracy and model size. The objective is to introduce CTC framework on spotting process, and to enhance the system by WUW-oriented model training and refinement steps. We test the performance of the proposed architecture on a conversational telephone dataset which illustrate that the computation time can be significantly reduced without a significant decrease in the spotting accuracy.	algorithmic efficiency;artificial neural network;computation;connectionism;convolutional neural network;deep learning;end system;end-to-end encryption;end-to-end principle;experiment;long short-term memory;mobile phone;recurrent neural network;refinement (computing);speech recognition;super robot monkey team hyperforce go!;time complexity	Shilei Zhang;Wen Liu;Yong Qin	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900073	speech recognition;logic gate;computer science;artificial intelligence;machine learning;time delay neural network;computational model;artificial neural network;hidden markov model	Robotics	-17.564176992644473	-88.09273211655488	134022
435217e38709ca2238ed0330fc01d70fdd59fc14	database of volumetric and real-time vocal tract mri for speech science		We present the USC Speech and Vocal Tract Morphology MRI Database, a 17-speaker magnetic resonance imaging database for speech research. The database consists of real-time magnetic resonance images (rtMRI) of dynamic vocal tract shaping, denoised audio recorded simultaneously with rtMRI, and 3D volumetric MRI of vocal tract shapes during sustained speech sounds. We acquired 2D real-time MRI of vocal tract shaping during consonant-vowel-consonant sequences, vowelconsonant-vowel sequences, read passages, and spontaneous speech. We acquired 3D volumetric MRI of the full set of vowels and continuant consonants of American English. Each 3D volumetric MRI was acquired in one 7-second scan in which the participant sustained the sound. This is the first database to combine rtMRI of dynamic vocal tract shaping and 3D volumetric MRI of the entire vocal tract. The database provides a unique resource with which to examine the relationship between vocal tract morphology and vocal tract function. The USC Speech and Vocal Tract Morphology MRI Database is provided free for research use at http://sail.usc.edu/span/morphdb.	mathematical morphology;noise shaping;real-time clock;real-time transcription;resonance;spontaneous order;tract (literature)	Tanner Sorensen;Z.-I. Skordilis;Asterios Toutios;Yoon-Chul Kim;Yinghua Zhu;Jangwon Kim;Adam C. Lammert;Vikram Ramanarayanan;Louis Goldstein;Dani Byrd;Krishna S. Nayak;Shrikanth (Shri) Narayanan	2017			speech recognition;speech science;computer science;vocal tract	Vision	-7.91728923988519	-84.11069525184882	134036
b287dad7b19174585d550f5361bf7a36b242bb01	identification of pronunciation defects in spoken arabic language		The detection of vocal pathologies is one of the novelties addressing automatic speech processing. There are several intervening approaches that are based on features contained in an acoustic signal and on natural language processing techniques. However, up to our knowledge, these studies are not extended to detect phonemes that pose degraded speeches. In this paper, we propose a new method to detect mispronounced sounds. We are based on a phonetic-probabilistic modeling. The invented study accounts four fundamental tasks. The first task summarizes the calculation of the probabilistic-phonetic model referring to Arabic speech. The second one is dedicated to calculate the probabilistic-phonetic model appropriate to a speaker whose elocution is classified as pathological. Thirdly, we compare between the two previous models to distinguish two main classes: the input speech can be healthy or pathological. The fourth stage consists in introducing an original algorithm based on a phonetic modeling to generate problematic sounds and to evaluate the elocution of each speaker having voice pathologies by attributing them a language level. This task will be only applied if the input speech is pathological. The obtained results are satisfactory. We have attained a problematic-sound identification rate of 96%.		Naim Terbeh;Mounir Zrigui	2017		10.1007/978-981-10-8438-6_28	speech recognition;arabic;elocution;automatic speech;computer science;pronunciation	NLP	-10.727328741318985	-85.56753389116845	134100
04d68e74aef7d7c735ddad4b4de3187c40f5242f	automatic detection of disturbing robot voice- and ping pong-effects in gsm transmitted speech		This contribution reports about a method to automatically detect the disturbing Robot Voice and Ping Pong e ect which occur in GSM transmitted speech. Both e ects are caused by the frame substitution technique, recommended by the GSM standard: in these cases the transmitted speech may be modulated by a disturbing 50 Hz component. These modulations can be detected very easily in the frequency domain. By a framewise comparision of the modulation amplitude of an undisturbed clean speech signal with a test signal it is possible to locate the occurrence of Robot Voice and Ping Pong very precisely. Comparing human perception to the outcome of the proposed algorithm shows a high degree of correspondence.	algorithm;modulation;quasiperiodicity;robotic voice effects	Martin Paping;Thomas Fahnle	1997			speech recognition;robot;gsm;voice activity detection;ping (video games);computer science	Robotics	-7.109075444277231	-87.62926283873574	134141
b29177a8871634d462b525ec62fca1481ad6b6ba	if i had a hammer: design and theory of an electromagnetically-prepared piano		In an attempt to create alternative methods of both playing and studying vibrating strings, we have modeled and constructed a software-driven instrument for use in the electromagnetic excitation of an acoustic grand piano’s strings. Both a virtual and material version of such a system are discussed, along with some of the theory of operation. Applications to musical composition and expression, piano string characteristic identification, arbitrarily long sustained tones, and digital waveguide model calibration are presented, along with ideas for future experimentation and creation of new music and sound.	acoustic cryptanalysis;string (computer science)	Edgar Berdahl;Steven Backer;Julius O. Smith	2005			waveguide;theory of operation;calibration;piano;acoustics;hammer;musical composition;computer science	HCI	-7.972994727895953	-85.14686242894832	134202
569e12a9fd610e525804d2e5b219eb31df8c3a5c	text-independent writer recognition using multi-script handwritten texts	short handwritten texts;multi script environment;writer verification;writer identification;run length features	This paper presents a text-independent writer recognition method in a multi-script environment. 1 Handwritten texts in Greek and English are considered in this study. The objective is to recognize the writer of a 2 handwritten text in one script from the samples of the same writer in another script and hence validate the hypothesis 3 that writing style of an individual remains constant across different scripts. Another interesting aspect of our study is 4 the use of short handwritten texts which was implied to resemble the real life scenarios where the forensic experts, 5 in general, find only short pieces of texts to identify a given writer. The proposed method is based on a set of run6 length features which are compared with the well-known state-of-the-art features. Classification is carried out using 7 K-Nearest Neighbors (K-NN) and Support Vector Machines (SVM). The experimental results obtained on a 8 database of 126 writers with 4 samples per writer show that the proposed scheme achieves interesting performances 9 on writer identification and verification in a multi-script environment. 10	handwritten biometric recognition;k-nearest neighbors algorithm;performance;real life;support vector machine;whole earth 'lectronic link	Chawki Djeddi;Imran Siddiqi;Labiba Souici-Meslati;Abdellatif Ennaji	2013	Pattern Recognition Letters	10.1016/j.patrec.2013.03.020	natural language processing;speech recognition;computer science;artificial intelligence	Vision	-4.59467725827443	-89.59668411928448	134230
4f420305be34cd763cf03b6193f50b175495149d	coping with unseen data conditions: investigating neural net architectures, robust features, and information fusion for robust speech recognition		The introduction of deep neural networks has significantly improved automatic speech recognition performance. For realworld use, automatic speech recognition systems must cope with varying background conditions and unseen acoustic data. This work investigates the performance of traditional deep neural networks under varying acoustic conditions and evaluates their performance with speech recorded under realistic background conditions that are mismatched with respect to the training data. We explore using robust acoustic features, articulatory features, and traditional baseline features against both in-domain microphone channel-matched and channel-mismatched conditions as well as out-of-domain data recorded using farand near-microphone setups containing both background noise and reverberation distortions. We investigate feature-combination techniques, both outside and inside the neural network, and explore neural-network-level combination at the output decision level. Results from this study indicate that robust features can significantly improve deep neural network performance under mismatched, noisy conditions, and that using multiple features reduces speech recognition error rates. Further, we observed that fusing multiple feature sets at the convolutional layer feature-map level was more effective than performing fusion at the input feature level or at the neural-network output decision level.	acoustic cryptanalysis;artificial neural network;baseline (configuration management);deep learning;distortion;microphone;network performance;speech recognition	Vikramjit Mitra;Horacio Franco	2016		10.21437/Interspeech.2016-966	speech recognition;computer science;machine learning;pattern recognition	ML	-15.007303204549576	-90.37985949452867	134315
75b03157567efed1009c76eab6b0e85d8725c81b	speech emotion recognition based on gaussian mixture models and deep neural networks		Recognition of speaker emotion during interaction in spoken dialog systems can enhance the user experience, and provide system operators with information valuable to ongoing assessment of interaction system performance and utility. Interaction utterances are very short, and we assume the speaker's emotion is constant throughout a given utterance. This paper investigates combinations of a GMM-based low-level feature extractor with a neural network serving as a high level feature extractor. The advantage of this system architecture is that it combines the fast developing neural network-based solutions with the classic statistical approaches applied to emotion recognition. Experiments on a Mandarin data set compare different solutions under the same or close conditions.	algorithm;artificial neural network;deep learning;dialog system;emotion recognition;google map maker;high- and low-level;high-level programming language;mixture model;randomness extractor;speaker recognition;spoken dialog systems;super robot monkey team hyperforce go!;sysop;systems architecture;user experience	Ivan Tashev;Zhongqiu Wang;Keith Godin	2017	2017 Information Theory and Applications Workshop (ITA)	10.1109/ITA.2017.8023477	speech recognition;machine learning;pattern recognition;time delay neural network	NLP	-16.89995755029295	-87.5940844439617	134336
46d970d6a0d7d7c8e39a1f2f5dfeab959eb96544	speaker adaptive training: a maximum likelihood approach to speaker normalization	word error rate;wall street journal;phonetically relevant information;maximum likelihood;hidden markov models speech recognition maximum likelihood estimation acoustic signal processing speech processing;acoustic modeling;speech processing;vocabulary;acoustic signal processing;word error rate reduction speaker adaptive training maximum likelihood approach speaker normalization speaker independent speech recognizers parameter estimation acoustic models linear transformations acoustic parameters inter speaker variability parsimonious acoustic models phonetically relevant information wall street journal corpus multiple training speakers batch supervised adaptation large vocabulary speech recognition;loudspeakers hidden markov models speech recognition parameter estimation maximum likelihood estimation error analysis acoustic testing training data vocabulary robustness;maximum likelihood estimation;word error rate reduction;parsimonious acoustic models;inter speaker variability;acoustic parameters;acoustic testing;training data;error analysis;hidden markov models;speaker adaptive training;speaker independent;multiple training speakers;loudspeakers;wall street journal corpus;speaker independent speech recognizers;linear transformations;large vocabulary speech recognition;maximum likelihood approach;linear transformation;speech recognition;robustness;acoustic models;parameter estimation;batch supervised adaptation;speaker normalization	This paper describes the speaker adaptive training (SAT) approach for speaker independent (SI) speech recognizers as a method for joint speaker normalization and estimation of the parameters of the SI acoustic models. In SAT, speaker characteristics are modeled explicitly as linear transformations of the SI acoustic parameters. The effect of inter-speaker variability in the training data is reduced, leading to parsimonious acoustic models that represent more accurately the phonetically relevant information of the speech signal. The proposed training method is applied to the Wall Street Journal (WSJ) corpus that consists of multiple training speakers. Experimental results in the context of batch supervised adaptation demonstrate the effectiveness of the proposed method in large vocabulary speech recognition tasks and show that significant reductions in word error rate can be achieved over the common pooled speaker-independent paradigm.	acoustic cryptanalysis;acoustic model;database normalization;finite-state machine;occam's razor;programming paradigm;spatial variability;speech recognition;teaching method;the wall street journal;vocabulary;word error rate	Tasos Anastasakos;John W. McDonough;John Makhoul	1997		10.1109/ICASSP.1997.596119	natural language processing;speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition;speech processing;mathematics;linear map;maximum likelihood;statistics	ML	-18.289278235091125	-91.51552479874903	134351
560d6872ed68dc5665043951df48058fc63ae117	quantitative evaluation of dysarthria and development of vowel sound voice training system				Hiroyuki Maeda;Naoya Arisaka;Wakana Hata;Noritaka Mamorita;Ikuyo Ishizaka;Kazuhiko Yamashita;Harukazu Tsuruta	2015			voice training;dysarthria;vowel;speech recognition;audiology;psychology	HCI	-7.887574333725931	-84.30727746576265	134518
7ac68110269ce9d7f1e4988193cec9e651b10367	text-dependent speaker verification using dynamic time warping and vector quantization of lsf.	speaker verification;vector quantizer;dynamic time warping			J.-L. Bonifas;I. Hernaez Rioja;B. Etxebarria Gonzalez;S. Saoudi	1995			speech recognition;computer science;machine learning;dynamic time warping	HCI	-14.56955021891471	-88.05546151845243	134569
a847f38830394e6b14fc4e68947345329cbc4b90	music performer recognition using an ensemble of simple classifiers	music performance;reference point;machine learning;subjective evaluation	This paper addresses the problem of identifying the most likely music performer, given a set of perform ances of the same piece by a number of skilled candidate pianist s. We propose a set of features for representing the stylistic char acteristics of a music performer. A database of piano performances o f 22 pianists playing two pieces by F. Chopin is used in the pres ented experiments. Due to the limitations of the training set size and the characteristics of the input features we propose an ensemble of simple classifiers derived by both subsampling the training set and subsampling the input features. Preliminary experim nts show that the resulting ensemble is able to efficiently cope with this difficult musical task, displaying a level of accuracy unlike ly to be matched by human listeners (under the same conditions).	chroma subsampling;experiment;performance;test set	Efstathios Stamatatos;Gerhard Widmer	2002			speech recognition;computer science;machine learning	ML	-12.666286706925932	-89.44968137611522	134570
6c7169af4a3ddac526a233bda215779f44fa9fb7	a stroke based representation of indian sign language signs incorporating global and local motion information	assistive technology gesture recognition trajectory feature extraction shape vectors splines mathematics;sign language recognition;indian sign language;local motion;bspline approximated trajectory stroke based representation indian sign language recognition local motion information global motion information hand gestures facial expressions phonemic representation speech signals maximum curvature points mcp points speech impaired people;splines mathematics;approximation theory;key maximum curvature points kmcps;key frames;dynamic signs;global motion;natural language processing;stroke sequence;local motion key maximum curvature points kmcps dynamic signs indian sign language stroke sequence key frames global motion;splines mathematics approximation theory natural language processing sign language recognition	Sign Language is a visual gesture language used by speech impaired people to convey their thoughts and ideas with the help of hand gestures and facial expressions. This paper presents a stroke based representation of dynamic gestures of Indian Sign Language Signs incorporating both local as well as global motion information. This compact representation of a gesture is analogous to phonemic representation of speech signals. To incorporate the local motion of the hand, each stroke contains the features corresponding to the hand shape as well. The dynamic gesture trajectories are segmented based on Maximum Curvature Points(MCPs). MCPs are selected based on the direction change of trajectories. The frames corresponding to the MCP points of the trajectory are considered as the key frames. Local information features are taken as the hand shape of the Key frames. The existing methods of Sign Language Recognition has scalability problems apart from high complexity and the need for extensive training data. In contrast, our proposed method of stroke based representation has less expensive training phase since it only requires the training of stroke features and stroke sequences of each word. Our algorithms also address the issue of scalability. We have tested our approach in the context of Indian Sign Language recognition and we present the results from this study.	algorithm;key frame;scalability	M. Geetha;P. V. Aswathi;M. R. Kaimal	2013	2013 2nd International Conference on Advanced Computing, Networking and Security	10.1109/ADCONS.2013.51	computer vision;speech recognition;gesture recognition;mathematics;communication	Vision	-15.381042591625633	-81.94683986505865	134580
156b32b2c72fac5ed7edd1f878c8caa310005d17	the uptake of spectral and temporal cues in vowel perception is rapidly influenced by context		"""Speech perception is dependent on auditory information within phonemes such as spectral or temporal cues. The perception of those cues, however, is affected by auditory information in surrounding context (e.g., a fast context sentence can make a target vowel sound subjectively longer). In a two-by-two design the current experiments investigated when these different factors influence vowel perception. Dutch listeners categorized minimal word pairs such as /tɑk/-/ta:k/ (""""branch""""-""""task"""") embedded in a context sentence. Critically, the Dutch /ɑ/-/a:/ contrast is cued by spectral and temporal information. We varied the second formant (F2) frequencies and durations of the target vowels. Independently, we also varied the F2 and duration of all segments in the context sentence. The timecourse of cue uptake on the targets was measured in a printed-word eye-tracking paradigm. Results show that the uptake of spectral cues slightly precedes the uptake of temporal cues. Furthermore, acoustic manipulations of the context sentences influenced the uptake of cues in the target vowel immediately. That is, listeners did not need additional time to integrate spectral or temporal cues of a target sound with auditory information in the context. These findings argue for an early locus of contextual influences in speech perception."""	acoustic cryptanalysis;categorization;embedded system;experiment;eye tracking;locus;printing;programming paradigm;recurrent word;stellar classification	Eva Reinisch;Matthias J Sjerps	2013	J. Phonetics	10.1016/j.wocn.2013.01.002	psychology;speech recognition;acoustics;communication	HCI	-9.345082348239808	-81.29479449231285	134607
f3e2a7d0ec1662a2a59e7576f77d101a72255a6f	automatic derivation of hmm alternative pronunciation network topologies.	network topology		hidden markov model;network topology	Alessandro Falaschi;Massimo Pucci	1991			artificial intelligence;hidden markov model;pattern recognition;computer science;pronunciation;derivation;network topology	NLP	-19.046533925753952	-85.57276041433552	134665
a08d2d1f278310a8aca23754ecc56c4e6a9d0787	restoration of voiced speech signals preserving prosodic features	phoneme;speech restoration;frecuencia muestreo;erreur quadratique moyenne;restoration;sampling frequency;speech;fonema;frecuencia fundamental;senal vocal;algorithme;feasibility;algorithm;signal vocal;frequence echantillonnage;senal voceada;mean square error;prosodie;signal acoustique;signal voise;voiced signal;acoustic signal;error medio cuadratico;prosody;vocal signal;accent;frequence fondamentale;practicabilidad;faisabilite;fundamental frequency;senal acustica;prosodia;algoritmo	The paper deals with the restoration of voiced speech signals that contain clicks, noise, and gaps of sufficient length so that entire phonemes are lost. A particular focus is on restoring the proper prosodic elements: accent and stress. The importance of the problem is grounded by the fact that the meaning of some words (known as homographs) is solely dependent on the prosody. A new restoration method, exploiting the developed simple polynomial accent model with averaged speech signal characteristics of intensity and a fundamental frequency period as its parameters, is proposed. Feasibility of the method is confirmed by experimental investigations of the restoration of both one period and multiple periods of a voiced speech signal, examination of an instantaneous error, the total mean-square-error, and the influence of sampling frequency on the restoration quality.	circuit restoration	Sarunas Paulikas;Dalius Navakauskas	2005	Speech Communication	10.1016/j.specom.2005.05.002	feasibility study;speech recognition;speech;speech processing;mean squared error;linguistics;fundamental frequency;prosody;sampling	NLP	-8.259178270976625	-88.97656550119157	134676
3c40c13a5f868e11697ef6a27183631e177f2482	improved spoken term detection with graph-based re-ranking in feature space	spoken term detection;silicon;acoustics lattices silicon adaptation models speech recognition bismuth feature extraction;pseudo relevance feedback approach;lattices;pseudo relevance feedback;information retrieval;pseudo relevance feedback prf spoken term detection re ranking;bismuth;acoustics;pseudo relevance feedback prf;feature space;speech recognition information retrieval natural language processing;random walk;feature extraction;first pass retrieved utterance;speech recognition;local similarity relationship spoken term detection graph based reranking feature space first pass retrieved utterance global similarity structure pseudo relevance feedback approach;local similarity relationship;global similarity structure;adaptation models;re ranking;natural language processing;graph based reranking	This paper presents a graph-based approach for spoken term detection. Each first-pass retrieved utterance is a node on a graph and the edge between two nodes is weighted by the similarity between the two utterances evaluated in feature space. The score of each node is then modified by the contributions from its neighbors by random walk or its modified version, because utterances similar to more utterances with higher scores should be given higher relevance scores. In this way the global similarity structure of all first-pass retrieved utterances can be jointly considered. Experimental results show that this new approach offers significantly better performance than the previously proposed pseudo-relevance feedback approach, which considers primarily the local similarity relationship between first-pass retrieved utterances, and these two different approaches can be cascaded to provide even better results.	experiment;feature vector;relevance feedback	Yun-Nung Chen;Chia-Ping Chen;Hung-yi Lee;Chun-an Chan;Lin-Shan Lee	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947640	natural language processing;speech recognition;feature vector;feature extraction;computer science;machine learning;bismuth;pattern recognition;lattice;mathematics;silicon;random walk;statistics	Vision	-9.66266274279535	-93.2810309118339	134723
0b771778298718f1656c332beb931a6c452988c4	a kernel for time series based on global alignments	dynamic programming;elementary operator;global alignments;support vector machines;kernel methods;positive definite;dynamic program;time series;indexing terms;speech recognition time series global alignments speech data kernel methods support vector machine dynamic time warping dynamic programming techniques;speech data;science learning;kernel support vector machines dynamic programming speech recognition mathematics heuristic algorithms polynomials databases bioinformatics buildings;support vector machines kernel methods dynamic time warping speech recognition;pattern recognition;speech recognition;kernel method;dynamic programming techniques;support vector machine;dynamic time warping;time series dynamic programming speech recognition support vector machines	We propose in this paper a new family of kernels to handle time series, notably speech data, within the framework of kernel methods which includes popular algorithms such as the support vector machine. These kernels elaborate on the well known dynamic time warping (DTW) family of distances by considering the same set of elementary operations, namely substitutions and repetitions of tokens, to map a sequence onto another. Associating to each of these operations a given score, DTW algorithms use dynamic programming techniques to compute an optimal sequence of operations with high overall score, in this paper we consider instead the score spanned by all possible alignments, take a smoothed version of their maximum and derive a kernel out of this formulation. We prove that this kernel is positive definite under favorable conditions and show how it can be tuned effectively for practical applications as we report encouraging results on a speech recognition task.	algorithm;dynamic programming;dynamic time warping;kernel (operating system);kernel method;sequence alignment;smoothing;speech recognition;support vector machine;time series;whole earth 'lectronic link	Marco Cuturi;Jean-Philippe Vert;Øystein Birkenes;Tomoko Matsui	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366260	support vector machine;kernel method;speech recognition;kernel embedding of distributions;computer science;machine learning;dynamic time warping;pattern recognition;variable kernel density estimation	ML	-18.240984410037612	-93.47315684059443	134728
338c7fa83755780a1909e6e36f968e2890b67ad0	a method for calculating the spectral response of a hair cell to a pure tone	appareil auditif;estimulo acustico;response stimulus relation;mammalia;spectral response;cobaye;guinea pig;oido interno;cochlea;forma descarga;vertebrata;electrophysiology;percepcion;methode calcul;rodentia;sound pressure level;metodo calculo;cochlee;inner ear;oreille interne;celula ciliada;acoustic stimulus;analyse spectrale;organ of hearing;relation stimulus reponse;analisis espectral;audition;hair cell;audicion;electrofisiologia;cobayo;perception;spectral analysis;relacion estimulo respuesta;coclea;ciliated cell;electrophysiologie;cellule ciliee;stimulus acoustique;hearing;computing method;discharge pattern;aparato auditivo;mode decharge	Using Corey and Hudspeth's hair cell transducer function and that of Assad and Corey as models, the output of the haircell is calculated for an input of a single tone. These results are then compared with experimental data obtained by Hubbard et al. (1979) from guinea pig cochlear. The theoretical and experimental data are found to be similar for low to moderate sound pressure levels.	auditory hair cell;cavia porcellus;cochlear implant;cochlear structure;hubbard model;selective calling;transducer	Marian P. Regan	1994	Biological Cybernetics	10.1007/BF00198907	psychology;electrophysiology;neuroscience;acoustics;sound pressure;engineering;communication;perception	ML	-6.2276347835406405	-83.27901338347355	134786
6ce95ae672b34b9bf0ffdf6af8160ba2ba61ab0a	a survey about asr for children		This paper is intended to surv ey the state of the art of automatic speech recognition (ASR) for children’s speech. Investigating ASR for children is a current trend in research. Therefore databases of children’s speech are needed for training and testing of ASR systems. In the first part of this paper the most relevant databases of children’s speech are described. There are less speech data of children available than of adults and speech of preschool children is even more rarely available. In the second part of this paper the common techniques for recognizing children’s speech are summarized. Most investigations about children’s ASR focus on the acoustic model. The common methods are described and approaches regarding the lexical and speech model are mentioned subsequently. In an extensive literature research we collected papers investigating ASR for children. Several studies have been carried out investigating children’s ASR. Due to the lack of data from preschool children only a few investigations for this age group have been accomplished. This is illustrated by presenting a statistic on the age of the children in past studies.	acoustic cryptanalysis;acoustic model;automated system recovery;automatic system recovery;database;speech recognition	Felix Claus;Hamurabi Gamboa Rosales;Rico Petrick;Horst-Udo Hain;Rüdiger Hoffmann	2013			natural language processing;artificial intelligence;speech recognition;computer science	NLP	-17.75811337291556	-82.87526155921292	134804
64d02189ab64f8403e256975d2ad0e073bb03a6d	discriminative feature extraction for speech recognition in noise	speech recognition	Signal representation is crucial for designinga speechrecognizer. The feature extractor selects the information to be used by the classifier to perform the recognition. In noisy environments, the data vectors representing the speech signal are changed and the recognizer performance is degraded by two main facts: (1) the mismatch between the training and the recognition conditions and (2) the degradation of the signal to be recognized. In such a situation, the representation of the speech signal plays an important role. In this paper, we analyze the importance of the representation for speechrecognition in noise. We apply the Discriminative Feature Extraction (DFE) method to optimize the representation. The experiments presented in this work show that the DFE method, which has been successfully applied in clean environments, leads also to improvements of the speech recognizers in noise.	elegant degradation;experiment;feature extraction;finite-state machine;randomness extractor;speech recognition;statistical classification	Ángel de la Torre;Antonio M. Peinado;Antonio J. Rubio;Pedro García-Teodoro	1997			discriminative model;artificial intelligence;speech recognition;pattern recognition;feature extraction;feature (machine learning);extractor;computer science	ML	-14.69978012832852	-91.39794694541611	134862
fb0a338b012017777db3fa6fb494272d2cc279ac	signal modeling enhancements for automatic speech recognition	speech processing;automatic speech recognition cepstral analysis working environment noise frequency speech analysis signal to noise ratio finite impulse response filter speech recognition testing sampling methods;speech enhancement;spectral analysis speech enhancement speech recognition modelling smoothing methods;automatic speech recognition;smoothing methods;noise free environment signal modeling enhancements automatic speech recognition phoneme classification standard speech processing methods basis vector representations dynamic feature trajectories morphological smoothing dilation spectral features short analysis windows timit database;speech recognition;spectral analysis;signal to noise ratio	Obtaining a compact, information-rich representation of the speech signal is an important first step in ASR. A large majority of ASR systems use some form of cepstral coefficients for this purpose. Computation of these cepstral coefficients typically includes several of the following steps: (1) Highfrequency preemphasis, using an FIR filter of the form y(k) = x(k) ax(k-1), with a taking values around 0.95; (2) partition of the signal into analysis frames of 20 to 30 ms, spaced 5 to 10 ms apart; (3) computation of ten to forty cepstral coefficients using a cosine transform of the logarithm of the output of a 40-channel triangular filter bank, which is designed to approximate a Bark frequency scale; and (4) Feature vectors are assembled from the instantaneous cepstral values, augmented with some form of dynamic information, e.g. delta-cepstra. This paper describes several enhancements to this procedure. We show that significant improvements in recognition accuracy can be achieved by modifications in all of these steps, particularly for speech corrupted by noise. In particular, we show that 1. The first order high-frequency pre-emphasis should be replaced by a second order preemphasis of the form:	approximation algorithm;cepstrum;coefficient;computation;emphasis (telecommunications);filter bank;finite impulse response;image noise;speech recognition	Zaki B. Nossair;Peter L. Silsbee;Stephen A. Zahorian	1995		10.1109/ICASSP.1995.479821	voice activity detection;linear predictive coding;speech recognition;computer science;speech coding;pattern recognition;speech processing;acoustic model;signal-to-noise ratio	ML	-11.240501773356664	-87.91728013373415	134916
8824d7a031d3808acf36d08c5c98e6798e17f52d	整合邊際資訊於鑑別式聲學模型訓練方法之比較研究 (a comparative study on margin-based discriminative training of acoustic models) [in chinese]		鑑別式聲學模型訓練在近代自動語音辨識(Automatic Speech Recognition, ASR)中扮演 重要的角色。在許多基於不同思維且能有效地提昇辨識率的鑑別式聲學模型訓練方法陸 續被提出後,對於訓練方法的相關推廣與改進便如雨後春筍般地興起;而這些方法在本 質上,皆是在描述訓練語句與語音辨識器所產生對應詞圖(Word Graph)之間的關係。本 論文首先將統整與歸納近年來所發展的多種鑑別式聲學模型訓練方法,並以三種最具代 表性鑑別式訓練方法:最小化分類錯誤(Minimum Classification Error, MCE)、最大化交 互資訊(Maximum Mutual Information, MMI)、最小化音素錯誤(Minimum Phone Error, MPE)為範例,透過有系統地轉換與化解方程式,得到聲學模型訓練準則的共通表示函 數型態。我們可以發現到,對於上述鑑別式訓練方法,此共通表示函數背後物理意義之 差別乃是在於欲觀察訓練語料不同層級的鑑別資訊,如音素(Phone)、語句(Utterance)等, 以及共通表示函數之參數設定。其次,本論文針對語音辨識結果所形成的假設空間上所 觀察到錯誤(或正確)率的不同細緻層度,在模型訓練時引入了機器學習領域中的邊際概 念;其背後的物理意義,事實上就是從不同層級的訓練語料中選取適合的資訊供聲學模 型訓練所使用。本論文的目的在於分析近代對於以隱藏式馬可夫模型為聲學模型之模型 訓練方法與邊際概念在演進上的一致性;從琳瑯滿目的訓練方法之中,闡述近年來鑑別 式聲學模型訓練發展演進之中心思想。最後,我們實作於中文大詞彙連續語音辨識系統, 驗證了多種鑑別式聲學模型訓練方法以及我們所提出方法之效能。	acoustic cryptanalysis;mutual information;speech recognition	Yueng-Tien Lo;Berlin Chen	2010			discriminative model;pattern recognition;artificial intelligence;computer science	ML	-14.921739811183741	-87.5999373682204	134949
a2f350c6b1a0268779f29db835b208f175e291b6	speech emotion classification using acoustic features	feature extraction acoustics emotion recognition support vector machines speech accuracy speech recognition;emotion recognition;acoustic features;support vector machines acoustic signal processing cepstral analysis emotion recognition feature extraction gaussian processes mixture models signal classification speech recognition;support vector machine;support vector machine emotion recognition acoustic features;statistical functions speech emotion classification emotion recognition system feature representation feature extraction low level acoustic features segmental cepstral based features emotion dependent gaussian mixture models low level acoustic codewords gmm supervectors support vector machine svm classifier iemocap database four class emotion recognition accuracy	Emotion recognition from speech is a challenging research area with wide applications. In this paper we explore one of the key aspects of building an emotion recognition system: generating suitable feature representation. We extract features from four angles: (1) low-level acoustic features such as intensity, F0, jitter, shimmer and spectral contours etc. and statistical functions over these features, (2) a set of features derived from segmental cepstral-based features scored against emotion-dependent Gaussian mixture models, (3) a set of features derived from a set of low-level acoustic codewords and (4) GMM supervectors constructed by stacking the means or covariance or weights of the adapted mixture components on each utterance. We apply these features for emotion recognition independently and jointly and compare their performance within this task. We build a support vector machine (SVM) classifier based on these features on the IEMOCAP database. The four-class emotion recognition accuracy of 71.9% of our system outperforms the previously reported best results on this dataset.	acoustic cryptanalysis;cepstrum;code word;emotion recognition;focus stacking;high- and low-level;mixture model;support vector machine	Shizhe Chen;Qin Jin;Xirong Li;Gang Yang;Jieping Xu	2014	The 9th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2014.6936664	support vector machine;speech recognition;feature vector;feature;computer science;machine learning;pattern recognition	NLP	-15.684214928961243	-91.32580433855257	135338
8cce79721eb5daae7f94fcc69e7c9fae752923c6	a robust characterization of audio signals using the level of information content per chroma	constant q transform audio fingerprints chroma values chromagrams entropy;chroma values;audio signal processing;chromagrams;spectral analysis audio signal processing entropy fingerprint identification fourier transforms information retrieval music probability;probability;estimation method;information retrieval;probability density function;cbfp audio signal processing shannon entropy information content per chroma entropy chroma fingerprint spectral coefficients probability density function parzen window estimation method ecfp chromagram based audiofingerprint constant q transform afp;constant q transform;information content;fourier transforms;audio fingerprints;entropy;spectral analysis;music;fingerprint identification	In this paper we propose a new technique to characterize audio-signals. We use Shannon's Entropy to estimate the level of information content per chroma and we show that involving entropy contributes for a more robust audio characterization. A new audio-fingerprint (AFP) based on this feature is proposed in this paper which we have called Entropy-Chroma Fingerprint (ECFP). Two approaches were considered to estimate entropy; the first assumes the spectral coefficients distribute normally, while the second, estimates its probability density function (PDF) with the Parzen Windows Estimation method. We compared the robustness of the ECFP against the Chromagram-Based Audio-Fingerprint (CBFP) which is determined using the Constant Q Transform (CQT). Three thousand and five hundred AFPs were determined from songs of several genres. A subset of 350 songs were severely degraded and searched for using excerpts of 5 seconds for that matter. The ECFP determined assuming gaussianity on the PDF turned out to be much more robust than the CBFP. The ECFP determined assuming gaussianity is much faster to process than both, the CBFP and the ECFP determined with Parzen Windows and still more robust.	acoustic fingerprint;chroma feature;coefficient;kernel density estimation;microsoft windows;portable document format;self-information;shannon (unit)	Alain Manzo-Martinez;José Antonio Camarena Ibarrola	2011	2011 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)	10.1109/ISSPIT.2011.6151562	constant q transform;entropy;probability density function;speech recognition;audio signal processing;pattern recognition;probability;music;mathematics;statistics	Vision	-8.026187122175022	-92.75822482669659	135361
7b90758f43110a472d198edd284ca4bb8d25c789	efficient sparse banded acoustic models for speech recognition	hidden markov models computational modeling covariance matrices feature extraction acoustics sparse matrices data models;speech recognition inverse covariance matrix sparse banded models;covariance matrices;speech recognition;speech recognition covariance matrices sparse matrices;sparse matrices;cantonese data set sparse banded acoustic model speech recognition accuracy weighted lasso regularization wall street journal data set full covariance model	We propose sparse banded acoustic models to significantly improve the recognition accuracy and reduce the computational cost of speech recognition systems. The sparse banded models are trained using a weighted lasso regularization. In addition, we propose new feature orders to reduce the bandwidth of sparse banded models in order to speed up computation. Experimental results on the Wall Street Journal data set show that sparse banded models significantly outperform diagonal and full covariance models by 9.5% and 15.1% relatively. Sparse banded models also run the fastest. The advantages of sparse banded models are also demonstrated on the collected Cantonese data set.	acoustic cryptanalysis;algorithmic efficiency;artificial neural network;colour banding;computation;computational complexity theory;experiment;fastest;lasso;matrix regularization;sparse matrix;speech recognition;the wall street journal	Weibin Zhang;Pascale Fung	2014	IEEE Signal Processing Letters	10.1109/LSP.2013.2292920	speech recognition;sparse matrix;computer science;machine learning;pattern recognition;sparse approximation;mathematics	ML	-17.16442428665948	-92.3764813671915	135428
9a61538146c721a6672b8babe9cea11375d7f4ae	how many phonologies are there in one speaker? some experimental evidence.				Michael S. Ziolkowski;Mayumi Usami;Karen L. Landahl;Brenda K. Tunnock	1992			speech recognition;computer science	NLP	-14.272777736951156	-85.18354092868245	135438
f149b6695364c944e4241e2b7f03dbb8826e876e	analyzing the relation between overall quality and the quality of individual phases in a telephone conversation		Assessing and analyzing the quality of transmitted speech in a conversational situation is an important topic in current research. For this, a conversation has been separated into three individual conversational phases (listening, speaking, and interaction), and for each phase corresponding quality-relevant perceptual dimensions have been identified. The dimensions can be used to determine the quality of each phase, and the qualities of all phases, in turn, can be be combined for overall conversational quality estimation. In this article we present the work that has been conducted to identify the weights of the individual phases for the overall quality. For this, we conducted an experiment that allows the participants to perceive each phase separately and to gather the overall quality as well as the quality ratings for each individual phase. The results enable to create a linear model to predict the overall quality on the basis of the three phases. This allows to draw first conclusions regarding the relation between the individual phases and the overall quality and provides a major landmark towards a diagnostic assessment of conversational quality.	experiment;linear model	Friedemann Köster;Sebastian Möller	2016		10.21437/Interspeech.2016-255	speech recognition;conversation;computer science	HCI	-11.463620874813993	-83.9774805315424	135514
8ea06e171a930d3cac392bf5b0547d3c366c6f6c	a pitch synchronous feature extraction method for speaker recognition	speaker identification;publikationer;spectral mismatch;signal analysis;speech analysis;learning artificial intelligence speaker recognition feature extraction cepstral analysis;konferensbidrag;testing;distortion measurement;speaker recognition;mel frequency cepstral coefficient;feature extraction speaker recognition mel frequency cepstral coefficient speech analysis signal analysis testing character generation cepstral analysis distortion measurement speech recognition;cepstral analysis;pitch synchronous feature extraction method;feature extraction;character generation;artiklar;mel frequency cepstral coefficients;rapporter;speech recognition;cepstral distortion pitch synchronous feature extraction method speaker recognition closed set speaker identification mel frequency cepstral coefficients spectral mismatch pitch synchronous mfcc;cepstral distortion;learning artificial intelligence;closed set speaker identification;pitch synchronous mfcc	The paper presents a novel feature extraction method to improve the performance of speaker identification systems. The proposed feature has the form of a typical conventional feature, Mel frequency cepstral coefficients (MFCC), but a flexible segmentation to reduce spectral mismatch between training and testing processes. Specifically, the length and shift size of the analysis frame are determined by a pitch synchronous method, pitch synchronous MFCC (PSMFCC). To verify the performance of the new feature, we measure the cepstral distortion between training and testing and also perform closed set speaker identification tests. With text-independent and text-dependent experiments, the proposed algorithm provides 44.3% and 26.7% relative improvement, respectively.	algorithm;coefficient;distortion;experiment;feature extraction;mel-frequency cepstrum;pitch (music);speaker recognition	Samuel Kim;Thomas Eriksson;Hong-Goo Kang;Dae Hee Youn	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326008	speaker recognition;speech recognition;computer science;signal processing;pattern recognition;mel-frequency cepstrum	Robotics	-13.059585906578306	-91.8207523178785	135533
613bc10fe7cdc80938709380abe3129047294e25	speech enhancement using transient speech components	hidden markov tree;speech intelligibility;mdct;transient speech components;hidden markov chain;auditory system;snr;hidden markov chain model;speech enhancement;trees mathematics;speech perception;speech enhancement hidden markov models gaussian distribution wavelet coefficients wavelet transforms auditory system visual system inference algorithms testing speech analysis;human subjects;trees mathematics discrete cosine transforms hidden markov models speech enhancement;hidden markov models;wavelet based hidden markov tree model;discrete cosine transforms;word recognition;rhyme protocol;visual system;decompose speech;snr transient speech components speech enhancement decompose speech hidden markov chain model wavelet based hidden markov tree model mdct rhyme protocol	This paper describes an algorithm to decompose speech into tonal, transient, and residual components. The algorithm uses an MDCT-based hidden Markov chain model to isolate the tonal component and a wavelet-based hidden Markov tree model to isolate the transient component. We suggest that the auditory system, like the visual system, is probably sensitive to abrupt stimulus changes and that the transient component in speech may be particularly critical to speech perception. To test this suggestion, the transient component isolated by our algorithm was selectively amplified and recombined with the original speech to generate enhanced speech, with energy adjusted to be equal to the energy of the original speech. The intelligibility of the original and enhanced speech was evaluated in eleven human subjects by the modified rhyme protocol. The word recognition rates show that the enhanced speech can provide substantial improvement in speech intelligibility at low SNR levels (8% at -15 dB, 14% at -20 dB, and 18% at -25 dB)	algorithm;crossover (genetic algorithm);hidden markov model;intelligibility (philosophy);markov chain;modified discrete cosine transform;signal-to-noise ratio;speech enhancement;wavelet	Charturong Tantibundhit;J. Robert Boston;Ching-Chung Li;John D. Durrant;Susan Shaiman;Kristie Kovacyk;Amro El-Jaroudi	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660150	voice activity detection;linear predictive coding;speech recognition;speech perception;modified discrete cosine transform;visual system;word recognition;computer science;pattern recognition;signal-to-noise ratio;intelligibility;hidden markov model	Robotics	-9.0499648814709	-88.00858626161411	135534
885f8a4767c18229d3a77875433748487399882d	mechanisms producing recurring utterances in a patient with slowly progressive aphasia				Masato Kaneko	1994			speech recognition;communication;computer science;aphasia	NLP	-7.553011971852643	-83.20856909041164	135682
7cb0bd5f6cd800ac0dd99722ff3b896428f26018	discriminative training of gaussian mixture models for large vocabulary speech recognition systems	maximum mutual information;gaussian processes;gaussian processes speech recognition maximum likelihood estimation information theory;standard ml;maximum likelihood estimation;objective function;gaussian mixture model;speech recognition;discriminative training;vocabulary speech recognition maximum likelihood estimation parameter estimation training data hidden markov models context modeling production systems;maximum likelihood estimation discriminative training gaussian mixture models large vocabulary speech recognition systems parameter estimation mmi objective function ml estimation heuristic correction gaussian parameters maximum mutual information;information theory	1 ABSTRACT Two discriminative techniques are described (and evaluated) for estimating the parameters of the Gaussians in a large vocabulary speech-recognition system. The rst technique is based on using a modiication of the MMI objective function, and appears to provide no improvement over standard ML estimation. The second technique is based on a heuristic correction of the Gaussian parameters, and is seen to give a 2-5% improvement over ML estimation. 2 INTRODUCTION One common feature of many speech-recognition systems is that they are based on statistical methods i.e., the acoustic observations are modelled by probability-density-functions (pdf's), whose parameters are estimated statistically, from large amounts of training data. Typically, maximum-likelihood (ML) estimation is used; i.e., given the correct lexical transcription of the training data, the objective of the estimation is to maximize the likelihood of the training observations (feature vectors), conditioned on the correct lexical transcription. If y T 1 represents the stream of T acoustic observations, and w N 1 represents the correct lexical transcription (comprising of N words), the ML estimate for the parameters is ^ ML = arg max p (y T 1 =w N 1) : (1) It is well known that if the true distribution of the data lies in the space of the assumed family of distributions , and if suucient amount of training data is available, then the pdf's parametrized by the ML estimates , converge to the true distribution of the data 1]. An additional reason for the popularity of ML estimation is that an eecient algorithm (the EM algorithm 2]) exists for computing the ML estimates of parameters. Now, in most speech recognition systems, for reasons of simplicity, the family of distributions modelling the acoustic observations is assumed to be a mixture of Gaussians. In practice however, this assumption is not always valid. In such circumstances, it is not clear that choosing the parameters to maximize the likelihood of the training observations conditioned on the correct lexical transcription is the best strategy, and one might start considering alternative objective functions for estimating the model parameters. One such alternative strategy is maximum-mutual-information (MMI) estimation 3]. This estimation strategy tries to obtain estimates of the model parameters that maximize the probability of the correct lexical transcription given the acoustic observations, i.e., ^ MMI = arg max p (w N 1 =y T 1) (2) = arg max p (y T 1 =w N 1) …	acoustic cryptanalysis;converge;expectation–maximization algorithm;feature vector;heuristic;lexicon;loss function;mixture model;mutual information;optimization problem;portable document format;speech recognition;standard ml;transcription (software);vocabulary	Lalit R. Bahl;Mukund Padmanabhan;David Nahamoo;Ponani S. Gopalakrishnan	1996		10.1109/ICASSP.1996.543195	speech recognition;information theory;computer science;machine learning;pattern recognition;mixture model;gaussian process;mathematics;maximum likelihood;maximum likelihood sequence estimation;estimation theory;statistics	ML	-18.746900903451838	-91.95087922055637	135754
bded9757ef12dca3996d6a5ef8825dedd4d1a9db	a recurrent neural network for word identification from phoneme sequences	recurrent neural network		artificial neural network;recurrent neural network	Robert B. Allen;Candace A. Kamm;S. B. James	1990			artificial intelligence;pattern recognition;recurrent neural network;time delay neural network;computer science	ML	-15.716069730238674	-87.41238368236057	136005
b93eb486b3093712d99fde30b81d657228d64a62	speaker2vec: unsupervised learning and adaptation of a speaker manifold using deep neural networks with an evaluation on speaker segmentation.		This paper presents a novel approach, we term Speaker2Vec, to derive a speaker-characteristics manifold learned in an unsupervised manner. The proposed representation can be employed in different applications such as diarization, speaker identification or, as in our evaluation test case, speaker segmentation. Speaker2Vec exploits large amounts of unlabeled training data and the assumption of short-term active-speaker stationarity to derive a speaker embedding using Deep Neural Networks (DNN). We assume that temporally-near speech segments belong to the same speaker, and as such a joint representation connecting these nearby segments can encode their common information. Thus, this bottleneck representation will be capturing mainly speaker-specific information. Such training can take place in a completely unsupervised manner. For testing, our trained model generates the embeddings for the test audio, and applies a simple distance metric to detect speaker-change points. The paper also proposes a strategy for unsupervised adaptation of the DNN models to the application domain. The proposed method outperforms the state-of-the-art speaker segmentation algorithms and MFCC based baseline methods on four evaluation datasets, while it allows for further improvements by employing this embedding into supervised training methods.	algorithm;application domain;baseline (configuration management);deep learning;encode;neural networks;neural network software;pc speaker;speaker diarisation;speaker recognition;stationary process;test case;unsupervised learning	Arindam Jati;Panayiotis G. Georgiou	2017		10.21437/Interspeech.2017-1650	speech recognition;speaker diarisation;unsupervised learning;artificial intelligence;artificial neural network;pattern recognition;speaker recognition;segmentation;computer science	NLP	-16.98578749927085	-88.80868154514299	136057
1cea19ccafb754b50f134e8a09640e64d48e6e5d	recognition and feedback of vowel utterance with a good mouth shape based on sensing platysma muscle bulging		In public speaking, speakers are evaluated on verbal delivery and nonverbal delivery, and in particular, the mouth shape has an important role to support both of these. The mouth shape is mainly set during vowel utterance. We define the mouth shape, which can prompt the pronunciation of the speaker clearly and enrich the facial expression, as a good mouth shape in this research. The authors assume that a good mouth shape can be inferred from the bulging of the platysma muscle in the neck. We aim to support vowel utterances with a good mouth shape, and propose a system to recognize them. Specifically, we measure the uplift of the platysma muscle with photoreflectors and apply a machine learning method to implement a system to judge whether vowel utterances are being performed with a good shape. We conduct an accuracy measurement experiment of the proposed system and report the result. Finally, we describe the application that provides feedback of vowel utterances with a good mouth shape.	earth bulge;experiment;machine learning;photoreflector;regular expression	Yukihiro Nishimura;Tomoko Hashida	2018		10.1145/3174910.3174944	speech recognition;computer vision;artificial intelligence;nonverbal communication;utterance;vowel;facial expression;mouth shape;computer science;public speaking;pronunciation;platysma muscle	ML	-12.378856312325015	-83.42154728101102	136154
0686b7acb2adca86ba23d28c7061cfb703413c7d	auto-adaptive resonance equalization using dilated residual networks		In music and audio production, attenuation of spectral resonances is an important step towards a technically correct result. In this paper we present a two-component system to automate the task of resonance equalization. The first component is a dynamic equalizer that automatically detects resonances and offers to attenuate them by a user-specified factor. The second component is a deep neural network that predicts the optimal attenuation factor based on the windowed audio. The network is trained and validated on empirical data gathered from an experiment in which sound engineers choose their preferred attenuation factors for a set of tracks. We test two distinct network architectures for the predictive model and find that a dilated residual network operating directly on the audio signal is on a par with a network architecture that requires a prior audio feature extraction stage. Both architectures predict humanpreferred resonance attenuation factors significantly better than a baseline approach.	adaptive equalizer;artificial neural network;baseline (configuration management);deep learning;equalization (communications);feature extraction;flow network;network architecture;predictive modelling;real-time cmix;real-time transcription;resonance;window function	Maarten Grachten;Emmanuel Deruty;Alexandre Tanguy	2018	CoRR		residual;attenuation;network architecture;computer science;artificial neural network;speech recognition;feature extraction;resonance;equalization (audio);audio signal	ML	-14.9673311391194	-90.57362292516325	136490
282d602a01e1d6e83bed2c9b6757541697c4f79b	acoustic and perceptual characteristics of mandarin speech in homosexual and heterosexual male speakers		The present study investigated both acoustic and perceptual characteristics of Mandarin speech in homosexual and heterosexual male speakers. Acoustic analyses of monosyllabic words showed significant differences between the two groups in F0 features (including the mean, the max, and the range), F1 and F2 of vowels, aspiration/frication duration of consonants, and center of gravity as well as skewness for /s/. Especially, the patterns were found to be opposite between Mandarin and American English speakers, which might be due to social psychological differences between the two societies. The perceptual experiment showed that the perceived score of gayness differed significantly between the speeches of the two groups. Among those acoustic parameters showing significant differences, fricative duration may be the most salient cue for sexual orientation of Mandarin male speakers.	acoustic cryptanalysis;super robot monkey team hyperforce go!	Puyang Geng;Wentao Gu;Hiroya Fujisaki	2018		10.21437/Interspeech.2018-2225	speech recognition;mandarin chinese;perception;computer science	HCI	-10.126372792593212	-82.36870929208348	136596
70dc18bb6607e408ec1cd3f71c0fdac3534c288d	speech enhancement with lstm recurrent neural networks and its application to noise-robust asr	tk electrical engineering electronics nuclear engineering	We evaluate some recent developments in recurrent neural network (RNN) based speech enhancement in the light of noise-robust automatic speech recognition (ASR). The proposed framework is based on Long Short-Term Memory (LSTM) RNNs which are discriminatively trained according to an optimal speech reconstruction objective. We demonstrate that LSTM speech enhancement, even when used ‘näıvely’ as front-end processing, delivers competitive results on the CHiME-2 speech recognition task. Furthermore, simple, feature-level fusion based extensions to the framework are proposed to improve the integration with the ASR back-end. These yield a best result of 13.76 % average word error rate, which is, to our knowledge, the best score to date.	artificial neural network;automated system recovery;automatic system recovery;discriminative model;long short-term memory;neural networks;random neural network;recurrent neural network;speech enhancement;speech recognition;word error rate	Felix Weninger;Hakan Erdogan;Shinji Watanabe;Emmanuel Vincent;Jonathan Le Roux;John R. Hershey;Björn W. Schuller	2015		10.1007/978-3-319-22482-4_11	natural language processing;speech recognition;engineering;machine learning	NLP	-15.991131698865296	-89.51926646823054	136601
aa281b5bae382d87a2b8cb754d66652d8fcefa6a	interspeech pathology challenge: investigations into speaker and sentence specific effects		In this paper, we report our experiments on Interspeech 2012 Speaker Trait Pathology challenge task [2]. Specifically, we investigate two factors that impact the acoustic properties of the utterances collected in this task. Although the task treats utterances as independent data points, multiple utterances are recorded from individual speakers. Furthermore, the utterances correspond to readings of 17 given written sentences. In one experiment, we attempt to reduce variation due to speaker through dimensionality reduction. While these experiments showed promising results on development set, the performance did not translate to the evaluation test. In another, we learn classifiers conditioned on the sentences to capture sentence-specific signatures. This approach showed improved performance over the baseline on development set and the improvement translated to marginal gains on evaluation set. These experiments demonstrates the need to pay attention to the independence assumptions while collecting and defining clinical tasks.	acoustic cryptanalysis;baseline (configuration management);data point;dimensionality reduction;electronic signature;experiment;marginal model	Anthony P. Stark;Alireza Bayestehtashk;Meysam Asgari;Izhak Shafran	2012			speech recognition;natural language processing;computer science;sentence;artificial intelligence	NLP	-12.186552333905409	-85.30948055076685	136645
9414f3af27d68090045b7a0fe87475b5ac744cc9	the effect of visual information on word initial consonant perception of dysarthric speech	telephony;human perception;human factors;speech intelligibility;automatic speech recognition;testing;speech recognition;visual perception;speech perception	Disabled individuals will realize many benefits from automatic speech recognition. To date, most automatic speech recognition research has focused on normal speech. However, many individuals with physical disabilities also exhibit speech disorders. While limited research has been conducted focusing on dysarthric speech recognition, the preliminary results indicate that additional study is necessary. Recently, increasing attention has been given to multimodal speech recognition schemes that utilize multiple input sources most commonly audio and video. This multimodal approach has been applied to normal speech with demonstrated effectiveness. Through studying the effect of audio and visual information in a human perception experiment, this study attempts to discover whether such an approach would be useful for dysarthric speech recognition. Results of a closed vocabulary perception test are presented. In this test, 15 normal hearing viewers were presented with videotapes of three dysarthric speakers speaking a series of one syllable nonsense words. These words differed only in the initial consonant. The words were presented in both audio-only and audio-visual modes. Perception rates in both modes were measured. The results are analyzed and compared to other studies of visual speech perception and dysarthric speech articulation.	biconnected component;depth perception;multimodal interaction;speech recognition;speech synthesis;syllable;vocabulary	Richard P. Schumeyer;Kenneth E. Barner	1996			psychology;speech production;speech recognition;speech perception;acoustics;speech;motor theory of speech perception;speech shadowing;communication	HCI	-8.638375660345352	-82.43776755619612	136724
f1c54b27843391da30616e2eb88e8aa8d0b53f2d	segment duration in spoken korean		An experiment was carried out to investigate the contextual effects of linguistic features on segment duration in spoken Korean. Contextual effects such as the segmental effects of surrounding segments, the prosodic phrasal effects and the syllable structure were considered for the investigation. This paper concentrates on segment duration analysis of a newsreading speech style, using a corpus of 670 read sentences collected from one speaker of standard Korean. Classification and Regression Tree (CART) analysis was used to explore the relationship between the context features and the realised duration. Results showed that prosodic phrase features had significant effects on segment duration. Most of the prosodic phrase influences were attributed to the accentual phrase boundary lengthening effect. Vowels either preceding or following the [nasal] feature were significantly shortened. More shortening effects were observed in the nasals than in the homorganic voiced obstruents. The [stiff vocal fold] feature, which covers the aspirated obstruents and tense obstruents in Korean, shortened the following vowel considerably. Subsequent to these linguistic findings, duration models based on Sums-of-Products (SoP) models and CART models were built for text-to-speech conversion.	decision tree learning;netware file system;speech synthesis;syllable;text corpus	Hyunsong Chung	2002			artificial intelligence;speech recognition;pattern recognition;computer science	NLP	-11.8048128666323	-80.90912110322687	136753
0fd7466f4178798633cfd931bf1a9355a2515573	a unified dnn approach to speaker-dependent simultaneous speech enhancement and speech separation in low snr environments		We propose a unified speech enhancement framework to jointly handle both background noise and interfering speech in a speaker-dependent scenario based on deep neural networks (DNNs). We first explore speaker-dependent speech enhancement that can significantly improve system performance over speaker-independent systems. Next, we consider interfering speech as one noise type, thus a speaker-dependent DNN system can be adopted for both speech enhancement and separation. Experimental results demonstrate that the proposed unified system can achieve comparable performances to specific systems where only noise or speech interference is present. Furthermore, much better results can be obtained over individual enhancement or separation systems in mixed background noise and interfering speech scenarios. The training data for the two specific tasks are also found to be complementary. Finally, an ensemble learning-based framework is employed to further improve the system performance in low signal-to-noise ratio (SNR) environments. A voice activity detection (VAD) DNN and an ideal ratio mask (IRM) DNN are investigated to provide prior information to integrate two sub-modules at frame level and time-frequency level, respectively. The results demonstrate the effectiveness of the ensemble method in low SNR environments.	artificial neural network;catastrophic interference;deep learning;ensemble learning;information rights management;interference (communication);performance;signal-to-noise ratio;speech enhancement;voice activity detection	Tian Gao;Jun Du;Li-Rong Dai;Chin-Hui Lee	2017	Speech Communication	10.1016/j.specom.2017.10.003	voice activity detection;artificial intelligence;speech recognition;artificial neural network;computer science;training set;speech enhancement;background noise;pattern recognition;ensemble learning	AI	-15.446695014399245	-90.63840661941168	136762
619bb00975055de0af354f04d5bf1c1819ff486e	compass: coding and multidirectional parameterization of ambisonic sound scenes		Current methods for immersive playback of spatial sound content aim at flexibility in terms of encoding and decoding, abstracting the two from the recording or playback setup. Ambisonics constitutes such a method, that is however signal-independent, and at low spatial resolutions fails to provide appropriate spatialization cues to the listener, with potential severe colouration effects and localization ambiguity. We present a new signal-dependent method for parametric analysis and synthesis of ambisonic sound scenes that takes advantage of the flexibility of Ambisonics as a spatial audio format, while improving reproduction. The proposed approach considers a more general acoustic model than previous proposals, with multiple source signals and a non isotropic ambient component. According to a listening test using headphones, the method is perceived closer to binaural reference sound scenes than ambisonic playback.	acoustic cryptanalysis;acoustic model;binaural beats;compass;headphones;surround sound	Archontis Politis;Sakari Tervo;Ville Pulkki	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462608	headphones;immersion (virtual reality);ambiguity;computer vision;decoding methods;acoustic model;binaural recording;artificial intelligence;pattern recognition;ambisonics;computer science;active listening	Visualization	-8.24837348258675	-87.12799978987489	136970
5ff1027e03f45a8ad1166ef2980308a682dadf3c	learning pronunciation and accent from the crowd	pronunciation;crowd;language learning;accent	Learning a second language is becoming a more popular trend around the world. But the act of learning another language in a place removed from native speakers is difficult as there is often no one to correct mistakes nor examples to imitate. With the idea of crowd sourcing, we would like to propose an efficient way to learn a second language better.	crowdsourcing	Frederick Liu;Jeremy Chiaming Yang;Jane Yung-jen Hsu	2014			language acquisition;natural language processing;speech recognition;computer science	NLP	-18.52066253173691	-81.33111568549059	137108
cd72155f435f5fc1c52908b8b1bd74ea1e4e50b5	interactive voice technology development for telecommunications applications	word spotting;text to speech synthesis;environmental variables;technology development;speech synthesis;modelo markov;line adaptation;reconocimiento palabra;hidden markov model;man machine dialogue;speech processing;rule based;tratamiento palabra;traitement parole;interactive voice processing;systeme conversationnel;markov model;telecomunicacion;interactive system;telecommunication;sistema conversacional;speech recognition;environment compensation;dialogo hombre maquina;speech communication;sintesis palabra;reconnaissance parole;information system;modele markov;speaker adaptation;systeme information;synthese parole;dialogue homme machine;sistema informacion	This paper describes the essential speech processing techniques for interactive voice applications in the telecommunications field. These techniques include speech recognition and speech synthesis, both of which aim to make interactive speech communications between man and machine more natural. Keyword spotting, background noise effects reduction, and speakers and/or telephone adaptation techniques are considered essential in speech recognition in order to allow a more natural voice input as well as an adequate robustness against environmental variabilities. In the area of text-to-speech synthesis, we propose a rule-based synthesis method applicable to the Japanese language, aiming to produce high quality speech. The commercial system ANSER of a former project is also described as an example of an interactive speech processing system. Finally, a recently developed speech recognition server which includes a vocabulary-flexible recognition function is described. It is meant to illustrate the concept of the techniques it employs which allow its range of applications to be easily extended and also allow it to adapt itselt to the changes which are rapidly occurring in the telecommunications field.	data acquisition;display resolution;logic programming;online and offline;rapid prototyping;server (computing);speech processing;speech recognition;speech synthesis;telephone line;tomotaka takahashi;vocabulary	Jun-ichi Takahashi;Noboru Sugamura;Tomohisa Hirokawa;Shigeki Sagayama;Sadaoki Furui	1995	Speech Communication	10.1016/0167-6393(95)00029-N	speech recognition;computer science;speech;artificial intelligence;speech processing;markov model;speech synthesis;information system;hidden markov model	HCI	-16.260343638416064	-85.26828086153407	137219
0f9a96fcf9a66fa0caeaec410dae1153f18f51ef	a dynamic programming approach to the extraction of phrase boundaries from tempo variations in expressive performances		We present an approach to phrase segmentation that starts with an expressive music performance. Previous research has shown that phrases are delineated by tempo speedups and slowdowns. We propose a dynamic programming algorithm for extracting phrases from tempo information. We test two hypotheses for modeling phrase tempo shapes: a quadratic model, and a spline curve. We test the two models on phrase extraction from performances of entire classical romantic pieces namely, Chopin’s Preludes Nos. 1 and7. The algorithms determined 21 of the 26 phrase boundaries correctly from Arthur Rubinstein’s and Evgeny Kissin’s performances. We observe that not all tempo slowdowns signify a boundary (some are agogic accents), and multiple levels of phrasing strategies shoul d be considered for detailed interpretation analyses.	algorithm;dynamic programming;nos;performance;quadratic equation;spline (mathematics)	Ching-Hua Chuan;Elaine Chew	2007			artificial intelligence;machine learning;speech recognition;quadratic equation;computer science;dynamic programming;spline (mathematics);phrase	NLP	-12.424107463825468	-81.14396284744434	137775
e3bc75e285289e5e0979eaa239b0024eeb61f489	wavecyclegan: synthetic-to-natural speech waveform conversion using cycle-consistent adversarial networks		We propose a learning-based filter that allows us to directly modify a synthetic speech waveform into a natural speech waveform. Speech-processing systems using a vocoder framework such as statistical parametric speech synthesis and voice conversion are convenient especially for a limited number of data because it is possible to represent and process interpretable acoustic features over a compact space, such as the fundamental frequency (F0) and mel-cepstrum. However, a well-known problem that leads to the quality degradation of generated speech is an over-smoothing effect that eliminates some detailed structure of generated/converted acoustic features. To address this issue, we propose a syntheticto-natural speech waveform conversion technique that uses cycle-consistent adversarial networks and which does not require any explicit assumption about speech waveform in adversarial learning. In contrast to current techniques, since our modification is performed at the waveform level, we expect that the proposed method will also make it possible to generate “vocoder-less” sounding speech even if the input speech is synthesized using a vocoder framework. The experimental results demonstrate that our proposed method can 1) alleviate the over-smoothing effect of the acoustic features despite the direct modification method used for the waveform and 2) greatly improve the naturalness of the generated speech sounds.	acoustic cryptanalysis;automatic sounding;elegant degradation;mel-frequency cepstrum;natural language;smoothing;speech processing;speech synthesis;synthetic data;synthetic intelligence;vocoder;waveform	Kou Tanaka;Takuhiro Kaneko;Nobukatsu Hojo;Hirokazu Kameoka	2018	CoRR		adversarial system;fundamental frequency;parametric statistics;naturalness;waveform;speech synthesis;artificial intelligence;pattern recognition;computer science	ML	-10.859279092969349	-86.95559803831833	137828
b387e2b7df763f38be547891ee1c807949d3695c	contextual scene segmentation of driving behavior based on double articulation analyzer	image segmentation;human recognition contextual scene segmentation double articulation analyzer advanced driver assistance systems adas adaptive cruise control precrash safety system contextual information recognition double articulation structure segmented driving behavior driving scene recognition natural language processing nlp language words;bayesian methods;acceleration;hidden markov models;natural language processing driver information systems image segmentation;safety;vehicles humans hidden markov models natural language processing bayesian methods acceleration safety;humans;vehicles;driver information systems;natural language processing	Various advanced driver assistance systems (ADASs) have recently been developed, such as Adaptive Cruise Control and Precrash Safety System. However, most ADASs can operate in only some driving situations because of the difficulty of recognizing contextual information. For closer cooperation between a driver and vehicle, the vehicle should recognize a wider range of situations, similar to that recognized by the driver, and assist the driver with appropriate timing. In this paper, we assumed a double articulation structure in driving behavior data and segmented driving behavior into meaningful chunks for driving scene recognition in a similar manner to natural language processing (NLP). A double articulation analyzer translated the driving behavior into meaningless manemes, which are the smallest units of the driving behavior just like phonemes in NLP, and from them it constructed navemes, which are meaningful chunks of driving behavior just like morphemes. As a result of this two-phase analysis, we found that driving chunks equivalent to language words were closer to the complicated or contextual driving scene segmentation produced by human recognition.	architecture design and assessment system;biconnected component;device driver;experiment;natural language processing;two-phase locking	Kazuhito Takenaka;Takashi Bando;Shogo Nagasaka;Tadahiro Taniguchi;Kentarou Hitomi	2012	2012 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2012.6385614	acceleration;computer vision;speech recognition;bayesian probability;computer science;engineering;image segmentation;hidden markov model	Robotics	-18.95811452184138	-80.56296000880624	137830
11c47d1b89957714ee5706bfcfe8e8b2ce669410	webtransc - a www interface for speech corpora production and processing			processing;text corpus;www	Tomás Valenta;Lubos Smídl	2015		10.1007/978-3-319-23132-7_60	natural language processing;speech recognition;multimedia	NLP	-15.79830410703834	-85.69309251251791	137842
7805f1ad78228a5921477da34ac5f7f39d10dc9d	sensitivity of automatic speaker identification to svd digital audio watermarking	speaker identification;speech watermarking;chaotic maps	This paper proposes the utilization of SVD digital audio watermarking to increase the security of automatic speaker identification (ASI) systems and presents a study for the effect of watermarking on the ASI system performance. The SVD audio watermarking algorithm can be implemented on audio signals in time domain or in another appropriate transform domain and can be applied to the audio signal as a whole or on a segment-by-segment basis. The speaker recognition system works by generating a database of speaker's features using the MFCCs and polynomial shape coefficients extracted from each speaker after they are lexicographically ordered into 1-D signals. A matching process is performed for any new speaker to determine if he is belonging to the database or not, using a trained neural network. Experimental results show that the SVD audio watermarking doesn't degrade the ASI system performance severely. So, it can be used with ASI to increase security. Also, it was shown the segment by segment watermarking in the time domain achieves the highest detectability of the watermark. So, we can say that it is recommended to use SVD segment by segment audio watermarking with ASI systems implementing features extracted from the DCT or the DWT.	singular value decomposition;speaker recognition	Fathi E. Abd El-Samie;Amira Shafik;Hala S. El-sayed;Said M. Elhalafawy;Salah Eldeen M. Diab;Bassioni M. Salam;Osama S. Faragallah	2015	I. J. Speech Technology	10.1007/s10772-015-9292-6	computer vision;speaker diarisation;speech recognition;computer science	HCI	-9.76499221319233	-90.63244148069217	137860
379c8251ca9bd9d129fa997d104931a7ac779c67	a statistical method of evaluating pronunciation proficiency for japanese words		In this paper, we propose a statistical method of evaluating the pronunciation proficiency of Japanese words. We analyze statistically the utterances to note a combination that has a high correlation between a Japanese teacher’s score and certain acoustic features. We found that the syllable recognition rates (accuracy) was the best measure of pronunciation proficiency. The effective measure which was highly correlated with Japanese teacher’s score was the combination of the posteriori probability, the substitution/accuracy rates and the standard deviation of mora lengths. We obtained a correlation coefficient of 0.712 with closed data and 0.591 with open data for speaker at the five words set level, respectively. The coefficient was near the correlation between humans’ scores, 0.600.	acoustic cryptanalysis;coefficient;syllable	Kei Ohta;Seiichi Nakagawa	2005			speech recognition;computer science;pronunciation	NLP	-12.072900904083577	-82.82526511223962	137995
baa258aad74abfee1d49a82c1453a041bbe34b2f	features of prominent particles in japanese discourse, frequency, functions and acoustic features			acoustic cryptanalysis	Toshiko Muranaka;Noriyo Hara	1994			speech recognition;computer science	NLP	-14.328177432872296	-85.34799020533104	137998
04dc74ce224683231d614c3f1d898dc3150c27b7	recognition of signed expressions using visually-oriented subunits obtained by an immune-based optimization	data driven procedure;subunit based classifier;pattern clustering;words recognition;video signal processing;sign language;homogeneous groups;immune optimization;training;computer vision sign language recognition time series segmentation immune based optimization;time series;words recognition automatic visual recognition signed expression data driven procedure time series homogeneous groups immune optimization quality assessment subunit based classifier;computer vision;video signal processing artificial immune systems feature extraction gesture recognition pattern clustering time series;recognition;handicapped aids;quality assessment;hidden markov models;signed expression;feature extraction;optimization hidden markov models pattern recognition handicapped aids training feature extraction clustering algorithms;pattern recognition;clustering algorithms;immune based optimization;automatic visual recognition;optimization;time series segmentation;artificial immune systems;gesture recognition	The paper considers automatic visual recognition of signed expressions. The proposed method is based on modeling gestures with subunits, which is similar to modeling speech by means of phonemes. To define the subunits a data-driven procedure is applied. The procedure consists in partitioning time series, extracted from video, into subsequences which form homogeneous groups. The cut points are determined by an immune optimization procedure based on quality assessment of the resulting clusters. In the paper the problem is formulated, its solution method is proposed and experimentally verified on a database of 100 Polish words. The results show that our subunit-based classifier outperforms its whole-word-based counterpart, which is particularly evident when new words are recognized on the basis of a small number of examples.	mathematical optimization	Mariusz Oszust;Marian Wysocki	2010		10.1109/SOCPAR.2010.5685855	computer vision;speech recognition;sign language;feature extraction;computer science;artificial intelligence;machine learning;time series;pattern recognition;gesture recognition;cluster analysis	Robotics	-5.200510879049701	-88.46513849195712	138051
d74103fcf1a1cb3cfd24779b738290fcd982f548	speaker and language factorization in dnn-based tts synthesis	statistical parametric speech synthesis;training pragmatics speech acoustics speech synthesis topology feature extraction;speaker and language factorization;polyglot speech synthesis;deep neural networks;polyglot speech synthesis statistical parametric speech synthesis deep neural networks speaker and language factorization;speech synthesis learning artificial intelligence natural language processing speaker recognition;deep neural networks speaker factorization language factorization dnn based tts synthesis multispeaker modelling speaker specific layers shared layers language specific layers linguistic feature transformation speech corpus mandarin english text to speech system	We have successfully proposed to use multi-speaker modelling in DNN-based TTS synthesis for improved voice quality with limited available data from a speaker. In this paper, we propose a new speaker and language factorized DNN, where speaker-specific layers are used for multi-speaker modelling, and shared layers and language-specific layers are employed for multi-language, linguistic feature transformation. Experimental results on a speech corpus of multiple speakers in both Mandarin and English show that the proposed factorized DNN can not only achieve a similar voice quality as that of a multi-speaker DNN, but also perform polyglot synthesis with a monolingual speaker's voice.	netware file system;speech corpus;speech synthesis;super robot monkey team hyperforce go!	Yuchen Fan;Yao Qian;Frank K. Soong;Lei He	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472737	natural language processing;speaker recognition;speaker diarisation;speech recognition;computer science	Robotics	-17.998622611118424	-87.64071335639106	138151
55c1e710fa6f57a4eaae257bc123538e07d19f70	fusing fast algorithms to achieve efficient speech detection in fm broadcasts	fast algorithm;speech detection	This paper describes a system aimed at detecting speech segments in FM broadcasts. To achieve high processing speeds, simple but fast algorithms are used. To output robust decisions, a combination of many different algorithms has been considered. The system is fully operational in the context of Open Source Intelligence, since 2007.	algorithm;fm broadcasting;fast fourier transform;open-source intelligence;operational system;radio broadcasting;relevance;sensor;speech synthesis;time complexity	Stéphane Pigeon;Patrick Verlinde	2009			voice activity detection;speech recognition;computer science;machine learning;pattern recognition;speech processing	OS	-14.145696398859535	-88.36603579283012	138271
c0aa7e6783fd5c14143dfa31ed96c99d46be91c3	focus, lexical stress and boundary tone: interaction of three prosodic features	modelizacion;lenguaje natural;speech synthesis;localization;speech processing;langage naturel;tratamiento palabra;traitement parole;localizacion;chino;lexical stress;modelisation;focus;localisation;natural language;prosodie;boundary tone;sintesis palabra;prosody;chinois;chinese;modeling;synthese parole;prosodia	This paper studies how focus, lexical stress and rising boundary tone act on F0 of the last preboundary word. We find that when the word is non focused, the rising boundary tone takes control almost from the beginning of the word and flattens F0 peak of the lexical stress. When the word is focused, the rising boundary tone is only dominant after F0 peak of lexical stress is formed. This peak is even higher than F0 height required by the rising boundary tone at the end of the word. Furthermore, the location of lexical stress restrains the height at F0 peak and high end to be reached. The interaction of these three factors on a single word leads to F0 competition due to limited articulatory dimensions. The study helps to build prosodic model for high quality speech	articulatory phonology;display resolution;speech synthesis	Lu Zhang;Yi-Qing Zu;Run-Qiang Yan	2006		10.1007/11939993_11	speech recognition;computer science;speech processing;linguistics;prosody;natural language;speech synthesis;chinese;focus	NLP	-11.8284489465722	-81.45877754756037	138293
79ad7f4de530d52ee51f65babb26951a7c549ee6	acoustic modelling using modular/ensemble combinations of heterogeneous neural networks	neural network	"""We have been investigating for some time the use of modular/ensemble neural networks to model phones, a commonly chosen acoustic unit for speech. We have demonstrated the advantage of using separately trained MLPs to estimate each phone's probability, posterior on a sequence of feature vectors representing the expression of the phone over some window in time. In this paper we show how MLPs trained on different feature vectors, derived from different pre-processing techniques, may be combined to produce better estimates of phone posteriors and hence lower word error rates. We also show how calculated broad-class posterior probabilities may be used to provide contextual information to train further nets. The combination of these techniques results in significant improvements for phone classification and word error rates on the TIMIT corpus. 1 I N T R O D U C T I O N Neural networks have been used in a number of speech recognition systems [13][14]. In particular many systems use an MLP to estimate posterior probabilities for acoustic units such as phones. These posteriors are converted into scaled likelihoods for use with one-state-per-phone Hidden Markov Models (HMMs). Input to the MLP is a sequence of speech feature vectors centred on the current moment in time. This allows the MLP to model the acoustic time stream with the minimum of assumptions about the speech generation process. Typically, a single monolithic MLP with thousands of hidden nodes is used to simultaneously model all phones. Minimising output error by several criteria will lead to the MLP outputs yielding phone probabilities, posterior on the input space. This is provided it has sufficient hidden nodes, unlimited training data is available and training does not get stuck in a local minimum. Three problems hamper this approach: 1. Even when large amounts of training data are available, satisfactory modelling will always be challenged by the sparsity of data for some phones, in some regions of the input space. This is because the data is scattered in a highly non-uniform manner throughout the space. A potential strength of an MLP classifier is its ability to capture correlations in the high-dimensional space of sequences of input features. Unfortunately as longer sequences of input features are employed, the dimension of the input space increases and the data sparsity problem gets worse. 2. Best estimates of posterior probabilities are not obtained by minimising error on a finite training set. Instead, training is usually stopped early using a minimum error criterion on a separate cross-validation data set. The reasoning is that training must be prevented from fitting too closely to a finite training set. However, applying this procedure to a net with multiple outputs is necessarily an awkward compromise. The best stopping point for all outputs combined is not necessarily ideal for any particular one. Work we published in [2] supports this view as well as that in [16]. 3. A monolithic MLP is efficient in its use of network parameters. However, such a network can be difficult and time-consuming to train and impractical for systems employing HMMs with many thousands of states. These observations have motivated work on decomposing the acoustic modelling task. Hierarchical Mixtures of Experts [10] and the ACID/HNN architecture [6] can be used to decompose acoustic modelling in a data-driven fashion. This has obvious merit and leads to encouraging results. The decomposing algorithms for both these architectures depend on a uniform feature extraction procedure for all acoustic units. Another approach is a modular decomposition guided by existing acoustic units, such as phones. Phonemic neural networks have some history [15] and there is some related work in [1],[3] and [17]. We extend these approaches by introducing a layer of separately trained combining nets. Combining nets have been used by Hermansky and Sharma [9], but to combine the outputs of monolithic MLPs. A static decomposition by phone or other acoustic units has a number of advantages: (1) We can match network resources and training to the needs of each phone. (2) Heterogeneous nets, trained on different feature vectors can be combined. The feature vectors may be obtained from different front-end pre-processing techniques or by exploiting recent work on subband feature extraction ! """" ISCA Archive"""	acid;acoustic cryptanalysis;acoustic fingerprint;algorithm;archive;best practice;cross-validation (statistics);feature extraction;feature vector;hidden markov model;international symposium on computer architecture;learning classifier system;markov chain;maxima and minima;memory-level parallelism;modular decomposition;neural networks;preprocessor;quad flat no-leads package;sparse matrix;speech recognition;speech synthesis;timit;test set	Christos Andrea Antoniou;T. Jeff Reynolds	2000			artificial intelligence;types of artificial neural networks;artificial neural network;pattern recognition;time delay neural network;stochastic neural network;modular design;computer science	ML	-16.83572158412245	-89.69206243610789	138325
3d95bcb1deb86d5b89410bae824a39fdb60a4864	spoken language classification using hybrid classifier combination	classifier combination;support vector machines;dialogue;classification;support vector;simple recurrent network;spontaneous language;recurrent neural networks;recurrent neural network;support vector machine	In this paper we describe an approach for spoken language analysis for helpdesk call routing using a combination of simple recurrent networks and support vector machines. In particular we examine this approach for its potential in a difficult spoken language classification task based on recorded operator assistance telephone utterances. We explore simple recurrent networks and support vector machines using a large, unique telecommunication corpus of spontaneous spoken language. The main contribution of the paper is a combination of techniques in the domain of call routing. First, we find that simple recurrent networks perform better than support vector machines for this task. Second, we claim that the combination of simple recurrent networks and support vector machines provides slightly improved performance compared to the performance of either simple recurrent networks or support vector machines.	recurrent neural network;routing;spontaneous order;support vector machine	Sheila Garfield;Stefan Wermter;Siobhan Devlin	2005	Int. J. Hybrid Intell. Syst.	10.3233/HIS-2005-2102	margin classifier;support vector machine;speech recognition;computer science;machine learning;pattern recognition	ML	-17.585798365045655	-87.53396534223917	138352
bcf4ccbbdb11e17bed6dc630179ef31181b290b3	toward detecting voice activity employing soft decision in second-order conditional map	second order	In this paper, we propose a novel approach to statistical modelbased voice activity detection (VAD) that incorporates a secondorder conditional maximum a posteriori (MAP) criterion. As a technical improvement for the first-order conditional MAP criterion in [1], we consider both the current observation and the voice activity decision in the previous two frames to take full consideration of the inter-frame correlation of voice activity. The soft decision scheme is incorporated to result in time-varying thresholds for further performance improvement. Experimental results show that the proposed algorithm outperforms the conventional CMAP-based VAD technique under various experimental conditions.	algorithm;cmap (font);first-order predicate;sensor;statistical model;voice activity detection	Sang-Kyun Kim;Jae-Hun Choi;Sang-Ick Kang;Ji-Hyun Song;Joon-Hyuk Chang	2010			speech recognition;computer science;artificial intelligence;machine learning;second-order logic	Vision	-12.957004919303682	-93.82349040278197	138518
fe6f3dd621c7154c13f3b8910bdfb6b18a1ccae2	approximate matching algorithms for music information retrieval using vocal input	query by humming;music information retrieval;error rate;approximate matching	Effective use of multimedia collections requires efficient and intuitive methods of searching and browsing. This work considers databases which store music and explores how these may best be searched by providing input queries in some musical form. For the average person, humming several notes of the desired melody is the most straightforward method for providing this input, but such input is very likely to contain several errors. Previously proposed implementations of so-called query-by-humming systems are effective only when the number of input errors is small. We conducted experiments which revealed that the expected error rate for user queries is much higher than existing algorithms can tolerate. We then developed algorithms based on approximate matching techniques which deliver much improved results when comparing error-filled vocal user queries against a music collection.	approximation algorithm;database;experiment;information retrieval;query by humming;regular expression	Richard L. Kline;Ephraim P. Glinert	2003		10.1145/957013.957042	speech recognition;word error rate;computer science;theoretical computer science;machine learning;pattern recognition;world wide web;information retrieval	DB	-7.498446041595466	-94.08805218879975	138600
25b88dc99cbacd188f2c2b6eb1d408e9e2fb54be	integrating multilingual articulatory features into speech recognition	word error rate;articulatory feature;error rate;speech recognition;difference set	The use of articulatory features, such as place and manner of articulation, has been shown to reduce the word error rate of speech recognition systems under different conditions and in different settings. For example recognition systems based on features are more robust to noise and reverberation. In earlier work we showed that articulatory features can compensate for inter language variability and can be recognized across languages. In this paper we show that using crossand multilingual detectors to support an HMM based speech recognition system significantly reduces the word error rate. By selecting and weighting the features in a discriminative way, we achieve an error rate reduction that lies in the same range as that seen when using language specific feature detectors. By combining feature detectors from many languages and training the weights discriminatively, we even outperform the case where only monolingual detectors are being used.	biconnected component;discriminative model;hidden markov model;sensor;spatial variability;speech recognition;word error rate	Sebastian Stüker;Florian Metze;Tanja Schultz;Alexander H. Waibel	2003			natural language processing;speech recognition;word error rate;computer science;pattern recognition	NLP	-15.492186452298501	-89.70238803896659	138743
0fedbdd4f14c1b1a8be6a0e1d72d0c1dd3d9ca64	automatic transcription of polyphonic music based on the constant-q bispectral analysis	estimation harmonic analysis filter bank hidden markov models fourier transforms psychoacoustic models spectral analysis;front end;estimation theory;audio signal processing;filter bank;fourier transform;constant q bispectral analysis;hidden markov model;information retrieval;music information retrieval mir;polyphonic music transcription audio signals processing bispectrum constant q analysis higher order spectra music information retrieval mir;constant q analysis;bispectrum;rwc classical audio database automatic music transcription polyphonic music transcription constant q bispectral analysis music information retrieval mir audio recordings pitch estimation 2d harmonic pattern onset detection duration detection signal spectrogram real world computing classical audio database;2d harmonic pattern;signal spectrogram;duration detection;a priori knowledge;hidden markov models;estimation;audio signals processing;higher order spectra;polyphonic music transcription;fourier transforms;music information retrieval;music audio signal processing estimation theory information retrieval;real world computing classical audio database;rwc classical audio database;onset detection;audio recordings;spectral analysis;psychoacoustic models;music;music transcription;pitch estimation;automatic music transcription;mir;harmonic analysis	In the area of music information retrieval (MIR), automatic music transcription is considered one of the most challenging tasks, for which many different techniques have been proposed. This paper presents a new method for polyphonic music transcription: a system that aims at estimating pitch, onset times, durations, and intensity of concurrent sounds in audio recordings, played by one or more instruments. Pitch estimation is carried out by means of a front-end that jointly uses a constant-Q and a bispectral analysis of the input audio signal; subsequently, the processed signal is correlated with a fixed 2-D harmonic pattern. Onsets and durations detection procedures are based on the combination of the constant-Q bispectral analysis with information from the signal spectrogram. The detection process is agnostic and it does not need to take into account musicological and instrumental models or other a priori knowledge. The system has been validated against the standard Real-World Computing (RWC)-Classical Audio Database. The proposed method has demonstrated good performances in the multiple F0 tracking task, especially for piano-only automatic transcription at MIREX 2009.	automatic differentiation;estimation theory;information retrieval;medical transcription;onset (audio);performance;pitch detection algorithm;spectrogram;transcription (software);video game music	Fabrizio Argenti;Paolo Nesi;Gianni Pantaleo	2011	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2010.2093894	fourier transform;speech recognition;acoustics;computer science;mathematics;hidden markov model;statistics	Vision	-10.558148244847445	-93.30784086607963	138765
acaae052f2e3deea112044a371ce5b8ea2f669b6	packet loss concealment with natural variations using hmm	signal parameter estimates packet loss concealment hmm speech quality voice over ip signal repetition variations overlap add interpolation hidden markov models statistical signal processing conditional density functions statistical estimation methods;hidden markov model;speech processing;statistical signal processing;voice over ip;natural variation;hmm;signal parameter estimates;internet telephony;speech quality;speech processing hidden markov models internet telephony;hidden markov models;packet loss concealment;signal repetition variations;parameter estimation;hidden markov models programmable control speech interpolation yield estimation internet telephony signal processing parameter estimation statistics probability density function;matematik;conditional density functions;statistical estimation methods;statistical estimation;density functional;overlap add interpolation	Packet loss concealment (PLC) at a receiver has a substantial effect on the speech quality in voice over IP. Most conventional PLC systems have largely relied upon variations of signal repetition and overlap-add interpolation which can produce speech signals that do not follow the larger overall statistical trends. in this paper, we demonstrate how hidden Markov models can be utilized to effect PLC based on statistical signal processing. In particular, we show how HMM-based PLC yields conditional density functions that can be utilized by various statistical estimation methods that produce signal parameter estimates that produce more natural variation than conventional PLC methods, thereby providing much better speech quality	estimation theory;hidden markov model;interpolation;markov chain;overlap–add method;statistical signal processing	Manohar N. Murthi;Christoffer Rødbro;Søren Vang Andersen;Søren Holdt Jensen	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1659947	speech recognition;computer science;machine learning;pattern recognition;voice over ip;hidden markov model;statistics	Visualization	-13.815358323508631	-93.83924287007251	138876
8c3679bab6b379a3f487c68f631f55bb18292bc0	towards end-to-end speech recognition with deep convolutional neural networks		Convolutional Neural Networks (CNNs) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition (ASR). Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models (HMMs/GMMs) have achieved the state-of-the-art in various benchmarks. Meanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural Networks (RNNs), which is proposed for labeling unsegmented sequences, makes it feasible to train an ‘end-to-end’ speech recognition system instead of hybrid settings. However, RNNs are computationally expensive and sometimes difficult to train. In this paper, inspired by the advantages of both CNNs and the CTC approach, we propose an end-to-end speech framework for sequence labeling, by combining hierarchical CNNs with CTC directly without recurrent connections. By evaluating the approach on the TIMIT phoneme recognition task, we show that the proposed model is not only computationally efficient, but also competitive with the existing baseline systems. Moreover, we argue that CNNs have the capability to model temporal correlations with appropriate context information.	acoustic cryptanalysis;algorithmic efficiency;analysis of algorithms;baseline (configuration management);benchmark (computing);connectionism;convolutional neural network;end-to-end principle;hidden markov model;markov chain;mixture model;neural networks;recurrent neural network;sequence labeling;speech recognition;timit	Mohammad Pezeshki;Philemon Brakel;Saizheng Zhang;César Laurent;Yoshua Bengio;Aaron C. Courville	2016		10.21437/Interspeech.2016-1446	speech recognition;computer science;machine learning	ML	-17.068882615241932	-88.38292195272965	139092
9aee274eb69b5ade509e1d39d7282513bba222ae	assessing the tuning of sung indian classical music		The issue of tuning in Indian classical music has been, historically, a matter of theoretical debate. In this paper, we study its contemporary practice in sung performances of Carnatic and Hindustani music following an empiric and quantitative approach. To do so, we select stable fundamental frequencies, estimated via a standard algorithm, and construct interval histograms from a pool of recordings. We then compare such histograms against the ones obtained for different music sources and against the theoretical values derived from 12-note just intonation and equal temperament. Our results evidence that the tunings in Carnatic and Hindustani music differ, the former tending to a just intonation system and the latter having much equal-tempered influences. Carnatic music also presents signs of a more continuous distribution of pitches. Further subdivisions of the octave are partially investigated, finding no strong evidence of them.	algorithm;interval arithmetic;performance	Joan Serrà;Gopala K. Koduri;Marius Miron;Xavier Serra	2011			speech recognition	ML	-10.077788391863166	-83.13675832390116	139184
2ce0f615a526673c216c405b0ade2d79c47622d4	mandarin digit recognition assisted by selective tone distinction.		Continuous Mandarin digit recognition is an important function to provide a useful user interface for in-car applications. In this paper, as opposed to the conventional N-best rescoring, we propose a direct modification approach on the 1-best hypothesis of recognition results using selective tone distinction. Experiments were performed on noisy speech at SNRs of 20dB and 9dB. Over the baseline without using tone information, our proposal achieved error reductions of 24%~27% for both SNRs, which is significantly better than the error reduction of 10-best rescoring. Moreover, the relatively constant error reduction seen in wide-ranging SNR demonstrates the robustness of our proposal.	baseline (configuration management);experiment;hough transform;pitch (music);signal-to-noise ratio;speech recognition;super robot monkey team hyperforce go!;user interface	Xiaodong Wang;Kunihiko Owa;Makoto Shozakai	2010			speech recognition	ML	-13.706157187276995	-90.56265181314322	139236
5aff93fa2e69c57de5b5caeba48ad0d96b7f9e7a	stress and emotion classification using jitter and shimmer features	stress;animals;bioacoustics;gaussian processes;gfcc;hidden markov model;human speech;speech processing;speech analysis;stress classification emotion classification jitter features shimmer features human speaking styles classification animal vocalization arousal levels frequency contour mel frequency cepstral coefficients human speech greenwood function cepstral coefficients hidden markov models gaussian mixture models;hmm;stress classification models;speech processing bioacoustics cepstral analysis gaussian processes hidden markov models;human speaking styles classification;mel frequency cepstral coefficient;shimmer;gaussian mixture model;cepstral analysis;hidden markov models;animal vocalization arousal levels;emotion classification;feature extraction;gaussian mixture models;stress classification;greenwood function cepstral coefficients;mel frequency cepstral coefficients;hmm jitter shimmer mfcc gfcc;frequency contour;humans;african elephant;jitter;classification accuracy;stress jitter humans animals cepstral analysis hidden markov models speech analysis feature extraction mel frequency cepstral coefficient acoustic measurements;acoustic measurements;jitter features;shimmer features;mfcc;rhesus monkey;fundamental frequency;hidden markov	In this paper, we evaluate the use of appended jitter and shimmer speech features for the classification of human speaking styles and of animal vocalization arousal levels. Jitter and shimmer features are extracted from the fundamental frequency contour and added to baseline spectral features, specifically Mel-frequency cepstral coefficients (MFCCs) for human speech and Greenwood function cepstral coefficients (GFCCs) for animal vocalizations. Hidden Markov models (HMMs) with Gaussian mixture models (GMMs) state distributions are used for classification. The appended jitter and shimmer features result in an increase in classification accuracy for several illustrative datasets, including the SUSAS dataset for human speaking styles as well as vocalizations labeled by arousal level for African elephant and Rhesus monkey species.	baseline (configuration management);coefficient;hidden markov model;markov chain;mel-frequency cepstrum;mixture model;spatial variability	Xi Li;Jidong Tao;Michael T. Johnson;Joseph Soltis;Anne Savage;Kirsten M. Leong;John D. Newman	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367261	speech recognition;computer science;machine learning;mel-frequency cepstrum;hidden markov model	Vision	-11.87260293770718	-88.78941725514935	139287
898a3d9ddc9bd220341b107dcc77ca0239ac8cca	building acoustic model ensembles by data sampling with enhanced trainings and features	pattern clustering;intermodel diversity speaker clustering sc data sampling speech recognition cross validation expectation maximization cvem discriminative training dt multiple layer perceptron mlp ensemble acoustic model eam timit phoneme recognition task telemedicine automatic captioning task recognition accuracy hidden markov model hmm baseline system;signal sampling;multilayer perceptrons;hidden markov models data models acoustics training speech recognition computational modeling diversity reception;acoustic signal processing;speaker recognition;hidden markov models;mlp feature ensemble acoustic model cross validation data sampling speaker clustering data sampling discriminative training;speaker recognition acoustic signal processing expectation maximisation algorithm hidden markov models multilayer perceptrons pattern clustering signal sampling;expectation maximisation algorithm	We propose a novel approach of using Cross Validation (CV) and Speaker Clustering (SC) based data samplings to construct an ensemble of acoustic models for speech recognition. We also investigate the effects of the existing techniques of Cross Validation Expectation Maximization (CVEM), Discriminative Training (DT), and Multiple Layer Perceptron (MLP) features on the quality of the proposed ensemble acoustic models (EAMs). We have evaluated the proposed methods on TIMIT phoneme recognition task as well as on a telemedicine automatic captioning task. The proposed methods have led to significant improvements in recognition accuracy over conventional Hidden Markov Model (HMM) baseline systems, and the integration of EAMs with CVEM, DT, and MLP has also significantly improved the accuracy performances of the single model systems based on CVEM, DT, and MLP, where the increased inter-model diversity is shown to have played an important role in the performance gain.	acoustic cryptanalysis;acoustic model;baseline (configuration management);cluster analysis;cross-validation (statistics);discriminative model;embedded atom model;expectation–maximization algorithm;gibbs sampling;hp multi-programming executive;hidden markov model;markov chain;memory-level parallelism;perceptron;performance;quad flat no-leads package;sampling (signal processing);speech recognition;timit;vocabulary	Xin Chen;Yunxin Zhao	2013	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2012.2227729	speaker recognition;speech recognition;computer science;machine learning;pattern recognition	Vision	-18.210965235265544	-90.58306004846489	139340
eca405b95fd934dc44071da85613742dd7454dfc	speaker verification using gaussian posteriorgrams on fixed phrase short utterances		This work explores the speaker verification using fixed phrase short utterances. A novel speaker verification system using Gaussian posteriorgrams is proposed in which the posteriorgram vectors are computed from speaker specific Gaussian mixture model (GMM). The enrollment utterances for each of the target speakers are labeled with GMM trained on the corresponding speaker’s data. The test trials are then labeled with the claimed speaker’s GMMmodel. Dynamic time warping (DTW) is used to find a match score between the posteriorgrams of the claimed speaker and that of test trial. The proposed approach is evaluated on the fixed pass phrase subset of the recent RSR2015 database. For contrast purpose, we have also developed stateof-the-art i-vector system including probabilistic linear discriminant analysis (PLDA) classifier. The proposed framework is found to result in highly improved performance when compared with the i-vector based contrast system. We hypothesize that the cause of this large improvement lies in the use of speaker specific variances information in generation of the posteriorgram representations. On evaluating the proposed framework with non-speaker specific variances, it resulted in significant performance degradation which confirmed our hypothesis.	dynamic time warping;elegant degradation;google map maker;linear discriminant analysis;mixture model;passphrase;speaker recognition	Sarfaraz Jelil;Rohan Kumar Das;Rohit Sinha;S. R. Mahadeva Prasanna	2015			pattern recognition;artificial intelligence;speech recognition;computer science;gaussian;phrase	ML	-16.54747511781336	-91.28497419523245	139355
4947bba0c52b170e07fc03c0cf1a9dee3214cd10	the role of a coda consonant as error trigger in repetition tasks	health research;uk clinical guidelines;biological patents;europe pubmed central;ultrasound;citation search;uk phd theses thesis;life sciences;error rate;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	Speech errors are known to exhibit an intrusion bias in that segments are added rather than deleted; also, a shared final consonant can cause an interaction of the initial consonants. A principled connection between these two phenomena has been drawn in a gestural account of errors: Articulatory measures revealed a preponderance of errors in which both the target and intruding gesture are co-produced, instead of one replacing the other. This gestural intrusion bias has been interpreted as an errorful coupling of gestures in a dynamically stable coordination mode (1:1, in-phase), triggered by the presence of a shared coda consonant. Capturing tongue motion with ultrasound, the current paper investigates whether shared gestural composition other than a coda can trigger gestural co-production errors. Subjects repeated two-word phrases with alternating initial stop or fricative consonants in a coda condition (e.g., top cop), a nocoda condition (e.g., taa kaa) and a three-word phrase condition (e.g., taa kaa taa). The no-coda condition showed a lower error rate than the coda condition. The three-word phrase condition elicited an intermediate error rate for the stop consonants, but a high error rate for the fricative alternations. While all conditions exhibited both substitution and co-production errors, a gestural intrusion bias emerged mainly for the coda condition. The findings suggest that the proportion of different error types (substitutions, co-production errors) differs as a function of stimulus type: not all alternating stimulus patterns that trigger errors result in an intrusion bias.	1:1 pixel mapping;amazon simple storage service;aortic aneurysm, thoracic;coda (file system);cognitive dimensions of notations;crazy stone (software);curie;dental intrusion;maureen c. stone;phonetics;phrases;physiological sexual disorders;review [publication type];standard deviation;stress ball;subgroup;surface acoustic wave device component;thioacetamide;tract (literature);united states national institutes of health;urinary bladder calculi (disorder);urinary tract infection	Marianne Pouplier	2008	Journal of phonetics	10.1016/j.wocn.2007.01.002	natural language processing;speech recognition;word error rate;ultrasound;linguistics;communication	HCI	-9.439561994576056	-80.86196135380699	139359
78fb73d813b28fd4fc688dfc4910a5d8ff121417	introduction to the issue on statistical parametric speech synthesis	engineering;speech synthesis;special issues and sections;technology;hidden markov models;science technology;special issues and sections speech synthesis hidden markov models adaptation models synthesizers;synthesizers;adaptation models;engineering electrical electronic	STATISTICAL parametric speech synthesis has been a hot topic for some time. Classic statistical parametric speech synthesizers are able to produce fairly natural-sounding and flexible voices, needing only a relatively small training database, and can be more  asily adapted to a new voice or speaking style than concatenative systems.	speech synthesis	Jianhua Tao;Keikichi Hirose;Keiichi Tokuda;Alan W. Black;Simon King	2014	J. Sel. Topics Signal Processing	10.1109/JSTSP.2014.2309416	natural language processing;speech recognition;computer science;artificial intelligence;speech synthesis;technology	PL	-16.988273436048566	-85.27596752427014	139470
1414633fa5cb9de4a46fc1b32c77ed3e78d40ec8	spectral features based on local normalized center moments for speech emotion recognition			emotion recognition	Huawei Tao;Ruiyu Liang;Xinran Zhang;Lu Zhao	2016	IEICE Transactions		speech recognition;pattern recognition	Vision	-13.385219169854295	-87.76628844495194	139506
99f02e9d6811d1d061d996c73c17df11a97263b2	compressing lstms into cnns.		We show that a deep convolutional network with an architecture inspired by the models used in image recognition can yield accuracy similar to a long-short term memory (LSTM) network, which achieves the state-of-the-art performance on the standard Switchboard automatic speech recognition task. Moreover, we demonstrate that merging the knowledge in the CNN and LSTM models via model compression further improves the accuracy of the convolutional model.	computer vision;data compression;long short-term memory;speech recognition;telephone switchboard	Krzysztof Geras;Abdel-rahman Mohamed;Rich Caruana;Gregor Urban;Shengjie Wang;Özlem Aslan;Matthai Philipose;Matthew Richardson;Charles A. Sutton	2015	CoRR		computer science	Vision	-17.449818887662914	-88.034018112049	139572
f352ac054e523b5ccbf8de23dc0d4d02daf58032	improving accented mandarin speech recognition by using recurrent neural network based language model adaptation	decoding;training;speech;artificial neural networks;speech recognition;recurrent neural networks;adaptation models	In this paper, we propose adapt the recurrent neural network (RNN) based language model to improve the performance of multi-accent Mandarin speech recognition. N-gram based language model has already been applied to speech recognition system, but it is hard to describe the long span information in a sentence and arises a serious phenomenon of data sparse. Instead, RNN based language model can overcome these two shortcomings, but it will take a long time to decode directly. Taking these into consideration, this paper combines these two types of language model (LM) together and adapts the RNN based language model to rescore lattices for different accented Mandarin speech. The architecture of the adapted RNN LM is accent-specific top layers and shared hidden layer. The accent-specific top layers are used to adapt different accents and the shared hidden layer stores history information, which can be seen as memory layer. Experiments on the RASC863 corpus show that the proposed method can improve the performance of accented Mandarin speech recognition over the baseline system.	artificial neural network;baseline (configuration management);experiment;language model;n-gram;random neural network;recurrent neural network;sparse matrix;speech recognition;super robot monkey team hyperforce go!	Hao Ni;Jiangyan Yi;Zhengqi Wen;Bin Liu;Jianhua Tao	2016	2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)	10.1109/ISCSLP.2016.7918364	natural language processing;speech recognition;computer science;speech;recurrent neural network;machine learning;time delay neural network;linguistics;artificial neural network	NLP	-18.759955717639407	-87.45139826779115	139667
9a4c3963cca03de320ff62245ac6f84fb86f80ba	confidence measure based unsupervised target model adaptation for speaker verification	confidence measure;model adaptation;speaker verification	This paper proposes a new method for updating online the client models of a speaker recognition system using the test data. This problem is called unsupervised adaptation. The main idea of the proposed approach is to adapt the client model using the complete set of data gathered from the successive test, without deciding if the test data belongs to the client or to an impostor. The adaptation process includes a weighting scheme of the test data, based on the a posteriori probability that a test belongs to the targeted client model. The proposed approach is evaluated within the framework of the NIST 2005 and 2006 Speaker Recognition Evaluations. The links between the adaptation method and channel mismatch factors is also explored, using both Feature Mapping and Latent Factor Analysis (LFA) methods. The proposed unsupervised adaptation outperforms the baseline system, with a relative DCF improvement of 27% (37% for EER). When the LFA channel compensation technique is used, the proposed approach achieves a reduction in DCF of 20% (12.5% for EER).	baseline (configuration management);design rule for camera file system;enhanced entity–relationship model;factor analysis;speaker recognition;test data;unsupervised learning	Alexandre Preti;Jean-François Bonastre;Driss Matrouf;François Capman;Bertrand Ravera	2007			speech recognition;computer science;pattern recognition;data mining	SE	-17.442811297323754	-90.39393625929748	139685
d8d38ae9c4b30b35e87ddbfc1f305354923d30ce	anton: an animatronic model of a human tongue and vocal tract	vocal tract;speech energetics;energy consumption;speech variation;vocal tract modelling;speech production;animatronic modelling;biomimetics	This paper describes AnTon, the first animatronic model of a human tongue and vocal tract which is being developed using biomimetic design principles. The present model consists of movable tongue and jaw models that are connected to a fixed hyoid bone and a skull. AnTon’s ability to reproduce speech gestures has been investigated using MRI scans of human subjects producing standard vowels. In the future, the animatronic vocal tract will be used to study the relation between speech variation and energy investment, specifically in the context of the Lombard reflex.	animatronics;anton (computer);biomimetics;digital single-lens reflex camera;movable type;tract (literature)	Robin Hofe;Roger K. Moore	2008	Connect. Sci.	10.1080/09540090802413251	biomimetics;vocal tract;speech production;speech recognition;linguistics	AI	-8.182748496291142	-83.97854343983239	139741
157d9e015e5f35aaa9bf8519c6bc1cabb90f8d4c	durational characteristics and timing patterns of russian onset clusters at two speaking rates		This study presents articulatory data on the durational and timing characteristics of Russian onset clusters and their change as a function of speaking rate. While there is increasing evidence that languages differ systematically in their consonant-to-consonant timing, little is known on whether this difference also entails different implementations of speaking rate changes. Russian contrasts with languages like English or German in that it has little overlap between onset consonants. Relatedly, stop consonants are obligatorily released. We investigate whether these global timing characteristics have implications for the implementation of speech rate changes. We hypothesize that Russian onset timing may vary little as a function of speaking rate, with rate affecting consonant duration rather than C1-C2 timing. Also a cluster's sonority profile (e.g. /bl-/; /lb-/) may factor into the implementation of speech rate changes. Results show, contrary to expectation, that both duration and timing of the consonants in a cluster are subject to change. However, there was a less of a rate effect for clusters with falling sonority, pointing towards their lesser flexibility in timing. Our results also reveal significant differences in the durational control of C1 and C2, challenging current models of durational organization of consonant clusters.	mike lesser;onset (audio)	Marianne Pouplier;Stefania Marin;Alexei Kochetov	2015			artificial intelligence;pattern recognition;speech recognition;cluster (physics);computer science	OS	-10.625406430196447	-81.63431562321212	139766
58711fb0111fe2f597789d05d896874e737ab946	gender-dependent emotion recognition based on hmms and sphmms		It is well known that emotion recognition performance is not ideal. The work of this research is devoted to improving emotion recognition performance by employing a two-stage recognizer that combines and integrates gender recognizer and emotion recognizer into one system. Hidden Markov Models (HMMs) and Suprasegmental Hidden Markov Models (SPHMMs) have been used as classifiers in the two-stage recognizer. This recognizer has been tested on two distinct and separate emotional speech databases. The first database is our collected database and the second one is the Emotional Prosody Speech and Transcripts database. Six basic emotions including the neutral state have been used in each database. Our results show that emotion recognition performance based on the two-stage approach (gender-dependent emotion recognizer) has been significantly improved compared to that based on emotion recognizer without gender information and emotion recognizer with correct gender information by an average of 11% and 5%, respectively. This work shows that the highest emotion identification performance takes place when the classifiers are completely biased towards suprasegmental models and no impact of acoustic models. The results achieved based on the two-stage framework fall within 2.28% of those obtained in subjective assessment by human judges.	acoustic cryptanalysis;database;emotion recognition;finite-state machine;hidden markov model;markov chain;semantic prosody	Ismail Shahin	2018	CoRR	10.1007/s10772-012-9170-4.	speech recognition;computer science;pattern recognition;mel-frequency cepstrum;hidden markov model	NLP	-12.62930454946738	-87.96682098734271	139991
48731f65d06bfa53bd2a1b46c715799af676ef01	reconocimiento automático de emociones utilizando parámetros prosódicos	computacion informatica;filologias;emotion recognition;info eu repo semantics article;informacion documentacion;linguistica;ciencias basicas y experimentales;reconocimiento de emociones;habla emocional;grupo a;ciencias sociales;grupo b;emotional speech	This paper presents the experiments made to automatically identify emotion in an emotional speech database for Basque. Three different classifiers have been built: one using spectral features and GMM, other with prosodic features and SVM and the last one with prosodic features and GMM. 86 prosodic features were calculated and then an algorithm to select the most relevant ones was applied. The first classifier gives the best result with a 98.4% accuracy when using 512 mixtures, but the classifier built with the best 6 prosodic features achieves an accuracy of 92.3% in spite of its simplicity, showing that prosodic information is very useful to identify emotions.	algorithm;database;experiment;google map maker	Iker Luengo;Eva Navas;Inma Hernáez;Jon Sánchez	2005	Procesamiento del Lenguaje Natural		computer science;linguistics;emotion recognition	NLP	-12.533849738383102	-87.72837811248624	140141
b4e889af57295dff9498ba476893a359a91b8a3e	improving speaker turn embedding by crossmodal transfer learning from face embedding		Learning speaker turn embeddings has shown considerable improvement in situations where conventional speaker modeling approaches fail. However, this improvement is relatively limited when compared to the gain observed in face embedding learning, which has proven very successful for face verification and clustering tasks. Assuming that face and voices from the same identities share some latent properties (like age, gender, ethnicity), we propose two transfer learning approaches to leverage the knowledge from the face domain learned from thousands of identities for tasks in the speaker domain. These approaches, namely target embedding transfer and clustering structure transfer, utilize the structure of the source face embedding space at different granularities to regularize the target speaker turn embedding space as optimizing terms. Our methods are evaluated on two public broadcast corpora and yield promising advances over competitive baselines in verification and audio clustering tasks, especially when dealing with short speaker utterances. The analysis gives insight into characteristics of the embedding spaces and shows their potential applications.	broadcast domain;cluster analysis;experiment;interaction;mummer;modality (human–computer interaction);speaker diarisation;text corpus;dialog	Nam Le;Jean-Marc Odobez	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.58	transfer of learning;computer vision;computer science;crossmodal;cluster analysis;speaker diarisation;pattern recognition;artificial intelligence;machine learning;embedding	Vision	-16.001014014805705	-90.11106840979173	140218
ff4d5126cf7a41a97f1827beb6a72405e6a2ff42	automatically grading learners' english using a gaussian process		There is a high demand around the world for the learning of English as a second language. Correspondingly, there is a need to assess the proficiency level of learners both during their studies and for formal qualifications. A number of automatic methods have been proposed to help meet this demand with varying degrees of success. This paper considers the automatic assessment of spoken English proficiency, which is still a challenging problem. In this scenario, the grader should be able to accurately assess the learner’s ability level from spontaneous, prompted, speech, independent of L1 language and the quality of the audio recording. Automatic graders are potentially more consistent than humans. However, the validity of the predicted grade varies. This paper proposes an automatic grader based on a Gaussian process. The advantage of using a Gaussian process is that as well as predicting a grade, it provides a measure of the uncertainty of its prediction. The uncertainty measure is sufficiently accurate to decide which automatic grades should be re-graded by humans. It can also be used to determine which candidates are hard to grade for humans and therefore need expert grading. Performance of the automatic grader is shown to be close to human graders on real candidate entries. Interpolation of human and GP grades further boosts performance.		Rogier C. van Dalen;Kate Knill;Mark J. F. Gales	2015				AI	-17.31708950135627	-82.04914259503391	140338
1edae83ebb9bcaede043b972ab2975da69e32e30	intelligibility of electrolarynx speech using a novel hands-free actuator	control theory;vocal tract;biomedical engineering;signal processing;intelligibility	During voiced speech, the larynx provides quasi-periodic acoustic excitation of the vocal tract. In most electrolarynxes, mechanical vibrations are produced by a linear electromechanical actuator, the armature of which percusses against a metal or plastic plate at a frequency within the range of glottal excitation. In this paper, the intelligibility of speech produced using a novel hands-free actuator is compared to speech produced using a conventional electrolarynx. Two able-bodied speakers (one male, one female) performed a closed response test containing 28 monosyllabic words, once using a conventional electrolarynx and a second time using the novel design. The resulting audio recordings were randomized and replayed to ten listeners who recorded each word that they heard. The results show that the speech produced using the hands-free actuator was substantially more intelligible to the majority of listeners than that produced using the conventional electrolarynx. The new actuator has properties (size, weight, shape, cost) which lends itself as a suitable candidate for possible hands-free operation. This is one of the research ideals for the group and this test methodology presented as a means of testing intelligibility. This paper outlines the procedure for the possible testing of intelligibility of electrolarynx designs.	acoustic cryptanalysis;armature (computer animation);intelligibility (philosophy);iteration;quasiperiodicity;randomized algorithm;signal processing;tract (literature);wake	Brian Madden;Mark Nolan;Edward Burke;James Condron;Eugene Coyle	2011			vocal tract;electrolarynx;speech recognition;computer science;signal processing;intelligibility	HCI	-7.797164517629518	-85.09688130611693	140445
c10d09a599ce555ec5daae029a44ccbc1274bc1c	advanced time shrinking using a drop classifier based on codec features		We present an integrated approach of full-band audio time scale modification for Voice over IP communication. The concept is based on a low complexity adaptive playout method that uses frame dropping and audio concealment for time shrinking and stretching, respectively. The existing version of this method is improved using a classifier that assists in choosing which audio frames can be dropped with the least subjective impact on audio quality. To maintain low complexity, we exclusively use audio signal features that are available in the audio codec. The classification of audio frames improves audio quality of the existing method without classification by 0.5 Mean Opinion Score points while requiring significantly less computational complexity by a factor of ca 10.	codec;computational complexity theory;playout;sound quality;statistical classification	Jochen Issing;Nikolaus Färber;Reinhard German	2015			speech recognition;pattern recognition;artificial intelligence;codec;computer science;classifier (linguistics)	HCI	-10.185958008834168	-89.47605725237024	140481
6bb0055a5a4232be6b8b29407ab7c07f1988b07d	the influence of preceding consonant on perceptual epenthesis in japanese		Research on perceptual epenthesis in Japanese has revealed high back [ɯ] to be the vowel commonly perceived in illicit consonant sequences. However, loanword studies suggest that there are three epenthetic vowels, which reflect phonotactic restrictions on certain consonant + vowel sequences. Expanding previous perception studies, this paper investigates the extent to which perceptual epenthesis in Japanese is also constrained by the language’s phonotactic patterns. In particular, we seek to determine to what extent the preceding consonant influences perceptual epenthesis, reflecting native phonotactics. Our results show that the preceding consonant does influence the vowel perceived yet, at the same time, there is a strong bias toward perceiving [ɯ] in contexts not predicted by the language’s phonotactic patterns.	perceptual computing;software design pattern	Elizabeth Hume;Kathleen Currie Hall;Wakayo Mattingley	2015			cognitive psychology;consonant;perception;epenthesis;psychology	HCI	-10.325556967573112	-81.20282518223566	140510
47facf2e9ca221ee024271c2bae36a682e300488	a noise robust content-based music retrieval system for mobile devices	music query content based music retrieval system mobile device humming audio singing audio music database noise disturbance noise robustness feature extraction;noise disturbance;audio signal processing;mobile device;noise robustness music information retrieval content based retrieval working environment noise spatial databases audio databases feature extraction engines noise level signal to noise ratio;working environment noise;noise robustness;humming audio;noise level;engines;feature extraction;spatial databases;music information retrieval;success rate;mobile handsets;music query;audio databases;signal to noise ratio;singing audio;audio feature extraction;content based retrieval;music database;music;content based music retrieval system;content based music retrieval;music audio databases audio signal processing content based retrieval feature extraction mobile handsets	This paper proposes a noise robust content-based music retrieval system for mobile devices. It takes the user's humming/singing audio input and queries the desired songs from music database. Since the system is deliberately designed for mobile devices, noise disturbance are inevitable in practical application. In order to improve the noise robustness of the retrieval system, we propose a new humming/singing audio feature extraction algorithm. A frame-to-note matching engine is employed to compute the similarity distance. The experimental results show that the proposed algorithm is efficient and robust under various noisy environments and SNR levels. For 91.46% queries, the correct songs can be retrieved among the top-10 matches in clean condition. About 85% average success rate of top-10 returns can be obtained in most noisy conditions. Even in low SNR conditions, the proposed algorithm can still achieve acceptable performance.	algorithm;feature extraction;list of online music databases;mobile device;signal-to-noise ratio	Lihui Guo;Xin He;Yaxin Zhang;Yue Lu;Ke Peng	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4285127	computer vision;speech recognition;audio signal processing;feature extraction;computer science;music;mobile device;multimedia	Robotics	-8.353023707528234	-93.98297261185873	140511
1b4bbfcfd71fa866cee4ce1244a133fb9c6489a0	blind clustering of music recordings based on audio fingerprinting	rand index;audio recording fingerprint recognition multiple signal classification humans encoding music information retrieval libraries acoustic signal processing acoustical engineering distortion measurement;pattern clustering;unsupervised clustering;audio clips;audio fingerprinting;rand index audio fingerprinting clustering;speech;music recording blind clustering;audio encoding;data mining;hierarchical agglomerative clustering;mel frequency cepstral coefficient;audio coding;indexes;multiple signal classification;fingerprint recognition;clustering;feature extraction;indexation;unsupervised clustering framework;number of clusters;audio transmission methods;source music recordings;source music recordings music recording blind clustering audio encoding audio transmission methods audio fingerprinting techniques unsupervised clustering framework audio clips hierarchical agglomerative clustering;audio fingerprinting techniques;music;pattern clustering audio coding music	Although multiple music recordings may sound identical to a human listener, the underlying representations of sound may differ due to the variations in their audio encoding and/or transmission methods. In contrast to the existing audio-fingerprinting techniques, which establishes the fingerprint of each source music to identify unknown, “distorted” audio clips, this paper proposes an unsupervised clustering framework to identify unknown, “distorted” audio clips derived from the same source music. By grouping together audio data derived from the same music, the human effort required to label music data can be dramatically reduced. This work develops methods to measure the similarities between audio clips and use hierarchical agglomerative clustering to group together audio clips that are similar to one another. Also proposed is a method based on the Rand Index to determine the optimal number of clusters automatically in relation to the number of source music recordings.	acoustic fingerprint;cluster analysis;digital audio workstation;distortion;experiment;fingerprint (computing);hierarchical clustering;machine learning;programming paradigm;radio fingerprinting;rand index;sound card;supervised learning	Wei-Ho Tsai;Wei-Che Hsieh	2009	2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2009.152	computer vision;audio mining;speech recognition;rand index;feature extraction;computer science;speech;multiple signal classification;machine learning;pattern recognition;music;multimedia;fingerprint recognition;statistics	SE	-8.346765318315162	-92.62933836573164	140672
8eae8ceba3c08a9667a7edd13ee7de79b2a881e5	investigating the impact of dialect prestige on lexical decision		The speech signal encodes both a talker's message and indexical information about a talker's identity. Dialectal variation is one way in which non-linguistic information about a talker is conveyed through her speech. A talker's dialect tends to correlate strongly with her demographic background, and listeners are known to form beliefs about speakers based on their dialect alone: talkers of lower-status dialects are consistently downgraded on positively-valued attributes relative to talkers of canonical dialects. Hypothesizing that pre-formed beliefs about a low-status talker might impact optimal perception of her speech, this study investigated the influence of the relative prestige of talker dialect on listeners' behavior in three lexical decision experiments. The finding of significantly increased propensity to incorrectly reject words uttered in an arguably low-prestige variety of American English relative to both normative General American English and British English suggests that talker status may play a role in the success with which talker messages are perceived by listeners. These results as well as unexpected interactions of dialect and word frequency in some but not all experiments are discussed in the context of signal detection theory.	detection theory;experiment;interaction;word lists by frequency	Mairym Lloréns Monteserín;Jason Zevin	2016		10.21437/Interspeech.2016-1549	speech recognition;linguistics	NLP	-10.576404680418424	-81.86939816306658	140688
52658ec8dfe6d252f2e127c470b6859b51c97458	a knowledge based system for recognition and transcription of xvith century guitar tablatures			guitar pro;knowledge-based systems;medical transcription	D. Derrien-Péden	1993			speech recognition;acoustics;engineering;communication	Vision	-14.692830368151306	-85.71936183119145	140911
daaf3e5b4b0a7619f8d9acc7c21d065fee62be8a	efficient classification of noisy speech using neural networks	neural networks classification algorithms speech enhancement computational complexity error analysis nearest neighbor searches speech coding noise generators information technology electronic mail;classification algorithm;learning algorithm;neural nets;kalman filters;neural network classifier;speech enhancement;computational complexity;computational complexity speech enhancement neural nets kalman filters;part of speech;error rate;speech enhancement algorithm noisy speech classification neural networks training algorithms extended kalman filter levenberg marquadt algorithm computational complexity hangover mechanism;nearest neighbor classifier;extended kalman filter;training algorithm;neural network	The classification of active speech vs. inactive speech in noisy speech is an important part of speech applications, typically in order to achieve a lower bit-rate. In this work, the error rates for raw classification (i.e. with no hangover mechanism) of noisy speech obtained with traditional classification algorithms are compared to the rates obtained with Neural Network classifiers, trained with different learning algorithms. The traditional classification algorithms used are the linear classifier, some Nearest Neighbor classifiers and the Quadratic Gaussian classifier. The training algorithms used for the Neural Networks classifiers are the Extended Kalman Filter and the Levenberg-Marquadt algorithm. An evaluation of the computational complexity for the different classification algorithms is presented. Our noisy speech classification experiments show that using Neural Network classifiers typically produces a more accurate and more robust classification than other traditional algorithms, while having a significantly lower computational complexity. Neural Network classifiers may therefore be a good choice for the core component of a noisy speech classifier, which would typically also include a hangover mechanism and possibly a speech enhancement algorithm.	algorithm;artificial neural network;computation;computational complexity theory;experiment;extended kalman filter;linear classifier;machine learning;speech enhancement	Cathy Shao;Martin Bouchard	2003		10.1109/ISSPA.2003.1224714	kalman filter;speech recognition;part of speech;word error rate;computer science;machine learning;linear classifier;pattern recognition;extended kalman filter;computational complexity theory;artificial neural network	ML	-16.964270305375948	-89.94852585732554	140930
2db0007c65f673652d3ea31fcd56581cceb9c0ee	characterization of atypical vocal source excitation, temporal dynamics and prosody for objective measurement of dysarthric word intelligibility	vocal source excitation;temporal dynamics;dysarthria;linear prediction;intelligibility	Objective measurement of dysarthric speech intelligibility can assist clinicians in the diagnosis of speech disorder severity as well as in the evaluation of dysarthria treatments. In this paper, several objective measures are proposed and tested as correlates of subjective intelligibility. More specifically, the kurtosis of the linear prediction residual is proposed as a measure of vocal source excitation oddity. Additionally, temporal perturbations resultant from imprecise articulation and atypical speech rates are characterized by shortand long-term temporal dynamics measures, which in turn, are based on log-energy dynamics and on an auditory-inspired modulation spectral signal representation, respectively. Motivated by recent insights in the communication disorders literature, a composite measure is developed based on linearly combining a salient subset of the proposed measures with conventional prosodic parameters. Experiments with the publicly-available ‘Universal Access’ database of spastic dysarthric speech (10 patient speakers; 300 words spoken in isolation, per speaker) show that the proposed composite measure can achieve correlation with subjective intelligibility ratings as high as 0.97; thus the measure can serve as an accurate indicator of dysarthric speech intelligibility. 2011 Elsevier B.V. All rights reserved.	biconnected component;elegant degradation;intelligibility (philosophy);modulation;resultant;semantic prosody;spastic;user agent	Tiago H. Falk;Wai-Yip Chan;Fraser Shein	2012	Speech Communication	10.1016/j.specom.2011.03.007	speech recognition;linear prediction;linguistics;intelligibility;statistics	NLP	-9.925203232734361	-87.73679840600903	141097
d24434b77943634e3d2ba150649d59ec3d746d2f	improvements to the equal-parameter bic for speaker diarization	speaker diarization	This paper discusses a set of modifications regarding the use of the Bayesian Information Criterion (BIC) for the speaker diarization task. We focus on the specific variant of the BIC that deploys models of equal or roughly equal statistical complexity under partitions of different number of speakers and we examine three modifications. Firstly, we investigate a way to deal with the permutation-invariance property of the estimators when dealing with mixture models, while the second is derived by attaching a weakly informative prior over the space of speaker-level state sequences. Finally, based on the recently proposed segmental-BIC approach, we examine its effectiveness when mixture of gaussians are used to model the emission probabilities of a speaker. The experiments are carried out using NIST rich transcription evaluation campaign for meeting data and show improvement over the baseline setting.	baseline (configuration management);bayesian information criterion;experiment;mixture model;speaker diarisation;transcription (software)	Themos Stafylakis;Xavier Anguera Miró	2010			speech recognition;artificial intelligence;mixture model;estimator;speaker diarisation;pattern recognition;nist;bayesian information criterion;baseline setting;computer science	NLP	-17.333950240048935	-90.991971738624	141120
aa322643d517625fc8a7904ae4065a390dd3abd5	application of vector filtering to pattern recognition	speaker identification;vector filtering;pattern recognition vectors cepstral analysis error analysis information filtering information filters system testing pattern analysis data mining speech processing;speech processing;information filtering;cepstral coefficients vector filtering pattern recognition contextual principal components speaker identification;data mining;speaker recognition;error analysis;principal component analysis speaker recognition filtering theory;cepstral analysis;vectors;principal component analysis;pattern recognition;contextual principal components;error rate;system testing;pattern analysis;information filters;filtering theory;principal component;cepstral coefficients	In this paper, we present a new formalism, called vector filtering, which consists in transforming a sequence of vectors through a matricial filtering. This formalism allows us to unify a number of classical approaches. We also show how vector filtering can be integrated in a pattern recognition system. We then propose a new filtering, called contextual principal components (CPC), which consists in calculating principal components on vectors augmented by their context. Then, we apply the new filtering in the framework of text-independent speaker identification, which consists in identifying a speaker by the voice without knowledge about the phonetic content. By using this new filtering, we are able to decrease the identification error rate of roughly 20 % compared to a system using the classical cepstral coefficients augmented by their delta parameters.	cartesian perceptual compression;cepstrum;coefficient;pattern recognition;semantics (computer science);speaker recognition;speech recognition	Ivan Magrin-Chagnolleau;Geoffrey Durou	2000		10.1109/ICPR.2000.903577	speaker recognition;speech recognition;computer science;machine learning;pattern recognition;speech processing;principal component analysis	Vision	-10.210685482131073	-91.5053850997406	141124
ef0a01658b89cddfa98d6101c3176e2868102483	audiovisual asynchrony detection for speech and nonspeech signals		This study investigated the “intersensory temporal synchrony window” [1] for audiovisual (AV) signals. A speede d asynchrony detection task was used to measure each participant’s temporal synchrony window for speech and nonspeech signals over an 800-ms range of AV asynch ro ies. Across three sets of stimuli, the video-leading thr es old for asynchrony detection was larger than the audio-lead ing threshold, replicating previous findings reported i n the literature. Although the audio-leading threshold d id not differ for any of the stimulus sets, the video-lead ing threshold was significantly larger for the point-light displa y (PLD) condition than for either the full-face (FF) or non speech (NS) conditions. In addition, a small but reliable phon otactic effect of visual intelligibility was found for the FF condition. High visual intelligibility words produced larger v ideoleading thresholds than low visual intelligibility words. Relationships with recent neurophysiological data o n multisensory enhancement and convergence are discus sed.	asynchronous i/o;asynchrony (computer programming);discus;intelligibility (philosophy);programmable logic device;reflow soldering;sed	Brianna L. Conrey;David B. Pisoni	2003			asynchrony;acoustics;psychology	ML	-8.661009404115303	-81.1264948998005	141216
15e7614738ac5ec01d3a8faa89d008304e0edf07	discriminative acoustic model using eigenspace mapping for rapid speaker adaptation	eigenvalues and eigenfunctions;eigenvalues and eigenfunctions acoustic signal processing speaker recognition covariance matrices;acoustic modeling;acoustic signal processing;rapidly adapting;speaker recognition;speaker independent;covariance matrices;block diagonalization;speaker dependent discrimination discriminative acoustic model rapid speaker adaptation correlation time invariant characteristics acoustic environment speaker environment eigendirections utterance covariance matrix fast speaker adaptation algorithm eigenspace mapping eigmap speaker independent models discriminative acoustic models unsupervised adaptation experiments mllr baseline recognizer baseline models;speaker dependent;loudspeakers maximum likelihood linear regression speech processing speech recognition maximum likelihood decoding linear discriminant analysis robustness natural languages acoustic testing training data;speaker adaptation;covariance matrix	It is widely believed that strong correlations exist across an utterance as a consequence of time-invariant characteristics of speaker and acoustic environments. It is verified in this paper that the first primary eigendirections of the utterance covariance matrix are speaker dependent. Based on this observation, a fast speaker adaptation algorithm entitled Eigenspace Mapping (EigMap) is proposed and described. EigMap rapidly adapts the speaker independent models by constructing discriminative acoustic models in the test speaker's eigenspace. Unsupervised adaptation experiments show that EigMap is effective in improving baseline models using very limited amounts of adaptation data with superior performance to conventional adaptation technique such as block diagonal MLLR. A relative improvement of 18.4% over baseline recognizer is achieved using EigMap with only about 4.5 seconds of adaptation data. It is also demonstrated that EigMap is additive to MLLR by encompassing the speaker dependent discrimination information. A significant relative improvement of 24.6% over baseline is observed by combining MLLR and EigMap techniques.	acoustic cryptanalysis;acoustic model	Bowen Zhou;John H. L. Hansen	2003		10.1109/ICASSP.2003.1198779	speaker recognition;covariance matrix;speaker diarisation;speech recognition;computer science;machine learning;pattern recognition;mathematics;statistics	NLP	-16.82909701974919	-91.91796710148539	141247
cc73eca37f62b37fa25f443e12da5cb7a609dd6a	in the beginning there were the weird: a phonotactic novelty preference in adult word learning		It has been argued that words that contain difficultto-pronounce sound sequences may be avoided in production, causing words with difficult phonotactics to drop out of the language at a disproportionate rate. We argue that there is also an opposing pressure favoring phonetically unusual words. We show that, at least for adults, word learning is more successful for words with unfamiliar phonetic properties to the listener. After a ten minute ambiguous training session where two novel objects were presented with an audio recording of nonce words, subjects were tested on their memory of the creatures’ “names”. The results show a preference for words that contain illegal word-initial consonant clusters over words that obey the subjects’ native language phonotactics.	cryptographic nonce	Lamia Haddad Johnston;Vsevolod Kapatsinski	2011			phonotactics;novelty;communication;natural language processing;psychology;artificial intelligence	HCI	-10.856747543856601	-80.78809125859534	141352
03fb05c0abb3e85accd1d18f705b98fb06acede3	sentence-level control vectors for deep neural network speech synthesis		This paper describes the use of a low-dimensional vector representation of sentence acoustics to control the output of a feed-forward deep neural network text-to-speech system on a sentence-by-sentence basis. Vector representations for sentences in the training corpus are learned during network training along with other parameters of the model. Although the network is trained on a frame-by-frame basis, the standard framelevel inputs representing linguistic features are supplemented by features from a projection layer which outputs a learned representation of sentence-level acoustic characteristics. The projection layer contains dedicated parameters for each sentence in the training data which are optimised jointly with the standard network weights. Sentence-specific parameters are optimised on all frames of the relevant sentence – these parameters therefore allow the network to account for sentence-level variation in the data which is not predictable from the standard linguistic inputs. Results show that the global prosodic characteristics of synthetic speech can be controlled simply and robustly at run time by supplementing basic linguistic features with sentencelevel control vectors which are novel but designed to be consistent with those observed in the training corpus.		Oliver Watts;Zhizheng Wu;Simon King	2015			speech recognition;neural gas;pattern recognition;artificial intelligence;artificial neural network;time delay neural network;training set;sentence;computer science;speech synthesis	NLP	-17.563132253742072	-87.6761497317118	141393
cb503887065a812f6739dfbf52e0a8121b8ac3e3	performance comparison of intrusive and non-intrusive instrumental quality measures for enhanced speech	reverberation;instruments;acoustic distortion;speech;speech enhancement;acoustic measurements	Instrumental quality prediction of speech processed by enhancement algorithms has become crucial with the proliferation of far-field speech applications. To date, while several instrumental measures have been proposed and standardized, their performance under a wide range of acoustic conditions and enhancement algorithms is still unknown. This paper aims to fill this gap. Specifically, the performance of eleven instrumental measures are compared; four are non-intrusive measures, i.e. not requiring a clean reference signal, and seven intrusive. Simulated and recorded speech under four different acoustic conditions involving varying levels of reverberation and noise are explored, as well as processed by three single- and multi-channel enhancement algorithms. Experimental results show that a recently developed non-intrusive measure called SRMRnorm outperforms all other considered measures in terms of overall quality prediction. The well-known PESQ measure, in turn, showed to better predict the perceived amount of reverberation, followed by SRMRnorm. These results are promising, as the latter measure does not require access to a clean reference signal, thus has the potential to be used for enhancement algorithm optimization in real-time.	acoustic cryptanalysis;algorithm;mathematical optimization;pesq;real-time clock;whole earth 'lectronic link	Anderson R. Avila;Benjamin Cauchi;Stefan Goetze;Simon Doclo;Tiago H. Falk	2016	2016 IEEE International Workshop on Acoustic Signal Enhancement (IWAENC)	10.1109/IWAENC.2016.7602907	speech recognition;acoustics;engineering;communication	Visualization	-9.80599657500568	-88.29154016939025	141424
7b5028f5a8ad7a83aa6dcdaa63fd936d1ce21322	frequency-warped and stabilized time-varying cepstral coefficients		This paper presents a set of cepstral parameters based on timevarying linear prediction. The lattice filter structure is utilized to accommodate efficient stabilization of models and a Bark-like warped frequency scale. As the proposed cepstral features are based on non-stationary spectral analysis there is a potential for complementary information not captured in conventional features. In classification and recognition experiments, the proposed features are shown to improve performance when augmenting MFCCs.	cepstrum;experiment;lattice phase equaliser;spectral density estimation;stationary process	Trond Skogstad;Torbjørn Svendsen	2011			speech recognition;pattern recognition;artificial intelligence;mel-frequency cepstrum;computer science	Vision	-12.5197466632824	-91.41774697573125	141566
159a47df26c67594d32c65cb1aacca4ea118781e	on the effects of filterbank design and energy computation on robust speech recognition	bandwidth estimation;front end;robust speech recognition;experimental analysis;speech processing;teager energy cepstrum coefficients;speech;gabor filters;filterbank design;energy computation;noisy speech signals;teager kaiser operator;noise measurement;gammatone filters;mel frequency cepstral coefficient;error analysis;automatic speech recognition;time frequency analysis bandpass filters cepstrum analysis error analysis parameter estimation robustness spectral analysis speech processing speech recognition;relative error;cepstral analysis;estimation;perceptual linear predicion features filterbank design energy computation robust speech recognition noisy speech signals teager energy cepstrum coefficients mel frequency cepstral coefficients automatic speech recognition triangular filters gabor filters gammatone filters teager kaiser operator;channel bank filters;bandpass filters;speech recognition channel bank filters gabor filters signal denoising;mel frequency cepstral coefficients;error rate;speech recognition;bandwidth;robustness;bandpass filter;parameter estimation;triangular filters;spectral analysis;noise noise measurement speech recognition speech bandwidth estimation cepstral analysis;time frequency analysis;energy estimate;noise;signal denoising;alternative energy;cepstrum analysis;perceptual linear predicion features	In this paper, we examine how energy computation and filterbank design contribute to the overall front-end robustness, especially when the investigated features are applied to noisy speech signals, in mismatched training-testing conditions. In prior work (“Auditory Teager energy cepstrum coefficients for robust speech recognition,” D. Dimitriadis, P. Maragos, and A. Potamianos, in Proc. Eurospeech'05, Sep. 2005), a novel feature set called “Teager energy cepstrum coefficients” (TECCs) has been proposed, employing a dense, smooth filterbank and alternative energy computation schemes. TECCs were shown to be more robust to noise and exhibit improved performance compared to the widely used Mel frequency cepstral coefficients (MFCCs). In this paper, we attempt to interpret these results using a combined theoretical and experimental analysis framework. Specifically, we investigate in detail the connection between the filterbank design, i.e., the filter shape and bandwidth, the energy estimation scheme and the automatic speech recognition (ASR) performance under a variety of additive and/or convolutional noise conditions. For this purpose: 1) the performance of filterbanks using triangular, Gabor, and Gammatone filters with various bandwidths and filter positions are examined under different noisy speech recognition tasks, and 2) the squared amplitude and Teager-Kaiser energy operators are compared as two alternative approaches of computing the signal energy. Our end-goal is to understand how to select the most efficient filterbank and energy computation scheme that are maximally robust under both clean and noisy recording conditions. Theoretical and experimental results show that: 1) the filter bandwidth is one of the most important factors affecting speech recognition performance in noise, while the shape of the filter is of secondary importance, and 2) the Teager-Kaiser operator outperforms (on the average and for most noise types) the squared amplitude energy computation scheme for speech recognition in noisy conditions, especially, for large filter bandwidths. Experimental results show that selecting the appropriate filterbank and energy computation scheme can lead to significant error rate reduction over both MFCC and perceptual linear predicion (PLP) features for a variety of speech recognition tasks. A relative error rate reduction of up to ~ 30% for MFCCs and ~ 39% for PLPs is shown for the Aurora-3 Spanish Task.	approximation error;auditory processing disorder;coefficient;computation;filter bank;mel-frequency cepstrum;pl/p;speech recognition;utility functions on indivisible goods	Dimitrios Dimitriadis;Petros Maragos;Alexandros Potamianos	2011	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2010.2092766	speech recognition;acoustics;computer science;pattern recognition;speech processing;band-pass filter;mel-frequency cepstrum;statistics	ML	-12.600399317062733	-91.13905420966654	141641
bccd6b9656988e0310d90533a8715ddea178917a	effects of speaking rate and vowel length on formant frequency displacement in japanese	macquarie university institutional repository;speaking rate;251901;acoustic analysis;researchonline;production du langage;digital repository;japonais;macquarie university;phonetica 2009;vol 66;phonological encoding;longueur vocalique;acoustic phonetics;vitesse d elocution;language production;phonetique acoustique;no 3;vowel length;japanese	This study examined effects of phonemic vowel length and speaking rate, two factors that affect vowel duration, on the first and second formants of all vowels in Japanese. The aim was to delineate the aspects of formant displacement that are governed by the physiological proclivity of vowel production shared across languages, and the aspects that reveal language-specific phenomena. Acoustic analysis revealed that the phonemic long vowels occupied a more peripheral portion of the F1 x F2 vowel space than the phonemic short vowels (effect of vowel length), but effects of speaking rate were less clear. This was because of the significant interactions of the two effects: the formants of phonemic short vowels were more affected by speaking rates than the phonemic long vowels. Regression analyses between F2 and duration revealed that formant displacement occurs when vowels are less than 200 ms. Similarities and differences found for Japanese and English are discussed in terms of physiological proclivity of vowel production versus language-specific phonological encoding.		Yukari Hirata;Kimiko Tsukada	2009	Phonetica	10.1159/000235657	psychology;japanese;digital library;speech recognition;mid vowel;acoustics;stress;philosophy;relative articulation;linguistics;nasal vowel	NLP	-10.690678916488025	-81.77042827301386	141647
3ea405c8fa80f1589dfe1105f565f7f90b8d9ded	discriminative training of gaussian mixture bigram models with application to chinese dialect identification	gaussian mixture;acoustic modeling;chinese dialect identification;minimum classification error algorithm;discriminative training;stochastic model;minimum classification error;gaussian mixture bigram model;system simulation;article	This study focuses on the parametric stochastic modeling of characteristic sound features that distinguish languages from one another. A new stochastic model, the so-called Gaussian mixture bigram model (GMBM), that allows exploitation of the acoustic feature bigram statistics without requiring transcribed training data is introduced. For greater efficiency, a minimum classification error (MCE) algorithm is employed to accomplish discriminative training of a GMBM-based Chinese dialect identification system. Simulation results demonstrate the effectiveness of the GMBM for dialect-specific acoustic modeling, and use of this model allows the proposed system to distinguish between the three major Chinese dialects spoken in Taiwan with 94.4% accuracy. 2002 Elsevier Science B.V. All rights reserved.	acoustic cryptanalysis;acoustic model;algorithm;bigram;cepstrum;codebook;discriminative model;language identification;linuxmce;simulation;speaker recognition;statistical classification;stochastic modelling (insurance);weight function	Wuei-He Tsai;Wen-Whei Chang	2002	Speech Communication	10.1016/S0167-6393(00)00090-X	speech recognition;computer science;stochastic modelling;machine learning;pattern recognition	AI	-19.034220517495612	-88.9479547148209	141706
f1eb8d68a6fce6c62c36b4435c255a50c2e2a218	evolving emotional prosody		Emotion is expressed by prosodic cues, and this study uses the active interactive Genetic Algorithm to search a wide space for sad and angry parameters of intensity, F0, and duration in perceptual resynthesis experiments with users. This method avoids large recorded databases and is flexible for exploring prosodic emotion parameters. Solutions from multiple runs are analyzed graphically and statistically. Average results indicate parameter evolution by emotion, and appear best for sad speech. Solutions are quite successfully classified by CART, with duration as main predictor.	database;emotion recognition;experiment;genetic algorithm;ibm notes;kerrison predictor;rom cartridge;sadness;semantic prosody	Cecilia Ovesdotter Alm;Xavier Llorà	2006			emotional prosody;artificial intelligence;genetic algorithm;pattern recognition;perception;computer science	NLP	-8.935311028902143	-82.76690748822543	141780
b939c8141dc262643b50f72a67fda0189a6c1502	measuring tongue movements during speech: adaptation of a magnetic jaw-tracking system	traitement signal;magnetic device;tracking system;measurement;magnetic;speech articulation;captador medida;lengua;detection mouvement;articulation parole;sistema coordenadas;measurement sensor;capteur mesure;signal processing;articulacion palabra;kinematic;poursuite cible;tongue;deteccion movimiento;dispositivo magnetico;dispositif magnetique;systeme coordonnee;target tracking;langue;procesamiento senal;motion detection;movement;coordinate system	The purpose of the present investigation was to determine whether measurements of tongue movement during speech could be obtained with an electronic device originally designed to record jaw movements via a magnetized pellet. The findings indicated that the system allowed basic quantification of tongue movements in a straightforward manner. The primary advantages of this system are that no distracting wires are attached to the pellet, and it is much less costly than other systems used for this purpose. Its main disadvantages are that it is unable to track multiple tongue fleshpoints simultaneously, lacks an anatomically based coordinate system, and the head must remain still during recordings.	tracking system	Christopher Dromey;Shawn L. Nissen;Petrea Nohr;Samuel G. Fletcher	2006	Speech Communication	10.1016/j.specom.2005.05.003	movement;kinematics;magnet;speech recognition;tracking system;computer science;coordinate system;signal processing;manner of articulation;measurement	NLP	-6.2882882277481364	-84.93021231510366	141853
136acf933c6e71fb81fe79b275eb0f512de8469a	cnn-based phonetic segmentation refinement with a cross-speaker setup		This work proposes a method to improve the performance of automatic phonetic alignment of speech data. The method uses a deep convolutional neural network (CNN) trained on a combination of acoustic features extracted from labeled data to fine tune the position of each boundary within a fixed-size window around the original boundary position. The proposed method is robust to speaker identity, which means that a system trained with enough labeled data can be used to fine tune alignment on any speech file, regardless of speaker identity. With an absolute gain between 20% and 33% in cross speaker scenario, our results demonstrate the applicability of deep learning for this task.		Luis Gustavo D. Cuozzo;Diego Augusto Santos Silva;Mario Uliani Neto;Flávio Olmos Simões;Edson Jose Nagle	2018		10.1007/978-3-319-99722-3_45	labeled data;speech recognition;convolutional neural network;absolute gain;deep learning;artificial intelligence;computer science;segmentation	Vision	-16.54945218459237	-88.98782122062988	141903
897bd0996ec72fe8e85a504b85c8dcc5c6aa1112	target speaker separation in a multisource environment using speaker-dependent postfilter and noise estimation	signal to interference ratio target speaker separation multisource environment speaker dependent postfilter noise estimation target speech nonstationary real life noise scenario spatial beamformer gcc phat estimated time delay of arrival postfilters wiener filter minimum mean square error estimator log amplitude model driven postfilter mdp speech signal statistics target speaker gaussian mixture model mmse speech enhancement chime development set chime challenge nonstationary interfering sources;speech speech enhancement noise measurement signal to noise ratio estimation;noise estimation;least mean squares methods;gaussian processes;wiener filters;array signal processing;speech enhancement;speaker recognition;wiener filters array signal processing gaussian processes least mean squares methods speaker recognition speech enhancement;speaker dependent;non stationary noise multisource noise speech enhancement speech quality;article in monograph or in proceedings	In this paper, we present a novel system for enhancing a target speech corrupted in a non-stationary real-life noise scenario. The proposed system consists of one spatial beamformer based on GCC-PHAT-estimated time-delay of arrival followed by three postfilters applied in a sequential way, namely: Wiener filter, minimum mean square error estimator (MMSE) of the log-amplitude, and a model-driven postfilter (MDP) that relies on particular speech signal statistics captured by target speaker Gaussian mixture model. The beamformer accounts for the directional interferences while the MMSE speech enhancement suppresses the stationary background noise, and MDP contributes to suppress the non-stationary sources from the binaural mixture. In our evaluation, multiple objective quality metrics are used to report the speech enhancement and separation performance, averaged on the CHiME development set. The proposed system performs better than standard state-of-the-art techniques and shows comparable performance with other systems submitted to the CHiME challenge. More precisely, it is successful in suppressing the non-stationary interfering sources at different SNR levels supported by the relatively high scores for signal-to-interference-ratio.	beamforming;binaural beats;interference (communication);major stationary source;mean squared error;mixture model;model-driven integration;real life;signal-to-noise ratio;speech enhancement;stationary process;wiener filter	Pejman Mowlaee Begzade Mahale;Rahim Saeidi	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6639071	speaker recognition;speech recognition;computer science;pattern recognition;gaussian process;mathematics;statistics	Robotics	-13.443774624188572	-92.0085773330524	142039
00d448f92362a87947309d9d4e1e7ad21a7f8450	unsupervised discovery of an extended phoneme set in l2 english speech for mispronunciation detection and diagnosis		Second language (L2) speech is often labelled with the native, phoneme categories. Hence, we often observe segments for which it is difficult, if not impossible, to decide on a categorical phoneme label. We refer to these segments as “non-categorical” phoneme units. Existing approaches to mispronunciation detection and diagnosis (MDD) mostly focus on categorical phoneme errors, where one native phoneme is substituted for another. However, noncategorical errors are not considered. To better represent L2 speech for improved MDD, this work aims to discover an Extended Phoneme Set in L2 speech (L2-EPS) which includes not only the categorical phonemes based on the native set, but also non-categorical phoneme units. We apply an optimized k-means algorithm to cluster phoneme-based phonemic posterior-grams (PPGs), which are generated through an acoustic-phonemic model (APM). Then we find the L2-EPS based on analysis of the clusters obtained. We verified experimentally that the non-categorical phonemes in L2-EPS can extend the native phoneme categories to better describe L2 speech. Hence L2-EPS can enrich the existing approaches to MDD for better performance.	acoustic cryptanalysis;advanced power management;algorithm;experiment;grams;k-means clustering;model-driven engineering;n-gram	Shaoguang Mao;Xu Li;Kun Li;Zhiyong Wu;Xunying Liu;Helen M. Meng	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462635	categorical variable;filter (signal processing);feature extraction;artificial intelligence;hidden markov model;pattern recognition;mel-frequency cepstrum;computer science	Vision	-17.918867703145082	-85.16910262084329	142089
aa104524adbaadfe6ccc999496d4de9dabea72e1	computerized voice recognition systems and their application to the mobility impaired	computerized voice recognition system		speech recognition	Michael A. Chamberlain	1993				Mobile	-6.839419187316503	-84.20909075705381	142277
62f4d9da1c4f624f88bb5a40320dd6617a2a2def	the variability of syllable patterns in tashlhiyt berber and polish		Abstract In this study we investigate the timing of word-initial clusters and its relation to distinct phonological syllable parses in Tashlhiyt Berber and Polish. To this end, we use experimental, articulographic data (steps 1 and 2) combined with computer-based simulation (step 3). In step 1, we test how temporal properties of consonantal clusters such as overlap can vary within a single language. In step 2, we relate articulatory coordination patterns to distinct phonological syllable parses, involving simple and complex onsets, in order to calculate stability indices for each language. In step 3, we test the robustness of these stability patterns by adding anchor variability to the system. The analysis reveals that variability plays a different role in the two languages. Tashlhiyt shows a tight cluster timing with low variability in overlap across clusters. The phonetic heuristics for Tashlhiyt reveal a simple onset parse with a phonetic outcome that is strikingly robust against temporally induced variability. In contrast, Polish shows a considerably high variability in overlap between the different cluster types. The phonetic heuristics for Polish reveal a general trend towards a complex onset parse, but this time the picture is less clear. Furthermore, the Polish timing patterns are more sensitive to anchor variability than Tashlhiyt. This difference in the degree of sensitivity to variability is interpreted to be the result of different language-specific regulatory mechanisms mediating between different levels of description, such as segmental context and prosodic marking of different pragmatic functions. Natural human communication requires both stability and variability regulated by different needs and constraints within a given language, leading to differing degrees of flexibility in the hierarchical network of local weights and clocks attached to the different constituents of the prosodic hierarchy.	heart rate variability;item unique identification;onset (audio);parsing;spatial variability;syllable;tree network	Anne Hermes;Doris Mücke;Bastian Auris	2017	J. Phonetics	10.1016/j.wocn.2017.05.004	robustness (computer science);speech recognition;communication;parsing;hierarchy;syllable;heuristics;computer science	NLP	-10.96787266153109	-80.73163416922101	142336
2cf3985276f5960e0719a307470f9f396bc6c789	inca algorithm for training voice conversion systems from nonparallel corpora	text independent cross lingual voice conversion frame alignment gaussian mixture model gmm nonparallel training corpus;loudspeakers speech analysis signal analysis speech synthesis iterative methods government signal processing laboratories signal synthesis;iterative alignment method;source speaker;speech synthesis;gaussian processes;training corpus;helium;signal analysis;speech analysis;government;nonparallel training corpus;transformation function;speech enhancement;nonparallel training corpus inca algorithm voice conversion system gaussian mixture model acoustic vectors target speaker source speaker transformation function training corpus iterative alignment method cross lingual voice conversion;inca algorithm;text independent cross lingual voice conversion;cross lingual voice conversion;iterative methods;voice conversion;voice conversion system;gaussian mixture model;frame alignment;loudspeakers;signal processing;speech recognition;gaussian mixture model gmm;signal synthesis;acoustic vectors;target speaker;subjective evaluation;speech recognition gaussian processes iterative methods speech enhancement	Most existing voice conversion systems, particularly those based on Gaussian mixture models, require a set of paired acoustic vectors from the source and target speakers to learn their corresponding transformation function. The alignment of phonetically equivalent source and target vectors is not problematic when the training corpus is parallel, which means that both speakers utter the same training sentences. However, in some practical situations, such as cross-lingual voice conversion, it is not possible to obtain such parallel utterances. With an aim towards increasing the versatility of current voice conversion systems, this paper proposes a new iterative alignment method that allows pairing phonetically equivalent acoustic vectors from nonparallel utterances from different speakers, even under cross-lingual conditions. This method is based on existing voice conversion techniques, and it does not require any phonetic or linguistic information. Subjective evaluation experiments show that the performance of the resulting voice conversion system is very similar to that of an equivalent system trained on a parallel corpus.	acoustic cryptanalysis;experiment;iteration;iterative method;k-nearest neighbors algorithm;mixture model;parallel computing;parallel text;point of view (computer hardware company);text corpus	Daniel Erro;Asunción Moreno;Antonio Bonafonte	2010	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2009.2038669	natural language processing;speech recognition;computer science;signal processing;helium;government;statistics	NLP	-17.263951840141974	-91.87173426675903	142383
0517b78378a27d6cfe00e1e9bad0403440bdc6e8	investigations on mandarin aspiratory animations using an airflow model		Various three-dimensional 3-D talking heads have been developed lately for language learning, with both external and internal articulatory movements being visualized to guide learning. Mandarin pronunciation animation is challenging due to its confusable stops and affricates with similar places of articulation. Until now, less attention has been paid to the biosignal information of aspiratory airflow, which is essential in distinguishing Mandarin consonants. This study fills a research gap by presenting the quantitative analyses of airflow, and then designing an airflow model for a 3-D pronunciation system. The airflow information was collected by Phonatory Aerodynamic System, so that confusable consonants in Mandarin could be discerned by mean airflow rate, peak airflow rate, airflow duration, and peak time. Based on the airflow parameters, an airflow model using the physical equation of fluid flow was proposed and solved, which was then combined and synchronized with the existing 3-D articulatory model. Therefore, the new multimodal system was implemented to synchronously exhibit the airflow motions and articulatory movements of uttering Mandarin syllables. Both an audio-visual perception test and a pronunciation training study were conducted to assess the effectiveness of our system. Perceptual results indicated that identification accuracy was improved for both native and nonnative groups with the help of airflow motions, while native perceivers exhibited higher accuracy due to long-term language experience. Moreover, our system helped Japanese learners of Mandarin enhance their production skills of Mandarin aspirated consonants, reflected by higher gain values of voice onset time after training.	biconnected component;multimodal interaction;natural language processing;onset (audio);super robot monkey team hyperforce go!	Fei Chen;Lan Wang;Hui Chen;Gang Peng	2017	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2017.2755400	mandarin chinese;computer science;artificial intelligence;animation;speech recognition;pattern recognition;place of articulation;airflow;pronunciation;aspirated consonant;voice-onset time	HCI	-12.047665464518023	-83.4214276091831	142693
796fc0453d33a9f42a21fbdfae794e91dfc0bfc7	regression class selection and speaker adaptation with mllr in mandarin continuous speech recognition		Currently, CDHMM based continuous speech recognition has been widely extended to speaker-independent (SI) system. However, the performance of the SI system is highly dependent on the speakers, especially for Mandarin speech with accent, speaker adaptation becomes crucial important for real application. In this paper, MLLR approach is studied for speaker adaptation in mandarin continuous speech recognition and three approaches for defining regression classes are investigated: the first is based on Chinese phonetic classification, the second is based on statistical information of mixture distribution parameters and the third is based on state duration using segmental information. Other experiments like the effect of adaptation data and mixtures are presented also in the paper. The new variance-based regression class selecting scheme is proposed and has been proved to be effective.	binary prefix;experiment;speech recognition;super robot monkey team hyperforce go!	Chengrong Li;Jingdong Chen;Bo Xu	1999			speech recognition;artificial intelligence;mandarin chinese;pattern recognition;computer science;speaker diarisation;mixture distribution;speaker recognition	ML	-18.349495469376503	-90.76012723430466	142715
541348489311c1f8d43f55f44587c406080eb97b	feature normalization using smoothed mixture transformations	broadcast news;indexing terms;speech recognition;cepstral mean normalization;speaker adaptation	We propose a method for estimating the parameters of SPLICElike transformations from individual utterances so that this type of transformation can be used to normalize acoustic feature vectors for speech recognition on an utterance-by-utterance basis in a similar manner to cepstral mean normalization. We report results on an in-house French language multi-speaker database collected while deploying an automatic closed-captioning system for live broadcast news. An unusual feature of this database is that there are very large amounts of training data for the individual speakers (typically several hours) so that it is very difficult to improve on multi-speaker modeling by using standard methods of speaker adaptation. We found that the proposed method of feature normalization is capable of achieving a 6% relative improvement over cepstral mean normalization on this task.	acoustic cryptanalysis;cepstrum;smoothing;speech recognition	Patrick Kenny;Vishwa Gupta;Gilles Boulianne;Pierre Ouellet;Pierre Dumouchel	2006			natural language processing;speech recognition;index term;computer science;pattern recognition	ML	-16.871784951329033	-91.15287778040236	142811
d60d0d469ecf04c644cf228ada21cf98d7686046	a new approach with score-level fusion for the classification of a speaker age and gender		In this study a new approach for classifying speakers according to their age and genders is proposed. This approach is composed of score-level fusion of seven sub-systems. In this fused system, which provides improved performance in three classification categories (age, gender and age & gender), spectral and prosodic features extracted from short-duration phone conversations are used with Gaussian Mixture Model (GMM), Support Vector Machine (SVM) and GMM supervector-based SVM classifiers. Also, by examining individual and various combinations of each system, the effect of feature types and classification methods on performance is investigated. With the proposed system, classification success rates are obtained 90.4%, 54.1%, and 53.5% in gender, age and age & gender categories respectively. © 2016 Elsevier Ltd. All rights reserved.	experiment;feature extraction;google map maker;local-density approximation;mixture model;pl/p;principal component analysis;statistical classification;support vector machine;systemverilog	Ergun Yucesoy;Vasif V. Nabiyev	2016	Computers & Electrical Engineering	10.1016/j.compeleceng.2016.06.002	speech recognition;pattern recognition	AI	-12.484500539448147	-88.1213034220241	142867
fb8ed53830d81ae4920a1cf2eb6d597e08c5e16c	cortical processing of pitch: model-based encoding and decoding of auditory fmri responses to real-life sounds	auditory cortex;decoding;encoding;pitch processing;real-life sounds;fmri	Pitch is a perceptual attribute related to the fundamental frequency (or periodicity) of a sound. So far, the cortical processing of pitch has been investigated mostly using synthetic sounds. However, the complex harmonic structure of natural sounds may require different mechanisms for the extraction and analysis of pitch. This study investigated the neural representation of pitch in human auditory cortex using model-based encoding and decoding analyses of high field (7 T) functional magnetic resonance imaging (fMRI) data collected while participants listened to a wide range of real-life sounds. Specifically, we modeled the fMRI responses as a function of the sounds' perceived pitch height and salience (related to the fundamental frequency and the harmonic structure respectively), which we estimated with a computational algorithm of pitch extraction (de Cheveigné and Kawahara, 2002). First, using single-voxel fMRI encoding, we identified a pitch-coding region in the antero-lateral Heschl's gyrus (HG) and adjacent superior temporal gyrus (STG). In these regions, the pitch representation model combining height and salience predicted the fMRI responses comparatively better than other models of acoustic processing and, in the right hemisphere, better than pitch representations based on height/salience alone. Second, we assessed with model-based decoding that multi-voxel response patterns of the identified regions are more informative of perceived pitch than the remainder of the auditory cortex. Further multivariate analyses showed that complementing a multi-resolution spectro-temporal sound representation with pitch produces a small but significant improvement to the decoding of complex sounds from fMRI response patterns. In sum, this work extends model-based fMRI encoding and decoding methods - previously employed to examine the representation and processing of acoustic sound features in the human auditory system - to the representation and processing of a relevant perceptual attribute such as pitch. Taken together, the results of our model-based encoding and decoding analyses indicated that the pitch of complex real life sounds is extracted and processed in lateral HG/STG regions, at locations consistent with those indicated in several previous fMRI studies using synthetic sounds. Within these regions, pitch-related sound representations reflect the modulatory combination of height and the salience of the pitch percept.	acoustic cryptanalysis;algorithm;auditory perception;auditory area;auditory system;cerebral cortex;computation;decoding methods;extraction;information;lateral thinking;numerous;open reading frames;parahippocampal gyrus;pitch (music);quasiperiodicity;real life;resonance;star trek generations;superior temporal gyrus;synthetic intelligence;voxel;fmri	Vittoria De Angelis;Federico De Martino;Michelle Moerel;Roberta Santoro;Lars Hausfeld;Elia Formisano	2018	NeuroImage	10.1016/j.neuroimage.2017.11.020	superior temporal gyrus;natural sounds;perception;auditory cortex;psychology;auditory system;functional magnetic resonance imaging;speech recognition;decoding methods;fundamental frequency	ML	-6.487482444448603	-81.50116368279986	142895
b52adb42e3b48b88cae537f39e709ff36348263a	replacing uncertainty decoding with subband re-estimation for large vocabulary speech recognition in noise	speech recognition	In this paper, we propose a novel approach for parameterized model compensation for large-vocabulary speech recognition in noisy environments. The new compensation algorithm, termed CMLLR-SUBREST, combines the model-based uncertainty decoding (UD) with subspace distribution clustering hidden Markov modeling (SDCHMM), so that the UD-type compensation can be realized by re-estimating the models based on small amount of adaptation data. This avoids the estimation of the covariance biases, which is required in model-based UD and usually needs a numerical approach. The Aurora 4 corpus is used in the experiments. We have achieved 16.9% relative WER (word error rate) reduction over our previous missing-feature (MF) based decoding and 16.1% over the combination of Constrained MLLR compensation and MF decoding. The number of model parameters is reduced by two orders of magnitude.	algorithm;aurora;cluster analysis;experiment;hidden markov model;markov chain;numerical analysis;speech recognition;urban dictionary;vocabulary;word error rate	Jianhua Lu;Ji Ming;Roger F. Woods	2009			speech recognition;artificial intelligence;voice activity detection;computer science;speech processing;pattern recognition;decoding methods;speaker recognition;vocabulary	ML	-17.341458733013592	-92.07029283899517	143054
06e430e297757b23d6183115b96e0656e050e783	variability in noise-masked consonant identification		Speech communication commonly occurs in the presence of noise. Perceptual errors in the perception of noise-masked speech vary as a function of noise type (e.g., white noise, speech-shaped noise, multi-talker babble), listener characteristics (e.g., listeners with hearing loss, non-native listeners), and target stimulus properties (e.g., native language of the talker, casual vs clear speech). There is evidence of talker-specific effects in multitalker-babble-masked sentence intelligibility as well as token-specific effects in speech-shaped-noisemasked CV syllables. The present work analyzes talkerand token-level variation in the identification of a large number of tokens of four consonant categories [t], [d], [s], [z] produced by 20 talkers and masked by multi-talker babble. A fitted multilevel logistic regression model illustrates variation in intelligibility between talkers and with respect to within-talker (between-token) variation. The results are discussed in relation to landmark theory and the glimpsing model of speech-in-noise perception.	heart rate variability;image noise;intelligibility (philosophy);logistic regression;psychoacoustics;white noise	Noah Silbert;Lina Motlagh Zadeh	2015			consonant;psychology;artificial intelligence;pattern recognition	HCI	-10.122344241981828	-83.1541654021813	143112
0296074e8fe764325af4b8ab45f1b9d9b3a1913d	regularized mvdr spectrum estimation-based robust feature extractors for speech recognition		In this paper, we present two robust feature extractors that use a regularized minimum variance distortionless response (RMVDR) spectrum estimator instead of the discrete Fourier transform-based direct spectrum estimator, used in many front-ends including the conventional MFCC, for estimating the speech power spectrum. Direct spectrum estimators, e.g., single tapered periodogram, have high variance and they perform poorly under noisy and adverse conditions. RMVDR spectrum estimator has low spectral variance and are robust to mismatch conditions. Based on RMVDR spectrum estimator two robust feature extractors, robust RMVDR cepstral coefficients (RRMCC) and normalized RMVDR cepstral coefficients (NRMCC), are proposed that incorporate an auditory domain spectrum enhancement (ASE) method and a medium duration power bias subtraction (MDPBS) technique, respectively, for enhancement of the speech spectrum. Experimental speech recognition results are conducted on the AURORA-4 corpus and performances are compared with the MFCC, PLP, MVDR-MFCC, RMVDR-MFCC, PMVDR, ETSI advancement front-end (ETSI-AFE), PNCC, CFCC, and the robust feature extractor (RFE) of [6]. Experimental results demonstrate that the proposed robust feature extractors outperformed the other robust front-ends in terms of percentage word accuracy on the AURORA-4 large vocabulary continuous speech recognition (LVCSR) task under different mismatch conditions.	adaptive server enterprise;analog front-end;auditory processing disorder;cepstrum;coefficient;discrete fourier transform;feature extraction;pl/p;performance;randomness extractor;spectral density estimation;speech analytics;speech recognition;vocabulary	Md. Jahangir Alam;Patrick Kenny;Douglas D. O'Shaughnessy	2013			artificial intelligence;minimum-variance unbiased estimator;speech recognition;normalization (statistics);estimator;pattern recognition;spectral density;subtraction;mel-frequency cepstrum;computer science;discrete fourier transform;extractor	ML	-13.621801149523987	-91.05508656052545	143330
33cd8e6bb1e69f4c75662fdd62ecc34e492d8a0b	ultrasonic sensing for robust speech recognition	robust speech recognition;ultrasound sources;digit recognition;sensors;ultrasound;fusion;ultrasonic measurement;ultrasonic imaging;acoustics;robustness speech recognition ultrasonic imaging loudspeakers speech analysis reproducibility of results natural languages laboratories audio recording transmitters;hmm;speech;indexing terms;audio sources;fusion ultrasound digit recognition;accuracy;hidden markov models;tuning;hmm ultrasonic sensing speech recognition digit recognition ultrasound sources audio sources;speech recognition;ultrasonic measurement hidden markov models speech recognition;ultrasonic sensing	In this paper, we present our work using ultrasonic sensing of speech for digit recognition. First, a set of spectral ultrasonic features are developed and tuned in order to achieve optimal performance for the digit recognition task. Using these features, we demonstrate an overall accuracy of 33.00% on a digit recognition task using HMMs with recordings from 6 speakers. The results indicate that ultrasonic sensing of speech is viable, but that further work is needed to achieve word accuracies that match those of audio. Finally, experimental results are presented which demonstrate that fusing information from ultrasound and audio sources show marginal improvements over audio-only performances.	marginal model;performance;spectral method;speech recognition	Sundararajan Srinivasan;Bhiksha Raj;Tony Ezzat	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495039	speech recognition;index term;fusion;computer science;sensor;speech;ultrasound;accuracy and precision;hidden markov model	Robotics	-14.166008435012778	-89.94560202678349	143379
291d6ae9f80072ef0d546e0a6cfbc93bbb6e9377	an error correction scheme for gci detection algorithms using pitch smoothness criterion		Detection of error-free glottal closure instants (GCI) is a critical requirement for many applications including text-to-speech synthesis, causal anti-causal decomposition and voice morphing. Many existing GCI detection algorithms commit errors under certain conditions. In this paper, we propose a postprocessing scheme for correcting errors of any GCI detection algorithm. The proposed error correction scheme works on the principle that the fundamental frequency over a voiced segment is slowly varying. The error correction is thus formulated as an optimization problem such that the pitch contour from the corrected GCIs has the least high frequency components. The proposed error correction scheme is experimentally evaluated on speech corpus with simultaneous EGG recordings using three state-of-the-art GCI detection algorithms viz., Dynamic Plosion Index (DPI), Zero Frequency Resonator (ZFR), and Speech Event Detection using the Residual Excitation And a Mean-based Signal (SEDREAMS). It is found that the proposed error correction scheme improves the performance of the GCI detection in clean speech as well as noisy conditions at different SNRs.	algorithm;causal filter;error detection and correction;experiment;google code-in;mathematical optimization;morphing;optimization problem;pitch (music);speech corpus;speech synthesis;viz: the computer game	P. Sujith;A. P. Prathosh;A. G. Ramakrishnan;Prasanta Kumar Ghosh	2015			error detection and correction;pattern recognition;pitch contour;speech corpus;residual;speech recognition;artificial intelligence;smoothness;algorithm;fundamental frequency;computer science;optimization problem	ML	-12.232371376527778	-94.06199667236966	143416
5f343ec4db8b71c8828facef4b7e426009ff408f	are you awerewolf? detecting deceptive roles and outcomes in a conversational role-playing game	nonverbal behavior deception role analysis;audio visual systems;audio signal processing;social context;social interaction;audio visual recordings deceptive roles detection conversational role playing game social interaction patterns nonverbal audio cues competitive role playing games rpg;nonverbal behavior;role analysis;niobium;audio visual recordings;speech;psychology;speaker recognition;accuracy;visualization;social interaction patterns;performance improvement;role playing game;automatic detection;feature extraction;games;nonverbal audio cues;speaker recognition audio signal processing audio visual systems behavioural sciences;behavioural sciences;audio visual;acoustic signal detection signal processing machine learning psychology audio recording signal analysis computer science electric breakdown storage automation organizing;rpg;competitive role playing games;conversational role playing game;deception;deceptive roles detection	This paper addresses the task of automatically detecting outcomes of social interaction patterns, using non-verbal audio cues in competitive role-playing games (RPGs). For our experiments, we introduce a new data set which features 3 hours of audio-visual recordings of the popular “Are you a Werewolf?” RPG. Two problems are approached in this paper: Detecting lying or suspicious behavior using non-verbal audio cues in a social context and predicting participants' decisions in a game-day by analyzing speaker turns. Our best classifier exhibits a performance improvement of 87% over the baseline for detecting deceptive roles. Also, we show that speaker turn based features can be used to determine the outcomes in the initial stages of the game, when the group is large.	baseline (configuration management);experiment;rpg;regular expression;sensor	Gokul Chittaranjan;Hayley Hung	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5494961	speaker recognition;games;social environment;niobium;computer vision;speech recognition;visualization;audio signal processing;feature extraction;behavioural sciences;computer science;speech;accuracy and precision;multimedia	Robotics	-17.16841717382824	-83.47873207811585	143431
b11e0d005bb217c65099e323abfd994590dca48b	coarticulation in cv syllables: a comparison of locus equation and epg data		Using electropalatographic (EPG) data, the following hypothesis is tested: the slope value generated by a locus equation (LE) analysis of the F2 transition in CV syllables is an accurate re#ection of the degree of coarticulation between the consonant and the vowel in that syllable. The consonants studied are /ð z E d g n F l r/. Comparisons between EPG and LE data suggest that the LE analysis provides an accurate re#ection of the degree of coarticulation for the stop and nasal classes in English, which have an alveolar and a velar place of articulation amongst the lingual consonants. By contrast, the correlation between EPG and LE data for the fricative consonants is very poor. Two explanations are o!ered for the poorer results for the fricatives: (1) the fricative noise following consonant release obscures the F2 transition, rendering formant measurement less accurate, and (2) the LE is capable of encoding gross di!erences in degree of coarticulation (such as that between an alveolar and a velar) but not more subtle di!erences such as those between the various coronal articulations. ( 2000 Academic Press	attribute–value pair;biconnected component;degree (graph theory);locus;syllable	Marija Tabain	2000	J. Phonetics	10.1006/jpho.2000.0110	speech recognition;acoustics;mathematics;communication	AI	-10.855895996618933	-81.58378249456393	143438
aadd69f297965b5a1c75ac7581e3d91af85a9d94	"""correction to """"a simulation approach to the evaluation of two telephone switchboard systems"""""""	design engineering;telephony;systems engineering and theory;modeling;telecommunications		simulation;telephone switchboard	Rickie H. Luk;Gerald F. Rabideau	1976	IEEE Trans. Systems, Man, and Cybernetics	10.1109/TSMC.1976.4309458	systems modeling;computer science;telephony	Embedded	-16.16332289470531	-85.59119941786768	143527
739cbdc19240f6f7b64901dbb5e55ddd08f9155b	nasal coarticulation in lexical perception: the role of neighborhood-conditioned variation		Nasal coarticulation has been shown to vary systematically in words depending on the number of phonological neighbors: words with many neighbors are produced with a greater degree of vowel nasality than words with fewer phonological neighbors [9]. This study examines the effect of this systematic low-level variation on lexical perception. The degree of nasality in natural real and nonsense words from high and low density neighborhoods was manipulated to neutralize the neighborhood-conditioned differences, and these original and manipulated stimuli were presented to subjects in a lexical decision task and a forced choice preference task. The findings of this study suggest that, for high neighborhood density words at least, listeners are indeed sensitive to this systematic low-level phonetic variation and that it has an influence on lexical perception.	high- and low-level;machine perception;undefined behavior	Rebecca Scarborough;Will Styler;Georgia Zellou	2011			coarticulation;perception;communication;psychology	HCI	-10.369316359554766	-81.24546782422605	143537
85a73ebaccaf2502bdc6a20230b0e61f39f70a66	speech enhancement with binaural cues derived from a priori codebook	speech;speech coding;speech enhancement;noise measurement;monos devices;discrete fourier transforms	In conventional codebook-driven speech enhancement, only spectral envelopes of speech and noise are considered, and at the same time, the type of noise is the priori information when we enhance the noisy speech. In this paper, we propose a novel codebook-based speech enhancement method which exploits a priori information about binaural cues, including clean cue and pre-enhanced cue, stored in the trained codebook. This method includes two main parts: offline training of cues and online enhancement by means of cues. That is, we use the trained codebook to model a priori information of speech and noise offline and extract the pre-enhanced cue from the noisy observation online. The clean cue is estimated by the mapping of the weighted code vectors online, and the enhanced speech is produced by the estimated clean cue. The experimental results show that the proposed approach performs better than the reference methods in both stationary and non-stationary noise condition.	binaural beats;codebook;iterative method;online and offline;speech enhancement;stationary process	Nan Chen;Changchun Bao;Feng Deng	2016	2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)	10.1109/ISCSLP.2016.7918377	voice activity detection;speech recognition;computer science;noise measurement;speech;speech coding;speech processing;linguistics	ML	-13.16639675167533	-93.6308280462129	143547
61949021b520863cd147c9e7c5b9ac3293d1affa	formant measurement in children's speech based on spectral filtering	formant;speech analysis;vocal tract;children s speech;speech modeling	"""Children's speech presents a challenging problem for formant frequency measurement. In part, this is because high fundamental frequencies, typical of a children's speech production, generate widely spaced harmonic components that may undersample the spectral shape of the vocal tract transfer function. In addition, there is often a weakening of upper harmonic energy and a noise component due to glottal turbulence. The purpose of this study was to develop a formant measurement technique based on cepstral analysis that does not require modification of the cepstrum itself or transformation back to the spectral domain. Instead, a narrow-band spectrum is low-pass filtered with a cutoff point (i.e., cutoff """"quefrency"""" in the terminology of cepstral analysis) to preserve only the spectral envelope. To test the method, speech representative of a 2-3 year-old child was simulated with an airway modulation model of speech production. The model, which includes physiologically-scaled vocal folds and vocal tract, generates sound output analogous to a microphone signal. The vocal tract resonance frequencies can be calculated independently of the output signal and thus provide test cases that allow for assessing the accuracy of the formant tracking algorithm. When applied to the simulated child-like speech, the spectral filtering approach was shown to provide a clear spectrographic representation of formant change over the time course of the signal, and facilitates tracking formant frequencies for further analysis."""	algorithm;cepstrum;low-pass filter;microphone device component;modulation;nomenclature;phase ii/iii trial;resonance;test case;tract (literature);transfer function;turbulence;vocal cord structure	Brad H. Story;Kate Bunton	2015	Speech communication	10.1016/j.specom.2015.11.001	vocal tract;speech recognition;formant;computer science;linguistics	ML	-9.130421517543907	-86.0612113378483	143635
8776ec4f2639710598c38c770ec0d907b75a53b6	digital computer usage in analysis of electroencephalograph and similar quasi-rhythmic patterns			computer;electroencephalography	M. G. Saunders	1962			communication;rhythm;computer science	Theory	-7.023068959590926	-83.66872834414221	143731
760136f6a7e73652808417a9ce8d2f4bba7face2	a component by component listening test analysis of the ibm trainable speech synthesis system	decision tree;speech synthesis	This paper reports on a listening test conducted to determine the impact on speech quality of each component in the IBM Trainable Speech Synthesiser. The study was originally conceived to direct future research effort to those components with the greatest potential for improvement. However, the results and conclusions regarding prosodic modification, concatenation unit length, and decision tree clustering are generally applicable and may be of wider interest.	cluster analysis;concatenation;decision tree;speech synthesis	Robert E. Donovan	2001			natural language processing;speech recognition;computer science;decision tree;speech synthesis	NLP	-18.890706661365186	-84.60428216197096	143746
2d183b9a4afe6eb268c3ae7c1f0a14831d984b46	estimating vocal tract shapes of thai vowels from contextual vowel variation	disyllabic sequences vocal tract shapes estimation thai vowels contextual vowel variation articulatory synthesizer analysis by synthesis acoustic data mfcc error speech synthesis;thai vowel;vocal tract shape;articulatory synthesis;speech synthesis estimation theory natural language processing;target approximation articulatory synthesis vocal tract shape thai vowel optimization;optimization;speech optimization shape approximation methods hidden markov models synthesizers;target approximation	This paper presents a computational estimation of vocal tract shape parameters as articulatory targets of Thai vowels in an articulatory synthesizer, by means of analysis-by-synthesis with acoustic data as input. A speech corpus designed to capture the contextual variants of nine Thai long vowels, consisting of 81 disyllabic utterances, was recorded by a native Thai speaker. For each utterance, two targets, one for each syllable, were estimated by optimizing the target parameters to minimize the MFCC error between original and synthesized speech. An analysis-by-synthesis approach was used to iteratively optimize the shape parameters. The estimated targets of each vowel type were then averaged, resulting in nine articulatory targets, each corresponding to a vowel. The optimized targets were then used to synthesize Thai vowels both in monosyllables and in disyllabic sequences. The results indicate that the estimated targets effectively represent the underlying articulatory goals of Thai vowels.	acoustic cryptanalysis;speech coding;speech corpus;speech synthesis;syllable;tract (literature)	Santitham Prom-on;Peter Birkholz;Yi Xu	2014	2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)	10.1109/ICSDA.2014.7051442	speech recognition;acoustics;computer science;communication	NLP	-18.341509875632404	-91.78547064851246	143862
1d0b20065989c7b65fce797910cc8c04a6905c47	biomimetic spectro-temporal features for music instrument recognition in isolated notes and solo phrases	signal image and speech processing;acoustics;mathematics in music;engineering acoustics	The identity of musical instruments is reflected in the acoustic attributes of musical notes played with them. Recently, it has been argued that these characteristics of musical identity (or timbre) can be best captured through an analysis that encompasses both time and frequency domains; with a focus on the modulations or changes in the signal in the spectrotemporal space. This representation mimics the spectrotemporal receptive field (STRF) analysis believed to underlie processing in the central mammalian auditory system, particularly at the level of primary auditory cortex. How well does this STRF representation capture timbral identity of musical instruments in continuous solo recordings remains unclear. The current work investigates the applicability of the STRF feature space for instrument recognition in solo musical phrases and explores best approaches to leveraging knowledge from isolated musical notes for instrument recognition in solo recordings. The study presents an approach for parsing solo performances into their individual note constituents and adapting back-end classifiers using support vector machines to achieve a generalization of instrument recognition to off-the-shelf, commercially available solo music.	acoustic cryptanalysis;biomimetics;feature vector;parsing;performance;structure of observed learning outcome;support vector machine	Kailash Patil;Mounya Elhilali	2015		10.1186/s13636-015-0070-9	speech recognition;acoustics;physics	ML	-7.434780786683322	-81.68107064157002	144158
046a95f3e9bfb78b5baec38e7dee3fa4d5f64f10	a novel method for selecting the number of clusters in a speaker diarization system	gaussian processes;speaker recognition;statistical analysis;c-score;gaussian mixture model mean supervectors;cluster score;inter-cluster similarity;intra-cluster similarity;linear discriminant analysis;low-dimensional discriminative subspace;speaker clustering;speaker diarization system;speech utterances;cluster similarity;linear discriminant analysis;speaker clustering	This paper introduces the cluster score (C-score) as a measure for determining a suitable number of clusters when performing speaker clustering in a speaker diarization system. C-score finds a trade-off between intra-cluster and extra-cluster similarities, selecting a number of clusters with cluster elements that are similar between them but different to the elements in other clusters. Speech utterances are represented by Gaussian mixture model mean supervectors, and also the projection of the supervectors into a low-dimensional discriminative subspace by linear discriminant analysis is assessed. This technique shows robustness to segmentation errors and, compared with the widely used Bayesian information criterion (BIC)-based stopping criterion, results in a lower speaker clustering error and dramatically reduces computation time. Experiments were run using the broadcast news database used for the Albayzin 2010 Speaker Diarization Evaluation.	bayesian information criterion;cluster analysis;computation;linear discriminant analysis;mixture model;speaker diarisation;time complexity	Paula Lopez-Otero;Laura Docío Fernández;Carmen García-Mateo	2014	2014 22nd European Signal Processing Conference (EUSIPCO)		speaker diarisation;speech recognition;machine learning;pattern recognition	ML	-17.400513456291527	-93.3800975466626	144181
0139fd1de03a071927dfc51bf4bdc2c38a594672	automated assessment of spoken modern standard arabic		Proficiency testing is an important ingredient in successful language teaching. However, repeated testing for course placement, over the course of instruction or for certification can be time-consuming and costly. We present the design and validation of the Versant Arabic Test, a fully automated test of spoken Modern Standard Arabic, that evaluates test-takers' facility in listening and speaking. Experimental data shows the test to be highly reliable (testretest r=0.97) and to strongly predict performance on the ILR OPI (r=0.87), a standard interview test that assesses oral proficiency.	ivo lola ribar institute;test automation;test case;vocabulary	Jian Cheng;Jared Bernstein;Ulrike Padó;Masanori Suzuki	2009			natural language processing;speech recognition;computer science;linguistics	SE	-17.28072895539576	-82.16484123591208	144294
1a5ca6775836c32f1b978fd601a3e85079842b22	frequency component restoration for music sounds using local probabilistic models with maximum entropy learning		We propose a method that estimates frequency component structures from musical audio signals and restores missing components due to noise. Restoration has become important in various music information processing systems including music information retrieval. Our method comprises two steps: (1) pattern classification for the initial component-state estimation, and (2) state optimization by a generative model (Markov random fields; MRF). Throughout the method, we use a probabilistic model defined for each local region on a spectrogram. Unlike conventional MRF models, the model parameters are learned using a maximum entropy method. Experiments using artificial noisy sounds show that a combination of the above two steps improves the performance with respect to restoration accuracy and robustness, compared with the sole use of pattern classification or a generative model. The method achieves an F-measure greater than 0.6 even in periods where signals are replaced by noises. In addition, the method is shown to be effective even for audio signals of real instruments.	circuit restoration;experiment;f1 score;generative model;information processing;information retrieval;markov chain;markov random field;mathematical optimization;principle of maximum entropy;spectrogram;statistical classification;statistical model	Tomonori Izumitani;Kunio Kashino	2006			pattern recognition;speech recognition;principle of maximum entropy;probabilistic logic;artificial intelligence;machine learning;computer science	ML	-16.387780610590944	-93.51393383954223	144562
e300f49c5d76ed50d2b0e38718d1c17108df4116	a discriminative approach to polyphonic piano note transcription using supervised non-negative matrix factorization	databases;discriminative training approach;accuracy training support vector machines databases feature extraction dictionaries instruments;instruments;support vector machines;information retrieval;training;piano music;music information retrieval transcription sparse coding non negative matrix factorization;acoustic signal processing;polyphonic piano note transcription;nmf dictionary learning;accuracy;supervised nonnegative matrix factorization;matrix decomposition;activation feature extraction discriminative training approach polyphonic piano note transcription supervised nonnegative matrix factorization svms support vector machines pitch activations low level spectral feature extraction piano music large scale evaluation nmf dictionary learning;transcription;feature extraction;non negative matrix factorization;music information retrieval;dictionaries;low level spectral feature extraction;learning artificial intelligence;svms;sparse coding;activation feature extraction;large scale evaluation;pitch activations;music;support vector machines acoustic signal processing feature extraction information retrieval learning artificial intelligence matrix decomposition music	We introduce a novel method for the transcription of polyphonic piano music by discriminative training of support vector machines (SVMs). As features, we use pitch activations computed by supervised non-negative matrix factorization from low-level spectral features. Different approaches to low-level feature extraction, NMF dictionary learning and activation feature extraction are analyzed in a large-scale evaluation on eight hours of piano music including synthesized and real recordings. We conclude that the proposed method delivers state-of-the-art results and clearly outperforms SVMs using simple spectral features.	dictionary;feature extraction;high- and low-level;machine learning;non-negative matrix factorization;supervised learning;support vector machine;transcription (software)	Felix Weninger;Christian Kirst;Björn W. Schuller;Hans-Joachim Bungartz	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6637598	support vector machine;speech recognition;feature extraction;computer science;machine learning;pattern recognition;music;matrix decomposition;transcription	Vision	-5.187405297974696	-88.08125273625726	144612
36905a07bc05c4d47a92785a6169191a392ace23	detection of dialogue in movie soundtrack for speech intelligibility enhancement		A method for detecting dialogue in 5.1 movie soundtrack based on interchannel spectral disparity is presented. The front channel signals (left, right, center) are analyzed in the frequency domain. The selected partials in the center channel signal, which yield high disparity with left and right channels, are detected as dialogue. Subsequently, the dialogue frequency components are boosted to achieve increased dialogue intelligibility. The techniques for reduction of artifacts in the processed signal are also introduced. Smoothing in the time domain and in the frequency domain is applied to reduce unpleasant artifacts. The results of objective tests are provided, which prove that increased dialogue intelligibility is achieved with the aid of the proposed algorithm. The algorithm is particularly applicable in mobile devices while listening in mobile conditions.		Kuba Łopatka	2014		10.1007/978-3-319-07569-3_12	natural language processing;speech recognition	AI	-8.124357452517744	-88.39205269994028	144626
c7bd83f588af3170976b69b754439b7bb8b3fdfc	clustering music recordings by their keys	cross correlation;structure analysis;indexation;feature extraction;distance metric	Music key, a high level feature of musical audio, is an effective tool for structural analysis of musical works. This paper presents a novel unsupervised approach for clustering music recordings by their keys. Based on chroma-based features extracted from acoustic signals, an inter-recording distance metric which characterizes diversity of pitch distribution together with harmonic center of music pieces, is introduced to measure dissimilarities among musical features. Then, recordings are divided into categories via unsupervised clustering, where the best number of clusters can be determined automatically by minimizing estimated Rand Index. Any existing technique for key detection can then be employed to identify key assignment for each cluster. Empirical evaluation on a dataset of 91 pop songs illustrates an average cluster purity of 57.3% and a Rand Index of close to 50%, thus highlighting the possibility of integration with existing key identification techniques to improve accuracy, based on strong cross-correlation data available from this framework for input dataset.	acoustic cryptanalysis;cluster analysis;cross-correlation;high-level programming language;pure function;rand index;structural analysis;unsupervised learning	Yuxiang Liu;Ye Wang;Arun Shenoy;Wei-Ho Tsai;Lianhong Cai	2008			machine learning;artificial intelligence;speech recognition;cross-correlation;metric (mathematics);feature extraction;cluster analysis;rand index;computer science;k-medians clustering;indexation;harmonic;pattern recognition	Web+IR	-8.262278832701327	-92.33977584967262	144647
4aeef8ee126fb4e67ffde4d73586193fe45061ea	examining vulnerability of voice verification systems to spoofing attacks by means of a tts system	unit selection;speech synthesis;hmm;speaker recognition;spoofing	This paper examines the method of spoofing text-dependent voice verification systems based on the most popular TTS approaches: Unit Selection and HMM. Research of this method shows the possibility of achieving a false acceptance error of 98%-100% if the duration of the TTS database is sufficiently large. A distinctive feature of the method is that it can be fully automatical if used in conjunction with a speech recognition system.	netware file system	Vadim Shchemelinin;Konstantin Simonchik	2013		10.1007/978-3-319-01931-4_18	speech recognition;engineering;pattern recognition;communication	Logic	-15.32135907586635	-88.85261523791122	144697
fe51e9d348c9ea1bfd38d965e3b3211cfb75335e	causal speech enhancement combining data-driven learning and suppression rule estimation		The problem of single-channel speech enhancement has been traditionally addressed by using statistical signal processing algorithms that are designed to suppress time-frequency regions affected by noise. We study an alternative data-driven approach which uses deep neural networks (DNNs) to learn the transformation from noisy and reverberant speech to clean speech, with a focus on real-time applications which require low-latency causal processing. We examine several structures in which deep learning can be used within an enhancement system. These include end-to-end DNN regression from noisy to clean spectra, as well as less intervening approaches which estimate a suppression gain for each time-frequency bin instead of directly recovering the clean spectral features. We also propose a novel architecture in which the general structure of a conventional noise suppressor is preserved, but the sub-tasks are independently learned and carried out by separate networks. It is shown that DNN-based suppression gain estimation outperforms the regression approach in the causal processing mode and for noise types that are not seen during DNN training.	algorithm;artificial neural network;causal filter;deep learning;end-to-end principle;real-time clock;speech enhancement;statistical signal processing;zero suppression	Seyedmahdad Mirsamadi;Ivan Tashev	2016		10.21437/Interspeech.2016-437	speech recognition;machine learning;pattern recognition	ML	-14.739131562028277	-91.0756334181399	144713
6ef0ffb06833d8a33e3d3486740da0648b969dde	lattice based transcription loss for end-to-end speech recognition	neural networks;lattices;acoustics;training;error analysis;hidden markov models;speech recognition	End-to-end speech recognition systems have been successfully implemented and have become competitive replacements for hybrid systems. A common loss function to train end-to-end systems is connectionist temporal classification (CTC). This method maximizes the log likelihood between the feature sequence and the associated transcription sequence. However there are some weaknesses with CTC training. The main weakness is that the training criterion is different from the test criterion, since the training criterion is log likelihood, while the test criterion is word error rate. In this work, we introduce a new lattice based transcription loss function to address this deficiency of CTC training. Compared to the CTC function, our new method optimizes the model directly using the transcription loss. We evaluate this new algorithm in both a small speech recognition task, the Wall Street Journal (WSJ) dataset and a large vocabulary speech recognition task, the Switchboard dataset. Results demonstrate that our algorithm outperforms a traditional CTC criterion, and achieving 7% WER relative reduction. In addition, we compare our new algorithm to some discriminative training algorithms, such as state-level minimum Bayes risk (SMBR) and minimum word error (MWE), showing that our algorithm is more convenient and contains more varieties for speech recognition.	algorithm;angular defect;connectionism;discriminative model;end-to-end principle;hybrid system;lattice model (finance);loss function;minimal working example;speech recognition;telephone switchboard;the wall street journal;transcription (software);vocabulary;word error rate	Jian Kang;Wei-Qiang Zhang;Jia Liu	2016	2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)	10.1109/ISCSLP.2016.7918455	speech recognition;computer science;machine learning;pattern recognition;lattice;artificial neural network;hidden markov model	NLP	-18.73519350469785	-88.1855345707702	144745
98cca7963382ce27b7136128e7105be151a16fcd	shaking acoustic spectral sub-bands can letxer regularize learning in affective computing		In this work, we investigate a recently proposed regularization technique based on multi-branch architectures, called Shake-Shake regularization, for the task of speech emotion recognition. In addition, we also propose variants to incorporate domain knowledge into model configurations. The experimental results demonstrate: 1) independently shaking subbands delivers favorable models compared to shaking the entire spectral-temporal feature maps. 2) with proper patience in early stopping, the proposed models can simultaneously outperform the baseline and maintain a smaller performance gap between training and validation.	acoustic cryptanalysis;affective computing;baseline (configuration management);early stopping;emotion recognition;map	Che-Wei Huang;Shrikanth (Shri) Narayanan	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461823	domain knowledge;machine learning;task analysis;early stopping;artificial neural network;artificial intelligence;affective computing;regularization (mathematics);computer science;pattern recognition;emotion recognition;performance gap	Robotics	-15.877652880850484	-90.32782322018134	144795
bd53be9649afff1b58caf8393bb97a64b2745c80	vidtimit audio visual phoneme recognition using aam visual features and human auditory motivated acoustic wavelet features	aam visual features;aam feature vidtimit audio visual phoneme recognition aam visual features human auditory motivated acoustic wavelet features audio visual phoneme recognition system visual modality speech recognizer audio modality auditory based equivalent rectangular bandwidth active appearance model based features aam based features proportional weighting factor stream audio visual synchronous hidden markov model shmm recognizer vidtimit database multimodal phoneme recognition system artificial noises werbc feature;artificial noises;aam feature;visualization active appearance model shape acoustics feature extraction hidden markov models speech;audio signal processing;vid timit corpus;audio visual phoneme recognition;audio visual phoneme recognition system;acoustics;audio modality;hci;speech;active appearance model;aam;speech recognizer;hci audio visual phoneme recognition werbc vid timit corpus aam;visualization;hidden markov models;shape;vidtimit audio visual phoneme recognition;feature extraction;active appearance model based features;stream audio visual synchronous hidden markov model;speech recognition audio signal processing hidden markov models;werbc;speech recognition;proportional weighting factor;auditory based equivalent rectangular bandwidth;werbc feature;vidtimit database;human auditory motivated acoustic wavelet features;visual modality;aam based features;shmm recognizer;multimodal phoneme recognition system	This paper presents an audio visual phoneme recognition system using the shape and appearance information extracted from jaw and lip region to enhance the robustness in noisy environment. Consideration of visual features along with traditional acoustic features have been found to be promising in complex auditory environment. Visual modality can provide complementary information to the speech recognizer when the audio modality is badly affected by background noise. Acoustic modality is represented by auditory based equivalent rectangular bandwidth (ERB) like wavelet features (WERBC) features, whereas visual modality is represented by statistically powerful active appearance model (AAM) based features. Audio and visual modalities are fused by using a proportional weighting factor to form the two stream audio visual synchronous Hidden Markov Model (SHMM) recognizer. The VidTIMIT database is chosen to study the performance of multi-modal phoneme recognition system. Artificial noises are injected to audio files at different SNR levels (0dB-20dB) to study the performance of system in noisy environment. Combination of WERBC and AAM features outperform the well known traditional combination of Mel scale cepstrum coefficients (MFCC) acoustic features and discrete cosine transform (DCT) visual features.	acoustic cryptanalysis;acoustic fingerprint;acoustic model;active appearance model;automatic acoustic management;baseline (configuration management);cepstrum;coefficient;discrete cosine transform;equivalent rectangular bandwidth;experiment;finite-state machine;hidden markov model;markov chain;mel scale;modal logic;modality (human–computer interaction);signal-to-noise ratio;speech recognition;statistical model;wavelet;whole earth 'lectronic link	Astik Biswas;Prakash Kumar Sahu;Anirban Bhowmick;Mahesh Chandra	2015	2015 IEEE 2nd International Conference on Recent Trends in Information Systems (ReTIS)	10.1109/ReTIS.2015.7232917	speech recognition;computer science;pattern recognition;communication	Robotics	-12.22564823449354	-88.34760383150035	144997
ca6f4ac497cfdc45fb152375479bfece03f79993	multi-domain recurrent neural network language model for medical speech recognition.				Ottokar Tilk;Tanel Alumäe	2014		10.3233/978-1-61499-442-8-149	natural language processing;speech recognition;machine learning;time delay neural network	NLP	-15.80282666967622	-86.94410433528486	145010
3da5c115df02bd7408987a16c22588f472e96b89	characteristics of spoken language required for objective quality evaluation of echo cancellers		Performance of the echo canceller can be conventionally measured by putting white Gaussian noise into the echo canceller system. However, white Gaussian noise is not adequate as the test signal, since the performance may depend on the characteristics of input test signal, and the characteristics of the white Gaussian noise differ from those of real spoken language. This paper describes characteristics of spoken language required for objective quality evaluation of echo cancellers. Following test signals having various time and frequency characteristics of spoken language are examined: white Gaussian noise, frequency weighted Gaussian noise, artificial voice, composite source signal, and real voice. It is concluded that artificial voice having average characteristics of spoken language in time and frequency domain is satisfied for objective quality evaluation of echo cancellers other than conventional white Gaussian noise, frequency weighted white Gaussian noise, and composite source signal.	echo (command);echo suppression and cancellation	Nobuhiko Kitawaki;Futoshi Asano;Takeshi Yamada	2000			speech recognition;spoken language;computer science	NLP	-8.919630492638063	-87.826899825083	145229
e4d347d04f2558ce50b61feec9f03bdb330c2d48	combinaison d'approches pour la reconnaissance du rôle des locuteurs (combination of approaches for speaker role recognition) [in french]		Combination of approaches for speaker role recognition In this article, we are particularly interested in recognizing speaker role inside broadcast news shows. Previous studies highlighted a link between speech spontaneity and speaker roles. An automatic spontaneous speech detection system has already been applied to recognize speaker roles, without any change in the method process (Dufour et al., 2011). We propose to improve this method by adding specific speaker role features. Thus, features from Social Network Analysis (SNA) are used in the method. This new information allowed to improve the two decision steps oh the method, with a gain of 3.2 and 1.9 points in absolute, respectively for the local then the global decision. Finally for a larger number of focused roles than usually retained, the proposed method allowed to associate the correct role to 76.3% of the speakers. MOTS-CLÉS : indexation automatique, analyse des réseaux sociaux, parole spontanée, rôle du locuteur.	emergence;linear algebra;social network analysis;spontaneous order	Richard Dufour;Antoine Laurent;Yannick Estève	2012				AI	-13.392224575921826	-82.2398307800783	145380
ba52ba5c9958c2cad0dbdb73c0ea5d18ba1b6ce2	a style control technique for speech synthesis using multiple regression hsmm	speech synthesis;multiple regression		hidden semi-markov model;speech synthesis	Takashi Nose;Junichi Yamagishi;Takao Kobayashi	2006			artificial intelligence;speech recognition;linear regression;pattern recognition;computer science;speech synthesis	Robotics	-15.572259707775435	-87.03128229045987	145398
cf22406ee62973e9dd13dd5bb19610d49eeaee63	consideration of correlation between noise and clean speech signals in autocorrelation-based robust speech recognition	autocorrelation based features;autocorrelation based noise subtraction;robust speech recognition;cross correlation;speech;mel frequency cepstral coefficient;aurora 2 corpus speech signals autocorrelation based robust speech recognition autocorrelation based features autocorrelation based noise subtraction cross correlation term;automatic speech recognition;feature extraction;aurora 2 corpus;speech signals;cross correlation term;speech recognition;robustness;speech recognition signal denoising;correlation;autocorrelation based robust speech recognition;speech enhancement autocorrelation noise robustness speech recognition additive noise automatic speech recognition feature extraction working environment noise testing noise cancellation;noise;signal denoising	This paper presents a new approach to consider the correlation of noise and clean speech signals, when an autocorrelation-based set of features are used. Autocorrelation-based features have recently been used in several cases for achieving robustness in automatic speech recognition (ASR) systems. Such methods usually consider the clean speech and noise to be uncorrelated. However, when some correlation between the clean speech and noise is possible, their assumptions might not be necessarily correct. We recently reported a robust set of features based on autocorrelation of the noisy signal, namely Autocorrelation-based Noise Subtraction (ANS). The correlation between the clean speech and noise was not considered in this approach either. In this paper, we try to consider this cross correlation term in the estimation of the clean speech signal autocorrelation. This new approach was tested on the Aurora 2 corpus and led to even better results in comparison to ANS.	aurora;autocorrelation;cross-correlation;feature extraction;image noise;speech recognition	Gholamreza Farahani;Seyed Mohammad Ahadi;Mohammad Mehdi Homayounpour;Amir Kashi	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555375	speech recognition;feature extraction;computer science;noise;speech;cross-correlation;pattern recognition;correlation;robustness	NLP	-13.35877540295751	-91.80276071971552	145431
f1c8fb1a5ad1d8e47e297feb4a219b1f209599df	connectionist segmental post-processing of the n-best solutions in isolated and connected word recognition task	word recognition		connectionism;video post-processing	Denys Boiteau;Patrick Haffner	1993			speech recognition;artificial intelligence;pattern recognition;connectionism;computer science;logogen model;word recognition	NLP	-15.32539263350758	-86.86526622310758	145565
b3db7d06123aea9a6a78cc85c814afc7526605ce	speech recognition with matrix-mce based two-dimension-cepstrum in cars	automobiles;gaussian processes;matrix algebra;speech recognition automobiles cepstral analysis gaussian processes interference suppression matrix algebra pattern matching;interference suppression;cepstral analysis;speech speech recognition noise measurement robustness principal component analysis signal to noise ratio;isolated mandarin digit speech recognition matrix mce 2d cepstrum cars background noise mmce tdc template matching gaussian mixture model gmm;pattern matching;speech recognition;gmm mmce speech recognition tdc	This study proposes matrix-MCE (MMCE) to reduce the influence of noises. Background noises usually degrade the performance of speech recognition. MMCE can efficiently minimize the classification error of two-dimension-cepstrum (TDC). Then the template matching employs the Gaussian-mixture-model (GMM). To evaluate the performance, the speech data used for our experiments are a set of isolated Mandarin digits. Experimental results indicate that MMCE-based TDC is very robust in the noisy environments.	cepstrum;experiment;google map maker;mixture model;speech recognition;super robot monkey team hyperforce go!;template matching;tinymce	Gin-Der Wu;Zhen-Wei Zhu	2012	2012 12th International Conference on ITS Telecommunications	10.1109/ITST.2012.6425199	speech recognition;engineering;pattern recognition;communication	Robotics	-15.299154925349136	-92.29686626039698	145586
cf35ddd79b57cbc1d1d9079e5fc14ecc68daedab	can we perceive attitudes before the end of sentences? the gating paradigm for prosodic contours			programming paradigm	Véronique Aubergé;Tuulikki Grepillat;Albert Rilliard	1997			speech recognition;computer science;gating	NLP	-14.336236374116057	-84.83442789032637	145784
2060fd509720f083d3c80634d03f6611f547fbe9	a cross-linguistic perspective to the study of dysarthria in parkinson's disease	speech intelligibility;cross linguistic;dysarthria;parkinson s disease;speech production;journal magazine article	HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.		Serge Pinto;Angel Chan;Isabel Guimarães;Rui Rothe-Neves;Jasmin Sadat	2017	J. Phonetics	10.1016/j.wocn.2017.01.009	psychology;speech production;speech recognition;linguistics;communication;intelligibility	ML	-8.174396196907196	-84.28558667994702	145844
3d83ce8941d8455c1f9fd2aea5d3a3d5a9167e5f	vocal pauses in teaching: statistical analysis and applications	statistical analysis			E. Angderi;M. Barsotti;L. Mazzei;L. Vttrano;R. Volpentesta	1990			speech recognition;computer science	Crypto	-13.922515795827248	-85.32452197516955	146060
78c237730aa84021909ec78098fe976dd632e1b1	"""procedure """"senza vibrato"""": a key component for morphing singing."""		A procedure to remove vibrato from singing voice was proposed to enable auditory morphing between musical performances played under different conditions. Analyses of singing samples in the RWC music database using a speech analysis, modification and synthesis system STRAIGHT provided necessary information to implement “senza vibrato,” the procedure that removes vibrato. A preliminary subjective evaluation for artificially adding and removing vibrato indicated that the proposed procedure effectively control perceived vibrato while preserving naturalness of the original singing.	list of online music databases;morphing;performance;voice analysis	Hideki Kawahara;Yumi Hirachi;Masanori Morise;Hideki Banno	2004			vibrato;artificial intelligence;speech recognition;pattern recognition;morphing;computer science;singing	ML	-9.859520809732858	-86.278627706379	146073
bd7bd734ae27b3ef7a60f2e0121960a213476c42	noise robust aurora-2 speech recognition employing a codebook-constrained kalman filter preprocessor	ar model;front end;expectation maximization approach;time measurement;speech signal estimation;speech processing;working environment noise;kalman filters;kalman filter;speech recognition autoregressive processes expectation maximisation algorithm kalman filters matrix algebra speech processing;matrix algebra;speech enhancement;state estimation;estimation algorithm;noise robustness;noise signals;automatic speech recognition;autoregressive models;expectation maximization;autoregressive processes;feature extraction;signal processing;speech signals;speech recognition;codebook constrained kalman filter preprocessor;noise robustness speech recognition speech enhancement speech processing automatic speech recognition signal processing state estimation working environment noise feature extraction time measurement;speech signals aurora 2 speech recognition codebook constrained kalman filter preprocessor speech signal estimation autoregressive models noise signals expectation maximization approach;aurora 2 speech recognition;expectation maximisation algorithm	In this paper, a speech signal estimation framework involving Kalman filters for use as a front-end to the Aurora-2 speech recognition task is presented. Kalman-filter based speech estimation algorithms assume autoregressive (AR) models for the speech and the noise signals. In this paper, the parameters of the AR models are estimated using a expectation-maximization approach. The key to the success of the proposed algorithm is the constraint on the AR model parameters corresponding to the speech signal to belong to a codebook trained on AR parameters obtained from clean speech signals. Aurora-2 noise-robust speech recognition experiments are performed to demonstrate the success of the codebook-constrained Kalman filter in improving speech recognition accuracy in noisy environments. Results with both clean and multi-conditional training are provided to show the improvements in the recognition accuracy compared to the base-line system where no pre-processing is employed	autoregressive model;codebook;expectation–maximization algorithm;experiment;kalman filter;preprocessor;speech recognition	Venkatesh Krishnan;Sabato Marco Siniscalchi;David V. Anderson;Mark A. Clements	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660137	kalman filter;linear predictive coding;speech recognition;computer science;machine learning;speech coding;signal processing;pattern recognition;speech processing;acoustic model;autoregressive model;statistics	ML	-15.642900110787547	-92.46933532463531	146103
cc4307c9346afe066f48ff09747349d2bd8c0137	three dimensional articulator model for speech acquisition by children with hearing loss	speech intelligibility;3d modeling;computer aided articulator model;speech perception;magnetic resonance image;three dimensional;computer graphic;hearing impaired;3d model;hearing loss;speech production;mri techniques	Our research indicates that acquisition of phonetic skills in voiced and voiceless speech sounds was improved by using Computer Aided Articulatory Tutor (CAAT). The interface of CAAT displays the place of articulation and relevant image objects for articulatory training simultaneously. The place of articulation was presented by using three dimensional articulatory tutor. Suitable computer graphics and Magnetic Resonance Imaging (MRI) techniques were used to develop inner articulatory movements of the animated tutor. Ten hearing impaired children between the ages 4 and 7 were selected and trained for 30 hours across four weeks on 50 words under 10 lessons. The words were selected from the categories of voiced and voiceless stops namely Bilabial, Dental, Alveolar, Retroflex and Velar. The articulatory performance of HI children was investigated to find out their speech intelligibility.	speech acquisition	Arumugam Rathinavelu;Hemalatha Thiagarajan;Anupriya Rajkumar	2007		10.1007/978-3-540-73279-2_87	speech recognition;acoustics;engineering;audiology	NLP	-7.703728869088673	-84.06561226689054	146127
46a25031abceb16ce899b41c713b140ed5e6a4f2	automatic identification of instrument classes in polyphonic and poly-instrument audio		We present and compare several models for automatic identification of instrument classes in polyphonic and poly-instrument audio. The goal is to be able to identify which categories of instrument (Strings, Woodwind, Guitar, Piano, etc.) are present in a given audio example. We use a machine learning approach to solve this task. We constructed a system to generate a large database of musically relevant poly-instrument audio. Our database is generated from hundreds of instruments classified in 7 categories. Musical audio examples are generated by mixing multi-track MIDI files with thousands of instrument combinations. We compare three different classifiers : a Support Vector Machine (SVM), a Multilayer Perceptron (MLP) and a Deep Belief Network (DBN). We show that the DBN tends to outperform both the SVM and the MLP in most cases.	automatic identification and data capture;database;deep belief network;midi;machine learning;memory-level parallelism;multilayer perceptron;string (computer science);support vector machine	Philippe Hamel;Sean Wood;Douglas Eck	2009			speech recognition;polyphony;computer science	ML	-5.08905367471926	-88.01218686893938	146134
297e661fa7b40b092ca1a48a947e321f6411f7a6	normalized minimum-redundancy and maximum-relevancy based feature selection for speaker verification systems	speaker recognition systems;maximum relevancy;training;speaker recognition computational complexity;data mining;speaker verification systems;speaker verification systems feature selection minimum redundancy maximum relevancy;speaker verification;speaker recognition;mutual information testing speaker recognition data mining feature extraction information theory nist spatial databases speech analysis pattern recognition;redundancy;computational complexity;feature extraction;speech signals;mathematical model;speech signals normalized minimum redundancy maximum relevancy feature selection speaker verification systems information theoretical approach speaker recognition systems;minimum redundancy;error rate;mutual information;feature selection;fixed interval;information theoretic;normalized minimum redundancy;information theoretical approach;information theory	In this paper, an information theoretical approach to select features for speaker recognition systems is proposed. Conventional approaches having a fixed interval of analysis frames are not appropriate to represent dynamically varying characteristics of speech signals. To maximize the speaker-related information varied by the characteristics of speech signals, we propose an information theory based feature selection method where features are selected to have minimum-redundancy with in selected features but maximumrelevancy to training speaker models. Experimental results verify that the proposed method reduces the error rates of speaker verification systems by 27.37 % in NIST 2002 database.	feature selection;information theory;relevance;speaker recognition	Chi-Sang Jung;Moo Young Kim;Hong-Goo Kang	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960642	speaker recognition;speaker diarisation;speech recognition;information theory;feature extraction;word error rate;computer science;machine learning;pattern recognition;mathematical model;redundancy;mutual information;computational complexity theory;feature selection;statistics	Robotics	-15.776051001326254	-91.63094014946365	146173
349dfa6de1aaa89dba7813f21ac43a913faa1ab1	tight integration of spatial and spectral features for bss with deep clustering embeddings		Recent advances in discriminatively trained mask estimation networks to extract a single source utilizing beamforming techniques demonstrate, that the integration of statistical models and deep neural networks (DNNs) are a promising approach for robust automatic speech recognition (ASR) applications. In this contribution we demonstrate how discriminatively trained embeddings on spectral features can be tightly integrated into statistical model-based source separation to separate and transcribe overlapping speech. Good generalization to unseen spatial configurations is achieved by estimating a statistical model at test time, while still leveraging discriminative training of deep clustering embeddings on a separate training set. We formulate an expectation maximization (EM) algorithm which jointly estimates a model for deep clustering embeddings and complex-valued spatial observations in the short time Fourier transform (STFT) domain at test time. Extensive simulations confirm, that the integrated model outperforms (a) a deep clustering model with a subsequent beamforming step and (b) an EM-based model with a beamforming step alone in terms of signal to distortion ratio (SDR) and perceptually motivated metric (PESQ) gains. ASR results on a reverberated dataset further show, that the aforementioned gains translate to reduced word error rates (WERs) even in reverberant environments.	artificial neural network;automated system recovery;beamforming;cluster analysis;deep learning;discriminative model;distortion;etsi satellite digital radio;emoticon;expectation–maximization algorithm;pesq;short-time fourier transform;simulation;source separation;speech recognition;statistical model;test set	Lukas Drude;Reinhold Häb-Umbach	2017			artificial intelligence;pattern recognition;cluster analysis;computer science	ML	-15.59023025632227	-90.6656767831298	146188
62abbff1e9c1eb57a80337055dd3cdb72465907a	the phonetic specification of contour tones: the rising tone in mandarin		This paper investigates the phonetic specification of contour tones through a case study of the rising tone in Mandarin. The patterns of variation in the realization of the rising tone as a function of speech rate indicate that the specification of this tone involves targets for the slope of the f0 rise, the magnitude of the rise, and the alignment of the onset and offset of the rise. These targets conflict so the realization of the tone is a compromise between them. This analysis is formalized as a quantitative model of tone realization formulated in terms of weighted constraints enforcing tone targets.	contour line;onset (audio);pitch (music);super robot monkey team hyperforce go!	Hyesun Cho;Edward Flemming	2011			mandarin chinese;acoustics;mathematics;speech recognition	SE	-9.64310793985614	-85.01137773259694	146206
7521b06eb886fc0e3c52adaf5c93a1a5cabcdcb2	evaluation of the slovenian hmm-based speech synthesis system	modelo markov oculto;linguistique;speech synthesis;modelo markov;forme onde;modele markov cache;hidden markov model;speech processing;tratamiento palabra;traitement parole;concatenacion;intelligence artificielle;concatenation;hmm based speech synthesis;linguistica;markov model;evaluation subjective;forma onda;artificial intelligence;synthetiseur;sintesis palabra;waveform;inteligencia artificial;modele markov;sintetizador;subjective evaluation;synthese parole;synthesizer;evaluacion subjetiva;linguistics	A new HMM-based speech synthesis system for Slovenian language is presented. The quality of synthesized speech has been assessed by subjective and objective tests. The results show that the new system outperforms our previously developed diphone-based waveform concatenation synthesizer in terms of naturalness and general impression.	hidden markov model;speech synthesis	Bostjan Vesnicer;France Mihelic	2004		10.1007/978-3-540-30120-2_65	concatenation;speech recognition;waveform;computer science;artificial intelligence;speech processing;linguistics;markov model;speech synthesis;hidden markov model	NLP	-16.333744847823212	-85.25488413408777	146357
e74c421e0c359d24a6c52aaa321294c63dc5434c	consideration of processing strategies for very-low-rate compression of wideband speech signals with known text transcription.				Peter Veprek;Alan B. Bradley	1997			speech recognition;computer science;speech processing	ML	-13.711932341705998	-86.86126385778661	146415
167a1a1a168de1eb1d72a6babb4024e65f16929a	quantile based histogram equalization for noise robust speech recognition	word error rate;quantile equalization;noise robustness;automatic speech recognition;informatik;speech recognition;robustness;cumulant;density functional;histogram equalization;noise	This paper describes an approach to increase the noise robustness of automatic speech recognition systems by, transforming the signal after Mel scaled filtering, to make the cumulative density functions of the signal’s values in recognition match the ones that where estimated on the training data. The cumulative density functions are approximated using a small number of quantiles. Recognition tests on several databases showed significant reductions of the word error rates. On a real life database recorded in driving cars with a large mismatch between the training and testing conditions the relative reductions of the word error rates where over 60%.	approximation algorithm;database;histogram equalization;real life;speech recognition	Florian Erich Hilger	2001			speech recognition;word error rate;computer science;noise;pattern recognition;adaptive histogram equalization;histogram equalization;statistics;robustness;cumulant	ML	-14.20323612472845	-91.58013450376686	146507
073029db430b1b8a1c8ae398712b178f7f8de1c2	syllable based dnn-hmm cantonese speech to text system		This paper reports our work on building up a Cantonese Speech-to-Text (STT) system with a syllable based acoustic model. This is a part of an effort in building a STT system to aid dyslexic students who have cognitive deficiency in writing skills but have no problem expressing their ideas through speech. For Cantonese speech recognition, the basic unit of acoustic models can either be the conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC) syllables where finals are further split into nucleus and coda to reflect the intra-syllable variations in Cantonese. By using the Kaldi toolkit, our system is trained using the stochastic gradient descent optimization model with the aid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model (DNN-HMM) with and without I-vector based speaker adaptive training technique. The input features of the same Gaussian Mixture Model with speaker adaptive training (GMM-SAT) to DNN are used in all cases. Experiments show that the ONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the best performance with the word error rate (WER) of 9.66% and the real time factor (RTF) of 1.38812.	acoustic cryptanalysis;acoustic model;angular defect;coda (file system);deep learning;graphics processing unit;hidden markov model;kaldi;markov chain;mathematical optimization;mixture model;onset (audio);open network computing remote procedure call;speech recognition;stochastic gradient descent;syllable;word error rate	Timothy Wong;Claire Li;Sam Lam;Billy Chiu;Qin Lu;Minglei Li;Dan Xiong;Roy Shing Yu;Vincent T. Y. Ng	2016			speech recognition;syllable;computer science;hidden markov model	ML	-18.429989053583576	-88.06450050771646	146552
ef17af1c46253b40fa5e3a7055076a16f2dd6104	deleted interpolation and density sharing for continuous hidden markov models	dictation experiments;parameter smoothing deleted interpolation density sharing continuous hidden markov models smoothing techniques semicontinuous hidden markov model discrete hidden markov model hmm gaussian mean covariance parameters probability density parameter sharing technique probability density function pdf markov state dictation experiments word error rate reduction;word error rate;discrete hidden markov model;interpolation;covariance analysis;probability;probability density;gaussian processes;density sharing;hidden markov model;probability density function;interpolation hidden markov models smoothing methods error analysis context modeling speech recognition probability distribution training data power system modeling power smoothing;hmm;word error rate reduction;training data;error analysis;smoothing methods;hidden markov models;probability distribution;error rate;parameter smoothing;speech recognition;smoothing techniques;smoothing methods hidden markov models interpolation speech recognition probability gaussian processes covariance analysis;markov state;pdf;deleted interpolation;continuous hidden markov models;power system modeling;parameter sharing technique;context modeling;power smoothing;gaussian mean;semicontinuous hidden markov model;covariance parameters	As one of the most powerful smoothing techniques, deleted interpolation has been widely used in both discrete and semi-continuous hidden Markov model (HMM) based speech recognition systems. For continuous HMMs, most smoothing techniques are carried out on the parameters themselves such as Gaussian mean or covariance parameters. In this paper, we propose to smooth the probability density values instead of the parameters of continuous HMMs. This allows us to use most of the existing smoothing techniques for both discrete and continuous HMMs. We also point out that our deleted interpolation can be regarded as a parameter sharing technique. We further generalize this sharing to the probability density function (PDF) level, in which each PDF becomes a basic unit and can be freely shared across any Markov state. For a wide range of dictation experiments, deleted interpolation reduced the word error rate by 11% to 23% over other simple parameter smoothing techniques like flooring. Generic PDF sharing further reduced the error rate by 3%.	acoustic cryptanalysis;acoustic model;algorithm;experiment;hidden markov model;interpolation;markov chain;portable document format;semi-continuity;semiconductor industry;smoothing;speech recognition;triphone;word error rate	Xuedong Huang;Mei-Yuh Hwang;Li Jiang;Milind Mahajan	1996		10.1109/ICASSP.1996.543263	probability density function;word error rate;computer science;machine learning;pattern recognition;mathematics;hidden markov model;statistics	ML	-18.899793372275735	-92.41153563996956	146558
c0cc15d4e3e10cc74d85e3fca304920102926291	microphone array speech recognition: experiments on overlapping speech in meetings	overlapping speech;databases;background noise;microphones;numbers;data collection;speech processing;working environment noise;performance;vocabulary;geometry;speech;array signal processing;speaker separation;microphone arrays speech recognition loudspeakers background noise speech processing geometry vocabulary databases working environment noise coherence;speaker recognition;speech acquisition;microphones speaker recognition speech processing array signal processing;loudspeakers;microphone array;microphone array geometry;meetings;speech recognition;coherence;mccowan;small vocabulary corpus;microphone arrays;moore;performance speech recognition overlapping speech meetings speech acquisition speech processing microphone array geometry speaker separation small vocabulary corpus numbers	Thispaperinvestigatestheuseof microphonearraysto acquireand recognisespeechin meetings.Meetingsposeseveral interesting problemsfor speechprocessing,asthey consistof multiple competingspeakerswithin asmallspace,typically aroundatable.Due to their ability to provide hands-freeacquisitionand directional discrimination,microphonearrayspresenta potentialalternati ve to close-talkingmicrophonesin suchanapplication.We first proposeanappropriatemicrophonearraygeometryandimprovedprocessingtechniquefor this scenario,payingparticularattentionto speaker separationduringpossibleoverlapsegments.Datacollection of a small vocabulary speechrecognitioncorpus(Numbers) wasperformedin a real meetingroom for a singlespeaker, and several overlappingspeechscenarios.In speechrecognitionexperimentson the acquireddatabase, the performanceof the microphonearraysystemis comparedto thatof a close-talkinglapel microphone,anda singletable-topmicrophone.	experiment;microphone;speech recognition;vocabulary	Darren Moore;Iain McCowan	2003		10.1109/ICASSP.2003.1200015	voice activity detection;loudspeaker;speaker recognition;speech recognition;coherence;performance;computer science;speech;speech processing;background noise;data collection	ML	-12.08268956610653	-90.64474174837673	146565
82cfa9ad42846f2f6c49d77d9280f2f3d74353a9	encoded speech recognition accuracy improvement in adverse environments by enhancing formant spectral bands	speech recognition	Spoken dialogue information retrieval applications are the future trend for mobile users in automobiles, on cellular phones, etc. Due to the limitation of resources in these platforms, it may be advantageous to extract speech features, and compress and transmit them to a central hub where the computation intensive tasks such as speech recognition and speech understanding, etc. can be performed. Generally, the speech recognition accuracy degrades when the decoded speech signal (that is obtained after re-synthesizing the signal from the compressed features) is used. In addition, the background noise that is present in the above mentioned mobile systems will reduce the recognition accuracy. Therefore, in order to improve the recognition accuracy it is essential to extract robust features that can jointly optimize compression and recognition. In this paper, we describe a technique that improves the recognition accuracy of noisy encoded speech signals by performing spectral correction and spectral formant band enhancement before synthesizing the speech signal from the compressed features. We have conducted experiments on 1831 telephone speech utterances from 1831 speakers. We added (a) the invehicle noise recorded from a Volvo car moving on an asphalt road at 134 kmph, (b) the factory noise recorded in a factory and (c) the speech (babble) noise recorded in a cafeteria to these utterances at various signal-to-noise ratios (SNR). Our experimental results indicate recognition accuracy improvement up to 10% at 0 dB SNR.	computation;decibel;emoticon;experiment;futures studies;information retrieval;mobile phone;signal-to-noise ratio;speech recognition;usb hub	Shubha Kadambe;Ron Burns	2000			speech recognition;spectral bands;artificial intelligence;pattern recognition;formant;computer science	NLP	-12.315983691003119	-89.56132712242281	146597
062e22b1dba58f0e64e46e2aeb57b962ffa31c06	line spectral frequency-based features and extreme learning machine for voice activity detection from audio signal		Voice activity detection (VAD) refers to the task of identifying vocal segments from an audio clip. It helps in reducing the computational overhead as well elevate the recognition performance of speech-based systems by helping to discard the non vocal portions from an input signal. In this paper, a VAD technique is presented that uses line spectral frequency-based statistical features namely LSF-S coupled with extreme learning-based classification. The experiments were performed on a database of more than 350 h consisting of data from multifarious sources. We have obtained an encouraging overall accuracy of 99.43%.		Himadri Mukherjee;Sk Md Obaidullah;K. C. Santosh;Santanu Phadikar;Kaushik Roy	2018	I. J. Speech Technology	10.1007/s10772-018-9525-6	overhead (computing);voice activity detection;speech recognition;artificial intelligence;audio signal;extreme learning machine;computer science;pattern recognition	ML	-11.950008454664847	-90.37362850911818	146688
8d097578c0c66f2188f1046023b63e89ce4f6683	prosodic and segmental effects on epg contact patterns of word-initial german clusters	consonant clusters;international organizations;linguistique;internal structure;systemes productifs locaux;boundary condition;discours;prosodie;initial strenghtening;production;electropalatography;articulatory coordination;prosody;german;allemand langue;speech production	This study investigates the effects of segmental composition and prosodic variation, namely boundary strength and lexical stress, on the production of word-initial clusters in German. The internal structure of the clusters /kl, kn, ks, sk/ has been analyzed by means of EPG recordings from seven speakers of German. Derived temporal and spatial parameters indicate that /kn/ is consistently produced with a lag between the consonants and /kl/ with considerable overlap. This categorical difference is also stable across stress and boundary conditions and is attributed to manner-based and perceptual recoverability constraints. No clear pattern emerges for /sk/ and /ks/. Therefore, stability of temporal organization across prosodic conditions is only tested for /kl/ and /kn/. Temporally, boundary level affects the duration of the adjacent consonant and the overlap within the clusters /kn/ and /kl/, whereas spatially /k/ is affected only in /kn/ but not in /kl/. Stress effects are not restricted to the nucleus but also affect the internal organization of the clusters. The interplay between segmental and prosodic timing effects indicates that the internal structure of clusters shows linguistically crucial and highly constrained timing patterns which can only vary within certain limits.		Lasse Bombien;Christine Mooshammer;Phil Hoole;Barbara Kühnert	2010	J. Phonetics	10.1016/j.wocn.2010.03.003	psychology;speech production;speech recognition;philosophy;german;linguistics;sociology;prosody;communication	HCI	-10.847437453605734	-81.10022075995425	146851
22c4674717bd3c489597474e09b09e89a155ac2d	automatic detection of prosodic boundaries in brazilian portuguese spontaneous speech		This paper presents some models based on multiple phonetic-acoustic parameters for the automatic detection of prosodic boundaries in spontaneous speech. A sample with seven excerpts of monologic Brazilian Portuguese spontaneous speech was segmented into prosodic units by 14 trained annotators. The perceived prosodic boundaries were annotated as terminal or non-terminal prosodic boundaries. A Praat script was prepared in order to extract a set of acoustic parameters during the speech signal. Two statistical classifiers, namely Random Forest e Linear Discriminant Analysis, were used to generate models of subgroups of acoustic parameters that could work as predictors of prosodic boundaries in comparison with the human annotators. The initial evaluation of the classifiers showed that both present relative success in detecting boundaries. The LDA performed better in predicting boundaries and therefore its models were refined. The final model for terminal boundaries showed 80% of agreement with human annotators. As for non-terminal boundaries, three models were obtained. The sum of boundaries identified by the three models together corresponds to an agreement of 98% with the human annotators.	spontaneous order	Bárbara J. S. Teixeira;Plínio A. Barbosa;Tommaso Raso	2018		10.1007/978-3-319-99722-3_43	brazilian portuguese;linear discriminant analysis;random forest;artificial intelligence;pattern recognition;mathematics	NLP	-17.372679626144585	-83.11707008451661	146899
124fc5f3e7f34e0bb773df0c1df87c0e4410ffb1	enhanced histogram normalization in the acoustic feature space		We describe two methods that aim at normalizing acoustic vectors at the filterbank level such that the test data distribution matches the training data distribution. They enhance the histogram normalization technique proposed earlier by taking care of the variable silence fraction for each speaker, and by rotating the feature space. We report a number of recognition tests under minor (different microphones in training and test, telephone data) and major (office vs. car recordings) mismatch conditions. Both methods give superior performance to the basic histogram normalization approach. The overall improvements in word error rate (WER) range between 6% and 85% relative.	acoustic cryptanalysis;care-of address;feature vector;filter bank;microphone;test data;word error rate	Sirko Molau;Florian Hilger;Daniel Keysers;Hermann Ney	2002			speech recognition;feature (computer vision);normalization (statistics);test data;histogram matching;pattern recognition;artificial intelligence;word error rate;filter bank;histogram;feature vector;computer science	ML	-14.01760197704853	-90.50440857654755	147106
1b69f4b89f08b093e65572b19b474b1c32fd7570	perceiving word prosodic contrasts as a function of sentence prosody in two dutch limburgian dialects	article letter to editor;intonational phrase;native language	This paper investigates the perception of word prosodic contrasts as a function of focus and position in the intonational phrase in two Dutch Limburgian dialects, Roermond and Weert. While their word prosodic contrasts share a historical source, the two dialects differ in that Weert realizes the prosodic contrast by duration, while Roermond uses f0. In addition, the Roermond dialect, but not the Weert dialect, appears to neutralize the prosodic distinction outside the focus constituent in phrase-internal syllables. The stimulus materials were naturally elicited word pairs in which the prosodic contrast marks a difference in grammatical number. In two perception experiments, listeners decided in a forced-choice task whether the words represented a singular or a plural form. Listeners with a Roermond Dutch background recognized the members of the opposition in focused contexts and phrase-final contexts, but failed to do so in phraseinternal, nonfocused contexts. By contrast, listeners whose native language was Weert Dutch perceived the grammatical number distinction in all contexts with comparable measures of success. Second, the presentation of stimuli consisting of words excised from their sentences significantly impaired the recognition of grammatical number in the Roermond group, but not in the Weert group. These results suggest that the perception of the tonal contrast, but not that of the duration contrast, depends on the intonational context. The fact that in the Roermond dialect lexical and intonational tones are integrated in see front matter r 2005 Elsevier Ltd. All rights reserved. wocn.2005.03.002 ding author. ress: r.fournier@let.ru.nl (R. Fournier).	acoustic cryptanalysis;alain fournier;ercim cor baayen award;experiment;lexical substitution;rendering (computer graphics);rietveld;robustness (computer science);semantic prosody	Rachel Fournier;Jo Verhoeven;Marc Swerts;Carlos Gussenhoven	2006	J. Phonetics	10.1016/j.wocn.2005.03.002	natural language processing;speech recognition;philosophy;computer science;first language;linguistics;sociology	AI	-10.55003945554798	-81.00285706712928	147287
8ddb2ce6220f158f5a7364709669d873cf22e503	probabilistic optimum filtering for robust speech recognition	multidimensional transversal filters;multidimensional transversal filters robust speech recognition probabilistic optimum filtering mapping algorithm simultaneous recordings noisy speech piecewise linear transformation multidimensional linear least squares filters conditional gaussian model decipher speech recognition system sri experimental results recognition errors reduction training testing acoustic environment training acoustic environments clean speech microphones;testing acoustic environment;piecewise linear transformation;microphones;nonlinear filters;filtering;robust speech recognition;least squares approximations;piecewise linear;mapping algorithm;probability;piecewise linear techniques;gaussian processes;multidimensional linear least squares filters;working environment noise;training;linear least square;clean speech;acoustic testing;filtering robustness speech recognition acoustic testing simultaneous localization and mapping piecewise linear techniques acoustic noise working environment noise multidimensional systems nonlinear filters;acoustic noise;simultaneous recordings;simultaneous localization and mapping;noisy speech;multidimensional digital filters speech recognition filtering theory piecewise linear techniques least squares approximations gaussian processes probability;decipher speech recognition system;speech recognition;sri;robustness;conditional gaussian model;recognition errors reduction;training acoustic environments;multidimensional digital filters;experimental results;probabilistic optimum filtering;filtering theory;multidimensional systems	In this paper we present a new mapping algorithm for speech recognition that relates the features of simultaneous recordings of clean and noisy speech. The model is a piecewise linear transformation applied to the noisy speech feature. The transformation is a set of multidimensional linear least-squares filters whose outputs are combined using a conditional Gaussian model. The algorithm was tested using SRI'S DECIPHERIM speech recognition system [1-5]. Experimental results show how the mapping is used to reduce recognition errors when the training and testing acoustic environments do not match.	acoustic cryptanalysis;algorithm;linear least squares (mathematics);piecewise linear continuation;speech recognition	Leonardo Neumeyer;Mitch Weintraub	1994		10.1109/ICASSP.1994.389267	filter;speaker recognition;speech recognition;piecewise linear function;multidimensional systems;computer science;machine learning;noise;pattern recognition;probability;gaussian process;acoustic model;mathematics;statistics;robustness;simultaneous localization and mapping	NLP	-17.54830853384321	-92.98450290548027	147618
32a786eddcea3fe8cf5a2ca3cc91f696c41494fc	deep learning based emotion recognition from chinese speech		Emotion Recognition is challenging for understanding people and enhance human computer interaction experiences. In this paper, we explore deep belief networks DBN to classify six emotion status: anger, fear, joy, neutral status, sadness and surprise using different features fusion. Several kinds of speech features such as Mel frequency cepstrum coefficient MFCC, pitch, formant, et al., were extracted and combined in different ways to reflect the relationship between feature combinations and emotion recognition performance. We adjusted different parameters in DBN to achieve the best performance when solving different emotions. Both gender dependent and gender independent experiments were conducted on the Chinese Academy of Sciences emotional speech database. The highest accuracy was 94.6i¾ź%, which was achieved using multi-feature fusion. The experiment results show that DBN based approach has good potential for practical usage of emotion recognition, and suitable multi-feature fusion will improve the performance of speech emotion recognition.	deep learning;emotion recognition	Weishan Zhang;Dehai Zhao;Xiufeng Chen;Yuanjie Zhang	2016		10.1007/978-3-319-39601-9_5	speech recognition	NLP	-12.311092452646557	-88.00867137822533	147689
68bf5ac046fd104fc1508b870cd65bbfdd22e816	audio feature and classifier analysis for efficient recognition of environmental sounds	audio signal processing;support vector machines;hmm;hidden markov models support vector machines mel frequency cepstral coefficient transform coding rain helicopters explosions;hidden markov models;svm environmental sound classification mpeg 7 mfcc hmm;feature extraction;environmental sound classification;speech recognition;svm;audio spectrum flatness centroid spread and audio harmonicity environmental sound recognition unstructured nature feature analysis classifier analysis emergency alarm car horn gun explosion automobile helicopter water wind rain applause crowd laughter audio features mpeg 7 family zcr mfcc hmm svm classifiers asfcs h;mfcc;mpeg 7;support vector machines audio signal processing feature extraction hidden markov models speech recognition	Environmental sounds (ES) have different characteristics, such as unstructured nature and typically noise-like and flat spectrums, which make recognition task difficult compared to speech or music sounds. Here, we perform an exhaustive feature and classifier analysis for the recognition of considerably similar ES categories and propose a best representative feature to yield higher recognition accuracy. In the experiments, thirteen (13) ES categories, namely emergency alarm, car horn, gun, explosion, automobile, helicopter, water, wind, rain, applause, crowd, and laughter are detected and tested based on eleven (11) audio features (MPEG-7 family, ZCR, MFCC, and combinations) by using the HMM and SVM classifiers. Extensive experiments have been conducted to demonstrate the effectiveness of these joint features for ES classification. Our experiments show that, the joint feature set ASFCS-H (Audio Spectrum Flatness, Centroid, Spread, and Audio Harmonicity) is the best representative feature set with an average F-measure value of 80.6%.	advanced synchronization facility;experiment;f1 score;hidden markov model;mpeg-7;sound card;spectral density;zero-crossing rate	Cigdem Okuyucu;Mustafa Sert;Adnan Yazici	2013	2013 IEEE International Symposium on Multimedia	10.1109/ISM.2013.29	support vector machine;speech recognition;computer science;machine learning;pattern recognition;hidden markov model	Vision	-9.705817342043446	-92.2526181080215	147752
4bcec492c4857b196645876188e707558da7fc9b	relative contributions of amplitude and phase to the intelligibility advantage of ideal binary masked sentences		Many studies have shown the advantage of using ideal binary masking (IdBM) to improve the intelligibility of speech corrupted by interfering maskers. Given the fact that amplitude and phase are two important acoustic cues for speech perception, the present work further investigated the relative contributions of these two cues to the intelligibility advantage of IdBM-processed sentences. Three types of Mandarin IdBM-processed stimuli (i.e., amplitude-only, phase-only, and amplitude-and-phase) were generated, and played to normal-hearing listeners to recognize. Experiment results showed that amplitudeor phase-only cue could lead to significantly improved intelligibility of IdBM-processed sentences in relative to noise-masked sentences. A maskerdependent amplitude over phase advantage was observed when accounting for their relative contributions to the intelligibility advantage of IdBM-processed sentences. Under steady-state speech-spectrum shaped noise, both amplitudeand phase-only IdBM-processed sentences contained intelligibility information close to that contained in amplitudeand-phase IdBM-processed sentences. In contrast, under competing babble masker, amplitude-only IdBM-processed sentences were more intelligible than phase-only IdBMprocessed sentences, and neither could account for the intelligibility advantage of amplitude-and-phase IdBMprocessed sentences.	acoustic cryptanalysis;intelligibility (philosophy);steady state;super robot monkey team hyperforce go!	Lei Wang;Shufeng Zhu;Diliang Chen;Yong Feng;Fei Chen	2016		10.21437/Interspeech.2016-18	speech recognition	NLP	-10.212973201710755	-82.22930472493842	147901
eb015d59fcbacc661de328157b9964460f563688	asr for emotional speech: clarifying the issues and enhancing performance	modelizacion;eficacia sistema;enhanced language modelling;performance systeme;british national corpus;emotion recognition;speech enhancement;system performance;modelisation;automatic recognition;reconocimiento voz;speech recognition;reconnaissance parole;spontaneous speech;modeling;reconnaissance emotion;language model;emotionally coloured speech;reconocimiento automatico;reconnaissance automatique	There are multiple reasons to expect that recognising the verbal content of emotional speech will be a difficult problem, and recognition rates reported in the literature are in fact low. Including information about prosody improves recognition rate for emotions simulated by actors, but its relevance to the freer patterns of spontaneous speech is unproven. This paper shows that recognition rate for spontaneous emotionally coloured speech can be improved by using a language model based on increased representation of emotional utterances. The models are derived by adapting an already existing corpus, the British National Corpus (BNC). An emotional lexicon is used to identify emotionally coloured words, and sentences containing these words are recombined with the BNC to form a corpus with a raised proportion of emotional material. Using a language model based on that technique improves recognition rate by about 20%.	altretamine;automatic system recovery;body of uterus;british national corpus;ephrin type-b receptor 1, human;hidden markov model;intelligibility (philosophy);language model;lexicon;linguistics;relevance;semantic prosody;speech disorders;speech recognition;spontaneous order;sentence	Theologos Athanaselis;Stelios Bakamidis;Ioannis Dologlou;Roddy Cowie;Ellen Douglas-Cowie;Cate Cox	2005	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2005.03.008	natural language processing;speech recognition;systems modeling;computer science;language model	NLP	-14.16166949559328	-83.43184899989132	147931
cf6e84a830636a3a2e9d9233a0112d596f6e331b	a dictionary approach to repetitive pattern finding in music	lempel ziv;compression algorithm;null;data mining;database management;multiple signal classification;hidden markov models;indexing;feature extraction;indexation;dictionaries multiple signal classification audio databases spatial databases indexing multimedia databases data mining feature extraction music information retrieval hidden markov models;spatial databases;music information retrieval;dictionaries;multimedia databases;audio databases;content based retrieval	A dictionary-based approach for extracting repetitive patterns in music aiming at music feature extraction and indexing for audio database management is proposed. In this system, segmentation is achieved with the tempo information, and a music score is decomposed into bars. Each bar is indexed to construct a bar index table. Then, an adaptive dictionary-based compression algorithm known as Lempel Ziv 78 (LZ-78) is applied to the barrepresented music scores to extract repetitive patterns. Finally, pruning is applied to this dictionary to remove non-repeating patterns and to combine shorter repeating patterns into a longer one. The LZ78 algorithm is slightly modified to achieve better results in the current application context. Experiments performed on a popular music database of MIDI files demonstrated that the proposed algorithm extracts repeating melodies effectively with a speed of four times faster compared to the traditional linear search approach.	algorithm;context (computing);dictionary;feature extraction;lz77 and lz78;lempel–ziv–welch;linear search;list of online music databases;midi	Hsuan-Huei Shih;Shrikanth (Shri) Narayanan;C.-C. Jay Kuo	2001	IEEE International Conference on Multimedia and Expo, 2001. ICME 2001.	10.1109/ICME.2001.1237711	data compression;search engine indexing;speech recognition;feature extraction;computer science;multiple signal classification;machine learning;pattern recognition;data mining;hidden markov model	DB	-7.090264384042508	-94.1021952345938	148017
63b9ef62e1a290a096cac46e982d840c92559705	a coupled hmm for audio-visual speech recognition	audio visual speech recognition;statistical properties;visualization;hidden markov models;hidden markov models visualization;acoustic noise;visual features;speech recognition;audio visual;coupled hidden markov model	In recent years several speech recognition systems that use visual together with audio information showed significant increase in performance over the standard speech recognition systems. The use of visual features is justified by both the bimodality of the speech generation and by the need of features that are invariant to acoustic noise perturbation. The audio-visual speech recognition system presented in this paper introduces a novel audio-visual fusion technique that uses a coupled hidden Markov model (HMM). The statistical properties of the coupled-HMM allow us to model the state asynchrony of the audio and visual observations sequences while still preserving their natural correlation over time. The experimental results show that the coupled HMM outperforms the multistream HMM in audio visual speech recognition.	acoustic cryptanalysis;asynchrony (computer programming);audio-visual speech recognition;hidden markov model;markov chain	Ara V. Nefian;Luhong Liang;Xiaobo Pi;Xiaoxiang Liu;Crusoe Mao;Kevin P. Murphy	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745027	voice activity detection;natural language processing;speaker recognition;audio mining;speech recognition;visualization;computer science;speech coding;noise;pattern recognition;speech processing;acoustic model;hidden markov model	Vision	-12.422109788024024	-90.83598888892786	148092
fd0518997677bf1bb30b20ed61f873bf6fd98620	consonant discrimination of degraded speech using an efferent-inspired closed-loop cochlear model.			cochlear implant	David P. Messing;Lorraine Delhorne;Ed Bruckert;Louis D. Braida;Oded Ghitza	2008			speech recognition;artificial intelligence;pattern recognition;consonant;efferent;computer science	NLP	-13.493598926239708	-86.88480159628627	148144
92d5aa4d729f2f88aafe6e3a67277303187d39ef	access to ethnic music: advances and perspectives in content-based music information retrieval	busqueda informacion;science general;ipemanalysis;hungarian folk music;analisis contenido;traitement signal;europa;self organizing maps;access;procesamiento informacion;articulo sintesis;metadata;article synthese;surexposition;information retrieval;overexposure;ethnomusicology;sobreexposicion;archive;digital music;content analysis;semantic information;archivo;recherche information;feature extraction;signal processing;music information retrieval;information processing;metadonnee;metadatos;extraction caracteristique;analyse contenu;europe;analisis semantico;traitement information;analyse semantique;review;procesamiento senal;music archives;semantic analysis	Access to digital music collections is nowadays facilitated by content-based methods that allow the retrieval of music on the basis of intrinsic properties of audio, in addition to advanced metadata processing. However, access to ethnic music remains problematic, as this music does not always correspond to the Western concepts that underlie the currently available content-based methods. In this paper, we examine the literature on access to ethnic music, while focusing on the reasons why the existing techniques fail or fall short of expectations and what can be done about it. The paper considers a review of the work on signals and feature extraction, on symbolic and semantic information processing, and on metadata and context tools. An overview is given of several European ethnic music archives and related ongoing research projects. Problems are highlighted and suggestions of the ways in which to improve access to ethnic music collections are given. & 2009 Elsevier B.V. All rights reserved.	archive;feature extraction;information processing;information retrieval	Olmo Cornelis;Micheline Lesaffre;Dirk Moelants;Marc Leman	2010	Signal Processing	10.1016/j.sigpro.2009.06.020	computer vision;speech recognition;self-organizing map;digital audio;content analysis;information processing;feature extraction;computer science;ethnomusicology;exposure;machine learning;signal processing;multimedia;metadata	Web+IR	-5.5836836242647445	-91.98006190360658	148463
cbed8726c09b44020906cfd3501a446627a8a8bd	a simple procedure to clarify the relation between text and prosody	hierarchical structure	"""Four texts originally broadcasted on radio, were presented to 52 participants in a paper-and-pencil task: they were instructed to assign a linear structure by placing slashes between segments, and a hierarchical structure by underscoring and crossing out segments. Strength of paragraph boundary had a regular relation with pause duration and F0 maximum. Hierarchical position tended to be marked in two ways: centrality by pause duration and redundancy by F0 maximum. Speech rate was independent of text structure. 1 TWO ASPECTS OF TEXT STRUCTURE Prosodic parameters are dependent on characteristics of the utterances produced by a speaker. Traditionally, most prosodic studies have paid attention to syntactic and informational aspects of individual utterances (see e.g. van Wijk & Kempen, 1984; van Wijk, 1987). But prosodic correlates of the ways utterances combine to form a text, are receiving attention more and more. So far, this research on textual prosody has mainly concentrated on the linear organization of texts. In this type of studies, a text is defined as a concatena-tion of elementary building blocks, the paragraphs, and the focus is on the prosodic marking of their beginning and end. This linear aspect of text structure accounts for the >chunking= function of prosody. But text organization has a hierarchical aspect as well. Paragraphs differ in importance: they are not equally essential to the message to be conveyed. And the same goes for the utterances within each paragraph; some of them are crucial, while others could be easily missed. This hierarchical aspect of text structure is expected to have a >levels= effect on prosody. A lot of evidence has been reported for the chunking function of prosody (see e.g. Sluijter & Terken, 1993; Swerts, 1997). Paragraphs are usually preceded by a relatively long pause and start with a higher pitch range. They also tend to close with a lower pitch range and a higher speaking rate (Hirschberg & Grosz, 1992). For the levels effect, however, evidence is mostly suggestive. It is based, for instance, on having subjects read aloud texts of a specific genre and with carefully designed structures such as narratives based on story grammar (Noordman, Dassen, Swerts & Terken, 1999). The question is whether the same result will be found in situations in which the speaker is free to determine both the genre and the organization of the text. 2 OBJECTIVE OF THIS STUDY To explore the prosodic correlates of text structure, we should also look at speech obtained in non-laboratory settings. This requires a flexible instrument to analyze texts that differ widely in terms of genre, content and, last but not least, quality. Such an instrument has been developed within the framework of Rhetorical Structure Theory (Mann & Thompson, 1988). The application of this technique demands a considerable proficiency and tends to be time-consuming. This has its drawback in terms of reliability and costs (den Ouden, van Wijk, Terken & Noordman, 1999; Sanders & van Wijk, 1996). Therefore we decided to supplement this theoretically based instrument with a practical procedure that would deliver a linear and hierarchical organization of randomly selected texts in a simple way, that is, fast and cheap. In ! """" ISCA Archive"""	archive;centrality;hirschberg's algorithm;international symposium on computer architecture;item unique identification;pitch (music);randomness;semantic prosody;shallow parsing	Hanny den Ouden;Carel van Wijk;Marc Swerts	2000			natural language processing;computer science;pattern recognition;linguistics	NLP	-13.204004450310638	-80.87989846340383	148619
377ac6270decdb5a3a726603f5e505b36d17e30c	a hybrid neural-markov approach for learning to compose music by example	duracion;chaine markov;cadena markov;proceso markov;musica;modelo hibrido;acoustique musicale;intelligence artificielle;modele hybride;duration;hybrid model;musical acoustics;hybrid approach;musique;neural net;machine learning;acustica musical;processus markov;information processing;markov process;pattern recognition;artificial intelligence;inteligencia artificial;reconnaissance forme;reseau neuronal;reconocimiento patron;music;red neuronal;duree;neural network;markov chain	In this paper we introduce a hybrid approach to autonomous music composition by example. Our approach utilizes pattern recognition, Markov chains, and neural networks. We first extract patterns from existing musical training sequences, and then construct a Markov chain based on these patterns with each state corresponding to a pattern. We then use a neural network to learn which shifts of pitch and duration are allowed for each pattern in the training sequences. Using this hybrid model, we compose novel musical sequences.	markov chain	Karsten A. Verbeurgt;Mikhail Fayer;Michael Dinolfo	2004		10.1007/978-3-540-24840-8_41	markov chain;speech recognition;computer science;artificial intelligence;machine learning;musical acoustics;music;duration;markov process;artificial neural network;statistics	ML	-16.13220694400837	-87.61115392152287	149102
0efbeb2576779b1da326cd98a1cfcc3db3dfc7be	transitional probability and phoneme monitoring	databases;linguistics;speech intelligibility;probability;speech recognition;frequency;pediatrics;skin;transition probability;speech processing;natural languages;psychology	Two phoneme monitoring experiments examined the influence of Transitional Probability (TP) on phoneme recognition. Target phonemes appeared at the end of Consonant-Vowel-Consonant (CVC) syllables, or as the first element of coda clusters in CVCC syllables. Reliable TP effects were found only for targets in CVCC syllables. The TPs both into and out of the targets influenced listeners' ability to detect them in CVCCs. Furthermore, targets were more difficult to detect in CVCCs than in CVCs. TP may only influence segment recognition when that segment is more difficult to recognise, as when it occurs in a cluster.	coda (file system);experiment;markov chain;vowel–consonant synthesis	James M. McQueen;Mark A. Pitt	1996			speech recognition;acoustics;computer science;communication	ML	-11.503683747808727	-82.89258150478996	149174
9fc25c171c55b76d1a2ecf4229e73e71efd92178	towards articulatory control of talking heads in humanoid robotics using a genetic-fuzzy imitation learning algorithm	humanoid robot;fuzzy genetic optimization;articulatory model;talking head;imitation learning;speech imitation	In human heads there is a strong structural linkage between the vocal tract and facial behavior during speech. For a robotic talking head to have human-like behavior, this linkage should be emulated. One way to do that is to estimate the articulatory features from a given utterance and to use them to control a talking head. In this paper, we describe an algorithm to estimate the articulatory features from a spoken sentence using a novel computational model of human vocalization. Our model uses a set of fuzzy rules and genetic optimization. That is, the places of articulation are considered as fuzzy sets whose degrees of membership are the values of the articulatory features. The fuzzy rules represent the relationships between places of articulation and speech acoustic parameters, and the genetic algorithm estimates the degrees of membership of the places of articulation according to an optimization criteria and it performs imitation learning. We verify our model by performing audio-visual subjective tests of animated talking heads showing that the algorithm is able to produce correct results. In particular, subjective listening tests of artificially generated sentences from the articulatory description resulted in an average phonetic accuracy slightly under 80%. Through the analysis of large amounts of natural speech, the algorithm can be used to learn the places of articulation of all phonemes of a given speaker. The estimated places of articulation are then used to control talking heads in humanoid robotics.	algorithm;humanoid robot;robotics	Enzo Mumolo;Massimiliano Nolich	2007	I. J. Humanoid Robotics	10.1142/S0219843607000959	natural language processing;speech recognition;computer science;humanoid robot;artificial intelligence	Robotics	-15.474614304734008	-82.79881723318348	149175
6a2ef01116bb914883f9442439a4128574ceb734	lip-reading of japanese vowels using neural networks	neural network		artificial neural network	Tomio Watanabe;Masaki Kohda	1990			speech recognition;artificial intelligence;pattern recognition;artificial neural network;time delay neural network;computer science	NLP	-15.549166651042018	-87.33116357954668	149328
209cffd14cad7aa2780a2d9772028f76788c6584	musical applications of digital synthesis and processing techniques : realisation using csound and the phase vocoder			csound;phase vocoder	Rajmil Fischman	1991				EDA	-13.23369748665705	-86.44838511470175	149465
bd668158232b18b90854929a69ce91ab72ce9635	experimental evidence on the syllabification of two-consonant clusters in czech		Abstract This study examines syllabification preferences of 30 speakers of Czech in two behavioural experiments using real disyllabic words with 61 intervocalic CC clusters as stimuli. The aim was to evaluate competing theoretical predictions about syllable boundaries in Czech. Participants synchronized individual syllables with metronome pulses in Experiment 1 (induced pause insertion) and produced syllables in reversed order in Experiment 2 (syllable reversal). Logistic regression analyses revealed significant effects of cluster sonority type, phonological length of the preceding vowel and word-edge phonotactics (also in relation to frequency of occurrence). Morphological structure of the items significantly influenced syllable boundary placement as well. The results of both experiments converge towards the effects found in previous studies on English and some other languages. However, ambisyllabic responses were virtually non-existent in pause insertion and relatively low (8%) in syllable reversal, which differs from the results on Germanic languages. Finally, the findings do not support strict onset maximization but rather indicate an onset-filling strategy.		Pavel Sturm	2018	J. Phonetics	10.1016/j.wocn.2018.08.002	phonotactics;speech recognition;czech;syllable;vowel;syllabification;consonant cluster;psychology;pattern recognition;artificial intelligence;sonority hierarchy	NLP	-10.907528670687315	-81.23764179509503	149482
80b4b8f2cd15c93877a6e3f0ec7afa5aa1fd1990	auditory perception of filled and empty time intervals, and mechanism of time discrimination.	auditory perception			Itaru F. Tatsumi;Hiroya Fujisaki	1994			speech recognition;computer science	HCI	-7.4241433855432915	-83.67878513133155	149582
5455af59fe696a8146c707a570f97283de95c3fd	acoustic correlates of contrastive stress in compound words versus verbal phrase in mandarin chinese	判解;weilin shen;frederic isel;morpholexical ambiguity;jacqueline vaissiere;法律詞典;論文;大陸法學;法規;月旦法學;compounding;法律題庫;裁判時報;acoustic features;月旦知識庫;法學資料庫;compound versus nuclear stress;tssci;教學	Duanmu (2000) proposed that tonal languages, such as Chinese, follow the same Compound and Nuclear Stress Rules (Chomsky & Halle, 1968) for phrasal stress as English. This study investigates the acoustic correlates of contrastive stress between compound words and verbal phrases in Mandarin Chinese. We focused on the durational, fundamental frequency, and intensity correlates of stress within minimal pair MN modifier-head compounds and VO verb-object phrases. Our results demonstrated that (1) the final syllable was more lengthened in [VO] than in [MN] and that (2) the F0 range was larger in [VO] than in [MN]. Moreover, the duration of the pause between the two syllables seems to play a role in distinguishing between [MN] and [VO]. In contrast, we showed that intensity contributed less to this distinction. Our results confirmed the right stress pattern in [VO]; however, we failed to find the lexical stress on the Left syllable we had expected, at least with the speakers we examined. Taken together, the present acoustic study lends support to the hypothesis that principles of stress upward of word level are universal through different languages.	acoustic cryptanalysis;modifier key;stress ball;super robot monkey team hyperforce go!;syllable	Weilin Shen;Jacqueline Vaissière;Frédéric Isel	2013	IJCLCLP		speech recognition;linguistics;communication	NLP	-10.777379994816174	-81.33666361570894	149651
1714e8f40509a61cc8d4920829c4559f594518c4	exploring the naturalness of several german high-quality-text-to-speech systems	prosody;perceptual evaluation.;modeling natural segment durations. keywords: tts;text to speech	The synthesis of near-to-natural F0 contours is an important issue in text-to-speech and crucial to the naturalness and intelligibility of synthetic speech. In earlier studies of the first author a model of German intonation was developed that is based on the quantitative Fujisaki-model. The current paper addresses a perception experiment comparing a TTS-system incorporating this new approach with several German TTS-systems with high segmental quality. Natural speech samples and a synthesis version with natural segment durations were used as references. Results show, that the natural speech samples unanimously received 10 points on a 0 to 10 point scale. The best TTS-systems cluster around a mean value of 5.0, whereas the variant with natural durations reached a mean score of 6.6 points, indicating the importance of closely modeling natural segment durations.	fujisaki model;intelligibility (philosophy);natural language;netware file system;speech synthesis;synthetic intelligence	Hansjörg Mixdorff;Dieter Mehnert	1999			speech recognition;perception;natural language processing;computer science;intelligibility (communication);naturalness;speech synthesis;german;artificial intelligence	ML	-13.902192689095319	-83.76922218791668	149672
156654395199ce852ca76e28965085bb6e0da8c4	timbre variations as an attribute of naturalness in clarinet play	busqueda informacion;duracion;desviacion tipica;motion control;timbre;envoltura senal;information retrieval;standard deviation;musica;acoustique musicale;hombre;duration;instrumento viento;commande mouvement;musical acoustics;control movimiento;musique;musical instrument;instrumento musical;sello;acustica musical;recherche information;signal envelope;human;instrument musique;ecart type;wind instrument;methode moyenne;enveloppe signal;music;averaging method;audio acoustics;metodo medio;instrument vent;duree;homme;acoustique audio	A digital clarinet played by a human and timed by a metronome was used to record two playing control parameters, the breath control and the reed displacement, for 20 repeated performances. The regular behaviour of the parameters was extracted by averaging and the fluctuation was quantified by the standard deviation. It was concluded that the movement of the parameters seem to follow rules. When removing the fluctuations of the parameters by averaging over the repetitions, the result sounded less expressive, although it still seemed to be played by a human. The variation in timbre during the play, in particular within a note’s duration, was observed and then fixed while the natural temporal envelope was kept. The result seemed unnatural, indicating that the variation of timbre is important for the naturalness.	displacement mapping;performance;quantum fluctuation	Snorre Farner;Richard Kronland-Martinet;Thierry Voinier;Sølvi Ystad	2005		10.1007/11751069_4	motion control;speech recognition;acoustics;musical acoustics;music;duration;standard deviation;information retrieval;statistics	HCI	-6.44170861356827	-83.23222509235895	149833
9a56e8efbbedce9f429689dd267233fae38e49f4	finding syntactic structures from human motion data	computer graphics;context free grammar;syntactic structure;human motion capture data;i 3 7 computer graphics three dimensional graphics and realism animation;motion classification;motion rearrangement	We present a new approach to motion rearrangement that preserves the syntactic structures of an input motion automatically by learning a context-free grammar from the motion data. For grammatical analysis, we reduce an input motion into a string of terminal symbols by segmenting the motion into a series of subsequences, and then associating a group of similar subsequences with the same symbol. In order to obtain the most repetitive and precise set of terminals, we search for an optimial segmentation such that a large number of subsequences can be clustered into groups with little error. Once the input motion has been encoded as a string, a grammar induction algorithm is employed to build up a context-free grammar so that the grammar can reconstruct the original string accurately as well as generate novel strings sharing their syntactic structures with the original string. Given any new strings from the learned grammar, it is straightforward to synthesize motion sequences by replacing each terminal symbol with its associated motion segment, and stitching every motion segment sequentially. We demonstrate the usefulness and flexibility of our approach by learning grammars from a large diversity of human motions, and reproducing their syntactic structures in new motion sequences.	algorithm;context-free grammar;context-free language;grammar induction;kinesiology;memory segmentation;terminal and nonterminal symbols;universal quantification	Jong Pil Park;Kang Hoon Lee;Jehee Lee	2011	Comput. Graph. Forum	10.1111/j.1467-8659.2011.01968.x	natural language processing;computer vision;speech recognition;computer science;theoretical computer science;motion estimation;context-free grammar;programming language;motion field;computer graphics;grammar-based code;algorithm	Vision	-15.858857003531165	-81.0541161263004	149883
bea1e028e025debd7a3e6660d6c746bfd918db64	robustness study of free-text speaker identification and verification	databases;filtering;speaker identification;operant conditioning;speech processing;free text speaker identification;filters;testing;equipment change;telephony;training data;cepstral analysis;robustness cepstral analysis databases testing telephony cepstrum cities and towns filters;long distance;voice verification systems;telephony learning artificial intelligence speech analysis and processing speech recognition;cepstrum;signal processing;signal processing techniques;dissertation;speech analysis and processing;speech recognition;cities and towns;robustness;learning artificial intelligence;systems integration;long distance telephone database;digital communications;equipment change free text speaker identification voice verification systems robustness signal processing techniques long distance telephone database training data;robust information processing	Usable free-text speaker identification and voice verification systems must exhibit robustness under varying operational conditions. The authors study the degree of robustness provided by various signal processing techniques by experimenting on a widely used long distance telephone database. This database consists of data recorded at two different sites, with data from one site much poorer in quality than that from the other. Further, the recording equipment had been inadvertently changed for the later half of the sessions, resulting in a significantly changed environment. The combination of techniques that provide consistent and significant improvements is identified. The present results surpass other published results on the same task. Specifically, in the task of identifying 16 speakers with training data from the recording prior to equipment change and testing on data from a set after the change (the most challenging condition), a correct identification rate of 87.5% with an average rank of 1.12 was obtained. >		Yu-Hung Kao	1993		10.1109/ICASSP.1993.319318	filter;training set;speech recognition;computer science;machine learning;cepstrum;signal processing;operant conditioning;pattern recognition;speech processing;software testing;telephony;robustness;system integration	Logic	-14.867467305045842	-92.36606188876267	150052
6c7fdf8812e3d8849bd3a0a58b831b8a13087464	acoustic-prosodic analysis of attitudinal expressions in german		This paper presents results from the prosodic analysis of short utterances of German produced with varying attitudinal expressions. It is based on the framework developed by Rilliard et al. eliciting 16 different kinds of social and/or propositional attitudes which place the subjects in various social interactions with a partner of inferior, equal or superior status, respectively as well as positive, neutral or negative, valence. Prosodic variations are analyzed in the framework of the Fujisaki model with respect to F0, as well as other prosodic features, such as duration, intensity and measures related to changes of voice quality. An analysis regarding the features that set apart two attitudes is presented. Expressive changes are discussed in light of previous results on USEnglish, and relative to universal codes proposed in the literature.	acoustic cryptanalysis;fujisaki model;interaction;regular expression;universal code (data compression)	Hansjörg Mixdorff;Angelika Hönemann;Albert Rilliard	2015			speech recognition;artificial intelligence;pattern recognition;computer science;expression (mathematics);german	NLP	-10.324707942243206	-82.18076578952856	150220
467d4b444e90972367631ebc2ade5738f081c48d	parsing mandarin compounds - a sentence game in the young generation			parsing;super robot monkey team hyperforce go!	Hsiu-Ying Liu;Cheng-Chung Kuo	2005			mandarin chinese;natural language processing;parsing;speech recognition;artificial intelligence;sentence;computer science	NLP	-15.926441264720648	-85.07231757248206	150246
1edf2a4365271fd8245b34ff155617bdd943a3e3	continuous pitch contour as an improvement feature for music information retrieval by humming/singing	query by humming;music information retrieval mir;query by sample;pitch tracking;music information retrieval;melodic matching;music query;query by humming singing;dynamic time warping	In this paper we present a method for smoothing the pitch sequence to remove the outliers caused by pitch tracking method and errors in sung/hummed voice. This approach is used for constructing a query by singing/humming system. After the pitch sequence is smoothed, the continuous pitch contour is calculated. Then, this feature can be used with Dynamic Time Warping (DTW) matching to compute the difference score between the each query and song. Experimental result of this method on TCS Corpus for query by Singing/Humming will be reported and discussed in detail in this paper.	contour line;information retrieval	Tri Nguyen Truong Duc;Minh Le Nhat;Ha Nguyen Duc Hoang;Quan Vu Hai	2008		10.1007/978-3-540-89197-0_111	speech recognition;computer science;machine learning;dynamic time warping	ML	-7.967522902303806	-93.64695644256977	150304
3622ab19cf886fb0739ffd346db66561ffe596d8	fast gmm-based voice conversion for text-to-speech synthesis systems	text to speech;functional testing;gaussian mixture model;complexity reduction	Voice conversion (VC) can be seen as a powerful technology for customizing Text-to-Speech (TTS) systems. This paper deals with the integration of a VC method based on Gaussian Mixture Model (GMM) in a TTS system. In this framework, an algorithm that enables complexity reduction of the VC processing is proposed. The main idea is to restrict the conversion function to the most representative components of the GMM for each frame and, if necessary, to store the component indices and their associated weights in the acoustic dictionary. This method is evaluated by comparison to a classical GMM-based transformation function. Tests show that both methods yield comparable results. Furthermore, additional experiments indicate that this new technique leads to a significant decrease of the computational load involved in the conversion process.	acoustic cryptanalysis;algorithm;data dictionary;experiment;google map maker;mixture model;netware file system;reduction (complexity);speech synthesis	Taoufik En-Najjary;Olivier Rosec;Thierry Chonavel	2004			artificial intelligence;reduction (complexity);voice activity detection;speech recognition;mixture model;pattern recognition;functional testing;computer science;speech synthesis	EDA	-10.943849139429721	-88.23681645702014	150409
478a7c0c1456786af04a95393c1684f2ba851e1f	feature sparsity analysis for i-vector based speaker verification	adaptive first order baum welch statistics analysis afsa;feature variability;i vector;speaker verification;total factor space	In recent years, the i-vector based framework has been proven to provide state-of-the-art performance in the speaker verification field. Each utterance is projected onto a total factor space and is represented by a low-dimensional i-vector. However, the degradation of performance in the i-vector space remains problematic and is commonly attributed to channel variability. Most techniques used for the channel compensation of the i-vectors, such as linear discriminant analysis (LDA) or probabilistic linear discriminant analysis (PLDA) aim to compensate for the variabilities caused by channel effects. However, in real-world applications, the duration of enrollment and test utterances by each user (speaker) are always very limited. In this paper, we demonstrate, from both analytical and experimental perspectives, that feature sparsity and imbalance widely exist in short utterances, in which case the conventional i-vector extraction algorithm, based on maximum likelihood estimation (MLE), may lead to over-fitting and decrease the performance of the speaker verification system, especially for short utterances. This prompted us to propose an improved i-vector extraction algorithm, which we term adaptive first-order Baum–Welch statistics analysis (AFSA). This new algorithm suppresses and compensates for the deviation from first-order Baum–Welch statistics caused by feature sparsity and imbalance. We reported results on the male telephone portion of the core trial condition (short2-short3) and other short time trial conditions (short210sec and 10sec-10sec) on NIST 2008 Speaker Recognition Evaluations (SREs) dataset. As measured both by Equal Error Rate (EER) and the minimum values of the NIST Detection Cost Function (minDCF), 10%–15% relative improvement is obtained compared to the baseline of traditional i-vector based system. © 2016 Elsevier B.V. All rights reserved.	baseline (configuration management);baum–welch algorithm;elegant degradation;enhanced entity–relationship model;first-order predicate;lempel–ziv–welch;linear discriminant analysis;loss function;overfitting;sparse matrix;spatial variability;speaker recognition;welch's method	Wei Li;Tianfan Fu;Hanxu You;Jie Zhu;Ning Chen	2016	Speech Communication	10.1016/j.specom.2016.02.008	speech recognition;computer science;machine learning;pattern recognition;statistics	AI	-17.042255115796777	-91.77529645720757	150434
1e3799e023dfe82b8f639c78e0a9e9ec548ddbcf	on the use of feature-space mllr adaptation for non-native speech recognition	word error rate;gaussian mixture;acoustic model adaptation non native speech recognition feature space maximum likelihood linear regression fmllr feature compensation;probability;feature compensation;average word error rate;gaussian processes;acoustic model adaptation;smoothing method;transformed feature vectors;probability density function;acoustics;acoustic modeling;viterbi decoding procedure;speech;feature space mllr adaptation;natural languages;feature space maximum likelihood linear regression fmllr;fmllr smoothing;korean spoken english continuous speech corpus;maximum likelihood estimation;feature space;viterbi decoding gaussian processes maximum likelihood estimation probability regression analysis smoothing methods speech recognition transforms;acoustic testing;feature vector;automatic speech recognition;smoothing methods;adaptation model;hidden markov models;maximum likelihood linear regression;viterbi algorithm;nonnative speech recognition;maximum likelihood decoding;adaptive method;viterbi decoder;transforms;feature space maximum likelihood linear regression adaptation;speech recognition;regression analysis;average word error rate feature space mllr adaptation nonnative speech recognition feature space maximum likelihood linear regression adaptation fmllr smoothing viterbi decoding procedure gaussian mixture probability density function observation probability density function transformed feature vectors korean spoken english continuous speech corpus;observation probability density function;non native speaker;viterbi decoding;gaussian mixture probability density function;non native speech recognition;maximum likelihood linear regression speech recognition smoothing methods automatic speech recognition probability density function viterbi algorithm maximum likelihood decoding adaptation model acoustic testing natural languages	In this paper, we address issues associated with a feature-space maximum likelihood linear regression (fMLLR) adaptation method applied to non-native speech recognition. In particular, fMLLR smoothing is proposed here to compensate for mismatches between adaptation and test data, caused by the various disfluencies of non-native speakers. The proposed fMLLR smoothing is performed with a Viterbi decoding procedure and implemented at two levels: a Gaussian mixture probability density function (mpdf) level and an observation probability density function (opdf) level. The mpdf-level smoothing is performed by comparing the pdf of each Gaussian mixture component of an original speech feature vector with that transformed by the fMLLR. On the other hand, the opdf-level smoothing compares the Gaussian mixture probabilities between the original and its fMLLR transformed feature vectors. It is shown from non-native automatic speech recognition experiments on a Korean-spoken English continuous speech corpus that an ASR system employing the proposed mpdf-level and opdf-level fMLLR smoothing methods can relatively reduce the average word error rate by 30.65% and 29.82%, respectively, when compared to a traditional fMLLR adaptation method.	experiment;feature vector;portable document format;smoothing;speech corpus;speech recognition;test data;word error rate	Yoo Rhee Oh;Hong Kook Kim	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495658	speech recognition;feature vector;computer science;machine learning;pattern recognition;mathematics;viterbi decoder;hidden markov model;statistics	Robotics	-18.64303422680758	-91.67688565155993	150458
0943169017c361f84c90bd43e31dbc8461121d50	speaking style effects in the production of disfluencies	ciencia;speaking styles;projetos;investigacao;publicacoes;lectures;disfluencies;prosody;iscte iul;article;dialogues	Abstract This work explores speaking style effects in the production of disfluencies. University lectures and map-task dialogues are analyzed in order to evaluate if the prosodic strategies used when uttering disfluencies vary across speaking styles. Our results show that the distribution of disfluency types is not arbitrary across lectures and dialogues. Moreover, although there is a statistically significant cross-style strategy of prosodic contrast marking (pitch and energy increases) between the region to repair and the repair of fluency, this strategy is displayed differently depending on the specific speech task. The overall patterns observed in the lectures, with regularities ascribed for speaker and disfluency types, do not hold with the same strength for the dialogues, due to underlying specificities of the communicative purposes. The tempo patterns found for both speech tasks also confirm their distinct behaviour, evidencing the more dynamic tempo characteristics of dialogues. In university lectures, prosodic cues are given to the listener both for the units inside disfluent regions and between these and the adjacent contexts. This suggests a stronger prosodic contrast marking of disfluency–fluency repair when compared to dialogues, as if teachers were monitoring the different regions – the introduction to a disfluency, the disfluency itself and the beginning of the repair – demarcating them in very contrastive ways.	biconnected component;item unique identification;limbo;maxima and minima;motion planning;text corpus;dialog	Helena Moniz;Fernando Batista;Ana Isabel Mata;Isabel Trancoso	2014	Speech Communication	10.1016/j.specom.2014.05.004	speech recognition;linguistics;prosody	NLP	-11.82909379492227	-80.93076956601953	150550
1b6add50e6be8d4f21e38cca9a154321cad3a4e0	looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation		"""We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to """"focus"""" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest)."""		Ariel Ephrat;Inbar Mosseri;Oran Lang;Tali Dekel;Kevin Wilson;Avinatan Hassidim;William T. Freeman;Michael Rubinstein	2018	ACM Trans. Graph.	10.1145/3197517.3201357	computer vision;artificial intelligence;deep learning;source separation;computer science;speech enhancement	Graphics	-11.16180102124393	-89.56606705665256	150561
e9d66bc5711fbe9f90c08161b653b0bd91b9f0f3	a framework for integrating heterogeneous sporadic knowledge sources into automatic speech recognition		Heterogeneous knowledge sources that model speech only at certain time frames are difficult to incorporate into speech recognition, given standard multimodal fusion techniques. In this work, we present a new framework for the integration of this sporadic knowledge into standard HMM-based ASR. In a first step, each knowledge source is mapped onto a logarithmic score by using a sigmoid transfer function. Theses scores are then combined with the standard acoustic models by weighted linear combination. Speech recognition experiments with broad phonetic knowledge sources on a broadcast news transcription task show improved recognition results, given knowledge that provides complementary information for the ASR system.	acoustic cryptanalysis;acoustic model;automated system recovery;experiment;hidden markov model;multimodal interaction;oracle fusion architecture;sigmoid function;speech recognition;transcription (software);transfer function	Stefan Ziegler;Guillaume Gravier	2013			natural language processing;speech recognition;computer science;communication	NLP	-16.089878476037548	-86.60485346821358	150605
a69fc3d50b588984067028b27e06380323fe3f31	ultrasound study of moroccan arabic labiovelarization	ultrasound;labial;velarization;arabic;velar;labialization	In this survey, we analyze acoustic and ultrasound data from two subjects in order to characterize a secondary articulation generally analyzed as labialization in Moroccan Arabic. Our results show that the so-called labialized consonants are rather labiovelarized. They also show that the vowel [a] adjacent to the labiovelarized consonants is velarized.	acoustic cryptanalysis;biconnected component	Chakir Zeroual;John H. Esling;Phil Hoole;Rachid Ridouane	2011			speech recognition;acoustics;audiology	HCI	-8.481613800543704	-84.30801462944082	150772
5c10676a9b17f5c566a6ef316f3280abed7ca411	challenges in and solutions to deep learning network acoustic modeling in speech recognition products at microsoft		Deep learning (DL) network acoustic modeling has been widely deployed in real-world speech recognition products and services that benefit millions of users. In addition to the general modeling research that academics work on, there are special constraints and challenges that the industry has to face, e.g., the run-time constraint on system deployment, robustness to variations such as the acoustic environment, accents, lack of manual transcription, etc. For large-scale automatic speech recognition applications, this chapter briefly describes selected developments and investigations at Microsoft to make deep learning networks more effective in a production environment, including reducing run-time cost with singular-value-decomposition-based training, improving the accuracy of small-size deep neural networks (DNNs) with teacher–student training, the use of a small amount of parameters for speaker adaptation of acoustic models, improving the robustness to the acoustic environment with variable-component DNN modeling, improving the robustness to accent/dialect with model adaptation and accent-dependent modeling, introducing time and frequency invariance with time–frequency long short-term memory recurrent neural networks, exploring the generalization capability to unseen data with maximum margin sequence training, the use of unsupervised data to improve speech recognition accuracy, and increasing language capability by reusing speech-training material across languages. The outcome has enabled the deployment of DL acoustic models across Microsoft server and client product lines including Windows 10 desktop/laptop/phones, XBOX, and skype speech-to-speech translation.	acoustic cryptanalysis;deep learning;speech recognition	Yifan Gong;Yan Huang;Kshitiz Kumar;Jinyu Li;Chaojun Liu;Guoli Ye;Shi-Xiong Zhang;Yong Zhao;Rui Zhao	2017		10.1007/978-3-319-64680-0_19	software deployment;robustness (computer science);laptop;reuse;deep learning;system deployment;artificial neural network;speech recognition;machine learning;recurrent neural network;computer science;artificial intelligence	ECom	-18.028775802841473	-89.21518008190246	151127
a3eacfb9cd9b9332b13750615b4ddd73c4998af7	dynamic soft sensor based on impulse response template and deep neural network for industrial processes			deep learning	Lei Liu;Kangcheng Wang;Chao Shang;Fan Yang;Wenxiang Lyu;Dexian Huang	2018		10.3233/978-1-61499-927-0-593		Robotics	-14.361077080304115	-87.083061358098	151168
94c531144b7d392030b07f1a0f1faee2e309f3c2	factor analyzed gaussian mixture models for speaker identification.	speaker identification;gaussian mixture model	In this paper, the statistical method of Factor Analysis(FA) is studied on Gaussian Mixture Model(GMM) based speaker identification(SI) system to model the data covariance which is usually neglected due to the training data sparseness. Because the variance of GMM can represents speaker variability, it is very important in SI systems. By FA modeled the data covariance, a relative gain of 39.6% over GMM baseline can be seen at the same amount of training data. Parameter tying is important when data is sparse and is helpful to balance precision and generalization of models. Various tying strategies are studied in this paper too. With the best result, a relative gain of 48.9% gain can be seen. We also present some interpretations of the factors tentatively.	baseline (configuration management);google map maker;mixture model;neural coding;sparse matrix;spatial variability;speaker recognition	Peng Ding;Yang Liu;Bo Xu	2002			speech recognition;computer science;pattern recognition;mixture model	ML	-16.778619180067675	-91.51109847671631	151378
7523bb1c229eb2b63817f58ae398eca39f87a59d	simulation of the violin section sound based on the analysis of orchestra performance	microphones;instruments;instruments acoustics vectors microphones amplitude modulation timing time frequency analysis;vocoders anechoic chambers acoustic microphones musical instruments time frequency analysis;acoustics;amplitude modulation;strings section simulation;musical instruments;vectors;anechoic chambers acoustic;vocoders;chorus effect strings section simulation;chorus effect;time frequency analysis;chorus effect violin section sound orchestra performance analysis string instrument section symphony orchestra anechoic recording instrument sound temporal differences violin recording phase vocoder time frequency domain amplitude modulation pitch shift duplicated signals contact microphones onset detection listening test;timing	A study on simulating a string instrument section of a symphony orchestra is presented. An anechoic recording of a single violin is used as the actual instrument sound, and the section sound is created by introducing temporal differences to the violin recording with a phase vocoder in the time-frequency domain. In addition, amplitude modulation and pitch shift are applied to the duplicated signals. The applied temporal differences follow a distribution that is obtained from a real violin section, recorded with contact microphones and analyzed with onset detection. Results of a listening test show that the proposed method provides a better simulation of a large instrument section than the traditional chorus effect.	chorusos;microphone;modulation;onset (audio);phase vocoder;pitch shift;simulation;symphony;time–frequency analysis	Jukka Pätynen;Sakari Tervo;Tapio Lokki	2011	2011 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)	10.1109/ASPAA.2011.6082305	chorus effect;speech recognition;time–frequency analysis;acoustics;telecommunications;amplitude modulation;engineering;physics	Vision	-9.059852954899393	-83.903061516704	151642
c45d1b34a399e356dd449c3f2ceb2381f91bf19c	the role of vowel duration cue in l1: effects on l2 learners' identification of phonological vowel length in japanese		This study investigated the effects of learners’ L1 on their identification of Japanese phonological vowel length at three speaking rates. Native listeners of Japanese (NJ) and learners of Japanese with L1 backgrounds in Finnish (NFin), American English (NE), Russian (NR), and French (NFr) participated in the study. The results showed that the proportion of “long” responses increased as a function of vowel duration for all groups. Meanwhile, only NJ and NFin shifted the category boundary location according to speaking rate, which occurred at a shorter duration for a faster speaking rate. In addition, NFr’s boundary width was significantly greater than NJ’s and NFin’s. These results suggest that L2 learners can access vowel duration as a cue regardless of their L1, but only those whose L1 uses vowel duration as a cue for phonological vowel length can shift the category boundary appropriately and identify L2 vowel length sharply as NJ do.	norm (social);numerical recipes	Izumi Takiguchi	2015			american english;vowel;psychology;vowel length;mid vowel;linguistics;relative articulation	NLP	-10.891135970154147	-81.6140006975352	151668
bc1b36ddac3304d2f5ec50a2d27224d9ed3042ab	towards a new speech event detection approach for landmark-based speech recognition	speech processing;speech recognition hidden markov models signal classification speech processing;phonetic landmark driven hmm based speech recognition improvement speech unit event detection speech unit classification event based speech recognition systems time variable speech unit model fixed dimensional observation vector boosted decision stump committee training labeled training data time frame maximum classification score phoneme classification accuracy improvement;speech hidden markov models speech recognition vectors training boosting accuracy;hidden markov models;signal classification;speech recognition;landmark driven asr speech event detection	In this work, we present a new approach for the classification and detection of speech units for the use in landmark or event-based speech recognition systems. We use segmentation to model any time-variable speech unit by a fixed-dimensional observation vector, in order to train a committee of boosted decision stumps on labeled training data. Given an unknown speech signal, the presence of a desired speech unit is estimated by searching for each time frame the corresponding segment, that provides the maximum classification score. This approach improves the accuracy of a phoneme classification task by 1.7%, compared to classification using HMMs. Applying this approach to the detection of broad phonetic landmarks inside a landmark-driven HMM-based speech recognizer significantly improves speech recognition.	finite-state machine;hidden markov model;speech recognition	Stefan Ziegler;Bogdan Ludusan;Guillaume Gravier	2012	2012 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2012.6424247	voice activity detection;natural language processing;sequence labeling;speaker recognition;audio mining;linear predictive coding;speech recognition;computer science;pattern recognition;speech processing;acoustic model;hidden markov model	NLP	-17.132321966564987	-89.26016667044978	151736
e4eb80b4c9c9e9a9e28cacce3d3f032b8aeb5d76	detection of acoustic patterns by stochastic matched filtering	filtering;audio signal processing;audio sequence;robot audition;acoustic pattern stochastic matched filtering pattern detection pattern isolation audio sequence robot audition voice activity detection;speech;stochastic matched filtering;acoustic signal processing;human robot interaction;acoustic pattern;pattern detection;estimation;stochastic processes;robots;likelihood ratio test;pattern isolation;speech recognition;robots covariance matrix signal to noise ratio estimation speech filtering;stochastic processes acoustic signal processing audio signal processing filtering theory human robot interaction speech recognition;voice activity detection;matched filter;signal to noise ratio;filtering theory;covariance matrix	The detection of a pattern in an audio sequence is considered. An approach relying on the Stochastic Matched Filtering theory is proposed. It consists in first defining offline a basis from the statistics of the pattern and of the noise, then in isolating the pattern by means of a likelihood ratio test involving the online decomposition of the audio sequence on this basis. A simulated case study is proposed, which provides some guidelines to the tuning of the algorithm. Then, experimental results concerning the application of the method to voice activity detection are presented.	acoustic cryptanalysis;algorithm;experiment;matched filter;online and offline;performance;signal processing;signal-to-noise ratio;stationary process;voice activity detection;window function	Julien Bonnal;Patrick Danès;Marc Renaud	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5653257	voice activity detection;filter;human–robot interaction;robot;stochastic process;covariance matrix;estimation;speech recognition;acoustics;audio signal processing;likelihood-ratio test;computer science;speech;matched filter;signal-to-noise ratio;statistics	Robotics	-11.936936887552946	-93.03565004532352	152093
227383af030109c382ebebfb33bcec4b96b4568b	sparse representation over learned and discriminatively learned dictionaries for speaker verification	kernel;nist;gmm mean supervector speaker verification learned dictionary sparse representation;measurement;training;exemplar dictionary based src approach discriminative learned dictionary sparse representation classification sv method gmm mean shifted supervectors speaker verification methods session channel variability compensation nist 2003 sre dataset i vector based approach;speaker verification;speaker recognition;vectors;gmm mean supervector;dictionaries;learned dictionary;sparse representation;dictionaries vectors nist kernel covariance matrix training measurement;vectors dictionaries speaker recognition;covariance matrix	In this work, a speaker verification (SV) method is proposed employing the sparse representation of GMM mean shifted supervectors over learned and discriminatively learned dictionaries. This work is motivated by recently proposed speaker verification methods employing the sparse representation classification (SRC) over exemplar dictionaries created from either GMM mean shifted supervectors or i-vectors. The proposed approach with discriminatively learned dictionary results in an equal error rate of 1.53 % which is found to be better than those of similar complexity SV systems developed using the i-vector based approach and the exemplar based SRC approaches with session/channel variability compensation on NIST 2003 SRE dataset.	dictionary;discriminative model;google map maker;sample rate conversion;sparse approximation;sparse matrix;spatial variability;speaker recognition;systemverilog	C. B. HarisB.;Rohit Sinha	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288989	speaker recognition;covariance matrix;kernel;speech recognition;nist;computer science;machine learning;pattern recognition;sparse approximation;mathematics;measurement;statistics	Robotics	-16.970282672229864	-91.87522253280014	152268
52702e14d7284670c08cd1268e76bab6d1e139f4	recognizing child's emotional state in problem-solving child-machine interactions	decision tree;information sources;spoken dialog system;spoken dialog systems;user adaptation;emotion recognition;performance improvement;child computer interaction;information fusion;support vector machine;problem solving	The need for automatic recognition of a speaker's emotion within a spoken dialog system framework has received increased attention with demand for computer interfaces that provide natural and user-adaptive spoken interaction. This paper addresses the problem of automatically recognizing a child's emotional state using information obtained from audio and video signals. The study is based on a multimodal data corpus consisting of spontaneous conversations between a child and a computer agent. Four different techniques-- k-nearest neighborhood (k-NN) classifier, decision tree, linear discriminant classifier (LDC), and support vector machine classifier (SVC)-- were employed for classifying utterances into 2 emotion classes, negative and non-negative, for both acoustic and visual information. Experimental results show that, overall, combining visual information with acoustic information leads to performance improvements in emotion recognition. We obtained the best results when information sources were combined at feature level. Specifically, results showed that the addition of visual information to acoustic information yields relative improvements in emotion recognition of 3.8% with both LDC and SVC classifiers for information fusion at the feature level over that of using only acoustic information.	acoustic cryptanalysis;decision tree;dialog system;emotion recognition;k-nearest neighbors algorithm;linear discriminant analysis;linguistic data consortium;multimodal interaction;problem solving;software agent;spoken dialog systems;spontaneous order;support vector machine	Serdar Yildirim;Shrikanth (Shri) Narayanan	2009		10.1145/1640377.1640391	natural language processing;speech recognition;computer science;communication	AI	-16.81000273430332	-87.09405432664484	152523
f6133d84cc59bb7bcf4b147741f42cb89c87ce31	data selection with kurtosis and nasality features for speaker recognition		We propose new data selection approaches based on speaker discriminability features, including kurtosis and a set of nasality features which exploit spectral properties of nasal speech sounds. Data selected based on the speaker discriminability features are used to implement end-to-end speaker recognition systems, which produce significant improvements when combined with the baseline system (which uses the speech-only data regions determined by a speech/non-speech detector), where the optimal combination of systems produces roughly a 24% improvement over the baseline. Results suggest that focusing the modeling power on data regions selected via the kurtosis and nasality speaker discriminability features, part of which are often discarded in the speech/non-speech detection process, can improvement speaker recognition.	baseline (configuration management);end-to-end principle;speaker recognition	Howard Lei;Nikki Mirghafori	2011			artificial intelligence;pattern recognition;speech recognition;nasality;speaker recognition;nasal speech;computer science;detector;kurtosis	NLP	-13.821860820404995	-90.65320947710373	152531
f99ddd64c9b0d8cb7729cbc0151a47b6b707797e	a comparison of training approaches for discriminative segmental models.		Segmental models such as segmental conditional random fields have had some recent success in lattice rescoring for speech recognition. They provide a flexible framework for incorporating a wide range of features across different levels of units, such as phones and words. However, such models have mainly been trained by maximizing conditional likelihood, which may not be the best proxy for the task loss of speech recognition. In addition, there has been little work on designing cost functions as surrogates for the word error rate. In this paper, we investigate various losses and introduce a new cost function for training segmental models. We compare lattice rescoring results for multiple tasks and also study the impact of several choices required when optimizing these losses.	conditional random field;loss function;speech recognition;surrogates;word error rate	Hao Tang;Kevin Gimpel;Karen Livescu	2014			discriminative model;pattern recognition;word error rate;artificial intelligence;computer science;conditional random field	ML	-18.4977410632962	-90.59093915005823	152636
28e2f2591c6439ffcdaf4e209ce223c3f7dc55e9	on medumba bilabial trills and vowels		The Medumba consonant inventory includes plain and prenasalized bilabial trills /ʙ, mʙ/, which contrast with bilabial stops. The trills occur most often before the central vowels /ʉ̜ ə/, while they are not attested before /u/. The central vowel /ʉ̜/ has a vowel posture that is particularly conducive to trilling of the lips: it does not form a circular opening, the corners of the mouth are drawn slightly apart, the lips are tensed, particularly at the corners of the mouth, and there is a narrow aperture between the lips. This suggests that it is close lip aperture of the following vowel— rather than rounding—that provides the most conducive environment for bilabial trill production.	poor posture;rounding	Kenneth S. Olson;Yohann Meynadier	2015			acoustics;vowel;consonant;geology	Robotics	-9.720620783381735	-81.53820920506702	152702
a78da7514faea8f6dfe09cc39a56cda180e23d16	bayesian changepoint detection for the automatic assessment of fluency and articulatory disorders	disfluency;articulation disorder;speech signal processing;speech pathology;changepoint detection	The accurate changepoint detection of different signal segments is a frequent challenge in a wide range of applications. With regard to speech utterances, the changepoints are related to significant spectral changes, mostly represented by the borders between two phonemes. The main aim of this study is to design a novel Bayesian autoregressive changepoint detector (BACD) and test its feasibility in the evaluation of fluency and articulatory disorders. The originality of the proposed method consists in its normalizing of a posteriori probability using Bayesian evidence and designing a recursive algorithm for reliable practice. For further evaluation of the BACD, we used data from (a) 118 people with various severity of stuttering to assess the extent of speech disfluency using a short reading passage, and (b) 24 patients with early Parkinson's disease and 22 healthy speakers for evaluation of articulation accuracy using fast syllable repetition. Subsequently, we designed two measures for each type of disorder. While speech disfluency has been related to greater distances between spectral changes, inaccurate dysarthric articulation has instead been associated with lower spectral changes. These findings have been confirmed by statistically significant differences, which were achieved in separating several degrees of disfluency and distinguishing healthy from parkinsonian speakers. In addition, a significant correlation was found between the automatic assessment of speech fluency and the judgment of human experts. In conclusion, the method proposed provides a cost-effective, easily applicable and freely available evaluation of speech disorders, as well as other areas requiring reliable techniques for changepoint detection. In a more modest scope, BACD may be used in diagnosis of disease severity, monitoring treatment, and support for therapist evaluation.		Roman Cmejla;Ján Rusz;Petr Bergl;Jan Vokral	2013	Speech Communication	10.1016/j.specom.2012.08.003	natural language processing;speech recognition;speech-language pathology	NLP	-6.359534975644132	-85.63358530080778	152854
839c457f82845d9984512ac37fd65af260c91e8b	predicting speech overlaps from speech tokens and co-occurring body behaviours in dyadic conversations	speech overlaps;multimodal corpora;machine learning;multimodal behaviours	This paper deals with speech overlaps in dyadic video record-ed spontaneous conversations. Speech overlaps are quite common in everyday conversations and it is therefore important to study their occurrences in different communicative situations and settings and to model them in applied communicative systems.  In the present work, we wanted to investigate the frequency and use of speech overlaps in a multimodally annotated corpus of first encounters. Speech overlaps were automatically tagged and a Bayesian Network learner was trained on the multimodal annotations in order to determine to which extent overlaps can be predicted so they can be dealt with in conversational devices and to investigate the relation between overlaps, speech tokens and co-occurring body behaviours. The annotations comprise shape and functions of head movements, facial expressions and body postures.  23% of the speech tokens and 90% of the spoken contributions of the first encounters are overlapping. The best classification results were obtained training the classifier on multimodal behaviours (speech and co-occurring head movements, facial expressions and body postures) which surround-ed the overlaps. Training the classifier on all speech tokens also gave good results while adding the shape of co-occurring body behaviours to them did not affect the results. Thus, the behaviours of the conversation participants does not change when there is a speech overlap. This could indicate that most of the overlaps in the first encounters are non competitive.	bayesian network;dyadic transformation;multimodal interaction;speech synthesis;spontaneous order	Costanza Navarretta	2013		10.1145/2522848.2522893	natural language processing;speech recognition;computer science;machine learning	NLP	-12.454225279329988	-83.34568136613517	152858
e42153892d264810903e2e8ba67e595e48dd5a51	subjective optimization	human judgment;proposed framework;subjective optimization;human conversation data;simulated naturalness criterion;training objective;common approach;gesture generator;actual goal;training data	An effective way to build a gesture generator is to apply machine learning algorithms to derive a model. In building such a gesture generator, a common approach involves collecting a set of human conversation data and training the model to fit the data. However, after training the gesture generator, what we are looking for is whether the generated gestures are natural instead of whether the generated gestures actually fit the training data. Thus, there is a gap between the training objective and the actual goal of the gesture generator. In this work we propose an approach that use human judgment of naturalness to optimize gesture generators. We take an important step towards our goal by performing a numerical experiment to assess the optimality of the proposed framework, and the experimental results show that the framework can effectively improve the generated gestures based on the simulated naturalness criterion.	algorithm;experiment;gesture recognition;gradient;machine learning;mathematical optimization;numerical analysis;semantic prosody;simulation;test set	Chung-Cheng Chiu;Stacy Marsella	2012		10.1007/978-3-642-33197-8_21	simulation;speech recognition;computer science;artificial intelligence;communication	AI	-16.082246656481814	-83.48895249297867	152908
33fc2a0f6d91926868c1e1f78e44fa92c51d09e4	learning soft mask with dnn and dnn-svm for multi-speaker doa estimation using an acoustic vector sensor		Using an acoustic vector sensor (AVS), an efficient method has been presented recently for direction-of-arrival (DOA) estimation of multiple speech sources via the clustering of the inter-sensor data ratio (AVS-ISDR). Through extensive experiments on simulated and recorded data, we observed that the performance of the AVS-DOA method is largely dependent on the reliable extraction of the target speech dominated time-frequency points (TD-TFPs) which, however, may be degraded with the increase in the level of additive noise and room reverberation in the background. In this paper, inspired by the great success of deep learning in speech recognition, we design two new soft mask learners, namely deep neural network (DNN) and DNN cascaded with a support vector machine (DNN-SVM), for multi-source DOA estimation, where a novel feature, namely, the tandem local spectrogram block (TLSB) is used as the input to the system. Using our proposed soft mask learners, the TD-TFPs can be accurately extracted under different noisy and reverberant conditions. Additionally, the generated soft masks can be used to calculate the weighted centers of the ISDR-clusters for better DOA estimation as compared with the original center used in our previously proposed AVS-ISDR. Extensive experiments on simulated and recorded data have been presented to show the improved performance of our proposed methods over two baseline AVSDOA methods in presence of noise and reverberation.	acoustic cryptanalysis;additive white gaussian noise;algorithm;artificial neural network;baseline (configuration management);cluster analysis;computational complexity theory;deep learning;direction of arrival;expectation propagation;experiment;linear separability;multi-source;scalability;sensor;spectrogram;speech recognition;support vector machine;utility functions on indivisible goods	Disong Wang;Yuexian Zou;Wenwu Wang	2018	J. Franklin Institute	10.1016/j.jfranklin.2017.05.002	speech recognition;engineering;pattern recognition	ML	-14.981133564857496	-90.8673851091907	152916
f181803e1eb9f739ed832e2befdcbf0435e647fe	dependence and independence in automatic speech recognition and synthesis	automatic speech recognition;speech recognition;acoustic patterns;article	When automatically recognising or synthesising speech by computer, we are forced to make a number of assumptions of statistical independence in order to make certain problems tractable. This paper gives a few examples of how phonetic knowledge is already usefully informing these decisions about independence, and a few examples of where it isn’t, yet. Temporal integration – how information from a region of speech is related, and is gathered together during perception – is an important aspect of this.	cobham's thesis;high-level programming language;intelligibility (philosophy);speech recognition	Simon King	2003	J. Phonetics	10.1016/j.wocn.2003.09.002	natural language processing;speech technology;speaker recognition;speech recognition;intelligent character recognition;pattern recognition;viseme;speech processing;acoustic model	AI	-13.467768009318636	-83.92179206414787	153056
fc37baaad1dc1329ec8f03fd8a1df66b10a4ec7d	"""articulatory features for """"meeting"""" speech recognition"""		“Meeting” speech, for example from the RT-04S task, contains a mixture of different speaking styles that leads to word error rates higher than 25% even when close-talking microphones are being used. The problem is even more serious, as word error rates are particularly high when speakers use a clear speaking mode, for example because they want to stress an important point. Previous work showed that an approach that combines standard phonebased acoustic models with models detecting the presence or absence of “Articulatory Features” such as “Rounded” or “Voiced” can improve ASR performance particularly for these cases. This paper presents a discriminative approach to automatically computing from training or adaptation data the feature stream weights needed for the above approach, therefore presenting a framework for integrating articulatory features into existing automatic speech recognition systems. We find a 7% relative improvements on top of our best RT-04S system using discriminative adaptation.	acoustic cryptanalysis;microphone;sensor;speech recognition	Florian Metze	2006			discriminative model;artificial intelligence;speech recognition;pattern recognition;computer science;speech segmentation	NLP	-15.214450048391996	-89.87687878350307	153161
7f7740a8a3a6bb0b7332b4ad4348ed15a431f41b	on data-derived temporal processing in speech feature extraction	au tomatic speech recognition;cepstral mean subtraction;temporal filtering;feature extraction;word recognition;temporal processing	Temporal processing and filtering in speech feature extraction are commonly used to aid in performance and robustness in automatic speech recognition. Among the techniques successfully employed are RASTA filtering, delta calculation, and cepstral mean subtraction. The work here explores the use of temporal filter design using LDA to further enhance performance using a few preprocessing configurations. In addition to RASTA filtering, we apply the filters to modulation-spectral features and cepstra while making sure that the assumptions of LDA are observed. We additionally test the use of filters that have been trained in different reverberation conditions, noting from previous work that the presence of reverberation alters the preferred frequency range of the derived filters. Our tests indicate a consistent advantage in phone classification. Word recognition tests, in contrast, reveal that the LDA filters often do not improve upon the existing filters previously used. They can also be made less effectual by allowing contextual frames to a trained probability estimator.	cepstrum;feature extraction;filter design;frequency band;linear discriminant analysis;local-density approximation;modulation;preprocessor;rasta filtering;speech recognition	Michael L. Shire;Barry Y. Chen	2000			speech recognition;feature extraction;word recognition;computer science;machine learning;pattern recognition;linguistics	ML	-12.724928992003848	-91.17199755289435	153478
6682afd58819ca6b3ddb4418776ee84b448aa04c	improvements in the speaker identification rate using feature-sets		In this paper we look at the parameterized feature-set that has been used in connected alpha-digit speech recognition and evaluate it on a speaker identification SID system. Compared to the popular mel-scaled featureset (MFCC) the parameterized feature-set gives over 21% improvement in identification rate on the NTIMIT database in some cases. On average it has a 14.0% improvement. This demonstrates how feature-sets can be used to improve the performance of speaker identification systems.	speaker recognition;speech recognition;timit	Daniel J. Mashao;N. Tinyiko Baloyi	2001			artificial intelligence;speech recognition;speaker diarisation;speaker recognition;pattern recognition;computer science	NLP	-14.032648418113087	-90.4773551447681	153596
3b4e734e36f16f95881669ec36581af2aaaffe3b	learning label embeddings for nearest-neighbor multi-class classification with an application to speech recognition	article	We consider the problem of using nearest neighbor methods to provide a conditional probability estimate, P (y|a), when the number of labels y is large and the labels share some underlying structure. We propose a method for learning label embeddings (similar to error-correcting output codes (ECO Cs)) to model the similarity between labels within a nearest neighbor framework . The learned ECOCs and nearest neighbor information are used to provide condit i al probability estimates. We apply these estimates to the problem of acoustic mo deling for speech recognition. We demonstrate significant improvements in te rms of word error rate (WER) on a lecture recognition task over a state-of-the-art b seline GMM model.	acoustic cryptanalysis;acoustic model;code;cognition;error detection and correction;finite-state machine;google map maker;k-nearest neighbors algorithm;linear algebra;probability amplitude;speech recognition;summit;word error rate	Natasha Singh-Miller;Michael Collins	2009			speech recognition;computer science;machine learning;pattern recognition	ML	-15.157198481447416	-88.359815440198	153685
e9de532b3a26918794208872e9423416c1c4ce57	from acoustic mismatch towards blind acoustic model selection in automatic speech recognition			acoustic cryptanalysis;acoustic model;model selection;speech recognition	Thomas Winkler	2012				NLP	-14.289581706241533	-87.69660079234393	153782
177fe2eb83c306c24c930de542d51ef874cd02b9	acoustic evidence of articulatory adjustments to sustain voicing during voiced stops		The present study seeks to provide acoustic evidence of articulatory adjustments to initiate and sustain voicing during stops. First, using aerodynamic and acoustic data, it examines variations in the amplitude of voicing in phraseinitial voiced stops in Spanish and English and relates these variations to articulatory adjustments to preserve a low oral pressure and voicing. Second, a correlation is sought between oral pressure and voicing amplitude during stop closure. The correlation is significant for all Spanish (4) and English (2) speakers; as oral pressure rises, voicing amplitude decreases. The study concludes that articulatory adjustments to keep a low intraoral pressure for voicing may be inferred from the time course of voicing amplitude during the stop closure. Significant differences between prevoiced stops in the two languages are found, with overall higher values for oral pressure and lower values for voicing amplitude in English than in Spanish.	acoustic cryptanalysis	Maria-Josep Solé	2015			voice;acoustics;psychology;voice-onset time;speech recognition	HCI	-10.553918437671221	-82.02390590409436	153852
39975b9e2a1d7c1d34253492099338c4545eab22	psychophysical evaluation of psola: natural versus synthetic speech		This paper presents the results of psychophysical experiments dealing with pitch-marker positioning within the Pitch Synchronous OverLap and Add (PSOLA) framework. Sustained natural vowels were PSOLAmodified in fundamental frequency. The experiments were aimed at determining the auditory sensitivity to (1) deterministic shifts of either all or single pitch markers within a sequence, and (2) random shifts of all pitch markers (“jitter”). As for deterministic shifts of all pitch markers, the results were in reasonable agreement with results obtained previously for synthetic formant signals. For deterministic shifts of single pitch markers, thresholds depended on position in the sequence. Detection thresholds for jittered shifts were comparable to thresholds for detecting jitter in pulse trains. The ranking of the thresholds for these three conditions indicated that the auditory system is more sensitive to dynamic (modulation) cues rather than to static (timbral) cues arising from shifts in pitch-marker positioning.	experiment;modulation;psola;sensor;speech synthesis;synthetic intelligence	Reinier Kortekaas;Armin Kohlrausch	1997			modulation;speech recognition;artificial intelligence;formant;jitter;fundamental frequency;psola;pattern recognition;pulse (signal processing);computer science	NLP	-9.563931803167021	-82.57557093349368	153865
b87719277606e1bafbf2150a5973d49983d82d23	automatic phonetic segmentation by using a spm-based approach for a mandarin singing voice corpus	statistical process control;code converters;phonetic segmentation;single point mooring;self phase modulation;score predictive model;security of data;linguistics	This paper proposes a score predictive model (SPM) based approach to integrate two segmentation results obtained by HMM and DTW for a Mandarin singing voice corpus. The SPM can predict the score of a boundary according to its corresponding 14 dimensional feature vector. In order to verify the performance of the proposed method, several experiments were performed. The experimental results demonstrate the feasibility of the proposed approach.	experiment;feature vector;hidden markov model;super paper mario;super robot monkey team hyperforce go!	Cheng-Yuan Lin;Jyh-Shing Roger Jang	2006			natural language processing;speech recognition;computer science;linguistics;statistical process control	Robotics	-13.012881379693411	-87.6743852853794	153870
d4e46fe968e7b781670259df7b104a81a7fc8e7c	a combined adaptive and decision tree based speech separation technique for telemedicine applications	decision tree;speech segmentation;gaussian mixture model;error rate;frame error rate	We present a novel technique for separation of doctor and patient’s speech in conversations over a telemedicine network. The mixed speech signals acquired at doctor’s site is first broken into single talkers’ speech segments and background by using thresholds of energy and duration. The speech segments are then identified as spoken by doctor or patient in two steps. In the first step, Gaussian mixture models (GMM) of doctor and patient are used, where the doctor’s model is obtained from his/her training speech, and the patient’s model is initialized by a general speaker model and then adapted by the patient’s speech. In the second step, a decision tree that uses contextual and confidence features is applied to refine the identification results. Preliminary experiments were performed on three data sets collected in telemedicine. Without adaptation and decision tree, error rates at the segment-level and frame-level were 25.44% and 16.53%, respectively. With adaptation, segment and frame error rates were reduced to 13.11% and 7.85%, and with decision tree, the error rates were further reduced to 10.48% and 6.73%, respectively.	decision tree;experiment;google map maker;ibm notes;mixture model	Yunxin Zhao;Xiao Zhang;Xiaodong He;Laura Schopp	2000			speech recognition;word error rate;computer science;machine learning;decision tree;pattern recognition;mixture model;speech segmentation	ML	-18.979175802298837	-89.35417432934712	153894
5e21b2a31c11ed03a358ba3e99894efc64e2d87b	improved cepstra minimum-mean-square-error noise reduction algorithm for robust speech recognition		In the era of deep learning, although beam-forming multi-channel signal processing is still very helpful, it was reported that single-channel robust front-ends usually cannot benefit deep learning models because the layer-by-layer structure of deep learning models provides a feature extraction strategy that automatically derives powerful noise-resistant features from primitive raw data for senone classification. In this study, we show that the single-channel robust front-end is still very beneficial to deep learning modelling as long as it is well designed. We improve a robust front-end, cepstra minimum mean square error (CMMSE), by using more reliable voice activity detector, refined prior SNR estimation, better gain smoothing and two-stage processing. This new front-end, improved CMMSE (ICMMSE), is evaluated on the standard Aurora 2 and Chime 3 tasks, and a 3400 hour Microsoft Cortana digital assistant task using Gaussian mixture models, feed-forward deep neural networks, and long short-term memory recurrent neural networks, respectively. It is shown that ICMMSE is superior regardless of the underlying acoustic models and the scale of evaluation tasks, with 25.46% relative WER reduction on Aurora 2, up to 11.98% relative WER reduction on Chime 3, and up to 11.01% relative WER reduction on Cortana digital assistant task, respectively.	acoustic cryptanalysis;algorithm;artificial neural network;aurora;cortana (halo);deep learning;feature extraction;front and back ends;long short-term memory;mdl chime;mean squared error;mixture model;noise reduction;personal digital assistant;recurrent neural network;signal processing;signal-to-noise ratio;smoothing;speech recognition;word error rate	Jinyu Li;Yan Huang;Yifan Gong	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953081	speech recognition;pattern recognition	Vision	-16.306224695446378	-89.9973037343065	154019
c9db46d25e227639e5c2618a58d6fa85c2494414	a comparison of three discriminant models for automatic speaker verification	databases;automatic speaker verification;speech processing;testing;data mining;systems engineering and theory;automatic speech recognition databases signal processing speech processing data mining testing pattern matching cepstral analysis australia systems engineering and theory;automatic speech recognition;cepstral analysis;pattern matching;signal processing;discriminative model;australia		discriminant;speaker recognition	Stefan Slomka;Pierre Castellano;Sridha Sridharan	1996		10.1109/ISSPA.1996.615747	natural language processing;speaker recognition;speaker diarisation;speech recognition;computer science;pattern matching;signal processing;pattern recognition;speech processing;software testing;discriminative model	NLP	-14.644986742359887	-88.6080380578229	154023
5284e02075f03a05faf4409d8b90353b2a267cf8	diagnosis of dysarthria subtype via spectral and waveform analysis		This study reports on the performance of a computerised digital signal processing system, known as the Computerised Frenchay Dysarthria Assessment (CFDA), which is designed to diagnose two sub-types of dysarthria – a family of speech disorders characterised by loss of control over the organs which facilitate speech production. This investigation explores the use of both spectral and waveform analysis to distinguish between the ataxic and mixed dysarthria subtypes by assessing articulatory competence in the execution of two speech-related tasks. It is demonstrated that waveform analysis of utterances representing the consonant /p/ can reliably measure a speaker’s lip seal competence; this combination of lip seal evaluation and voice quality measurement can then serve as a composite tool to detect certain pathological features characteristic of ataxic and mixed dysarthria respectively. To validate this hypothesis, this study compares the assessment accuracy of the CFDA application with that of a panel of expert clinicians when evaluating a series of speech samples from a selection of individuals, some of whom were previously diagnosed as suffering from either ataxic or mixed dysarthria. The CFDA’s diagnostic output from this data evaluation exercise produced an overall correlation of 0.91 with those of the expert clinicians. This close correlation reinforces the validity of the objective voice quality evaluation procedures developed during the course of this study.	acoustic cryptanalysis;algorithm;biological anthropology;data visualization;digital signal processing;gradient;spectral method;speech technology;waveform	James Carmichael	2014	Comput. Syst. Sci. Eng.		speech recognition	HCI	-6.042592110784697	-85.36094377109528	154049
85c7e4233d24aeae77c014ddc63de6f1369dd6d4	large vocabulary concatenative resynthesis		Traditional speech enhancement systems reduce noise by modifying the noisy signal, which suffer from two problems: undersuppression of noise and over-suppression of speech. As an alternative, in this paper, we use the recently introduced concatenative resynthesis approach where we replace the noisy speech with its clean resynthesis. The output of such a system can produce speech that is both noise-free and high quality. This paper generalizes our previous small-vocabulary system to large vocabulary. To do so, we employ efficient decoding techniques using fast approximate nearest neighbor (ANN) algorithms. Firstly, we apply ANN techniques on the original small vocabulary task and get 5× speedup. We then apply the techniques to the construction of a large vocabulary concatenative resynthesis system and scale the system up to 12× larger dictionary. We perform listening tests with five participants to measure subjective quality and intelligibility of the output speech.	approximation algorithm;concatenative programming language;decoding methods;dictionary;display resolution;intelligibility (philosophy);regular expression;speech enhancement;speedup;vocabulary;zero suppression	Soumi Maiti;Joey Ching;Michael I. Mandel	2018		10.21437/Interspeech.2018-2383	speech recognition;vocabulary;computer science	NLP	-13.77204389275745	-88.87192237365996	154099
4a644eba7fce60f998afe551afdf73e0b5b4736d	development of material automatic generation system based on the analysis of phonemic errors in english vocabulary listening	listening learning;data mining;call system;data analysis	With analyzing the learning log-data, personalized learning patterns of different students could be detected, and the personalized materials could be automatically created and offered to learners for guiding their specific learning processes and solving weak points. In this paper, we propose a new approach to develop a CALL system including an error-detecting algorithm and a material-creating module. And in our approach, we pay much of our attention on the detection of phonemic errors. The system can detect the phonemic errors of Japanese learners' in English vocabulary listening by analyzing the relative learning log data, and automatically create multiple-choice question materials to help students take practices to enhance perception on the phonemes that they distinguish difficultly.	vocabulary	Yaheng Zou;Harumi Kashiwagi;Kazuhiro Ohtsuki;Min Kang	2013		10.1007/978-3-642-41175-5_35	natural language processing;speech recognition;computer science;data analysis	NLP	-18.0708048377083	-82.13811134332097	154223
7d1e309a628636f423d306455099c449b504a552	the truth and nothing but the truth: multimodal analysis for deception detection	support vector machines;acoustics;visualization;face recognition;hidden markov models;feature extraction;videos	We propose a data-driven method for automatic deception detection in real-life trial data using visual and verbal cues. Using OpenFace with facial action unit recognition, we analyze the movement of facial features of the witness when posed with questions and the acoustic patterns using OpenSmile. We then perform a lexical analysis on the spoken words, emphasizing the use of pauses and utterance breaks, feeding that to a Support Vector Machine to test deceit or truth prediction. We then try out a method to incorporate utterance-based fusion of visual and lexical analysis, using string based matching.	acoustic cryptanalysis;lexical analysis;multimodal interaction;real life;support vector machine;the witness	Mimansa Jaiswal;Sairam Tabibu;Rajiv Bajpai	2016	2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)	10.1109/ICDMW.2016.0137	facial recognition system;support vector machine;speech recognition;visualization;feature extraction;computer science;machine learning;pattern recognition;hidden markov model	Robotics	-5.758283916746732	-88.95084146971946	154296
10f675d782e6122c3a9bdcd04880cc78b480fbcf	non-intrusive speech quality estimation as combination of estimates using multiple time-scale auditory features		Abstract The human auditory system is modeled by different auditory models representing the distribution of speech sound energy in different channels across the cochlea using filter-banks of different bandwidths. In previous algorithms of non-intrusive speech quality evaluation, auditory features are determined using these auditory models on per frame basis and then averaged over the entire speech utterance. In these approaches, the effect of impulsive noise and other non-stationary noise effects get averaged over the utterance. To include the variations in the features of speech over time in the speech utterance, a multiple time-scale features approach has been proposed as the speech features vary from frame to frame that accounts for variation of noise characteristics over the speech utterance and thus its affect on quality mapping. In this work, non-intrusive speech quality evaluation has been done as an optimal linear combination of quality mapping called objective mean opinion score (MOS), computed using multiple time-scale estimates of features. The objective MOS of each of the multiple time-scale estimates (the combination of multiple active speeches) are obtained using a probabilistic approach. The overall objective MOS of the speech utterance is computed by taking the optimal linear combination of the estimated objective MOS using multiple time-scale estimates of features, where the optimality is based on the minimum mean square error (MMSE) criterion or correlation maximization criterion. The results are given in terms of Pearsonu0027s correlation coefficient and root mean square error (RMSE) between the subjective MOS and the estimated overall objective MOS for three different standard databases. The results have been compared with a single time-scale features approach, the ITU-T Recommendation P.563 and recent algorithms.		Rajesh Kumar Dubey;Arun Kumar	2017	Digital Signal Processing	10.1016/j.dsp.2017.07.020	mean opinion score;artificial intelligence;pattern recognition;linear combination;probabilistic logic;maximization;minimum mean square error;utterance;correlation coefficient;speech recognition;computer science;mean squared error	NLP	-13.131541002341546	-93.41471216846828	154298
211123b3a290c2de89c2f88701503b51f382f05a	from timbre modulation method to research the relation between electronic music and chinese musical tradition			modulation	Censong Leng	1999			speech recognition;art;acoustics;music psychology;music;music theory;communication;musical composition	HCI	-8.261851137318455	-84.7565161769338	154523
6233df89351c92ba1db46a333af0648262b69b4f	the status of variable phonetic output in early speech		This paper presents an analysis of variable outputs in the speech of English-, Spanishand Catalanspeaking 2 year-olds, to evaluate the evidence for multiple targets. Temporal analysis shows syllable duration is determined by actual segmental composition, compatible with either multiple targets or a process of categorical deletion. However, spectral analysis suggests deletions may be ‘gradient’, with the possibility of a single target subject to variable implementation. This mismatch between (pervasively categorical) temporal and (optionally gradient) articulatory behaviour suggests variable output is not due to a multipleentry lexicon as such, but to a shifting association between articulation and timing tiers, possibly as a result of competing structural templates.	biconnected component;gradient;lexicon;spectrum analyzer;syllable	Elinor Payne;Eftychia Eftychiou;Brechtje Post;Lluïsa Astruc;Pilar Prieto;María Vanrell	2011			natural language processing;speech recognition;mathematics;communication	AI	-10.954581394709951	-80.81039866116484	154816
3ef19a40806ff5b23fe8b067d988eb9e93c9a6dc	audio-cut detection and audio-segment classification using fuzzy c-means clustering	background noise;audio segmentation;audio signal processing;fuzzy number;noise background audio cut detection audio segment classification fuzzy c means clustering audio signal boundaries audio effects fade in fade out audio signal segmentation silence speech music background;audio effects;speech;audio signal segmentation;transform coding;speech enhancement;fuzzy set theory;noise background;fade out;silence;multiple signal classification;speech enhancement multiple signal classification transform coding digital audio players background noise material storage signal processing ip networks bandwidth digital recording;audio segment classification;digital recording;audio cut detection;music background;signal processing;material storage;signal classification;fuzzy set theory audio signal processing signal classification;bandwidth;ip networks;audio signal boundaries;fade in;digital audio players;fuzzy c means clustering	This paper proposes an audio-cut detection and audio-segment classification method using fuzzy c-means clustering. In the proposed method, the boundaries between two different audio signals, which are called audio-cuts, can be detected by fuzzy c-means clustering. In the fuzzy c-means clustering, the fuzzy number represents the possibility that the audio-cut exists. Therefore, according to the possibility, qualified candidates for audio-cuts can be obtained even if audio effects such as fade-in, fade-out, etc. are included in the audio signal. The audio signal is segmented at the detected audio-cuts, and these segments are classified into the following five classes: silence, music, speech, speech with music background, and speech with noise background. This classification simultaneously deletes wrongly detected audio-cuts. Consequently, we can obtain accurate audio-cuts and classification results.	cluster analysis;fuzzy number;image noise;shot transition detection	Naoki Nitanda;Miki Haseyama;Hideo Kitajima	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326829	audio mining;transform coding;speech recognition;fuzzy clustering;audio signal processing;computer science;speech;fuzzy number;multiple signal classification;signal processing;background noise;fuzzy set;bandwidth	Robotics	-7.545052470766649	-90.14043780767238	154828
56279bc66236515f67fd466185e1c72272716a3d	omg emotion challenge - excouple team		The proposed model is only for the audio module. All videos in the OMG Emotion Dataset are converted to WAV files. The proposed model makes use of semi-supervised learning for the emotion recognition. A GAN is trained with unsupervised learning, with another database (IEMOCAP), and part of the GAN structure (part of the autoencoder) will be used for the audio representation. The audio spectrogram will be extracted in 1-second windows of 16khz frequency, and this will serve as input to the model of audio representation trained with another database in an unsupervised way. This audio representation will serve as input to a convolutional network and a Dense layer with 'tanh' activation that performs the prediction of Arousal and Valence values. For joining the 1-second pieces of audio, the median of the predicted values of a given utterance will be taken.	autoencoder;emotion recognition;microsoft windows;sms language;semi-supervised learning;semiconductor industry;spectrogram;supervised learning;unsupervised learning	Ingryd Pereira;Diego Santos	2018	CoRR		computer science;autoencoder;speech recognition;unsupervised learning;emotion recognition;spectrogram;utterance	AI	-16.88491949517487	-87.23792230673828	154985
1851d801a9aeab61b487a71687bcaff3a45318ea	eesen: end-to-end speech recognition using deep rnn models and wfst-based decoding	end to end asr recurrent neural network connectionist temporal classification;word error rate eesen framework end to end speech recognition deep rnn model recurrent neural networks wfst based decoding ctc objective function connectionist temporal classification weighted finite state transducer wer;end to end asr;hidden markov models decoding acoustics training recurrent neural networks computational modeling speech;recurrent neural network;speech recognition pattern classification recurrent neural nets;connectionist temporal classification	The performance of automatic speech recognition (ASR) has improved tremendously due to the application of deep neural networks (DNNs). Despite this progress, building a new ASR system remains a challenging task, requiring various resources, multiple training stages and significant expertise. This paper presents our Eesen framework which drastically simplifies the existing pipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen involves learning a single recurrent neural network (RNN) predicting context-independent targets (phonemes or characters). To remove the need for pre-generated frame labels, we adopt the connectionist temporal classification (CTC) objective function to infer the alignments between speech and label sequences. A distinctive feature of Eesen is a generalized decoding approach based on weighted finite-state transducers (WFSTs), which enables the efficient incorporation of lexicons and language models into CTC decoding. Experiments show that compared with the standard hybrid DNN systems, Eesen achieves comparable word error rates (WERs), while at the same time speeding up decoding significantly.	artificial neural network;automated system recovery;connectionism;deep learning;language model;lexicon;loss function;optimization problem;random neural network;recurrent neural network;speech recognition;transducer	Yajie Miao;Mohammad Gowayyed;Florian Metze	2015	2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)	10.1109/ASRU.2015.7404790	speech recognition;computer science;recurrent neural network;machine learning;pattern recognition;time delay neural network	NLP	-17.89948640143048	-87.7461791856995	154988
963e22558e55a871426368fb1cdddca4c3380709	posterior lingual gestures and tongue shape in mangetti dune !xung clicks	rarefaction gestures;tongue root;ultrasound imaging;tongue dorsum;click consonants;tongue shape	Clicks differ from pulmonic stops in that, in addition to containing lingual gestures that shape the filtering mechanism of the vocal tract, they also contain lingual “rarefaction gestures” that form the source of the lingual ingressive airstream. The current study uses mid-sagittal lingual ultrasound imaging to investigate (1) overall tongue shape, (2) tongue dorsum and root positions, and (3) dynamic rarefaction gestures involving the tongue dorsum and root, in the four coronal click types recognized by the IPA. The study provides quantitative evidence that the four click types differ in overall tongue shape. Additionally, results show that the palatal click has a farther back dorsal constriction than the three pre-palatal clicks, and the tongue root is raised and bunched in the upper pharynx in one variant of the palatal click, but involves retraction of the tongue root proper in the lower pharynx in the alveolar click. A second variant of the palatal click involves posterior gestures more similar to those found in the alveolar click. Results provide evidence that the kinematics of the posterior part of the tongue are important in describing click production, and shed light on synchronic and diachronic sound patterns involving the palatal click in Kx’a languages. & 2015 Elsevier Ltd. All rights reserved.	medical ultrasound;synchronicity;tract (literature)	Amanda L. Miller	2016	J. Phonetics	10.1016/j.wocn.2015.12.001	speech recognition;acoustics;communication	Web+IR	-9.348004852199242	-82.10387599144725	155155
401e6495dba69b32b2d42957ddd00ad1b0595c25	speech watermarking method based on formant tuning	speech watermarking;formant tuning;robustness;inaudibility;line spectral frequencies	This paper proposes a speech watermarking method based on the concept of formant tuning. The characteristic that formant tuning can improve the sound quality of synthesized speech was employed to achieve inaudibility for watermarking. In the proposed method, formants were firstly extracted with linear prediction (LP) analysis and then embedded with watermarks by symmetrically controlling a pair of line spectral frequencies (LSFs) as formant tuning. We evaluated the proposed method by two kinds of experiments regarding inaudibility and robustness compared with other methods. Inaudibility was evaluated with objective and subjective tests and robustness was evaluated with speech codecs and speech processing. The results revealed that the proposed method could satisfy both inaudibility and robustness that required for speech watermarking. key words: speech watermarking, formant tuning, line spectral frequencies, inaudibility, robustness	codec;digital watermarking;embedded system;experiment;line spectral pairs;robustness (computer science);sound quality;speech coding;speech processing;speech synthesis	Shengbei Wang;Masashi Unoki	2015	IEICE Transactions	10.1587/transinf.2014MUP0009	speech recognition;computer science;programming language;robustness	EDA	-9.708543911448338	-89.12475006854069	155197
8b190991be147b94664aa2f0261cc60f27b9fbc4	on the externalization of auditory images	reverberation;visual stimuli;sound localization;auditory perception;motion;monaural signals;head anatomy;psychoacoustics			Nathaniel I. Durlach;A. Rigopulos;X. D. Pang;W. S. Woods;A. Kulkarni;H. S. Colburn;E. M. Wenzel	1992	Presence: Teleoperators & Virtual Environments	10.1162/pres.1992.1.2.251	speech recognition;sound localization;visual perception;reverberation;motion;psychoacoustics;auditory scene analysis;auditory masking	Visualization	-7.178092890834803	-83.72708411854566	155507
4f2b25404a88a65dcb2525777530f0a3b177ad11	computational intelligence in a classification of audio recordings of nature		This paper presents different ways for a classification of sounds of birds using linguistic approach with a fuzzy system, neural network and WEKA system. Features of sounds of birds species are coded by the selected MPEG-7 descriptors. The models of classification system are based on the audio descriptors for a some chosen species of birds like: Corn Crake, Hawk, Blackbird, Cuckoo, Lesser Whitethroat, Chiffchaff, Eurasian Pygmy Owl, Meadow Pipit, House Sparrow, Firecrest. The paper proposes fuzzy models that definitely bases on the linguistic description. Moreover neural network for classification was proposed. As reference results WEKA system is used.	artificial neural network;computation;computational intelligence;fuzzy control system;mpeg-7;meadow;mike lesser;web ontology language;weka	Krzysztof Tyburek;Piotr Prokopowicz;Piotr Kotlarz	2014		10.5220/0005153801870192	natural language processing;speech recognition;computer science;machine learning	AI	-6.850039732996147	-90.14823376373805	155759
631a07c86fba4f8daf40c5a544648fe167b2ba0f	improved multitaper pncc feature for robust speaker verification	robust speaker verification power normalized cepstral coefficients compressive gammachirp filter bank spectrum estimation cgcfb subband power integration improved multitaper pncc feature impncc noisy conditions sre database;speaker recognition cepstral analysis channel bank filters;compressive gammachirp filter bank;multitaper method;speaker verification;compressive gammachirp filter bank robust feature speaker verification impncc multitaper method;robust feature;robustness speech noise feature extraction nist estimation databases;impncc	A major challenge for practical speaker verification is the significant performance degradation in noisy circumstances. To solve the problem, a novel feature is proposed in this paper based on power normalized cepstral coefficients (PNCC). In the new feature, multitaper method and compressive gammachirp filter-bank (cGCFB) are introduced. Multitaper method provides a better spectrum estimation while cGCFB makes the sub-band power integration more robust. The experiments shows that with a suitable configuration, the improved multitaper PNCC (imPNCC) outperforms the original PNCC under both clean and noisy conditions, and achieves the best overall results on NIST 2008 and 2010 SRE database compared to PLP and PNCC.	cepstrum;coefficient;elegant degradation;experiment;filter bank;pl/p;speaker recognition;spectral density estimation	Yi Liu;Liang He;Jia Liu	2014	The 9th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2014.6936721	speech recognition;pattern recognition;statistics	Arch	-13.99542527881876	-91.85369985462816	155781
923b03eafa2082be0ab93512739545caecebcced	motion history images for online speaker/signer diarization	speaker recognition image motion analysis image representation maximum likelihood estimation;history density estimation robust algorithm speech conferences assistive technology gesture recognition speech processing;motion energy images speaker diarization signer diarization motion history images;speaker diarization;part of book or chapter of book;information retrieval motion history images online speaker signer diarization gestural activity uttering activity sign languages spoken languages mhi likelihood measure ami meeting data kata kolok data sign language dataset video conferences;signer diarization	We present a solution to the problem of online speaker/signer diarization - the task of determining who spoke/signed when?. Our solution is based on the idea that gestural activity (hands and body movement) is highly correlated with uttering activity. This correlation is necessarily true for sign languages and mostly true for spoken languages. The novel part of our solution is the use of motion history images (MHI) as a likelihood measure for probabilistically detecting uttering activities. MHI is an efficient representation of where and how motion occurred for a fixed period of time. We conducted experiments on 4.9 hours of the AMI meeting data and 1.4 hours of sign language dataset (Kata Kolok data). The best performance obtained is 15.70% for sign language and 31.90% for spoken language (measurements are in DER). These results show that our solution is applicable in real-world applications like video conferences and information retrieval.	experiment;information retrieval;motion history images;sensor;speaker diarisation	Binyam Gebrekidan Gebre;Peter Wittenburg;Tom Heskes;Sebastian Drude	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6853855	natural language processing;speaker diarisation;speech recognition;computer science	Vision	-7.067640625567131	-89.39815935240098	155940
121b6bd7897127bdf39e3078d5a2a61050258a2b	video augmentation for improving audio speech recognition under noise	automatic segmentation;matching function;feature space;multiple scales;video recording;speaker dependent;speech recognition;audio visual;feature selection;conference proceeding	For the recognition of speech, in particular spoken digits, captured in video with poor sound due to noise, we develop a novel audio-v isual fusion technique that performs significantly better than util ising either audio or video signal alone. Specifically, we present an audio-vis ual intermediate fusion strategy to locate speaker dependant pronounced dig its in continuous video recorded with sound. A model template for each digit is represented in a single audio-visual feature space using a set of spatio-te mporal visual features at multiple scales together with a set of thirteen Mel F r quency Cepstral Coefficients as audio features. Using a unified structure for both visual and audio feature selection and extraction, we solve the proble m of one-to-one correspondence between the audio and visual spaces caused b y differences in data sampling rates. To combine the two modalities, we ado pt an intermediate fusion strategy by combining the two modalities in a probabilistic sequence matching function, permitting automatic segment atio of a continuous probe video sequence and matching with available model templates. For experiments, the CUAVE [17] database was used to compare our scheme with two alternative methods. The evaluation shows that the proposed approach outperforms the others both in recognition accuracy and robustness in coping with variations in probe sequences.	ado.net;coefficient;experiment;feature selection;feature vector;ising model;linear interpolation;mel-frequency cepstrum;one-to-one (data model);sampling (signal processing);sound card;speech recognition;synchro;utility	Samuel Pachoud;Shaogang Gong;Andrea Cavallaro	2008		10.5244/C.22.25	computer vision;audio mining;speech recognition;feature vector;computer science;machine learning;pattern recognition;feature selection	Vision	-9.084250052538478	-93.07127388708952	155993
5b4a80356acef199daa933e6e3ba92fbcdf9438e	an improved music representation method by using harmonic-based chord decision algorithm	audio signal processing;electronic mail;music indexing;ornaments;passing notes;perceptual accuracy;auditory system;global decision phase;music representation method;ornaments music representation method harmonic based chord decision algorithm monophonic music music indexing automatic composition systems local recognition phase global decision phase perceptual accuracy passing notes;data mining;local recognition phase;research and development;multiple signal classification;indexing;multiple signal classification indexing data mining partitioning algorithms cities and towns electronic mail algorithm design and analysis research and development music information retrieval content based retrieval;decision theory;music information retrieval;signal classification audio signal processing decision theory hearing music pattern recognition;signal classification;monophonic music;pattern recognition;cities and towns;harmonic based chord decision algorithm;automatic composition systems;content based retrieval;music;algorithm design and analysis;hearing;partitioning algorithms	We develop an algorithm for chord decision for monophonic music, and it can be applied to music indexing and automatic composition systems. The basic ideas of the proposed algorithm can be divided into two phases: the local recognition phase and the global decision phase. The algorithm analyzes the music and assigns a set of chord candidates in the local recognition phase, while deciding the best appropriate chord according to the chord progression rules in the global decision phase. In order to improve the perceptual accuracy, we consider the beat of corresponding notes in a measure and eliminate the effect of the passing notes and ornaments	algorithm;color gradient;left 4 dead 2	Chuan-Wang Chang;Hewijin Christine Jiau	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)	10.1109/ICME.2004.1394267	computer vision;speech recognition;decision theory;audio signal processing;computer science;multiple signal classification;machine learning;pattern recognition;music;statistics	Robotics	-8.13508994354644	-91.6788053541175	156059
6cb7016d62751d2362cb6b947ed682b9055d2af8	creation of classifier ensembles for handwritten word recognition using feature selection algorithms	classifier combination;classifier ensemble;handwriting recognition;hidden markov model;hidden markov models handwritten character recognition pattern classification;handwriting recognition hidden markov models pattern recognition vocabulary computer science character recognition machine learning principal component analysis search problems conferences;hidden markov model recognizer classifier ensembles handwritten word recognition feature selection algorithms multiple classifier systems handwriting recognition;ensemble creation method;hidden markov models;pattern classification;pattern recognition;hidden markov model hmm;handwritten word recognition;feature selection;multiple classifier system;handwritten character recognition	The study of multiple classifier systems has become an area of intensive research in pattern recognition recently. Also in handwriting recognition, systems combining several classifiers have been investigated. In this paper new methods for the creation of classifier ensembles based on feature selection algorithms are introduced. Those new methods are evaluated and compared to existing approaches in the context of handwritten word recognition, using a hidden Markov model recognizer as basic classifier.	algorithm;experiment;feature selection;finite-state machine;handwriting recognition;hidden markov model;markov chain;pattern recognition;random subspace method;search algorithm;statistical classification	Simon Günter;Horst Bunke	2002		10.1109/IWFHR.2002.1030906	margin classifier;speech recognition;intelligent character recognition;computer science;intelligent word recognition;machine learning;pattern recognition;hidden markov model	Vision	-4.86626030450044	-89.58175766123911	156157
104fed71d6f68940e0a995ccc8a778a3861ca904	the role of synchrony and ambiguity in speech–gesture integration during comprehension	semantic integration;speech segmentation;article letter to editor;face to face	During face-to-face communication, one does not only hear speech but also see a speaker's communicative hand movements. It has been shown that such hand gestures play an important role in communication where the two modalities influence each other's interpretation. A gesture typically temporally overlaps with coexpressive speech, but the gesture is often initiated before (but not after) the coexpressive speech. The present ERP study investigated what degree of asynchrony in the speech and gesture onsets are optimal for semantic integration of the concurrent gesture and speech. Videos of a person gesturing were combined with speech segments that were either semantically congruent or incongruent with the gesture. Although gesture and speech always overlapped in time, gesture and speech were presented with three different degrees of asynchrony. In the SOA 0 condition, the gesture onset and the speech onset were simultaneous. In the SOA 160 and 360 conditions, speech was delayed by 160 and 360 msec, respectively. ERPs time locked to speech onset showed a significant difference between semantically congruent versus incongruent gesture–speech combinations on the N400 for the SOA 0 and 160 conditions. No significant difference was found for the SOA 360 condition. These results imply that speech and gesture are integrated most efficiently when the differences in onsets do not exceed a certain time span because of the fact that iconic gestures need speech to be disambiguated in a way relevant to the speech context.	asynchrony (computer programming);erp;gesture recognition;list comprehension;movement;onset (audio);part-of-speech tagging;semantic integration;span distance;used quit cigarette smoking videos	Boukje Habets;Sotaro Kita;Zeshu Shao;Asli Özyürek;Peter Hagoort	2011	Journal of Cognitive Neuroscience	10.1162/jocn.2010.21462	psychology;speech production;semantic integration;speech;motor theory of speech perception;linguistics;speech error;speech segmentation;communication;gesture	HCI	-9.246683895726523	-80.82258411395446	156409
be0299c9074795f90c484b27ec7cc670ee1c26e0	discriminative adaptation for log-linear acoustic models	index terms: speech recognition;log-linear mod- els;adaptation;indexing terms;log linear model;feature space;maximum likelihood;speech recognition	Log-linear models have recently been used in acoustic modeling for speech recognition systems. This has been motivated by competitive results compared to systems based on Gaussian models, and a more direct parametrisation of the posterior model. To competitively use log-linear models for speech recognition, important methods, such as speaker adaptation, have to be reformulated in a log-linear framework. In this work, an approach to log-linear affine feature transforms for speaker adaptation is described. Experiments for both supervised and unsupervised adaptation are presented, showing improvements over a maximum likelihood baseline in the form of feature space maximum likelihood linear regression for the case of supervised adaptation.	acoustic cryptanalysis;acoustic model;baseline (configuration management);feature vector;linear model;log-linear model;speech recognition;supervised learning;unsupervised learning	Jonas Lööf;Ralf Schlüter;Hermann Ney	2010			log-linear model;discriminative model;speech recognition;parametrization;pattern recognition;gaussian;machine learning;feature vector;linear regression;computer science;artificial intelligence;affine transformation;maximum likelihood sequence estimation	NLP	-18.614674495869085	-91.62349247034197	156462
a89098853b0d2860b8356be09db8d21128049806	continuous speech phoneme segmentation method based on the instantaneous frequency.	instantaneous frequency		instantaneous phase	Anastasios Tsopanoglou;John Mourjopoulos;George K. Kokkinakis	1989			instantaneous phase;speech recognition;computer science	NLP	-13.657632997609074	-87.0325741617326	156502
83cff0ea89bfc2f0601899f352017193ec8c6af8	以語音辨識與評分輔助口說英文學習 (spoken english learning based on speech recognition and assessment) [in chinese]			speech recognition	Jiang-Chun Chen;Jui-Lin Lo;Jyh-Shing Roger Jang;Chun-Jen Lee	2004			natural language processing;chinese speech synthesis;computer science;speech recognition;artificial intelligence	NLP	-15.64796454936509	-85.51377473263726	156508
1eac7a2a2c5cc1f66c23f8d724997b941a2dace0	laughter animation synthesis	expression synthesis;laughter;multimodal animation;virtual agent	Laughter is an important communicative signal in humanhuman communication. However, very few attempts have been made to model laughter animation synthesis for virtual characters. This paper reports our work to model hilarious laughter. We have developed a generator for face and body motions that takes as input the sequence of pseudophonemes of laughter and each pseudo-phoneme’s duration time. Lip and jaw movements are further driven by laughter prosodic features. The proposed generator first learns the relationship between input signals (pseudo-phoneme and acoustic features) and human motions; then the learnt generator can be used to produce automatically laughter animation in real time. Lip and jaw motion synthesis is based on an extension of Gaussian Models, the contextual Gaussian Model. Head and eyebrow motion synthesis is based on selecting and concatenating motion segments from motion capture data of human laughter while torso and shoulder movements are driven from head motion by a PD controller. Our multimodal behaviors generator of laughter has been evaluated through perceptive study involving the interaction of a human and an agent telling jokes to each other.	acoustic cryptanalysis;computer animation;concatenation;motion capture;multimodal interaction	Yu Ding;Ken Prepin;Jing Huang;Catherine Pelachaud;Thierry Artières	2014			computer vision;speech recognition	Graphics	-15.054798865527678	-82.51784592929907	156559
ccd7c591ae42289b43e983efb352f1279ea635e4	robust speech recognition via modeling spectral coefficients with hmm's with complex gaussian components.		Robust speech recognition via hidden Markov modeling of spectral vectors is studied in this paper. The hidden Markov model (HMM) mixture components are assumed complex Gaussian with zero mean, diagonal covariance, and with incorporating an unknown scalar gain term. The gain term is associated with each spectral vector and it models the varying energy of speech signals. It is estimated by applying the maximum likelihood (ML) criterion. On an isolated digit database, in clean conditions, the spectral modeling with ML gain estimation approach achieved similar performance to cepstral modeling of speech. Two additive noise compensation approaches for the spectral modeling scheme are also considered. The rst approach requires a full noise HMM. This HMM is combined with the clean speech HMM to yield a noisy speech HMM. The second approach requires only the spectral shape of the noise. A term dependent on the spectral shape, together with an unknown magnitude term, is incorporated into the clean speech HMM to yield a noisy speech HMM. The unknown magnitude of the noise is estimated via the ML criterion. The performance of these two approaches for isolated digit recognition in noise is demonstrated and compared to a robust cepstral modeling approach from the literature.	additive white gaussian noise;cepstrum;coefficient;hidden markov model;markov chain;online and offline;speech recognition;utility functions on indivisible goods	William J. J. Roberts;Sadaoki Furui	2000			speech recognition;computer science;machine learning;pattern recognition	ML	-17.050745802716193	-92.36458003874512	156640
d9755ccd79ce12e5af28c4d29e452537c6b0abcf	siri's voice gets deep learning			deep learning;siri	Alex Acero	2016			speech recognition;deep learning;psychology;artificial intelligence;s voice	Robotics	-15.434428848027556	-87.34469597865136	156702
5b18f1ae13d3c75147ec484a330194ca9762b342	convolutional neural network for refinement of speaker adaptation transformation	publications convolutional neural network for refinement of speaker adaptation transformation;katedra kybernetiky;publikace convolutional neural network for refinement of speaker adaptation transformation;kybernetika;informacni a řidici systemy;automaticke řizeni;uměla inteligence	The aim of this work is to propose a refinement of the shift-MLLR (shift Maximum Likelihood Linear Regression) adaptation of an acoustics model in the case of limited amount of adaptation data, which can lead to ill-conditioned transformations matrices. We try to suppress the influence of badly estimated transformation parameters utilizing the Artificial Neural Network (ANN), especially Convolutional Neural Network (CNN) with bottleneck layer on the end. The badly estimated shift-MLLR transformation is propagated through an ANN (suitably trained beforehand), and the output of the net is used as the new refined transformation. To train the ANN the well and the badly conditioned shift-MLLR transformations are used as outputs and inputs of ANN, respectively.		Zbynek Zajíc;Jan Zelinka;Jan Vanek;Ludek Müller	2014		10.1007/978-3-319-11581-8_20	speech recognition;machine learning;pattern recognition	NLP	-17.142392640703317	-88.91510511624162	156740
5653cceb64ffe6035a062bde297520cf757e8090	real-time perceptual model for distraction in interfering audio-on-audio scenarios		This letter proposes a real-time perceptual model predicting the experienced distraction occurring in interfering audio-on-audio situations. The proposed model improves the computational efficiency of a previous distraction model, which cannot provide predictions in real time. The chosen approach was to utilize similar features as the previous model, but to use faster underlying algorithms to calculate these features. The results show that the proposed model has a root mean squared error of 11.9%, compared to the previous model's 11.0%, while only taking 0.04% of the computational time of the previous model. Thus, while providing similar accuracy as the previous model, the proposed model can be run in real time. The proposed distraction model can be used as a tool for evaluating and optimizing sound-zone systems. Furthermore, the real-time capability of the model introduces new possibilities, such as adaptive sound-zone systems.	algorithm;audio signal processing;computation;mean squared error;real-time clock;real-time computing;real-time locating system;real-time transcription;time complexity	Jussi Rämö;Soren Bech;Søren Holdt Jensen	2017	IEEE Signal Processing Letters	10.1109/LSP.2017.2733084	pattern recognition;distraction;perception;psychoacoustics;artificial intelligence;machine learning;computer science;mean squared error	Embedded	-9.9041840955019	-89.37814082718019	156922
87aafe3bab34e4400757339c8680f5974b059de4	factor analysis of mixture of auto-associative neural networks for speaker verification		This paper introduces the theory of factor analysis of the mixture of Auto-Associative Neural Networks (AANNs) with application in speaker verification. First, we formulate the problem of learning a low-dimensional subspace in part of the mixture of AANNs parameter space, and subsequently derive the update equations by minimizing loss function of the mixture. Second, we apply this technique to build a neural network based speaker verification system, in which the low-dimensional subspace is trained to capture both speaker and channel variabilities. This low-dimensional (or i-vector) representation is used as features for the probabilistic linear discriminant analysis (PLDA) model, as in state-of-the-art speaker verification systems. The proposed factor analysis approach shows promising results on the NIST-08 speaker recognition evaluation (SRE), and yields 18% relative improvement in minimum detection cost function (minDCF) over the previously proposed subspace based mixture of AANNs system.	artificial neural network;factor analysis;linear discriminant analysis;loss function;speaker recognition	Sri Garimella;Hynek Hermansky	2012			artificial neural network;artificial intelligence;associative property;machine learning;pattern recognition;computer science	AI	-16.72429917975342	-91.4207006575718	157337
07094bc51431a0029c3953267718af12f101fb25	the effects of linguistic experience on perceptual assimilation of lexical tone		This study examines the effects of L1 tone experience and L2 learning experience on tone assimilation. It has been proposed that perceptual assimilation of segments is tied to L1 and L2 contrasts at the phonetic level for listeners without L2 experience [1] but at both phonetic and phonological levels for those with L2 experience [2]. In this examination of perceptual assimilation of lexical tones, 80 native Mandarin and Thai listeners with or without L2 experience performed a mapping-rating assimilation task in which they first identified which L1 tone sounded most similar to the L2 tone they heard, and then rated its goodness on a 5-point scale. The inexperienced listeners assimilated L2 tones to L1 tones with the most similar phonetic properties, i.e., pitch height and direction. The experienced listeners were also influenced by phonemic tone sandhi changes in Mandarin. In other words, Mandarin rising tone and Thai low-falling tone were assimilated to the Thai and Mandarin falling-rising tone. These findings support the Perceptual Assimilation Model [1] and Perceptual Assimilation Model-L2 [2].	data assimilation;experience;pitch (music);super robot monkey team hyperforce go!	Xianghua Wu;Murray J. Munro;Yue Wang	2011			communication;psychology;perception;assimilation (phonology);linguistics	NLP	-11.38129184192869	-81.76302715751775	157438
6e1283b6ebfc913e06eb7b0d56373b96071459dd	synthetic individual binaural audio delivery by pinna image processing	settore inf 01 informatica;pinna;spatial sound;auditory localization;hrtf;headphones;3d audio;binaural	Purpose – The purpose of this paper is to present a system for customized binaural audio delivery based on the extraction of relevant features from a 2-D representation of the listener’s pinna. Design/methodology/approach – The most significant pinna contours are extracted by means of multi-flash imaging, and they provide values for the parameters of a structural head-related transfer function (HRTF) model. The HRTF model spatializes a given sound file according to the listener’s head orientation, tracked by sensor-equipped headphones, with respect to the virtual sound source. Findings – A preliminary localization test shows that the model is able to statically render the elevation of a virtual sound source better than non-individual HRTFs. Research limitations/implications – Results encourage a deeper analysis of the psychoacoustic impact that the individualized HRTF model has on perceived elevation of virtual sound sources. Practical implications – The model has low complexity and is suitable for implementation on mobile devices. The resulting hardware/software package will hopefully allow an easy and low-tech fruition of custom spatial audio to any user. Originality/value – The authors show that custom binaural audio can be successfully deployed without the need of cumbersome subjective measurements.	amiga walker;binaural beats;brewster's angle;ct scan;complexity;covox speech thing;dhrystone;head-related transfer function;headphones;image processing;image sensor;in-place algorithm;mac os x 10.4 tiger;mobile device;mobile phone;nl (complexity);pa degree;psychoacoustics;real-time clock;real-time computing;sandy bridge;surround sound;synthetic intelligence	Simone Spagnol;Michele Geronazzo;Davide Rocchesso;Federico Avanzini	2014	Int. J. Pervasive Computing and Communications	10.1108/IJPCC-06-2014-0035	speech recognition;binaural recording;head-related transfer function	HCI	-7.293927338851076	-87.01655076047228	157475
0bb7d8b42923fffe0aa4093bd1f4d9e17b8c4089	improving high quality tts using circular linear prediction and constant pitch transform	databases;circular linear prediction;text to speech synthesis;speech intelligibility;model combination;unit selection;speech synthesis;speech processing;speech segmentation;vocabulary;constant pitch transform;large database unit selection systems;speech coding;data engineering;speech segments;transforms speech processing speech synthesis;linear predictive;lsf tracks;prosodic modifications;linear prediction coding;linear predictive coding;speech synthesis databases predictive models speech processing speech coding robustness vocabulary data engineering scalability system testing;tts;speech signals;transforms;system testing;robustness;predictive models;speech communication;scalability;text to speech synthesis tts circular linear prediction constant pitch transform speech segments robust representation speech signals lsf tracks prosodic modifications large database unit selection systems;speech intelligibility speech synthesis speech communication linear predictive coding speech processing;robust representation	Current high quality concatenative TTS systems are based on unit selection from a database that is contextually and prosodically rich. These systems are computationally expensive and require a very large footprint. This paper presents a new method for representing speech segments that can improve the quality and scalability of concatenative TTS systems. The circular linear prediction model combined with the constant pitch transform provides a robust representation of speech signals that allows for limited prosodic movements without perceivable loss in quality. A method is presented for constraining the LSF tracks of speech segments to realize pitch modifications with minimal artifacts. The results of formal listening tests demonstrate that limited prosodic modifications can produce speech from fewer units whose quality equals or exceeds large database unit-selection systems. Additionally, this method is used to realize high quality emphasized speech.	analysis of algorithms;cpt (file format);database;display resolution;elegant degradation;lsf;netware file system;scalability;speech synthesis	Sunil Shukla;Thomas P. Barnwell	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367004	natural language processing;scalability;linear predictive coding;speech recognition;computer science;speech;speech coding;speech processing;predictive modelling;speech segmentation;speech synthesis;system testing;intelligibility;robustness	Visualization	-10.916467404626118	-88.79969910033526	157710
d4b29432c3e7bd4e2d06935169c91f781f441160	speech recognition of malayalam numbers	databases;automatic data entry;hidden markov model;acoustics;training;mel frequency cepstrum coefficient;pin entry;speech;banking system;speaker recognition;accuracy;hidden markov models;speaker independent;signal processing;speech recognition hidden markov models automatic speech recognition artificial neural networks support vector machines support vector machine classification management training telephony databases natural languages;hidden markov model malayalam numbers digit speech recognition automatic data entry pin entry voice dialing telephone automated banking system speaker independent speech recognition system mel frequency cepstrum coefficient signal processing;word recognition;hidden markov model hmm;speaker recognition hidden markov models;speech recognition;article;age groups;digit speech recognition;mel frequency cepstrum coefficient mfcc	Digit speech recognition is important in many applications such as automatic data entry, PIN entry, voice dialing telephone, automated banking system, etc. This paper presents speaker independent speech recognition system for Malayalam digits. The system employs Mel frequency cepstrum coefficient (MFCC) as feature for signal processing and Hidden Markov model (HMM) for recognition. The system is trained with 21 male and female voices in the age group of 20 to 40 years and there was 98.5% word recognition accuracy (94.8% sentence recognition accuracy) on a test set of continuous digit recognition task.	cepstrum;coefficient;hidden markov model;markov chain;signal processing;speech recognition;test set;voice command device	Cini Kurian;Kannan Balakrishnan	2009	2009 World Congress on Nature & Biologically Inspired Computing (NaBIC)	10.1109/NABIC.2009.5393692	natural language processing;speaker recognition;speech recognition;word recognition;computer science;speech;pattern recognition;accuracy and precision;demographic profile;hidden markov model	NLP	-14.617765226551905	-87.62556909225965	157789
be4577593028135322d2408980a7d4f96349e0f6	frequency-warping in speech	databases;scale factor;speech recognition;formant frequencies;mel scale;cepstrum;parameter estimation;cepstral coefficients;loudspeakers;psychology;feature extraction;fourier transforms	In this paper we present results that indicate that the formant frequencies between di erent speakers scale di erently at di erent frequencies. Based on our experiments on speech data, we then numerically compute a universal frequencywarping function, to make the scale-factor independent of frequency in the warped domain. The proposed warping function is found to be similar to the mel-scale, which has previously been derived from purely psycho-acoustic experiments. The motivation for the present experiments stems from our recently proposed use of scale-transform based cepstral coe cients [6] as acoustic features, since they provide superior separability of vowels than mel-cepstral coe cients.	acoustic cryptanalysis;bilinear transform;cepstrum;experiment;linear separability;mel scale;numerical analysis	Srinivasan Umesh;Leon Cohen;Nenad Marinovic;Douglas J. Nelson	1996			speech recognition;acoustics;computer science;pattern recognition	Robotics	-9.806945262813713	-90.49143029081552	157808
12cc79c688e9664fbcc10faadb9717a6c1dfb019	experimental evaluation of the relevance of prosodic features in spanish using machine learning techniques	input output;machine learning;experimental evaluation	In this work, machine learning techniques have been applied for the assessment of the relevance of several prosodic features in TTS for Spanish. Using a two step correspondence between sets of prosodic features and intonation parameters, the influence of the number of different intonation patterns and the number and order of prosodic features is evaluated. The output of the trained classifiers is proposed as a labelling mechanism of intonation units which can be used to synthesize high quality pitch contours. The input output correspondence of the classifier also provides a bundle of relevant prosodic knowledge.	display resolution;input/output;machine learning;netware file system;relevance	David Escudero Mancebo;Valentín Cardeñoso-Payo;Antonio Bonafonte	2003			natural language processing;input/output;speech recognition;computer science;machine learning;pattern recognition	ML	-18.931940590474895	-83.04063264047262	157850
b12f1ba6db94acb115c8b182a9e1e717c9ed3c3a	investigating the cog ratio as feature for speaker verification on high-effort speech.		Vocal effort mismatch in training and test data leads to immense degradations of speaker recognition systems. The changes on the acoustics of a speech signal induced by raised vocal effort are complex and despite several studies from various authors not completely known yet. Instead of just gaining knowledge about these differences for automatic speaker recognition it is rather an essential to discover features that remain relatively stable in changing vocal effort conditions and contain speaker specific information. In this study we investigate the center of gravity (COG) ratio for high and mid frequency bands as feature for speaker recognition. We find that vocal effort mismatch leads to an equal error rate (EER) more than six times higher for a standard MFCCbased GMM-UBM system. For the COG ratio we observe a much smaller degradation of around 25%. When adapting the UBM with additional high-effort speech data the EER of the COG ratio gets even better for the mismatch condition than for the matching task. Combining MFCC and the COG ratio leads to best results with an overall improvement of 16% compared to the standard MFCC-based system.	elegant degradation;enhanced entity–relationship model;frequency band;google map maker;speaker recognition;test data	Corinna Harwardt	2010			natural language processing;speaker recognition;speaker diarisation;speech recognition;pattern recognition	NLP	-13.284718387541272	-89.46619000989186	157921
6bb034b67db944322857af973feda858d2b0f41b	a non-linear filtering approach to stochastic training of the articulatory-acoustic mapping using the em algorithm	acoustic variables measurement;filtering theory;function approximation;maximum likelihood estimation;parameter estimation;speech recognition;speech synthesis;state estimation;statistical analysis;stochastic processes;em algorithm;acoustic measurements;articulatory measurements;articulatory-acoustic inversion;articulatory-acoustic mapping;artificial simulations;codebooks;expectation maximization;functional approximation;nonlinear filtering approach;parameter estimation;real speech;state estimation;stochastic training	Current techniques for training representations of the articulatory-acoustic mapping from data rely on arti cial simulations to provide codebooks of articulatory and acoustic measurements, which are then modelled by simple functional approximations. This paper outlines a stochastic framework for adapting an arti cial model to real speech from acoustic measurements alone, using the EM algorithm. It is shown that parameter and state estimation problems for articulatory-acoustic inversion can be solved by adopting a statistical approach based on non-linear ltering.	acoustic cryptanalysis;approximation;codebook;expectation–maximization algorithm;nonlinear system;simulation	Gordon Ramsay	1996			speech recognition;computer science;machine learning;pattern recognition	Robotics	-18.02502533164942	-93.16100610068621	157929
d2d7b3918dccfcb8c2ae03ce07e9dbe1e673a14b	inverting mappings from smooth paths through rn to paths through rm: a technique applied to recovering articulation from acoustics	analyse parole;traitement signal;funcion no lineal;speech inverse problem;analisis palabra;speech processing;speech analysis;channel normalization;tratamiento palabra;traitement parole;vocal tract;non linear function;energy spectrum;metodo subespacio;time series;problema inverso;speech perception;carta de datos;vocal;verbal perception;methode sous espace;data recovery;band pass;automatic speech recognition;canal vocal;automatic recognition;percepcion verbal;espectro energia;dimensionality reduction;reconocimiento voz;inverse problem;conducto vocal;mappage;signal processing;serie temporelle;fonction non lineaire;serie temporal;subspace method;spectre energie;speech recognition;time series data;voyelle;mapping;reconnaissance parole;parameter estimation;recuperation donnee;vowel;procesamiento senal;dimensional reduction;probleme inverse;reconocimiento automatico;reconnaissance automatique;perception verbale	Motor theories, which postulate that speech perception is related to linguistically significant movements of the vocal tract, have guided speech perception research for nearly four decades but have had little impact on automatic speech recognition. In this paper, we describe a signal processing technique named MIMICRI that may help link motor theory with automatic speech recognition by providing a practical approach to recovering articulator positions from acoustics. MIMICRI's name reflects three important operations it can perform on time-series data: it can reduce the dimensionality of a data set (manifold inference); it can blindly invert nonlinear functions applied to the data (mapping inversion); and it can use temporal context to estimate intermediate data (contextual recovery of information). In order for MIMICRI to work, the signals to be analyzed must be functions of unobservable signals that lie on a linear subspace of the set of all unobservable signals. For example, MIMICRI will typically work if the unobservable signals are band-pass and we know the pass-band, as is the case for articulator motions. We discuss the abilities of MIMICRI as they relate to speech processing applications, particularly as they relate to inverting the mapping from speech articulator positions to acoustics. We then present a mathematical proof that explains why MIMICRI can invert nonlinear functions, which it can do even in some cases in which the mapping from the unobservable variables to the observable variables is many-to-one. Finally, we show that MIMICRI is able to infer accurately the positions of the speech articulators from speech acoustics for vowels. Five parameters estimated by MIMICRI were more linearly related to articulator positions than 128 spectral energies.	biconnected component	John Hogden;Philip Rubin;Erik McDermott;Shigeru Katagiri;Louis Goldstein	2007	Speech Communication	10.1016/j.specom.2007.02.008	speech recognition;computer science;artificial intelligence;signal processing;time series;speech processing;linguistics;statistics	ML	-8.068306371599833	-89.05289545288782	158147
97ab9fac6451c0cdb4346afa72cf7344bb5d6b02	efficient implementation of virtual 3d sound synthesis based on combining grouped pca and bmt	distortion performance;azimuth;grouped pca;interpolation;optimization scheme;covariance analysis;acoustic distortion;storage requirement;sound synthesis;virtual 3d sound synthesis;3d sound hkssl grouped pca bmt;3d sound;hkssl;covariance;acoustic signal processing;head related impulse response;decision support system;efficient implementation;linear interpolation;principal component analysis acoustic distortion acoustic signal processing covariance analysis hearing interpolation;principal component analysis;computation complexity;contralateral hrir virtual 3d sound synthesis bmt principal component analysis head related impulse response azimuth covariance distortion performance storage requirement balanced model truncation optimization scheme computation complexity linear interpolation pca model error;decision support systems;bmt;pca model error;contralateral hrir;hearing;balanced model truncation	In this paper, Principal Component Analysis (PCA) is performed on HRIRs grouped according to azimuth and covariance to improve distortion performance while reducing storage requirement. By further combining Balanced Model Truncation (BMT) using a joint optimization scheme, a virtual 3D sound synthesis scheme with low computation complexity, little storage requirement and efficient linear interpolation is proposed. A smaller model distortion is achieved as high PCA model error of contralateral HRIR is decreased by grouping PCA strategy.	barcelona moon team;computation;distortion;linear interpolation;mathematical optimization;principal component analysis;truncation	Zhixin Wang;Cheung-Fat Chan	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946437	speech recognition;decision support system;analysis of covariance;interpolation;computer science;covariance;machine learning;mathematics;azimuth;linear interpolation;statistics;principal component analysis	Robotics	-16.46540181981589	-93.85664039686002	158252
77a4c150c93de562006aaa9db56e626347b278b7	maximum-likelihood updates of hmm duration parameters for discriminative continuous speech recognition	maximum likelihood	Previous studies showed that a signi cantly enhanced recognition performance can be achieved by incorporating information about HMM duration along with the cepstral parameters. The reestimation formula for the duration parameters have been derived in the past using xed segmentation during K-means training and the duration statistics are always xed throughout the additional minimum string error (MSE) training process. In this study, we update the duration parameters along with other model parameters during discriminative training iterations. The convergence property of the training property based on the MSE approach is investigated, and experimental results on wireline connected digit recognition task demonstrated a 6% word error rate reduction by using the newly trained duration model parameters as compared to xed duartion parameters during MSE training.	cepstrum;discriminative model;hidden markov model;iteration;k-means clustering;speech recognition;word error rate	Rathinavelu Chengalvarayan	1998			speech recognition;pattern recognition;hidden markov model;artificial intelligence;discriminative model;computer science;maximum likelihood	ML	-18.799624826287342	-91.8317109006769	158377
90ddcc563c29921882af539f243a9493bc6e10e8	bayesian recurrent neural network language model	hessian matrix recurrent neural network language model bayesian learning;hessian matrix;training data bayesian recurrent neural network language model rnn lm speech recognition estimated model parameter uncertainty compensation gaussian prior objective function regularized cross entropy error function maximum a posteriori criterion gaussian hyperparameter estimation marginal likelihood estimation hessian matrix rapid approximation salient outer products brnn lm sparser model;bayesian learning;proceedings paper;speech recognition approximation theory bayes methods gaussian processes hessian matrices maximum likelihood estimation recurrent neural nets;recurrent neural network;bayes methods training approximation methods neurons recurrent neural networks computational modeling history;language model	This paper presents a Bayesian approach to construct the recurrent neural network language model (RNN-LM) for speech recognition. Our idea is to regularize the RNN-LM by compensating the uncertainty of the estimated model parameters which is represented by a Gaussian prior. The objective function in Bayesian RNN (BRNN) is formed as the regularized cross entropy error function. The regularized model is not only constructed by training the regularized parameters according to the maximum a posteriori criterion but also estimating the Gaussian hyperparameter by maximizing the marginal likelihood. A rapid approximation to Hessian matrix is developed by selecting a small set of salient outer-products and illustrated to be effective for BRNN-LM. BRNN-LM achieves sparser model than RNN-LM. Experiments on different corpora show promising improvement by applying BRNN-LM using different amount of training data.	approximation;artificial neural network;bayesian network;cross entropy;experiment;gaussian blur;hessian;language model;limited-memory bfgs;loss function;marginal model;optimization problem;random neural network;recurrent neural network;speech recognition;text corpus	Jen-Tzung Chien;Yuan-Chu Ku	2014	2014 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2014.7078575	computer science;recurrent neural network;machine learning;pattern recognition;bayesian inference;hessian matrix;statistics;language model	ML	-18.838916158186176	-92.82695625476262	158666
7c2edf50388f25b3980a91e1534f45ed6dbf7e76	a two stage fuzzy decision classifier for speaker identification	speaker identification;fuzzy set;fuzzy sets;communication channels;artificial neural network	The Two Stage Fuzzy Decision Classifier (TSFDC) consists of an Artificial Neural Network (ANN) providing a first stage of discrimination and a second post-processing stage. The latter stage uses reference fuzzy set information, for each class of data considered. The ANN isolates the two most likely classes for each test vector. Post-processing selects the final class from amongst the two. The TSFDC applies post-processing only to those classes which its ANN has difficulty recognising. Three text-independent Automatic Speaker Identification (ASI) experiments are conducted with emphasis on forensic needs. In these experiments, the signal is degraded by a range of factors affecting communication channels. The TSFDC increases the percentage of correctly identified speech frames, for those speakers poorly identified by its ANN, by a mean of 3.27% over the three experiments. Concurrently, the difference in number of identified frames between true and corresponding runner-up speakers improves by a mean of 5.27%. Post-processing better than halves the number of speakers misclassified by the ANN.	algorithm;artificial neural network;bandlimiting;experiment;final (java);fuzzy set;jumbo frame;set theory;speaker recognition;speech synthesis;test vector;video post-processing	Pierre Castellano;Sridha Sridharan	1996	Speech Communication	10.1016/0167-6393(95)00041-0	speech recognition;computer science;machine learning;pattern recognition;fuzzy set;artificial neural network	ML	-13.964372443918354	-90.11113092870518	158742
174bc94c7bd057d7f659070efcde0bce771abf44	data augmentation and feature extraction using variational autoencoder for acoustic modeling		"""A data augmentation and feature extraction method using a variational autoencoder (VAE) for acoustic modeling is described. A VAE is a generative model based on variational Bayesian learning using a deep learning framework. A VAE can extract latent values its input variables to generate new information. VAEs are widely used to generate pictures and sentences. In this paper, a VAE is applied to speech corpus data augmentation and feature vector extraction from speech for acoustic modeling. First, the size of a speech corpus is doubled by encoding latent variables extracted from original utterances using a VAE framework. The latent variables extracted from speech waveforms have latent """"meanings"""" of the waveforms. Therefore, latent variables can be used as acoustic features for automatic speech recognition (ASR). This paper experimentally shows the effectiveness of data augmentation using a VAE framework and that latent variable-based features can be utilized in ASR."""	acoustic cryptanalysis;acoustic model;autoencoder;automated system recovery;convolutional neural network;deep learning;encoder;experiment;feature extraction;feature vector;generative model;latent variable;semiconductor industry;speech corpus;speech recognition;variational principle	Hiromitsu Nishizaki	2017	2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)	10.1109/APSIPA.2017.8282225	autoencoder;feature vector;feature extraction;deep learning;generative model;speech corpus;artificial intelligence;latent variable;bayesian inference;computer science;pattern recognition	NLP	-17.478901841169748	-87.81646433174116	158894
2a409ca1ea12216c7f063b1d738b3ba02bdec5b3	language-resource independent speech segmentation using cues from a spectrogram image	image segmentation;spectrogram;speech processing;low resource languages speech processing spectrogram processing image processing speech segmentation;speech;speech processing hidden markov models image segmentation;visualization;hidden markov models;speech recognition;spectrogram image segmentation speech hidden markov models speech processing speech recognition visualization;image based features language resource independent speech segmentation spectrogram image image processing technique speech spectrogram speech phoneme segmentation timit corpus hidden markov models speech processing task	In this paper, we use image processing techniques on the speech spectrogram to perform speech phoneme segmentation. The proposed method relies solely on visual cues on the spectrogram, without the need for language-specific training data. The results are evaluated on the TIMIT corpus, and compared to other unsupervised speech segmentation techniques, with comparable results obtained. We also fuse the results with those obtained by hidden Markov models (HMM) and HMM-based forced alignment to investigate if image features can provide an additional feature representation for speech processing tasks. With the fusion, up to 10% absolute improvement in segmentation accuracy over the HMM baselines can be obtained. Results are promising and suggests a strong potential for image-based features applying to speech processing.	hidden markov model;image processing;markov chain;spectrogram;speech processing;speech segmentation;timit	Su Jun Leow;Chng Eng Siong;Chin-Hui Lee	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7179086	natural language processing;linear predictive coding;speech recognition;visualization;computer science;speech;spectrogram;speech coding;pattern recognition;speech processing;image segmentation;scale-space segmentation	Vision	-14.734511797907174	-89.2575829847192	158916
c22cca1264e84b62cd4bb0c85c9c0095dad659d8	gmm-based maghreb dialect identificationsystem	core set;gaussian mixture models gmm;minimal enclosing ball meb;kernel methods;universal background model ubm;support vector machines svms;quadratic programming qp	While Modern Standard Arabic is the formal spoken and written language of the Arab world; dialects are the major communication mode for everyday life. Therefore, identifying a speaker’s dialect is critical in the Arabic-speaking world for speech processing tasks, such as automatic speech recognition or identification. In this paper, we examine two approaches that reduce the Universal Background Model (UBM) in the automatic dialect identification system across the five following Arabic Maghreb dialects: Moroccan, Tunisian, and 3 dialects of the western (Oranian), central (Algiersian), and eastern (Constantinian) regions of Algeria. We applied our approaches to the Maghreb dialect detection domain that contains a collection of 10-second utterances and we compared the performance precision gained against the dialect samples from a baseline GMM-UBM system and the ones from our own improved GMM-UBM system that uses a Reduced UBM algorithm. Our experiments show that our approaches significantly improve identification performance over purely acoustic features with an identification rate of 80.49%.	acoustic cryptanalysis;algorithm;approximation;automatic identification and data capture;baseline (configuration management);decision problem;experiment;google map maker;speech processing;speech recognition;spontaneous order	Nour-Eddine Lachachi;Abdelkader Adla	2015	JIPS	10.3745/JIPS.02.0015	natural language processing;kernel method;speech recognition;computer science;machine learning	NLP	-14.386723770993301	-84.89098881809355	158925
c4ae1d792262036f26db97b71fa5a687c5224c8f	morphing sound in real time through the timbre tunnel			carpal tunnel syndrome;morphing	Johannes Kretz	2015			speech recognition;acoustics	AI	-7.7245634467265365	-84.55644125369783	158934
8872bf69277113cd7e01692c62801ffda546a58f	the effects of talker variability on phonetic accommodation		This paper compares spontaneous phonetic accommodation in high and low variability talker conditions. If multi-talker processing is a kind of increased cognitive load, we predict an increase in imitation in the low variability contexts. Our results suggest that, indeed, phonetic accommodation is greater in low variability contexts, but that this higher level of accommodation is moderated as perceptual experiences unfold in the task. That is, while phonetic accommodation in the low variability context stays relatively constant across the task, participants in the high variability context imitate less across the course of the experiment.	code talker;experience;experiment;heart rate variability;spatial variability;spontaneous order	Molly Babel;Grant McGuire	2015			communication;accommodation;psychology	NLP	-9.926313864207032	-82.19503254461408	159079
2c0b1e6e90b360ea03bdb7c85080d46d3873f3bf	a method for extracting a musical unit to phrase music data in the compressed domain of twinvq audio compression	filtering theory audio coding data compression correlation theory laplace transforms music sequences feature extraction information retrieval audio databases;sequences;data compression;correlation theory;information retrieval;laplacian filter;autocorrelation sequence;bit rate;data mining;audio coding;music data phrase;multiple signal classification;mpeg 4 standard;laplace transforms;twinvq audio compression;feature extraction;music information retrieval;musical unit extraction;audio databases;laplacian filter musical unit extraction music data phrase twinvq audio compression autocorrelation sequence;encoding;content based retrieval;music;audio compression;filtering theory;autocorrelation;data mining multiple signal classification audio compression autocorrelation music information retrieval encoding content based retrieval mpeg 4 standard feature extraction bit rate	"""A method for phrasing music data into meaningful musical pieces (e.g., bar and phrase) is an important function to analyze music data. To realize this function, we propose a method for extracting a unit of music data (musical unit) in the compressed domain of TwinVQ audio compression (MPEG-4 audio). Our key idea is to extract a musical unit from a sequence of autocorrelation coefficients computed in the encoding step of TwinVQ audio compression. We call the sequence of the autocorrelation coefficients the """"autocorrelation sequence r"""". We use the k-th autocorrelation sequence r/sub k/ (k=1, 2, ..., 20) of music data for extracting a musical unit of music data. First, we calculate the j/sub k/-th autocorrelation coefficient a/sub k//sup j//sub k/ of the k-th autocorrelation sequence r/sub k/ (j/sub k/=38, 39, ..., 208; k=1, 2, ...,20). Second, for detecting the peak in the sequence (a/sub k//sup 38/, a/sub k//sup 39/, ..., a/sub k//sup 208/), the Laplacian filter is applied to the sequence. We then obtain the order p/sub k/ for which the maximum differential coefficient is attained. Finally, we compute the musical unit using p/sub k/. To evaluate the performance of extracting the musical unit by our method, we collected 64 music data and obtained autocorrelation sequences by applying the TwinVQ encoder to each data. We then applied our extraction algorithm to each autocorrelation sequence. The experimental results reveal a very good performance in the extraction of a musical unit for phrasing music data."""	algorithm;autocorrelation;coefficient;data compression;differential cryptanalysis;encoder;monkey's audio;sensor	Motohiro Nakanishi;Michihiro Kobayakawa;Mamoru Hoshi;Tadashi Ohmori	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521491	data compression;speech recognition;autocorrelation;feature extraction;computer science;theoretical computer science;multiple signal classification;pattern recognition;music;encoding;statistics	ML	-6.806812158848704	-94.1870125079865	159085
eda62aaa490b8d75ad8a55c4efb339e1ecbcb5f4	efficient reconstruction of speakerphone-mode cellular phone sound for application to sound quality assessment			mobile phone;sound quality	Hee-Suk Pang;Jun-Seok Lim;Oh-Jin Kwon;Sang Bae Chon;Mingu Lee;Jeong-Hun Seo	2012	IEICE Transactions		speech recognition	Visualization	-13.110986458919307	-86.60933458221004	159168
156f76245147a176c1e68729bd8790f612e300f6	the intervalgram: an audio feature for large-scale cover-song recognition	machine hearing;auditory image model;cover song recognition	We present a system for representing the musical content of short pieces of audio using a novel chroma-based representation known as the ‘intervalgram’, which is a summary of the local pattern of musical intervals in a segment of music. The intervalgram is based on a chroma representation derived from the temporal profile of the stabilized auditory image [10] and is made locally pitch invariant by means of a ‘soft’ pitch transposition to a local reference. Intervalgrams are generated for a piece of music using multiple overlapping windows. These sets of intervalgrams are used as the basis of a system for detection of identical melodic and harmonic progressions in a database of music. Using a dynamicprogramming approach for comparisons between a reference and the song database, performance is evaluated on the ‘covers80’ dataset [4]. A first test of an intervalgram-based system on this dataset yields a precision at top-1 of 53.8%, with an ROC curve that shows very high precision up to moderate recall, suggesting that the intervalgram is adept at identifying the easier-to-match cover songs in the dataset with high robustness. The intervalgram is designed to support locality-sensitive hashing, such that an index lookup from each single intervalgram feature has a moderate probability of retrieving a match, with few false matches. With this indexing approach, a large reference database can be quickly pruned before more detailed matching, as in previous content-identification systems.	bibliographic database;locality of reference;locality-sensitive hashing;lookup table;microsoft windows;pitch shift;receiver operating characteristic;spectrogram	Thomas C. Walters;David A. Ross;Richard F. Lyon	2012		10.1007/978-3-642-41248-6_11	speech recognition;computer science;artificial intelligence;machine learning;pattern recognition;information retrieval	ML	-7.740367262386759	-93.81791230886896	159231
6c4658722920aa85547c52574e8df1ecca7fdfbc	svm based voice activity detection by fusing a new acoustic feature plms with some existing acoustic features of speech			acoustic cryptanalysis;voice activity detection	Shambhu Shankar Bharti;Manish Gupta;Suneeta Agarwal	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-169692	machine learning;support vector machine;voice activity detection;mathematics;pattern recognition;artificial intelligence	Robotics	-14.262929499160517	-87.84157121169645	159507
9f00e9c9fd4245ad428792f73d18f52f248ccb54	the measurement and analysis on chinese mandarin syllable and phoneme articulation in band-limited condition		Chinese Mandarin speech articulation in band-limited condition were measured and analyzed by subjective evaluation experiments. The changing patterns of Chinese syllable, initial, final and tone articulation with cut-off frequency, bandwidth, center frequency and RBF were analyzed. Results show that Chinese syllable and phoneme articulation increase with the bandwidth or RBF rises and tends to get saturated gradually, the minimum bandwidth needed for achieving some certain articulation level is related to the center frequency. Under the same condition, tone articulation is highest, final articulation is higher than initial articulation, and syllable articulation is lowest. The best-fitting exponential function relationships between Chinese syllable, initial, final, tone articulation and RBF were respectively established on the basis of the least square method. Further, the perceptual relationships between syllable and initial, final, tone are analyzed, it is found that syllable articulation is highly correlated with initial, final and tone articulation, in which the relationship between syllable and initial is nearly linear, the relationship between syllable and final, tone are exponential, and the functional model of Chinese syllable articulation and initial, final, tone articulation is established.	bandlimiting;biconnected component;experiment;function model;radial basis function;super robot monkey team hyperforce go!;syllable;time complexity	Siyu Zhang;Hui Song	2017	2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2017.8302105	mandarin chinese;center frequency;artificial intelligence;manner of articulation;syllable;pattern recognition;cutoff frequency;computer science;limiting;bandwidth (signal processing)	Comp.	-9.889054137477128	-86.4906427976075	159541
ff0fea2aed560a0001ca581b7a34b41041c8eaea	speaker characterization using long-term and temporal information		This paper presents new techniques for front-end analysis using long-term and temporal information for speaker recognition. We propose a long-term feature analysis strategy that averages short-time spectral features over a period of time in an effort to capture the speaker traits that are manifested over a speech segment longer than a spectral frame. We found that the moving averages of temporal information are effective in speaker recognition as well. The experiments on the 2008 NIST Speaker Recognition Evaluation dataset show the longterm and temporal information contribute to substantial EER reductions.	enhanced entity–relationship model;experiment;speaker recognition	Chien-Lin Huang;Hanwu Sun;Bin Ma;Haizhou Li	2010			speech recognition;artificial intelligence;speaker recognition;nist;pattern recognition (psychology);pattern recognition;speaker diarisation;computer science;moving average	NLP	-12.145739580987295	-89.18981608058446	159544
9217e9c0f8b9fd27b0f38c52673cd9ec9dc45ac4	global variance equalization for improving deep neural network based speech enhancement	dnn based speech enhancement global variance equalization post processing post training modified object function reference speech estimated clean speech signal quality perceptual evaluation of speech quality pesq dnn training over smoothing deep neural network based speech enhancement;over smoothing speech enhancement global variance equalization deep neural networks;speech speech enhancement noise noise measurement training speech recognition;speech enhancement estimation theory neural nets smoothing methods	We address an over-smoothing issue of enhanced speech in deep neural network (DNN) based speech enhancement and propose a global variance equalization framework with two schemes, namely post-processing and post-training with modified object function for the equalization between the global variance of the estimated and the reference speech. Experimental results show that the quality of the estimated clean speech signal is improved both subjectively and objectively in terms of perceptual evaluation of speech quality (PESQ), especially in mismatch environments where the additive noise is not seen in the DNN training.	additive white gaussian noise;artificial neural network;deep learning;pesq;smoothing;speech enhancement;utility functions on indivisible goods;video post-processing	Yong Xu;Jun Du;Li-Rong Dai;Chin-Hui Lee	2014	2014 IEEE China Summit & International Conference on Signal and Information Processing (ChinaSIP)	10.1109/ChinaSIP.2014.6889204	voice activity detection;linear predictive coding;speech recognition;acoustics;computer science;speech coding;pattern recognition;psqm	HPC	-14.568037596472347	-91.54556394499232	159672
080c858879fd22c80a0ee21d1eb275fe6a9734bb	speech errors on frequently observed homophones in french: perceptual evaluation vs automatic classification.	acoustic analysis;contextual information;data mining;automatic speech recognition;automatic classification;language model	The present contribution aims at increasing our understanding of automatic speech recognition (ASR) errors involving frequent homophone or almost homophone words by confronting them to perceptual results. The long-term aim is to improve acoustic modelling of these items to reduce automatic transcription errors. A first question of interest is whether homophone words such as et, (and) and est (to be), for which ASR systems rely on language model weights, can be discriminated in a perceptual transcription test with similar n-gram constraints. A second question concerns the acoustic separability of the two homophone words using appropriate acoustic and prosodic attributes. The perceptual test reveals that even though automatic and perceptual errors correlate positively, human listeners in conditions attempting to approximate the information available for decision for a 4-gram language model deal with local ambiguity more efficiently than ASR systems. The corresponding acoustic analysis shows that the homophone words may be distinguished thanks to relevant acoustic and prosodic attributes. A first experiment in automatic classification of the two words using data mining techniques highlights the role of the prosodic (duration and voicing) and contextual information (co-occurrence of pauses). Preliminary results suggests that additional levels of information may be considered in order to efficiently represent and factorize the word variants observed in speech and to improve the automatic speech transcription.	acoustic cryptanalysis;approximation algorithm;data mining;language model;linear separability;n-gram;speech recognition;speech synthesis;transcription (software)	Rena Nemoto;Ioana Vasilescu;Martine Adda-Decker	2008			natural language processing;speech recognition;computer science;pattern recognition;acoustic model;language model;speech analytics	NLP	-12.130112499900251	-84.70502245186988	159698
5a8acbb252ec57cdfaf41f4aa4b2c684a0768e30	a reliable technique for detecting the second subglottal resonance and its use in cross-language speaker adaptation	speech recognition;maximum likelihood;indexing terms;data collection	In previous work [1], we proposed a speaker adaptation technique based on the second subglottal resonance (Sg2), which showed good performance relative to vocal tract length normalization (VTLN). In this paper, we propose a more reliable algorithm for automatically estimating Sg2 from speech signals. The algorithm is calibrated on children’s speech data collected simultaneously with accelerometer recordings from which Sg2 frequencies can be directly measured. To investigate whether Sg2 frequencies are independent of speech content and language, we perform a cross-language study with bilingual Spanish-English children. The study verifies that Sg2 is approximately constant for a given speaker and thus can be a good candidate for limited data speaker normalization and cross-language adaptation. We then present a cross-language speaker normalization method based on Sg2, which is computationally more efficient than maximum-likelihood based VTLN, and performs more robustly than VTLN.	algorithm;database normalization;resonance;sensor;tract (literature)	Shizhen Wang;Steven M. Lulich;Abeer Alwan	2008			speech recognition;normalization (statistics);artificial intelligence;computer science;maximum likelihood;pattern recognition;resonance;speaker recognition	ML	-12.869599199681607	-90.90289494803689	159904
c3afc1bc300da9803ccf54c1a7adf23df1176e02	articulatory-acoustic relationships during vocal tract growth for french vowels: analysis of real data and simulations with an articulatory model	control theory;young children;vocal tract;object relational	This paper reports on the articulatory–acoustic relationships involved during vocal tract growth. Data were taken from a database of ten French vowels uttered by 15 speakers ranging in age from 3 years old to adulthood. Despite the important acoustic variation encountered, one feature is displayed by all the speakers: the production of extreme focal vowels /i/, /u/, /]/, and /y/, realized with a strong concentration of spectral energy related to the proximity of two formant peaks. This feature represents an acoustic goal guiding the speaker’s task. Our simulations using an articulatory model demonstrate that the realization of the focalization feature may require different articulatory gestures for young children compared to adults, consisting of adaptive articulatory strategies exploited to compensate for the small pharynx of the former. Perceptual tests show that achieving focalization results in a lower intelligibility for the children than for the adults. Due to the relatively shorter pharyngeal cavity of the child compared to the adult, focalization cannot be achieved together with the perceptual objective related to rounded vowels /y/ in French. Results are discussed in light of the dispersionfocalization theory and the perception for action control theory (PACT). r 2006 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;articulatory synthesis;control theory;focal (programming language);intelligibility (philosophy);simulation;tract (literature)	Lucie Ménard;Jean-Luc Schwartz;Louis-Jean Boë;Jérôme Aubin	2007	J. Phonetics	10.1016/j.wocn.2006.01.003	psychology;vocal tract;speech recognition;acoustics;philosophy;linguistics;sociology;communication	AI	-9.827082696250658	-82.63147596396507	159924
e40974bff5fabdc1efbe1bea4f0beea734cdff27	phoneme class based adaptation for mismatch acoustic modeling of distant noisy speech.	distribution;acoustics;training;speech;mean;strategy;symposia;adaptation;models;acoustic equipment;phonemes	A new adaptation strategy for distant noisy speech is created by phoneme class based approaches for context-independent acoustic models. Unlike the previous approaches such as MLLR-MAP adaptation which adapts acoustic model to the features, our phoneme-class based adaptation (PCBA) adapts the distant data features to our acoustic model which has trained on close microphone TIMIT sentences. The essence of PCBA is to create a transformation strategy which makes the distribution of phoneme-classes of distant noisy speech be similar to those of close microphone acoustic model in thirteen dimensional MFCC space (mostly in c0-c1 plane). It creates a mean, orientation and variance adaptation scheme for each phoneme class to compensate the mismatch. New adapted features, and new and improved acoustic models which are produced by PCBA are outperforming those created by MLLR-MAP adaptation for ASR and KWS. And PCBA offers a new powerful understanding in acoustic-modeling of distant speech.	acoustic cryptanalysis;acoustic model;microphone;printed circuit board;timit	Seçkin Uluskan;John H. L. Hansen	2012			distribution;speech recognition;strategy;computer science;speech;linguistics;statistics;mean;adaptation	NLP	-15.563429608609589	-89.54062766471414	160100
1bb01755aa1d57fbbb2bb7f0a37ed34840977ddf	pitch synchronous residual excited speech reconstruction on the mfcc	speech;mel frequency cepstral coefficient;hidden markov models;vectors;production;speech recognition;speech mel frequency cepstral coefficient speech recognition vectors production hidden markov models	Practical applications of speech recognition and dialogue systems bring sometimes a requirement to synthesize or reconstruct the speech from the saved or transmitted mel-frequency cepstral coefficients (MFCCs). Presented paper describes several approaches to the speech reconstruction based on the MFCC parameterization. Approaches differ mainly in their various possible excitations. Let us mention that for designing a MFCC reconstruction module we applied some principles usually used in speech recognition and synthesis process. We suppose the speech reconstructor together with speech synthesizer and recognizer to be a part of a speech dialogue system developed in our department.	coefficient;dialog system;finite-state machine;mel-frequency cepstrum;reconstructor;speech recognition;speech synthesis;synchronous circuit	Zbynek Tychtl;Josef Psutka	2000	2000 10th European Signal Processing Conference	10.5281/zenodo.37501	voice activity detection;linear predictive coding;speech recognition;acoustics;computer science;speech coding;pattern recognition;speech processing	NLP	-13.793747602465137	-87.83986331314803	160427
29e9f436989b1a053610da3a8e3eb73a3e89098a	multi-modal recording and modeling of vocal tract movements	vocal tract;ema;ultrasound images;alignment	The complexity of vocal tract movement causes the difficult to record whole information of vocal tract during speech. Dynamic articulation has been acquired by implementing a variety of instruments, each of which has its advantages and shortcomings. However, the measurement of vocal tract movements is a difficult task to accomplish using one type of recording technique, and this has led to the simultaneous application of multiple instruments. Thus, we used an ultrasound system in combination with the electromagnetic articulography (EMA) system to record the multi-modality movement of the tongue. Data of the vocal tract movements were obtained by the ultrasound-based speech recording system developed by us, with which ultrasound images and synchronized audio signals are recorded synchronously. The EMA system is also used for the simultaneous collection of articulatory data with the audio. The EMA and ultrasound data were registered and matched to the same audio signal, after which these two sets of data were fused for each time point. In addition, a method for vocal tract shape reconstruction and modeling is proposed for the ultrasound dataset by using an active shape model. The averaged reconstruction error does not exceed 1.26 mm.	active shape model;biconnected component;interpolation;modal logic;modality (human–computer interaction);tract (literature)	Song Wang;Wenhuan Lu;Qingzhi Hou;Qiang Fang;Jianwu Dang	2015	Multimedia Tools and Applications	10.1007/s11042-015-3040-4	vocal tract;speech recognition	Graphics	-7.487493969800141	-85.64076229454778	160492
00e44e94d74b3b1a994615720bba9ae047fff623	discrimination of speech, musical instruments and singing voices using the temporal patterns of sinusoidal segments in audio signals	musical instruments;temporal pattern	We developed a method for discriminating speech, musical instruments and singing voices based on sinusoidal decomposition of audio signals. Although many studies have been conducted, few have worked on the problem of the temporal overlapping of the categories of sounds. In order to cope with such problems, we used sinusoidal segments with variable lengths as the discrimination units, although most of traditional work has used fixed-length units. The discrimination is based on the temporal characteristics of the sinusoidal segments. We achieved an average discrimination rate of 71.56% in classifying sinusoidal segments in non-mixed audio data. In the time segments, the accuracy 87.9% in non-mixed-category audio data and 66.4% in 2-mixed-category are achieved. In the comparison of the proposed and the MFCC methods, the effectiveness of temporal features and the importance of the use of both the spectral and temporal characteristics were proved.	monte carlo method	Toru Taniguchi;Akishige Adachi;Shigeki Okawa;Masaaki Honda;Katsuhiko Shirai	2005			speech recognition;computer science	HCI	-10.01840469670313	-90.1621432688084	160686
04835f507f9bd41677147eb4d0ddb8abed0c1d65	a preliminary study of mandarin filled pauses		The paper reports preliminary results on Mandarin filled pauses (FPs), based on a large speech corpus of Mandarin telephone conversation. We find that Mandarin intensively uses both demonstratives (zhege ‘this’, nage ‘that’) and uh/ mm as FPs. Demonstratives are more frequent FPs and are more likely to be surrounded by other types of disfluency phenomena than uh/mm, as well as occurring more often in nominal environments. We also find durational differences: FP demonstratives are longer than non-FP demonstratives, and mm is longer than uh. The study also revealed dialectal influence on the use of FPs. Our results agree with earlier work which shows that a language may divide conversational labor among different FPs. Our work also extends this research in suggesting that different languages may assign conversational functions to FPs in different ways.	speech corpus;super robot monkey team hyperforce go!	Yuan Zhao;Daniel Jurafsky	2005			speech recognition;acoustics;engineering;communication	NLP	-11.258761896799165	-81.56029083494752	160810
10296b6c9cb9297214c720748488d7f0d4eded0a	an information-theoretic framework for automated discovery of prosodic cues to conversational structure	manuals;sprakteknologi sprakvetenskaplig databehandling;speech manuals yttrium lead entropy;speech processing;speech;yttrium;lead;datavetenskap datalogi;entropy;speech segmentation information theoretic framework automated discovery prosodic cues conversational structure interaction timing conditonal mutual information measure;language technology computational linguistics;computer science;automated discovery interaction modeling speaking rate conditional mutual information neural networks	Interaction timing in conversation exhibits myriad variabilities, yet it is patently not random. However, identifying consistencies is a manually labor-intensive effort, and findings have been limited. We propose a conditonal mutual information measure of the influence of prosodic features, which can be computed for any conversation at any instant, with only a speech/non-speech segmentation as its requirement. We evaluate the methodology on two segmental features: energy and speaking rate. Results indicate that energy, the less controversial of the two, is in fact better on average at predicting conversational structure. We also explore the temporal evolution of model “surprise”, which permits identifying instants where each feature's influence is operative. The method corroborates earlier findings, and appears capable of large-scale data-driven discovery in future research.	information theory;mutual information;speech segmentation	Kornel Laskowski;Anna Hjalmarsson	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178998	natural language processing;entropy;lead;speech recognition;computer science;speech;yttrium;machine learning;speech processing	Visualization	-14.082750227737801	-82.38794237057148	160824
b97647e61f2b882d2d1410922b482d243dc6ebdb	delta, theta, beta, and gamma brain oscillations index levels of auditory sentence processing	brain oscillations;syntactic processing;auditory sentence;semantic processing;eeg;phonological processing;journal magazine article	A growing number of studies indicate that multiple ranges of brain oscillations, especially the delta (δ, <4Hz), theta (θ, 4-8Hz), beta (β, 13-30Hz), and gamma (γ, 30-50Hz) bands, are engaged in speech and language processing. It is not clear, however, how these oscillations relate to functional processing at different linguistic hierarchical levels. Using scalp electroencephalography (EEG), the current study tested the hypothesis that phonological and the higher-level linguistic (semantic/syntactic) organizations during auditory sentence processing are indexed by distinct EEG signatures derived from the δ, θ, β, and γ oscillations. We analyzed specific EEG signatures while subjects listened to Mandarin speech stimuli in three different conditions in order to dissociate phonological and semantic/syntactic processing: (1) sentences comprising valid disyllabic words assembled in a valid syntactic structure (real-word condition); (2) utterances with morphologically valid syllables, but not constituting valid disyllabic words (pseudo-word condition); and (3) backward versions of the real-word and pseudo-word conditions. We tested four signatures: band power, EEG-acoustic entrainment (EAE), cross-frequency coupling (CFC), and inter-electrode renormalized partial directed coherence (rPDC). The results show significant effects of band power and EAE of δ and θ oscillations for phonological, rather than semantic/syntactic processing, indicating the importance of tracking δ- and θ-rate phonetic patterns during phonological analysis. We also found significant β-related effects, suggesting tracking of EEG to the acoustic stimulus (high-β EAE), memory processing (θ-low-β CFC), and auditory-motor interactions (20-Hz rPDC) during phonological analysis. For semantic/syntactic processing, we obtained a significant effect of γ power, suggesting lexical memory retrieval or processing grammatical word categories. Based on these findings, we confirm that scalp EEG signatures relevant to δ, θ, β, and γ oscillations can index phonological and semantic/syntactic organizations separately in auditory sentence processing, compatible with the view that phonological and higher-level linguistic processing engage distinct neural networks.	acoustic cryptanalysis;antivirus software;artificial neural network;auditory perception;bands;brainwave entrainment;categories;electroencephalography phase synchronization;electronic signature;epilepsies, partial;experimental autoimmune encephalomyelitis;hearing loss, high-frequency;index;inference;interaction;linguistics;neural network simulation;neuroimaging;pseudo brand of pseudoephedrine;super robot monkey team hyperforce go!;syllable;type signature;version;sentence	Guangting Mai;James W. Minett;William S.-Y. Wang	2016	NeuroImage	10.1016/j.neuroimage.2016.02.064	psychology;natural language processing;neuroscience;speech recognition;semantic memory;communication	NLP	-9.0087107597752	-80.23763577420863	160844
6d50710f5e6fe6a6f5a5478e6792de16dbe12be7	backchannel-inviting cues in task-oriented dialogue	information technology;indexing terms;computer science;communication;interactive voice response	We examine BACKCHANNEL-INVITING CUES — distinct prosodic, acoustic and lexical events in the speaker’s speech that tend to precede a short response produced by the interlocutor to convey continued attention— in the Columbia Games Corpus, a large corpus of task-oriented dialogues. We show that the likelihood of occurrence of a backchannel increases quadratically with the number of cues conjointly displayed by the speaker. Our results are important for improving the coordination of conversational turns in interactive voice-response systems, so that systems can produce backchannels in appropriate places, and so that they can elicit backchannels from users in expected places.	acoustic cryptanalysis;backchannel	Agustín Gravano;Julia Hirschberg	2009			natural language processing;speech recognition;index term;computer science;information technology	NLP	-12.676546250345893	-81.34730542489037	160876
67fbad34396c6d51ac281523f89c1db8d8efcc60	muscle forces in vowel vocal tract formation	vocal tract		tract (literature)	Katherine S. Harris;Eric Vatikiotis-Bateson;Peter J. Alfonso	1992			speech recognition;vowel;computer science;vocal tract	Robotics	-8.34183163415112	-84.25113508362686	160884
278174d1bf613f0886dd612616dbdf78cef0c7d8	the evaluation of the stability of acoustic features in affective conveyance across multiple emotional databases			acoustic cryptanalysis;affective computing;database	Rui Sun	2013			communication;affect (psychology);natural language processing;artificial intelligence;psychology	NLP	-13.086406157596231	-84.91396036800697	160887
96c70cd2f325a5200d70120472f79ce959c2d68b	hearing aid speech enhancement: a multiresolution analysis approach	multiresolution analysis		multiresolution analysis;speech enhancement	Soundararajan Ezekiel;William Oblitey;Robert Trimble	2005			computer science;multiresolution analysis;hearing aid;speech enhancement;speech recognition	SE	-13.398374368662477	-87.00198485235589	160888
b9b8fab91f8272a568c022e5594c7c2a43931b29	new warped lpc-based feature for fast and robust speech/music discrimination		Automatic discrimination of speech and music is an important tool in many multimedia applications. The paper presents a low complexity but effective approach for speech/music discrimination, which exploits only one simple feature, called Warped LPC-based Spectral Centroid (WLPC-SC). A three-component Gaussian Mixture Model (GMM) classifier is used because it showed a slightly better performance than other Statistical Pattern Recognition (SPR) classifiers. Comparison between WLPC-SC and the timbral features proposed in [11] is performed, aiming to assess the good discriminatory power of the proposed feature. Experimental results reveal that our speech/music discriminator is robust and fast, making it suitable for realtime multimedia applications.	discriminator;google map maker;mixture model;pattern recognition;spectral centroid;tree rearrangement;warped linear predictive coding	J. Enrique Muñoz Expósito;Sebastian García Galán;Nicolás Ruiz-Reyes;Pedro Vera-Candeas;F. Rivas-Peña	2005			speech recognition;acoustics;pattern recognition	Vision	-10.842408003636045	-91.68160631149777	161004
8867f94c6f7b9c3e228701b4880f297d4362a9b2	effects of speaking rate on temporal patterns of english	variabilite individuelle;speaking rate;phonetique;individual variability;speech delivery;etude experimentale;natural variation;production de la parole;vol 59;rate of speech;phonetica 2002;temporal structure;anglais;temporal pattern;debit oratoire;structure temporelle;vitesse d elocution;english;phonetics;228912;no 4;speech production	Individual subjects have been found to show considerable variation in the extent to which they manifest certain temporal patterns of English in their speech. Because at least some differences across speakers appear to be related to the fact that some subjects typically talk faster/slower than others, the present investigation examined variability in temporal patterns by having subjects produce target stimuli in sentences spoken at normal and fast rates. The results from these manipulations of speaking rate generally supported previous findings related to temporal patterns as a function of 'natural' variations in rate of speech. That is, there was considerable variability among the 15 subjects in the normal speaking rate condition in the extent to which temporal parameters such as final-syllable vowel lengthening occurred. In addition, there was substantial variation in these parameters in the fast speaking rate condition, and some systematic changes in certain patterns also occurred as a function of the rate change. For example, vowel lengthening preceding voiced obstruents tended to decrease when subjects spoke at a fast versus a normal rate, whereas phrase-final vowel lengthening was typically greater when they spoke at a faster rate.		Bruce L. Smith	2002	Phonetica	10.1159/000068348	psychology;phonetics;speech production;speech recognition;philosophy;english;linguistics;sociology;communication	Security	-10.468195310307486	-81.74551153309747	161008
edceba2cbe322b805de4e2d6fea4ec9ed98cd99d	the synthesis of sung vowels in female opera and belt qualities	arte		opera (web browser)	Michelle Evans;David M. Howard	1996			speech recognition;acoustics;engineering;communication	NLP	-7.71609631265873	-84.58832748393559	161058
23c46348ecefe74a4b85bcdf872d34b44ddcdc13	a multimedia framework for effective language training	learning process;learning;dyslexia;computer graphic;linguistic analysis;large scale;visualization;visual information;natural language;tree structure;coding;entropy;language model;information theory	We present a novel framework for the multimodal display of words using topological, appearance, and auditory representations. The methods are designed for effective language training and serve as a learning aid for individuals with dyslexia. Our topological code decomposes the word into its syllables and displays it graphically as a tree structure. The appearance code assigns color attributes and shape primitives to each letter and takes into account conditional symbol probabilities, code ambiguities, and phonologically confusable letter combinations. An additional auditory code assigns midi events to each symbol and thus generates a melody for each input string. The entire framework is based on information theory and utilizes a Markovian language model derived from linguistic analysis of language corpora for English, French, and German. For effective word repetition a selection controller adapts to the user’s state and optimizes the learning process by minimizing error entropy. The performance of the method was evaluated in a large scale experimental study involving 80 dyslexic and non-dyslexic children. The results show significant improvements in writing skills in both groups after small amounts of daily training. Our approach combines findings from 3D computer graphics, visualization, linguistics, perception, psychology, and information theory.	3d computer graphics;experiment;information theory;language model;midi;machine perception;multimedia framework;multimodal interaction;speech repetition;text corpus;tree structure	Markus H. Gross;Christian Voegeli	2007	Computers & Graphics	10.1016/j.cag.2007.09.001	natural language processing;dyslexia;computer vision;entropy;speech recognition;visualization;information theory;computer science;artificial intelligence;tree structure;coding;natural language;programming language;algorithm;statistics;language model;computer graphics (images)	NLP	-15.794130642182983	-81.01078705921215	161082
0d836a0461e9c21fa7a25622115de55b81ceb446	estimation of presentations skills based on slides and audio features	presentation skills;multimodal learning analytics;slides features;audio features	This paper proposes a simple estimation of the quality of student oral presentations. It is based on the study and analysis of features extracted from the audio and digital slides of 448 presentations. The main goal of this work is to automatically predict the values assigned by professors to different criteria in a presentation evaluation rubric. Machine Learning methods were used to create several models that classify students in two clusters: high and low performers. The models created from slide features were accurate up to 65%. The most relevant features for the slide-base models were: number of words, images, and tables, and the maximum font size. The audio-based models reached up to 69% of accuracy, with pitch and filled pauses related features being the most significant. The relatively high degrees of accuracy obtained with these very simple features encourage the development of automatic estimation tools for improving presentation skills.	automatic control;machine learning;simple features;trusted computer system evaluation criteria	Gonzalo Luzardo;Bruno Guamán;Katherine Chiluiza;Jaime Castells;Xavier Ochoa	2014		10.1145/2666633.2666639	simulation;speech recognition;computer science;multimedia	HCI	-17.259279592392573	-81.09091281269046	161130
185eda6a982f40e86b2c2f7f25888a5777e8e69d	frequency and time filtering of filter-bank energies for hmm speech recognition	cepstral analysis;correlation methods;filtering theory;hidden markov models;spectral analysis;speech recognition;hmm speech recognition;cepstral representations;computationally inexpensive filter;decorrelation;discriminative frequency weighting;frequency filtering;frequency sequence;hidden markov model;log mel-scaled filter-bank energies;spectral parameters;speech signals;time filtering	In speech recognition, a discriminative quefrency weighting can be achieved by somewhat decorrelating the frequency sequence of log mel-scaled filter-bank energies with a computationally inexpensive filter. In this paper, we show how the spectral parameters that result from this kind of frequency filtering, both alone and combined with filtering of their time trajectories, are competitive with respect to the conventional cepstral representations of speech signals.	cepstrum;decorrelation;filter bank;hidden markov model;speech recognition	Climent Nadeu;José B. Mariño;Javier Hernando;Albino Nogueiras	1996			speech recognition;computer science;pattern recognition;statistics	ML	-12.464616074870102	-91.53909038990842	161145
e7159e623a8dec65e698f7eda7dfdff325097a92	pitch modelling for the nguni languages	southern africa;rule based;intonation modelling;neural network classifier;pitch tracking;nguni languages;prosody;intonation corpus;article;fundamental frequency	Although the complexity of prosody is widely recognised, the lack of widely-accepted descriptive standards for prosodic phenomena has meant that prosodic systems for most of the languages of the world have, at best, been described in impressionistic rule-based terms. For the languages of Southern Africa, the deficiencies in our modelling capabilities are acute. Little work of a quantitative nature has been published for the languages of the Nguni family (such as isiZulu and isiXhosa), and there are significant contradictions and imprecisions in the literature on this topic, which partially stems from the lack of quantitative, measurement-driven analysis. This paper therefore embarks on a programme aimed at understanding the relationship between linguistic and physical variables of a prosodic nature in this family of languages. Firstly we undertake a set of experiments to select an appropriate pitch tracking algorithm for the the Nguni family of languages. We then use this pitch tracking algorithm to extract relevant data from speech recordings to build intonation corpora for isiZulu and isiXhosa. Using the extracted data in the intonation corpus, we show that it is possible to develop fairly accurate intonation models using a neural network classifier for isiZulu and isiXhosa.	artificial neural network;experiment;logic programming;pitch detection algorithm;semantic prosody;text corpus	Natasha Govender;Etienne Barnard;Marelie H. Davel	2007	South African Computer Journal		rule-based system;speech recognition;computer science;fundamental frequency;prosody	NLP	-16.373461809438197	-83.62334653631684	161151
db1e1b9b548f2ccb127b848cf7d2f3f78e1023dd	prediction of handwriting legibility	belief networks;probability neural network;bayesian classification;electronic mail;probability;handwriting recognition;neural networks;neural nets;image contour;handwriting legibility prediction;probability handwriting recognition belief networks neural nets;bayesian methods;linear discriminant function handwriting legibility prediction independent handwriting style classifier feature extraction phase image contour probability neural network bayesian decision parzen method;parzen method;density functional theory;linear discriminant function;phase estimation;feature extraction;writing;humans;bayesian decision;iris;feature extraction phase;institutional repository research archive oaister;handwriting recognition writing feature extraction bayesian methods humans electronic mail iris neural networks phase estimation density functional theory;density functional;independent handwriting style classifier;neural network	"""The Write-On Handwriting products are developmentally sound — supported by research as well as years of experience teaching individuals and groups. Since handwriting is a complex skill, it was essential to incorporate well-recognized developmental components into the handwriting instruction plan. Write-On Handwriting incorporates two key developmental skills: the graphomotor function (sequencing pencil movements) and the retrieval memory function. Applicable research providing the basis for the methodology developed by Write-On Handwriting follows. In addition, foundational research is outlined that supports the importance of teaching handwriting to insure future academic success. In clinics and schools, children frequently struggle with motor memory. For them, the retrieval of motor patterns is a slow, strenuous, and sometimes futile exercise. Writing can be thought of as a rotating drum connected to memory: it is a tracing, an indelible record of what is recalled and then transmitted through the writing implement. Children with motor memory deficits have characteristic handwriting, marred by frequent hesitation, retracing and illegibility. Memory disorders of this type, in fact, may be the most common cause of poor handwriting (Levine, Mel Dr., """" Developmental Variation and Learning Disorders """" , 1999). It is argued here that automatic legible writing is an essential basis for written expression. And yet, crowded school curricula and neglect by educational institutions and researchers often leave no room for appropriate and sufficient attention to teaching this critical skill.. .. There are at least three reasons handwriting must be carefully taught to all children. First, handwriting allows access to kinesthetic memory — our earliest, strongest and most reliable memory channel. Second, serviceable handwriting needs to be at a spontaneous level so that a student is free to concentrate on spelling, and to focus on higher-level thought and written expression. Third, teachers judge and grade students based on the appearance of their work Direct kinesthetic steps to teaching handwriting leaves nothing to chance in developing writing skill to its highest level.. .. Essentially all children learn to write more expeditiously using kinesthetic techniques (Benbow, Mary, """" The Way to Go! Kinesthetic Approach to Handwriting """" , 1990)."""	go!;memory bound function;spontaneous order;tracing (software);way to go	Mandana Ebadian Dehkordi;Nasser Sherkat;Tony Allen	2001		10.1109/ICDAR.2001.953935	naive bayes classifier;speech recognition;feature extraction;bayesian probability;computer science;machine learning;pattern recognition;probability;density functional theory;writing;artificial neural network	ML	-5.182068303934797	-81.22862113009346	161183
59f883a44759b43cd9f982e8505fdfa8261cb92d	a neural network based algorithm for speaker localization in a multi-room environment	neural networks;estimation;transforms;microphone arrays;signal processing algorithms;direction of arrival estimation	A Speaker Localization algorithm based on Neural Networks for multi-room domestic scenarios is proposed in this paper. The approach is fully data-driven and employs a Neural Network fed by GCC-PHAT (Generalized Cross Correlation Phase Transform) Patterns, calculated by means of the microphone signals, to determine the speaker position in the room under analysis. In particular, we deal with a multi-room case study, in which the acoustic scene of each room is influenced by sounds emitted in the other rooms. The algorithm is tested against the home recorded DIRHA dataset, characterized by multiple wall and ceiling microphone signals for each room. In particular, we focused on the speaker localization problem in two distinct neighbouring rooms. We assumed the presence of an Oracle multi-room Voice Activity Detector (VAD) in our experiments. A three-stage optimization procedure has been adopted to find the best network configuration and GCC-PHAT Patterns combination. Moreover, an algorithm based on Time Difference of Arrival (TDOA), recently proposed in literature for the addressed applicative context, has been considered as term of comparison. As result, the proposed algorithm outperforms the reference one, providing an average localization error, expressed in terms of RMSE, equal to 525 mm against 1465 mm. Concluding, we also assessed the algorithm performance when a real VAD, recently proposed by some of the authors, is used. Even though a degradation of localization capability is registered (an average RMSE equal to 770 mm), still a remarkable improvement with respect to the state of the art performance is obtained.	acoustic cryptanalysis;algorithm;applicative programming language;elegant degradation;experiment;mathematical optimization;microphone;multilateration;neural networks;voice activity detection	Fabio Vesperini;Paolo Vecchiotti;Emanuele Principi;Stefano Squartini;Francesco Piazza	2016	2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2016.7738817	speech recognition;telecommunications;engineering;communication	ML	-10.75094526757094	-93.37625947473505	161392
cd91e3109b0d24f034639fc6bb50d0b58d80467b	a study on demonstrative words extraction in instructor utterance on communication support for hearing impaired persons	hearing impaired;support system;cross validation	Remote transcription system is one of the communication support systems for hearing impaired students in a lecture. Because the remote transcription system requires several seconds from the utterance of the instructor to displaying the summarized text, hearing impaired students cannot associate the demonstrative word. In this paper, we analyze how demonstrative words appear in the utterance of instructor. About 75% of demonstrative words follow the pause and are uttered within 500ms after the pause. We propose a method for extracting demonstrative words based on acoustic features and conducted experiments. Experimental results show that the recall rate is more than 80% but the relevance rate is less than 20% in the cross validation test.	acceptance testing;acoustic cryptanalysis;cross-validation (statistics);experiment;relevance;sensitivity and specificity;transcription (software)	Ayaka Ito;Ken Saito;Yoshinori Takeuchi;Noboru Ohnishi;Shigeyoshi Iizuka;Shinya Nakajima	2008		10.1007/978-3-540-70540-6_90	speech recognition;engineering;communication;audiology	NLP	-16.476710172072828	-82.79057470015702	161400
6999676c337efd895f3c318c806269aea2ee480f	architectures of neural networks applied for lvcsr language modeling	neural network architectures;language modeling;self organized maps;speech recognition	The n-gram model and its derivatives are both widely applied solutions for Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, Slavonic languages require a language model that considers word order less strictly than English, i.e. the language that is the subject of most linguistic research. Such a language model is a necessary module in LVCSR systems, because it increases the probability of finding the right word sequences. The aim of the presented work is to create a language module for the Polish language with the application of neural networks. Here, the capabilities of Kohonen's Self-Organized Maps will be explored to find the associations between words in spoken utterances. To fulfill such a task, the application of neural networks to evaluate sequences of words will be presented. Then, the next step of language model development, the network architectures, will be discussed. The network proposed for the construction of the considered model is inspired by the Cocke-Young-Kasami parsing algorithm.	artificial neural network;language model;speech analytics	Leszek Gajecki	2014	Neurocomputing	10.1016/j.neucom.2013.11.033	natural language processing;language identification;cache language model;speech recognition;computer science;machine learning;modeling language;language model	NLP	-18.259525673585408	-86.0643360378427	161460
e2f015bbddd7bade7caca693e37f84c4cf70a7f5	coupled initialization of multi-channel non-negative matrix factorization based on spatial and spectral information		Multi-channel non-negative matrix factorization (MNMF) is a multi-channel extension of NMF and often outperforms NMF because it can deal with spatial and spectral information simultaneously. On the other hand, MNMF has a larger number of parameters and its performance heavily depends on the initial values. MNMF factorizes an observation matrix into four matrices: spatial correlation, basis, cluster-indicator latent variables, and activation matrices. This paper proposes effective initialization methods for these matrices. First, the spatial correlation matrix, which shows the largest initial value dependencies, is initialized using the cross-spectrum method from enhanced speech by binary masking. Second, when the target is speech, constructing bases from phonemes existing in an utterance can improve the performance: this paper proposes a speech bases selection by using automatic speech recognition (ASR). Third, we also propose an initialization method for the cluster-indicator latent variables that couple the spatial and spectral information, which can achieve the simultaneous optimization of above two matrices. Experiments on a noisy ASR task show that the proposed initialization significantly improves the performance of MNMF by reducing the initial value dependencies.	activation function;asch conformity experiments;automated system recovery;latent variable;mathematical optimization;non-negative matrix factorization;speech recognition	Yuuki Tachioka;Tomohiro Narita;Iori Miura;Takanobu Uramoto;Natsuki Monta;Shingo Uenohara;Ken'ichi Furuya;Shinji Watanabe;Jonathan Le Roux	2017			spatial correlation;initialization;pattern recognition;artificial intelligence;computer science;communication channel;non-negative matrix factorization	NLP	-15.502318770651897	-90.96966739941068	161717
619f40e4c7a6592d2b2bd0a6e424cdf1111046ed	improvement of speaker vector-based speaker verification	speaker identification;speaker recognition gaussian processes hidden markov models;hidden markov models speaker vector based speaker verification text independent speaker verification anchor models phonetic based models gaussian mixture models speaker identification;gaussian processes;hidden markov model;training;phonetic based models;kl transform;speech;anchor models;data mining;speaker vector based speaker verification;speaker verification;speaker recognition;text independent speaker verification;performance improvement;gaussian mixture model;kl transform speaker recognition speaker verification hidden markov model hmm gaussian mixture model gmm;hidden markov models;indexing;gaussian mixture models;transforms;hidden markov model hmm;speech recognition;gaussian mixture model gmm;hidden markov models loudspeakers karhunen loeve transforms indexing information security computer vision speech	This paper describes the improvement in the performance of a text-independent speaker verification based on a speaker vector. The verification system is based on the technique of anchor models. In our previous work, the performance improvement could be obtained by using phonetic-based models instead of Gaussian mixture models (GMMs) in speaker identification. This is because the phonetic models can represent a detailed difference in pronunciation. Therefore, we aim to improve the performance of speaker verification by using phonetic-based modeling. Comparative experiments between GMMs and Hidden Markov Models (HMMs) were conducted in the speaker verification task. In the experiments, the EER of 2.68% was obtained at 1000-dimensional speaker space when HMMs were used as anchor models.	consistency model;enhanced entity–relationship model;experiment;google map maker;hidden markov model;markov chain;mixture model;principal component analysis;speaker recognition	Naoki Tadokoro;Tetsuo Kosaka;Masaharu Katoh;Masaki Kohda	2009	2009 Fifth International Conference on Information Assurance and Security	10.1109/IAS.2009.162	speaker recognition;speaker diarisation;speech recognition;machine learning;pattern recognition	SE	-16.185968203579126	-92.58554478034475	161734
c2fb4113ea3a8470e0540a6d74f2143b0dd475fa	the perception of charisma from voice: a cross-cultural study	acoustics;speech processing;semantics;speech;voice;lead;culture;political speech charisma voice culture;speech acoustics cultural differences lead semantics correlation acoustic measurements;correlation;cultural aspects;acoustic measurements;extraversion introversion cross cultural study acoustic parameters charismatic voice mascharp temporal structure pitch structure italian politician french politician proactive attractive dimensions calm benevolent dimensions;political speech;speech processing cultural aspects natural language processing politics;natural language processing;politics;charisma;cultural differences	This paper provides an overview of previous works on the acoustic parameters of charismatic voice and illustrates MASCharP, a scale for measuring the perception of charisma in voice. A study is then presented on the perception of charisma through the temporal and pitch structure of the voices of an Italian and a French politician. Results show some cultural differences in charisma perception and how acoustic features such as pitch (normal, higher, or lower) and types of pauses (short or long) can affect the Proactive-Attractive and Calm-Benevolent dimensions of charisma. The same dimension of charisma can be conveyed by different acoustic correlates of voice by connecting them to the dimension of leader extraversion-introversion.	acoustic cryptanalysis;pitch (music);proactive parallel suite;subversion	Francesca D'Errico;Rosario Signorello;Didier Demolin;Isabella Poggi	2013	2013 Humaine Association Conference on Affective Computing and Intelligent Interaction	10.1109/ACII.2013.97	psychology;politics;lead;charisma;speech;speech processing;semantics;linguistics;communication;social psychology;correlation;voice;culture	Robotics	-10.866052979100939	-82.07041041390679	161756
68ee5ba843189e3ebd63bb69b122e3e7d903dd40	time-warped longest common subsequence algorithm for music retrieval	time warp;music retrieval;selected works;longest common sub sequence;longest common subsequence;music information retrieval;bepress;music transcription	Recent advances in music information retrieval have enabled users to query a database by singing or humming into a microphone. The queries are often inaccurate versions of the original songs due to singing errors and errors introduced in the music transcription process. In this paper, we present the Time-Warped Longest Common Subsequence algorithm (T-WLCS), which deals with singing errors involving rhythmic distortions. The algorithm is employed on song retrieval tasks, where its performance is compared to the longest common subsequence algorithm.	algorithm;distortion;information retrieval;longest common subsequence problem;microphone;transcription (software)	AnYuan Guo;Hava T. Siegelmann	2004			speech recognition;computer science;longest common subsequence problem;world wide web	Web+IR	-7.836666764302652	-93.56794802618519	161762
25c698d5586f048010bb0d12f1f37483e50c79a6	trainable speech synthesis with trended hidden markov models	speech synthesis;hidden markov model;source speaker identity trainable speech synthesis system hmm trended hidden markov model trajectories generation spectral features synthesis units transcribed continuous speech corpus psola synthesiser natural sounding speech;hidden markov models;spectral analysis;speech synthesis hidden markov models databases synthesizers context modeling testing industrial training maximum likelihood linear regression laboratories systems engineering and theory;hidden markov models speech synthesis spectral analysis	In this paper we present a trainable speech synthesis system that uses the trended Hidden Markov Model to generate the trajectories of spectral features of synthesis units. The synthesis units are trained from a transcribed continuous speech corpus, making the speech more natural than that produced by conventional diphone synthesisers which are generally trained from a highly articulated speech database and require a large investment of time and effort in order to train a new voice. The overall system has been incorporated into a PSOLA synthesiser to produce speech that is natural sounding and preserves the identity of the source speaker.	automatic sounding;hidden markov model;markov chain;psola;speech corpus;speech synthesis	John Dines;Sridha Sridharan	2001		10.1109/ICASSP.2001.941044	natural language processing;speech recognition;computer science;machine learning;pattern recognition;hidden markov model	ML	-17.72517580981394	-90.78940531778784	161801
1bc55bef2be99e03183c05c03e5eb0f00f60ff8c	crss systems for 2012 nist speaker recognition evaluation	speaker recognition calibration;system performance;speaker recognition evaluation;speaker recognition;quality measure fusion feature normalization nist sre robust features speaker verification;calibration crss systems 2012 nist speaker recognition evaluation center for robust speech systems national institute of standards and technology noisy test conditions short duration test conditions robust acoustic features feature normalization schemes back end strategies multi session training multi condition training quality measure based system fusion active speech duration signal to noise ratio snr estimation;signal to noise ratio abstracts mel frequency cepstral coefficient filter banks smoothing methods cepstrum training;system development;quality measures;signal to noise ratio;calibration;national institute of standards and technology	This paper describes the systems developed by the Center for Robust Speech Systems (CRSS), for the 2012 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE). Given that the emphasis of SRE'12 is on noisy and short duration test conditions, our system development focused on: (i) novel robust acoustic features, (ii) new feature normalization schemes, (iii) various back-end strategies utilizing multi-session and multi-condition training, and (iv) quality measure based system fusion. Noisy and short duration training/test conditions are artificially generated and effectively utilized. Active speech duration and signal-to-noise-ratio (SNR) estimates are successfully employed as quality measures for system calibration and fusion. Overall system performance was very successful for the given test conditions.	acoustic cryptanalysis;signal-to-noise ratio;speaker recognition	Taufiq Hasan;Seyed Omid Sadjadi;Gang Liu;Navid Shokouhi;Hynek Boril;John H. L. Hansen	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638975	speaker recognition;calibration;speech recognition;computer science;pattern recognition;signal-to-noise ratio	Robotics	-13.615700269045893	-90.62640871013852	161834
3a1c53a4f5312a79f2380ce9160d8ac8fd124f9d	a segmental dnn/i-vector approach for digit-prompted speaker verification		DNN/i-vectors have achieved state-of-the-art performance in text-independent speaker verification systems. For such systems, the UBM posteriors are replaced with the DNN posteriors when training the i-vector extractor to better model the phonetic space. However, the DNN/i-vector systems have limited success on text-dependent speaker verification systems as the lexical variabilities, which are important for such applications, are suppressed in the utterance-level i-vectors. In this paper, we propose a segmental DNN/i-vector approach for the digit-prompted speaker verification task. Specifically, we segment the utterance into digits and model each digit using an individual DNN/i-vector system. By modeling the variability for each digit independently, we can focus more on the speaker characteristics for each digit. To take into consideration the uncertainties in the DNN posteriors, we propose a confidence measure based weighting method. On the RSR2015 dataset, the proposed approach yields an equal error rate of 3.44%, compared to 5.76% of the baseline utterance-level DNN/i-vector system and 4.54% of the joint factor analysis (JFA) system.	baseline (configuration management);context tree weighting;enhanced entity–relationship model;experiment;factor analysis;randomness extractor;spatial variability;speaker recognition	Jie Yan;Lei Xie;Guangsen Wang;Zhong-Hua Fu	2017	2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)	10.1109/APSIPA.2017.8281992	task analysis;word error rate;utterance;data modeling;numerical digit;extractor;hidden markov model;artificial intelligence;computer science;weighting;pattern recognition	SE	-17.000930478410428	-90.38739861226635	161866
7763288e7de389932d8e4450add3e18755c9382b	speaking rate estimation based on deep neural networks		In this paper we propose a method for estimating speaking rate by means of Deep Neural Networks (DNN). The proposed approach is used for speaking rate adaptation of an automatic speech recognition system. The adaptation is performed by changing step in front-end feature processing according to the estimations of speaking rate. Experiments show that adaptation results using the proposed DNN-based speaking rate estimator are better than the results of adaptation using the speaking rate estimator based on the recognition results.		Natalia A. Tomashenko;Yuri Y. Khokhlov	2014		10.1007/978-3-319-11581-8_52	artificial intelligence;machine learning;pattern recognition	EDA	-14.611739620001941	-90.87082323298237	161901
1fa9ae7f3ee9faf47ab378fa9a6055d844a855b0	a text-independent method for estimating pronunciation quality of chinese students		In this paper, a novel approach is proposed for text-independent pronunciation quality assessment of Chinese students. We call the proposed method as double-models pronunciation scoring algorithm, which separates recognition from assessment stage. It can solve low recognition performance of standard method and score mismatch of nonstandard one. Applying the combination of Maximum Likelihood Linear Regression and Maximum A Posteriori adaptation achieves good recognition results for speech of Chinese students. Adjustment of scoring features signifies further improvement in correlation between machine scores and human judgment. The experimental results showed the proposed double-models technique reached good outcome for text-independent pronunciation quality assessment of Chinese students.		Guimin Huang;Huijuan Li;Rong Zhou;Ya Zhou	2015		10.1007/978-3-319-38771-0_20	scoring algorithm;maximum likelihood;linear regression;maximum a posteriori estimation;computer science;pronunciation;artificial intelligence;pattern recognition	NLP	-18.07529237112967	-84.995684566663	162055
bf3ad2883857730386565f392271799a79ba928e	development of crim system for the automatic speaker verification spoofing and countermeasures challenge 2015		The automatic speaker verification spoofing and countermeasures challenge 2015 provides a common framework for the evaluation of spoofing countermeasures or anti-spoofing techniques in the presence of various seen and unseen spoofing attacks. This contribution proposes a system consisting of amplitude, phase, linear prediction residual, and combined amplitude phase-based countermeasures for the detection of spoofing attacks. In this task we use following features: Mel-frequency cepstral coefficients (MFCC), product spectrum-based cepstral coefficients, modified group delay cepstral coefficients, weighted linear prediction group delay cepstral coefficients, linear prediction residual cepstral coefficients, cosine normalized phase-based cepstral features (CNPCC), and a combination of MFCC-CNPCC. The product spectrum-based features are influenced by both the amplitude and phase spectra. The Gaussian Mixture Model (GMM) classifier is used for the discrimination of the human and spoofed speech signals. Our primary submitted system is a linear fusion of the sub-systems based on the features mentioned above with fusion weights trained on the development dataset. Experimental results on the challenge evaluation data provided an average EER (equal error rate) of 0.041%, 5.347%, and 2.69% on the known, unknown and all (known + unknown) spoofing attacks, respectively. Among all the systems product spectrum-based cepstral coefficientsand conventional MFCC (without any feature normalization)based systems performed the best in terms of EER measure. On the known, unknown and all conditions the EER obtained by the MFCC and product spectrum-based features are 0.78% & 0.65%, 5.39% & 5.37% and 3.09% & 3.01%, respectively.	coefficient;countermeasure (computer);enhanced entity–relationship model;google map maker;group delay and phase delay;mel-frequency cepstrum;mixture model;speaker recognition;spoofing attack	Md. Jahangir Alam;Patrick Kenny;Gautam Bhattacharya;Themos Stafylakis	2015			speech recognition;pattern recognition	NLP	-11.268831536880642	-91.90349680134649	162359
2c8d291db3198da9a3144e4523a930b8a12d775e	tree distributions approximation model for robust discrete speech recognition		This paper proposes a new discrete speech recognition method which investigates the capability of graphical models based on tree distributions that are widely used in many optimization areas. A novel spanning tree structure that utilizes the temporal nature of speech signal is proposed. The proposed tree structure significantly reduces complexity in so far that can reflect simply a few essential relationships rather than all possible structures of trees. The application of this model is illustrated with different isolated word databases. Experimentally it has been shown that, the proposed approaches compared to the conventional discrete hidden Markov model (DHMM) yield reduced error rates of 2.54 %–12 % and improve recognition speed minimum 3-fold. In addition, an impressive gain in learning time is observed. The overall recognition accuracy was 93.09 %– 95.34 %, thereby confirming the effectiveness of the proposed methods.	acoustic cryptanalysis;approximation;computational complexity theory;database;experiment;file spanning;graphical model;hidden markov model;interaction;markov chain;mathematical optimization;spanning tree;speech recognition;tree structure	Nacereddine Hammami;Mouldi Bedda;Nadir Farah	2012	I. J. Speech Technology	10.1007/s10772-012-9141-9	speech recognition;artificial intelligence;machine learning;statistics	ML	-16.97566226132659	-92.91793372058436	162568
c5f0108e6661377723818b283bf0391150dc5dfc	off-line handwritten word recognition with explicit character juncture modeling	vector quantization;principal component analysis;character juncture modeling hidden markov models;handwritten word recognition			Wongyu Cho;Jin Hyung Kim	1995	IEICE Transactions		natural language processing;speech recognition;computer science;intelligent word recognition;machine learning;vector quantization;principal component analysis	Vision	-15.236207680114505	-87.60866265889373	162822
f1eb2054949e85f8510fbccdee05bcac438cd581	emphasis and tonal implementation in standard chinese	mandarin chinese;article letter to editor;range expansion;lexical tone;speech communication	Despite the greatly improved understanding of tonal articulation in Standard Chinese, no consensus has been reached on the most appropriate model of tonal implementation [Xu, Y., & Wang, Q. (2001). Pitch targets and their realization: Evidence from Mandarin Chinese. Speech Communication, 33, 319–337; Kochanski, G., & Shih, C. (2003). Prosody modeling with soft templates. Speech Communication, 39(3/4), 311–352]. To shed new light on the issue, all four lexical tones, embedded in sentences with different preceding and following tonal contexts, were elicited under corrective focus, with two degrees of emphasis (Emphasis and MoreEmphasis), in addition to a NoEmphasis base-line condition, so as to bring systematic variation in duration and F0 to bear on the issue of tonal realization in different pragmatic contexts. Results showed comparable increases in syllable duration from the NoEmphasis condition to the Emphasis condition and from the latter to the MoreEmphasis condition. F0 range expansion, however, was non-gradual: while there was a substantial increase in the F0 range from the NoEmphasis to the Emphasis condition, the expansion from the Emphasis to the MoreEmphasis condition was marginal. Analyses of the F0 patterns revealed that under emphasis, lexical tones were realized with magnified F0 contours which were adapted to both the neighbouring tones and the durational increase of the tone-bearing syllables, and therefore maximally distinguishable from each other. Implications of these findings on models of tone and focus realization are discussed. r 2008 Elsevier Ltd. All rights reserved.	biconnected component;embedded system;marginal model;pitch shift;semantic prosody;super robot monkey team hyperforce go!;syllable	Yiya Chen;Carlos Gussenhoven	2008	J. Phonetics	10.1016/j.wocn.2008.06.003	speech recognition;philosophy;mandarin chinese;computer science;speech;linguistics;sociology;communication	NLP	-11.019855138089332	-81.25659859988647	162823
edcbd50eb5e10359c4e6a345fe9647c1a9c5fb9c	differences in glottal stop perception between english and japanese listeners	ucl;discovery;theses;conference proceedings;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;ucl research	This study examines whether English L1 and Japanese L2 listeners differ in the way they perceive the glottal stop as a signal to a phonological contrast in English. Glottal stops are often used by native English speakers as an allophone of /t/, including intervocalic environments, while this variation is not found in Japanese. Thus, the different L1 experience of the listeners may cause differences in their perceptual sensitivity to the glottal stop for a lexical contrast in English. In this experiment synthetic continua were constructed between ‘bear’ [beə] and ‘better’ [beə] using acoustic dimensions of fundamental frequency, voicing amplitude and diplophonia. Identification task results showed English and Japanese listeners differed in terms of how they used the acoustic cues of the glottal gesture for the contrast. Interestingly, Japanese listeners were more sensitive to the amplitude dip than English listeners, suggesting they were more sensitive to the acoustic properties of the stimuli.	acoustic cryptanalysis;http 404;synthetic intelligence	Yasuaki Shinohara;Mark Huckvale;Michael Ashby	2011			speech recognition;acoustics;computer science;communication	NLP	-10.161580616031173	-82.28825534370038	163319
6d2ad5eb8f3acbb42f7543ced00d5a4cc5b1ed20	polyphonic music information retrieval based on multi-label cascade classification system	hierarchical structure;mathematics;auditory perception;information retrieval;acoustics;semantics;data processing;spectrum;classification;estimation algorithm;musical instruments;feature vector;indexing;feature extraction;music techniques;data mining algorithm;music information retrieval;classification system;automatic indexing;pattern recognition;auditory stimuli;similarity measure;music	With the fast booming of online music repositories, there are increasing needs for content-based Automatic Indexing to help  users find their favorite music objects. Melody matching based on pitch detection technology has drawn much attention and  many MIR systems have been developed to fulfill this task. However music instrument recognition remains an unsolved problem  in the domain. Numerous approaches to acoustic feature extraction have already been proposed. Unfortunately, none of those  monophonic (one distinct instrument) timbre estimation algorithms can be successfully applied to polyphonic (multiple distinct  instruments) sounds, which occur more often in the real music world. This has stimulated the research on multi-labeled instrument  classification and new features development for content-based automatic music information retrieval. The original audio signals  are a large volume of unstructured sequential values, which are not suitable for traditional data mining algorithms, while  the higher level data representative of acoustical features are sometimes not sufficient for instrument recognition in polyphonic  sounds. We propose a multi-labeled classification system to estimate multiple timbre information from the polyphonic sound  according to a similarity measure based on both feature vectors and spectrum envelope. In order to achieve a higher estimation  rate, we introduced the hierarchical structured classification model under the inspiration of the human perceptual process.  This cascade classification system would first estimate the higher level decision attribute, which stands for the musical  instrument family. Then further estimation would be done within that specific family range. This could be applied with different  kind of features according to the specific characteristics of instruments in this family. Experiments showed better performance  of cascade system than the flattened classifiers.  	information retrieval;multi-label classification	Wenxin Jiang;Amanda Cohen;Zbigniew W. Ras	2009		10.1007/978-3-642-04141-9_6	speech recognition;computer science;pattern recognition;information retrieval	Web+IR	-8.092360150284236	-91.96592194421048	163443
5e8557f14c32d366030bb25b3d7086bcc5515598	esophageal voices: glottal flow restoration	speech processing;speech enhancement;air flow;spectral subtraction;esophagus;pattern matching;signal restoration;pattern matching speech enhancement signal restoration signal denoising;glottal air flow characteristic spectral components esophageal voice quality enhancement no sonority frames detection esophageal voice glottal flow restoration speech signal noise removal inter syllable breathing noise patterns speech patterns esophageal noise pattern spectral matching inter speech period noisy intervals;signal denoising	One of the main problems when working with esophageal voices is the uselessness of traditional speech processing techniques. Concretely, when trying to delete noise from the speech signal, spectral subtraction techniques are not valid because the breathing noise which appears between syllables does not follow the same patterns as the one which appears during the speech. A new technique is needed that allows us to eliminate the breathing noise, which seriously affects later processing of this speech. The aim of this paper is to apply a new technique, which is based on a spectral match with esophageal noise patterns, to the treatment of noisy intervals between speech periods and the deleting of the glottal air flow characteristic spectral components.	circuit restoration;speech processing	Begoña García Zapirain;Javier Vicente Sáez;Ibon Ruiz;Asier Alonso;Estibaliz Loyo	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415965	speech recognition;computer science;pattern matching;airflow;speech processing	Visualization	-7.46596818386896	-89.2118276875666	163503
b1d5cda264b6a8bc3a6ad9d8f46fa4a3c1dc717c	mutual features for robust identification and verification	text independent speaker verification mutual interdependence analysis mutual feature extraction speaker recognition problem face recognition problem image classification image database mutual speaker signature;mutual feature extraction;mutual interdependence analysis;speaker face recognition algorithms signal processing pattern classification signal analysis;mutual speaker signature;speaker face recognition;robustness testing image databases error analysis feature extraction scattering speaker recognition data mining lighting art;signal analysis;image database;image classification;indexing terms;speaker verification;speaker recognition;text independent speaker verification;speaker recognition problem;face recognition;statistical analysis;feature extraction;signal processing;equal error rate;pattern classification;error rate;algorithms;illumination invariance;face recognition problem;leave one out;statistical analysis face recognition feature extraction image classification speaker recognition	"""Noisy or distorted video/audio training sets represent constant challenges in automated identification and verification tasks. We propose the method of Mutual Interdependence Analysis (MIA) to extract """"mutual features"""" from a high dimensional training set. Mutual features represent a class of objects through a unique direction in the span of the inputs that minimizes the scatter of the projected samples of the class. They capture invariant properties of the object class and can therefore be used for classification. The effectiveness of our approach is tested on real data from face and speaker recognition problems. We show that """"mutual faces"""" extracted from the Yale database are illumination invariant, and obtain identification error rates of 2.2% in leave-one-out tests for differently illuminated images. Also, """"mutual speaker signatures"""" for text independent speaker verification achieve state-of-the- art equal error rates of 6.8% on the NTIMIT database."""	antivirus software;interdependence;speaker recognition;test set	Heiko Claussen;Justinian P. Rosca;Robert I. Damper	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517993	computer vision;contextual image classification;speech recognition;index term;feature extraction;word error rate;computer science;machine learning;signal processing;pattern recognition	Vision	-12.728567947933817	-92.9389718564659	163541
b9b8607fc1d0b384e213be0d339ac0f665003362	discriminative autoencoders for speaker verification		This paper presents a learning and scoring framework based on neural networks for speaker verification. The framework employs an autoencoder as its primary structure while three factors are jointly considered in the objective function for speaker discrimination. The first one, relating to the sample reconstruction error, makes the structure essentially a generative model, which benefits to learn most salient and useful properties of the data. Functioning in the middlemost hidden layer, the other two attempt to ensure that utterances spoken by the same speaker are mapped into similar identity codes in the speaker discriminative subspace, where the dispersion of all identity codes are maximized to some extent so as to avoid the effect of over-concentration. Finally, the decision score of each utterance pair is simply computed by cosine similarity of their identity codes. Dealing with utterances represented by i-vectors, the results of experiments conducted on the male portion of the core task in the NIST 2010 Speaker Recognition Evaluation (SRE) significantly demonstrate the merits of our approach over the conventional PLDA method.	artificial neural network;autoencoder;code;cosine similarity;discriminative model;experiment;generative model;loss function;optimization problem;speaker recognition	Hung-Shin Lee;Yu-Ding Lu;Chin-Cheng Hsu;Yu Tsao;Hsin-Min Wang;Shyh-Kang Jeng	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953183	autoencoder;discriminative model;artificial neural network;speaker diarisation;generative model;cosine similarity;speaker recognition;pattern recognition;utterance;computer science;machine learning;artificial intelligence	Vision	-16.27717726210282	-91.02382023104643	163550
924c999be82f28f612d66a1c8e2a971ef01941a2	evaluating a dynamic time warping based scoring algorithm for facial expressions in asl animations		Advancing the automatic synthesis of linguistically accurate and natural-looking American Sign Language (ASL) animations from an easy-to-update script would increase information accessibility for many people who are deaf by facilitating more ASL content to websites and media. We are investigating the production of ASL grammatical facial expressions and head movements coordinated with the manual signs that are crucial for the interpretation of signed sentences. It would be useful for researchers to have an automatic scoring algorithm that could be used to rate the similarity of two animation sequences of ASL facial movements (or an animation sequence and a motioncapture recording of a human signer). We present a novel, sign-language specific similarity scoring algorithm, based on Dynamic Time Warping (DTW), for facial expression performances and the results of a user-study in which the predictions of this algorithm were compared to the judgments of ASL signers. We found that our algorithm had significant correlations with participants’ comprehension scores for the animations and the degree to which they reported noticing specific facial expressions.	accessibility;algorithm;dynamic time warping;image warping;list comprehension;performance;regular expression	Hernisa Kacorri;Matt Huenerfauth	2015		10.18653/v1/W15-5106	natural language processing;speech recognition;computer science;communication	Graphics	-15.830096882018022	-81.63913651789642	163561
55d58805c32f7547228c8d80b10bb7b7b4f2f429	audio retrieval based on perceptual similarity	audio database;audio retrieval;indexing method;audio similarity;indexing;audio matching method;perceptual similarity;user personal experience;audio databases;query audio clip;audio signal processing;subjective method;content-based retrieval;mel frequency cepstral coefficient;radio frequency	Given a short query audio clip, the goal of audio retrieval is to automatically fetch all similar clips from a given audio database. Different from traditional audio similarity which is mainly based on priori knowledge of objective reality, this paper proposes to use a more subjective method to measure the perceptual similarity between audio clips. These perceptual features focus on users' personal experience, which can be very helpful for audio retrieval across different databases. In addition, indexing and audio matching methods are introduced to speed up the retrieval process. Experimental results on four different datasets are conducted to evaluate the effectiveness and efficiency of our proposed approaches..	acoustic cryptanalysis;database;experiment;real-time clock;relevance;semantic similarity;sensitivity and specificity;vii	Teng Zhang;Ji Wu;Dingding Wang;Tao Li	2014	10th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing		audio mining;speech recognition;telecommunications;computer science;speech coding;pattern recognition;sound quality;radio frequency;information retrieval	DB	-8.44107594692681	-93.9049928644334	163795
575035fbaade139845856a035f2f95bd6fd5541b	can you tell if tongue movements are real or synthesized?	computer science	We have investigated if subjects are aware of what natural tongue movements look like, by showing them animations based on either measurements or rule-based synthesis. The issue is of interest since a previous audiovisual speech perception study recently showed that the word recognition rate in sentences with degraded audio was significantly better with real tongue movements than with synthesized. The subjects in the current study could as a group not tell which movements were real, with a classification score at chance level. About half of the subjects were significantly better at discriminating between the two types of animations, but their classification score was as often well below chance as above. The correlation between classification score and word recognition rate for subjects who also participated in the perception study was very weak, suggesting that the higher recognition score for real tongue movements may be due to subconscious, rather than conscious, processes. This finding could potentially be interpreted as an indication that audiovisual speech perception is based on articulatory gestures.	logic programming;word error rate	Olov Engwall;Preben Wik	2009				ML	-8.395565049377103	-81.52904626141748	163811
4efd1d4b531109d7f1c26141918c109bd39a1fb5	expressive speech synthesis using a concatenative synthesizer	speech synthesis	1 This paper describes an experiment in synthesizing four emotional states anger, happiness, sadness and neutral – using a concatenative speech synthesizer. To achieve this, five emotionally (i.e., semantically) unbiased target sentences were prepared. Then, separate speech inventories, comprising the target diphones for each of the above emotions, were recorded. Using the 16 different combinations of prosody and inventory during synthesis resulted in 80 synthetic test sentences. The results were evaluated by conducting listening tests with 33 naïve listeners. Synthesized anger was recognized with 86.1% accuracy, sadness with 89.1%, happiness with 44.2%, and neutral emotion with 81.8% accuracy. According to our results, anger was classified as inventory dominant and sadness and neutral as prosody dominant. Results were not sufficient to make similar conclusions regarding happiness. The highest recognition accuracies were achieved for sentences synthesized by using prosody and diphone inventory belonging to the same emotion.	concatenative programming language;inventory;naivety;sadness;semantic prosody;speech synthesis;synthetic intelligence	Murtaza Bulut;Shrikanth (Shri) Narayanan;Ann K. Syrdal	2002			speech recognition;computer science;speech synthesis	NLP	-13.877466807164687	-83.17752475079435	163814
8824757bc4916e656bb6214afb9cd95ead69a38d	query by tapping: a new paradigm for content-based music retrieval from acoustic input	dynamic programming;music retrieval;programacion dinamica;base donnee;extraction forme;database;acoustique musicale;base dato;systeme recherche;search system;musical acoustics;internet;extraccion forma;sistema investigacion;acustica musical;programmation dynamique;content based retrieval;pattern extraction;recherche par contenu;content based music retrieval	"""This paper presents a query by tapping system, which represents a new paradigm for CBMRAI (content-based music retrieval via acoustic input) systems. Most CBMRAI systems take the user acoustic input in the format of singing or humming, and the timing or beat information (durations of notes) is sometimes discarded during the retrieval process in order to save computation. Our query by tapping"""" mechanism, on the other hand, takes the user input in the format of tapping on the microphone and the extracted duration of notes is then used to retrieve the intended song in the database. Since there is no singing or humming, no pitch information is used in the retrieval process at all. Most people would think that it is hard to do music retrieval via beat information alone. However, our experiments demonstrate that beat information is also an effective feature in the sense that it can be used to retrieve the intended song from a large collection of music database with a satisfactory recognition rate."""	acoustic cryptanalysis;paradigm	Jyh-Shing Roger Jang;Hong-Ru Lee;Chia-Hui Yeh	2001		10.1007/3-540-45453-5_76	the internet;speech recognition;computer science;dynamic programming;musical acoustics;database;multimedia	Crypto	-6.504006557619081	-93.20075303591867	163867
fda04ed430ebbdc39c118e6361268c4eccbcf3d9	a comparative study on various state of the art face recognition techniques under varying facial expressions		Through face we can know the emotions and feelings of a person. It can also be used to judge a person’s mental aspect and psychomatic aspects. There are 5 state of the art approaches for recognizing faces under varying facial expressions. These 5 approaches are overlapping Discrete Cosine Transform (DCT), Hierarchical Dimensionality Reduction (HDR), Local and Global combined Computational Features (LGCF), Combined Statistical Moments (CSM), and Score Level Fusion Techniques (SLFT). Matlab code has been developed for all the 5 systems and tested using common set of train and test images. The train and test images are considered from standard public face databases ATT, JAFFE, and FEI. The key contribution of this article is, we have developed and analyzed the 5 state of the art approaches for recognizing faces under varying facial expressions using a common set of train and test images. This evaluation gives us the exact face recognition rates of the 5 systems under varying facial expressions. The face recognition rate of overlap DCT on ATT database was 95% and FEI 99% which was better than HDR, LGCM, CSM and SLFT. But the face recognition rate of CSM on JAFE database, which contains major facial expression variations, was 100% which was better than overlap DCT, HDR, LGCM, and SLFT.	community climate system model;computation;database;dimensionality reduction;discrete cosine transform;facial recognition system;flight dynamics (fixed-wing aircraft);matlab;unified extensible firmware interface	Steven Fernandes;Josemin Bala	2017	Int. Arab J. Inf. Technol.		computer science;artificial intelligence;facial recognition system;facial expression;pattern recognition	Vision	-14.478573963138478	-89.00788779212147	163978
288ce056b4a182f289c28a5d9517132b4e2e5f91	pca-pmc: a novel use of a priori knowledge for fast parallel model combination	eigenvalues and eigenfunctions;robust speech recognition;real time;acoustic modeling;speech coding;matrix algebra;parallel model combination;computational complexity vectors covariance matrix acoustic noise principal component analysis discrete cosine transforms noise robustness speech recognition speech enhancement computational modeling;a priori knowledge;computational complexity;discrete cosine transforms;principal component analysis;speech recognition;spectral domain analysis;experimental evaluation;speech coding computational complexity speech recognition principal component analysis spectral domain analysis discrete cosine transforms inverse problems eigenvalues and eigenfunctions matrix algebra;phone recognition a priori knowledge fast parallel model combination pca pmc computational complexity robust speech recognition performance noise corrupted acoustic model clean speech noise models principal component analysis protype vectors prototype vectors matrices mean covariances linear spectral domain rectangular dct matrices inverse dct matrices transformation linear spectral domain projection eigenspace;inverse problems	This paper describes an algorithm to reduce computational complexity of the parallel model combination (PMC) method for robust speech recognition while retaining the same level of performance. Although, PMC is e ective in composing a noise corrupted acoustic model from clean speech and noise models, the intense computational complexity limits its use in real-time use. The novel approach here is to encode the clean models using principal component analysis (PCA) and pre-compute the protype vectors and matrices for the means and covariances in the linear spectral-domain using rectangular DCT and inverse DCT matrices. Therefore, transformation into the linear spectral domain is reduced to nding the projection of each vector in the Eigen space of means and covariances followed by a linear combination of vectors and matrices obtained from the projections. Furthermore, the Eigen space allows a better trade-o for reducing computational complexity versus accuracy. The computational savings are demonstrated both analytically and through experimental evaluations. Experiments using context independent phone recognition with TIMIT data shows that the new PMC framework can outperforms the baseline method by a factor of 1.9 with the same level of accuracy.	acoustic cryptanalysis;acoustic model;algorithm;baseline (configuration management);cepstrum;computation;computational complexity theory;discrete cosine transform;encode;eigen (c++ library);hidden markov model;jean;online and offline;principal component analysis;prototype;real-time clock;requirement;speech recognition;timit	Ruhi Sarikaya;John H. L. Hansen	2000		10.1109/ICASSP.2000.859159	a priori and a posteriori;speech recognition;computer science;inverse problem;machine learning;speech coding;pattern recognition;mathematics;computational complexity theory;principal component analysis	ML	-16.637575550527256	-92.77490284512278	164018
f377d21e8a4f61580df20021502b180ee51c7866	recognition of subsampled speech using a modified mel filter bank	modified mel filter bank;sub-sampled speech;automatic speech recognition engine;different sampling frequency;speech feature;different sampling rate;speech engine;subsampled speech;certain sampling rate;re-sampled speech;acoustic model;mfcc feature	modified mel filter bank;sub-sampled speech;automatic speech recognition engine;different sampling frequency;speech feature;different sampling rate;speech engine;subsampled speech;certain sampling rate;re-sampled speech;acoustic model;mfcc feature	filter bank	Sunil Kumar Kopparapu;Kiran Kumar Bhuvanagiri	2013	Computers & Electrical Engineering	10.1016/j.compeleceng.2012.10.002	speech recognition;computer science;pattern recognition;acoustic model	ML	-13.688867469102346	-88.60122645295236	164100
32d19a2ffffc1159e71fdde8eece28bf4d5be746	evidence for the strength of the relationship between automatic speech recognition and phoneme alignment performance	hmms;acoustic phonetic models;speech synthesis;hmms phoneme alignment speech recognition optimal performance acoustic phonetic models;speech processing;training;speech;data analysis automatic speech recognition phoneme alignment performance acoustic phonetic models;system performance;optimal performance;acoustic testing;automatic speech recognition;accuracy;data analysis;hidden markov models;phoneme alignment;speech recognition data analysis hidden markov models speech processing;speech recognition;system testing;humans;phoneme alignment performance;correlation;automatic speech recognition hidden markov models humans system testing acoustic testing laboratories data analysis speech recognition speech synthesis system performance	It might be naïvely assumed that the performance of an Automatic Speech Recognition (ASR) system, and that of an Automatic Speech-to-Phoneme Alignment (ASPA) system using the same acoustic-phonetic models, would be closely related. However many researchers believe this relationship to be, at best weak - but this belief has not previously been tested in an objective and quantitative manner. This paper quantifies the strength of the relationship using analysis of data without reference to manually defined alignment labels. By avoiding comparison with a set of reference labels, both the ASR and the ASPA systems can be considered equivalent, removing any bias due to any difference of “opinion” between the human labeller and the automatic system.	acoustic cryptanalysis;automatic system recovery;speech recognition	Ladan Baghai-Ravary	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5494977	natural language processing;speech recognition;computer science;speech;pattern recognition;speech processing;accuracy and precision;data analysis;speech synthesis;system testing;correlation	Robotics	-12.439783596059197	-84.11067509204098	164194
be67bb3a243e6b3b89d650fce1131e760b30718d	towards an automated screening tool for developmental speech and language impairments.		Approximately 60% of children with speech and language impairments do not receive the intervention they need because their impairment was missed by parents and professionals who lack specialized training. Diagnoses of these disorders require a time-intensive battery of assessments, and these are often only administered after parents, doctors, or teachers show concern. An automated test could enable more widespread screening for speech and language impairments. To build classification models to distinguish children with speech or language impairments from typically developing children, we use acoustic features describing speech and pause events in story retell tasks. We developed and evaluated our method using two datasets. The smaller dataset contains many children with severe speech or language impairments and few typically developing children. The larger dataset contains primarily typically developing children. In three out of five classification tasks, even after accounting for age, gender, and dataset differences, our models achieve good discrimination performance (AUC > 0.70).	acoustic cryptanalysis;test automation	Jen J. Gong;Maryann Gong;Dina Levy-Lambert;Jordan R. Green;Tiffany P. Hogan;John V. Guttag	2016		10.21437/Interspeech.2016-549	natural language processing;speech recognition	NLP	-17.503516461232742	-82.67743701166775	164314
25ebced98d6aaaa62fc9dc467eff2b2817e9fdfe	unifying background models over complex audio using entropy	complex audio scenes;background modeling;online adaptive gaussian mixture model;audio signal processing;audio background modelling;gaussian processes;audio surveillance;gaussian processes audio signal processing entropy;entropy robustness surveillance signal analysis condition monitoring layout pattern recognition clustering algorithms event detection;failure mode;model variations;gaussian mixture model;audio monitoring audio background modelling online adaptive gaussian mixture model complex audio scenes audio surveillance;complex audio environments;audio monitoring;entropy;adaptive gaussian mixture models	In this paper we extend an existing audio background modelling technique, leading to a more robust application to complex audio environments. The determination of background audio is used as an initial stage in the analysis of audio for surveillance and monitoring applications. Knowledge of the background serves to highlight unusual or infrequent sounds. An existing modelling approach uses an online, adaptive Gaussian mixture model technique that uses multiple distributions to model variations in the background. The method used to determine the background distributions of the GMM leads to a failure mode of the existing technique when applied to complex audio. We propose a method incorporating further information, the proximity of distributions determined using entropy, to determine a more complete background model. The method was successful in more robustly modelling the background for complex audio scenes	algorithm;cluster analysis;failure cause;google map maker;heart rate variability;mixture model	Simon Moncrieff;Svetha Venkatesh;Geoff A. W. West	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.1141	entropy;speech recognition;audio signal processing;computer science;machine learning;pattern recognition;mixture model;gaussian process;failure mode and effects analysis;statistics	Vision	-11.918165404836309	-93.50755303724438	164337
69eaf306803c7eeedba0933244bc7fb105ea1a01	data augmentation for deep convolutional neural network acoustic modeling	speech related applications data augmentation deep convolutional neural network acoustic modeling label preserving transformations cnn acoustic modeling limited training data stochastic feature mapping sfm log mel features vocal tract length perturbation vtlp stacked architecture limited language pack llp haitian creole;feedforward neural networks;acoustics;stochastic processes data handling neural nets speech processing;stochastic feature mapping convolutional neural networks bottleneck features data augmentation vocal tract length perturbation;adaptation models acoustics atmospheric modeling feedforward neural networks;atmospheric modeling;adaptation models	This paper investigates data augmentation based on label-preserving transformations for deep convolutional neural network (CNN) acoustic modeling to deal with limited training data. We show how stochastic feature mapping (SFM) can be carried out when training CNN models with log-Mel features as input and compare it with vocal tract length perturbation (VTLP). Furthermore, a two-stage data augmentation scheme with a stacked architecture is proposed to combine VTLP and SFM as complementary approaches. Improved performance has been observed in experiments conducted on the limited language pack (LLP) of Haitian Creole in the IARPA Babel program.	acoustic cryptanalysis;acoustic model;artificial neural network;authorization;convolutional neural network;creole (markup);dod ipv6 product certification;experiment;tract (literature)	Xiaodong Cui;Vaibhava Goel;Brian Kingsbury	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178831	computer vision;atmospheric model;feedforward neural network;speech recognition;computer science;machine learning;deep learning	Robotics	-17.85369396958822	-89.21251465400796	164413
0fb338267c164b7f76f55a01f3eed1e83186ce83	combining gaussian mixture model with global variance term to improve the quality of an hmm-based polyglot speech synthesizer	polyglot hmm based speech synthesis gaussian mixture global variance;gaussian mixture;speech synthesis;global variance;gaussian processes;acoustic modeling;hmm based speech synthesis;gaussian mixture model;hidden markov models;polyglot;gradient methods;likelihood function;hidden markov models speech synthesis synthesizers cepstral analysis computer science electronic mail probability vocoders density functional theory;speech synthesis gaussian processes gradient methods hidden markov models;speech quality gaussian mixture acoustic model global variance term hmm based polyglot speech synthesizer cepstral coefficients direct log likelihood function maximization gradient ascent algorithm	This paper proposes a new method to calculate the cepstral coefficients for an HMM-based synthesizer. It consists of a direct maximization of the log-likelihood function of a Gaussian mixture model using a gradient ascent algorithm. The method permits to integrate efficiently the global variance factor with a Gaussian mixture acoustic model. The perceptual experiments confirmed that these two factors produce significant improvements on the speech quality, which are independent from each other. By using the proposed method, it is possible to get the benefits of both factors. This paper also proposes a 2-class model for the global variance that discriminates between consonants and vowels. Such 2-class global variance model produces more stable cepstral coefficients than the single-class one.	acoustic cryptanalysis;acoustic model;cepstrum;coefficient;expectation–maximization algorithm;experiment;gradient descent;hidden markov model;mixture model;speech synthesis;times ascent	Javier Latorre;Koji Iwano;Sadaoki Furui	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367301	speech recognition;computer science;machine learning;pattern recognition;mixture model;gaussian process;mathematics;likelihood function;speech synthesis;hidden markov model;statistics	Robotics	-18.448560614666622	-92.05066069288983	164496
6642ea7baed08381a41b8f1bbf3299635c6221d5	an algorithm for the dynamic inference of hidden markov models (dihmm)	databases;topology;gaussian mixture;nonlinear spectral subtraction dynamic inference of hidden markov models dihmm algorithm hmm topology speech variability gaussian mixture density functions transition matrix probabilistic grammatical inference techniques viterbi like training framework databases recognition rates adverse environments;probabilistic grammatical inference techniques;robust estimator;heuristic algorithms inference algorithms hidden markov models databases robustness topology speech enhancement state estimation yield estimation density functional theory;hidden markov model;dynamic inference of hidden markov models;inference mechanisms;speech enhancement;yield estimation;viterbi like training framework;state estimation;adverse environments;transition matrix;density functional theory;spectral subtraction;speech variability;hidden markov models;heuristic algorithms;nonlinear spectral subtraction;speech recognition;robustness;grammatical inference;inference algorithms;parameter estimation;learning artificial intelligence;recognition rates;dihmm algorithm;speech recognition hidden markov models inference mechanisms learning artificial intelligence parameter estimation;gaussian mixture density functions;density functional;hmm topology	The DIHMM algorithm performs a robust estimation of the HMM topology and parameters. It allows a better control of the speech variability within each state of the HMM, yielding enhanced estimates. The DIHMM parameters (number of states, structure of the Gaussian mixture density functions, transition matrix) are obtained from the training data via probabilistic grammatical inference techniques welded in a Viterbi-like training framework. Experimental results on various databases indicate a global improvement of the recognition rates in adverse environments; the results averaged on three databases show an increase of 12.8% on raw data and 2.4% when using NSS (nonlinear spectral subtraction). >		Philip Lockwood;Marc Blanchet	1993		10.1109/ICASSP.1993.319282	robust statistics;speech recognition;computer science;machine learning;pattern recognition;stochastic matrix;estimation theory;density functional theory;hidden markov model;statistics;robustness	NLP	-18.131903659723584	-92.81322818318424	164545
0a8b58f7c84fa14041d0d32747822c704cf26443	an hmm based system for acoustic event detection	hidden markov models;chil project;acoustic event detection;classifying block;audio sequence;error rate;acoustic event;proposed system;possible event label;good result;mfcc feature;hidden markov model	This paper deals with the CLEAR 2007 evaluation on the detection of acoustic events which happen during seminars. The proposed system first converts an audio sequence in a stream of MFCC features, then a detecting/classifying block identifies an acoustic event with time stamps and assign to it a label among all possible event labels. Identification and classification are based on Hidden Markov Models (HMM). The results, measured in terms of two metrics (accuracy and error rate) are obtained applying the implemented system on the interactive seminars collected under the CHIL project. Final not very good results highlight the task complexity.	acoustic cryptanalysis;hidden markov model	Christian Zieger	2007		10.1007/978-3-540-68585-2_32	speech recognition;computer science;pattern recognition;data mining	Robotics	-6.37700066266997	-89.2501563102222	164847
ba505876ae0055443ba50cf476d77b52d60a44c4	multiple codebook spanish phone recognition using semicontinuous hidden markov models	hidden markov model		codebook;hidden markov model;markov chain;semi-continuity	M. Inés Torres;Francisco Casacuberta	1993			viterbi algorithm;speech recognition;artificial intelligence;pattern recognition;hidden markov model;hidden semi-markov model;machine learning;markov process;computer science;maximum-entropy markov model;markov model;forward algorithm;variable-order markov model	Vision	-16.2447541530805	-87.14256859579677	164889
da1dbb922d8524a3ac77a5fb714ae18e451cdee0	a set of japanese word cohorts rated for relative familiarity	article in monograph or in proceedings	A database is presented of relative familiarity ratings for 24 sets of Japanese words, each set comprising words overlapping in the initial portions. These ratings are useful for the generation of material sets for research in the recognition of spoken words.	database	Takashi Otake;Anne Cutler	2000			natural language processing;speech recognition;computer science;linguistics	NLP	-13.144205287503869	-81.1086184825656	164938
5fd1297b0f2714db1eee19cfb028b2799889978d	the ibm trainable speech synthesis system	speech synthesis	The speech synthesis system described in this paper uses a set of speaker-dependent decision-tree state-clustered hidden Markov models to automatically generate a leaf level segmentation of a large single-speaker continuous-read-speech database. During synthesis, the phone sequence to be synthesised is converted to an acoustic leaf sequence by descending the HMM decision trees. Duration, energy and pitch values are predicted using separate trainable models. To determine the segment sequence to concatenate, a dynamic programming (d.p.) search is performed over all the waveform segments aligned to each leaf in training. The d.p. attempts to ensure that the selected segments join each other spectrally, and have durations, energies and pitches such that the amount of degradation introduced by the subsequent use of TD-PSOLA is minimised. Algorithms embedded within the d.p. can alter the required acoustic leaf sequence, duration and energy values to ensure high quality synthetic speech. The selected segments are concatenated and modi ed to have the required prosodic values using the TD-PSOLA algorithm. The d.p. results in the system e ectively selecting variable length units, based upon its leaf level framework.	acoustic cryptanalysis;algorithm;concatenation;decision tree;display resolution;dynamic programming;elegant degradation;embedded system;hidden markov model;markov chain;psola;speech synthesis;synthetic data;waveform	Robert E. Donovan;Ellen Eide	1998			artificial intelligence;phone;speech recognition;concatenation;decision tree;hidden markov model;speech technology;pattern recognition;dynamic programming;computer science;waveform;speech synthesis	ML	-11.601144411808466	-89.12087379747791	165091
77aef8465bcb3a80cba1fc9f331e5b669700ab52	cross-language acoustic modeling in large vocabulary continuous speech recognition.	large vocabulary continuous speech recognition;acoustic modeling			Shengmin Yu;Shuwu Zhang;Bo Xu	2004	Journal of Chinese Language and Computing		speech recognition;speech corpus;computer science;acoustic model	NLP	-15.49556507340785	-86.3014758249808	165160
1be64914bdba4492b10d3ba77a4897bb3ad51a4c	an instantaneous vector representation of delta pitch for speaker-change prediction in conversational dialogue systems	instantaneous vector representation;sprakteknologi sprakvetenskaplig databehandling;dialogue system;user interface;frequency domain analysis;acoustic modeling;speech processing;jamforande sprakvetenskap och lingvistik;acoustic signal processing;indexing terms;speaker recognition acoustic signal processing signal representation;spoken dialogue system;speaker recognition;human human conversations;speaker change prediction;signal representation;general language studies and linguistics;datavetenskap datalogi;signal representation speech communication user interfaces speech processing frequency domain analysis;spoken dialogue systems;run time control;humans automatic control control systems automatic speech recognition predictive models delay loudspeakers runtime communication system control oral communication;speech communication;language technology computational linguistics;computer science;complex domains;flow control;delta pitch variation;run time control instantaneous vector representation delta pitch variation speaker change prediction spoken dialogue systems complex domains human human conversations acoustic modeling;user interfaces	As spoken dialogue systems become deployed in increasingly complex domains, they face rising demands on the naturalness of interaction. We focus on system responsiveness, aiming to mimic human-like dialogue flow control by predicting speaker changes as observed in real human-human conversations. We derive an instantaneous vector representation of pitch variation and show that it is amenable to standard acoustic modeling techniques. Using a small amount of automatically labeled data, we train models which significantly outperform current state-of-the-art pause-only systems, and replicate to within 1% absolute the performance of our previously published hand-crafted baseline. The new system additionally offers scope for run-time control over the precision or recall of locations at which to speak.	acoustic cryptanalysis;acoustic model;baseline (configuration management);dialog system;responsiveness;self-replicating machine	Kornel Laskowski;Jens Edlund;Mattias Heldner	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518791	natural language processing;speaker recognition;speech recognition;computer science;speech processing;user interface	Robotics	-14.958022877444256	-83.06427721249734	165188
02a75ab6a70c2746a5b884c1d3009d055822b5dd	overlapped speech detection for improved speaker diarization in multiparty meetings	channel signal relative improvement;speaker diarization systems;overlapped speech detection;indexing terms;overlap detection;single mixed headset channel signal;overlap detection speaker diarization;ami meeting corpus;speech recognition;multiparty meetings;speaker diarization;baseline diarization system;channel signal relative improvement overlapped speech detection multiparty meetings speaker diarization systems ami meeting corpus single mixed headset channel signal baseline diarization system;speech detection;detectors ambient intelligence computer science merging density estimation robust algorithm speech processing speech recognition speech analysis error analysis	State-of-the-art speaker diarization systems for meetings are now at a point where overlapped speech contributes significantly to the errors made by the system. However, little if no work has yet been done on detecting overlapped speech. We present our initial work toward developing an overlap detection system for improved meeting diarization. We investigate various features, with a focus on high-precision performance for use in the detector, and examine performance results on a subset of the AMI Meeting Corpus. For the high-quality signal case of a single mixed-headset channel signal, we demonstrate a relative improvement of about 7.4% DER over the baseline diarization system, while for the more challenging case of the single far-field channel signal relative improvement is 3.6%. We also outline steps towards improvement and moving beyond this initial phase.	baseline (configuration management);headset (audio);sensor;speaker diarisation	Kofi Boakye;B. Trueba-Hornero;Oriol Vinyals;Gerald Friedland	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518619	speaker diarisation;speech recognition;index term;computer science;pattern recognition	Robotics	-13.562736995830216	-90.02553816916107	165359
19c78f84735b98546c3cf0809d4e603af7476a7a	automatic emotion recognition from speech a phd research proposal	simililarity concept;speech signal;emotion recognition	This paper contains a PhD research proposal related to the domain of automatic emotion recognition from speech signal. We started by identifying our research problem, namely the acute confusion problem between emotion classes and we have cited different sources of this ambiguity. In the methodology section, we presented a method based on simililarity concept between a class and an instance patterns. We dubbed this method as Weighted Ordered classes – Nearest Neighbors. The first result obtained exceeds in performance the best result of the state-of-the art. Finally, as future work, we have made a proposition to improve the performance of the proposed system.	emotion recognition	Yazid Attabi;Pierre Dumouchel	2011		10.1007/978-3-642-24571-8_20	psychology;speech recognition;artificial intelligence;machine learning;communication	AI	-12.06677828358317	-86.51834979973094	165498
0df8931fcf6c17624ad972e3d2401716969c6c45	speech recognition based on lateral inhibition network auditory model	robust speech recognition;neural nets;spectrum;speaker independent;speaker dependent;lateral inhibition;speech recognition;spectral analysis;speech recognition noise robustness ear humans speech analysis speech processing signal processing auditory system working environment noise feature extraction;physiological models;white noise;physiological models speech recognition neural nets spectral analysis;spectral analysis lateral inhibition network auditory model speech recognition speaker dependent recognition speaker independent recognition chinese mandarin white noise neural nets	This paper studies the robust speech recognition based on auditory model. The spectrum of auditory representation shows that the lateral inhibition network (LIN) can shape the spatial input patterns and highlight their edges and peaks. Also, it can make the speech recognition to be more robust to noise than the LPC cepstral (LPC-CEP) approach. Some preliminary experiments including speaker dependent and speaker independent recognition were performed using both the model and LPC-CEP respectively. A set of 405 Chinese mandarin syllables was taken as the reference. The results of the experiment show that the recognition ratio based on the auditory model was equal to that of LPC-CEP in the case of non-noise. However, the recognition ratio based on the auditory model was much higher under the white noise of about 2.8 dB. Both theoretical and experimental studies show that the auditory model not only represents speech signal well but is also robust.	lateral thinking;speech recognition	Yan Zhang;Jie Zhang;Zhitong Huang	1998		10.1109/ICSMC.1998.727496	voice activity detection;speaker recognition;spectrum;speech recognition;lateral inhibition;computer science;neurocomputational speech processing;machine learning;speech processing;acoustic model;computational auditory scene analysis;white noise;artificial neural network;statistics	Vision	-11.826707621833949	-89.50965051539593	165619
41578b1e9c2a5c2140e4fba62fe2bce431dba4ff	digit recognition in noisy environments via a sequential gmm/svm system	databases;kernel;support vector machines;argon;noise measurement;speaker recognition;hidden markov models;hidden markov models robustness support vector machines noise measurement argon databases kernel;speech recognition;robustness	This paper exploits the fact that when GMM and SVM classifiers with roughly the same level of performance exhibit uncorrelated errors they can be combined to produce a better classifier. The gain accrues from combining the descriptive strength of GMM models with the discriminative power of SVM classifiers. This idea, first exploited in the context of speaker recognition [1, 2], is applied to speech recognition - specifically to a digit recognition task in a noisy environment - with significant gains in performance.	google map maker;speaker recognition;speech recognition	Shai Fine;George Saon;Ramesh A. Gopinath	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5743651	speaker recognition;support vector machine;kernel;speech recognition;computer science;noise measurement;machine learning;pattern recognition;argon;robustness	Robotics	-15.508989223324393	-91.72172633407314	165711
60acfa14ecddef17fe55e6f423d021785187436d	word stress in tashlhiyt - post lexical prominence in disguise?		This paper reports on a study of lexical and postlexical prominence asymmetries in Tashlhiyt Tamazight (Berber), a language that is renowned for its typologically rare prosodic structures. Carrier phrases were designed to elicit the presence or absence of a postlexical tone on the target word. Contrary to previous reports on the language, measures of acoustic durations and intensity reveal no consistent prominence asymmetries at the level of t he word. However, we found evidence for a prominence lending function of tonal events at the postlexical level. We conclude that Tashlhiyt does not show any acoustic evidence for lexical stress and that postlexical events do not appear to be related to lexically determined metrical structures.	acoustic cryptanalysis;peer-to-peer lending	Timo B. Roettger;Anna Bruggeman;Martine Grice	2015			stress (linguistics);natural language processing;linguistics;psychology;artificial intelligence	NLP	-10.864161757871278	-81.20045871878692	165785
a16efa7f7feb390905cac6b6ed7333fda88ef9bd	unseen appliances identification		We assess the feasibility of unseen appliance recognition through the analysis of their electrical signatures recorded using lowcost smart plugs. By unseen, we stress that our approach focuses on the identification of appliances that are of different brands or models than the one in training phase. We follow a strictly defined protocol in order to provide comparable results to the scientific community. We first evaluate the drop of performance when going from seen to unseen appliances. We then analyze the results of different machine learning algorithms, as the k-Nearest Neighbor (k-NN) and Gaussian Mixture Models (GMMs). Several tunings allow us to achieve 74% correct accuracy using GMMs which is our current best system.	antivirus software;computer appliance;k-nearest neighbors algorithm;machine learning;mixture model	Antonio Ridi;Christophe Gisler;Jean Hennebert	2013		10.1007/978-3-642-41827-3_10	pattern recognition;artificial intelligence;mixture model;computer science	AI	-11.023562422234193	-92.35185364407839	165925
cffec23f89874dec84542530b0b6fcb50e1f12b8	phonological features in the bilingual lexicon: insights from tonal accent in swedish		Scandinavian languages like Swedish employ tonal accent as a lexical phonological feature, where suprasegmental information can be the sole factor differentiating between words. Using cross-modal semantic fragment priming we tested the following: (a) Do monolingual speakers of Swedish use tonal accent information during lexical access? (b) Do bilingual speakers, who grew up with one tonal (Swedish) and one non-tonal language, treat this feature the same way as monolinguals? Our results show that for monolinguals, accent mispronunciations eliminate priming effects, implying that tone is used during lexical access. For bilinguals, by contrast, mispronunciation sensitivity depends on both the accent type and its distribution across the linguistic input, as well as on the lexical neighbourhood.	bilingual dictionary;emoticon;lexicon;modal logic	Nadja Althaus;Allison Wetterlin;Aditi Lahiri	2017			cognitive psychology;psychology;bilingual lexicon	NLP	-11.101647540530127	-80.91315681415588	165937
d26561f6425b5d829064662a2ced10c1a8541155	temporal characteristics of utterance units and topic structure of spoken dialogs	topic change;temporal structure;speaking mode;spoken dialog;utterance unit			Kazuyuki Takagi;Shuichi Itahashi	1995	IEICE Transactions		natural language processing;speech recognition	NLP	-15.355338792607288	-84.93445913291103	165947
c65ae3edbfa7289340f8de257e43e90df09ee79c	new rule-based and data-driven strategy to incorporate fujisaki's f 0 model to a text-to-speech system in castillian spanish	linguistic features;fujisaki f 0 model;castillian spanish;damping;control systems;search problems speech synthesis parameter estimation knowledge based systems;speech synthesis neural networks spatial databases damping control systems circuits parameter estimation data analysis data mining feature extraction;neural networks;speech synthesis;rule based;spanish prosody database;data mining;linguistic feature comparison;k nearest neighbour search;rule base filtering;analysis database;data analysis;feature extraction;spatial databases;text to speech;rule based data driven strategy;circuits;k nearest neighbour;search problems;text to speech system;parameter estimation;knowledge based systems;neural network;rule base filtering rule based data driven strategy fujisaki f sub 0 model text to speech system castillian spanish spanish prosody database linguistic features analysis database k nearest neighbour search linguistic feature comparison	We will present the analysis of a Spanish prosody database by estimating the parameters of Fujisaki's model for FO contours. These parameters are classified attending to linguistic features and they form the analysis database. When synthesizing FO contours we extract the linguistic features from the text and perform a k-Nearest Neighbour search. Linguistic feature comparison distance is trained using data from the prosody database. To avoid artifacts we perform a rule-base filtering on synthesis parameters. The results of our evaluation test show that the proposed system is significantly better than the previous neural network approach. This evaluation confirms the ability of Fujisaki's model to represent prosody information based on linguistic features.	artificial neural network;logic programming;nearest neighbor search;semantic prosody;speech synthesis;turing test	Juana M. Gutiérrez-Arriola;Juan Manuel Montero-Martínez;D. Saiz;José Manuel Pardo	2001		10.1109/ICASSP.2001.941041	natural language processing;damping;electronic circuit;speech recognition;feature extraction;computer science;machine learning;pattern recognition;estimation theory;data analysis;speech synthesis;artificial neural network	NLP	-17.85042569223215	-85.40261961355029	166097
cd33d4ded04915c955b0ce40f5a33088386d8a72	audio proto objects for improved sound localization	azimuth;speech recognition audio proto objects improved sound localization auditory processing feature extraction grouping processes precise sound localization behavior control robot audition;robots audio signal processing blind source separation;audio signal processing;behavior control;robot audition;sound localization;blind source separation;improved sound localization;auditory processing;audio proto objects;data mining;noise measurement;estimation;compact representation;feature extraction;robots;position measurement;speech recognition;group process;speech recognition signal processing streaming media time measurement acoustic noise filters histograms intelligent robots usa councils feature extraction;behavioral control;precise sound localization;grouping processes	In this article we present a new framework for auditory processing that combines feature extraction and grouping processes to form what we call audio proto objects. These proto objects combine an arbitrary number of audio features in a compact representation that allows a more precise sound localization and also better interfacing to behavior-control in robotics. We compare our standard sound localization system with the new approach in several scenarios to demonstrate the potential of the new approach.	audio signal processing;binaural beats;bregman divergence;concatenation;feature extraction;hausdorff dimension;microsoft outlook for mac;multi-source;neural coding;object-based language;robot;robotics;source separation	Tobias Rodemann;Frank Joublin;Christian Goerick	2009	2009 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2009.5354698	robot;estimation;speech recognition;acoustics;sound localization;audio signal processing;feature extraction;computer science;noise measurement;blind signal separation;azimuth;group dynamics	Robotics	-11.575661852163197	-90.26333117680811	166188
408f83e7520c83216cc0ace9e5db316e7b40cfee	cross-entropic comparison of formants of british, australian and american english accents	phoneme;cepstrum features;cross entropy;medida informacion;phonetique;entropia;sex;accents;formant;spectrum analysis;analyse spectre;acento linguistico;analisis espectro;mesure information;signal analysis;speech processing;gender difference;database;tratamiento palabra;traitement parole;base dato;linguistic accent;sexe;analisis de senal;physical sciences;fonema;similitude;cepstral analysis;reconocimiento voz;anglais;information measure;analyse cepstrale;accent linguistique;entropie;similarity;base de donnees;formant features;fonetica;speech recognition;english;entropy;phonetics;reconnaissance parole;similitud;probability model;formante;ingles;sexo;structural similarity;analyse signal;formant space;american english	This paper highlights the differences in spectral features between British, Australian and American English accents and applies the cross-entropy information measure for comparative quantification of the impacts of the variations of accents, speaker groups and recordings on the probability models of spectral features of phonetic units of speech. Comparison of the cross entropies of formants and cepstrum features indicates that formants are a better indicator of accents. In particular it appears that the measurements of differences in formants across accents are less sensitive to different recordings or databases compared to cepstrum features. It is found that the cross entropies of the same phonemes across speaker groups with different accents (inter-accent distances) are significantly greater than the cross entropies of the same phonemes across speaker groups of the same accent (intra-accent distances). Comparative evaluations presented on cross-gender speech recognition shows that accent differences have an impact comparable to gender differences. The cross entropy measure is also used to construct cross-accent phonetic trees, which serve to show the structural similarities and differences of the phonetic systems across accents.	cepstrum;cross entropy;database;speech recognition	Seyed Ghorshi;Saeed Vaseghi;Qin Yan	2008	Speech Communication	10.1016/j.specom.2008.03.013	phonetics;entropy;speech recognition;computer science;speech processing;linguistics	NLP	-8.685571644009164	-89.62484767985654	166249
025cc35d890180579dfeac5ee386a4b398cdef7f	classification of speech under stress based on modeling of the vocal folds and vocal tract	signal image and speech processing;acoustics;mathematics in music;engineering acoustics	In this study, we focus on the classification of neutral and stressed speech based on a physical model. In order to represent the characteristics of the vocal folds and vocal tract during the process of speech production and to explore the physical parameters involved, we propose a method using the two-mass model. As feature parameters, we focus on stiffness parameters of the vocal folds, vocal tract length, and cross-sectional areas of the vocal tract. The stiffness parameters and the area of the entrance to the vocal tract are extracted from the two-mass model after we fit the model to real data using our proposed algorithm. These parameters are related to the velocity of glottal airflow and acoustic interaction between the vocal folds and the vocal tract and can precisely represent features of speech under stress because they are affected by the speaker’s psychological state during speech production. In our experiments, the physical features generated using the proposed approach are compared with traditionally used features, and the results demonstrate a clear improvement of up to 10% to 15% in average stress classification performance, which shows that our proposed method is more effective than conventional methods.	acoustic cryptanalysis;algorithm;cross-sectional data;experiment;mental state;tract (literature);velocity (software development)	Xiao Yao;Takatoshi Jitsuhiro;Chiyomi Miyajima;Norihide Kitaoka;Kazuya Takeda	2013	EURASIP J. Audio, Speech and Music Processing	10.1186/1687-4722-2013-17	speech recognition;acoustics;physics	NLP	-9.544777611595455	-84.78531044180828	166273
112656bd610d7193b04b14e7e60d826c4351b533	accounting for uncertainty of i-vectors in speaker recognition using uncertainty propagation and modified imputation		One of the biggest challenges in speaker recognition is incomplete observations in test phase caused by availability of only short duration utterances. The problem with short utterances is that speaker recognition needs to be handled by having information from only limited amount of acoustic classes. By considering limited observations from a test speaker, the resulting i-vector as a representative of short utterance will be uncertain; the shorter the duration, the higher the uncertainty. In recent studies, an uncertainty decoding technique has been employed in probabilistic linear discriminant analysis (PLDA) modeling in order to account for uncertain i-vectors. In this paper, we propose to extend uncertainty handling using simplified PLDA scoring and modified imputation. We experiment with a state-of-the-art speaker recognition system focusing on uncertainty caused by controlled utterance duration. The uncertainties after i-vector extraction are being propagated through pre-processing steps and both uncertainty decoding and modified imputation are considered. Our experimental results indicate improved equal error rate and detection cost attained by using uncertainty-of-observation techniques in dealing with short duration utterances.	acoustic cryptanalysis;geo-imputation;linear discriminant analysis;preprocessor;propagation of uncertainty;software propagation;speaker recognition;uncertainty principle	Rahim Saeidi;Paavo Alku	2015			speech recognition;pattern recognition;imputation (statistics);artificial intelligence;speaker diarisation;speaker recognition;propagation of uncertainty;computer science	ML	-14.482884911475361	-92.07498562280239	166308
3b19c4c71d425af5b4a4ec6ad55b7e67e7e58967	a fully time-domain neural model for subband-based speech synthesizer		This paper introduces a deep neural network model for subband-based speech 1 synthesizer. The model benefits from the short bandwidth of the subband signals 2 to reduce the complexity of the time-domain speech generator. We employed 3 the multi-level wavelet analysis/synthesis to decompose/reconstruct the signal to 4 subbands in time domain. Inspired from the WaveNet, a convolutional neural 5 network (CNN) model predicts subband speech signals fully in time domain. Due 6 to the short bandwidth of the subbands, a simple network architecture is enough to 7 train the simple patterns of the subbands accurately. In the ground truth experiments 8 with teacher forcing, the subband synthesizer outperforms the fullband model 9 significantly. In addition, by conditioning the model on the phoneme sequence 10 using a pronunciation dictionary, we have achieved the first fully time-domain 11 neural text-to-speech (TTS) system. The generated speech of the subband TTS 12 shows comparable quality as the fullband one with a slighter network architecture 13 for each subband. 14	artificial neural network;deep learning;dictionary;experiment;ground truth;network architecture;network model;speech synthesis;sub-band coding;wavelet	Azam Rabiee;Soo-Young Lee	2018	CoRR		wavelet;convolutional neural network;artificial neural network;network architecture;ground truth;machine learning;time domain;speech synthesis;computer science;artificial intelligence;bandwidth (signal processing)	ML	-17.40170682276749	-87.87990187993056	166616
d0f88f49914db4cbe6a6f4aa4ab50cd6115ca502	anchor model fusion for emotion recognition in speech	emotion recognition;anchor models;feature space;performance improvement;conferenceobject;gmm supervectors;prosodic features;svm;bookpart	In this work, a novel method for system fusion in emotion recognition for speech is presented. The proposed approach, namely Anchor Model Fusion (AMF), exploits the characteristic behaviour of the scores of a speech utterance among different emotion models, by a mapping to a back-end anchor-model feature space followed by a SVM classifier. Experiments are presented in three different databases: Ahumada III, with speech obtained from real forensic cases; and SUSAS Actual and SUSAS Simulated. Results comparing AMF with a simple sum-fusion scheme after normalization show a significant performance improvement of the proposed technique for two of the three experimental set-ups, without degrading performance in the third one.	action message format;database;emotion recognition;feature vector	Carlos Ortego-Resa;Ignacio Lopez-Moreno;Daniel Ramos-Castro;Joaquín González-Rodríguez	2009		10.1007/978-3-642-04391-8_7	psychology;speech recognition;pattern recognition;communication	NLP	-14.487072695246741	-89.13744442887396	166651
826a8f6812d27270de036cd3ce0b54ad6a87bedc	maximum likelihood successive state splitting	decision tree;maximum likelihood;cost reduction;natural languages;trees mathematics;maximum likelihood estimation;continuous speech recognition;training conditions maximum likelihood successive state splitting contextual variations modeling continuous speech recognition system robust models decision tree clustering output distributions hmm topologies hmm design algorithm hmm topology greedy search contextual split temporal split japanese phone recognition experiments constrained em algorithm ml sss recognition performance gains training cost reduction;hidden markov models;search problems speech recognition maximum likelihood estimation hidden markov models natural languages decision theory trees mathematics;hidden markov models context modeling topology algorithm design and analysis speech recognition robustness decision trees clustering algorithms performance gain costs;decision theory;speech recognition;search problems;em algorithm	Modeling contextual variations of phones is widely accepted as an important aspect of a continuous speech recognition system, and much research has been devoted to finding robust models of context for HMhiI systems. In particular, decision tree clustering has been used to tie output distributions across pre-defined states, and successive state splitting (SSS) has been used to define parsimonious HMM topologies. In this paper, we describe a new HMM design algorithm, called maximum likelihood successive state splitting (ML-SSS), that combines advantages of both these approaches. Specifically, an HMM topology is designed using a greedy search for the best temporal and contextual splits using a coiistrained EM algorithm. In Japanese phone recognition experiments, ML-SSS shows recognition performance gains and training cost reduction over SSS under several training conditions.	cluster analysis;decision tree;expectation–maximization algorithm;experiment;greedy algorithm;hidden markov model;occam's razor;speech recognition	Harald Singer;Mari Ostendorf	1996		10.1109/ICASSP.1996.543192	speech recognition;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;hidden markov model;statistics	ML	-19.056799380828394	-91.78421814598849	166674
fa451c21ae980405f9aebe68420da0a0c356dc0e	a multilayer perceptron postprocessor to hidden markov modeling for speech recognition	error reduction;neural networks;hidden markov model;multilayer perceptrons;vocabulary;multilayer perceptron;speech enhancement;maximum likelihood estimation;base isolation;classification capability;continuous speech recognition;multilayer perceptron postprocessor;hidden markov models;internet;speech recognition equipment;speech recognition;feedforward neural nets;neural network postprocessor;parameter estimation;hidden markov modeling;power system modeling;speech recognition equipment feedforward neural nets hidden markov models;chinese multilayer perceptron postprocessor neural network postprocessor classification capability hidden markov modeling speech recognition stimuli error reduction;chinese;hidden markov models multilayer perceptrons speech recognition neural networks maximum likelihood estimation parameter estimation vocabulary internet speech enhancement power system modeling;neural network;stimuli	A novel neural network postprocessor for enhancing the classification capability of hidden Markov modeling for speech recognition is introduced. This postprocessor receives stimuli not from one but from all word HMMs and does not require segmentation of speech frames at the subword level. This postprocessor achieved 20% to 30% initial part error reduction on an HMM (hidden Markov model)-based isolated Chinese whole syllable speech recognition system, and can also be used for continuous speech recognition. >		Jin Guo;Ho-Chung Lui	1993		10.1109/ICASSP.1993.319286	base isolation;speech recognition;computer science;machine learning;pattern recognition;multilayer perceptron;chinese;artificial neural network;hidden markov model	NLP	-16.023302468500127	-87.65929090726816	166798
a09fec0c446dfd82a9ad3c0ba74d47a7fd5ba9da	sensitivity to voicing similarity in printed stimuli: effect of a training programme in dyslexic children		Dyslexic children exhibit great difficulties in acquiring reading skills, despite adequate intelligence and instruction, and in the absence of any obvious neurological or sensory disorders. Both phonological and surface dyslexics are impaired in phonological skills (Sprenger-Charolles, Col! e, Lacert, u0026 Serniclaes, 2000), but the origin of these disabilities is controversial. According to the ‘rapid processing hypothesis’, phonological impairments in dyslexia stem from an auditory deficit in the processing of brief and/or rapidly changing acoustic events, which compromises phoneme discrimination, and the acquisition of metaphonological skills and grapheme–phoneme correspondence rules (Nagarajan et al., 1999; Tallal, 1980). According to the ‘linguistic hypothesis’, auditory deficits and language disorders may be associated but are not causally related (Cornelissen, Hansen, Hutton, Evangelinou, u0026 Stein, 1998; Nittrouer, 1999; Rosen, 2003). The evidence for this view is twofold. First, many studies demonstrated the existence of a speech-specific impairment in dyslexia (Mody, Studdert-Kennedy, u0026 Brady, 1997; Rosen u0026 Manganari, 2001). Second, the processing of short and/or rapidly varying acoustic signals may not be the fundamental problem in dyslexia (Bradlow et al., 1999). Intensive training with artificially slowed speech did not improve reading and phonemic awareness, compared with training on unmodified speech (Rey, De Martino, Espesser, u0026 Habib, 2002). Whatever the origin of linguistic disorders in dyslexic children, it is generally agreed that the core deficit is phonological. In this paper, we ask which phonological mechanisms and which aspects of phonological knowledge are impaired in dyslexia. We assume that this impairment relates to the phonetic underpinnings of phonemic knowledge. This hypothesis is supported by recent experiments on the effect of phonetic similarity in reading. ARTICLE IN PRESS	printing	Nathalie Bedoin	2003	J. Phonetics	10.1016/S0095-4470(03)00044-5	speech recognition;acoustics;communication	HCI	-8.959806645311282	-81.05081078387836	166949
6606df16a290ca089b29ffaa637913d271158e81	analysis and critical-band-based group wavetable synthesis of piano tones			critical band	Hua Zheng;James W. Beauchamp	1999			acoustics;piano;critical band;computer science	Logic	-8.528695542766416	-84.59749336860351	166964
4e53d558880e33083e9b3d9418a2327428ee29d7	common sounds in bedrooms (csibe) corpora for sound event recognition of domestic robots		Although sound event recognition attracted much attention in the scientific community, applications in the robotics domain have not been in the focus. A new database was published in this paper and classifiers were evaluated with this dataset to guide the future practical developments of domestic robots. A corpus (CSIBE-RAW) was collected from the internet to build acoustic models to recognize 13 sound events and omit ambient sounds. As a case study, CSIBE-RAW was rerecorded in four room settings (CSIBE-AIBO) to create reverberation-tolerant classifiers for a Sony ERS-7. After eight classifiers were reviewed, the convolutional neural network achieved the best accuracy (95.07%) after multi-conditional learning and it was suitable for real-time classification on the robot. The effects of lossy audio codecs were studied, lossy encoder-tolerant audio statistics were specified for the feature vector and the Ogg Vorbis encoder with 128 kbit VBR was found superior to store big data and avoid any significant accuracy loss with the compression ratio 1:8.	domestic robot;text corpus	Csaba Kertész;Markku Turunen	2018	Intelligent Service Robotics	10.1007/s11370-018-0258-9	speech recognition;computer vision;lossy compression;encoder;convolutional neural network;variable bitrate;codec;deep learning;feature vector;vorbis;computer science;artificial intelligence	Robotics	-5.567845555322156	-87.42113760141113	166994
097273bd67320404a6767d9f8fdc3a28845221d7	automatic voice-source parameterization of natural speech	quadratic program;speech synthesis;vocal tract;synthetic data;voice quality	We present here our work in automatic parameterization of natural speech by means of a pitch synchronous source-filter decomposition algorithm. The derivative glottal source is modelled using the Liljencrants-Fant (LF) model. The model parameters are obtained simultaneously with the coefficients of an all-pole filter representing the vocal tract response by means of a quadratic programming algorithm. Synthetic data has been created and analyzed in order to show the appropriate function of the estimation method. The parameterization results in high quality synthesized speech for voiced frames. Voice quality extraction is performed on basis to the LF source representation. The inherent modelling of the voice source makes it suitable for voice modification tasks. Work is in progress to add this speech representation to emotional speech synthesis and voice conversion algorithms.	algorithm;coefficient;display resolution;natural language;newton's method;quadratic programming;speech synthesis;synthetic data;tract (literature)	Javier Pérez;Antonio Bonafonte	2005			speech recognition;artificial intelligence;quadratic programming;parametrization;pattern recognition;computer science;synthetic data;speech synthesis	NLP	-9.443173058012665	-86.72728921652629	167381
9afdf691da2b20bc8836fc5c54f2b4c55731db9c	transforming hmms for speaker-independent hands-free speech recognition in the car	automobiles;digit error rate hmm speaker independent hands free speech recognition car recording condition target environment quiet office high quality microphone performance acoustic conditions linear regression based model adaptation adaptation utterances transformation speaker independent test results;model adaptation;linear regression;maximum likelihood estimation hidden markov models automobiles speech recognition acoustic noise;maximum likelihood estimation;hidden markov models speech recognition microphones additive noise speech enhancement maximum likelihood linear regression acoustic testing character recognition nonlinear filters target recognition;hidden markov models;speaker independent;acoustic noise;error rate;speech recognition	In the absence of HMMs trained with speech collected in the target environment, one may use HMMs trained with a large amount of speech collected in another recording condition (e.g., quiet office, with high quality microphone). However, this may result in poor performance because of the mismatch between the two acoustic conditions. We propose a linear regression-based model adaptation procedure to reduce such a mismatch. With some adaptation utterances collected for the target environment, the procedure transforms the HMMs trained in a quiet condition to maximize the likelihood of observing the adaptation utterances. The transformation must be designed to maintain speaker-independence of the HMM. Our speaker-independent test results show that with this procedure about 1% digit error rate can be achieved for hands-free recognition, using target environment speech from only 20 speakers.	acoustic cryptanalysis;display resolution;hidden markov model;microphone;speech recognition	Yu Gong;John J. Godfrey	1999		10.1109/ICASSP.1999.758121	speaker recognition;speech recognition;word error rate;computer science;linear regression;noise;pattern recognition;maximum likelihood;hidden markov model;statistics	ML	-15.183904913415619	-92.25000135397657	167431
ddc22098fee8cfef67c528cf9f5dcd12f4aeab56	high performance automatic mispronunciation detection method based on neural network and trap features	high performance;neural network	In this paper, we propose a new approach to utilize temporal information and neural network (NN) to improve the performance of automatic mispronunciation detection (AMD). Firstly, the alignment results between speech signals and corresponding phoneme sequences are obtained within the classic GMM-HMM framework. Then, the long-time TempoRAl Patterns (TRAPs) [5] features are introduced to describe the pronunciation quality instead of the conventional spectral features (e.g. MFCC). Based on the phoneme boundaries and TRAPs features, we use Multi-layer Perceptron (MLP) to calculate the final posterior probability of each testing phoneme, and determine whether it is a mispronunciation or not by comparing with a phone dependent threshold. Moreover, we combine the TRAPs-MLP method with our existing methods to further improve the performance. Experiments show that the TRAPs-MLP method can give a significant relative improvement of 39.04% in EER (Equal Error Rate) reduction, and the fusion of TRAPs-MLP, GMM-UBM and GLDS-SVM [4] methods can yield 48.32% in EER reduction relatively, both compared with the baseline GMM-UBM method.	artificial neural network;baseline (configuration management);enhanced entity–relationship model;google map maker;hidden markov model;perceptron;quad flat no-leads package;temporal logic	Hongyan Li;Shijin Wang;Jiaen Liang;Shen Huang;Bo Xu	2009			computer science;time delay neural network;artificial neural network	NLP	-17.110290654085542	-88.60991902820902	167484
89615864ceb412304fc3ee361f96df26164532a5	the modeling and realization of natural speech generation system	language understanding;speech intelligibility;natural languages speech synthesis bidirectional control signal processing australia information science man machine systems interactive systems character generation;speech synthesis;speech intelligibility speech synthesis grammars speech based user interfaces linguistics;chinese text to speech conversion system natural speech generation system chinese natural speech generation chinese bi directional grammar chinese language understanding naturalness intelligibility;speech based user interfaces;grammars;text to speech;text generation;linguistics	The paper gives an overall discussion on problems in Chinese natural speech generation. We considered not only how to convert text into speech but also how to generate the necessary text in text-tospeech conversion.A Chinese Bi-directional Grammar is developed to suit for Chinese Language understanding and generation. The system gets the right text and generates speech which have good quality in naturalness and intelligibility using Chinese Text-toSpeech Conversion System.	intelligibility (philosophy);natural language;speech synthesis;synthetic intelligence	Fang Chen;Baozong Yuan	1998		10.1109/ISSPA.1999.818122	natural language processing;speech technology;telegraphic speech;cued speech;speech recognition;speech corpus;computer science;speech;speech processing;chinese speech synthesis;speech synthesis;intelligibility;speech analytics	NLP	-16.065341736988426	-84.93652879243385	167510
198f83ab41b8115d6f5d6592b4613bbaf31284c8	a cluster-based multiple deep neural networks method for large vocabulary continuous speech recognition	pattern clustering;decoding;backpropagation;hidden markov models;time 309 hour error backpropagation algorithm bp algorithm multiple computing threads modern gpu board posterior probabilities training data decoding mandarin transcription task switchboard hub5 task asr large scale automatic speech recognition hidden markov model method large scale hmm context dependent hybrid dnn large vocabulary continuous speech recognition cluster based multiple deep neural networks method time 64 hour;speech recognition;training hidden markov models speech recognition training data neural networks acoustics speech;speech recognition backpropagation decoding hidden markov models pattern clustering;parallelization among gpus lvcsr dnn state clustering cluster based multi dnn	Recently a pre-trained context-dependent hybrid deep neural network (DNN) and HMM method has achieved significant performance gain in many large-scale automatic speech recognition (ASR) tasks. However, the error back-propagation (BP) algorithm for training neural networks is sequential in nature and is hard to parallelize into multiple computing threads. Therefore, training a deep neural network is extremely time-consuming even with a modern GPU board. In this paper we have proposed a new acoustic modelling framework to use multiple DNNs instead of a single DNN to compute the posterior probabilities of tied HMM states. In our method, all tied states of context-dependent HMMs are first grouped into several disjoined clusters based on the training data associated with these HMM states. Then, several hierarchically structured DNNs are trained separately for these disjoined clusters of data using multiple GPUs. In decoding, the final posterior probability of each tied HMM state can be calculated based on output posteriors from multiple DNNs. We have evaluated the proposed method on a 64-hour Mandarin transcription task and 309-hour Switchboard Hub5 task. Experimental results have shown that the new method using clusterbased multiple DNNs can achieve over 5 times reduction in total training time with only negligible performance degradation (about 1-2% in average) when using 3 or 4 GPUs respectively.	acoustic cryptanalysis;algorithm;artificial neural network;backpropagation;cluster analysis;context-sensitive language;deep learning;elegant degradation;graphics processing unit;hidden markov model;software propagation;speech recognition;super robot monkey team hyperforce go!;telephone switchboard;transcription (software);vocabulary	Pan Zhou;Cong Liu;Qingfeng Liu;Li-Rong Dai;Hui Jiang	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638948	speech recognition;computer science;backpropagation;machine learning;pattern recognition;hidden markov model	Robotics	-18.717565181185503	-88.5227328945573	167564
616b8fad682feff8a044d971ea7291508eca1d7f	selected topics from 40 years of research on speech and speaker recognition	speaker recognition	This paper summarizes my 40 years of research on speech and speaker recognition, focusing on selected topics that I have investigated at NTT Laboratories, Bell Laboratories and Tokyo Institute of Technology with my colleagues and students. These topics include: the importance of spectral dynamics in speech perception; speaker recognition methods using statistical features, cepstral features, and HMM/GMM; text-prompted speaker recognition; speech recognition using dynamic features; Japanese LVCSR; robust speech recognition; spontaneous speech corpus construction and analysis; spontaneous speech recognition; automatic speech summarization; and WFST-based decoder development and its applications.	cepstrum;google map maker;hidden markov model;part-of-speech tagging;speaker recognition;speech analytics;speech corpus;speech recognition;spontaneous order	Sadaoki Furui	2009			natural language processing;speech technology;speaker recognition;speaker diarisation;speech recognition;speech corpus;computer science;speech synthesis	NLP	-15.405944119064015	-86.63711467566762	167623
2a7b54f0fb4bc1c451a48f7d46656e8b7a499ac0	modeling temporal dependency for robust estimation of lp model parameters in speech enhancement		This paper presents a novel approach to robust estimation of linear prediction (LP) model parameters in the application of speech enhancement. The robustness stems from the use of prior knowledge on the clean speech and the interfering noise, which are represented by two separate codebooks of LP model parameters. We propose to model the temporal dependency between short-time model parameters with a composite hidden Markov model (HMM) that is constructed by combining the speech and the noise codebooks. Optimal speech model parameters are estimated from the HMM state sequence that best matches the input observation. To further improve the estimation accuracy, we propose to perform interpolation of multiple HMM state sequences such that the estimated speech parameters would not be limited by the codebook coverage. Experimental results demonstrate the benefits and effectiveness of temporal dependency modeling and states interpolation in improving the segmental signal-to-noise ratio, PESQ and spectral distortion of enhanced speech.	codebook;distortion;hidden markov model;interpolation;markov chain;pesq;signal-to-noise ratio;speech enhancement	Chun Hoy Wong;Tan Lee;Yu Ting Yeung;Pak-Chung Ching	2015			pattern recognition;artificial intelligence;speech recognition;speech enhancement;computer science	ML	-13.660153840787778	-93.45783969389593	167648
c1e954b40ab893358a367c8072a2f8c63f38ca5a	the effect of early bilingualism on perceived foreign accent		High degree of between-rater variability in pronunciation assessment is often reported in literature. However, human assessments of pronunciation skills of second language (L2) learners are used in standardized languageproficiency tests. Besides, these scores are used as a reference point in evaluating computer-based systems for pronunciation teaching and testing. Therefore it is important to be aware of rater-related factors that might affect the degree of perceived foreign accent in L2 speech. We used Cronbach’s alpha and inter-class coefficient to estimate the between-rater agreement of 10 native English speakers who assessed accentedness in L2 utterances. We found that early immersion into bilingual environment might affect the degree of perceived foreign accent. This finding can be explained by interaction of two linguistic systems in early language acquisition, when phoneme prototypes are formed based on language-specific fine phonetic details.	coefficient;heart rate variability;immersion (virtual reality);linguistic systems	Leona Polyanskaya	2015			cronbach's alpha;language acquisition;linguistics;neuroscience of multilingualism;psychology;pronunciation	HCI	-11.221097339547265	-82.31480285378825	167654
091aec3ad200403cae8c8c757641751f6bdc0eec	sddd: a new dissimilarity index for the comparison of speech spectra	spectrum analysis;analyse spectre;reconocimiento palabra;french;analisis espectro;speech processing;tratamiento palabra;traitement parole;intelligence artificielle;voice;voz;frances;francais;identification;indexation;characterization;speech recognition;artificial intelligence;identificacion;inteligencia artificial;reconnaissance parole;caracterisation;speaker;locutor;caracterizacion;locuteur;voix	Abstract   SDDD, an index for the comparison of speech spectra, is introduced in this paper. Its discriminatory ability is established by comparing SDDD with the best classical index on the basis of a speaker-recognition experiment using Long Term Average Spectra.		Bernard Harmegnies	1988	Pattern Recognition Letters	10.1016/0167-8655(88)90093-1	speech recognition;french;computer science;artificial intelligence;speech processing	Vision	-8.652917086564646	-89.70115281493007	167672
168ad9f14176588cda42e20a51a0fabab07e6c8d	towards improving speech detection robustness for speech recognition in adverse conditions	likelihood ratio;wireless network;speech enhancement;spectral subtraction;adaptive algorithm;speech recognition;likelihood ratio criterion;speech non speech detection;wavelets;speech detection	Recognition performance decreases when recognition systems are used over the telephone network, especially wireless network and noisy environments. It appears that non-efficient speech/non-speech detection (SND) is an important source of this degradation. Therefore, speech detection robustness to noise is a challenging problem to be examined, in order to improve recognition performance for the very noisy communications. Several studies were conducted aiming to improve the robustness of SND used for speech recognition in adverse conditions. The present paper proposes some solutions aiming to improve SND in wireless environment. Speech enhancement prior detection is considered. Then, two versions of SND algorithm, based on statistical criteria, are proposed and compared. Finally, a post-detection technique is introduced in order to reject the wrongly detected noise segments. 2002 Elsevier Science B.V. All rights reserved.	algorithm;elegant degradation;finite-state machine;noise reduction;part-of-speech tagging;preprocessor;speech enhancement;speech recognition;statistical learning theory;statistical model;video post-processing;wavelet	Lamia Karray;Arnaud Martin	2003	Speech Communication	10.1016/S0167-6393(02)00066-3	voice activity detection;wavelet;speech recognition;likelihood-ratio test;computer science;wireless network;pattern recognition;statistics	AI	-13.096437812201502	-91.86975385450924	167893
067ca020ec80616f4efb5c9c8ef387dad13eacdd	on parameter filtering in continuous subword-unit-based speech recognition	finite impulse response filter;context modeling;fir filters;filtering;bandwidth;hidden markov models;fir filter;side effect;word recognition;context dependent;speech recognition;testing	Simple IIR or FIR filters have been widely used in isolated or connected word recognition tasks to filter the time sequence of speech spectral parameters, since, despite their simplicity, they significantly improve recognition performance. Those filters, when applied to continuous speech recognition, where phoneme-sized modelling units are used, induce spectral transition spreading and a cross-boundary effect. In this work, we show how the use of context-dependent units reduces the side effects of the filters and may result in improved recognition performance. When dynamic parameters are not used, filtering seems to be especially useful, even for clean speech, and when they are, filters do well under unmatched training and testing conditions.	context-sensitive language;finite impulse response;infinite impulse response;side effect (computer science);speech recognition;substring;time series	Pau Pachès-Leal;Climent Nadeu	1996			speech recognition;computer science;pattern recognition;communication	ML	-12.693840282218869	-91.55354958007165	168046
802e657145244f2da7f3c0b4fdb201b35b65081d	model-based speaker normalization methods for speech recognition.	speech recognition	A speaker normalization method using a speech generation model is proposed in order to achieve high-performance speaker adaptation with a small amount of adaptation data. The speaker- and phoneme-dependent vocal tract area function is approximated by the corresponding area function produced by the articulatory model of a standard speaker, combined with phoneme-independent feature quantities of the vocal-tract shape of the normalized target speaker as estimated from the formant frequencies of two vowels. The frequency warping functions are determined from the formant frequencies of speech calculated from the vocal-tract area functions thus obtained, and normalization of the uttered speech is performed by stretching the speech spectrum in the frequency-axis direction. Continuous phoneme recognition experiments using phoneme connection rules show that the recognition error using a gender-dependent model is reduced by about 30% in the proposed method and that recognition performance superior to that of vocal-tract length normalization is obtained. The recognition performance of the proposed method is also equivalent to that of speaker adaptation by moving vector field smoothing (VFS) using 10 phonetically balanced sentences, showing that high-performance speaker adaptation using a small amount of adaptation data can be achieved by the proposed method. © 2003 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 86(2): 45–56, 2003; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/ecjb.10119		Masaki Naito;Li Deng;Yoshinori Sagisaka	1999			speaker recognition;speaker diarisation;speech recognition;computer science;voice analysis;acoustic model	ML	-10.357322333958203	-87.21521927217601	168179
7884225d08704bd2555814421e39588400680db5	compression techniques applied to multiple speech recognition systems	speech recognition	Speech recognition systems typically contain many Gaussian distributions, and hence a large number of parameters. This makes them both slow to decode speech, and large to store. Techniques have been proposed to decrease the number of parameters. One approach is to share parameters between multiple Gaussians, thus reducing the total number of parameters and allowing for shared likelihood calculation. Gaussian tying and subspace clustering are two related techniques which take this approach to system compression. These techniques can decrease the number of parameters with no noticeable drop in performance for single systems. However, multiple acoustic models are often used in real speech recognition systems. This paper considers the application of Gaussian tying and subspace compression to multiple systems. Results show that two speech recognition systems can be modelled using the same number of Gaussians as just one system, with little effect on individual system performance.	acoustic cryptanalysis;acoustic model;cluster analysis;clustering high-dimensional data;speech recognition	Catherine Breslin;Matthew N. Stuttle;Kate Knill	2009			speech recognition;computer science;machine learning;pattern recognition	ML	-17.005152185652356	-92.29484474997437	168200
3e0119945a345853123744bc6c3b28a9a16010fe	relationship between chinese speech intelligibility and speech transmission index using diotic listening	analyse parole;pulse response;speech transmission;transmission parole;speech intelligibility;earphone;anechoic room;peng jianxin 双听技术 双耳听力 立体声 脉冲反应 relationship between chinese speech intelligibility and speech transmission index in rooms using dichotic listening;analisis palabra;speech analysis;weighting;simulation;simulacion;respuesta impulsion;chino;ponderacion;senal vocal;ecouteur;verbal perception;intelligibilite parole;algorithme;algorithm;room impulse response;mandarin chinese;signal vocal;simulation software;percepcion verbal;evaluation subjective;reponse impulsion;chambre anechoique;signal acoustique;audifono;acoustic signal;ponderation;speech transmission index;transmision palabra;vocal signal;chinois;subjective evaluation;chinese;room acoustics;senal acustica;chinese speech intelligibility;perception verbale;evaluacion subjetiva;algoritmo	The speech intelligibility in rooms is evaluated using the room impulse responses obtained from the room acoustical simulation software ODEON. The simulated room impulse responses are first convolved with the speech intelligibility test signals recorded in an anechoic chamber, then reproduced through the earphone. The subjective Chinese speech intelligibility scores are obtained and the relationship between Chinese speech intelligibility scores and speech transmission index (STI) is built and validated. The result shows that there is high correlation between Chinese speech intelligibility scores and STI. The STI method can predict and evaluate the speech intelligibility for Mandarin Chinese without changes in the algorithm of the weighting values for diotic listening in rooms.	intelligibility (philosophy)	Peng Jianxin	2007	Speech Communication	10.1016/j.specom.2007.06.001	speech recognition;simulation software;mandarin chinese;room acoustics;computer science;weighting;linguistics;chinese;intelligibility	Graphics	-9.946747840483813	-86.51719673119665	168369
29f824df773908610c51563c762b7d3e95effcc3	the development of an acoustic based home monitoring system to improve home care			acoustic cryptanalysis	Bert Van Den Broeck	2017				HCI	-6.599997076629004	-84.75146405924069	168732
cedeb3ed9d55a47cb63a8f9da7c26a7eb13e4fbe	improve the implementation of pitch features for mandarin digit string recognition task	automatic speech recognition;mandarin digit string recognition;pitch feature extraction	Mandarin digit string recognition (MDSR) is a difficult task in the field of automatic speech recognition (ASR) and using pitch feature can significantly improve the performance. In conventional methods of pitch feature extraction, random value is commonly used as pitch output in unvoiced (UV) frames, which causes serious statistical confusion between voiced (V) and UV units and incurs abnormal likelihood in decoding. In this paper we propose to normalize the distribution of random values assigned in UV frames to avoid the above side-effects and introduce extra discrimination information in statistics. Besides, voice-level (VL), which is an intermedial parameter used in pitch estimation for V/UV decision, is adopted to expand the acoustic feature stream. VL features indicate the intensity of periodicity of speech frames and provide complementary information for ASR. In the experiments the proposed methods significantly increase the accuracy of MDSR tasks and achieve the sentence error reduction rate (ERR) of 13.3% and 15.1% versus the baseline in the evaluation on free-length and 6-digit testing set, respectively.	acoustic cryptanalysis;automated system recovery;baseline (configuration management);experiment;feature extraction;jumbo frame;kullback–leibler divergence;pitch (music);pitch detection algorithm;quasiperiodicity;speech recognition;super robot monkey team hyperforce go!;uv mapping	Pei Ding;Liqiang He	2012			pattern recognition;artificial intelligence;speech recognition;mandarin chinese;computer science;numerical digit	NLP	-13.347250584541344	-89.09138787355079	168928
b4796efd6e3fbc55d397fff2afebbb5e7a6060d9	model-based noise reduction leveraging frequency-wise confidence metric for in-car speech recognition	relative word error rate model based noise reduction leveraging frequency wise confidence metric in car speech recognition model based approach automatic speech recognition noisy environments minimum mean square estimate mmse criterion de noised speech estimates speech dominant bands noise dominant bands mel spectral domain speech dominated bands posterior probability local peak weight lpw feature reconstruction integrated probabilistic model baseline mmse method;robust speech recognition;least mean squares methods;noise speech noise reduction speech recognition harmonic analysis noise measurement;speech;speech enhancement;noise measurement;missing feature harmonic analysis speech enhancement robust speech recognition model based noise reduction;model based noise reduction;noise reduction;speech recognition;missing feature;speech recognition least mean squares methods signal denoising;noise;signal denoising;harmonic analysis	Model-based approaches for noise reduction effectively improve the performance of automatic speech recognition in noisy environments. Most of them use the Minimum Mean Square Estimate (MMSE) criterion for de-noised speech estimates. In general, an observation has speech-dominant bands and noise-dominant bands in the Mel spectral domain. This paper introduces a method to add weight to speech-dominated bands when evaluating the posterior probability of each speech state, as these bands are generally more reliable. To leverage high-resolution information in the Mel domain, we use Local Peak Weight (LPW) as the confidence metric for the degree of speech dominance. This information is also used to regulate the amount of compensation that is applied to each frequency band during feature reconstruction under an integrated probabilistic model. The method produced relative word error rate improvements of up to 33.8% over the baseline MMSE method on an isolated word task with car noise.	baseline (configuration management);frequency band;image resolution;mean squared error;noise reduction;speech recognition;statistical model;word error rate	Osamu Ichikawa;Steven J. Rennie;Takashi Fukuda;Masafumi Nishimura	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6289023	speech recognition;computer science;noise measurement;noise;speech;harmonic analysis;pattern recognition;noise reduction;mathematics	Vision	-13.328075067514522	-92.6230581456202	168965
0b374a2af41cb47cdd54cff4176e809ebf2fa68b	modeling the creaky excitation for parametric speech synthesis		In order to produce natural sounding output, corpus-based speech synthesis systems need to be able to properly model the acoustic variability in the corpus. Creaky voice is a voice quality frequently produced in many languages, in both read and conversational speech settings. However, the creaky excitation displays different acoustic characteristics than modal excitations and is, hence, not suitably modelled by standard vocoders. This study presents an analysis of the creaky excitation which is used to derive an extension of the Deterministic plus Stochastic Model of the residual signal. This proposed model is designed to appropriately model creaky voice and is integrated into a vocoder for parametric speech synthesis. Copy-synthesis versions of short speech segments containing creaky voice were used in a subjective listening test which revealed clearly better rendering of the voice quality than a standard vocoder.	acoustic cryptanalysis;automatic sounding;heart rate variability;modal logic;norm (social);speech synthesis;text corpus;vocoder	Thomas Drugman;John Kane;Christer Gobl	2012			residual;speech recognition;rendering (computer graphics);creaky voice;artificial intelligence;pattern recognition;parametric statistics;excitation;active listening;computer science;speech synthesis	NLP	-13.70053087682551	-84.49763056377843	169158
115c65c1e369d2fce684b91dd91b88dad0136454	a classification of byzantine singing voices based on singer's formant	color;larynx;trajectory;shape;power harmonic filters;music;harmonic analysis	A classification of different singing voices of Greek psaltes (chanters) of the Church, based on the relative positions and shapes of the ≪singer's formant≫ (SF) is presented. It was found that there are 8 categories defined on basis of the following criteria: (1) the location of the center of the clustered vowels in the corre-logram (F3-F4), (2) the maximum of F3 and minimum of F4 or F5, (3) the clustering of F3-F4-F5, and, (4) the variability of singer's formant shape among vowels of individual psaltic voice. There are two main singer's formants contrary to that (only one) of famous opera male singers.	cluster analysis;spatial variability	D. S. Delviniotis	1998	9th European Signal Processing Conference (EUSIPCO 1998)		speech recognition;formant;acoustics;communication	Vision	-11.417323734772138	-84.87419802873345	169163
033dae9a74ca50f227bcd10d9354f027c6972555	generating and evaluating musical harmonizations that emulate style	theoretical framework;decision tree;metric entropy;hybrid approach;markov model;statistical learning;quantitative method;divide and conquer;markov chain	In this article we present a hybrid approach to the design of an automatic, style-specific accompaniment system that combines statistical learning with a music-theoretic framework, and we propose quantitative methods for evaluating the results of machine-generated accompaniment. The system is capable of learning accompaniment style from sparse input information, and of capturing style over a variety of musical genres. Generating accompaniments involves several aspects, including choosing chords, determining the bass line, arranging chords for voicing, instrumentation, etc. In this article we focus on harmonization: selecting chords for melodies, with an emphasis on style. Given exemplar songs as MIDI melodies with corresponding chords labeled as text, the system uses decision trees to learn the melody–chord relations shared among the songs. Markov chains on the neo-Riemannian framework are applied to model the likelihood of chord patterns. Harmonization is generated in a divide-and-conquer manner: Melody fragments that strongly imply certain triads are designated as checkpoints that are in turn connected by chord progressions generated using the Markov model. Chord subsequences are then refined and combined to form the final sequence. We propose two types of measures to quantify the degree to which a machine-generated accompaniment achieves its style emulation goal: one based on chord distance, and the other based on the statistical metrics entropy and perplexity. Using these measures, we conduct two sets of experiments using Western popular songs. Two albums by each of three artists (Green Day, Keane, and Radiohead), for a total of six albums, are used to evaluate the	beneath a steel sky;chord diagram;computer music journal;computer-generated holography;content-addressable memory;database index;decision tree;emulator;entropy (information theory);experiment;hidden markov model;information retrieval;knowledge-based systems;list of online music databases;logic programming;midi;machine learning;markov chain;markov model;natural language processing;perplexity;programming tool;rule-based system;smoothing;sparse matrix;test set;theory;turing test;user interface	Ching-Hua Chuan;Elaine Chew	2011	Computer Music Journal	10.1162/COMJ_a_00091	markov chain;divide and conquer algorithms;speech recognition;quantitative research;computer science;artificial intelligence;decision tree;markov model;world wide web	ML	-18.55971519388834	-81.77446544486537	169240
4f40d79d05def8c033549463afc37219b4cbd267	constructing multi-level speech database for spontaneous speech processing	natural language interfaces;speech synthesis;database management systems;speech processing databases speech synthesis speech recognition speech analysis process design context modeling synthesizers information processing natural languages;speech processing;spontaneous speech;interactive systems;database management systems speech processing natural language interfaces speech synthesis interactive systems;transcribed texts multi level speech database spontaneous speech processing acoustic variations declarative speech interview simulated interview transcription symbols transcription rules prosodic information prosodic analysis diversified speech database dialogue speech synthesis	This paper describes a new database, called muti-level speech database, for spontaneous speech processing. We designed the database to cover textual and acoustic variations from declarative speech to spontaneous speech. The database is composed of 5 categories which are, in the order of decreasing spontaneity, spontaneous speech, interview, simulated interview, declarative speech with context, and declarative speech without context. We collected total 112 sets from 23 subjects(male: 19, female: 4). Then the database was rstly transcribed using 15 transcription symbols according to our own transcription rules. Secondly, prosodic information will be added. The goal of this research is a comparative textual and prosodic analysis at each level, quanti cation of spontaneity of diversi ed speech database for dialogue speech synthesis and recognition. From the preliminary analysis of transcribed texts, the spontaneous speech has more corrections, repetitions, and pauses than the others as expected. In addition, average number of sentences per turn of spontaneous speech is greater than the others. From the above results, we can quantify the spontaneity of speech database.	acoustic cryptanalysis;declarative programming;emergence;speech corpus;speech processing;speech synthesis;spontaneous order;transcription (software)	Minsoo Hahn;Sanghun Kim;Jung-Chul Lee;Yong-Ju Lee	1996		10.1109/ICSLP.1996.608012	voxforge;natural language processing;speech technology;speech production;audio mining;speech recognition;speech corpus;computer science;speech;speech processing;acoustic model;chinese speech synthesis;communication;speech synthesis;speech analytics	NLP	-15.660481723922025	-83.87201086754844	169321
a1cf7261133f0565eba811cb00abb378b5a78e51	english and mandarin speakers may think about time differently, but for a different reason		Do English and Mandarin speakers think about time differently? Boroditsky (2001) claimed they do, but the claim did not stand in three failed replications (Chen, 2007; January & Kako, 2007; Tse & Altarriba, 2008). Recently she and her colleagues reported data from a different task to support the claim (Boroditsky, Fuhrman, & McCormick, 2010). We repeated their study with English speakers in US, Mandarin speakers in Taiwan and Mandarin speakers in China. The Mandarin speakers in Taiwan showed an opposite pattern of results than the English speakers, but the Mandarin speakers in China performed similarly to the English speakers. These results are interpreted as reflecting differences in orthographic directionality adopted in different linguistic communities, uniformly horizontal in US and China, but horizontal mixed with vertical in Taiwan. English and Mandarin speakers may think about time differently, but not because of the different metaphorical usages in the languages.	entity–relationship model;orthographic projection;super robot monkey team hyperforce go!	Jenn-Yeu Chen;Bishan Liang;Padraig O'Seaghdha;Xishan Huang	2011			mandarin chinese;social psychology;china;psychology;linguistics	NLP	-10.682626141229994	-80.3126089378155	169421
513d3fe5c88ee442871ea95d2f2a444f85e7dad6	visual speech recognition using pca networks and lstms in a tandem gmm-hmm system		Automatic visual speech recognition is an interesting problem in pattern recognition especially when audio data is noisy or not readily available. It is also a very challenging task mainly because of the lower amount of information in the visual articulations compared to the audible utterance. In this work, principle component analysis is applied to the image patches — extracted from the video data — to learn the weights of a two-stage convolutional network. Block histograms are then extracted as the unsupervised learning features. These features are employed to learn a recurrent neural network with a set of long short-term memory cells to obtain spatiotemporal features. Finally, the obtained features are used in a tandem GMM-HMM system for speech recognition. Our results show that the proposed method has outperformed the baseline techniques applied to the OuluVS2 audiovisual database for phrase recognition with the frontal view cross-validation and testing sentence correctness reaching 79% and 73%, respectively, as compared to the baseline of 74% on cross-validation.	artificial neural network;baseline (configuration management);circa;correctness (computer science);cross-validation (statistics);google map maker;hidden markov model;long short-term memory;pattern recognition;principal component analysis;recurrent neural network;speech recognition;timit;teller assist unit;unsupervised learning;vocabulary	Marina Zimmermann;Mostafa Mehdipour-Ghazi;Hazim Kemal Ekenel;Jean-Philippe Thiran	2016		10.1007/978-3-319-54427-4_20	computer vision;speech recognition;feature;computer science;machine learning;pattern recognition	ML	-16.17210219895687	-88.84761920597187	169462
2041bc1aefea036b0866d85924cb00fdf1000416	experiments on speech tracking in audio documents using gaussian mixture modeling	audio signal processing;acoustic analysis;gaussian processes;speech processing;speech segmentation;acoustic signal processing;tracking speech processing gaussian processes audio signal processing signal representation cepstral analysis acoustic signal processing;gaussian mixture model;cepstral analysis;covariance matrices speech segments tracking cepstral based acoustic analysis audio documents gaussian mixture modeling training data representation audio document scoring frame level likelihood database television programs smoothed log likelihood ratio news reports advertisements equal error rate music noise segments;signal representation;equal error rate;covariance matrix speech enhancement cepstral analysis testing smoothing methods training data databases tv error analysis indexing;tracking	This paper deals with the tracking of speech segments in audio documents. We use a cepstral-based acoustic analysis and gaussian mixture models for the representation of the training data. Three ways of scoring an audio document based on a frame-level likelihood calculation are proposed and compared. Our experiments are done on a database composed of television programs including news reports, advertisements, and documentaries. The best equal error rate obtained is approximately 12%.	acoustic cryptanalysis;cepstrum;database;experiment;mixture model;report	Mouhamadou Seck;Ivan Magrin-Chagnolleau;Frédéric Bimbot	2001		10.1109/ICASSP.2001.940903	natural language processing;audio mining;speech recognition;audio signal processing;computer science;speech coding;pattern recognition;mixture model;gaussian process;speech processing;acoustic model;tracking;speech segmentation;statistics	NLP	-15.288346416723659	-92.99888125956603	169581
694b208cbe34296062847cd4863fcea13008033f	vibrotactile stimulation can affect auditory loudness: a pilot study	tactile audio interaction;crossmodal interaction;loudness perception	Very few cases have been reported where tactile stimulation affects auditory perception. In this pilot study, we asked volunteers to compare the loudness of combinations of vibrotactile and auditory stimuli. A 50-300 Hz band-limited pink noise signal was used as the stimulus in the two modalities, simultaneously heard through headphones and felt in the hands to be compared to when it was heard only. On average, the same auditory stimulus was judged to be about one dB louder when it was simultaneously heard and felt rather than when it was heard only. This condition could be interpreted as having enhanced the perception of loudness by a whole jnd.	bandlimiting;decibel;headphones;pink noise	Ryuta Okazaki;Hiroyuki Kajimoto;Vincent Hayward	2012		10.1007/978-3-642-31404-9_18	acoustics;communication;audiology	HCI	-5.677550574022876	-84.24148111689803	169684
0c0dde0ab5bf1ce86a73a7f546c21c84d94ea0f8	acoustic model transformations based on random projections	speech recognition acoustic signal processing matrix algebra random processes;model domain acoustic model transformation random projection random matrix;acoustics;speech;acoustic signal processing;random matrix;random matrices acoustic model transformations speech recognition dimensionality reduction transform matrix random projection based feature combination technique linear transformations;matrix algebra;computational modeling;hidden markov models;vectors;hidden markov models acoustics computational modeling speech recognition vectors covariance matrix speech;acoustic model transformation;random processes;speech recognition;model domain;random projection;covariance matrix	This paper proposes a novel acoustic model transformation method for speech recognition based on random projections. Random projections have been suggested as a means of dimensionality reduction, where the original data are projected onto a subspace using a random matrix. Moreover, as we are able to produce various random matrices, it may be possible to find a transform matrix that is superior to conventional transformation matrices among random matrices. In our previous work, a random-projection-based feature combination technique has been proposed but had a high computational cost. In order to deal with this cost, in this paper, we introduce random projections on the acoustic model domain, where linear transformations are applied to an acoustic model using random matrices. Its effectiveness is confirmed by word recognition experiments on noisy speech.	acoustic cryptanalysis;acoustic model;algorithmic efficiency;computation;dimensionality reduction;experiment;model transformation;random projection;speech recognition;transformation matrix	Tetsuya Takiguchi;Mariko Yoshii;Yasuo Ariki;Jeff A. Bilmes	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288283	stochastic process;covariance matrix;random field;speech recognition;random element;computer science;speech;random matrix;machine learning;random function;pattern recognition;mathematics;computational model;hidden markov model;statistics	Robotics	-16.532654783393454	-92.82245149776003	169866
f6951608b20d81792aa98b7c23263b81a962d23d	estimation of children's physical characteristics from their voices		To date, multiple strategies have been proposed for the estimation of speakers’ physical parameters such as height, weight, age, gender etc. from their voices. These employ various types of feature measurements in conjunction with different regression and classification mechanisms. While some are quite effective for adults, they are not so for children’s voices. This is presumably because in children, the relationship between voice and physical parameters is relatively more complex. The vocal tracts of adults, and the processes that accompany speech production, are fully mature and do not undergo changes within small age differentials. In children, however, these factors change continuously with age, causing variations in style, content, enunciation, rate and quality of their speech. Strategies for the estimation of children’s physical parameters from their voice must take this variability into account. In this paper, using different formant-related measurements as exemplary analysis features generated within articulatory-phonetic guidelines, we demonstrate the nonlinear relationships of children’s physical parameters to their voice. We also show how such analysis can help us focus on the specific sounds that relate well to each parameter, which can be useful in obtaining more accurate estimates of the physical parameters.	nonlinear system;spatial variability;the diamond age	Jill Fain Lehman;Rita Singh	2016		10.21437/Interspeech.2016-146	speech recognition;computer science	HCI	-9.182645443215632	-84.93228859146099	169885
83dcee4210bcf69bbb8f2a80fd1ac889b77b3fe8	investigation on unknown word processing and strategies for spontaneous speech understanding.	spontaneous speech;word processing			Atsuhiko Kai;Seiichi Nakagawa	1995			natural language processing;speech recognition;linguistics	NLP	-15.69202158955081	-84.80258533058444	169891
fe6e764f8256fa33629d27acbcade85c9401653a	the relationship between the perception and production of english nasal codas by brazilian learners of english.	indexing terms;native speaker;american english	This study aims at investigating the perception and production of the English nasals /m/ and /n/ in syllable-final position by 20 Brazilian EFL learners and 3 native speakers of American English. Perception was assessed by means of both a discrimination and an identification test. Production data was collected by means of a Sentence Reading Test. The results from the perception tests revealed that the Brazilian learners and the native speakers seemed to have difficulties in distinguishing between the coda nasals, although to different degrees. Interestingly, the context of the preceding vowel influenced the perception of both natives and non-natives. The production results show that the participants had difficulty to produce the coda nasals. Concerning the relationship between perception and production, a positive correlation was found between the results of the Brazilian learners in the two perception tests and in the production test.	coda (file system);enlightenment foundation libraries;syllable	Denise Cristina Kluge;Andréia S. Rauber;Mara Silvia Reis;Ricardo Augusto Hoffmann Bion	2007			natural language processing;speech recognition;index term;computer science;first language;linguistics	HCI	-11.605477834583736	-82.06612397576437	169939
354ac3ca9a404150db56f927d5622f4cd07ccb42	phoneme recognition in timit with blstm-ctc	computer sciences;error rate;recurrent neural network;neural network;evolutionary computing	We compare the performance of a recurrent neural network with the best results published so far on phoneme recognition in the TIMIT database. These published results have been obtained with a combination of classifiers. However, in this paper we apply a single recurrent neural network to the same task. Our recurrent neural network attains an error rate of 24.6%. This result is not significantly different from that obtained by the other best methods, but they rely on a combination of classifiers for achieving comparable performance.	artificial neural network;computer performance;experiment;recurrent neural network;timit	Santiago Fernández;Alex Graves;Jürgen Schmidhuber	2008	CoRR		probabilistic neural network;speech recognition;word error rate;computer science;recurrent neural network;machine learning;pattern recognition;time delay neural network;artificial neural network;evolutionary computation	ML	-16.7511273875815	-88.21003270200762	170458
f3bc1282aa568be61901b0fb4ef6193df13f410f	speech recognition based on feature extraction with variable rate frequency sampling	chevauchement;sampling rate;spectral function;frecuencia muestreo;reconocimiento palabra;variable frequency;variable rate;transformacion fourier discreta;extraction forme;discrete fourier transformation;sampling frequency;spectrum;overlap;funcion espectral;imbricacion;razon muestreo;fonction caracteristique;fonction spectrale;transformation fourier discrete;automatic speech recognition;automatic recognition;frequence echantillonnage;frecuencia variable;extraccion forma;feature extraction;discrete fourier transform;taux echantillonnage;characteristic function;pattern recognition;speech recognition;audition;audicion;reconnaissance forme;reconnaissance parole;spectral sensitivity;reconocimiento patron;frequence variable;pattern extraction;funcion caracteristica;hearing;reconocimiento automatico;reconnaissance automatique;spectral resolution	Most feature extraction techniques involve in their primary stage a Discrete Fourier Transform (DFT) of consecutive, short, overlapping windows. The spectral resolution of the DFT representation is uniform and is given by Δf=2π/Ν where N is the length of the window The present paper investigates the use of non-uniform rate frequency sampling, varying as a function of the spectral characteristics of each frame, in the context of Automatic Speech Recognition. We are motivated by the non-uniform spectral sensitivity of human hearing and the necessity for a feature extraction technique that autofocuses on most reliable parts of the spectrum in noisy cases.	discrete fourier transform;feature extraction;microsoft windows;sampling (signal processing);speech recognition	Ilyas Potamitis;Nikos Fakotakis;George K. Kokkinakis	2001		10.1007/3-540-44805-5_44	speech recognition;telecommunications;computer science;mathematics;sampling	AI	-4.731532862175194	-92.95774574167415	170570
4fb458724393fcc360d37c40066d8523c0dbfae1	an evaluation of unsupervised acoustic model training for a dysarthric speech interface	psi_speech	In this paper, we investigate unsupervised acoustic model training approaches for dysarthric-speech recognition. These models are first, frame-based Gaussian posteriorgrams, obtained from Vector Quantization (VQ), second, so-called Acoustic Unit Descriptors (AUDs), which are hidden Markov models of phone-like units, that are trained in an unsupervised fashion, and, third, posteriorgrams computed on the AUDs. Experiments were carried out on a database collected from a home automation task and containing nine speakers, of which seven are considered to utter dysarthric speech. All unsupervised modeling approaches delivered significantly better recognition rates than a speaker-independent phoneme recognition baseline, showing the suitability of unsupervised acoustic model training for dysarthric speech. While the AUD models led to the most compact representation of an utterance for the subsequent semantic inference stage, posteriorgram-based representations resulted in higher recognition rates, with the Gaussian posteriorgram achieving the highest slot filling F-score of 97.02%.	acoustic cryptanalysis;acoustic model;baseline (configuration management);hidden markov model;home automation;markov chain;speech recognition;unsupervised learning;vector quantization	Oliver Walter;Vladimir Despotovic;Reinhold Häb-Umbach;Jort F. Gemmeke;Bart Ons;Hugo Van hamme	2014			natural language processing;speech recognition;computer science	NLP	-16.900290534513	-88.58777619266911	170673
75ea34ea7fab25dc65f63b73aae81047f1cdf687	a method for analyzing the coarticulated cv and vc components of vowel-formant trajectories in cvc syllables	consonant locus;vowel target;coarticulation;context;linear scaling;formant dynamics	To better understand the dynamic structure of vowels in CVC′ contexts one must account for the temporally-overlapping effects of the initial and final consonants C and C′. Here we present a linear-decomposition (LD) method for analyzing these effects as perturbations of the vowel-formant trajectories from their targets. The perturbations are modeled by the superposition of their CV and VC′ components, which are scaled by the differences between the vowel targets and their respective consonant loci. We use a dataset of second-formant frequencies (F2) from bVd, dVd, and gVd syllables containing seven vowels to illustrate how to estimate each element of the model by taking advantage of its additive structure and scaling properties. The model represents a family of formant trajectories unified by its scaling relationships, and the LD method that follows from it reveals how contextual effects combine and change over the duration of a vowel. Formulas for implementing the method are presented in appendices along with the two speakers' F2 datasets employed in this study.		David J. Broad;Frantz Clermont	2014	J. Phonetics	10.1016/j.wocn.2014.09.003	speech recognition;coarticulation;linear scale;acoustics;philosophy;computer science;linguistics;sociology;communication	HCI	-9.704295004435812	-82.80442539615714	170717
4d10bc753510070292fe7c0bb49c0d926c245fc2	signals, cochlear mechanics and pragmatism: a new vista on human hearing?	signals;helmholtz;corti;pragmatism;fourier analysis;hearing	The sense of hearing in humans became a research field in its own right around the middle of the nineteenth century. The foundations of the field were laid by physicists such as Georg Ohm and Hermann von Helmholtz and by the physiologist Alfonso Corti. The concepts of pure tone and Fourier analysis have informed hearing research ever since. The miracle of our exquisite capacity for perceiving salient qualities of music and speech such as pitch and prosody has often been explained in terms of Fourier analysis of acoustic stimuli into pure tones. In this article, I combine a historical–philosophical approach with acoustic signal modelling, cochlear mechanics and qualitative experiment. I discuss auditory phenomena irreducible to Fourier analysis of acoustic stimuli and reappraise the conceptual foundations of the field by proposing a pragmatist framework for understanding the sense of hearing in humans.	acoustic cryptanalysis;angularjs;approximation;cochlear implant;control theory;dewey decimal classification;experiment;fourier analysis;humans;irreducibility;pitch (music);reversi;semantic prosody;virtual world;vocabulary	Paolo Palmieri	2012	J. Exp. Theor. Artif. Intell.	10.1080/0952813X.2012.693687	helmholtz free energy;pragmatism;artificial intelligence;fourier analysis	ML	-8.098619839907887	-84.85275389204928	170838
a6e6559d8e2af8b5938440b2a9ebb53f99f0cd21	age verification using a hybrid speech processing approach	speech processing	The human speech production system is a multi-level system. On the upper level, it starts with information that one wants to transmit. It ends on the lower level with the materialization of the information into a speech signal. Most of the recent work conducted in age estimation is focused on the lower-acoustic level. In this research the upper lexical level information is utilized for age-group verification and it is shown that one's vocabulary reflects one's age. Several age-group verification systems that are based on automatic transcripts are proposed. In addition, a hybrid approach is introduced, an approach that combines the word-based system and an acoustic-based system. Experiments were conducted on a four age-groups verification task using the Fisher corpora, where an average equal error rate (EER) of 28.7% was achieved using the lexical-based approach and 28.0% using an acoustic approach. By merging these two approaches the verification error was reduced to 24.1%.	acoustic cryptanalysis;asch conformity experiments;enhanced entity–relationship model;fisher information;production system (computer science);speech processing;text corpus;vocabulary	Ron M. Hecht;Omer Hezroni;Amit Manna;Ruth Aloni-Lavi;Gil Dobry;Amir Alfandary;Yaniv Zigel	2009			voice activity detection;speech recognition;speech technology;natural language processing;speech processing;artificial intelligence;computer science;speech coding	NLP	-15.005747176055692	-85.79314264467382	170898
855316cbdb7da9bb9eea1254095031ea74a209bf	a la recherche des temps perdus : variations sur le rythme en français (regional variations of speech rhythm in french: in search of lost times) [in french]		______________________________________________________ Regional Variations of Speech Rhythm in French: In Search of Lost Times This paper addresses the relevance of speech rhythm acoustic measures for the description of some standard, regional and contact varieties of French. First, the limitation of conventional speech rhythm measures (e.g. %V, ∆C or PVI) for the description of French regional variations is pointed out. Then, alternative acoustic measures of speech rhythm, based on supra-segmental characteristics associated with timing (regularity of accentual phrases) and tempo (articulation rate, speech rate) are introduced and discussed. A comparison with the conventional measures indicates that long-term measures lead to a classification which is more consistent with the expected classification, either for the description of continuous similarities or categorical grouping. MOTS-CLES : rythme, métrique, français régional, français en contact.	acoustic cryptanalysis;biconnected component;linear algebra;relevance;speech synthesis;supra, inc.	Nicolas Obin;Mathieu Avanzi;Guri Bordal;Alice Bardiaux	2012				NLP	-11.567656438223139	-81.6350158896347	170918
1ea1e39720a30000ecb268d03b47d12ab4eec5ea	preprocessing and neural classification of english stop consonants [b, d, g, p, t, k]	time delay neural network;neural networks;neural network;artificial neural network;feature extraction;testing;computer vision;speech;pattern recognition;speech recognition;artificial neural networks	Neural networks are accepted as powerful learning tools in pattern recognition in which they proved their performance. Nevertheless, many problems like phoneme classification with a multi-speaker continuous speech database are hard even for neural networks. The authorsu0027 aim is to propose an artificial neural network architecture that detects acoustic features in speech signals and classifies them correctly. They reached this goal with English stop consonants [b, d, g, p, t, k] extracted from the general multi-speaker database (TIMlT) by modifying some parameter values in the preprocessing algorithm and by using a modified TDNN (time delay neural network) architecture. The net performed a good classification giving as testing recognition percentage the following results: 92.9 for [b], 91.8 for [d], 92.4 for [g], 80.3 for [p], 90.2 for [t], 91.2 for [k].	preprocessor	Anna Esposito;Eugène C. Ezin;M. Ceccarelli	1996			neural gas;cellular neural network;probabilistic neural network;speech recognition;types of artificial neural networks;feature;computer science;recurrent neural network;machine learning;pattern recognition;time delay neural network;deep learning;neocognitron	NLP	-17.710859285536426	-86.70737049322865	170937
dcaa8f0bd9bf93d7c1fc470acc723f7e3d5002e8	a nucleus-based timing model applied to multi-dialect speech synthesis by rule	speech synthesis		speech synthesis	Susan R. Hertz;Marie K. Huffman	1992			speech recognition;nucleus;artificial intelligence;pattern recognition;computer science;speech synthesis	Logic	-14.282785670338145	-86.62961128068868	171146
aaf874d5f734bc6c8489a4dcd2cd1bf25037a561	multi-timescale feature-extraction architecture of deep neural networks for acoustic model training from raw speech signal		This paper describes a new architecture of deep neural networks (DNNs) for acoustic models. Training DNNs from raw speech signals will provide 1) novel features of signals, 2) normalization-free processing such as utterance-wise mean subtraction, and 3) low-latency speech recognition for robot audition. Exploiting the longer context of raw speech signals seems useful in improving recognition accuracy. However, naive use of longer contexts results in the loss of short-term patterns; thus, recognition accuracy degrades. We propose a multi-timescale feature-extraction architecture of DNNs with blocks of different time scales, which enable capturing long- and short-term patterns of speech signals. Each block consists of complex-valued networks that correspond to Fourier and filterbank transformations for analysis. Experiments showed that the proposed multi-timescale architecture reduced the word error rate by about 3% compared with those only with the longterm context. Analysis of the extracted features revealed that our architecture efficiently captured the slow and fast changes of speech features.		Ryu Takeda;Kazuhiro Nakadai;Kazunori Komatani	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8593925	computer vision;artificial intelligence;fourier transform;subtraction;computer science;artificial neural network;robot;acoustic model;filter bank;word error rate;feature extraction;speech recognition	Robotics	-11.115516063023431	-90.47846039921119	171265
aef992c51f8b2f81ed925b169b6f33c8f139746f	investigations on linear transformations for speaker adaptation and normalization		This thesis deals with linear transformations at various stages of the automatic speech recognition process. In current state-of-the-art speech recognition systems linear transformations are widely used to care for a potential mismatch of the training and testing data and thus enhance the recognition performance. A large number of approaches has been proposed in literature, though the connections between them have been disregarded so far. By developing a unified mathematical framework, close relationships between the particular approaches are identified and analyzed in detail. Mel frequency Cepstral coefficients (MFCC) are commonly used features for automatic speech recognition systems. The traditional way of computing MFCCs suffers from a twofold smoothing, which complicates both the MFCC computation and the system optimization. An improved approach is developed that does not use any filter bank and thus avoids the twofold smoothing. This integrated approach allows a very compact implementation and needs less parameters to be optimized. Starting from this new computation scheme for MFCCs, it is proven analytically that vocal tract normalization (VTN) equals a linear transformation in the Cepstral space for arbitrary invertible warping functions. The transformation matrix for VTN is explicitly calculated exemplary for three commonly used warping functions. Based on some general characteristics of typical VTN warping functions, a common structure of the transformation matrix is derived that is almost independent of the specific functional form of the warping function. By expressing VTN as a linear transformation it is possible, for the first time, to take the Jacobian determinant of the transformation into account for any warping function. The effect of considering the Jacobian determinant on the warping factor estimation is studied in detail. The second part of this thesis deals with a special linear transformation for speaker adaptation, the Maximum Likelihood Linear Regression (MLLR) approach. Based on the close interrelationship between MLLR and VTN proven in the first part, the general structure of the VTN matrix is adopted to restrict the MLLR matrix to a band structure, which significantly improves the MLLR adaptation for the case of limited available adaptation data. Finally, several enhancements to MLLR speaker adaptation are discussed. One deals with refined definitions of regression classes, which is of special importance for fast adaptation when only limited adaptation data are available. Another enhancement makes use of confidence measures to care for recognition errors that decrease the adaptation performance in the first pass of a two-pass adaptation process.	acoustic cryptanalysis;acoustic model;coefficient;computation;database normalization;electronic band structure;filter bank;geo warping;hidden markov model;higher-order function;image warping;interconnection;jacobian matrix and determinant;markov chain;mathematical optimization;mel-frequency cepstrum;microsoft outlook for mac;program optimization;signal processing;smoothing;spatial variability;speech recognition;text corpus;tract (literature);transformation matrix	Michael Pitz	2005			normalization (statistics);natural language processing;artificial intelligence;linear map;computer science	ML	-17.090091406960756	-92.531160628479	171276
ce1e8d04fdff25e91399e2fbb8bc8159dd1ea58a	low-dimensional representation of spectral envelope without deterioration for full-band speech analysis/synthesis system		A speech coding for a full-band speech analysis/synthesis system is described. In this work, full-band speech is defined as speech with a sampling frequency above 40 kHz, whose Nyquist frequency covers the audible frequency range. In prior works, speech coding has generally focused on the narrowband speech with a sampling frequency below 16 kHz. On the other hand, statistical parametric speech synthesis currently uses the full-band speech, and low-dimensional representation of speech parameters is being used. The purpose of this study is to achieve speech coding without deterioration for full-band speech. We focus on a high-quality speech analysis/synthesis system and mel-cepstral analysis using frequency warping. In the frequency warping function, we directly use three auditory scales. We carried out a subjective evaluation using the WORLD vocoder and found that the optimum number of dimensions was around 50. The kind of frequency warping did not significantly affect the sound quality in the dimensions.	bilinear transform;cepstrum;frequency band;nyquist frequency;sampling (signal processing);sound quality;speech coding;speech synthesis;vocoder;voice analysis	Masanori Morise;Genta Miyashita;Kenji Ozawa	2017			speech recognition;artificial intelligence;pattern recognition;spectral envelope;computer science	NLP	-9.663803401763174	-87.740727120847	171299
8e98a5f680180b82b378ea57001c195ee3f7327a	recognition of voiced speech from the bispectrum	speech;speech recognition;speech processing;estimation;feature extraction;transfer functions;harmonic analysis	Recognition of voiced speech phonemes is addressed in this paper using features extracted from the bispectrum of the speech signal. Voiced speech is modeled as a superposition of coupled harmonics, located at frequencies that are multiples of the pitch and modulated by the vocal tract. For this type of signal, nonzero bispectral values are shown to be guaranteed by the estimation procedure employed. The vocal tract frequency response is reconstructed from the bispectrum on a set of frequency points that are multiples of the pitch. An AR model is next fitted on this transfer function. The AR coefficients are used as the feature vector for the subsequent classification step. Any finite dimension vector classifier can be employed at this point. Experiments using the LVQ neural classifier give satisfactory classification scores on real speech data, extracted from the DARPA/TIMIT speech corpus.	autoregressive model;bispectrum;coefficient;feature vector;frequency response;learning vector quantization;modulation;pitch (music);quantum superposition;speech corpus;timit;tract (literature);transfer function	Anastasios Delopoulos;Maria Rangoussi;Janne Andersen	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)		linear predictive coding;speech recognition;acoustics;computer science;speech coding;pattern recognition;speech processing;acoustic model	ML	-10.445414254615713	-90.12202925227922	171336
2ab9fd2be2bf82e0bbd558cc64c1c46728fc4f8a	model adaptation for automatic speech recognition based on multiple time scale evolution		The change in speech characteristics is originated from various factors, at various (temporal) rates in a real world conversation. These temporal changes have their own dynamics and therefore, we propose to extend the single (time-) incremental adaptations to a multiscale adaptation, which has the potential of greatly increasing the model’s robustness as it will include adaptation mechanism to approximate the nature of the characteristic change. The formulation of the incremental adaptation assumes a time evolution system of the model, where the posterior distributions, used in the decision process, are successively updated based on a macroscopic time scale in accordance with the Kalman filter theory. In this paper, we extend the original incremental adaptation scheme, based on a single time scale, to multiple time scales, and apply the method to the adaptation of both the acoustic model and the language model. We further investigate methods to integrate the multi-scale adaptation scheme to realize the robust speech recognition performance. Large vocabulary continuous speech recognition experiments for English and Japanese lectures revealed the importance of modeling multiscale properties in speech recognition.	acoustic cryptanalysis;acoustic model;approximation algorithm;content adaptation;experiment;filter design;kalman filter;language model;speech recognition;vocabulary	Shinji Watanabe;Atsushi Nakamura;Biing-Hwang Juang	2011			speech recognition;computer science;artificial intelligence;machine learning	Vision	-13.215842920862986	-93.96859951036008	171478
4edae1c443cd9bede2af016c23e13d6e664bfe7e	ensemble methods for spoken emotion recognition in call-centres	base donnee;ensemble method;emotional intelligence;centre appel;information transmission;speech processing;relacion hombre maquina;database;tratamiento palabra;traitement parole;call centre;base dato;man machine relation;emotion recognition;speech databases;natural interaction;ensemble methods;affect recognition;call centres;relation homme machine;transmision informacion;transmission information;reconnaissance emotion	Machine-based emotional intelligence is a requirement for more natural interaction between humans and computer interfaces and a basic level of accurate emotion perception is needed for computer systems to respond adequately to human emotion. Humans convey emotional information both intentionally and unintentionally via speech patterns. These vocal patterns are perceived and understood by listeners during conversation. This research aims to improve the automatic perception of vocal emotion in two ways. First, we compare two emotional speech data sources: natural, spontaneous emotional speech and acted or portrayed emotional speech. This comparison demonstrates the advantages and disadvantages of both acquisition methods and how these methods affect the end application of vocal emotion recognition. Second, we look at two classification methods which have not been applied in this field: stacked generalisation and unweighted vote. We show how these techniques can yield an improvement over traditional classification methods. 2006 Elsevier B.V. All rights reserved.	computer;emotion recognition;humans;spontaneous order	Donn Morrison;Ruili Wang;Liyanage C. De Silva	2007	Speech Communication	10.1016/j.specom.2006.11.004	speech recognition;emotional intelligence;computer science;emotional expression;artificial intelligence;machine learning;speech processing;linguistics	AI	-12.913750736676464	-85.02125447030942	171513
16598590a30502f2b2e585f5382693f7da94be31	on the applicability of speaker diarization to audio indexing of non-speech and mixed non-speech/speech video soundtracks	multimedia;information retrieval;speaker diarization;computer science;audio indexing	A video‘s soundtrack is usually highly correlated to its content. Hence, audio-based techniques have recently emerged as a means for video concept detection complementary to visual analysis. Most state-of-the-art approaches rely on manual definition of predefined sound concepts such as “engine sounds”, “outdoor/indoor sounds”. These approaches come with three major drawbacks: manual definitions do not scale as they are highly domain-dependent, manual definitions are highly subjective with respect to annotators and a large part of the audio content is omitted since the predefined concepts are usually found only in a fraction of the soundtrack. This paper explores how unsupervised audio segmentation systems like speaker diarization can be adapted to automatically identify low-level sound concepts similar to annotator defined concepts and how these concepts can be used for audio indexing. Speaker diarization systems are designed to answer the question “who spoke when?” by finding segments in an audio stream that exhibit similar properties in feature space, i.e. sound similar. Using a diarization system, all the content of an audio file is analyzed and similar sounds are clustered. This article provides an in-depth analysis on the statistic properties of similar acoustic segments identified by the diarization system in a predefined document set and the theoretical fitness of this approach to discern one document class from another. It also discusses how diarization can be tuned in order to better reflect the acoustic properties of general sounds as opposed to speech and introduces a proof-of-concept system for multimedia event classification working with diarization-based indexing.	acoustic cryptanalysis;feature vector;high- and low-level;speaker diarisation;streaming media	Robert Mertens;Po-Sen Huang;Luke R. Gottlieb;Gerald Friedland;Ajay Divakaran;Mark Hasegawa-Johnson	2012	IJMDEM	10.4018/jmdem.2012070101	speaker diarisation;speech recognition;computer science;multimedia	Web+IR	-7.256776052323891	-91.98838712295216	171578
311109741b0c847858c76b93c9289a6ce69484ab	a low bit rate speech coding method using a formant-articulatory parameter nomogram	speech coding	In this paper, we propose a new method for low bit rate speech coding using a nomogram that is a pair of codebooks representing the functional relationship between formant frequencies and articulatory parameters. Significant features of our approach are 1) using the codebooks derived theoretically from the computation using a stylized vocal tract model and 2) independent coding by separating frequency information from the amplitude in a speech segment. From these features, the method is also characterized by little dependency upon speech databases and/or languages in the acoustic domain, so that it has a potential to construct a more flexible rule-based speech synthesis system. We have conducted articulatory encode-decode experiments with the bit rate range from 3.2kbps to 1.6kbps using speech samples in ASJ and TIMIT speech databases and confirmed that good quality speech synthesis is achieved with improvements on the bit allocation scheme and a frame sampling method.	acoustic cryptanalysis;codebook;computation;database;encode;experiment;logic programming;nomogram;sampling (signal processing);speech coding;speech synthesis;timit;tract (literature)	Hiroshi Ohmura;Akira Sasou;Kazuyo Tanaka	2000			artificial intelligence;speech recognition;coding gain;speech coding;codec2;pattern recognition;computer science;vector sum excited linear prediction;harmonic vector excitation coding;coding tree unit;adaptive multi-rate audio codec;linear predictive coding	NLP	-10.830617946039554	-89.1669173080943	171677
5187e20aa33c9f6342f8847cc38bc0a786a5bb0c	multi-level adaptive networks in tandem and hybrid asr systems	word error rate;out of domain data;hidden layers;hmm state likelihoods;slavic language;multigenre broadcast data;feature extractors;neural networks;neural nets;logistic regression classifier;hidden markov model;tandem;acoustics;training;vocabulary;vocabulary error analysis speech recognition;ted;speech;hidden markov models speech neural networks speech recognition training acoustics adaptation models;ted lecture recordings;multilingual modeling;multilingual training;romance language;hybrid;error analysis;vocabulary speech recognition systems;bbc;hidden markov models;deep neural network;deep learning;deep neural networks;feature extraction;multilevel adaptive networks;pattern classification;speech recognition;word error rate multilevel adaptive networks hybrid asr systems out of domain data vocabulary speech recognition systems multigenre broadcast data ted lecture recordings;regression analysis;hybrid dnn hmm;germanic language;bbc deep neural networks tandem hybrid mlan ted;mlan;adaptation models;hybrid asr systems;natural language processing;globalphone corpus;cross lingual transfer;linguistics	In this paper we investigate the use of Multi-level adaptive networks (MLAN) to incorporate out-of-domain data when training large vocabulary speech recognition systems. In a set of experiments on multi-genre broadcast data and on TED lecture recordings we present results using of out-of-domain features in a hybrid DNN system and explore tandem systems using a variety of input acoustic features. Our experiments indicate using the MLAN approach in both hybrid and tandem systems results in consistent reductions in word error rate of 5-10% relative.	acoustic cryptanalysis;automated system recovery;experiment;speech recognition;vocabulary;word error rate	Peter Bell;Pawel Swietojanski;Steve Renals	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6639014	natural language processing;speech recognition;hybrid;feature extraction;word error rate;computer science;speech;machine learning;artificial neural network;hidden markov model;regression analysis	Robotics	-18.946927130624143	-88.41071208436371	171684
fa0ca73ec4ef2ffdc6c34d363438b5593885cce9	investigation of sub-band discriminative information between spoofed and genuine speech.		A speaker verification system should include effective precautions against malicious spoofing attacks, and although some initial countermeasures have been recently proposed, this remains a challenging research problem. This paper investigates discrimination between spoofed and genuine speech, as a function of frequency bands, across the speech bandwidth. Findings from our investigation inform some proposed filter bank design approaches for discrimination of spoofed speech. Experiments are conducted on the Spoofing and Anti-Spoofing (SAS) corpus using the proposed frequency-selective approach demonstrates an 11% relative improvement in terms of equal error rate compared with a conventional mel filter bank.	bandwidth (signal processing);filter bank;frequency band;malware;sas;speaker recognition;spoofing attack	Kaavya Sriskandaraja;Vidhyasaharan Sethu;Phu Ngoc Le;Eliathamby Ambikairajah	2016		10.21437/Interspeech.2016-844	discriminative model;speech recognition;pattern recognition;artificial intelligence;computer science;spoofing attack	HCI	-11.182075711891567	-91.90017072705248	171691
9dc7e26fc95073bd8edb7e54997b38400c555a55	efficient modeling of acoustic feedback path in hearing aids by voice activity detector-supervised multiple noise injections		Adaptive Feedback Cancellation (AFC) techniques are widely used to eliminate the undesired acoustic feedback effect arising in the Hearing Aid Devices (HADs) due to the coupling between the speaker and the microphone of the HAD. This paper proposes a method to eliminate the acoustic feedback effect in the HADs in presence of noisy environment. The method involves utilization of a computationally efficient Spectral Flux feature-based voice activity detector (VAD), which controls the process of Noise Injection in the proposed AFC algorithm (SFNIAFC). The proposed algorithm’s performance is objectively evaluated using Misalignment (MISA) and Perceptual Evaluation of Speech Quality (PESQ) criteria for realistic noisy conditions. The simulations performed for the proposed method shows faster convergence and reduction in the MISA values with high PESQ values in comparison to the earlier method. Subjective test results support the effectiveness and better performance of the proposed algorithm for the HAD applications over earlier method.	algorithm;algorithmic efficiency;assistive technology;audio feedback;automatic frequency control;convergence (action);detector device component;detectors;hearing aids;microphone device component;pesq;real-time clock;self-help devices;simulation;smartphone;spectral flux;voice activity detection	Parth Mishra;Serkan Tokgoz;Issa M. S. Panahi	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8513007	computer vision;noise measurement;adaptive feedback cancellation;hearing aid;pesq;microphone;white noise;detector;pattern recognition;signal-to-noise ratio;computer science;artificial intelligence	SE	-12.310606012942822	-94.01714757265309	171758
b1df495b8ace644799d9c5b05895ede6dba035e6	efficient pitch detection techniques for interactive music	autocorrelation function;interactive music;maximum likelihood;spectrum;pitch tracking;computer music;detection algorithm	Several pitch detection algorithms are examined for use in interactive computer-music performance. We define criteria necessary for successful pitch tracking in real-time and survey four tracking techniques: Harmonic Product Spectrum (HPS), Cepstrum-Biased HPS (CBHPS), Maximum Likelihood (ML), and the Weighted Autocorrelation Function (WACF).	autocorrelation;cepstrum;pitch detection algorithm;real-time clock	Patricio de la Cuadra;Aaron S. Master;Craig Stuart Sapp	2001			speech recognition;acoustics;pitch detection algorithm;communication	Vision	-13.44112127408647	-87.71816695160679	171838
76dd541990c1060443b1880d134bf2d30b5aeade	on identification of violin strokes in a real-time performance system			real-time transcription	Birute Sinkeviciùte;Saulius Sondeckis;Arturas Medonis	1986			violin;acoustics;computer science	Embedded	-7.350136516128012	-84.60135836752427	171869
7a3517a457da78e5b2aa3d182e3245db1d97d3cc	robust segmentation and annotation of folk song recordings	visual search	Even though folk songs have been passed down mainly by oral tradition, most musicologists study the relation between folk songs on the basis of score-based transcriptions. Due to the complexity of audio recordings, once having the transcriptions, the original recorded tunes are often no longer studied in the actual folk song research though they still may contain valuable information. In this paper, we introduce an automated approach for segmenting folk song recordings into its constituent stanzas, which can then be made accessible to folk song researchers by means of suitable visualization, searching, and navigation interfaces. Performed by elderly non-professional singers, the main challenge with the recordings is that most singers have serious problems with the intonation, fluctuating with their voices even over several semitones throughout a song. Using a combination of robust audio features along with various cleaning and audio matching strategies, our approach yields accurate segmentations even in the presence of strong deviations.	plasma cleaning	Meinard Müller;Peter Grosche;Frans Wiering	2009			speech recognition;art;acoustics;communication	HCI	-18.923116241234275	-83.09550680965755	171992
92c4cfba7c607cfbbc87c3103df4ccb24050b522	perception of stress and speaking style for selected elements of the susas database	susas;speech preception;repeated measures;speech perception;analysis of variance;speech recognition;stressed speech	The Speech Under Simulated and Actual Stress (SUSAS) database is a collection of utterances recorded under conditions of simulated or actual stress, the purpose of which is to allow researchers to study the effects of stress and speaking style on the speech waveform. The aim of the present investigation was to assess the perceptual validity of the simulated portion of the database by determining the extent to which listeners classify its utterances according to their assigned labels. Seven listeners performed an eight-alternative, forced-choice response, judging whether monosyllabic or disyllabic words spoken by talkers from three different regional accent classes (Boston, General American, New York) were best classified as, clear, fast, loud, neutral, question, slow, or soft. Mean percentages of ‘‘correct’’ judgments were analysed using a 3 (regional accent class) 2 (number of syllables) 8 (speaking style) repeated measures analysis of variance. Results indicate that, overall, listeners correctly classify the utterances only 58% of the time, and that the percentage of correct classifications varies as a function of all three independent variables. 2002 Elsevier Science B.V. All rights reserved.	waveform	Robert S. Bolia;Raymond E. Slyh	2003	Speech Communication	10.1016/S0167-6393(02)00129-2	natural language processing;repeated measures design;speech recognition;speech perception;analysis of variance;statistics	NLP	-11.910973600293424	-82.76099284004746	172147
1016b51733d2e6a094bba0600a09d6637753ad96	considerations in voice transformation with physiologic scaling principles	speech synthesis;voice transformation;speech simulation;voice quality;voice conversion;vowel quality;linear prediction coding;signal processing	Abstract   This study begins to explore the importance of the physiological domain in voice transformation. A general approach is outlined for transforming the voice quality of sentence-level speech while maintaining the same phonetic content. Transformations will eventually include gender, age, voice quality, emotional state, disordered state, dialect or impersonation. In this paper, only a specific voice quality, twang, is described as an example. The basic question is: relative to pure signal processing, can voices be transformed more effectively if biomechanical, acoustic and anatomical scaling principles are applied? At present, two approaches are contrasted, a Linear Predictive Coding approach and a biomechanical simulation approach.		Ingo Titze;Darrell Wong;Brad H. Story;Russell Long	1997	Speech Communication	10.1016/S0167-6393(97)00014-9	voice activity detection;speech recognition;phonation;computer science;signal processing;voice analysis;linguistics;speech synthesis	PL	-9.667000111666008	-83.8939123360966	172244
10d1e6070403bbebbbd17aa431dee2dffe9c29af	sub-band spectral variance feature for noise robust asr	databases;speech recognition feature extraction reverberation;speech processing;speech;noise measurement;mel frequency cepstral coefficient;speech recognition;reverberant acoustic conditions noise robust asr automatic speech recognition performance reverberant environments noisy environments sub band spectral variance normalization based feature extraction advanced etsi 2 frontend rasta plp mfcc spectral subtraction meeting recorder digit subset mrd subset noisy standard aurora 2 databases aurora 5 databases additive noise;speech noise speech recognition speech processing noise measurement mel frequency cepstral coefficient databases;noise	The goal of this work is to improve automatic speech recognition (ASR) performance in very noisy and reverberant environments. The solution is based on extracting sub-band spectral variance normalization based features, which are capable of estimating the relative strengths of speech and noise components both in presence and absence of speech. The advanced ETSI-2 frontend, RASTA-PLP, MFCC alone and in combination with spectral subtraction are tested for comparison purposes. Speech recognition evaluations are performed on the noisy standard AURORA-2 and meeting recorder digit (MRD) subset of AURORA-5 databases, which represent additive noise and reverberant acoustic conditions. The results reveal that the proposed method is robust and reliable for both low SNR and reverberant scenarios, and provide considerable improvements with respect to the traditional feature extraction techniques.	acoustic cryptanalysis;additive white gaussian noise;automated system recovery;database normalization;dynamic range;feature extraction;machine-readable dictionary;online and offline;pl/p;robustness (computer science);signal-to-noise ratio;speech recognition;utility functions on indivisible goods;vocabulary	Hari Krishna Maganti;Silvia Zanon;Marco Matassoni;Alessio Brutti	2011	2011 19th European Signal Processing Conference		speech recognition;acoustics;computer science;pattern recognition;speech processing	Vision	-13.4550755839183	-91.1468349460041	172392
85d553191724b9d21a0ed582adbf8a04846df312	speaker verification with combined threshold, identification front-end, and ubm	gaussian processes speaker recognition;front end;false reject rate;gaussian processes;speaker verification;speaker recognition;standalone system speaker verification system threshold combining identification front end universal background model ubm false rejection rate false acceptance rate standard benchmark speech corpus;universal background model;robustness support vector machines kernel loudspeakers performance analysis benchmark testing speech system testing boosting neural networks;false accept rate;uniform distribution	"""This paper presents a novel approach to improve accuracy performance of a speaker verification system through combination or cascading three different verification methods using an identification """"front-end"""", a universal background model, and an individual matching score threshold. The performance of a speaker verification system can be determined in terms of false rejection rate and false acceptance rate using a standard benchmark speech corpus, which represents fixed common populations in testing voice and claimed identities. By further assuming uniform distributions, it can show analytically that the false acceptance rate of a standalone system either using the threshold or the universal background model can be significantly reduced when combined with the identification ''front-end''. Experiments have provided clear evidence, and even more gains to combine all three methods together. The results show 60% reduction in the false acceptance rate for combining with the identification """"front-end"""" alone, and 80% reduction for combining all three methods without adding penalty in the false rejection rate."""	benchmark (computing);experiment;nyquist rate;population;rejection sampling;speaker recognition;speech corpus	Ningping Fan;Justinian P. Rosca;Radu V. Balan	2005	Fourth IEEE Workshop on Automatic Identification Advanced Technologies (AutoID'05)	10.1109/AUTOID.2005.45	speaker recognition;speech recognition;computer science;machine learning;pattern recognition	NLP	-14.934677383132499	-92.43370508637472	172639
5be2953ed687cc7bf99f3d2f3ab55daa96ae95e3	jacobian approach to fast acoustic model adaptation	speech recognition jacobian approac fast acoustic model adaptation acoustic models noisy environments noise assumption jacobian matrices noise cepstra mathematical formulation algorithm experiments training data noise models speech models computational cost reduction matrix arithmetic real time environmental noise adaptation spectrum subtraction;jacobian matrices adaptation model acoustic noise working environment noise loudspeakers speech enhancement computational modeling noise level taylor series gold;real time;acoustic modeling;speech processing;acoustic signal processing;spectrum;acoustic noise;noise pollution;speech recognition;noise pollution speech recognition acoustic signal processing speech processing jacobian matrices acoustic noise;jacobian matrices;model composition	This paper describes a Jacobian approach to fast adaptation of acoustic models to noisy environments. Acoustic models under a noise assumption are compensated by Jacobian matrices with the di erence between assumed and observed noise cepstra. Detailed mathematical formulation and algorithm derivation are presented. Experiments showed that when a small amount of training data is given, this approach outperforms the existing approaches (such as PMC and NOVO) for composing a model from speech and noise models. It drastically reduces computational cost by replacing the complicated computation of model composition by simple matrix arithmetic and enables real-time environmental noise adaptation. Combination with spectrum subtraction is also discussed.	acoustic cryptanalysis;acoustic model;algorithm;computation;computational complexity theory;experiment;jacobian matrix and determinant;real-time clock;signal-to-noise ratio;speech recognition	Shigeki Sagayama;Yoshikazu Yamaguchi;Satoshi Takahashi;Jun-ichi Takahashi	1997		10.1109/ICASSP.1997.596063	gradient noise;spectrum;speech recognition;noise pollution;value noise;computer science;noise measurement;machine learning;noise;noise;speech processing	ML	-16.644772630461564	-92.97344926454396	172657
07b21f1da76d943085af521915c9fd766ed2c888	strategies for improving audible quality and speech recognition accuracy of reverberant speech	microphones;correlation shaping;reverberation;automatic speech recognition accuracy;speech intelligibility;asr accuracy;convolution;speech recognition automatic speech recognition finite impulse response filter reverberation microphones speech enhancement interference convolution blind equalizers additive noise;speech processing;correlation shaping audible quality automatic speech recognition accuracy reverberant speech signal to reverberation ratio asr accuracy speech dereverberation speaker to receiver impulse response linear prediction residual additive noise convolutional interference speech enhancement algorithm;finite impulse response filter;additive noise;convolutional interference;blind equalizers;speech enhancement;interference;speech dereverberation;linear predictive;signal to reverberation ratio;acoustic correlation;speaker to receiver impulse response;automatic speech recognition;transient response;acoustic noise;reverberant speech;speech recognition;impulse response;speech enhancement algorithm;quality measures;linear prediction residual;acoustic correlation speech recognition speech intelligibility speech processing transient response reverberation acoustic noise speech enhancement;audible quality	__________________________________________ * Currently affiliated with Microsoft Corporation. ABSTRACT We showed in [1] that penalizing long-term reverberation energy is more effective than maximizing the signal-to-reverberation ratio (SRR) for improving audible quality and automatic speech recognition (ASR) accuracy. Using this knowledge we propose a blind approach to speech dereverberation that reduces the length of the equalized speaker-to-receiver impulse response. The approach reduces the long-term correlation in the linear prediction (LP) residual of reverberant speech. We show that this approach improves both the audible quality (measured with subjective listening tests) and ASR accuracy (measured with two commercial ASR systems) of reverberant speech.	automated system recovery;automatic system recovery;simpl;speech recognition	Bradford W. Gillespie;Les E. Atlas	2003		10.1109/ICASSP.2003.1198871	voice activity detection;speech recognition;impulse response;reverberation;computer science;finite impulse response;noise;speech processing;mathematics;interference;convolution;transient response;intelligibility	NLP	-13.582962198379441	-91.49127258465943	172713
0ec17a9cda73b810c259402b2556b0878e2f37a0	robust speaker turn role labeling of tv broadcast news shows	broadcast news;time 6 5 hr robust speaker turn role labeling tv broadcast news shows speaker clustering automatic speech recognition;speaker role;automatic segmentation;speech processing;training;speech;television broadcasting;speaker recognition;broadcast news and conversation speaker distillation;automatic speech recognition;accuracy;feature extraction;speech recognition;classification accuracy;television broadcasting speaker recognition speech recognition;spoken language understanding speaker role broadcast news and conversation speaker distillation;labeling;speech accuracy labeling speech recognition feature extraction training speech processing;spoken language understanding	Speaker role recognition in TV Broadcast News shows is addressed in this paper with a particular focus on speaker turn role labeling. A mixed approach combining speaker clustering and analysis of Automatic Speech Recognition output is proposed for assigning speaker turns a role among: anchor, reporter and other. 86% classification accuracy is obtained for automatically segmented speaker turns on a 6.5 hours test corpus of 14 TVBN shows mixing news and conversational speech.	acoustic cryptanalysis;cluster analysis;sensor;speech recognition;transcription (software)	Géraldine Damnati;Delphine Charlet	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947650	natural language processing;speaker recognition;speaker diarisation;labeling theory;speech recognition;feature extraction;computer science;speech;speech processing;accuracy and precision	Vision	-17.513505558606234	-83.65872884257507	172716
67d167f22759881cacf81860032fd7c3e3140f33	dynamic selection of feature spaces for robust speech recognition	feature space	Selection of acoustic features for robust speech recognition has been the subject of research for several years. In the past, algorithms that use feature vectors from multiple frequency bands [9], or employ techniques to switch between multiple feature streams [10] have been reported in the literature to handle robustness under di erent acoustic conditions. Acoustic models built out of di erent feature sets produce di erent kinds of recognition errors. In this paper, we propose a likelihood-based scheme to combine the acoustic feature vectors from multiple signal processing schemes within the decoding framework, in order to extract maximum bene t from these di erent acoustic feature vectors and models. The proposed technique is general enough to be applied to other pattern recognition elds, such as, OCR, handwriting recognition, etc. The fundamental idea behind this approach is to pick the set of features that classi es a frame of speech accurately with no apriori information about the phonetic class or acoustic channel that this speech comes from. Two methods of merging any set of acoustic features, such as, formant-based features, cepstral feature vectors, PLP features, LDA features etc., are presented here: Use of a weighted set of likelihoods obtained from these several alternative feature sets and Selection of the feature space that ranks the best when used in a rank-based recognizer These merging algorithms provide an impressive reduction in error rate between 8% to 15% relative across a wide variety of wide-band, clean and noisy large vocabulary continuous speech recognition tasks. Much of this gain is from the reduced insertion and substitution errors. Using the approach presented in this paper, we have achieved better improved acoustic modeling without increasing the number of parameters, i.e. two 40K Gaussian systems, when merged perform better than a 180K Gaussian system trained on the better of the two feature spaces.	acoustic cryptanalysis;acoustic model;algorithm;cepstrum;feature vector;finite-state machine;frequency band;handwriting recognition;local-density approximation;optical character recognition;pl/p;pattern recognition;signal processing;speech recognition;vocabulary	Bhuvana Ramabhadran;Yuqing Gao;Michael Picheny	2000			artificial intelligence;feature (computer vision);speech recognition;pattern recognition;speaker recognition;feature (machine learning);computer science;feature vector	ML	-14.192450585761303	-90.95600047282555	172780
9437717bd2402e9c4d660afdfb62984283742870	speech emotion detection based on neural networks	human computer interaction;t technology;neural nets;emotion recognition;neural network classifier;speech based user interfaces emotion recognition feature extraction human computer interaction neural nets signal classification speech recognition;speech based user interfaces;linguistic analysis;feature vector;feature extraction;signal processing;berlin database of emotional speech speech emotion detection spoken dialogues neural network classifiers spoken language man machine interfaces psychological analysis linguistic analysis speech fundamental frequency formants energy rate voicing rate feature extraction features vector language independent emotion recognition tool;signal classification;speech recognition;speech neural networks psychology signal analysis spatial databases natural languages user interfaces signal processing frequency feature extraction;man machine interface;fundamental frequency;neural network	Emotion detection in spoken dialogues is an area that has traditionally been studied in psychology and linguistics but in recent years the engineering community has become increasingly active in this area, due largely to its importance in spoken language man-machine interfaces. Besides techniques in signal processing and analysis it also requires psychological and linguistic analysis. This paper reports an experimental study on six emotions, happiness, sadness, anger, fear, neutral and boredom. It uses speech fundamental frequency, formants, energy and voicing rate as extracted features. Features are selected manually for different experiments in order to get the best results. The selected features are included into a features vector with different sizes as input for different neural network classifiers. To carry out this experimental study a specific tool for language-independent emotion recognition tool has been designed and used. The database which is used for this experiment is the Berlin Database of Emotional Speech.	artificial neural network;emotion recognition;experiment;language-independent specification;sadness;signal processing	Kamran Soltani;Raja Noor Ainon	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555476	human–machine interface;natural language processing;speech recognition;feature vector;feature extraction;computer science;machine learning;fundamental frequency;artificial neural network	NLP	-12.540968206309614	-88.02381256388077	172808
053998913f78c6a2b929ab8cb32e03a053cef29d	connected digit recognition using statistical template matching	error rate;linear transformation;signal processing;template matching	In this paper we describe the optimization of 'conven-tional' template matching techniques for connected digit recognition (TI/NIST connected digit corpus). In particular we carried out a series of experiments in which we studied various aspects of signal processing, acoustic modeling, mixture densities and linear transforms of the acoustic vector. After all optimization steps, our best string error rate on the TI/NIST connected digit corpus was 1.71% for single densities and 0.74% for mixture densities.	acoustic cryptanalysis;acoustic model;discriminant;discriminative model;experiment;mathematical optimization;signal processing;speech recognition;template matching;whitening transformation	Lutz Welling;Hermann Ney;Andreas Eiden;C. Forbrig	1995			artificial intelligence;word error rate;speech recognition;signal processing;pattern recognition;template matching;computer science;numerical digit;linear map	ML	-16.630122738709865	-92.29351579328925	172894
3de7c41af7950f030d1f1ee1147f1a4d0a88f4a8	enhanced perceptual model for non-intrusive speech quality assessment	databases;degradation;auditory nonintrusive quality estimation plus;filter bank;auditory system;human auditory system;testing;speech enhancement;anique;distortion;quality assessment;statistical learning;statistical analysis;mean opinion score;mean opinion score databases;anique enhanced perceptual model nonintrusive speech quality assessment auditory nonintrusive quality estimation plus human auditory system statistical learning methods mean opinion score databases;speech enhancement quality assessment distortion humans auditory system testing filter bank statistical learning databases degradation;humans;experimental evaluation;learning artificial intelligence;statistical analysis learning artificial intelligence speech enhancement;statistical learning methods;nonintrusive speech quality assessment;enhanced perceptual model;model simulation	In this paper, we propose a novel model for estimating the quality of speech without the reference speech information. The proposed auditory non-intrusive quality estimation plus (ANIQUE+) model is a perceptual model simulating the functional role of human auditory system, and employs improved modeling of quality estimation by statistical learning methods. Experimental evaluation demonstrated that the performance of the ANIQUE+ model is significantly superior to that of the current ITU-T standard recommendation P.563 on 34 different subjective mean opinion score (MOS) databases - the averaged correlation between subjective and objective quality scores is about 0.97 for ANIQUE+, whereas P.563 shows 0.87 averaged correlation	database;machine learning;simulation	Doh-Suk Kim;Ahmed Tarraf	2004	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660149	mean opinion score;natural language processing;computer vision;speech recognition;degradation;distortion;telecommunications;computer science;filter bank;software testing	Robotics	-9.423210271780809	-88.56966675728792	173416
36afd72815aa15c8731ff063d7f806ddc13f7d06	automatic evaluation of karaoke singing based on pitch, volume, and rhythm features	karaoke;music audio signal processing;singing evaluation;rhythm;audio signal processing;score function;timbre;automatic evaluation;rhythm humans accuracy timbre lead;accuracy;lead;evaluation criteria;pearson product moment correlation coefficient karaoke singing pitch features volume features rhythm features automatic singing evaluation system karaoke scoring mechanism karaoke video compact disk music;compact disk;solo vocal accompaniment karaoke singing evaluation;humans;correlation coefficient;solo vocal;music;accompaniment	This study aims to develop an automatic singing evaluation system for Karaoke performances. Many Karaoke systems in the market today come with a scoring function. The addition of the feature enhances the entertainment appeal of the system due to the competitive nature of humans. The automatic Karaoke scoring mechanism to date, however, is still rudimentary, often giving inconsistent results with scoring by human raters. A cause of blunder arises from the fact that often only the singing volume is used as the evaluation criteria. To improve on the singing evaluation capabilities on Karaoke machines, this study exploits various acoustic features, including pitch, volume, and rhythm to assess a singing performance. We invited a number of singers having different levels of singing capabilities to record for Karaoke solo vocal samples. The performances were rated independently by four musicians, and then used in conjunction with additional Karaoke Video Compact Disk music for the training of our proposed system. Our experiment shows that the results of automatic singing evaluation are close to the human rating, where the Pearson product-moment correlation coefficient between them is 0.82.	acoustic cryptanalysis;coefficient;ground truth;performance;pitch (music);scoring functions for docking;structure of observed learning outcome	Wei-Ho Tsai;Hsin-Chieh Lee	2012	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2011.2174224	lead;speech recognition;acoustics;audio signal processing;rhythm;music;statistics	Visualization	-6.26031353127694	-90.65481328619116	173482
1f53949e490616318304e103bb5ad89d0773e396	characteristics of a low reject mode speaker verification system	sprakteknologi sprakvetenskaplig databehandling;speaker verification;language technology computational linguistics	The performance of a speaker verification (SV) system is normally determined by the false reject (FRR) and false accept (FAR) rates as averages on a population of test speakers. However, information on the FRR distribution is required when estimating the portion of clients that will suffer from an unacceptably high reject rate. This paper studies this distribution in a population using a SV system operating in low reject mode. Two models of the distribution are proposed and compared with test data. An attempt is also made to tune the decision threshold in order to obtain a desired portion of clients having a reject rate lower than a specified value.	speaker recognition;systemverilog;test data	Daniel Elenius;Mats Blomberg	2002			speaker recognition;speech recognition;computer science;artificial intelligence;machine learning	Metrics	-14.698890276293023	-92.4724491340659	173593
cfd94c80d0fdb6178cbde316bbdb02c5c32532c7	singer identification using time-frequency audio feature	time frequency audio feature;feed forward neural network;spectrogram;time frequency;singer identification;least square;k nearest neighbor;k nearest neighbor knn	Singer identification is a difficult topic in music information Retriveal research area. Because the background instrumental accompaniment in audio music is regarded as noise source that has to reduce a performance.#R##N##R##N#This paper proposes a singer identification algorithm thai is able to automatically identify a singer in an audio music signal with background music by using Time-Frequency audio feature. The main idea is used a spectrogram to able effective Time-Frequency feature and used as the input for classification. The proposed technique is test with 20 different singer. Sereval classification technique are compared,such as Feed-Forward Neural Network, k-Nearest Neighbor (kNN) and Minimum least square linear classifier(Fisher). The experimental result on singer identification using a spectrogram with Feed-Forward Neural Networkand and k-Nearest Neighbor (kNN) can effectively identify the singer in music signal with background music more than 92%.		Pafan Doungpaisan	2011		10.1007/978-3-642-21090-7_57	feedforward neural network;speech recognition;time–frequency analysis;computer science;machine learning;spectrogram;pattern recognition;least squares;k-nearest neighbors algorithm	HCI	-9.265956164954229	-90.83504511069131	173790
ac2e5a8436f8302312f6ab5c011381f33a1aa068	adapting a duration synthesis model to rate children's oral reading prosody		We describe an automated method to assess children’s oral reading using a prosodic synthesis model trained on multiple adults’ speech. We evaluate it against a previous method that correlated the prosodic contours of children’s oral reading against adult narrations of the same sentences. We compare how well the two methods predict fluency and comprehension test scores and gains of 55 children ages 7-10 who used Project LISTEN’s Reading Tutor. The new method does better on both tasks without requiring an adult narration of every sentence.	semantic prosody	Minh Duong;Jack Mostow	2010			speech recognition;prosody;computer science	NLP	-16.69157133987326	-82.67269123848679	173856
0b1a73ebf998ef90105ada2e39efa5b1b413d885	multiclass svm-based language-independent emotion recognition using selective speech features	support vector machines emotion recognition natural languages speech recognition;german corpus emo db multiclass svm language independent emotion recognition selective speech features speech signal germen telugu mel frequency cepstral coefficient mfcc short term energy spectral roll off zero crossing rate surrey audio visual expressed emotion savee database support vector machine;informatics;language independent emotion feature extraction multiclass svm libsvm mfcc energy	In this paper, we emphasize on recognizing six basic emotions viz. Anger, Disgust, Fear, Happiness, Neutral and Sadness using selective features of speech signal of different languages like Germen and Telugu. The feature set includes thirteen Mel-Frequency Cepstral Coefficients (MFCC) and four other features of speech signal such as Energy, Short Term Energy, Spectral Roll-Off and Zero-Crossing Rate (ZCR). The Surrey Audio-Visual Expressed Emotion (SAVEE) Database is used to train the Multiclass Support Vector Machine (SVM) classifier and a German Corpus EMO-DB (Berlin Database of Emotional Speech) and Telugu Corpus IITKGP: SESC are used for emotion recognition. The results are analyzed for each speech emotion separately and obtained accuracies of 98.3071% and 95.8166 % for Emo-DB, IITKGP: SESC databases respectively.	coefficient;database;decibel;emotion recognition;language-independent specification;mel-frequency cepstrum;multiclass classification;roll-off;sadness;support vector machine;viz: the computer game;zero-crossing rate	T. Amol KokaneAmol;Ram Mohana Reddy Guddeti	2014	2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2014.6968337	natural language processing;speech recognition;computer science;pattern recognition;informatics	NLP	-12.684449416928604	-87.89619938170398	173896
8d31a8ac8ee6d4d6b96f6f1641b5b973d6974602	audio-guided audiovisual data segmentation, indexing, and retrieval	audio content analysis;reconocimiento palabra;indexation automatique;hidden markov model;automatic segmentation;speech processing;modele markov variable cachee;tratamiento palabra;traitement parole;video segmentation;segmentation;indexing and retrieval;hidden markov models;statistical analysis;indexation;automatic indexing;speech recognition;reconnaissance parole;video;data retrieval;time frequency analysis;segmentacion;indizacion automatica	While current approaches for video segmentation and indexing are mostly focused on visual information, audio signals may actually play a primary role in video content parsing. In this paper, we present an approach for automatic segmentation, indexing, and retrieval of audiovisual data, based on audio content analysis. The accompanying audio signal of audiovisual data is first segmented and classified into basic types, i.e., speech, music, environmental sound, and silence. This coarse-level segmentation and indexing step is based upon morphological and statistical analysis of several short-term features of the audio signals. Then, environmental sounds are classified into finer classes, such as applause, explosions, bird sounds, etc. This fine-level classification and indexing step is based upon time- frequency analysis of audio signals and the use of the hidden Markov model as the classifier. On top of this archiving scheme, an audiovisual data retrieval system is proposed. Experimental results show that the proposed approach has an accuracy rate higher than 90 percent for the coarse-level classification, and higher than 85 percent for the fine-level classification. Examples of audiovisual data segmentation and retrieval are also provided.© (1998) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Tong Zhang;C.-C. Jay Kuo	1999		10.1117/12.333851	computer vision;speech recognition;computer science;pattern recognition	Vision	-7.8322688681495025	-91.85224798427532	173902
7791f8f64fe8bb69d155834041ec2290c7eed952	content-based identification of audio material using mpeg-7 low level description	pattern recognition	"""Along with investigating similarity metrics between audio material, the topic of robust matching of pairs of audio content has gained wide interest recently. In particular, if this matching process is carried out using a compact representation of the audio content (""""audio fingerprint""""), it is possible to identify unknown audio material by means of matching it to a database with the fingerprints of registered works. This paper presents a system for reliable, fast and robust identification of audio material which can be run on the resources provided by today's standard computing platforms. The system is based on a general pattern recognition paradigm and exploits low level signal features standardized within the MPEG-7 framework, thus enabling interoperability on a world-wide scale. Compared to similar systems, particular attention is given to issues of robustness with respect to common signal distortions, i.e. recognition performance for processed/modified audio signals. The system's current performance figures are benchmarked for a range of real-world signal distortions, including low bitrate coding and transmission over an acoustic channel. A number of interesting applications are discussed."""	acoustic cryptanalysis;acoustic fingerprint;benchmark (computing);distortion;high- and low-level;interoperability;mpeg-7;pattern recognition;programming paradigm;semantic similarity	Eric Allamanche	2001			robustness (computer science);interoperability;theoretical computer science;audio signal;computer vision;artificial intelligence;computer science;exploit;communication channel	Vision	-6.189544783705497	-91.50411218577462	174223
3556670d0ac7f30696797f58ea35e0d23c195198	physiology of vocal production in the newborn		Vocal folds of newborns are histologically different from children and adults. Reinke’s space is not clearly individualized. As shown by Titze, this structure is absolutely needed for vocal fold vibration [1]. The hypothesis for vocal production in newborn is that the air column generates itself the acoustic turbulences (vortex) from which the sound merges. Some other possible vibrators within the mammalian production system include the vocal tract [2]. Acoustic analysis of excised larynx of 38 weekstime dead human foetus was performed. An acoustic analysis and a phase portrait were calculated on each recorded sample. A newborn cry was also recorded with the same DAT. Anatomical measurements were performed and a virtual model (Gambit) was designed to modelize turbulences with vocal folds in phonatory position (Fluent 6.0). All data were correlated with those obtained by Laser Doppler Velocimetry. The fundamental frequency of the sound produced by a fixed larynx was higher than those produced by fresh sample or newborn. Phase portraits are very different in each sample. High-frequency whirlwinds were modelized upon each vocal fold. Preliminary results suggest that newborn phonation is a vortex effect coupled with a vibration of supraglottic structures.	3d modeling;acoustic cryptanalysis;intel dynamic acceleration;johannes reinke;monty newborn;production system (computer science);tract (literature);vortex	Richard Nicollas;Maurice Ouaknine;Antoine Giovanni;J. Berger;J. P. To;D. Dumoulin;Jean-Michel Triglia	2003			physiology;biology	HCI	-8.246107932855265	-84.12506703713164	174295
d8bda6742a70cdc07d5a69ac427d9f95bf6d7f31	perceptual effects of coarticulation in fricatives	spl int coarticulation fricatives perceptual effects perceptual interaction consonant vowel syllables conflicting cue stimuli spl theta f s;speech intelligibility;speech intelligibility hearing;automatic speech recognition physics frequency signal processing acoustic noise acoustic measurements;hearing	The perceptual interaction between the consonant and the vowel in fricative + vowel syllables is evaluated. A set of conflicting cue stimuli was used to measure the relative importance of: a) the influence of the vowel in the previous consonant, and b) the influence of the fricative in the following vowel. It is concluded that the perceptual interaction between the consonant and the vowel in fricative-vowel syllables can not be explained only by the coarticulatory influence of consonant or vowel on adjacent segments. The influence of the vowel in the previous fricative is perceptually irrelevant, while the influence of the fricative in the following vowel is more important for the identification of / e / and /f/ than for / s / and /J/. Actually it is perceptually irrelevant for /J/ and a little important for I s / . Besides the influence of the fricative in the following vowel is notably dependent on the particular vowel.	emoticon;relevance	Santiago Fernández;Sergio Feijóo;Ramón Balsa;Nieves Barros	2000		10.1109/ICASSP.2000.861828	speech recognition;mid vowel;relative articulation;intelligibility	ML	-9.945982522626375	-82.30329578381969	174357
15834ca152fbedfbcf123678497b89f621abd7e1	adaptive dispersion theory and phonological vowel reduction in russian	russian;bayesian classification;vol 62;euclidean distance;neutralization;russe;phonological reduction;phonetic reduction;accentuation;stressing;acoustic phonetics;voyelle;phonetique acoustique;neutralisation;reduction phonologique;vowel;231270;no 1;phonetica 2005;reduction phonetique	Russian exhibits a rich pattern of phonological vowel reduction, by which some vowel contrasts are neutralized in unstressed syllables. Recent work in phonology suggests a mechanism by which phonetic vowel reduction--compression of the overall vowel space due to target undershoot--might lead to patterns like Russian. Presenting acoustic data from 9 speakers of Russian, we use Euclidean distance measures, measures of F1-F0 and F2-F1, and Bayesian classification to provide a basic picture of how the overall vowel space, as well as the distribution of vowels, change as stress is reduced. We are particularly interested in whether contraction of the vowel space in unstressed positions is primarily due to raising, and in whether contrasting pairs of vowels are evenly spaced within and across contexts. Our results provide qualified support for the first hypothesis, but largely do not support the hypothesis of equal spacing, in particular across contexts. Of additional interest, we find that some impressionistically described neutralizations are incomplete.	acoustic cryptanalysis;acta informatica;auditory processing disorder;bayesian network;biological anthropology;blackwell (series);body of uterus;columbus;categories;cerebellar folium;compression;contract agreement;data collection;description;dylan;emergence;euclidean distance;exhibits as topic;fletcher's checksum;formal grammar;generative grammar;hume (programming language);international unit;jart armin;jones calculus;largest;linguistics;naive bayes classifier;neutralization (chemistry);ocimum basilicum;p (complexity);paper;phonetics;prince;r language;radio frequency;ramer ladda syndrome;scientific publication;simulation;social sciences;speaking (activity);speech disorders;speech synthesis;springer (tank);surgical revision;syllable;text corpus;william wiswesser;phonology	Jaye Padgett;Marija Tabain	2005	Phonetica	10.1159/000087223	speech recognition;mid vowel;stress;relative articulation;neutralization;mathematics;linguistics;communication	NLP	-10.237224854239429	-80.57636110110322	174400
b62f2ac276ce11b4209b5d053a8728b4ee8d3ec2	cascade realization of digital inverse filter for extracting speaker dependent features	filtering;speech;data mining;linear predictive;speaker recognition;linear predictive coding;semantic information;automatic speaker recognition;statistical analysis;feature extraction;signal processing;digital filters;digital filters feature extraction linear predictive coding speech data mining speaker recognition filtering dynamic range statistical analysis signal processing;dynamic range;speaker dependent;electrical communication engineering	Quest for new speaker dependent features is a constant problem in the design of automatic speaker recognition systems. In speech, information about the speaker usually arises along with the semantio information which makes its independent use difficult.	inverse filter;speaker recognition	V. V. S. Sarma;Bayya Yegnanarayana	1976		10.1109/ICASSP.1976.1169982	filter;speaker recognition;computer vision;dynamic range;linear predictive coding;speech recognition;digital filter;feature extraction;computer science;speech;signal processing;pattern recognition	Graphics	-9.501132927358231	-92.15342289650621	174480
b21e6c35921b13ad7747708ae6babf946978f538	cent filter-banks and its relevance to identifying the main song in carnatic music		Carnatic music is a classical music tradition from Southern India. It is primarily based on vocal music, where the lead performer is a singer. A typical Carnatic music concert is made up of several items. Each item can be made up of a number of segments, namely, monophonic vocal solo, monophonic violin solo, polyphonic (vocal and accompanying instruments) composition (or song) and monophonic percussion (thaniavarthanam). The composition (or song) segment is mandatory in every item. The identification of composition segments is necessary to determine the different items in a concert. Owing to the improvisation possibilities in a composition, the compositional segments can further consist of monophonic segments. The objective of this paper is to determine the location of song segments in a concert. The improvisational aspects of a concert lead to the number of applauses being much larger than the number of items. The concert is first segmented using the applauses. Next, inter-applause segments are classified as vocal solo, violin solo, composition and thaniavarthanam segments. Unlike Western music, the key used for different items in the concert is fixed by the performer. The key also referred to as tonic can vary from musician to musician and can also vary across concerts by the same musician. In order to classify different inter-applause segments across musicians, the features must be normalised with respect to the tonic. A new feature called Cent Filter-bank based Cepstral Coefficients (CFCC) that is tonic invariant is proposed. Song identification is performed on 50 live recordings of Carnatic music. The results are compared with that of the Mel Frequency Cepstral Coefficients (MFCC), and Chroma based Filter-bank Cepstral Coefficients (ChromaFCC). The song identification accuracy with MFCC is 80 %, with CFCC features is 95 % and with ChromaFCC features is 75 %. The results show that CFCC features give promising results for Carnatic music processing tasks.	relevance	Padi Sarala;Hema A. Murthy	2013		10.1007/978-3-319-12976-1_40	speech recognition;literature	NLP	-7.892095076024728	-91.53181424091383	174548
47ad2486a35eb9b26e68179297aa5fadad887fd2	how natural is chinese l2 english prosody?		Standard varieties of Chinese and English have major typological prosodic differences, which present considerable difficulties for Chinese L2 learners of English at all levels: first, differences in the phonotactic foundations of prosody (syllable and syllable sequence patterns); second, the difference between lexical tone language and lexical stress-accent language; third, timing differences in the prosodic hierarchy, including the timing of grammatical units. We compare Chinese L2 and English native speakers in respect of temporal distribution patterns at the phoneticsphonology interface. The SPPAS and TGA phonetic analysis tools are used. Results indicate clear relations between timing patterns at different L2 proficiency levels and native patterns.	semantic prosody;syllable;truevision tga	Jue Yu;Dafydd Gibbon	2015				NLP	-11.652384362533393	-80.23046205327482	174581
d4d1d88a4000670b2ad001e4907b4fb26af3e845	rank based decoding for improved dnn/hmm hybrid acoustic models in the eml transcription platform			acoustic cryptanalysis;emotion markup language;hidden markov model;medical transcription	Volker Fischer;Siegfried Kunzmann	2016				NLP	-15.248874164556742	-86.21869436152008	175013
832fc989fa7883d8ef31726cd79ecfbd7ac87986	explaining the visual and masked-visual advantage in speech perception in noise: the role of visual phonetic cues		Visual enhancement of speech intelligibility, although clearly established, still resists a clear description. We attempt to contribute to solving that problem by proposing a simple account based on phonetically motivated visual cues. This work extends a previous study quantifying the visual advantage in sentence intelligibility across three conditions with varying degrees of visual information available: auditory only, auditory visual orally masked and auditory-visual. We explore the role of lexical as well as visual factors, the latter derived from groupings in visemes. While lexical factors play an undiscriminative role across modality conditions, some measure of viseme confusability seems to capture part of the performance results. A simple characterisation of the phonetic content of sentences in terms of visual information occurring exclusively inside the mask region was found to be the strongest predictor for the auditory-visual masked condition only, demonstrating a direct link between localised visual information and auditory-visual speech processing performance.	inline linking;intelligibility (philosophy);kerrison predictor;lexicon;mask (computing);modality (human–computer interaction);speech processing	Vincent Aubanel;Chris Irwin Davis;Jeesun Kim	2015			speech perception;communication;cue-dependent forgetting;psychology	HCI	-9.693602823796457	-81.0464433087301	175080
f59c8ad5a1527df51ad51d74bd02fdacf5910df4	synthesizing multimodal utterances for conversational agents	model based computer animation;motion control;product model;multimodal conversational agents;temporal constraints;gesture animation;conversational agent;computer animation;animal model	Conversational agents are supposed to combine speech with non-verbal modalities for intelligible multimodal utterances. In this paper, we focus on the generation of gesture and speech fromXML-based descriptions of their overt form. An incremental production model is presented that combines the synthesis of synchronized gestural, verbal, and facial behaviors with mechanisms for linking them in fluent utterances with natural co-articulation and transition effects. In particular, an efficient kinematic approach for animating hand gestures from shape specifications is presented, which provides fine adaptation to temporal constraints that are imposed by cross-modal synchrony. Copyright # 2004 John Wiley & Sons, Ltd.	anim;biconnected component;context-sensitive language;dialog system;john d. wiley;library (computing);max;modal logic;modulation;multimodal interaction;syllable;synthetic intelligence;velocity (software development);virtual world;xml	Stefan Kopp;Ipke Wachsmuth	2004	Journal of Visualization and Computer Animation	10.1002/cav.6	natural language processing;motion control;speech recognition;computer science;artificial intelligence;dialog system;computer animation	AI	-15.171963900451367	-82.03838022028494	175146
aef54207aef5a95d26cb41d41ad1fdc09ff561c5	a phonetic dictionary for demisyllabic speech synthesis	speech synthesis;dictionaries speech synthesis speech analysis linear predictive coding application specific processors smoothing methods inspection natural languages liquids;speech analysis;natural languages;inspection;liquids;linear predictive coding;smoothing methods;dictionaries;application specific processors	Demisyllables and affixes (1,2,3) have been found to be very promising units for concatenative speech synthesis	dictionary;speech synthesis	Marian J. Macchi	1980		10.1109/ICASSP.1980.1170969	natural language processing;linear predictive coding;speech recognition;inspection;speech corpus;computer science;speech coding;natural language;speech synthesis	Crypto	-16.074060737991477	-85.58474944774134	175355
b1b74360e229c2b281e78804e12bda5ab655f451	regularized subspace gaussian mixture models for cross-lingual speech recognition	swedish;word error rate;supervised automatic subtitling;language use;acoustic model training;vectors acoustics hidden markov models training data data models estimation speech recognition;light supervision;neural nets;gaussian processes;acoustics;acoustic modeling;training;cross lingual speech recognition;approximate transcriptions;weather forecasting;speech;acoustic signal processing;segmentation;global subspace parameter;word error rates;cross lingual acoustic modelling;training data;asr;automatic speech recognition;gaussian mixture model;hidden markov models;low resource language;vectors;estimation;weatherview;regularized subspace gaussian mixture model;transcription;target language;spanish;subtitling;word error rates regularized subspace gaussian mixture model cross lingual speech recognition cross lingual acoustic modelling low resource language global subspace parameter globalphone corpus spanish portuguese swedish german;weather forecasts;source language;speech recognition;hybrid deep neural network based recognition system;learning artificial intelligence;german;portuguese;natural language processing;speech recognition acoustic signal processing gaussian processes natural language processing;globalphone corpus;data models;light supervision methods;daily weather report	We investigate cross-lingual acoustic modelling for low resource languages using the subspace Gaussian mixture model (SGMM). We assume the presence of acoustic models trained on multiple source languages, and use the global subspace parameters from those models for improved modelling in a target language with limited amounts of transcribed speech. Experiments on the GlobalPhone corpus using Spanish, Portuguese, and Swedish as source languages and German as target language (with 1 hour and 5 hours of transcribed audio) show that multilingually trained SGMM shared parameters result in lower word error rates (WERs) than using those from a single source language. We also show that regularizing the estimation of the SGMM state vectors by penalizing their ℓ1-norm help to overcome numerical instabilities and lead to lower WER.	acoustic cryptanalysis;acoustic model;compiler;experiment;google map maker;numerical analysis;quantum state;speech recognition;subspace gaussian mixture model;taxicab geometry;word error rate	Liang Lu;Arnab Ghoshal;Steve Renals	2011	2011 IEEE Workshop on Automatic Speech Recognition & Understanding	10.1109/ASRU.2011.6163959	natural language processing;estimation;speech recognition;weather forecasting;german;word error rate;computer science;speech;machine learning;pattern recognition;linguistics;transcription;segmentation;artificial neural network;spanish;hidden markov model;portuguese	NLP	-18.53859074058844	-89.33148879098322	175511
28064d26481b17c1fda1cd58000d9877eadfae18	phone-based speech synthesis with neural network and articulatory control	speech intelligibility;neural network;network synthesis;controllability;backpropagation;speech processing;speech coding;natural languages;speech synthesis;neural networks	This paper presents a novel method for synthesizing speech signal using a phone-based concatenation approach. Neural network is employed for the generalization of the phone templates during synthesis. Simpli ed articulatory space input parameters based on a modi ed vowel diagram are used to provide exible and e ective articulatory control. It also enables the design of an articulatory control model for allophonic variations in speech signal. The network approach is chosen for its non-linear mapping of the relationship between the articulatory space parameters and the spectral information of speech signal. In addition, non-linear approximation for phone template transitions is facilitated. The phone templates of the synthesizer are implicitly stored as network parameters of a medium size network. The performance of this new speech synthesis technique is demonstrated with a prototype system speci cally designed for Cantonese (a common Chinese dialect) and the synthetic speech quality is assessed by informal listening tests.	artificial neural network;concatenation;diagram;linear approximation;nonlinear system;prototype;speech synthesis;synthetic intelligence	Wai Kit Lo;Pak-Chung Ching	1996			voice activity detection;natural language processing;speech recognition;computer science;speech processing;time delay neural network;communication	AI	-15.24853753209173	-84.5335238120062	175526
9a639d3876fddfa878ede11b1f7ffad4ca0576cd	multichannel filters for speech recognition using a particle swarm optimization	pso multichannel filters speech recognition particle swarm optimization speech control functions speech recognizer speech command identification noisy speech command enhancement;speech enhancement;conference paper;swarm optimization;speech enhancement speech recognition multi channel filter swarm optimization;channel bank filters;speech recognition;multi channel filter;speech recognition speech noise equations speech enhancement accuracy particle swarm optimization;particle swarm optimisation;speech recognition channel bank filters particle swarm optimisation	Speech recognition has been used in various real-world applications such as automotive control, electronic toys, electronic appliances etc. In many applications involved speech control functions, a commercial speech recognizer is used to identify the speech commands voiced out by the users and the recognized command is used to perform appropriate operations. However, users' commands are often corrupted by surrounding ambient noise. It decreases the effectiveness of speech recognition in order to implement the commands accurately. This paper proposes a multichannel filter to enhance noisy speech commands, in order to improve accuracy of commercial speech recognizers which work under noisy environment. An innovative particle swarm optimization (PSO) is proposed to optimize the parameters of the multichannel filter which intends to improve accuracy of the commercial speech recognizer working under noisy environment. The effectiveness of the multichannel filter was evaluated by interacting with a commercial speech recognizer, which was worked in a warehouse.	algorithm;control function (econometrics);embedded system;finite-state machine;interaction;mathematical optimization;optimization problem;particle swarm optimization;phase-shift oscillator;software release life cycle;speech recognition;toys	Kit Yan Chan;Sven Nordholm;Ka Fai Cedric Yiu	2012	2012 12th International Conference on Control Automation Robotics & Vision (ICARCV)	10.1109/ICARCV.2012.6485283	voice activity detection;speech recognition;computer science;engineering;speech coding;speech processing;acoustic model	Robotics	-10.187543842118734	-89.89861182830786	175746
e3e499e52c2fe2d464f6270c6834305e9181116a	production and perception of vietnamese final stop consonants /p, t, k/		The bursts and voiced formant transitions are well known as separate cues to the place of articulation of initial stop consonant. The Vietnamese presents three final voiceless stop consonants /p, t, k/ without bursts. It is an opportunity to study these final stop consonants and to compare their characteristics with those of the corresponding initial stop consonants. This paper analyses these final consonants in terms of the vowel-consonant (VC) transition duration, the starting formant transition values and the slopes of the VC transition. Measurements have shown that in the same vocalic contexts (the same preceding vowel contexts), the three final stop consonants /p, t, k/ are always clearly different by at least one of the three slopes of F1, F2 and F3. In perception tests, synthesized consonant C in the context /a/-C are recognized as /p/, or /t/, or /k/ when the slopes of the /a/-C transition of F2 and F3 are varied. It means that slopes of the VC transition is an important parameter that allows Vietnamese distinguishing three final voiceless stop consonant /p, t, k/ in Vietnamese language.	biconnected component;vc dimension;vowel–consonant synthesis	Viet Son Nguyen;Eric Castelli;René Carré	2010			vietnamese;social psychology;psychology;perception	NLP	-10.729049854894761	-82.0759951813988	176137
1ceceb8c720a93902c078e0c7514d937d250155c	use of novel feature extraction technique with subspace classifiers for speech recognition	wavelet analysis;pattern recognition system;human interaction;common vector approach;human computer interaction;ti digit database;data engineering;linear subspace classifier;wavelet transforms;feature vector;mel frequency cepstral coefficient;wavelet transforms feature extraction human computer interaction speech recognition vectors visual databases;linear predictive coding;feature extraction technique;vectors;speaker independent;system design;feature extraction;feature extraction speech recognition continuous wavelet transforms fourier transforms wavelet transforms wavelet analysis humans linear predictive coding mel frequency cepstral coefficient data engineering;fourier transforms;ti digit database feature extraction technique speech recognition human interaction pattern recognition system linear subspace classifier common vector approach multiple similarity method;word recognition;pattern recognition;speech recognition;humans;multiple similarity method;continuous wavelet transforms;visual databases	Speech recognition is one of the fast moving research areas in pervasive services requiring human interaction. Like any type of pattern recognition system, selection of the feature extraction method and the classifier play a crucial role for speech recognition in terms of accuracy and speed. In this paper, an efficient wavelet based feature extraction method for speech data is presented. The feature vectors are then fed into three widely used linear subspace classifiers for recognition analysis. These classifiers are Class Featuring Information Compression (CLAFIC), Multiple Similarity Method (MSM) and Common Vector Approach (CVA). TI-DIGIT database is used to evaluate the performance of speaker independent isolated word recognition system designed. Experimental results indicate that the proposed feature extraction method together with the CLAFIC and CVA classifiers give considerably high recognition rates.	computational complexity theory;feature extraction;feature vector;markov switching multifractal;pattern recognition;pervasive informatics;speech recognition;wavelet;wavelet transform	Serkan Günal;Rifat Edizkan	2007	IEEE International Conference on Pervasive Services	10.1109/PERSER.2007.4283894	random subspace method;wavelet;speaker recognition;fourier transform;interpersonal relationship;linear predictive coding;speech recognition;feature vector;feature;information engineering;feature extraction;word recognition;computer science;machine learning;pattern recognition;wavelet transform;systems design	Vision	-9.4005834125559	-92.34818349653713	176154
c017cbd40614c13b71c9cb1d8db81ae83940deef	the use of lexical knowledge in phonetic categorisation		Lexical effects on phonetic categorisation have been taken as evidence that the listener's word knowledge inßuences phonetic processing during normal speech perception. Tbe present study examined word-nonword effects in the categorisation of word-initial and wordfinal stop consonants. Natural speech was edited to produce bilabial, alveolar and velar voicing continua. Tbe data revealed a significant word-nonword effect, such that subjects were more likely to categorise an ambiguous consonant as voiced if the voiced endpoint of tbe continuum was a word, bot as unvoiced if tbe unvoiced endpoint of tbe continuum ·was a word. Bot it was found tbat within some blocks tbere was no evidence of tbis lexical sbift. Subsequent experimental manipulations provided additional evidence that this shift is highly variable. The fact that the effect is nonmandatory provides support for tbe view tbat phonetic processing can occur independently of lexical processing.	apache continuum;categorization;communication endpoint	James M. McQueen	1989			artificial intelligence;lexical functional grammar;pattern recognition;computer science	NLP	-9.769023558303093	-80.87141672824971	176166
ee73e7831b23f3fe5caefecc060dff79d4249e0b	dnn-based duration modeling for synthesizing short sentences		Statistical parametric speech synthesis conventionally utilizes deci- sion tree clustered context-dependent hidden Markov models (HMMs) to model speech parameters. But decision trees are unable to capture complex context dependencies and fail to model the interaction between linguistic features. Recently deep neural networks (DNNs) have been applied in speech synthesis and they can address some of these limitations. This paper focuses on the prediction of phone durations in Text-to-Speech (TTS) systems using feedfor- ward DNNs in case of short sentences (sentences containing one, two or three syllables only). To achieve better prediction accuracy hyperparameter opti- mization was carried out with manual grid search. Recordings from a male and a female speaker were used to train the systems, and the output of various con- figurations were compared against conventional HMM-based solutions and natural speech. Experimental results of objective evaluations show that DNNs can outperform previous state-of-the-art solutions in duration modeling.		Péter Nagy;Géza Németh	2016		10.1007/978-3-319-43958-7_30	natural language processing;speech recognition;computer science;communication	NLP	-18.64495091353843	-85.45300669789174	176320
55d5cce5037507a62bc738a9a6388f684f48da0a	effectiveness of kl-transformation in spectral delta expansion		MFCC is widely used together with its delta and delta-delta features in the field of speech recognition based on HMM. MFCC is designed to apply DCT to the MF output. We propose in this paper to employ KL transformation instead of DCT, because it can reflect the statistics of speech data more precisely. MFCC is the compressed feature of the log MF so that some detailed features seem to be lost. In this sense, we propose to compute the delta and delta-delta feature on the MF, and apply the KL transformation to a set of MF, its delta and delta-delta features.	discrete cosine transform;hidden markov model;kl-one;speech recognition	Michihide Tokuhira;Yasuo Ariki	1999			speech recognition;artificial intelligence;delta;pattern recognition;computer science	NLP	-11.157532381418333	-90.73963081125846	176572
97f52cbc9d57a8d3bdfa52b59b295f8bc65168b8	a study of duration in continuous speech recognition based on ddbhmm		1 Now work at Intel China Research Center, Beijing. Email: qingwei.zhao@intel.com ABSTRACT DDBHMM solved the defects of traditional HMM. Based on DDBHMM, the problem of how to effectively utilize the duration information is studied in detail. The approach on estimating the duration distribution is introduced firstly, then the data file is classified according to the speak rate. The recognition experiment shows that, the duration information behaves best on the data of low speak rate, behaves normal on the data of medium speak rate and has little effect on the data of fast speak rate. Therefore, the most importance of duration is that by it the more accurate state segmentation point could be obtained and then the recognition rate can be improved. At the same time, the robustness of the system to speaking rate is improved with the employment of the duration information. Furthermore, the method of classified duration and normalized duration is also put forward and studied in detail, it shows that both of the two method can improve the effect. In order to study the dependency between the duration, the method of using the Bigram of the duration is proposed and analyzed. At last, the approach of post processing duration is studied, it shows that only based on DDBHMM, and utilizing the duration information synchronously in the recognition process, then the performance can be improved greatly.	bigram;email;hidden markov model;speech recognition	Qingwei Zhao;Zuoying Wang;Dajin Lu	1999			speech recognition;computer science;artificial intelligence;communication	AI	-16.7330500175041	-88.37416693636943	176604
1050aed90f3a90c6e22a1200d59de0139cceaca3	frequency, collocation, and statistical modeling of lexical items: a case study of temporal expressions in two conversational corpora	判解;temporal expression;法律詞典;論文;大陸法學;法規;shu kai hsieh;月旦法學;clustering;法律題庫;裁判時報;collocation;corpus linguistics;yu wen liu;月旦知識庫;法學資料庫;sheng fu wang;tssci;gerontology;jing chen yang;教學;yu yun chang	This study examines how different dimensions of corpus frequency data may affect the outcome of statistical modeling of lexical items. Our analysis mainly focuses on a recently constructed elderly speaker corpus that is used to reveal patterns of aging people’s language use. A conversational corpus contributed by speakers in their 20s serves as complementary material. The target words examined are temporal expressions, which might reveal how the speech produced by the elderly is organized. We conduct divisive hierarchical clustering analyses based on two different dimensions of corporal data, namely raw frequency distribution and collocation-based vectors. When different dimensions of data were used as the input, results showed that the target terms were clustered in different ways. Analyses based on frequency distributions and collocational patterns are distinct from each other. Specifically, statistically-based collocational analysis generally produces more distinct clustering results that differentiate temporal terms more delicately than do the ones based on raw frequency. 1 Acknowledgement: Thanks Wang Chun-Chieh, Liu Chun-Jui, Anna Lofstrand, and Hsu Chan-Chia for their involvement in the construction of the elderly speakers’ corpus and the early development of this paper. ∗ Graduate Institute of Linguistics, National Taiwan University, 3F, Le-Xue Building, No. 1, Sec. 4, Roosevelt Rd., Taipei Taiwan, 106 E-mail: {sftwang0416; flower75828; june06029}@gmail.com; shukaihsieh@ntu.edu.tw + Department of English, National Taiwan Normal University, No. 162, He-ping East Road, Section 1, Taipei, Taiwan, 106 E-mail: Yw_L7@hotmail.com 38 Sheng-Fu Wang et al.	anna karlin;cluster analysis;collocation;execution unit;hierarchical clustering;speech corpus;statistical model;temporal expressions;text corpus	Sheng-Fu Wang;Jing-Chen Yang;Yu-Yun Chang;Yu-Wen Liu;Shu-Kai Hsieh	2012	IJCLCLP		psychology;speech recognition;artificial intelligence;communication	NLP	-11.93806912111124	-80.33795085551178	176778
0ce37f923a3511f8e57727b1ef449382978b6784	sub-band based recognition of noisy speech	estimation theory;probability;acoustic noise speech recognition probability estimation theory spectral analysis;speech recognition automatic speech recognition merging power system modeling frequency estimation speech enhancement degradation humans decoding vocabulary;frequency spectrum;automatic speech recognition;acoustic noise;speech recognition;frequency spectrum sub band based recognition noisy speech automatic speech recognition independent class conditional probability estimates frequency sub bands partial corruption;spectral analysis;conditional probability	A new approach to automatic speech recognition based on independent class-conditional probability estimates in several frequency sub-bands is presented. The approach is shown to be especially applicable to environments which cause partial corruption of the frequency spectrum of the signal. Some of the issues involved in the implementation of the approach are also addressed.	automated system recovery;bernard greenberg;elegant degradation;ibm notes;morgan;signal-to-noise ratio;spectral density;speech recognition	Sangita Tibrewala;Hynek Hermansky	1997		10.1109/ICASSP.1997.596173	voice activity detection;speaker recognition;frequency spectrum;linear predictive coding;speech recognition;conditional probability;entropy estimation;computer science;noise;pattern recognition;probability;speech processing;acoustic model;mathematics;estimation theory;statistics;automatic target recognition	Comp.	-14.469025065754682	-92.9350311000895	177074
a2fa05f305a1b176896673ba20545e85ceefbce7	interaction of phonetics, phonology, and sociophonology - illustrated by the vowels of standard austrian german		Standard Austrian German, which distinguishes eight front vowels, exploits two front constriction locations for distinguishing these vowels, a pre-palatal and a palatal one. However, the pre-palatal location is acoustically unstable, and, consequently, phonologically undesirable. The existence of a sound change which neutralizes the /i/ and /ɪ/ vowels in a first step and shifts the constriction location of the remaining /i/ towards the palatal region in a second step is investigated.	control theory	Sylvia Moosmüller	2008			bernese german phonology;phonology;phonetics;linguistics;natural language processing;artificial intelligence;computer science;german	NLP	-10.776042407118675	-81.58369902278277	177092
024647070a85e2f58498b157c2bf291ba87e0b32	beyond linear transforms: efficient non-linear dynamic adaptation for noise robust speech recognition	linear transformation;non linear dynamics;speech recognition	In this paper, we present new theory and results that combine constrained Maximum Likelihood Linear Regression (MLLR), known as feature space MLLR (fMLLR), a state-of-the-art model adaptation technique, with Dynamic Noise Adaptation (DNA), a state-of-the-art noise adaptation algorithm. We explain how DNA implements a highly non-linear transform on speech model features, and why DNA is better suited for compensating for additive noise than fMLLR. Tests results are presented on the DNA + Aurora II framework, which is based upon a collection of challenging in-car noise recordings, as a function of SNR. The results demonstrate that DNA significantly outperforms block fMLLR on additive noise, and that DNA + fMLLR outperforms the ETSI advanced front-end (AFE) system + fMLLR by a significant margin (over 7% absolute).	additive white gaussian noise;algorithm;analog front-end;aurora;feature vector;image noise;nonlinear system;signal-to-noise ratio;speech recognition;utility functions on indivisible goods	Steven J. Rennie;Pierre L. Dognin	2008			speech recognition;pattern recognition;nonlinear system;machine learning;computer science;artificial intelligence;linear map	ML	-14.511645016573391	-91.61433432930554	177111
7237530ca0a6231322bce7f3465357362a1ed71a	logitboost weka classifier speech segmentation	discrete wavelet transforms;discrete wavelet transform;discrete wavelet transforms speech speech recognition transforms time frequency analysis classification algorithms wavelet transforms;speech recognition discrete wavelet transforms pattern classification;speech segmentation;logitboost speech segmentation weka machine learning classifier;weka;speech;dis crete wavelet transform;statistical classification method;polish language logitboost weka classifier speech segmentation time frequency analysis discrete wavelet transform phonemes boundary statistical classification method;indexing terms;power spectrum;phonemes boundary;polish language;wavelet transforms;classifier;machine learning;logitboost;classification algorithms;transforms;pattern classification;speech recognition;logitboost weka classifier speech segmentation;time frequency analysis	Segmenting the speech signals on the basis of time-frequency analysis is the most natural approach. Boundaries are located in places where energy of some frequency subband rapidly changes. Speech segmentation method which bases on discrete wavelet transform, the resulting power spectrum and its derivatives is presented. This information allows to locate the boundaries of phonemes. A statistical classification method was used to check which features are useful. The efficiency of segmentation was verified on a male speaker taken from a corpus of Polish language.	discrete wavelet transform;frequency analysis;logitboost;spectral density;speech segmentation;statistical classification;text corpus;time–frequency analysis;weka	Bartosz Ziólko;Suresh Manandhar;Richard C. Wilson;Mariusz Ziólko	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607680	statistical classification;speech recognition;time–frequency analysis;index term;classifier;computer science;speech;machine learning;logitboost;pattern recognition;polish;speech segmentation;scale-space segmentation;discrete wavelet transform;spectral density;wavelet transform	Vision	-8.456022197357239	-90.95877509425438	177132
a27c32d48d606027333bd47b143ca5ddb030d6cc	sounds of the human vocal tract		Previous research suggests that beatboxers only use sounds that exist in the world’s languages. This paper provides evidence to the contrary, showing that beatboxers use non-linguistic articulations and airstream mechanisms to produce many sound effects that have not been attested in any language. An analysis of real-time magnetic resonance videos of beatboxing reveals that beatboxers produce non-linguistic articulations such as ingressive retroflex trills and ingressive lateral bilabial trills. In addition, beatboxers can use both lingual egressive and pulmonic ingressive airstreams, neither of which have been reported in any language. The results of this study affect our understanding of the limits of the human vocal tract, and address questions about the mental units that encode music and phonological grammar.	encode;lateral thinking;real-time transcription;resonance;tract (literature)	Reed Blaylock;Nimisha Patil;Timothy Greer;Shrikanth (Shri) Narayanan	2017			speech recognition;vocal tract;computer science;human voice	HCI	-8.600718612241286	-82.95738097610067	177573
d7773bd7d8d5eef6fe322d442455b432d15a5599	gmm-based intermediate matching kernel for classification of varying length patterns of long duration speech using support vector machines	support vector machines feature selection gaussian processes mixture models pattern matching probability signal classification speaker recognition;speaker identification tasks gmm based intermediate matching kernel dynamic kernel based support vector machines dk based support vector machines varying length patterns classification imk long duration speech representation virtual feature vectors local feature vectors selection class independent gaussian mixture model cigmm responsibility term weighted base kernels discrimination ability posterior probability weighted dk classification performance svm based classifiers speech emotion recognition;vectors kernel speech training data support vector machines probabilistic logic phase shift keying;varying length pattern intermediate matching kernel imk long duration speech speaker recognition speech emotion recognition ser support vector machine svm	Dynamic kernel (DK)-based support vector machines are used for the classification of varying length patterns. This paper explores the use of intermediate matching kernel (IMK) as a DK for classification of varying length patterns of long duration speech represented as sets of feature vectors. The main issue in construction of IMK is the choice for the set of virtual feature vectors used to select the local feature vectors for matching. This paper proposes to use components of class-independent Gaussian mixture model (CIGMM) as a representation for the set of virtual feature vectors. For every component of CIGMM, a local feature vector each from the two sets of local feature vectors that has the highest probability of belonging to that component is selected and a base kernel is computed between the selected local feature vectors. The IMK is computed as the sum of all the base kernels corresponding to different components of CIGMM. It is proposed to use the responsibility term weighted base kernels in computation of IMK to improve its discrimination ability. This paper also proposes the posterior probability weighted DKs (including the proposed IMKs) to improve their classification performance and reduce the number of support vectors. The performance of the support vector machine (SVM)-based classifiers using the proposed IMKs is studied for speech emotion recognition and speaker identification tasks and compared with that of the SVM-based classifiers using the state-of-the-art DKs.	acoustic cryptanalysis;acoustic model;audio media;binary classification;class;classification;computation;emotion recognition;extraction;feature vector;forward kinematics;gm(m);google map maker;kernel;kernel (operating system);matching;mixture model;multiclass classification;normal statistical distribution;pre-shared key;software release life cycle;speaker recognition;substring;support vector machine	Aroor Dinesh Dileep;Chellu Chandra Sekhar	2014	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2013.2293512	least squares support vector machine;kernel method;speech recognition;radial basis function kernel;machine learning;pattern recognition;mathematics;relevance vector machine;polynomial kernel	ML	-16.306804408355102	-91.69844845829043	177690
dea2a49df812935bc922cc9fcc6ec99bb7ea7446	dnn-hmm based automatic speech recognition for hri scenarios		In this paper, we propose to replace the classical black box integration of automatic speech recognition technology in HRI applications with the incorporation of the HRI environment representation and modeling, and the robot and user states and contexts. Accordingly, this paper focuses on the environment representation and modeling by training a deep neural network-hidden Markov model based automatic speech recognition engine combining clean utterances with the acoustic-channel responses and noise that were obtained from an HRI testbed built with a PR2 mobile manipulation robot. This method avoids recording a training database in all the possible acoustic environments given an HRI scenario. Moreover, different speech recognition testing conditions were produced by recording two types of acoustics sources, i.e. a loudspeaker and human speakers, using a Microsoft Kinect mounted on top of the PR2 robot, while performing head rotations and movements towards and away from the fixed sources. In this generic HRI scenario, the resulting automatic speech recognition engine provided a word error rate that is at least 26% and 38% lower than publicly available speech recognition APIs with the playback (i.e. loudspeaker) and human testing databases, respectively, with a limited amount of training data.	acoustic cryptanalysis;artificial neural network;black box;database;deep learning;hidden markov model;human–robot interaction;kinect;loudspeaker;markov chain;mobile manipulator;robot;speech recognition;testbed;word error rate	José Novoa;Jorge Wuth;Juan Pablo Escudero;Josué Fredes;Rodrigo Mahu;Néstor Becerra Yoma	2018		10.1145/3171221.3171280	loudspeaker;computer science;word error rate;hidden markov model;speech recognition;markov model;testbed;training set	NLP	-16.499292562543594	-89.25545682301102	177784
fdf7b1d01d8aebff8cd7b7591bf01498b9ff158c	the hybrid brain computer music interface - integrating brainwave detection methods for extended control in musical performance systems	gesture;motion capture;body motion;arts and architecture;movement	Body movement and embodied knowledge play an important#R##N#part in how we express and understand music. The gestures of a#R##N#musician playing an instrument are part of a shared knowledge that contributes to musical expressivity by building expectations and influencing perception. In this study, we investigate the extent in which the movement vocabulary of violin performance is part of the embodied knowledge of individuals with no experience in playing the instrument. We#R##N#asked people who cannot play the violin to mime a performance along#R##N#an audio excerpt recorded by an expert. They do so by using a silent#R##N#violin, specifically modified to be more accessible to neophytes. Preliminary motion data analyses suggest that, despite the individuality of each performance, there is a certain consistency among participants in terms of overall rhythmic resonance with the music and movement in response to melodic phrasing. Individualities and commonalities are then analysed using Functional Principal Component Analysis.	neural oscillation	Joel Eaton;Eduardo Reck Miranda	2015		10.1007/978-3-319-46282-0_8	movement;motion capture;speech recognition;acoustics;computer science;artificial intelligence;linguistics;multimedia;gesture	Robotics	-7.912570033973066	-81.540896861539	177883
a5f7802d86e0dd1aadfc6667f23eb3e9eb226ef6	prosody conversion for emotional mandarin speech synthesis using the tone nucleus model		In this paper, tone nucleus model is employed to represent and convert F0 contour for synthesizing an emotional Mandarin speech from a neutral speech. Compared with previous prosody transforming methods, the proposed method 1) only converts the tone nucleus part of each syllable rather than the whole F0 contour to avoid the data sparseness problems; 2) builds mapping functions for well-chosen tone nucleus model parameters to better capture Mandarin tonal information. Using only a modest amount of training data, the perceptual accuracy achieved by our method was shown to be comparable to that obtained by a professional speaker.	neural coding;semantic prosody;speech synthesis;super robot monkey team hyperforce go!;syllable	Miaomiao Wen;Miaomiao Wang;Keikichi Hirose	2011			speech recognition;acoustics	NLP	-17.497720750350855	-84.52508875360826	177921
3d3922c7af91857013dd0ac77512c778297db21f	discriminative disfluency modeling for spontaneous speech recognition			discriminative model;speech recognition;spontaneous order	Chung-Hsien Wu;Gwo-Lang Yan	2001			speech recognition;discriminative model;artificial intelligence;pattern recognition;computer science	NLP	-14.845344486307528	-87.32840789715954	177939
77ecec63d7fc0bac3b38c032c597f6a503af8c48	speech animation using coupled hidden markov models	speech processing;explicit modelling audio speech animation coupled hidden markov model audio visual speech intermodal synchronization visual speech visual animation parameter;audio speech animation;hidden markov models facial animation speech processing speech synthesis viterbi algorithm augmented virtuality visual databases face multimedia databases application software;visual speech;hidden markov models;intermodal synchronization;speech processing computer animation hidden markov models;audio visual;audio visual speech;coupled hidden markov model;visual animation parameter;explicit modelling;computer animation	We present a novel speech animation approach using coupled hidden Markov models (CHMMs). Different from the conventional HMMs that use a single state chain to model the audio-visual speech with tight inter-modal synchronization, we use the CHMMs to model the asynchrony, different discriminative abilities, and temporal coupling between the audio speech and the visual speech, which are important factors for animations looking natural. Based on the audio-visual CHMMs, visual animation parameters are predicted from audio through an EM-based audio to visual conversion algorithm. Experiments on the JEWEL AV database show that compared with the conventional HMMs, the CHMMs can output visual parameters that are much closer to the actual ones. Explicit modelling of audio-visual speech is promising in speech animation	algorithm;asynchronous i/o;hidden markov model;markov chain;modal logic;speech synthesis	Lei Xie;Zhi-Qiang Liu	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.1074	natural language processing;audio mining;speech recognition;computer science;speech coding;speech processing;acoustic model;computer animation	Vision	-14.903946692696357	-83.69790214301902	177959
1f24854db4adca763fb292060441a8844ca3b5dc	first demonstration of a musical emotion bci	bci;emotion;ica;music;affective computing	Development of EEG-based brain computer interface (BCI) methods has largely focused on creating a communication channel for subjects with intact cognition but profound loss of motor control from stroke or neurodegenerative disease that allows such subjects to communicate by spelling out words on a personal computer. However, other important human communication channels may also be limited or unavailable for handicapped subjects -direct non-linguistic emotional communication by gesture, vocal prosody, facial expression, etc.. We report and examine a first demonstration of a musical ‘emotion BCI’ in which, as one element of a live musical performance, an able-bodied subject successfully engaged the electronic delivery of an ordered sequence of five music two-tone bass frequency drone sounds by imaginatively re-experiencing the human feeling he had spontaneously associated with the sound of each drone sound during training sessions. The EEG data included activities of both brain and non-brain sources (scalp muscles, eye movements). Common Spatial Pattern classification gave 84% correct pseudo-online performance and 5-of-5 correct classification in live performance. Re-analysis of the training session data including only the brain EEG sources found by multiple-mixture Amica ICA decomposition achieved five-class classification accuracy of 59-70%, confirming that different voluntary emotion imagination experiences may be associated with distinguishable brain source EEG dynamics.	beneath a steel sky;brain–computer interface;channel (communications);cognition;common spatial pattern;electroencephalography;emotion markup language;hoc (programming language);independent computing architecture;personal computer;selective calling;semantic prosody;spatiotemporal pattern	Scott Makeig;Grace Leslie;Tim R. Mullen;Devpratim Sarma;Nima Bigdely Shamlo;Christian Kothe	2011		10.1007/978-3-642-24571-8_61	psychology;neuroscience;speech recognition;emotion;artificial intelligence;music;affective computing;communication	ML	-7.122228678809021	-83.41314380959763	178003
ce3214e0e4005dea6a677e22ea9df19d695751c2	query by humming by using locality sensitive hashing based on combination of pitch and note	locality sensitive hashing;recursive alignment;query by humming;information retrieval;vectors feature extraction indexing music information retrieval robustness;music information retrieval;mean reciprocal rank pitch query by humming content based music information retrieval humming errors note based locality sensitive hashing index construction recursive alignment midi files mirex qbh query corpus;music;music information retrieval query by humming locality sensitive hashing recursive alignment	Query by humming (QBH) is a technique that is used for content-based music information retrieval. It is a challenging unsolved problem due to humming errors. In this paper a novel retrieval method called note-based locality sensitive hashing (NLSH) is presented and it is combined with pitch-based locality sensitive hashing (PLSH) to screen candidate fragments. The method extracts PLSH and NLSH vectors from the database to construct two indexes. In the phase of retrieval, it automatically extracts vectors similar to the index construction and searches the indexes to obtain a list of candidates. Then recursive alignment (RA) is executed on these surviving candidates. Experiments are conducted on a database of 5,000 MIDI files with the 2010 MIREX-QBH query corpus. The results show by using the combination approach the relatively improvements of mean reciprocal rank are 29.7% (humming from anywhere) and 23.8% (humming from beginning), respectively, compared with the current state-of-the-art method.	algorithm;database;information retrieval;locality of reference;locality-sensitive hashing;midi;query by humming;recursion;traffic collision avoidance system	Qiang Wang;Zhiyuan Guo;Gang Liu;Jun Guo;Yueming Lu	2012	2012 IEEE International Conference on Multimedia and Expo Workshops	10.1109/ICMEW.2012.58	speech recognition;computer science;machine learning;pattern recognition;music;information retrieval	DB	-7.669421991494205	-93.93273388657654	178090
de22d32f4a316447967fb71992c06d9585457d6c	effect of aging on speech features and phoneme recognition: a study on bengali voicing vowels	phoneme similarity measure;aging voice;speech recognition;speech characteristics	The article studies age related variations of speech characteristics of two age groups, in the Bengali language. The study considers 60 speakers in the each age groups, 60– 80 years and 20–40 years, respectively. We have considered different voice source features like fundamental frequency, formant frequencies, jitter, shimmer and harmonic to noise ratio. Cepstral domain feature, Mel Frequency Cepstral coefficients (MFCC) of different voiced Bengali vowels are also analyzed for younger and older adult groups. MFCC feature and Hidden Markov model parameter of different voiced vowels are used to study phoneme dissimilarities measure between two age groups. Age related changes in elderly speech affect the automatic speech recognition performance as was observed in our study, raising the need for specific acoustic models for elderly persons.	acoustic cryptanalysis;acoustic model;automated system recovery;bengali input methods;coefficient;database normalization;degree (graph theory);hidden markov model;language model;markov chain;mel-frequency cepstrum;speech recognition;tract (literature);triphone	Biswajit Das;Sandipan Mandal;Pabitra Mitra;Anupam Basu	2013	I. J. Speech Technology	10.1007/s10772-012-9147-3	speech recognition;computer science	NLP	-11.726779708148396	-85.43362119519789	178219
114eab7dab7f4edd29a05e088fe94a8df6280550	a methodology for recognition of emotions based on speech analysis, for applications to human-robot interaction. an exploratory study			exploratory testing;human–robot interaction;voice analysis	Mohammad Rabiei;Alessandro Gasparetto	2014	Paladyn	10.2478/pjbr-2014-0001	natural language processing;speech recognition	HCI	-13.614664088365334	-85.1706664439061	178297
2bae32ea8ed667b3264e10438458149be1722556	controlling perceived degradation in spectrum envelope modeling via predistortion	perceived quality;distance measure;spectrum;model error;compact representation	The compact representation of the discrete amplitude spectrum of voiced speech by an all-pole model of the spectral envelope is considered. Based on the properties of the all-pole modeling error, the use of spectrum predistortion for improving the perceptual fit at low model orders is motivated. Warping of the frequency scale before modeling of the spectral envelope of narrowband voiced sounds is investigated by subjective listening and objective measures. It is found that, contrary to what is generally accepted, the improvement in perceived quality brought about by frequency warping actually depends to a large extent on the underlying signal spectrum distribution. An objective distance measure based on partial noise loudness is found to show high correlation with subjective judgements of degradation, indicating that auditory frequency masking plays an important role in determining the perceptual accuracy of the spectrum envelope model.	bilinear transform;elegant degradation;image warping;spectral density;spectrum analyzer	Pushkar Patwardhan;Preeti Rao	2002			spectrum;speech recognition;errors-in-variables models	ML	-9.465889640117288	-87.67984093680928	178406
472847ed1d6e715196040e7c10c19fcf8d5b5458	speaker normalization via springy discriminant analysis and pitch estimation	vocal tract length normalization;maximum likelihood;discriminant analysis;feature vector;speaker independent;speech recognition	Speaker normalization techniques are widely used to improve the accuracy of speaker independent speech recognition. One of the most popular group of such methods is Vocal Tract Length Normalization (VTLN). These methods try to reduce the inter-speaker variability by transforming the input feature vectors into a more compact domain, to achieve better separations between the phonetic classes. Among others, two algorithms are commonly applied: the Maximum Likelihood criterion-based, and the Linear Discriminant criterion-based normalization algorithms. Here we propose the use of the Springy Discriminant criterion for the normalization task. In addition we propose a method for the VTLN parameter determination that is based on pitch estimation. In the experiments this proves to be an efficient and swift way to initialize the normalization parameters for training, and to estimate them for the voice samples of new test speakers.		Dénes Paczolay;András Bánhalmi;András Kocsor	2007		10.1007/978-3-540-74628-7_33	speech recognition;feature vector;computer science;machine learning;pattern recognition;maximum likelihood;linear discriminant analysis	Vision	-16.274495083652443	-91.83254788473235	178448
7f43003c983168458812f7daea7a1980e13f993d	quality and intelligibility improvements in a greek text-to-speech system			intelligibility (philosophy);speech synthesis	Nickolas Yiourgalis;George K. Kokkinakis	1990			speech recognition;speech synthesis;computer science;intelligibility (communication)	Crypto	-14.990984981851918	-85.42787551826778	178689
5228604386fba84a416a2eeae3756ea63b9cb9ac	boosted binary audio fingerprint based on spectral subband moments	degradation;information loss;pairwise independence;spectral subband moments;indexing terms;audio fingerprint;feature vector;pairwise independence boosted binary audio fingerprinting system spectral subband moments information loss performance degradation binary conversion method;audio coding;boosting;fingerprint recognition boosting degradation robustness performance loss feature extraction indexing spatial databases audio databases large scale systems;indexing;fingerprint recognition;feature extraction;spatial databases;boosted binary audio fingerprinting system;robustness;feature extraction audio coding;audio databases;spectral subband moment;performance degradation;performance loss;boosting audio fingerprint spectral subband moment;binary conversion method;large scale systems	An audio fingerprinting system identifies an audio based on a unique feature vector called the audio fingerprint. The performance of an audio fingerprinting system is directly related to the fingerprint that the system uses. To reduce both the DB size and the DB search time, binary fingerprints are often used. However converting a real-valued fingerprint into a binary fingerprint results in loss of information and leads to severe degradation in performance. In this paper, an algorithm known as boosting is used as a binary conversion method which minimizes the degradation. The experimental results showed that the proposed binary audio fingerprint obtained by boosting the spectral subband moments outperformed some of the state-of-the-art binary audio fingerprints in the context of both robustness and pair-wise independence (reliability).	acoustic fingerprint;algorithm;boosting (machine learning);elegant degradation;feature vector;fingerprint (computing)	Sungwoong Kim;Chang Dong Yoo	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366661	search engine indexing;speech recognition;pairwise independence;degradation;index term;feature vector;feature extraction;computer science;machine learning;pattern recognition;fingerprint recognition;boosting;robustness	Mobile	-15.379951401305204	-93.34145349344102	178710
ca506cc8e86d6c425b6c17e995ced6b1a8392417	speech analysis and synthesis methods based on spectral envelopes and voiced/unvoiced functions				Xavier Rodet;Philippe Depalle;Gilles Poirot	1987			speech recognition;mathematics	Logic	-13.324089090557107	-86.83021331627623	178769
fb7f0cfe9e2e2a498433863ad417206f6c79cdb1	spoofing detection in automatic speaker verification systems using dnn classifiers and dynamic acoustic features		With the development of speech synthesis technology, automatic speaker verification (ASV) systems have encountered the serious challenge of spoofing attacks. In order to improve the security of ASV systems, many antispoofing countermeasures have been developed. In the front-end domain, much research has been conducted on finding effective features which can distinguish spoofed speech from genuine speech and the published results show that dynamic acoustic features work more effectively than static ones. In the back-end domain, Gaussian mixture model (GMM) and deep neural networks (DNNs) are the two most popular types of classifiers used for spoofing detection. The log-likelihood ratios (LLRs) generated by the difference of human and spoofing log-likelihoods are used as spoofing detection scores. In this paper, we train a five-layer DNN spoofing detection classifier using dynamic acoustic features and propose a novel, simple scoring method only using human log-likelihoods (HLLs) for spoofing detection. We mathematically prove that the new HLL scoring method is more suitable for the spoofing detection task than the classical LLR scoring method, especially when the spoofing speech is very similar to the human speech. We extensively investigate the performance of five different dynamic filter bank-based cepstral features and constant Q cepstral coefficients (CQCC) in conjunction with the DNN-HLL method. The experimental results show that, compared to the GMM-LLR method, the DNN-HLL method is able to significantly improve the spoofing detection accuracy. Compared with the CQCC-based GMM-LLR baseline, the proposed DNN-HLL model reduces the average equal error rate of all attack types to 0.045%, thus exceeding the performance of previously published approaches for the ASVspoof 2015 Challenge task. Fusing the CQCC-based DNN-HLL spoofing detection system with ASV systems, the false acceptance rate on spoofing attacks can be reduced significantly.	acoustic cryptanalysis;anatomic node;anomaly detection;artificial neural network;baseline dental cement;baseline (configuration management);cepstrum;coefficient;deep learning;filter bank;google map maker;high-level programming language;lucas–lehmer–riesel test;mixture model;neural network simulation;normal statistical distribution;replay attack;scientific publication;score;speaker recognition;speech recognition;speech synthesis;spoofing attack;verification of theories;likelihood ratio	Hong Yu;Zheng-Hua Tan;Zhanyu Ma;Rainer Martin;Jun Guo	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2017.2771947	artificial intelligence;artificial neural network;word error rate;feature extraction;spoofing attack;pattern recognition;computer science;cepstrum;attack model;mel-frequency cepstrum;speech synthesis	SE	-11.053471839963288	-92.01220515399328	178837
28f0f97e0ef9223994a38fb40bac805ea4b08c17	predicting the quality of processed speech by combining modulation-based features and model trees.				Benjamin Cauchi;João Felipe Santos;Kai Siedenburg;Tiago H. Falk;Patrick A. Naylor;Simon Doclo;Stefan Goetze	2016			speech recognition;machine learning;pattern recognition	NLP	-14.76009620852455	-87.29201656011293	178903
5c6d7acdc59cd8ce1c20093bc03ecd8364f09de7	speaker verification using sparse representations on total variability i-vectors	sparse representation i-vector modeling;speaker verification	In this paper, the sparse representation computed by lminimization with quadratic constraints is employed to model the i-vectors in the low dimensional total variability space after performing the Within-Class Covariance Normalization and Linear Discriminate Analysis channel compensation. First, we propose the background normalized l residual as a scoring criterion. Second, we demonstrate that the Tnorm can be efficiently achieved by using the Tnorm data as the non-target samples in the over-complete dictionary. Finally, by fusing with the conventional i-vector based support vector machine (SVM) and cosine distance scoring system, we demonstrate overall system performance improvement. Experimental results show that the proposed fusion system achieved 4.05% (male) and 5.25% (female) equal error rate (EER) after Tnorm on the single-single multi-language handheld telephone task of NIST SRE 2008 and outperformed the SVM baseline by yielding 7.1% and 4.9% relative EER reduction for the male and female tasks, respectively.	baseline (configuration management);cosine similarity;dictionary;enhanced entity–relationship model;handheld game console;heart rate variability;language identification;linear discriminant analysis;nist rbac model;sparse approximation;sparse matrix;spatial variability;speaker recognition;support vector machine;t-norm	Ming Li;Xiang Zhang;Yonghong Yan;Shrikanth (Shri) Narayanan	2011			natural language processing;speech recognition;machine learning	AI	-15.049995555236597	-91.49836236885716	178932
32750b75ef617c8e83647236c714a6f0107b9118	cosmo, a bayesian computational model of speech communication: assessing the role of sensory vs. motor knowledge in speech perception	robot sensing systems;speech computational modeling robot sensing systems narrowband wideband speech recognition speech processing;wideband;speech processing;speech;computational modeling;wideband interpretation bayesian computational model speech communication sensory knowledge motor knowledge speech perception functional relationship speech production systems human brain cosmo model speech categorization narrowband interpretation;speech recognition;narrowband;speech processing bayes methods brain hearing	It is now widely accepted that there is a functional relationship between the speech perception and production systems in the human brain. However, the precise mechanisms and role of this relationship still remain debated.	computational model	Marie-Lou Barnaud;Julien Diard;Pierre Bessière;Jean-Luc Schwartz	2015	2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)	10.1109/DEVLRN.2015.7346149	speech technology;trace;speech recognition;acoustics;computer science;neurocomputational speech processing;speech processing;acoustic model;communication	Robotics	-5.2943783283685395	-82.00824121906659	179042
6fc996d0c9ef53e3a64aca97233661d2e7b77ee0	rule-based female speech synthesis - segmental level improvements			speech synthesis	Inger Karlsson;Lennart Neovius	1994			rule-based system;speech recognition;computer science;speech synthesis	NLP	-14.944384536704566	-85.70726053935836	179066
6c414896b5f7f19371185b366f1106d2a4167423	german boundary tones show categorical perception and a perceptual magnet effect when presented in different contexts	context information;indexing terms;categorical perception	The experiment presented in this paper examines categorical perception as well as the perceptual magnet effect in German boundary tones, taking also context information into account. The test phrase is preceded by different context sentences that are assumed to affect the location of the category boundary in the stimulus continuum between the low and the high boundary tone. Results provide evidence for the existence of a low and a high boundary tone in German, corresponding to statement versus question interpretation, respectively. Furthermore, in contrast to previous findings, a prototype was found not only in the category of the low but also in the category of the high boundary tone, supporting the hypothesis that context might have been taken into account to solve a possible ambiguity between H% and a previously hypothesized non-low and non-terminal boundary tone.	prototype;triune continuum paradigm	Katrin Schneider;Grzegorz Dogil;Bernd Möbius	2009			natural language processing;speech recognition;index term;computer science;categorical perception	HCI	-10.172760049691991	-80.32536054873962	179090
839910712c4cc4aefa58907077faed0ffdf004c9	a computational study of the role of tonal tension in expressive piano performance		Expressive variations of tempo and dynamics are an important aspect of music performances, involving a variety of underlying factors. Previous work has showed a relation between such expressive variations (in particular expressive tempo) and perceptual characteristics derived from the musical score, such as musical expectations, and perceived tension. In this work we use a computational approach to study the role of three measures of tonal tension proposed by Herremans and Chew (2016) in the prediction of expressive performances of classical piano music. These features capture tonal relationships of the music represented in Chew’s spiral array model, a three dimensional representation of pitch classes, chords and keys constructed in such a way that spatial proximity represents close tonal relationships. We use non-linear sequential models (recurrent neural networks) to assess the contribution of these features to the prediction of expressive dynamics and expressive tempo using a dataset of Mozart piano sonatas performed by a professional concert pianist. Experiments of models trained with and without tonal tension features show that tonal tension helps predict change of tempo and dynamics more than absolute tempo and dynamics values. Furthermore, the improvement is stronger for dynamics than for tempo.	artificial neural network;computation;experiment;noise shaping;nonlinear system;performance;recurrent neural network	Carlos Cancino Chacón;Maarten Grachten	2018	CoRR		speech recognition;computer science;pitch class;artificial intelligence;machine learning;piano;perception;spiral array model;recurrent neural network;chord (music);piano sonata	ML	-8.021473449680087	-82.3736098232767	179170
c639e39275882d3039d8112c44a7a8636941f809	verifying session level pronunciation accuracy in a speech therapy application		This paper investigates the problem of verifying the pronunciations of phonemes from continuous utterances collected from impaired children speakers engaged in a speech therapy session. A new pronunciation verification (PV) approach based on the subspace Gaussian mixture model (SGMM) is presented. A single SGMM is trained from test utterances collected from impaired and unimpaired speakers. PV measures are derived from model space based distances estimated from impaired and unimpaired speaker dependent SGMM parameters. The PV performance is presented in terms of the method’s ability to detect and report phone level pronunciation errors for an entire speech therapy session and for individual phoneme occurrences. Comparisons are made with respect to an approach using phone level confidence measures estimated from lattice posterior probabilities.	concatenation;dimensionality reduction;enhanced entity–relationship model;subspace gaussian mixture model	Shou-Chun Yin;Richard C. Rose;Yun Tang	2012			speech recognition;natural language processing;pronunciation;artificial intelligence;computer science	ML	-18.941680755535916	-89.68510974471843	179285
a7af16fb9d7ed261008f4b8ff0be096fc41c18ee	robust speech enhancement techniques for asr in non-stationary noise and dynamic environments		In the current ASR systems the presence of competing speakers greatly degrades the recognition performance. This phenomenon is getting even more prominent in the case of hands-free, far-field ASR systems like the “Smart-TV” systems, where reverberation and non-stationary noise pose additional challenges. Furthermore, speakers are, most often, not standing still while speaking. To address these issues, we propose a cascaded system that includes Time Differences of Arrival estimation, multi-channel Wiener Filtering, non-negative matrix factorization (NMF), multicondition training, and robust feature extraction, whereas each of them additively improves the overall performance. The final cascaded system presents an average of 50% and 45% relative improvement in ASR word accuracy for the CHiME 2011(non-stationary noise) and CHiME 2012 (nonstationary noise plus speaker head movement) tasks, respectively.	acoustic cryptanalysis;acoustic model;automated system recovery;automatic system recovery;comefrom;experiment;feature extraction;language identification;non-negative matrix factorization;real life;signal processing;smart tv;speaker recognition;speech enhancement;speech recognition;stationary process	Gang Liu;Dimitrios Dimitriadis;Enrico Bocchieri	2013			speech recognition	AI	-14.481233240892482	-89.73033558274828	179388
2d982e48adaaca19403d774f913f97837e452fec	comparative evaluation of casa and bss models for subband cocktail-party speech separation		For speech segregation, a recurrent blind separation model (BSS) is tested together with a CASA model, which is based on the localisation cue and the evaluation of the time delay of arrival (TDOA). The test database is composed of 332 binary mixture sentences recorded in stereo with a static set-up. These are truncated at 1 second for the simulations. For applying the two models, we cut the frequency domain in a variable number of subbands, which are processed independently. Then, we evaluate the gain, using reference signals recorded in isolation. Without using this reference, a coherence index is also established for the BSS model, which measures the degree of convergence. After a careful analysis, we find gains of about 23dB for the two methods, which are smaller than those published for the same task. The variation of the number of subbands allows an optimisation, and we obtain a significant peak at 4 subbands for the CASA model, as well as a maximum at 2 subbands for the BSS model.	blind signal separation;broadcast delay;computational auditory scene analysis;mathematical optimization;multilateration;simulation	Frédéric Berthommier;Seungjin Choi	2002			speech recognition;artificial intelligence;pattern recognition;computer science	ML	-10.805785355609078	-92.95099983860557	179516
3aee3595748353a1089894fe884d544f27dd4e71	prosodic modeling for improved speech recognition and understanding	mandarin chinese;electrical engineering and computer science;thesis;speech recognition	The general goal of this thesis is to model the prosodic aspects of speech to improve humancomputer dialogue systems. Towards this goal, we investigate a variety of ways of utilizing prosodic information to enhance speech recognition and understanding performance, and address some issues and difficulties in modeling speech prosody during this process. We explore prosodic modeling in two languages, Mandarin Chinese and English, which have very different prosodic characteristics. Chinese is a tonal language, in which intonation is highly constrained by syllable F0 patterns determined by lexical tones. Hence, our strategy is to focus on tone modeling and account for intonational aspects within the context of improving tone models. On the other hand, the acoustic expression of lexical stress in English is obscure and highly influenced by intonation. Thus, we examine the applicability of modeling lexical stress for improved speech recognition, and explore prosodic modeling beyond the lexical level as well. We first developed a novel continuous pitch detection algorithm (CPDA), which was designed explicitly to promote robustness for telephone speech and prosodic modeling. The algorithm achieved similar performance for studio and telephone speech (4.25% vs. 4.34% in gross error rate). It also has superior performance for both voiced pitch accuracy and Mandarin tone classification accuracy compared with an optimized algorithm in xwaves. Next, we turned our attention to modeling lexical tones for Mandarin Chinese. We performed empirical studies of Mandarin tone and intonation, focusing on analyzing sources of tonal variations. We demonstrated that tone classification performance can be significantly improved by taking into account F0 declination, phrase boundary, and tone context influences. We explored various ways to incorporate tone model constraints into the summit speech recognition system. Integration of a simple four-tone model into the first-pass Viterbi search reduced the syllable error rate by 30.2% for a Mandarin digit recognition task, and by 15.9% on the spontaneous utterances in the yinhe domain. However, further improvements by using more refined tone models were not statistically significant. Leveraging the same mechanisms developed for Mandarin tone modeling, we incorporated lexical stress models into spontaneous speech recognition in the jupiter weather domain, and achieved a 5.5% reduction in word error rate compared to a state-of-the-art baseline performance. However, our recognition results obtained with a one-class (including all vowels) prosodic model seemed to suggest that the gain was mainly due to the elimination of implausible hypotheses, e.g., preventing vowel/non-vowel or vowel/non-phone confusions, rather than by distinguishing the fine differences among different stress and vowel classes.	acoustic cryptanalysis;baseline (configuration management);consistency model;dialog system;pitch detection algorithm;semantic prosody;speech recognition;spontaneous order;statistical classification;super robot monkey team hyperforce go!;syllable;viterbi algorithm;word error rate	Chao Wang	2001			natural language processing;speech technology;speech recognition;computer science;linguistics	NLP	-18.306825124870144	-85.64285426605979	179580
4a62c159b8a7f50a3902d50ff36c0d70439a071b	investigations on error minimizing training criteria for discriminative training in automatic speech recognition	maximum mutual information;maximum likelihood;acoustic modeling;automatic speech recognition;large scale;speech recognition;discriminative training	Discriminative training criteria have been shown to consistently outperform maximum likelihood trained speech recognition systems. In this paper we employ the Minimum Classification Error (MCE) criterion to optimize the parameters of the acoustic model of a large scale speech recognition system. The statistics for both the correct and the competing model are solely collected on word lattices without the use of N -best lists. Thus, particularly for long utterances, the number of sentence alternatives taken into account is significantly larger compared to N -best lists. The MCE criterion is embedded in an extended unifying approach for a class of discriminative training criteria which allows for direct comparison of the performance gain obtained with the improvements of other commonly used criteria such asMaximum Mutual Information(MMI) and Minimum Word Error (MWE). Experiments conducted on large vocabulary tasks show a consistent performance gain for MCE over MMI. Moreover, the improvements obtained with MCE turn out to be in the same order of magnitude as the performance gains obtained with the MWE criterion.	acoustic cryptanalysis;acoustic model;akaike information criterion;discriminative model;embedded system;experiment;linked list;linuxmce;minimal working example;mutual information;speech recognition;vocabulary	Wolfgang Macherey;Lars Haferkamp;Ralf Schlüter;Hermann Ney	2005			speech recognition;computer science;machine learning;pattern recognition;maximum likelihood	NLP	-18.14938046009049	-91.23342292789525	179606
95a774d5481c5109bd6df5e68bbbc67e57c2694e	comprehensive assessment of the telephone intelligibility of synthesized and natural speech	intelligibility;synthesis;assessment	Abstract   This paper describes a monosyllabic corpus for use in testing the consonant intelligibility of synthesized speech. It differs from those used in other tests in that it spans a wide variety of English sounds and is thus useful for diagnosis as well as for comparative assessment. Some “standard” tests of intelligibility use restricted phonetic material, which is possibly easier to understand than a representative sample of English; thus, the results from those tests may not reflect the intelligibility of a wider sample of speech. For illustration, we present the results of a telephone comparison between a demisyllable synthesizer currently being developed at Bellcore (“ Orator ”), a commercially available phoneme-based synthesizer, and natural speech obtained from 2 talkers. The natural speech data can be used by other laboratories wishing to compare the consonant intelligibility of other synthesis systems to natural speech.	intelligibility (philosophy);natural language	Murray F. Spiegel;Mary Jo Altom;Marian J. Macchi;Karen L. Wallace	1990	Speech Communication	10.1016/0167-6393(90)90004-S	speech recognition;speech corpus;intelligibility	NLP	-15.352428504130463	-83.95636739873952	179708
0cfc6448885addde34c704897157134df7c7b98d	lip-reading from parametric lip contours for audio- visual speech recognition		This paper describes the incorporation of a visual lip tracking and lip-reading algorithm that utilizes the affine-invariant Fourier descriptors from parametric lip contours to improve the audio-visual speech recognition systems. The audio-visual speech recognition system presented here uses parallel hidden Markov models (HMMs), where a joint decision, using an optimal decision rule, is made after processing. This work describes the extraction of affine-invariant Fourier descriptors (AI-FDs) from parametric lip contour data. Finally, this work validates the use of optimal weight selection, which is based on the noise type and signal-to-noise ratio (SNR) for joint audio-visual automatic speech recognition (JAV-ASR).	algorithm;audio-visual speech recognition;hidden markov model;markov chain;signal-to-noise ratio	Sabri Gurbuz;Eric K. Patterson;Zekeriya Tufekci;John N. Gowdy	2001				Vision	-15.312974070541134	-93.1427987187257	179728
dc00dfdfb848c2af46d33f6da8b838e482c34057	comparison of speech quality with and without sensors in electromagnetic articulograph ag 501 recording		In the recordings using electromagnetic articulograph AG 501, sensors are glued to subject’s articulators such as jaw, lips and tongue and both speech and articulatory movements are simultaneously recorded. In this work, we study the effect of the presence of the sensors on the quality of speech spoken by the subject. This is done by recording when a subject speaks a set of 19 VCV stimuli while sensors are attached to subject’s articulators. For comparison we also record the same set of stimuli spoken by the same subject but with no sensors attached to subject’s articulators. Both subjective and objective comparisons are made on the recorded stimuli in these two settings. Subjective evaluation is carried out using 16 evaluators. Listening experiments with recordings from five subjects show that the recordings with sensors attached are significantly different from those without sensors attached in terms of human recognition score as well as on a perceptual difference measure. This is also supported in the objective comparison which computes dissimilarity measure using the spectral shape information.	experiment;sensor	Nisha Meenakshi;Chiranjeevi Yarra;B. K. Yamini;Prasanta Kumar Ghosh	2014			speech recognition;computer science	HCI	-9.243324821940462	-83.75447121897083	179817
b7c111f15f76f546d7dcb02b98e1ba2d8fd352ed	a swedish cookie-theft corpus		Language disturbances can be a diagnostic marker for neurodegenerative diseases, such as Alzheimer’s disease, at earlier stages. Connected speech analysis provides a non-invasive and easy-to-assess measure for determining aspects of the severity of language impairment. In this paper we focus on the development of a new corpus consisting of audio recordings of picture descriptions (including transcriptions) of the Cookie-theft, produced by Swedish speakers. The speech elicitation procedure provides an established method of obtaining highly constrained samples of connected speech that can allow us to study the intricate interactions between various linguistic levels and cognition. We chose the Cookie-theft picture since it’s a standardized test that has been used in various studies in the past, and therefore comparisons can be made based on previous research. This type of picture description task might be useful for detecting subtle language deficits in patients with subjective and mild cognitive impairment. The resulting corpus is a new, rich and multi-faceted resource for the investigation of linguistic characteristics of connected speech and a unique dataset that provides a rich resource for (future) research and experimentation in many areas, and of language impairment in particular. The information in the corpus can also be combined and correlated with other collected data about the speakers, such as neuropsychological tests, brain physiology and cerebrospinal fluid markers as well as imaging.	cognition;faceted classification;http cookie;interaction;sensor;speech synthesis;text corpus;voice analysis	Dimitrios Kokkinakis;Kristina Lundholm Fors;Kathleen C. Fraser;Arto Nordlund	2018			speech recognition;natural language processing;artificial intelligence;computer science	NLP	-6.603754013741019	-85.58997263157217	180048
d82bf4ff133d5d991de7df4e7b32440d9903cdb5	computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech	automated speech;large set;unseen test set;automated scoring;best scoring model;annotated clause;syntactic complexity feature;spontaneous non-native speech;automatic scoring model;human rating;automated classifier;human rater score;human transcription	This paper focuses on identifying, extracting and evaluating features related to syntactic complexity of spontaneous spoken responses as part of an effort to expand the current feature set of an automated speech scoring system in order to cover additional aspects considered important in the construct of communicative competence. Our goal is to find effective features, selected from a large set of features proposed previously and some new features designed in analogous ways from a syntactic complexity perspective that correlate well with human ratings of the same spoken responses, and to build automatic scoring models based on the most promising features by using machine learning methods. On human transcriptions with manually annotated clause and sentence boundaries, our best scoring model achieves an overall Pearson correlation with human rater scores of r=0.49 on an unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2.	automated system recovery;entity–relationship model;experiment;feature extraction;finite-state machine;higgins;lu decomposition;machine learning;parse tree;parsing;preprocessor;sensor;speech recognition;spontaneous order;test set	Miao Chen;Klaus Zechner	2011			natural language processing;speech recognition;computer science;machine learning;pattern recognition	NLP	-18.578438533178872	-82.19737208349636	180143
be3628999c0d07266b50fcf5f2932e193d484096	musical instrument timbres classification with spectral features	multimedia content description;hierarchical structure;spectral centroid;audio signal processing;instruments;timbre;support vector machines;inharmonicity;spectral characteristics;instruments multimedia databases timbre signal processing data mining power harmonic filters frequency estimation testing error analysis support vector machines;audio signal processing feature extraction musical instruments signal classification spectral analysis learning automata acoustic signal processing;frequency estimation;acoustic signal processing;testing;learning automata;data mining;musical instruments;error analysis;musical instrument recognition;canonical discriminant analysis;compact representation;power harmonic filters;feature extraction;signal processing;feature extraction musical instrument timbres classification spectral features musical instrument recognition monophonic musical signals spectral characteristics quadratic discriminant analysis canonical discriminant analysis support vector machines nearest neighbours inharmonicity spectral centroid multimedia content description sound databases;nearest neighbours;signal classification;multimedia databases;musical instrument timbres classification;error rate;nearest neighbour;sound databases;monophonic musical signals;support vector machine;spectral analysis;spectral features;quadratic discriminant analysis	A set of features is evaluated for recognition of musical instruments out of monophonic musical signals. Aiming to achieve a compact representation, the adopted features regard only spectral characteristics of sound and are limited in number. On top of these descriptors, various classification methods are implemented and tested. Over a dataset of 1007 tones from 27 musical instruments, support vector machines and quadratic discriminant analysis show comparable results with success rates close to 70% of successful classifications. Canonical discriminant analysis never had momentous results, while nearest neighbours performed on average among the employed classifiers. Strings have been the most misclassified instrument family, while very satisfactory results have been obtained with brass and woodwinds. The most relevant features are demonstrated to be the inharmonicity, the spectral centroid, and the energy contained in the first partial.	linear discriminant analysis;quadratic classifier;spectral centroid;support vector machine	Giulio Agostini;Maurizio Longari;Emanuele Pollastri	2001		10.1109/MMSP.2001.962718	support vector machine;speech recognition;computer science;machine learning;signal processing;pattern recognition	ML	-8.138408203015313	-92.00932973024639	180195
b012fce867909b8b129f2d4ba4bf1fdfc84930c9	gender-specific articulatory-acoustic relations in vowel sequences	vocal tract;university of wisconsin;speech production;x rays	Abstract   Differences in male and female vocal tract dimensions are hypothesized to have a number of dynamic consequences—differences in target attainment, articulatory speed, and acoustic vowel space dimension. Evidence for some of these predictions is sought by investigating articulatory and acoustic patterns in interword vowel sequences in the  University of Wisconsin X-ray Microbeam Speech Production Database  ( UW-XRMBDB ). Means of formant and lingual pellet tracks throughout such vocalic stretches exhibit similarities in acoustic and articulatory form for male and female groups, but show significant gender-specific differences in both articulatory and acoustic space traversed, with females making greater acoustic excursions for shorter articulatory distances.	acoustic cryptanalysis	Adrian P. Simpson	2002	J. Phonetics	10.1006/jpho.2002.0171	psychology;vocal tract;speech production;speech recognition;acoustics;philosophy;linguistics;sociology;communication	NLP	-9.372549347163758	-82.67764622751716	180303
1fe87097f76428940a13e6f742f9add5df2d8654	acoustic correlates of emotion dimensions in view of speech synthesis	speech synthesis;linear regression	In a database of emotional speech, dimensional descriptions of emotional states have been correlated with acoustic variables. Many stable correlations have been found. The predictions made by linear regression widely agree with the literature. The numerical form of the description and the choice of acoustic variables studied are particularly well suited for future implementation in a speech synthesis system, possibly allowing for the expression of gradual emotional states.	acoustic cryptanalysis;acoustic model;numerical analysis;speech synthesis	Marc Schröder;Roddy Cowie;Ellen Douglas-Cowie;Machiel Westerdijk;Stan C. A. M. Gielen	2001			speech recognition;computer science;linear regression;speech synthesis	AI	-10.433174611459407	-83.9403920511497	180350
48318cbb106ff4b02522e0906b4355b5934246be	deep triphone embedding improves phoneme recognition		In this paper, we present a novel Deep Triphone Embedding (DTE) representation derived from Deep Neural Network (DNN) to encapsulate the discriminative information present in the adjoining speech frames. DTEs are generated using a four hidden layer DNN with 3000 nodes in each hidden layer at the first-stage. This DNN is trained with the tied-triphone classification accuracy as an optimization criterion. Thereafter, we retain the activation vectors (3000) of the last hidden layer, for each speech MFCC frame, and perform dimension reduction to further obtain a 300 dimensional representation, which we termed as DTE. DTEs along with MFCC features are fed into a second-stage four hidden layer DNN, which is subsequently trained for the task of tied-triphone classification. Both DNNs are trained using tri-phone labels generated from a tied-state triphone HMM-GMM system, by performing a forced-alignment between the transcriptions and MFCC feature frames. We conduct the experiments on publicly available TED-LIUM speech corpus. The results show that the proposed DTE method provides an improvement of absolute 2.11% in phoneme recognition, when compared with a competitive hybrid tied-state triphone HMM-DNN system.		Mohit Yadav;Vivek Tyagi	2017	CoRR		discriminative model;computer science;speech recognition;dimensionality reduction;artificial neural network;transcription (linguistics);speech corpus;triphone;embedding;mel-frequency cepstrum;pattern recognition;artificial intelligence	AI	-16.585097890965383	-88.93519609039961	180509
65de78cdfab9ac4d898694c23f0f8977ef2e7dd7	a novel unit selection method for concatenation speech system using similarity measure	speech synthesis natural language processing;speech synthesis;hidden markov models speech acoustics speech synthesis training context modeling vectors;speech synthesis system novel unit selection method concatenation speech system corpus based tts system synthetic target parametric synthesizer human perceptual judgments mandarin female;target cost unit selection hybird speech synthesis;natural language processing	This paper presents a new approach to unit selection for corpus-based TTS system, in which the units are selected according to their similarity with synthetic target generated by a parametric synthesizer. In the training stage, a group of classifiers are trained based on human perceptual judgments. The outputs of the classifiers are used to make a distinction rather than using traditional methods such as continuously-valued cost. In order to obtain a better classification result, different combinations of features are tried as input vectors, and the similarity rating is carried out dexterously. Subjective listening tests on a Mandarin female TTS system show that the proposed classifier based speech synthesis system outperforms the traditional unit-selection system.	concatenation;netware file system;parametric polymorphism;similarity measure;speech synthesis;super robot monkey team hyperforce go!;synthetic intelligence	Ran Zhang;Jianhua Tao;Ya Feng Li;Zhengqi Wen	2013	2013 International Conference Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)	10.1109/ICSDA.2013.6709846	voice activity detection;natural language processing;speech recognition;computer science;speech processing;communication	Robotics	-16.987379118857042	-85.18542781321638	180585
bd7dc6d735f650ab4400c7e9a8ba695c5b080f12	do oral contraceptives really have an adverse effect on voice quality?		Traditionally, oral contraceptives are considered to have adverse effect on women's voice quality. The purpose of this study was to evaluate the impact of oral contraceptives on voice quality, using acoustic analysis. Acoustic vocal parameters of seven women who use oral contraceptives and seven women who do not were measured repeatedly during the menstrual cycle. Repeated-measure analyses-of-variance were performed to test for group differences. Results did not reveal an adverse effect in the oral contraceptive users group. Moreover, amplitude and frequency perturbation, as well as noise-to-harmonics ratio values within the test group were found to be significantly lower than those observed among the control group; indicating a more stable voice quality. I. INTRODUCTION The interaction between the human larynx and ovarian hormones has been previously demonstrated. Several researchers have discovered receptors for androgen, estrogen and progesterone in the gingival epithelium [1] and in the vocal folds [2,3]. The effect of these sex hormones on the human voice has been previously demonstrated in different cases of endocrine dysfunction. Such vocal changes include increase in vocal instability, hoarseness and pronounced lowered pitch [4]. Vocal changes related to hormonal balance were also reported in relation to the menstrual cycle in healthy women. Specifically, vocal changes were observed either at the premenstrual phase [5,6] or shortly prior to ovulation [7]. It should be noted that these changes in vocal quality were reported primarily among professional voice users and less so among non-professionals voice users. While the mechanism underlying these voice changes is not clear yet, it is assumed that the physiological changes which occur during the menstrual cycle impact voice quality. For example, the lowered pitch during the premenstrual phase is assumed to be the result of the edema and venous dilatation observed in the vocal folds at that time [4]. It was also suggested that changes in ovarian hormones levels could affect laryngeal neuromotor control [7], which in turn, could affect vocal stability and quality. It is interesting to note that the two phases along the menstrual cycle in which vocal changes were reported in the literature (premenstrual phase and	acoustic cryptanalysis;color balance;http 404;instability	Ofer Amir;Tal Biron-Shental	2003				HCI	-8.125964528808192	-82.31574646720858	180652
5380a0f7a68f09080ac44c6a65cb7f6682670889	assessing the visual speech perception of sampled-based talking heads		Focusing on flexible applications for limited computing devices, this paper investigates the improvement on the visual speech perception obtained by the implicitly modeling of coarticulation on a sample-based talking head that is characterized by a compact image database and a morphing visemes synthesis strategy. Speech intelligibility tests were applied to assess the effectiveness of the proposed context-dependent visemes (CDV) model, comparing it to a simpler model that does not handle coarticulation. The results show that, when compared to the simpler model, the CDV approach improves speech intelligibility in situations in which the audio is degrade by noise. Moreover the CDV model achieves 80% to 90% of visual speech intelligibility of video of a real talker in the tested cases. Additionally, when the audio is heavily degraded by noise, the results suggest that the mechanisms that explain visual speech perception depends on the quality of the audible information.		Paula Dornhofer Paro Costa;José Mario De Martino	2013			coarticulation;speech perception;viseme;speech recognition;computer science;intelligibility (communication)	HCI	-9.799899374191991	-87.39193056002006	180679
cb45783318d84d86eaabe9c0d250db74c9dc2ada	experimental studies on effect of speaking mode on spoken term detection	training;speech;mel frequency cepstral coefficient;indexes;hidden markov models;telugu broadcast news spoken term detection system speaking mode effect std system dynamic time warping dtw query posterior features multilayer perceptron;feature extraction;speech mel frequency cepstral coefficient training indexes feature extraction hidden markov models;electricity;speech processing multilayer perceptrons pattern matching	The objective of this paper is to study the effect of speaking mode on spoken term detection (STD) system. The experiments are conducted with respect to query words recorded in isolated manner and words cut out from continuous speech. Durations of phonemes in query words greatly vary between these two modes. Hence pattern matching stage plays a crucial role which takes care of temporal variations. Matching is done using Subsequence dynamic time warping (DTW) on posterior features of query and reference utterances, obtained by training Multilayer perceptron (MLP). The difference in performance of the STD system for different phoneme groupings (45, 25, 15 and 6 classes) is also analyzed. Our STD system is tested on Telugu broadcast news. Major difference in STD system performance is observed for recorded and cut-out types of query words. It is observed that STD system performance is better with query words cut out from continuous speech compared to words recorded in isolated manner. This performance difference can be accounted for large temporal variations.	care-of address;dynamic time warping;experiment;memory-level parallelism;multilayer perceptron;pattern matching;std bus;time complexity	Kallola Rout;Pappagari Raghavendra Reddy;K. Sri Rama Murty	2015	2015 Twenty First National Conference on Communications (NCC)	10.1109/NCC.2015.7084926	speech recognition;computer science;pattern recognition;communication	Web+IR	-12.053833968063914	-89.72359552507456	180683
b95b8a0a12c658ebccf19ef542cbaa7fd80d2b85	combination of training criteria to improve continuous speech recognition			speech recognition	Laurence Devillers;Christian Dugast	1993			speech recognition;artificial intelligence;pattern recognition;speaker recognition;computer science	NLP	-14.848082476435208	-87.58994966920639	180754
2dd52ff0b9105e405ef102b809f254a53c120b6c	objective quality and intelligibility prediction for users of assistive listening devices: advantages and limitations of existing tools	speech intelligibility hearing aids speech enhancement;assistive devices;speech processing;auditory system;speech enhancement strategies objective quality speech intelligibility prediction assistive listening devices speech quality prediction normal hearing listeners hearing impaired listeners hearing aids cochlear implants speech to reverberation modulation energy ratio srmr ha srmr ci modulation spectrum area moda hasqi perception indices perception model based quality prediction method hearing impairments pemo q hi reverberation alone condition noise alone condition reverberation plus noise degradation conditions nonlinear frequency compression;acoustic signal processing;speech processing filter banks auditory system modulation quality of service assistive devices acoustic signal processing;filter banks;quality of service;modulation	This article presents an overview of 12 existing objective speech quality and intelligibility prediction tools. Two classes of algorithms are presented?intrusive and nonintrusive?with the former requiring the use of a reference signal, while the latter does not. Investigated metrics include both those developed for normal hearing (NH) listeners, as well as those tailored particularly for hearing impaired (HI) listeners who are users of assistive listening devices [i.e., hearing aids (HAs) and cochlear implants (CIs)]. Representative examples of those optimized for HI listeners include the speech-to-reverberation modulation energy ratio (SRMR), tailored to HAs (SRMR-HA) and to CIs (SRMR-CI); the modulation spectrum area (ModA); the HA speech quality (HASQI) and perception indices (HASPI); and the perception-model-based quality prediction method for hearing impairments (PEMO-Q-HI). The objective metrics are tested on three subjectively rated speech data sets covering reverberation-alone, noise-alone, and reverberation-plus-noise degradation conditions, as well as degradations resultant from nonlinear frequency compression and different speech enhancement strategies. The advantages and limitations of each measure are highlighted and recommendations are given for suggested uses of the different tools under specific environmental and processing conditions.	algorithm;assistive technology;auditory perception;class;cochlear implants;cochlear implant;compression;elegant degradation;hasqi;hearing aids;heartbeat;intelligibility (philosophy);modulation;nethack;nonlinear system;resultant;speech disorders;speech enhancement;hearing impairment	Tiago H. Falk;Vijay Parsa;João Felipe Santos;Kathryn Hoberg Arehart;Oldooz Hazrati;Rainer Huber;James M. Kates;Susan Scollie	2015	IEEE Signal Processing Magazine	10.1109/MSP.2014.2358871	speech recognition;quality of service;computer science;speech processing;intelligibility;modulation	ML	-9.556719505450971	-88.13671882438724	180771
610fe71d99621a0b2583f0795df5cf2e8f6eee6e	comparing affect recognition in peaks and onset of laughter		Laughter is an important social signal that conveys different emotions like happiness, sadness, anger, fear, surprise, and disgust. Therefore, detecting emotions in the laughter is useful for estimating the emotional state of the user. This paper presents work that detects the emotions in Iranian laughter by using audio features and running four machine learning algorithms, namely, Sequential Minimal Optimization (SMO), Multilayer Perceptron (MLP), Logistic, and Radial Basis Function Network (RBFNetwork). We extracted features such as intensity (minimum, maximum, mean, and standard deviation), energy, power, first 3 formants, and the first thirteen Mel Frequency Cepstral Coefficients. Two datasets are used: one that contains segments of full laughter episodes and one that contains only laughter onsets. Results indicate that MLP algorithm produce the highest rate of accuracy which is 86.1372% for first dataset and 85.0123% for second dataset. Besides, using the combination of MFCC and prosodic features led to better results. This means that recognition of emotions is possible at the start of laughter, which is useful for real-time applications.	onset (audio)	Faramarz Ataollahi;Merlin Suarez	2015		10.1007/978-3-319-46218-9_8	psychology;aesthetics;communication;social psychology	Vision	-11.321018609625687	-87.8820542502563	180872
be60ce4e51cd9ceecbbd6a7d7d54d7cfcdebdd70	a robust analyzer for spoken language understanding			alloy analyzer;natural language understanding	Evelyne Millien;Roland Kuhn	1993			spectrum analyzer;speech recognition;spoken language;computer science	NLP	-15.771127987462018	-85.71480348384478	181327
3b470a0d230bfcdeee30abc8780bd953b8e81f55	small-footprint deep neural networks with highway connections for speech recognition		For speech recognition, deep neural networks (DNNs) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models, DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g., mobile devices. In this paper, we study the application of the recently proposed highway network to train small-footprint DNNs, which are thinner and deeper, and have significantly smaller number of model parameters compared to conventional DNNs. We investigated this approach on the AMI meeting speech transcription corpus which has around 80 hours of audio data. The highway neural networks constantly outperformed their plain DNN counterparts, and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy.	acoustic cryptanalysis;artificial neural network;benchmark (computing);deep learning;mixture model;mobile device;speech recognition;transcription (software)	Liang Lu;Steve Renals	2016		10.21437/Interspeech.2016-39	speech recognition;artificial intelligence;machine learning	ML	-18.037243435675304	-88.82734827046245	181460
261360d46a61ca36d5d75cc1b24098f6f1f1ed60	automatic assessment of dysarthria severity level using audio descriptors		Dysarthria is a motor speech impairment, often characterized by speech that is generally indiscernible by human listeners. Assessment of the severity level of dysarthria provides an understanding of the patient's progression in the underlying cause and is essential for planning therapy, as well as improving automatic dysarthric speech recognition. In this paper, we propose a non-linguistic manner of automatic assessment of severity levels using audio descriptors or a set of features traditionally used to define timbre of musical instruments and have been modified to suit this purpose. Multitapered spectral estimation based features were computed and used for classification, in addition to the audio descriptors for timbre. An Artificial Neural Network (ANN) was trained to classify speech into various severity levels within Universal Access dysarthric speech corpus and the TORGO database. An average classification accuracy of 96.44% and 98.7% was obtained for UA speech corpus and TORGO database respectively.	artificial neural network;color gradient;spectral density estimation;speech corpus;speech recognition;user agent	Chitralekha Bhat;Bhavik Vachhani;Sunil Kumar Kopparapu	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953122	computer science;acoustic model;dysarthria;artificial neural network;speech corpus;motor speech;speech recognition;dysarthric speech;timbre	ML	-6.79213648966915	-85.94038895502628	181484
90429cb5ae2fa93318b550213cb6fda30949cacf	an instrumental intelligibility metric based on information theory		We propose a monaural intrusive instrumental intelligibility metric called speech intelligibility in bits (SIIB). SIIB is an estimate of the amount of information shared between a talker and a listener in bits per second. Unlike existing information theoretic intelligibility metrics, SIIB accounts for talker variability and statistical dependencies between time-frequency units. Our evaluation shows that relative to state-of-the-art intelligibility metrics, SIIB is highly correlated with the intelligibility of speech that has been degraded by noise and processed by speech enhancement algorithms.	algorithm;data rate units;heart rate variability;information theory;intelligibility (philosophy);matlab;occam's razor;speech enhancement;occam	Steven Van Kuyk;W. Bastiaan Kleijn;Richard Christian Hendriks	2018	IEEE Signal Processing Letters	10.1109/LSP.2017.2774250	mathematics;artificial intelligence;pattern recognition;information theory;mutual information;speech recognition;speech enhancement;intelligibility (communication);monaural	Arch	-9.803249201857255	-88.08780106139388	181628
aa1b2cc42f54c0080579dab60290e9a16c53bd4b	database development and automatic speech recognition of isolated pashto spoken digits using mfcc and k-nn	pashto digits;knn;mfcc	Automatic recognition of isolated spoken digits is one of the most challenging tasks in the area of automatic speech recognition. In this paper, database development and automatic speech recognition of isolated Pashto spoken digits from sefer (0) to naha (9) has been presented. A number of 50 individual Pashto native speakers (25 male and 25 female) of different ages, ranging from 18 to 60 years, were involved to utter from sefer (0) to naha (9) digits separately. Sony PCM-M 10 linear recorder is used for recoding purpose in the office and home in noise free environment. Adobe audition version 1.0 is used to split the audio of digits into individual digits and result is saved in .wav format. Mel Frequency Cepstral Coefficients (MFCC) is used to extract speech features. K Nearest Neighbor (K-NN) classifier is used for the first time up to author knowledge in Pashto language to classify the features of speech. The experimental results are evaluated, and the overall average recognition accuracy of 76.8% is obtained.	adobe audition;database;k-means clustering;k-nearest neighbors algorithm;matlab;mel-frequency cepstrum;speech recognition;support vector machine;vocabulary	Zakir Ali;Arbab Waseem Abbas;T. M. Thasleema;Burhan Uddin;Tanzeela Raaz;Sahibzada Abdur Rehman Abid	2015	I. J. Speech Technology	10.1007/s10772-014-9267-z	speech recognition;computer science;machine learning;pattern recognition;mel-frequency cepstrum;k-nearest neighbors algorithm	AI	-13.889721582475783	-89.90753195555865	181732
9ac61459a0e7d20b01e98c52ffcdd29cc2c8ca87	structural and dialectal effects on pitch peak alignment in two varieties of british english	experience base	We report three experiments, based on test sentences read aloud, on the influence of sentence position and phonological vowel length on the alignment of accent-related F0 peaks in Scottish Standard English (SSE) and Southern British English (RP). One experiment deals with prenuclear accent peaks and the other two with nuclear accent peaks. Three findings confirm reports in the recent literature on several other European languages. First, as has been reported for Dutch (Ladd, Mennen & Schepman, JASA, 2000), the alignment of prenuclear peaks is later with phonologically short vowels than with long ones, and the effect cannot be explained by actual vowel duration but appears to reflect syllable structure. Second, nuclear peaks are aligned much earlier (relative to the accented vowel) than prenuclear peaks, and, as in Dutch (Schepman, Lickley & Ladd, JPhon, 2006), the effect of syllable structure appears to be absent in nuclear accents; instead, their alignment is strongly influenced by whether the accented syllable is in utterance-final position. Third, as in a number of other studies, we find evidence for differences of phonetic detail between languages or language varieties: both nuclear and prenuclear peaks are aligned later in SSE than in RP, and nuclear peaks appear to be aligned earlier in English than in Dutch.		D. Robert Ladd;Astrid Schepman;Laurence White;Louise May Quarmby;Rebekah Stackhouse	2009	J. Phonetics	10.1016/j.wocn.2008.11.001	psychology;speech recognition;philosophy;linguistics;sociology;communication	NLP	-11.006800478997306	-81.57579531214063	181825
ddad39854d23034c31c00e0819246e2137e4289f	a method to combine acoustic and morphological constraints in the speech production inverse problem	speech production inverse problem;formant;vocal tract acoustics;vocal tract;fourier cosine series;inverse problem;vocal tract morphology;parameter space;fourier analysis;speech production;area function	This paper approaches the articulatory-to-acoustic speech production inverse case. A framework based on an explicit combination of vocal-tract morphological and acoustic constraints is proposed. The solution is based on a Fourier analysis of the vocal-tract log-area function: the relationship between the log-area Fourier cosine coe cients and the corresponding formants is used to formulate an acoustic constraint. The same set of coe cients is then used to express a morphological constraint. This representation of both acoustic and morphological constraints in the same parameter space allows an e cient solution for the inverse problem. The basis of the acoustic constraint formulation was rst proposed by Mermelstein (1967). However, at that time, the combination with morphological constraints was not realized. The method is tested for some vowels. The results con rm the validity of the method, but they also show the need for dynamic constraints.	acoustic cryptanalysis;dynamic programming;fourier analysis;naruto shippuden: clash of ninja revolution 3;tract (literature)	Hani Yehia;Fumitada Itakura	1996	Speech Communication	10.1016/0167-6393(95)00042-9	vocal tract;speech production;speech recognition;formant;inverse problem;mathematics;linguistics;fourier analysis;parameter space	Vision	-9.17816865754329	-86.61837159180988	181848
dc99c7b7348b7e3a67199786216012f24a05f40a	specificity and abstractness of vot imitation	level 2;speech perception;speech production	The imitation paradigm (Goldinger, 1998) has shown that speakers shift their production phonetically in the direction of the imitated speech, indicating the use of episodic traces in speech perception. Although word-level specificity of imitation has been shown, it is unknown whether imitation also can take place with sub-lexical units. By using a modified imitation paradigm, the current study investigated: (1) the generalizability of phonetic imitation at phoneme and sub-phonemic levels, (2) word-level specificity through acoustic measurements of speech production; and (3) automaticity of phonetic imitation and its sensitivity to linguistic structure. The sub-phonemic feature manipulated in the experiments was VOT on the phoneme /p/. The results revealed that participants produced significantly longer VOTs after being exposed to target speech with extended VOTs. Furthermore, this modeled feature was generalized to new instances of the target phoneme /p/ and the new phoneme /k/, indicating that sub-lexical units are involved in phonetic imitation. The data also revealed that lexical frequency had an effect on the degree of imitation. On the other hand, target speech with reduced VOT was not imitated, indicating that phonetic imitation is phonetically selective. & 2011 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;experiment;programming paradigm;sensitivity and specificity;tracing (software)	Kuniko Y. Nielsen	2011	J. Phonetics	10.1016/j.wocn.2010.12.007	psychology;speech production;speech recognition;speech perception;philosophy;linguistics;sociology;communication	NLP	-10.099358013473255	-82.0306830755838	181947
9794add3f0a99bb59e4e4094ab4cf5546647f91c	a single channel speech enhancement approach by combining statistical criterion and multi-frame sparse dictionary learning		In this paper, we consider the single-channel speech enhancement problem, in which a clean speech signal needs to be estimated from a noisy observation. To capture the characteristics of both the noise and speech signals, we combine the well-known Short-Time-Spectrum-Amplitude (STSA) estimator with a machine learning based technique called Multi-frame Sparse Dictionary Learning (MSDL). The former utilizes statistical information for denoising, while the latter helps better preserve speech, especially its temporal structure. The proposed algorithm, named STSA-MSDL, outperforms standard statistical algorithms such as the Wiener filter, STSA estimator, as well as dictionary based algorithms when applied to the TIMIT database, using four different objective metrics that measure speech intelligibility, speech distortion, background noise reduction, and the overall quality.	algorithm;distortion;intelligibility (philosophy);machine learning;military scenario definition language;noise reduction;sparse dictionary learning;sparse matrix;speech enhancement;timit;whole earth 'lectronic link;wiener filter	Hung-Wei Tseng;Srikanth Vishnubhotla;Mingyi Hong;Xiangfeng Wang;Jinjun Xiao;Zhi-Quan Luo;Tao Zhang	2013			pattern recognition;timit;estimator;speech recognition;artificial intelligence;wiener filter;speech enhancement;noise reduction;k-svd;background noise;computer science;intelligibility (communication)	ML	-13.726517428346845	-92.86429567900696	182206
4137c745d8aa009cdd464d4e9f0359ca49e6d1dc	multilevel sampling and aggregation for discriminative training	training hidden markov models data models accuracy acoustics computational modeling maximum likelihood estimation;discriminative training extended baum welch algorithm maximum mutual information based estimation speech acoustic models sampled training sets denominator sufficient statistics numerator sufficient statistics data sampling based ensemble acoustic modeling data sampling based ebw two level data sampling mechanism acoustic model training telehealth conversational speech recognition task multilevel sampling multilevel aggregation;ensemble acoustic model;ensemble acoustic model speech recognition discriminative training extended baum welch algorithm data sampling;extended baum welch algorithm;telemedicine acoustic signal processing estimation theory signal sampling speech recognition;speech recognition;discriminative training;data sampling	We propose to use data sampling in the extended Baum-Welch (EBW) algorithm for maximum mutual information (MMI) based estimation of speech acoustic models, and to randomize the configurations of the sampled training sets and aggregate the numerator and denominator sufficient statistics for improving model robustness. We further combine data sampling based ensemble acoustic modeling with the data sampling based EBW, forming a two-level data sampling mechanism for acoustic model training. We conducted experiments on a telehealth conversational speech recognition task, where the two-level data sampling mechanism gave a statistically significant, absolute word accuracy gain of 3.56% over the conventional MMI baseline, corresponding to a 19.44% relative word error rate reduction.	acoustic cryptanalysis;acoustic model;aggregate data;algorithmic inference;baseline (configuration management);baum–welch algorithm;discriminative model;experiment;mutual information;sampling (signal processing);speech recognition;welch's method;word error rate	Yunxin Zhao;Tuo Zhao;Xin Chen	2014	The 9th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2014.6936677	speech recognition;computer science;machine learning;pattern recognition;statistics	NLP	-18.760669793050884	-90.7641000096925	182237
cc7b431cceb7fe8df9fa31cee399b1a4a8676c41	noise adaptation using linear regression for continuous noisy speech recognition	linear transformation;signal to noise ratio;linear regression;speech recognition	We present an approach for recognising continuous speech in the presence of an additive noise, based on model adaptation. The method consists in transforming the parameters of acoustic mod- els to reduce the acoustic mismatch between a test utterance and a set of clean speech models. We assume that speech is modelled by a set of Stochastic Trajectory Models (STM). The mean vec- tors of STMs are adapted using a set of linear transformations. The transformations are derived from a small labelled adaptation corpus, so that the likelihood of the adaptation corpus given the adapted models is maximised. Experiments performed on dif- ferent additive noises and for various signal-to-noise ratio (SNR) show that the adaptation scheme significantlyincreases the accu- racy. For SNR from 12dB to 36dB, we observed that the perfor- mance of the linear regression is similar or better than the perfor- mance obtained when training and testing the recogniser in noise.	speech recognition	Olivier Siohan;Yifan Gong;Jean Paul Haton	1995			artificial intelligence;speech recognition;gaussian noise;pattern recognition;linear regression;linear map;computer science;signal-to-noise ratio	ML	-17.48138374110539	-91.74024462283498	182279
e98a842244ece9de132c438a8793b1af44381aa0	handwritten digit recognition by a neocognitron with improved bend-extractors			neocognitron	Kunihiko Fukushima;Eiji Kimura;Hayaru Shouno	1998			machine learning;artificial intelligence;mathematics;speech recognition;neocognitron;intelligent word recognition;numerical digit	Vision	-15.553282187572924	-87.51866625709019	182469
2d48d74b99590fb9829fd7c93128a0e2116eeacd	advances in phone-based modeling for automatic accent classification	signal conversion;busqueda informacion;modelizacion;vocabulaire;evaluation performance;metodo estadistico;analisis componente principal;speech processing natural languages speech recognition principal component analysis;speaker identification;prononciation;conversion senal;spectral trajectory modeling;performance evaluation;dialect modeling;automatic system;hidden markov model;information retrieval;lexicon;evaluacion prestacion;acoustic modeling;speech processing;vocabulary;modele markov variable cachee;tratamiento palabra;traitement parole;linear discriminant analysis phone based modeling automatic accent classification text independent accent classification spectral evolution information principal component analysis;statistical method;natural languages;metodo subespacio;methode acoustique;vocabulario;linear discriminate analysis;conversion signal;probabilistic approach;spoken document retrieval;senal vocal;methode sous espace;speaker recognition;algorithme;discriminant analysis;analyse discriminante;modelisation;voice conversion;algorithm;mandarin chinese;signal vocal;analisis discriminante;system evaluation;acoustic method;recherche documentaire;hidden markov models;reconocimiento voz;sistema automatico;monitoring;native american;speech recognition automatic accent classification dialect modeling open accent classification phoneme recognition spectral trajectory modeling;recherche information;methode statistique;pronunciation;enfoque probabilista;approche probabiliste;busqueda documental;principal component analysis;thai;modele spectral;metodo acustico;signal classification;classification system;reconnaissance locuteur;analyse composante principale;systeme automatique;speech recognition hidden markov models loudspeakers streaming media tagging information retrieval monitoring adaptation model natural languages vocabulary;subspace method;classification signal;signal acoustique;speech recognition;phoneme recognition;open accent classification;document retrieval	It is suggested that algorithms capable of estimating and characterizing accent knowledge would provide valuable information in the development of more effective speech systems such as speech recognition, speaker identification, audio stream tagging in spoken document retrieval, channel monitoring, or voice conversion. Accent knowledge could be used for selection of alternative pronunciations in a lexicon, engage adaptation for acoustic modeling, or provide information for biasing a language model in large vocabulary speech recognition. In this paper, we propose a text-independent automatic accent classification system using phone-based models. Algorithm formulation begins with a series of experiments focused on capturing the spectral evolution information as potential accent sensitive cues. Alternative subspace representations using principal component analysis and linear discriminant analysis with projected trajectories are considered. Finally, an experimental study is performed to compare the spectral trajectory model framework to a traditional hidden Markov model recognition framework using an accent sensitive word corpus. System evaluation is performed using a corpus representing five English speaker groups with native American English, and English spoken with Mandarin Chinese, French, Thai, and Turkish accents for both male and female speakers.	acoustic cryptanalysis;acoustic model;algorithm;baseline (configuration management);biasing;concatenation;display resolution;document retrieval;estimation theory;experiment;futures studies;ground truth;heart rate variability;hidden markov model;language model;lexicon;linear discriminant analysis;local-density approximation;markov chain;polynomial texture mapping;population;principal component analysis;programming language;prototype;software transactional memory;speaker recognition;speech recognition;speech synthesis;statistical classification;streaming media;super robot monkey team hyperforce go!;test data;text corpus;vocabulary	Pongtep Angkititrakul;John H. L. Hansen	2006	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TSA.2005.851980	natural language processing;speech recognition;systems modeling;mandarin chinese;computer science;linguistics;natural language;hidden markov model;principal component analysis	NLP	-16.517535490752593	-85.32791529855706	182605
9855c3e8637b7bb5ca4ea72df20e728872dc8dfd	visibabble demo	real time analysis;pre speech vocalizations;feedback	The visiBabble system responds with animations to an infant's syllable-like productions and records the acoustic-phonetic analysis. The system reinforces production of syllabic utterances associated with later language and cognitive development. This demo will show off new animated responses and recent improvements in acoustic-phonetic feature detection.	acoustic cryptanalysis;cognitive science;feature detection (computer vision);feature detection (web development);syllable	Harriet J. Fell;Joel MacAuslan;Jun Gong;Josh Ostrow	2005		10.1145/1090785.1090829	natural language processing;speech recognition;computer science;feedback	NLP	-7.657807000787537	-83.44481307707667	182666
0f792ad223008aea378878650e8e8b3b8a49b25a	structured gmm based on unsupervised clustering for recognizing adult and child speech	unsupervised clustering;stochastic trajectory modeling;speech recognition;speaker class modeling	Speaker variability is a well-known problem of state-of-theart Automatic Speech Recognition (ASR) systems. In particular, handling children speech is challenging because of substantial differences in pronunciation of the speech units between adult and child speakers. To build accurate ASR systems for all types of speakers Hidden Markov Models with Gaussian Mixture Densities were intensively used in combination with model adaptation techniques. This paper compares different ways to improve the recognition of children speech and describes a novel approach relying on Class-Structured Gaussian Mixture Model (GMM). A common solution for reducing the speaker variability relies on gender and age adaptation. First, it is proposed to replace gender and age by unsupervised clustering. Speaker classes are first used for adaptation of the conventional HMM. Second, speaker classes are used for initializing structured GMM, where the components of Gaussian densities are structured with respect to the speaker classes. In a first approach mixture weights of the structured GMM are set dependent on the speaker class. In a second approach the mixture weights are replaced by explicit dependencies between Gaussian components of mixture densities (as in stranded GMMs, but here the GMMs are class-structured). The different approaches are evaluated and compared on the TIDIGITS task. The best improvement is achieved when structured GMM is combined with feature adaptation.	algorithm;automated system recovery;baseline (configuration management);cluster analysis;google map maker;hidden markov model;markov chain;mixture model;spatial variability;speech recognition;subspace gaussian mixture model;unsupervised learning;vocabulary	Arseniy Gorin;Denis Jouvet	2014		10.1007/978-3-319-11397-5_8	speaker recognition;speaker diarisation;speech recognition;computer science;pattern recognition;communication	NLP	-18.12699010466356	-91.59519851584683	182668
