id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
a3d04635fc4ca58a76e4542050fdf7a72146da6d	pccf: periodic and continual temporal co-factorization for recommender systems		Rating-only collaborative filtering has been extensively studied for decades with great improvements achieved in predicting a user’s preference on a target item at a particular time point. Yet, it remains a research challenge on how to capture users’ rating patterns which may drift over time. In this article, we propose a time-aware matrix co-factorization model, called PCCF, which considers two types of temporal effects, i.e., periodic and continual. Specifically, periodic effects refer to the impact of discrete periodic time slices with which users’ preferences may be associated, and continual effects refer to the impact of continuous gradual time over which users’ preference patterns may change. The fact that users exhibit different preference patterns with respect to different time aspect has been further confirmed by our analysis on three real-world data sets. Together with time-based user biases, we integrate the two kinds of temporal effects into a unified matrix factorization model. Experimental results on the three data sets demonstrate the effectiveness of both kinds of temporal effects for rating prediction as well as the superiority of our approach’s performance over that of the other counterparts.	collaborative filtering;recommender system;user (computing)	Guibing Guo;Feida Zhu;Shilin Qu;Xingwei Wang	2018	Inf. Sci.	10.1016/j.ins.2018.01.019	machine learning;artificial intelligence;recommender system;mathematics;periodic graph (geometry);collaborative filtering;theoretical computer science;time point;data set;matrix decomposition;matrix (mathematics);factorization	AI	-19.103694503828716	-47.278066956633836	58311
4c335ba6e3bb42a4b52e7fea71fa6f17f4170429	trie for similarity matching in large video databases	video databases;computacion informatica;query processing;window based feature;index structure;video indexing;ciencias basicas y experimentales;indexation;synthetic data;similarity matching;trie;string matching;grupo a;video database;window order heuristic;digital video library;matching method;indexing technique	Similarity matching in video databases is of growing importance in many new applications such as video clustering and digital video libraries. In order to provide efficient access to relevant data in large databases, there have been many research efforts in video indexing with diverse spatial and temporal features. However, most of the previous works relied on sequential matching methods or memory-based inverted file techniques, thus making them unsuitable for a large volume of video databases. In order to resolve this problem, this paper proposes an effective and scalable indexing technique using a trie, originally proposed for string matching, as an index structure. For building an index, we convert each frame into a symbol sequence using a window order heuristic and build a disk-resident trie from a set of symbol sequences. For query processing, we perform a depth-first traversal on the trie and execute a temporal segmentation. To verify the superiority of our approach, we perform several experiments with real and synthetic data sets. The results reveal that our approach consistently outperforms the sequential scan method, and the performance gain is maintained even with a large volume of video databases. r 2003 Elsevier Ltd. All rights reserved.	cluster analysis;depth-first search;digital video;experiment;full table scan;heuristic;inverted index;library (computing);scalability;string searching algorithm;synthetic data;synthetic intelligence;tree traversal;trie;versant object database;video copy detection	Sanghyun Park;Ki-Ho Hyun	2004	Inf. Syst.	10.1016/S0306-4379(03)00046-2	computer science;trie;video tracking;data mining;database;information retrieval;algorithm;string searching algorithm;synthetic data	DB	-6.38176605815003	-41.27353855372957	58374
4d68d92f1d114d88597c986a50011a4e498d6b58	finding image exemplars using fast sparse affinity propagation	sparse affinity propagation;user experience;affinity propagation;image search;web search;image exemplar	In this paper, we propose a novel approach to organize image search results obtained from state-of-the-art image search engines in order to improve user experience. We aim to discover exemplars from search results and simultaneously group the images. The exemplars are delivered to the user as a summary of search results instead of the large amount of unorganized images. This gives the user a brief overview of search results with a small amount of images, and helps the user to further find the images of interest. We adopt the idea of affinity propagation and design a fast sparse affinity propagation algorithm to find exemplars that best represent the image search results. Experiments on real-world data demonstrate the effectiveness of our method both visually and quantitatively.	affinity propagation;algorithm;experiment;image retrieval;processor affinity;software propagation;sparse matrix;user experience;web search engine	Yangqing Jia;Jingdong Wang;Changshui Zhang;Xian-Sheng Hua	2008		10.1145/1459359.1459448	computer vision;user experience design;computer science;theoretical computer science;machine learning;pattern recognition;affinity propagation	Vision	-16.515964553325045	-48.134665034143445	58426
1bd6e73aa13426fa00cf4663a97d9995322965b2	measuring and maximizing group closeness centrality over disk-resident graphs	group centrality;greedy algorithm	As an important metric in graphs, group closeness centrality measures how close a group of vertices is to all other vertices in a graph, and it is used in numerous graph applications such as measuring the dominance and influence of a node group over the graph. However, when a large-scale graph contains hundreds of millions of nodes/edges which cannot reside entirely in computer's main memory, measuring and maximizing group closeness become challenging tasks. In this paper, we present a systematic solution for efficiently calculating and maximizing the group closeness for disk-resident graphs. Our solution first leverages a probabilistic counting method to efficiently estimate the group closeness with high accuracy, rather than exhaustively computing it in an exact fashion. In addition, we design an I/O-efficient greedy algorithm to find a node group that maximizes group closeness. Our proposed algorithm significantly reduces the number of random accesses to disk, thereby dramatically improving computational efficiency. Experiments on real-world big graphs demonstrate the efficacy of our approach.	closeness centrality;computation;computer data storage;greedy algorithm;input/output;vertex (geometry)	Junzhou Zhao;John C. S. Lui;Donald F. Towsley;Xiaohong Guan	2014		10.1145/2567948.2579356	mathematical optimization;random walk closeness centrality;greedy algorithm;graph center;computer science;centrality	ML	-11.087391474724088	-41.186670678711515	58467
ed446c75eed2a4cb97d38c8e9a33f9ae1c93e5a1	the temporal aspects of the evidence-based influence maximization on social networks	influence maximization;91d30;time horizon;seed selection;90c11;social networks;optimization;97m70;stable cascade	The temporal aspects of the evidence-based influence maximization on social networks Mohammadreza Samadi, Alexander Nikolaev & Rakesh Nagi To cite this article: Mohammadreza Samadi, Alexander Nikolaev & Rakesh Nagi (2016): The temporal aspects of the evidence-based influence maximization on social networks, Optimization Methods and Software, DOI: 10.1080/10556788.2016.1214957 To link to this article: http://dx.doi.org/10.1080/10556788.2016.1214957	entropy maximization;expectation–maximization algorithm;social network	Mohammadreza Samadi;Alexander G. Nikolaev;Rakesh Nagi	2017	Optimization Methods and Software	10.1080/10556788.2016.1214957	econometrics;mathematical optimization;time horizon;artificial intelligence;social network	ML	-17.175990769539094	-43.560306689278185	58850
ab55f3161513b48f70d0aeddd40d904ec71e3d82	a co-training method for identifying the same person across social networks		In order to fit the diverse scenes in life, more and more people choose to join different types of social networks simultaneously. These different networks often contain the information that people leave in a particular scene. Under the circumstances, identifying the same person across different social networks is a crucial way to help us understand the user from multiple aspects. The current solution to this problem focuses on using only profile matching or relational matching method. Some other methods take the two aspect of information into consideration, but they associate the profile similarity with relation similarity simply by a parameter. The matching results on two dimensions may have large difference, directly link them may reduce the overall similarity. Unlike the most of the previous work, we propose to utilize collaborative training method to tackle this problem. We run experiments on two real-world social network datasets, and the experimental results confirmed the effectiveness of our method.	co-training;experiment;social network;teaching method	Zheng Fang;Yanan Cao;Yanbing Liu;Jianlong Tan;Li Guo;Yanmin Shang	2017	2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2017.8309194	machine learning;social network;co-training;computer science;artificial intelligence	Web+IR	-18.659624472720207	-46.52763026060836	58883
5361e751f7143dedfbcafb322fcfdabb7d5182ee	drug addiction: a computational multiscale model combining neuropsychology, cognition and behavior	behavioral processes;bio-signal modeling;neuro-;addiction;cognitive processes;multiscale modeling;drug addiction;computer model;cognitive process	According to the United Nations, approximately 24.7 million people used amphetamines, 16 million used cocaine, and 12 million used heroin in 2006/07 (Costa, 2008). Full recovery from drug addiction by chemical treatment and/or social and psychological support is uncertain. The present investigation was undertaken to expand our understanding of the factors that drive the dynamics of addiction. A new multiscale computational model is presented which integrates current theories of addiction, unlike previous models, considers addiction as a reversible process (Siegelmann, 2008). Explicit time dependency is added to the inhibition and the compulsion processes. Preliminary computational predictions of drug-seeking behavior are presented and potential correlation with experimental data is discussed. Validation of the model appears promising, however additional investigation is required.	cognition;computation;computational model;theory	Yariv Z. Levy;Dino J Levy;Jerrold S. Meyer;Hava T. Siegelmann	2009			pattern recognition;drug;neuroscience;artificial intelligence;experimental data;machine learning;multiscale modeling;computer science;addiction;neuropsychology;cognition	AI	-9.397925267951761	-51.598875098260024	58908
a7e8e42643e84492ccbec31f071915a818cfa7cb	automatic categorization of health indices for risk quantification		Classification of health data into categories is routinely used for the analysis and understanding of health risks; however, the selection of cut-off points of categories is not a simple task, and mistakes can lead to incorrect interpretation of data. Since inappropriate selection of the cut-off points can lead to unreliable and wrong conclusions, it is desirable to have an automatic method that balances the bias and the variance for constructing categories, and which allows the verification if the amount of available data is enough to draw a conclusion. Such a method is also useful in making decisions on next actions in experiment planning. We show here that a better formulation of cut-off point estimation is required for health data involving wide comparability, and demonstrate how a different method for comparing categorizations can be applied to such data. Our method can help in automation of data analysis pipeline and in promotion of scientific discoveries from health data. c © 2015 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Program Chairs.	categorization;interpretation (logic)	Atsunori Kanemura;Gérard Lipowski;Hidehiko Komine;Shotaro Akaho	2015		10.1016/j.procs.2015.08.350	computer science;data science;data mining;statistics	ML	-9.327934410709183	-45.56414509857595	59220
30db7fec069139a1d5392f388e3ff48f06c58b37	obtaining system robustness by mimicking natural mechanisms	electronic circuits;dynamic change;system function;agent performance;degeneracy;evolutionary computation;biological system modeling;system robustness;biology;data mining;biomembranes;software agents;evolution biology;natural mechanism;robustness biological system modeling cells biology evolution biology protein engineering biomembranes electronic circuits circuit synthesis logic gates logic programming;proteins;logic programming;logic gates;damaging environment;system function system robustness natural mechanism real working agents agent performance changing environment evolution degeneracy damaging environment;normal operator;real working agents;robustness;software agents evolutionary computation;changing environment;protein engineering;circuit synthesis;cells biology;evolution	Real working agents normally operate in dynamic changing environments. These changes could either affect the efficiency of the agents' performance or even damage the functionality of the agent. Robustness is the key requirement to solve this problem. Inspired by nature, this paper demonstrates two mechanisms that contribute to individual's robustness in changing environments: evolution and degeneracy. Through evolution in damaging environment, evolved agents have to cope with changes in the environment and acquire robustness. Through degeneracy, individuals can maintain their fitness even when some damaged parts are involved in system function.	degeneracy (biology);degeneracy (graph theory);evolution;fitness function;knockout;robustness (computer science);spatial variability	Song Zhan;Julian Francis Miller;Andrew M. Tyrrell	2009	2009 IEEE Congress on Evolutionary Computation	10.1109/CEC.2009.4983326	computer science;artificial intelligence;evolution;protein engineering;logic programming;normal operator;degeneracy;robustness;evolutionary computation	AI	-4.673658587252412	-48.495810225562174	59228
80c8de5761710ac207f6d07f56058477f89ef96f	approximating personalized katz centrality in dynamic graphs		Dynamic graphs can capture changing relationships in many real datasets that evolve over time. One of the most basic questions about networks is the identification of the “most important” vertices in a network. Measures of vertex importance called centrality measures are used to rank vertices in a graph. In this work, we focus on Katz Centrality. Typically, scores are calculated through linear algebra but in this paper we present an new alternative, agglomerative method of calculating Katz scores and extend it for dynamic graphs. We show that our static algorithm is several orders of magnitude faster than the typical linear algebra approach while maintaining good quality of the scores. Furthermore, our dynamic graph algorithm is faster than pure static recomputation every time the graph changes and maintains high recall of the highly ranked vertices on both synthetic and real graphs.	approximation algorithm;katz centrality;linear algebra;list of algorithms;numerical analysis;personalization;symbolic computation;synthetic intelligence;vertex (geometry);vertex (graph theory)	Eisha Nathan;David A. Bader	2017		10.1007/978-3-319-78024-5_26	discrete mathematics;linear algebra;distributed computing;katz centrality;centrality;vertex (geometry);hierarchical clustering;computer science;orders of magnitude (numbers);ranking;graph	DB	-10.983019951144161	-41.345001369047566	59684
3bb040720b180c01ecee01617bda1358924e760c	performance of procedures for identifying influentials in a social network: prediction of time and memory usage as a function of network properties		Identification of influential nodes in a social network is an interesting problem since these nodes assist in faster information propagation in the network. In the current work, the procedures based on Cole and Weiss are used to identify influentials in networks of various sizes and their performances are analyzed. Further it is established that preprocessing techniques like clustering and sampling improve the performance of Cole and Weiss procedures. Finally, the computational resources required for these procedures are estimated based on network properties.	cluster analysis;computation;computational resource;performance;preprocessor;sampling (signal processing);social network;software propagation	P. M. Krishnaraj;Ankith Mohan;G. K. SrinivasaK.	2017	Social Network Analysis and Mining	10.1007/s13278-017-0454-1	sampling (statistics);data mining;cluster analysis;social network;preprocessor;machine learning;computer science;artificial intelligence	ML	-10.589801183152527	-42.48455852490785	60639
cdf3691c47396d0688a1d192c24ff64abaa3bd56	uniwalk: unidirectional random walk based scalable simrank computation over large graph	computers;computational modeling;indexing;mathematical model;optimization;scalability;monte carlo methods	SimRank is an effective structural similarity measurement between two vertices in a graph, which can be used in many applications like recommender systems. Although progresses have been achieved, existing methods still face challenges to handle large graphs. Besides huge index construction and maintenance cost, the existing methods require considerable search space and time overheads in the online SimRank query. In this paper, we design a Monte Carlo based method, Uni-Walk, to enable the fast top-k SimRank computation over large undirected graphs without indexing. UniWalk directly locates the top-k similar vertices for any single source vertex u via O(R) sampling paths originating from u only, which avoids the selection of candidate vertex set C and the following O(|C|R) bidirectional sampling paths starting from u and each candidate respectively in existing methods. We also design a space-efficient method to reduce intermediate results, and a path-sharing strategy to optimize path sampling for multiple source vertices. Furthermore, we extend UniWalk to existing distributed graph processing frameworks to improve its scalability. We conduct extensive experiments to illustrate that UniWalk has high scalability, and outperforms the state-of-the-art methods by orders of magnitude, and such an improvement is achieved without any indexing overheads.	computation;directed graph;e-commerce;experiment;graph (abstract data type);graph (discrete mathematics);graph theory;jun wang (scientist);mathematical optimization;monte carlo method;parallel computing;recommender system;sampling (signal processing);scalability;simrank;structural similarity;vertex (geometry)	Xiongcai Luo;Jun Gao;Chang Zhou;Jeffrey Xu Yu	2017	2017 IEEE 33rd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2017.92	search engine indexing;scalability;computer science;theoretical computer science;mathematical model;data mining;database;distributed computing;computational model;statistics;monte carlo method	DB	-9.27856431231779	-40.231657555258344	61046
060fae40022d43cd6bdeb598142f5f1659746025	usher: improving data quality with dynamic forms	databases;modelizacion;quality assurance;modern databases;metodo adaptativo;integridad de datos;data integrity;data entry;integrite donnee;real world data sets;real time;end to end system;databases quality assurance cleaning computer science artificial intelligence laboratories feedback costs decision making error correction;database;base dato;bayesian methods;methode adaptative;probabilistic approach;data mining;modelisation;probabilistic model;dynamic forms;aseguracion calidad;feedback;adaptation model;qualite information;enfoque probabilista;approche probabiliste;error correction;temps reel;adaptive method;information quality;base de donnees;tiempo real;usher;artificial intelligence;predictive models;computer science;data quality;calidad de la informacion;probabilistic logic;data quality improvement;real world data sets usher data quality improvement dynamic forms modern databases automatic methods end to end system probabilistic model;dynamic adaptation;modeling;assurance qualite;adaptive form;automatic methods;cleaning;data models;form design	Data quality is a critical problem in modern databases. data-entry forms present the first and arguably best opportunity for detecting and mitigating errors, but there has been little research into automatic methods for improving data quality at entry time. In this paper, we propose Usher, an end-to-end system for form design, entry, and data quality assurance. Using previous form submissions, Usher learns a probabilistic model over the questions of the form. Usher then applies this model at every step of the data-entry process to improve data quality. Before entry, it induces a form layout that captures the most important data values of a form instance as quickly as possible and reduces the complexity of error-prone questions. During entry, it dynamically adapts the form to the values being entered by providing real-time interface feedback, reasking questions with dubious responses, and simplifying questions by reformulating them. After entry, it revisits question responses that it deems likely to have been entered incorrectly by reasking the question or a reformulation thereof. We evaluate these components of Usher using two real-world data sets. Our results demonstrate that Usher can improve data quality considerably at a reduced cost when compared to current practice.	cognitive dimensions of notations;correctness (computer science);data quality;database;end system;end-to-end principle;greedy algorithm;information capture;information gain in decision trees;kullback–leibler divergence;litmus;predictive modelling;real-time transcription;reduced cost;section 508 amendment to the rehabilitation act of 1973;sensor;statistical model;usability testing;user interface	Kuang Chen;Harr Chen;Neil Conway;Joseph M. Hellerstein;Tapan S. Parikh	2010	IEEE Transactions on Knowledge and Data Engineering	10.1109/ICDE.2010.5447832	statistical model;quality assurance;data modeling;simulation;error detection and correction;systems modeling;data quality;bayesian probability;computer science;artificial intelligence;data integrity;data mining;feedback;database;predictive modelling;information quality;probabilistic logic;statistics	DB	-10.039530353668303	-45.92881053620057	61655
6d583cc0d9223e84b32d1995d41a4e2b4dc7b7a2	conflict-aware event-participant arrangement	social network services;approximation algorithms;pruning technique conflict aware event participant arrangement web 2 0 online to offline marketing model online event based social networks ebsn meetup whova global event participant arrangement with conflict and capacity arrangement problem geacc problem np hard problem;approximation methods approximation algorithms algorithm design and analysis economic indicators noise measurement web 2 0 social network services;noise measurement;web 2 0;approximation methods;social networking online approximation theory computational complexity internet marketing;algorithm design and analysis;economic indicators	With the rapid development of Web 2.0 and Online To Offline (O2O) marketing model, various online event-based social networks (EBSNs), such as Meetup and Whova, are getting popular. An important task of EBSNs is to facilitate the most satisfactory event-participant arrangement for both sides, i.e. events enroll more participants and participants are arranged with personally interesting events. Existing approaches usually focus on the arrangement of each single event to a set of potential users, and ignore the conflicts between different events, which leads to infeasible or redundant arrangements. In this paper, to address the shortcomings of existing approaches, we first identify a more general and useful event-participant arrangement problem, called Global Event-participant Arrangement with Conflict and Capacity (GEACC) problem, focusing on the conflicts of different events and making event-participant arrangements in a global view. Though it is useful, unfortunately, we find that the GEACC problem is NP-hard due to the conflict constraints among events. Thus, we design two approximation algorithms with provable approximation ratios and an exact algorithm with pruning technique to address this problem. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments on real and synthetic datasets.	approximation algorithm;exact algorithm;experiment;np-hardness;online and offline;provable security;social network;synthetic intelligence;web 2.0	Jieying She;Yongxin Tong;Lei Chen;Caleb Chen Cao	2015	2015 IEEE 31st International Conference on Data Engineering	10.1109/ICDE.2015.7113329	algorithm design;computer science;noise measurement;theoretical computer science;machine learning;economic indicator;data mining;database;web 2.0;approximation algorithm	DB	-17.011908219763345	-45.3277199778153	61759
158d1246b8e4d389b2b829c1c0f4a825727b871c	unconventional computation and natural computation		One of the motivations behind computing with molecules is to “computerize” living systems, for example to prevent disease or control artificial tissues. Biology, however, is already very good at computing — the human brain being one example. Even on a single cell level information is constantly being processed, and the development of a functional organism from a single fertilized cell is controlled by an ingenious if only partially understood program encoded in DNA. Does this mean that the efforts to “write” new molecular programs are redundant? Not at all — natural programs have taken three billion years to evolve and, despite their beauty, are very difficult to alter in any way. In my view the optimal approach is to balance the engineering principles inspired by computer science and engineering, such as universal models, reprogrammability, modularity, etc., with the harsh reality of cell and organismal biology. The simple fact is that we do not know yet, even at the theory level, whether it is possible to perform reliable information processing in actual living cells as opposed to idealized “well-mixed reactors”. Despite these limitations, the field of molecular computing in cells, or biological computing, has made significant steps forward with new design principles, new architectures, and new exciting experimental results. These developments also inform basic biological research. From Quantum Dynamics to Physical	computation;computer science;dna computing;fractal;information processing;living systems;modularity (networks);natural computing;quantum dynamics;self-similarity	Peter Banda;Christof Teuscher	2014		10.1007/978-3-319-08123-6		Comp.	-6.794286962481943	-48.03592177047721	62103
1d70f0d7bd782c65273bc689b6ada8723e52d7a3	empirical comparison of algorithms for network community detection	spectral methods;flow based methods;community detection;approximate algorithm;conductance;data mining;information network;database management;objective function;optimization problem;graph partitioning;community structure;detection algorithm;spectral method;data structure	"""Detecting clusters or communities in large real-world graphs such as large social or information networks is a problem of considerable interest. In practice, one typically chooses an objective function that captures the intuition of a network cluster as set of nodes with better internal connectivity than external connectivity, and then one applies approximation algorithms or heuristics to extract sets of nodes that are related to the objective function and that """"look like"""" good communities for the application of interest.  In this paper, we explore a range of network community detection methods in order to compare them and to understand their relative performance and the systematic biases in the clusters they identify. We evaluate several common objective functions that are used to formalize the notion of a network community, and we examine several different classes of approximation algorithms that aim to optimize such objective functions. In addition, rather than simply fixing an objective and asking for an approximation to the best cluster of any size, we consider a size-resolved version of the optimization problem. Considering community quality as a function of its size provides a much finer lens with which to examine community detection algorithms, since objective functions and approximation algorithms often have non-obvious size-dependent behavior."""	approximation algorithm;heuristic (computer science);mathematical optimization;optimization problem	Jure Leskovec;Kevin J. Lang;Michael W. Mahoney	2010		10.1145/1772690.1772755	mathematical optimization;data structure;computer science;machine learning;data mining;approximation algorithm;spectral method	Web+IR	-13.325655179951635	-40.95349158962838	62223
45b5e8e57507a6bc1191b06c0c4cafaee239b40f	unsupervised feature selection for accurate recommendation of high-dimensional image data	high dimensionality;generic model;user preferences;visual features;feature selection	Content-based image suggestion (CBIS) targets the recommendation of products based on user preferences on the visual content of images. In this paper, we motivate both feature selection and model order identification as two key issues for a successful CBIS. We propose a generative model in which the visual features and users are clustered into separate classes. We identify the number of both user and image classes with the simultaneous selection of relevant visual features using the message length approach. The goal is to ensure an accurate prediction of ratings for multidimensional non-Gaussian and continuous image descriptors. Experiments on a collected data have demonstrated the merits of our approach.	feature selection;generative model;user (computing);visual descriptor	Sabri Boutemedjet;Djemel Ziou;Nizar Bouguila	2007			computer science;machine learning;pattern recognition;data mining;feature selection;information retrieval	ML	-18.07238192201409	-49.73136417821379	62283
eddeac7a40d74ec9a501379fc2d4e691384d9b97	posgrami: possibilistic frequent subgraph mining in a single large graph		The frequent subgraph mining has widespread applications in many different domains such as social network analysis and bioinformatics. Generally, the frequent subgraph mining refers to graph matching. Many research works dealt with structural graph matching, but a little attention is paid to semantic matching when graph vertices and/or edges are attributed. Therefore, the discovered frequent subgraphs should become more pruned by applying a new semantic filter instead of using only structural similarity in the graph matching process. In this paper, we present POSGRAMI, a new hybrid approach for frequent subgraph mining based principally on approximate graph matching. To this end, POSGRAMI first uses an approximate structural similarity function based on graph edit distance function. POSGRAMI then uses a semantic vertices similarity function based on possibilistic information affinity function. In fact, our proposed approach is a new possibilistic version of existing approach in literature named GRAMI. This paper had shown the effectiveness of POSGRAMI on some real datasets. In particular, it achieved a better performance than GRAMI in terms of processing time, number and quality of discovered subgraphs.		Mohamed Moussaoui;Montaceur Zaghdoud;Jalel Akaichi	2016		10.1007/978-3-319-40596-4_46	factor-critical graph;machine learning;pattern recognition;data mining;subgraph isomorphism problem;induced subgraph isomorphism problem	ML	-8.623640009120694	-39.077495605904296	62619
542a3e2b97f6df5eedc2509f915ea12508118aec	asynchronous distributed matrix factorization with similar user and item based regularization	asynchronous distributed optimization;similarity based regularization;recommender systems	We introduce an asynchronous distributed stochastic gradient algorithm for matrix factorization based collaborative filtering. The main idea of this approach is to distribute the user-rating matrix across different machines, each having access only to a part of the information, and to asynchronously propagate the updates of the stochastic gradient optimization across the network. Each time a machine receives a parameter vector, it averages its current parameter vector with the received one, and continues its iterations from this new point. Additionally, we introduce a similarity based regularization that constrains the user and item factors to be close to the average factors of their similar users and items found on subparts of the distributed user-rating matrix. We analyze the impact of the regularization terms on MovieLens (100K, 1M, 10M) and NetFlix datasets and show that it leads to a more efficient matrix factorization in terms of Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), and that the asynchronous distributed approach significantly improves in convergence time as compared to an equivalent synchronous distributed approach.	algorithm;collaborative filtering;gradient;iteration;mathematical optimization;matrix regularization;mean squared error;movielens	Bikash Joshi;Franck Iutzeler;Massih-Reza Amini	2016		10.1145/2959100.2959161	mathematical optimization;computer science;theoretical computer science;machine learning;world wide web;recommender system	ML	-18.462181275442987	-48.04880484082258	62674
9cdcabc5e7cc4f16d6a8b36e8d12a5e900c8ecf6	predicting the quality of experience for internet video with fuzzy decision tree	pragmatics;measurement;quality metrics;fuzzy decision tree internet video qoe quality metrics;mobile access devices fuzzy decision tree user quality of experience prediction qoe web sites internet video service providers service log data china semistructured log data video quality metrics user engagement fdt classification algorithms machine learning;internet;internet video;streaming media;video recording;qoe;predictive models;web sites decision trees fuzzy set theory internet pattern classification quality of experience video on demand;measurement streaming media internet decision trees predictive models pragmatics video recording;fuzzy decision tree;decision trees	In this paper, we attempt to predict users' quality of experience (QoE) with the log data collected from the web sites of Internet video service providers. To this end, we first collect service log data in the wild from one of the Top 5 most popular providers in China. Then we do a series of data preprocessing to format the original semi-structured log data to structured. We calculate several key video quality metrics, such as join time and frame rate, and explore the distributions of each quality metric, as well as the relationship between individual quality metric and user engagement. Considering that user engagement may be a result of comprehensive effect of several metrics, we apply fuzzy decision tree (FDT), a kind of classification algorithms in the area of machine learning, to develop the predictive model of user QoE for Internet video. Finally, we compare the prediction accuracy of our model with the model developed using decision tree on several different datasets. Our model separately achieves about 20% and 10% improvement in prediction accuracy on the dataset of sessions with the same content type and the dataset of sessions with mobile access devices.	algorithm;data pre-processing;decision tree;internet;machine learning;powerflasher fdt;preprocessor;requirement;semiconductor industry;web service	Yuran Zhang;Ting Yue;Hongbo Wang;Anming Wei	2014	2014 IEEE 17th International Conference on Computational Science and Engineering	10.1109/CSE.2014.230	subjective video quality;the internet;computer science;video quality;machine learning;decision tree;incremental decision tree;data mining;database;predictive modelling;multimedia;world wide web;pevq;measurement;computer network;pragmatics	Metrics	-18.422498924048778	-52.06531101714001	63362
099e1326c581138569d8687cf2fcab539685aaa8	analysis of fieldwork activities during milk production recording in dairy ewes by means of individual ear tag (et) alone or plus rfid based electronic identification (eid)		Abstract In compliance with EU Reg. 21/2004, the individual electronic identification (EID) of sheep is mandatory in EU Countries since the 1st of January 2008. However, the impact of innovation within animal identification systems has not yet been carried out to quantify benefits during common fieldwork in real dairy sheep flocks. In particular, systematic milk production record represents a fundament to monitor real time performance of each single ewe, before a cumulative datum about flock output. The rational management of dairy ewes should rely on technological tools to create sub-groups according to production performance in a quick and reliable way. This trial aimed to investigate efficacy, reliability and efficiency associate to two different identification systems for monitoring individual milk production throughout the lactation in sheep flocks. Qualitative and quantitative indicators for each criterion, were useful for the assessment of activities and output correctness. Experimental activities were carried in two different farms of Sardinia. In both farms, animals were individually identified by means of an endoruminal ceramic bolus (transponder HDX 134.2 kHz, according to ISO Standards 11784–11785) and by conventional plastic ear tag. In both farms, during milking, groups of ewes were monitored (24 animals per round) by 2 trained technicians. In EID ewes, equipment to check individual transponder’s code and data recording consisted of a handy reader (Gesimpex Com. S.L., Barcelona, Spain); in ET ewes, individual ET code and milk production were visually inspected and typed on a handy data store equipment (ABB Immediate Business System PLC – Radix). Time needed for accomplishment of activities was taken for the two identification systems with a chronometer. In the two farms, a total of 553 animals were monitored for five times (controls were scheduled monthly) throughout the lactation period. Average time for reading and recording individual code and production for EID vs . ET resulted 6″/sheep vs . 11″/sheep. Transponder’s code resulted correctly checked and repeatedly associated in 100% of readings. EID system of diary ewes shows to improve production recording and data transfer in comparison with conventional ET system with advantageous technical relationships between fieldwork activities.	ear tag;electronic identification;field research;radio-frequency identification	M. G. Cappai;N. G. Rubiu;G. Nieddu;M. P. L. Bitti;W. Pinna	2018	Computers and Electronics in Agriculture	10.1016/j.compag.2017.11.002	flock;electronic identification;engineering;control engineering;operations management;milking;transponder (aeronautics);animal identification;ear tag	HCI	-10.854296422855407	-49.12821106980999	63472
515de436f65bddcf9e24cc0d3b82689bfaae3f14	grid-based indexing and search algorithms for large-scale and high-dimensional data		The rapid development of Internet has resulted in massive information overloading recently. These information is usually represented by high-dimensional feature vectors in many related applications such as recognition, classification and retrieval. These applications usually need efficient indexing and search methods for such large-scale and high-dimensional database, which typically is a challenging task. Some efforts have been made and solved this problem to some extent. However, most of them are implemented in a single machine, which is not suitable to handle large-scale database.In this paper, we present a novel data index structure and nearest neighbor search algorithm implemented on Apache Spark. We impose a grid on the database and index data by non-empty grid cells. This grid-based index structure is simple and easy to be implemented in parallel. Moreover, we propose to build a scalable KNN graph on the grids, which increase the efficiency of this index structure by a low cost in parallel implementation. Finally, experiments are conducted in both public databases and synthetic databases, showing that the proposed methods achieve overall high performance in both efficiency and accuracy.	algorithmic trading;apache spark;database;database index;experiment;feature vector;function overloading;k-nearest neighbors algorithm;nearest neighbor search;scalability;search algorithm;statistical classification;synthetic intelligence	Chuanfu Yang;Zhiyang Li;Wenyu Qu;Zhaobin Liu;Heng Qi	2017	2017 14th International Symposium on Pervasive Systems, Algorithms and Networks & 2017 11th International Conference on Frontier of Computer Science and Technology & 2017 Third International Symposium of Creative Computing (ISPAN-FCST-ISCC)	10.1109/ISPAN-FCST-ISCC.2017.85	theoretical computer science;grid;scalability;clustering high-dimensional data;feature vector;search engine indexing;cluster analysis;computer science;search algorithm;nearest neighbor search	DB	-4.813650316597543	-40.27982097028631	63649
1223e4c0659abeda65cb55d2cb48d6c5f3aa4f58	a multi-modal hashing learning framework for automatic image annotation		Automatic Image Annotation (AIA) plays an important role in large-scaled intelligent image management and retrieval. Based on the correlation between image low-level features and high-level semantic concepts, images can be efficiently retrieved from large-scaled image dataset. Recently, many researchers leverage machine learning techniques to annotate images automatically. However, these methods still have many challenges regarding efficiency and scalability in the massive image dataset. Moreover, manually labeling massive images is a costly and time-consuming task, which is unacceptable in practical applications. Therefore, only a few labeled images can be obtained as samples in the training dataset. However, the tags associated with labeled and unlabeled images found on social network websites may be helpful for improving the performance of AIA. In this work, we propose a Multi-Modal Semantic Hash Learning framework named MMSHL for AIA. MMSHL seamlessly integrates multi-graph learning, multi-modal correlation learning and latent semantic hashing learning into a joint optimization framework. Based on MMSHL, we annotate images using a two-step semi-supervised learning approach. Since our AIA method makes use of associated tags of images, good results can be achieved. Extensive experiments are performed based on two real-world datasets MIR Flickr and NUS-WIDE. Experimental results show that our framework can improve the performance of AIA effectively.	automatic image annotation;deep learning;experiment;flickr;hash function;high- and low-level;intelligent agent;machine learning;mathematical optimization;modal logic;scalability;semi-supervised learning;semiconductor industry;social network;supervised learning;world file	Jiale Wang;Guohui Li	2017	2017 IEEE Second International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2017.48	automatic image annotation;hash function;scalability;information retrieval;image retrieval;semantics;computer science	Vision	-16.469733889722672	-50.06339532731827	63764
2fd20edbef1dda36bba7bf0cf441e8549786e356	distributed estimation over complex networks	distributed estimation;complex network;small world;network topology;scale free;diffusion lms;期刊论文;article	Distributed estimation is an appealing technique for in-network signal processing. In this paper, we investigate the impacts of network topology on the performance of a distributed estimation algorithm, namely adaptive-then-combine diffusion LMS, based on the data with or without the temporal and spatial independence assumptions. The study covers different network models, including the regular, the small-world, the random and the scale-free, while the performance is analyzed according to the mean stability, mean-square errors, communication cost and robustness. Simulation results show that the estimation performance is largely dependent on the topological properties of the networks, such as the average path length, the clustering coefficient and the degree distribution, indicating that the network topology indeed plays an important role in distributed estimation. From the design point of view, this study also provides some guidelines on how to design a network such that the qualities of estimates are optimized.	complex network	Ying Liu;Chunguang Li;Wallace Kit-Sang Tang;Zhaoyang Zhang	2012	Inf. Sci.	10.1016/j.ins.2012.02.008	average path length;computer science;theoretical computer science;scale-free network;machine learning;network simulation;distributed computing;network topology;complex network;statistics	DB	-15.227987630427364	-40.79065028412391	64068
3634203e2d0b3f8cf4ff400a65c3ad8cff0f57cf	size matters: a comparative analysis of community detection algorithms		Understanding the community structure of social media is critical due to its broad applications such as friend recommendations, user modeling, and content personalization. Existing research uses structural metrics such as modularity and conductance and functional metrics such as ground truth to measure the quality of the communities discovered by various community detection algorithms, while overlooking a natural and important dimension, community size. Recently, the anthropologist Dunbar suggests that the size of a stable community in social media should be limited to 150, referred to as Dunbar’s number. In this paper, we propose a systematic way of algorithm comparison by orthogonally integrating community size as a new dimension into existing structural metrics for consistently and holistically evaluating the community quality in the social media context. We design a heuristic clique-based algorithm which controls the size and overlap of communities with adjustable parameters and evaluate it along with six state-of-the-art community detection algorithms on both Twitter and DBLP networks. Specifically, we divide the discovered communities based on their size into four classes called a close friend, a casual friend, acquaintance, and just-a-face, and then calculate the coverage, modularity, triangle participation ratio, conductance, transitivity, and the internal density of communities in each class. We discover that communities in different classes exhibit diverse structural qualities and many existing community detection algorithms tend to output extremely large communities.		Paul Wagenseller;Feng Wang	2018	IEEE Transactions on Computational Social Systems	10.1109/TCSS.2018.2875626	computer science;personalization;clique;user modeling;dunbar's number;social media;modularity;heuristic;community structure;algorithm	DB	-16.4579690192646	-40.48245133863705	64197
e2172c67857319d3a27cd45ae889cc0a3b6d1af9	scientific collaboration network evolution model based on motif emerging	databases;analytical models;groupware;complex networks;motif emerging;collaboration;informing science;scale free network;motif emerging model;natural sciences computing complex networks digital simulation groupware;scientific collaboration network evolution model;degree distribution;evolution biology;computational modeling;theoretical analysis;theoretic analysis;collaborative networks;author collaboration network;network model;scientific cooperation;demonstration;power law;network model motif emerging scientific cooperation demonstration;natural sciences computing;complex networks ip networks world wide web information science erbium databases computer networks international collaboration laboratories information technology;computer simulation;scientific thesis;author collaboration network scientific collaboration network evolution model scientific thesis motif emerging model theoretic analysis computer simulation scale free network journal of information science;digital simulation;journal of information science;data models	Based on the evolvement characteristic of the author collaboration in the scientific thesis, a motif emerging model of the scientific cooperation network is presented. It is validated by the theoretic analysis and computer simulation that the degree distribution of the network nodes is a power law and this network is a scale-free network. In order to make sure the validity of the network model, we gather the articles in Journal of Information Science, and construct a author collaboration network. By analyzing the data of the network, the demonstration results are consistent with that of the model.	computer simulation;degree distribution;embnet.journal;information science;motif;network model;scientific collaboration network;theory	Xiuchun Shi;Longde Wu;Hongyong Yang	2008	2008 The 9th International Conference for Young Computer Scientists	10.1109/ICYCS.2008.31	computer simulation;organizational network analysis;data modeling;power law;degree distribution;network formation;computer science;dynamic network analysis;artificial intelligence;data science;scale-free network;network model;software engineering;data mining;database;network simulation;management science;computational model;complex network;collaboration	HPC	-17.719072967212735	-40.45106933427368	64541
a71ccaf8de61efcc9d289a1f991ee87f9ffe0b8e	new permutation dissimilarity measures for proximity searching		Proximity searching consists in retrieving the most similar objects to a given query from a database. To do so, the usual approach consists in using an index in order to improve the response time of online queries. Recently, the permutation based algorithms (PBA) were presented, and from then on, this technique has been very successful. In its core, the PBA uses a metric between permutations, typically Spearman Footrule or Spearman Rho. Until now, several proposals based on the PBA have been developed and all of them uses one of those metrics. In this paper, we present a new family of dissimilarity measures between permutations. According to our experimental evaluation, we can reduce up to 30% the original technique costs, while preserving its exceptional answer quality. Since our dissimilarity measures can be applied in any state-of-the-art PBA variant, the impact of our proposal is significant for the similarity search community.	proximity search (text)	Karina Figueroa;Rodrigo Paredes;Nora Reyes	2018		10.1007/978-3-030-02224-2_10	permutation;machine learning;proximity search;spearman's rank correlation coefficient;response time;artificial intelligence;computer science;nearest neighbor search	Theory	-6.070910742987432	-41.60627574812357	64710
07611cb1fde942c4d727dabd03c361b2852a88b9	keyword-aware continuous knn query on road networks	roads indexes search problems query processing keyword search probes algorithm design and analysis	It is nowadays quite common for road networks to have textual contents on the vertices, which describe auxiliary information (e.g., business, traffic, etc.) associated with the vertex. In such road networks, which are modelled as weighted undirected graphs, each vertex is associated with one or more keywords, and each edge is assigned with a weight, which can be its physical length or travelling time. In this paper, we study the problem of keyword-aware continuous k nearest neighbour (KCkNN) search on road networks, which computes the k nearest vertices that contain the query keywords issued by a moving object and maintains the results continuously as the object is moving on the road network. Reducing the query processing costs in terms of computation and communication has attracted considerable attention in the database community with interesting techniques proposed. This paper proposes a framework, called a Labelling AppRoach for Continuous kNN query (LARC), on road networks to cope with KCkNN query efficiently. First we build a pivot-based reverse label index and a keyword-based pivot tree index to improve the efficiency of keyword-aware k nearest neighbour (KkNN) search by avoiding massive network traversals and sequential probe of keywords. To reduce the frequency of unnecessary result updates, we develop the concepts of dominance interval and region on road network, which share the similar intuition with safe region for processing continuous queries in Euclidean space but are more complicated and thus require more dedicated design. For high frequency keywords, we resolve the dominance interval when the query results changed. In addition, a path-based dominance updating approach is proposed to compute the dominance region efficiently when the query keywords are of low frequency. We conduct extensive experiments by comparing our algorithms with the state-of-the-art methods on real data sets. The empirical observations have verified the superiority of our proposed solution in all aspects of index size, communication cost and computation time.	computation;database;experiment;graph (discrete mathematics);k-nearest neighbors algorithm;pivot table;regional lockout;time complexity;univac larc;vertex (geometry);vertex (graph theory)	Bolong Zheng;Kai Zheng;Xiaokui Xiao;Han Su;Hongzhi Yin;Xiaofang Zhou;Guohui Li	2016	2016 IEEE 32nd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2016.7498297	sargable;query optimization;query expansion;web query classification;computer science;theoretical computer science;machine learning;data mining;database	DB	-9.097816698495153	-38.72728651641341	64903
c460e476024dd38a0348aae1233d8771a1997181	the comparison of significance of fuzzy community partition across optimization methods		The analysis of fuzzy(overlapping) community structure in complex networks is an important problem in data mining of network data sets. However, due to the exist of random factors and error edges in real networks, how to measure the significance of community structure efficiently is a crucial question. In this paper, we present a novel statistical framework comparing the significance of fuzzy community structure across various optimization models. Different from the universal approaches, we calculate the similarity between a given node and its leader and employ the distribution of link tightness to derive the significance score, instead of a direct comparison to a randomized model. Based on the distribution of community tightness, a new “pvalue” form significance measure is proposed for community structure analysis. Specially, the well-known approaches and their corresponding quality functions are unified to a novel general formulation, which facilitate providing a detail comparison across them. To determine the position of leaders and their corresponding followers, an efficient algorithm is proposed based on the spectral theory. Finally, we apply the significance analysis to some famous benchmark networks and the good performance verified the effectiveness and efficiency of our framework.	benchmark (computing);complex network;data mining;mathematical optimization;randomized algorithm;whole earth 'lectronic link	Hui-Jia Li	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-151974	artificial intelligence;machine learning;data mining;mathematics;statistics	ML	-14.428528393586156	-42.344434129333415	65123
56ab991d0ace4bee6d1a4e94416d591a4e795212	searching web data using minhash lsh	lsh;web data;dataset search;minhash	In this extended abstract, we explore the use of MinHash Locality Sensitive Hashing (MinHash LSH) to address the problem of indexing and searching Web data. We discuss a statistical tuning strategy of MinHash LSH, and experimentally evaluate the accuracy and performance, compared with inverted index. In addition, we describe an on-line demo for the index with real Web data.	experiment;game demo;inverted index;locality of reference;locality-sensitive hashing;minhash;online and offline;lsh	BiChen Rao;Erkang Zhu	2016		10.1145/2882903.2914838	computer science;minhash;database;world wide web;information retrieval	Web+IR	-5.933004276825578	-41.02523222791416	65239
9d4b8df1ae50f7ee35606637ecb4b08cdb39e194	multi-view community detection in facebook public pages		Community detection in social networks is widely studied because of its importance in uncovering how people connect and interact. However, little attention has been given to community structure in Facebook public pages. In this study, we investigate the community detection problem in Facebook newsgroup pages. In particular, to deal with the diversity of user activities, we apply multi-view clustering to integrate different views, for example, likes on posts and likes on comments. In this study, we explore the community structure in not only a given single page but across multiple pages. The results show that our method can effectively reduce isolates and improve the quality of community structure.		Zhige Xin;George A. Barnett;Zhi-Yong Lu;Shyhtsun Felix Wu	2018	CoRR		data mining;computer science;social network;cluster analysis;community structure	Web+IR	-19.082745339887143	-44.80950667170838	65381
251fc1ab3de832b4602500b31b0c6e480c1d130d	public opinion propagation model on social networks	social network services;complex networks;social network services mathematical model computational modeling market research complex networks equations educational institutions;market research;public opinion propagation;computational modeling;social networks;disseminate probability social networks seir model public opinion propagation informed probability;informed probability public opinion propagation model seir node degree social network peculiar dissemination rules user habits complex network theories epidemiology dynamic evolution equations mathematical models disseminate probability;mathematical model;social networking online complex networks evolutionary computation information dissemination network theory graphs probability;disseminate probability;seir model;informed probability	This paper proposes a public opinion propagation model on social networks based on SEIR. This model consider impacts of the node degree, social networks peculiar dissemination rules and users' habits, utilizes epidemiology and complex network theories, and establishes the dynamic evolution equations by building mathematical models of informed probability and disseminate probability. Simulation results show that pages of updating information can affect the propagation behavior of S, E nodes directly, the value of spread probability is the key impact factor of propagation velocity and scale. Additional, the number of infected nodes should be maintained at an appropriate range in order to make the maximum range of public opinion propagation.	complex network;mathematical model;simulation;social network;software propagation;theory;velocity (software development)	Jinlou Zhao;Junhui Cheng;Hongyu Gao	2014	2014 Seventh International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2014.68	market research;computer science;artificial intelligence;machine learning;mathematical model;data mining;mathematics;management science;computational model;complex network;statistics;social network	Robotics	-18.650352050332305	-42.48070949608169	65441
54c2d1a54d380bacde41b66c7e701ec94be64b6a	clustering spectrum of hierarchical scale-free networks		Real-world networks often have power-law degrees and scale-free properties such as ultra-small distances and ultra-fast information spreading. We provide evidence of a third universal property: three-point correlations that suppress the creation of triangles and signal the presence of hierarchy. We quantify this property in terms of c̄(k), the probability that two neighbors of a degree-k node are neighbors themselves. We investigate how the clustering spectrum k 7→ c̄(k) scales with k and discover a universal curve that consists of three k-ranges where c̄(k) remains flat, starts declining, and eventually settles on a power law c̄(k) ∼ k−α with α depending on the power law of the degree distribution. We test these results against ten contemporary real-world networks and explain analytically why the universal curve properties only reveal themselves in large networks.	cluster analysis;degree distribution;menger sponge	Clara Stegehuis;Remco van der Hofstad;Johan van Leeuwaarden;Augustus J. E. M. Janssen	2017	CoRR		hierarchical clustering of networks;correlation clustering;cluster analysis;dendrogram;artificial intelligence;hierarchical clustering;single-linkage clustering;biclustering;pattern recognition;mathematics;brown clustering	ML	-15.581589110502573	-39.79399128264477	65452
5804fd4ebae9e4ac0f9dced62721bdea9b15b56f	similarity search problem research on multi-dimensional data sets	k nearest search;nearest neighbor searches histograms search problems algorithm design and analysis data preprocessing data mining accuracy;query processing;multiquery search approach similarity search problem research multidimensional data sets nearest neighbors multiple queries query point distribution data distribution;multi query;multi query k nearest search similarity search;search problems;data handling;search problems data handling query processing;similarity search	In this paper, we present our continuous work on designing an algorithm to find nearest neighbors to given queries. In our previous work, we analyze the situation that there are multiple queries with different level of importance, and define a weight for each query point. We also propose an algorithm to find nearest neighbors to multiple queries with weights and enhanced our algorithm based on query point distribution. In this paper we analyze the data distribution on various dimensions, and apply the shrinking concept for the improvement and enhancement of our multi-query search approach.	algorithm;data point;dimensionality reduction;experiment;nearest neighbor search;search algorithm;search problem;similarity search;synthetic intelligence	Yong Shi;Brian Graham	2013	2013 10th International Conference on Information Technology: New Generations	10.1109/ITNG.2013.72	beam search;sargable;query optimization;query expansion;web query classification;best bin first;computer science;machine learning;group method of data handling;pattern recognition;jump search;data mining;best-first search;nearest neighbor search;web search query;information retrieval;search engine	DB	-5.155138679686528	-40.65159541514206	65550
c8f038624b17bea1ea7f78a9de6319b6061547ba	which targets to contact first to maximize influence over social network	target node selection;information diffusion;influence degree	"""We address a new type of influence maximization problem which we call """"target selection problem"""". This is different from the traditionally thought influence maximization problem, which can be called """"source selection problem"""", where the problem is to find a set of K nodes that together maximizes their influence over a social network. The very basic assumption there is that all these K nodes can be the source nodes, i.e. can be activated. In """"target selection problem"""" we maximize the influence of a new user as a source node by selecting K nodes in the network and adding a link to each of them. We show that this is the generalization of """"source selection problem"""" and also satisfies the submodularity. The selected nodes are substantially different from those of """"source selection problem"""" and use of the solution of """"source selection problem"""" results in a very poor performance."""	social network	Kazumi Saito;Masahiro Kimura;Kouzou Ohara;Hiroshi Motoda	2013		10.1007/978-3-642-37210-0_39	mathematical optimization;computer science;artificial intelligence;machine learning;spreading activation	ECom	-16.91093251866504	-44.26365158021833	65634
786ddfc79c4c5c841535f2239df85c7383e86eda	social network clustering by using genetic algorithm: a case study		With the rapid growth of large-scaled social networks, the analysis of social network data has become an extremely challenging computational issue. To meet the challenge, it is possible to significantly reduce the complexity of the problem by properly clustering a large social network into groups, and then analyzing data within each group, or studying the relationship among groups. Hence, social network clustering can be regarded as one of the essential problems in social network analysis. To address the issue, we propose an evolutionary computation approach to social network clustering. We first formulate social network clustering as an optimization problem and then develop a genetic algorithm to solve the problem. We also applied the proposed approach to a case study based on data of some Facebook users.	genetic algorithm;social network	Ming-Feng Tsai;Chun-Yi Lu;Churn-Jung Liau;Tuan-Fang Fan	2016		10.1007/978-3-319-42007-3_25	correlation clustering;flame clustering;canopy clustering algorithm;cure data clustering algorithm;cluster analysis;brown clustering;hierarchical clustering of networks;population-based incremental learning;clustering high-dimensional data	AI	-13.85358793902053	-43.49760260739048	65725
3270a66e0cf3b1bebc3603d1a0044ee89d8d250e	a nonparametric latent factor model for location-aware video recommendations		We are interested in learning customers’ video preferences from their historic viewing patterns and geographical location. We consider a Bayesian latent factor modeling approach for this task. In order to tune the complexity of the model to best represent the data, we make use of Bayesian nonparameteric techniques. We describe an inference technique that can scale to large real-world data sets. Finally we show results obtained by applying the model to a large internal Netflix data set, that illustrates that the model was able to capture interesting relationships between viewing patterns and geographical location.	bayesian network;location (geography);recommender system	Ehtsham Elahi	2016	CoRR		computer science;data science;machine learning;data mining;statistics	ML	-17.6967899326271	-48.51787983672302	65855
252641fb2fc5ddff7bb9f679d66609f1115d1548	methods to determine node centrality and clustering in graphs with uncertain structure	betweenness centrality;network measurement;information network;network analysis;structural complexity;social network;clustering coefficient;graph representation	Much of the past work in network analysis has focused on analyzing discrete graphs, where binary edges represent the “presence” or “absence” of a relationship. Since traditional network measures (e.g., betweenness centrality) assume a discrete link structure, data about complex systems must be transformed to this representation before calculating network properties. In many domains where there may be uncertainty about the relationship structure, this transformation to a discrete representation will result in a lose of information. In order to represent and reason with network uncertainty, we move beyond the discrete graph framework and develop social network measures based on a probabilistic graph representation. More specifically, we develop measures of path length, betweenness centrality, and clustering coefficient— one set based on sampling and one based on probabilistic paths. We evaluate our methods on two real-world networks, Enron and Facebook, showing that our proposed methods more accurately capture salient effects without being susceptible to local noise.	aggregate data;approximation;betweenness centrality;cluster analysis;clustering coefficient;complex systems;computation;computer cluster;evolving networks;flow network;graph (abstract data type);graph (discrete mathematics);sampling (signal processing);shortest path problem;social network analysis;spatial variability;timeline	Joseph J. Pfeiffer;Jennifer Neville	2011	CoRR		network theory;network science;random walk closeness centrality;structural complexity;combinatorics;discrete mathematics;social science;network analysis;computer science;katz centrality;alpha centrality;machine learning;clustering coefficient;mathematics;graph;centrality;betweenness centrality;social network	ML	-15.547169294830933	-41.15105918551808	65919
cba1d56c97fd558b6dd35c3295157173686e7a5d	coupling multiple views of relations for recommendation	conference proceeding	Learning user/item relation is a key issue in recommender system, and existing methods mostly measure the user/item relation from one particular aspect, e.g., historical ratings, etc. However, the relations between users/items could be influenced by multifaceted factors, so any single type of measure could get only a partial view of them. Thus it is more advisable to integrate measures from different aspects to estimate the underlying user/item relation. Furthermore, the estimation of underlying user/item relation should be optimal for current task. To this end, we propose a novel model to couple multiple relations measured on different aspects, and determine the optimal user/item relations via learning the optimal way of integrating these relation measures. Specifically, matrix factorization model is extended in this paper by considering the relations between latent factors of different users/items. Experiments are conducted and our method shows good performance and outperforms other baseline methods.	approximation algorithm;baseline (configuration management);latent variable;loss function;mechatronics;recommender system	Bin Fu;Guandong Xu;Longbing Cao;Zhihai Wang;Zhiang Wu	2015		10.1007/978-3-319-18032-8_57	computer science;artificial intelligence;data mining;database;world wide web;information retrieval	Web+IR	-18.34620070775873	-47.60619676823161	66127
b67a9698d54e9c6f87d5cef788b71afe8be6c9d4	hits is principal components analysis	institutional repositories;graph theory;fedora;web pages;crm;binary time series data;pattern;association rules;matrix algebra;web sites principal component analysis matrix algebra graph theory;vital;principal component analysis;correspondence analysis;web sites;sequential mining;principal component analysis web pages statistical analysis algorithm design and analysis information systems computer science data mining statistics;vtls;adjacency matrix;ils;correspondence analysis hits principal components analysis hubs and authorities model multivariate statistical analysis adjacency matrix web page salsa;multivariate statistical analysis;principal component	In this work, we show that Kleinbergýs hubs and authorities model (HITS) is simply Principal Components Analysis (PCA; maybe the most widely used multivariate statistical analysis method), albeit without centering, applied to the adjacency matrix of the graph of web pages. We further show that a variant of HITS, SALSA, is closely related to correspondence analysis, another standard multivariate statistical analysis method. In addition to provide a clear statistical interpretation for HITS, this result suggests to rely on existing work already published in the multivariate statistical analysis litterature (extensions of PCA or correspondence analysis) in order to analyse or design new web pages scoring procedures.	adjacency matrix;correspondence analysis;ensemble interpretation;principal component analysis;salsa;web page	Marco Saerens;François Fouss	2005	The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05)	10.1109/WI.2005.71	computer science;graph theory;data science;machine learning;data mining;correspondence analysis;statistics;principal component analysis	SE	-11.630213536437948	-44.433154569735564	66556
66e701e4dfb84fa10528f109f3e9f321b9137fd5	answering reachability queries on incrementally updated graphs by hierarchical labeling schema	tak lam wong 增量更新 查询处理 可达性 标记 有向无环图 性能比较 有向循环图 hls answering reachability queries on incrementally updated graphs by hierarchical labeling schema	We proposed a novel solution schema called the Hierarchical Labeling Schema (HLS) to answer reachability queries in directed graphs. Different from many existing approaches that focus on static directed acyclic graphs (DAGs), our schema focuses on directed cyclic graphs (DCGs) where vertices or arcs could be added to a graph incrementally. Unlike many of the traditional approaches, HLS does not require the graph to be acyclic in constructing its index. Therefore, it could, in fact, be applied to both DAGs and DCGs. When vertices or arcs are added to a graph, the HLS is capable of updating the index incrementally instead of re-computing the index from the scratch each time, making it more efficient than many other approaches in the practice. The basic idea of HLS is to create a tree for each vertex in a graph and link the trees together so that whenever two vertices are given, we can immediately know whether there is a path between them by referring to the appropriate trees. We conducted extensive experiments on both real-world datasets and synthesized datasets. We compared the performance of HLS, in terms of index construction time, query processing time and space consumption, with two state-of-the-art methodologies, the path-tree method and the 3-hop method. We also conducted simulations to model the situation when a graph is updated incrementally. The performance comparison of different algorithms against HLS on static graphs has also been studied. Our results show that HLS is highly competitive in the practice and is particularly useful in the cases where the graphs are updated frequently.	algorithm;database;definite clause grammar;directed acyclic graph;experiment;http live streaming;high-level synthesis;reachability;simulation;vertex (geometry);vertex (graph theory)	Tak-Lam Wong	2016	Journal of Computer Science and Technology	10.1007/s11390-016-1633-7	computer science;database;algorithm	DB	-9.393844228501683	-40.00937549419552	66803
5a65d13d4a7ab5f9815550f217c6b3e4d5be65e1	ivibrate: interactive visualization-based framework for clustering large datasets	labeling;data analysis;clustering;interactive visualization;performance;cluster analysis	With continued advances in communication network technology and sensing technology, there is astounding growth in the amount of data produced and made available through cyberspace. Efficient and high-quality clustering of large datasets continues to be one of the most important problems in large-scale data analysis. A commonly used methodology for cluster analysis on large datasets is the three-phase framework of sampling/summarization, iterative cluster analysis, and disk-labeling. There are three known problems with this framework which demand effective solutions. The first problem is how to effectively define and validate irregularly shaped clusters, especially in large datasets. Automated algorithms and statistical methods are typically not effective in handling these particular clusters. The second problem is how to effectively label the entire data on disk (disk-labeling) without introducing additional errors, including the solutions for dealing with outliers, irregular clusters, and cluster boundary extension. The third obstacle is the lack of research about issues related to effectively integrating the three phases. In this article, we describe iVIBRATE---an interactive visualization-based three-phase framework for clustering large datasets. The two main components of iVIBRATE are its VISTA visual cluster-rendering subsystem which invites human interplay into the large-scale iterative clustering process through interactive visualization, and its adaptive ClusterMap labeling subsystem which offers visualization-guided disk-labeling solutions that are effective in dealing with outliers, irregular clusters, and cluster boundary extension. Another important contribution of iVIBRATE development is the identification of the special issues presented in integrating the two components and the sampling approach into a coherent framework, as well as the solutions for improving the reliability of the framework and for minimizing the amount of errors generated within the cluster analysis process. We study the effectiveness of the iVIBRATE framework through a walkthrough example dataset of a million records and we experimentally evaluate the iVIBRATE approach using both real-life and synthetic datasets. Our results show that iVIBRATE can efficiently involve the user in the clustering process and generate high-quality clustering results for large datasets.	cluster analysis;interactive visualization	Keke Chen;Ling Liu	2006	ACM Trans. Inf. Syst.	10.1145/1148024	interactive visualization;fuzzy clustering;computer science;bioinformatics;data science;machine learning;cure data clustering algorithm;data mining;database;cluster analysis	Graphics	-12.991112481154762	-38.87856591905864	67480
579fc6fc047c973ed844537192c3925b2bdba5eb	social media network simplification using morphological operators		Complex networks arise in many diverse contexts, ranging from web pages and their links, computer networks, and social networks interactions. The modelling and mining of these large-scale, self-organizing systems is a broad effort spanning many disciplines. This article proposes the use of morphological operators, based on Mathematical Morphology, to simplify a set of interactions in a complex social media network. By applying these morphological operators, it is possible to simplify the social network and thus identify important communities and actors in the network. An analysis based on the visualization of the communities was carried out to verify the pertinence of the detection and simplification.	file spanning;interaction;level of detail;mathematical morphology;organizing (structure);relevance;self-organization;social media;social network;text simplification;web page	Érick Lopez-Ornelas;Rocío Abascal-Mena	2017	Research in Computing Science		management science;operator (computer programming);social media;computer science	Web+IR	-18.500720037884197	-40.49208485345051	67485
d60d1cc2b89dbc6b5be985cc002c07120cec79e1	boosting the performance of cbr applications with jcolibri	software;similarity metric;optimisation;case base reasoning;jcolibri;case based reasoning jcolibri cbr;boosting artificial intelligence application software software engineering buildings recommender systems noise reduction visualization software tools software prototyping;data mining;noise measurement;optimisation case based reasoning;accuracy;cbr applications;clustering;noise reduction;similarity metrics;cognition;textual case classification cbr applications case based reasoning jcolibri optimization clustering noise reduction similarity metrics;textual case classification;cbr;artificial intelligence;optimization;case based reasoning;automatic classification	jCOLIBRI is currently a reference platform in the CBR community for building CBR systems that includes facilities to design different types of CBR applications \cite{ICCBR05CBRT,jscp07BuildingCBRsystems,AI06OntBasedCBR}. In this paper we focus in some recently included tools that allow the improvement of performance of previously designed applications. These optimization tools mainly facilitate to adjust features on large case bases like clustering and noise reduction techniques, and to adjust processes like refine similarity metrics through case base visualization, parallelization of retrieval or distribution of the case base and reasoning thought different agents. We present the tools and exemplify how to use them in a real scenario. We have developed an experiment for the automatic classification of a textual case base made of 1500 academic journals belonging to 20 different areas.	case-based reasoning;cluster analysis;display resolution;exemplification;experiment;image noise;mathematical optimization;natural language;noise reduction;optimization problem;parallel computing;recommender system;refinement (computing);response time (technology);software framework	Juan A. Recio-García;Belén Díaz-Agudo;Pedro A. González-Calero	2009	2009 21st IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2009.130	case-based reasoning;cognition;computer science;noise measurement;artificial intelligence;theoretical computer science;machine learning;noise reduction;data mining	Robotics	-16.12522996052617	-51.56354501923893	67561
047bd0afb398fbba739c829e6f04cbdb62ee62db	localizing users and items from paired comparisons	standards;psychology;indexes;computational modeling;optimization;context modeling;context	Suppose that we wish to determine an embedding of points given only paired comparisons of the form “user x prefers item qi to item qj.” Such observations arise in a variety of contexts, including applications such as recommendation systems, targeted advertisement, and psychological studies. In this paper we first present an optimization-based framework for localizing new users and items when an existing embedding is known. We demonstrate that user localization can be formulated as a simple constrained quadratic program, and that although item localization produces a set of non-convex constraints, we can make the problem convex by strategically combining comparisons to produce a set of convex linear constraints. Finally, we show that by iteratively applying this method to every user and item, we can recover an accurate embedding, allowing us to iteratively improve a given embedding or even generate an embedding from scratch.	mathematical optimization;quadratic programming;recommender system	Matthew R. O'Shaughnessy;Mark A. Davenport	2016	2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2016.7738837	mathematical optimization;theoretical computer science;machine learning;mathematics	ML	-18.24290889739919	-47.14006603351457	67972
3fb733e6dc2b17b7c91403448f0979dd5febd16a	instant foodie: predicting expert ratings from grassroots	latent variables;google places;zagat survey;restaurant ratings	Consumer review sites and recommender systems typically rely on a large volume of user-contributed ratings, which makes rating acquisition an essential component in the design of such systems. User ratings are then summarized to provide an aggregate score representing a popular evaluation of an item. An inherent problem in such summarization is potential bias due to raters self-selection and heterogeneity in terms of experience, tastes and rating scale interpretation. There are two major approaches to collecting ratings, which have different advantages and disadvantages. One is to allow a large number of volunteers to choose and rate items directly (a method employed by e.g. Yelp and Google Places). Alternatively, a panel of raters may be maintained and invited to rate a predefined set of items at regular intervals (such as in Zagat Survey). The latter approach arguably results in more consistent reviews and reduced selection bias, however, at the expense of much smaller coverage (fewer rated items).  In this paper, we examine the two different approaches to collecting user ratings of restaurants and explore the question of whether it is possible to reconcile them. Specifically, we study the problem of inferring the more calibrated Zagat Survey ratings (which we dub 'expert ratings') from the user-generated ratings ('grassroots') in Google Places. To that effect, we employ latent factor models and provide a probabilistic treatment of the ordinal rankings. We can predict Zagat Survey ratings accurately from ad hoc user-generated ratings by joint optimization on two datasets. We analyze the resulting model, and find that users become more discerning as they submit more ratings. We also describe an approach towards cross-city recommendations, answering questions such as 'What is the equivalent of the Per Se restaurant in Chicago'?	aggregate data;hoc (programming language);instant messaging;jamie wilkinson;mathematical optimization;ordinal data;rating scale;recommender system;selection bias;user-generated content	Chenhao Tan;Ed Huai-hsin Chi;David A. Huffaker;Gueorgi Kossinets;Alexander J. Smola	2013		10.1145/2505515.2505712	latent variable;computer science;machine learning;data mining;database;world wide web;statistics	Web+IR	-17.28116422531569	-48.96083684277737	68234
84fa6f0cc5ea81e3b80882580ca2640ba7be4e66	experimental comparison of set intersection algorithms for inverted indexing		The set intersection problem is one of the main problems in document retrieval. Query consists of two keywords, and for each of keyword we have a sorted set of document IDs containing it. The goal is to retrieve the set of document IDs containing both keywords. We perform an experimental comparison of Galloping search and a new algorithm by Cohen and Porat (LATIN2010), which has a better theoretical time complexity. We show that the new algorithm has often worse performance than the trivial one on real data. We also propose a variant of the Cohen and Porat algorithm with a similar complexity but better empirical performance. Finally, we investigate influence of document ordering on query time.	algorithm;document retrieval;intersection algorithm;speedup;time complexity;utility	Vladimír Boza	2013			computer science;theoretical computer science;data mining;algorithm	Web+IR	-6.354767132876813	-40.33065172392823	68239
19080ed863a74164b4b80a5475f4f285154d74a2	pegasus: mining billion-scale graphs in the cloud	sparse matrices;algorithm design and analysis;data mining;symmetric matrices;matrix multiplication;cloud computing;public domain software;vectors;belief propagation;iterative methods	We have entered in an era of big data. Graphs are now measured in terabytes or even petabytes; analyzing them has become increasingly challenging. How do we find patterns and anomalies in these graphs that no longer fit in memory? How should we exploit parallel computation to boost our analysis capabilities? We present PEGASUS, the first open-source, peta-scale graph mining library, for the HADOOP platform (open-source implementation of MAPREDUCE). By observing that many graph mining operations can be described by repeated matrix-vector multiplications, we devised an important primitive called GIM-V for PEGASUS that applies to all such operations. GIM-V (Generalized Iterative Matrix-Vector multiplication) is highly optimized, achieving (1) good scale-up with the number of machines, (2) linear run time on the number of edges, and (3) more than 9 times faster performance over the non-optimized version. We ran experiments for PEGASUS on M45, one of the largest HADOOP clusters in the world. We report our findings on several real graphs with billions of nodes and edges. Selected findings include (a) the discovery of adult advertisers in the who-follows-whom on Twitter, and (b) the 7-degrees of separation in the Web graph.	apache hadoop;big data;cloud computing;computation;emoticon;experiment;iterative method;mapreduce;open-source software;parallel computing;pegasus;petabyte;run time (program lifecycle phase);structure mining;terabyte;webgraph;world wide web	U. Kang;Duen Horng Chau;Christos Faloutsos	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6289127	belief propagation;algorithm design;big data;petabyte;theoretical computer science;multiplication;cloud computing;artificial intelligence;computer science;pattern recognition;exploit;terabyte;distributed computing	DB	-9.771751173617767	-41.771139530184996	68437
38cc0ecb9df9d682e02422b570c429cf60da0c11	an attention-based collaboration framework for multi-view network representation learning		Learning distributed node representations in networks has been attracting increasing attention recently due to its effectiveness in a variety of applications. Existing approaches usually study networks with a single type of proximity between nodes, which defines a single view of a network. However, in reality there usually exists multiple types of proximities between nodes, yielding networks with multiple views. This paper studies learning node representations for networks with multiple views, which aims to infer robust node representations across different views. We propose a multi-view representation learning approach, which promotes the collaboration of different views and lets them vote for the robust representations. During the voting process, an attention mechanism is introduced, which enables each node to focus on the most informative views. Experimental results on real-world networks show that the proposed approach outperforms existing state-of-the-art approaches for network representation learning with a single view and other competitive approaches with multiple views.	feature learning;information;machine learning	Meng Qu;Jian Tang;Jingbo Shang;Xiang Ren;Ming Zhang;Jiawei Han	2017		10.1145/3132847.3133021	artificial intelligence;machine learning;computer science;data mining;voting;feature learning	AI	-14.794791775324688	-46.52085651746273	68609
1b6bbdb1ec6fd377721cbaad1957a3fe21ed2ab6	which craft is best in bioinformatics?	complete genome;comparative analysis;protein function;database;artificial intelligent;protein structure;its sequences;genome;artificial intelligence;protein folding;knowledge representation;biological complexity;ontology;biological process	'Silicon-based' biology has gathered momentum as the world-wide sequencing projects have made possible the investigation and comparative analysis of complete genomes. Central to the quest to elucidate and characterise the genes and gene products encoded within genomes are pivotal concepts concerning the processes of evolution, the mechanisms of protein folding, and, crucially, the manifestation of protein function. Our use of computers to model such concepts is limited by, and must be placed in the context of, the current limits of our understanding of these biological processes. It is important to recognise that we do not have a common understanding of what constitutes a gene; we cannot invariably say that a particular sequence or fold has arisen via divergence or convergence; we do not fully understand the rules of protein folding, so we cannot predict protein structure; and we cannot invariably diagnose protein function, given knowledge only of its sequence or structure in isolation. Accepting what we cannot do with computers plays an essential role in forming an appreciation of what we can do. Without this understanding, it is easy to be misled, as spurious arguments are often used to promote over-enthusiastic notions of what particular programs can achieve. There are valuable lessons to be learned here from the field of artificial intelligence, principal among which is the realisation that capturing and representing complex knowledge is time consuming, expensive and hard. If bioinformatics is to tackle biological complexity meaningfully, the road ahead must therefore be paved with caution, rigour and pragmatism.	artificial intelligence;bioinformatics;biological system;computer;computers;convergence (action);experiment;foundations;fractal;genome;hematological disease;holism;inspiration function;labyrinth;qualitative comparative analysis;reductionism;review [publication type];rule (guideline);throughput;protein folding	Terri K. Attwood;Crispin J. Miller	2001	Computers & chemistry	10.1016/S0097-8485(01)00069-9	protein folding;qualitative comparative analysis;biology;protein structure;computer science;bioinformatics;artificial intelligence;machine learning;ontology;mathematics;biological process;genetics;genome	AI	-6.9043250527479465	-48.29276204029259	68708
f0cc53fad782b6d87f182f8a2c12547453a57578	frequent subgraph mining from streams of linked graph structured data		Nowadays, high volumes of high-value data (e.g., semantic web data) can be generated and published at a high velocity. A collection of these data can be viewed as a big, interlinked, dynamic graph structure of linked resources. Embedded in them are implicit, previously unknown, and potentially useful knowledge. Hence, efficient knowledge discovery algorithms for mining frequent subgraphs from these dynamic, streaming graph structured data are in demand. Some existing algorithms require very large memory space to discover frequent subgraphs; some others discover collections of frequently co-occurring edges (which may be disjoint). In contrast, we propose—in this paper—algorithms that use limited memory space for discovering collections of frequently co-occurring connected edges. Evaluation results show the effectiveness of our algorithms in frequent subgraph mining from streams of linked graph structured data.	algorithm;dspace;graph (abstract data type);graph embedding;semantic web;velocity (software development)	Alfredo Cuzzocrea;Fan Jiang;Carson Kai-Sang Leung	2015			computer science;machine learning;data mining;database	ML	-9.773763135626282	-39.459408511968896	69003
97cefa2f54d30bf40b2bf71a9297949ed5a327ee	adversarial personalized ranking for recommendation		Item recommendation is a personalized ranking task. To this end, many recommender systems optimize models with pairwise ranking objectives, such as the Bayesian Personalized Ranking (BPR). Using matrix Factorization (MF) - the most widely used model in recommendation - as a demonstration, we show that optimizing it with BPR leads to a recommender model that is not robust. In particular, we find that the resultant model is highly vulnerable to adversarial perturbations on its model parameters, which implies the possibly large error in generalization. To enhance the robustness of a recommender model and thus improve its generalization performance, we propose a new optimization framework, namely Adversarial Personalized Ranking (APR). In short, our APR enhances the pairwise ranking method BPR by performing adversarial training. It can be interpreted as playing a minimax game, where the minimization of the BPR objective function meanwhile defends an adversary, which adds adversarial perturbations on model parameters to maximize the BPR objective function. To illustrate how it works, we implement APR on MF by adding adversarial perturbations on the embedding vectors of users and items. Extensive experiments on three public real-world datasets demonstrate the effectiveness of APR - by optimizing MF with APR, it outperforms BPR with a relative improvement of 11.2% on average and achieves state-of-the-art performance for item recommendation. Our implementation is available at: \urlhttps://github.com/hexiangnan/adversarial_personalized_ranking.	arp spoofing;adversary (cryptography);algorithm;apache portable runtime;bayesian network;cold start;context awareness;document retrieval;experiment;knowledge graph;loss function;mathematical optimization;minimax;optimization problem;personalization;question answering;recommender system;resultant;web search engine	Xiangnan He;Zhankui He;Xiaoyu Du;Tat-Seng Chua	2018		10.1145/3209978.3209981	data mining;computer science;recommender system;adversarial system;robustness (computer science);business process reengineering;machine learning;adversary;pairwise comparison;ranking;minimax;artificial intelligence	Web+IR	-17.739686852550964	-49.289290818849764	69162
2433eb4626a6092b9ac8e21bb48d85960c88178c	improving peer-to-peer search performance through intelligent social search	intelligent social search;selected works;social routing;social relationship;search method;p2p;social grouping;social network;large scale;network traffic;bepress;social groups;p2p networks;social search;peer to peer	0957-4174/$ see front matter 2009 Elsevier Ltd. A doi:10.1016/j.eswa.2009.01.045 * Corresponding author. Tel.: +886 3 4227151x353 E-mail address: jhyang@csie.ncu.edu.tw (S.J.H. Yan As a large amount of information is added onto the Internet on a daily basis, the efficiency of peer-to-peer (P2P) search has become increasingly important. However, how to quickly discover the right resource in a large-scale P2P network without generating too much network traffic remains highly challenging. In this paper, we propose a novel P2P search method, by applying the concept of social grouping and intelligent social search; we derive peers into social groups in a P2P network to improve search performance. Through a super-peer-based architecture, we establish and maintain virtual social groups on top of a P2P network. The interactions between the peers in the P2P network are used to incrementally build the social relationships between the peers in the associated social groups. In such a P2P network, a search query is propagated along the social groups in the overlay social network. Our preliminary experiments have demonstrated that our method can significantly shorten search routes and result in a higher peer search performance. In addition, our method also enhances the trustworthiness of search results because searches go through trusted peers. 2009 Elsevier Ltd. All rights reserved.	cluster analysis;control theory;experiment;interaction;mail (macos);network traffic control;peer-to-peer;routing;search algorithm;social network;social search;trust (emotion)	Stephen J. H. Yang;Jia Zhang;Leon Lin;Jeffrey J. P. Tsai	2009	Expert Syst. Appl.	10.1016/j.eswa.2009.01.045	organizational network analysis;social group;social heuristics;peer-to-peer;data mining;world wide web;social network	Web+IR	-19.009159202260186	-44.73104563674978	69208
107a2c807e8aacbcd9afcdeb2ddc9222ac25b15b	a search engine for 3d models	search engine;construccion arquitectura tecnologia ambiental;computacion informatica;spherical harmonic;grupo de excelencia;search method;shape representation;3d model;ciencias basicas y experimentales;shape matching;shape retrieval;tecnologias;similarity measure	As the number of 3D models available on the Web grows, there is an increasing need for a search engine to help people find them. Unfortunately, traditional text-based search techniques are not always effective for 3D data. In this article, we investigate new shape-based search methods. The key challenges are to develop query methods simple enough for novice users and matching algorithms robust enough to work for arbitrary polygonal models. We present a Web-based search engine system that supports queries based on 3D sketches, 2D sketches, 3D models, and/or text keywords. For the shape-based queries, we have developed a new matching algorithm that uses spherical harmonics to compute discriminating similarity measures without requiring repair of model degeneracies or alignment of orientations. It provides 46 to 245% better performance than related shape-matching methods during precision--recall experiments, and it is fast enough to return query results from a repository of 20,000 models in under a second. The net result is a growing interactive index of 3D models available on the Web (i.e., a Google for 3D models).	3d modeling;algorithm;degenerate energy levels;experiment;text-based (computing);web search engine;world wide web	Thomas A. Funkhouser;Patrick Min;Michael M. Kazhdan;Joyce Chen;J. Alex Halderman;David P. Dobkin;David Pokrass Jacobs	2003	ACM Trans. Graph.	10.1145/588272.588279	computer science;theoretical computer science;mathematics;world wide web;search engine;spherical harmonics	Graphics	-13.65335593962954	-50.36014217506902	69480
a4db26e4d5af46694b08294d23419fb3c77fe052	hd-gdd: high dimensional graph dominance drawing approach for reachability query	graph dominance drawing;graph reachability query;high dimensional coordinate;online search	Efficiently answering reachability queries, which checks whether one vertex can reach another in a directed graph, has been studied extensively during recent years. However, the size of the graph that people are facing and generating nowadays is growing so rapidly that simple algorithms, such as BFS and DFS, are no longer feasible. Although Refined Online Search algorithms can scale to large graphs, they all suffer from the false positive problem. In this paper, we analyze the cause of false positive and propose an efficient High Dimensional coordinate generating method based on Graph Dominance Drawing (HD-GDD) to answer reachability queries in linear or even constant time. We conduct experiments on different graph structures and different graph sizes to fully evaluate the performance and behavior of our proposal. Empirical results demonstrate that our method outperforms state-of-the-art algorithms and can handle extensive graphs.	algorithm;breadth-first search;depth-first search;directed graph;dominance drawing;experiment;game design document;graph (discrete mathematics);online search;reachability;time complexity	Lei Li;Wen Hua;Xiaofang Zhou	2016	World Wide Web	10.1007/s11280-016-0407-z	implicit graph;lattice graph;graph power;graph bandwidth;null graph;graph property;computer science;regular graph;clique-width;simplex graph;forbidden graph characterization;machine learning;comparability graph;online search;database;voltage graph;distance-hereditary graph;graph;world wide web;complement graph;graph database	DB	-9.783164361581974	-40.13018488380623	69708
0da6e590e4debeddf05b75f157576a30483e1065	data driven multi-index hashing	searching speed data driven multiindex hashing binary representation large scale nearest neighbor search binary codes hash tables nonuniform codes distribution statistics properties uk bench dataset;会议论文;statistical analysis;clustering algorithms nearest neighbor search binary codes indexing;image representation;search problems;statistical analysis image representation search problems	Binary representation for large scale nearest neighbor search received more and more concern recently. Although binary codes can be directly used as indices of the hash tables, correlations between the bits may lead to non-uniform codes distribution and reduce the performance of the hash table. In this paper, we propose a data driven multi-index hashing method for exact nearest neighbor search in Hamming space. By exploring the statistics properties of the dataset, we can separate the correlated bits into different segments during the process of building multiple hash tables, and thus make binary codes distributed as uniformly as possible in each hash table. Experiments conducted on a huge amount of binary codes extracted from the UK Bench dataset show that our method can achieve significant acceleration in searching speed for large scale dataset.	binary code;experiment;hamming space;hash function;hash table;nearest neighbor search	Ji Wan;Sheng Tang;Yongdong Zhang;Lei Huang;Jintao Li	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738550	feature hashing;hopscotch hashing;hash table;double hashing;hash function;linear hashing;perfect hash function;dynamic perfect hashing;primary clustering;open addressing;consistent hashing;theoretical computer science;universal hashing;pattern recognition;data mining;mathematics;k-independent hashing;rolling hash;nearest neighbor search;2-choice hashing;locality-sensitive hashing;statistics;hash tree	Vision	-4.80799597001015	-41.84704453326748	69917
887b3466095602b0bc215879e78d0e41fbf6242c	diversum: towards diversified summarisation of entities in knowledge graphs	databases;graph theory;imdb database diversum diversified entity summarization rdf like knowledge graphs presentation budget greedy algorithm dataset;diversified entity summarization;motion pictures;approximation algorithms;information technology;diversum;bridges;greedy algorithms;presentation budget;resource description framework;knowledge engineering graph theory greedy algorithms;rdf like knowledge graphs;redundancy;imdb database;greedy algorithm;web mining;informatics;humans;database languages;context;motion pictures web mining information technology informatics greedy algorithms resource description framework database languages data models humans bridges;knowledge based systems;dataset;data models;knowledge engineering	A problem of diversified entity summarisation in RDF-like knowledge graphs, with limited ¿presentation budget¿, is formulated and studied. A greedy algorithm that adapts previous ideas from IR is proposed and preliminary but promising experimental results on real dataset extracted from IMDB database are presented.	co-occurrence matrix;computational complexity theory;continuation;diversification (finance);document-term matrix;entity;greedy algorithm;heuristic;internet movie database (imdb);knowledge graph;numerical analysis;refinement (computing);treemapping	Marcin Sydow;Mariusz Pikula;Ralf Schenkel	2010	2010 IEEE 26th International Conference on Data Engineering Workshops (ICDEW 2010)	10.1109/ICDEW.2010.5452707	web mining;greedy algorithm;computer science;graph theory;knowledge engineering;data mining;database;information technology;information retrieval	DB	-12.599293812695745	-49.99531561517458	69922
708ff767e8c673a05c416829028ff911f79f092d	chemical reactant recommendation using a network of organic chemistry		This paper focuses on the overall task of recommending to the chemist candidate molecules (reactants) necessary to synthesize a given target molecule (product), which is a novel application as well as an important step for the chemist to find a synthesis route to generate the product. We formulate this task as a link-prediction problem over a so-called Network of Organic Chemistry (NOC) that we have constructed from 8 million chemical reactions described in the US patent literature between 1976 and 2013. We leverage state-of-the-art factorization algorithms for recommender systems to solve this task. Our empirical evaluation demonstrates that Factorization Machines, trained with chemistry-specific knowledge, outperforms current methods based on similarity of chemical structures.	algorithm;chemical database;deep learning;fingerprint;logic programming;network on a chip;protein structure prediction;recommender system;sparse language;sparse matrix	John Savage;Akihiro Kishimoto;Beat Buesser;Ernesto Diaz-Aviles;Carlos Alzate	2017		10.1145/3109859.3109895	machine learning;recommender system;artificial intelligence;chemist;computer science;factorization	Web+IR	-16.259916037712344	-50.20156005844408	70063
735dbceeba1bb84faaf2bd9058a3d0424421fc53	literature driven method for modeling frustration in an its	mindspark literature driven method its intelligent tutoring system affect based computing state identification sensor based method laboratory settings real world scenario mathematical model frustration modelling data mining correlation analysis;sensors;correlation theory;student log data;student log data affective state detection its modeling frustration;data mining;affective state detection;brain modeling;its;intelligent tutoring systems correlation theory data mining;mathematical model adaptation models sensors data mining predictive models humans brain modeling;intelligent tutoring systems;mathematical model;predictive models;humans;adaptation models;modeling frustration	In Intelligent Tutoring Systems, affect-based computing is an important research area. Common approaches to deal with the affective state identification are based on input data from external sensors such as eye-tracker and EEG, as well as methods based on mining of ITS log data. Sensor based methods are viable in laboratory settings but they are tough to implement in real-world scenario which might cater to a large number of students. In our research, we create a mathematical model of frustration based on its theoretical definition. We identify the variables in the model by applying the theoretical definition of frustration to the ITS log data. This approach is different from existing data mining techniques, which use correlation analysis with labeled data. We apply our model to Mindspark, a commercial maths Intelligent Tutoring System, used by several thousand students. We validate our model with human observations of frustration.	data mining;electroencephalography;eye tracking;mathematical model;sensor;theoretical definition	Ramkumar Rajendran;Sridhar Iyer;Sahana Murthy	2012	2012 IEEE 12th International Conference on Advanced Learning Technologies	10.1109/ICALT.2012.167	simulation;computer science;sensor;artificial intelligence;machine learning;mathematical model;database;predictive modelling;world wide web	Robotics	-10.736986103229878	-47.975387399033764	70174
2e1640021db4c1610138b4fbfde77d651fdb2286	top-k frequent induced subgraph mining on a sliding window using sampling	top k frequent subgraph;data streams;sampling subgraph;induced subgraph	Finding Frequent Induced Subgraph in a stream of graph data is critical for many application such as frequent substructures in biological networks, chemical compounds, or community detection in social networks. Some approaches have been proposed for mining frequent induced subgraph in graph database. However, existing methods take long execution time since they perform numerous subgraph isomorphism (SI) operations, thus, these approach is not efficience for mining on streaming environment. In this paper, we propose k-FISMW, a new sampling-based method for top-k Frequent Induced Subgraph Mining on a sliding Window. We use a specialized data structure called WSTable (Window Summary Table) to maintain information of recent graphs in the sliding windows. To avoid SI operations in kFISM, we present a measure, indFreq, to compute frequency of subgraphs. k-FISMW executes a biased random walkbased sampling over fixed-size vertex-induced subgraphs on the current window so that the potentially frequent subgraphs are visited with high probability. We evaluate execution time and accuracy of finding our desired types of subgraphs using k-FISMW on both real-life and synthetic dataset. We observe that our proposed method outperforms state of the art approach in execution time and accuracy.	data structure;effective method;email;experiment;graph database;induced subgraph;microsoft windows;real life;run time (program lifecycle phase);sampling (signal processing);social network;subgraph isomorphism problem;synthetic intelligence;with high probability	Van T. T. Duong;Kifayat-Ullah Khan;Young-Koo Lee	2017		10.1145/3022227.3022242	computer science;theoretical computer science;machine learning;data mining;subgraph isomorphism problem;distributed computing;induced subgraph isomorphism problem;data stream mining	DB	-9.715719346051936	-39.703346513862286	70226
6fe1a6f713ffe40a92389b6381ed517af2f9861d	graph modeling and mining methods for brain images	graph model;brain images;frequent approximate subgraph mining;graph edit distance	Brain disease is a top cause of death. Currently, its main diagonosis is to take advantage of medical brain images to analyse patients’ condition. In medical big data analysis field, it has been a research hotspot that how to effectively represent medical images and discover significant information hidden in them to further assist doctors to achieve a better diagnosis. Graphs, as one of the most general forms of data representation, can easily represent entities, their attributes and their relationships well. However, the existing medical image graph models do not exploit the specific relationships of brain images very well so that some essential information is lost. Therefore, aiming at brain images, we firstly construct a domain knowledge-oriented graph about the Topological Relationships among Ventricles and Lesions (TRVL) to represent a brain image, and give the algorithm of modeling a brain Image to a TRVL Graph (denoted as I2G). Then we propose a method named Frequent Approximate Subgraph Mining based on Graph Edit Distance (FASMGED) to exactly discover meaningful patterns hidden in brain images. This method employs a strong error-tolerant graph matching strategy which is accordant with ubiquitous noise in practice. Moreover, an approximate method of frequent approximate subgraph mining is proposed based on the greedy strategy. We have evaluated our algorithms on real and simulated data. Results show that I2G is computationally scalable, FASMGED can discover more significant patterns than other state-of-the-art frequent subgraph mining methods, and the approximate method of frequent approximate subgraph mining outperforms FASMGED.	approximation algorithm;big data;data (computing);data mining;entity;error-tolerant design;graph edit distance;greedy algorithm;matching (graph theory);mined;orientation (graph theory);scalability;trvl	Linlin Gao;Haiwei Pan;Xiaoqin Xie;Zhiqiang Zhang;Qing Li;Qilong Han	2016	Multimedia Tools and Applications	10.1007/s11042-016-3482-3	computer science;machine learning;pattern recognition;data mining;graph;complement graph	ML	-11.93648191657496	-44.749308330144785	70277
2c4071bfa44603a2a0e1634d599dc69f2b54bc62	dir-st2: delineation of imprecise regions using spatio–temporal–textual information		"""An imprecise region is referred to as a geographical area without a clearly defined boundary in the literature. Previous clustering-based approaches exploit spatial information to find such regions. However, the prior studies suffer from the following two problems: the subjectivity in selecting clustering parameters and the inclusion of a large portion of the undesirable region (i.e., a large number of noise points). To overcome these problems, we present <italic>DIR-ST</italic><sup>2</sup>, a novel framework for delineating an imprecise region by iteratively performing <italic>density-based clustering of applications with noise</italic> (DBSCAN) along with not only <italic>spatio–textual</italic> information but also <italic>temporal</italic> information on social media. Specifically, we aim at finding a proper radius of a circle used in the iterative DBSCAN process by gradually reducing the radius for each iteration in which the temporal information acquired from all resulting clusters is leveraged. Then, we propose an efficient and automated algorithm delineating the imprecise region via hierarchical clustering. Experimental results show that by virtue of the significant noise reduction in the region, our <italic>DIR-ST</italic><sup>2</sup> method outperforms the state-of-the-art approach employing one-class support vector machine in terms of the <inline-formula> <tex-math notation=""""LaTeX"""">$\mathcal {F}_{1}$ </tex-math></inline-formula> score from comparison with precisely defined regions regarded as a ground truth, and returns apparently a better delineation of imprecise regions. The computational complexity of <italic>DIR-ST</italic><sup>2</sup> is also analytically and numerically shown."""	algorithm;cluster analysis;computational complexity theory;dbscan;geographic coordinate system;ground truth;hierarchical clustering;iteration;noise reduction;numerical analysis;regular expression;social media;support vector machine	Cong Tran;Won-Yong Shin;Sang-Il Choi	2018	IEEE Access	10.1109/access.2018.2845843	support vector machine;distributed computing;computer science;noise reduction;computational complexity theory;spatial analysis;cluster analysis;dbscan;ground truth;hierarchical clustering;pattern recognition;artificial intelligence	DB	-11.942387957041733	-46.44557884882633	70537
2620e6bcdd3abfff855e8181e09d301af71f4db1	mining swarm patterns in sliding windows over moving object data streams		Several emerging applications such as traffic management and urban emergency response systems often need to identify groups from recent window of the moving object data. These requirements mandate algorithmic solutions that are time and memory efficient for adding new data incrementally and removing stale data. In this paper, we consider the problem of finding closed Swarms over a sliding window. The key challenges in computing closed Swarms over a sliding window are: (i) Search space for computing closed Swarms from new data is large (ii) Removal of old data leaves many non-closed Swarms which need to be identified and deleted. None of the existing methods are efficient in adding new data and removing old data for large datasets. This paper presents an efficient incremental graph based method for computing Swarms over sliding windows. We use a real dataset to show the performance of our method. The complexity analysis as well as experimental results demonstrate that our method is significantly faster than the existing incremental method over sliding windows with increased memory requirement. In particular, our method is shown to be 7-13 times faster with 3-5 times memory overhead in all the experiments.	algorithm;analysis of algorithms;emergency response systems;experiment;microsoft windows;overhead (computing);requirement;run time (program lifecycle phase);stream cipher;swarm;time complexity	Alka Bhushan;Umesh Bellur;Kuldeep Sharma;Srijay Deshpande;Nandlal L. Sarda	2017		10.1145/3139958.3139988	sliding window protocol;artificial intelligence;swarm behaviour;machine learning;cluster analysis;computer science;real-time computing;emergency response systems;data stream mining;graph	DB	-8.186007576675886	-38.08731752948718	70814
b14c62748f82192d15aa200550439603eae43730	qmsampler: joint sampling of multiple networks with quality guarantee		Because Online Social Networks (OSNs) have become increasingly important in the last decade, they have motivated a great deal of research on Social Network Analysis (SNA). Currently, SNA algorithms are evaluated on real datasets obtained from large-scale OSNs, which are usually sampled by Breadth-First-Search (BFS), Random Walk (RW), or some variations of the latter. However, none of the released datasets provides any statistical guarantees on the difference between the sampled datasets and the ground truth. Moreover, all existing sampling algorithms only focus on sampling a single OSN, but each OSN is actually a sampling of a complete social network. Hence, even if the whole dataset from a single OSN is sampled, the results may still be skewed and may not fully reflect the properties of the complete social network. To address the above issues, we have made the first attempt to explore the joint sampling of multiple OSNs and propose an approach called Quality-guaranteed Multi-network Sampler (QMSampler) that can jointly sample multiple OSNs. QMSampler provides a statistical guarantee on the difference between the sampled real dataset and the ground truth (the perfect integration of all OSNs). Our experimental results demonstrate that the proposed approach generates a much smaller bias than any existing method. QMSampler has also been released as a free download.	algorithm;download;ground truth;read-write memory;sampling (signal processing);shortest path problem;social network analysis;web crawler	Hong-Han Shuai;De-Nian Yang;Chih-Ya Shen;Philip S. Yu;Ming-Syan Chen	2018	IEEE Transactions on Big Data	10.1109/TBDATA.2017.2715847	random walk;data mining;artificial intelligence;social network analysis;machine learning;ground truth;big data;computer science;sampling (statistics);social network;download;gibbs sampling	Vision	-11.05073452629438	-43.47804582358875	71214
3c0b574b98e2fae687f021d77637e24e2d2d641f	fast exact shortest-path distance queries on large networks by pruned landmark labeling	web graph;shortest path;shortest paths;query processing;search space;graphs;social network;large scale;breadth first search	We propose a new exact method for shortest-path distance queries on large-scale networks. Our method precomputes distance labels for vertices by performing a breadth-first search from every vertex. Seemingly too obvious and too inefficient at first glance, the key ingredient introduced here is pruning during breadth-first searches. While we can still answer the correct distance for any pair of vertices from the labels, it surprisingly reduces the search space and sizes of labels. Moreover, we show that we can perform 32 or 64 breadth-first searches simultaneously exploiting bitwise operations. We experimentally demonstrate that the combination of these two techniques is efficient and robust on various kinds of large-scale real-world networks. In particular, our method can handle social networks and web graphs with hundreds of millions of edges, which are two orders of magnitude larger than the limits of previous exact methods, with comparable query time to those of previous methods.	bitwise operation;breadth-first search;experiment;shortest path problem;social network;vertex (geometry);web 2.0	Takuya Akiba;Yoichi Iwata;Yuichi Yoshida	2013		10.1145/2463676.2465315	combinatorics;breadth-first search;theoretical computer science;machine learning;mathematics;graph;shortest path problem;distance;social network	Web+IR	-9.723347648392116	-40.27738065572268	71295
2ae8ebe9053522a48be23d66bbc65d0ae4eaf78e	approximate graph mining with label costs	label costs;cmdb;approximate subgraph isomorphism;data mining;approximation techniques;approximate graph mining;graph analysis	Many real-world graphs have complex labels on the nodes and edges. Mining only exact patterns yields limited insights, since it may be hard to find exact matches. However, in many domains it is relatively easy to define a cost (or distance) between different labels. Using this information, it becomes possible to mine a much richer set of approximate subgraph patterns, which preserve the topology but allow bounded label mismatches. We present novel and scalable methods to efficiently solve the approximate isomorphism problem. We show that approximate mining yields interesting patterns in several real-world graphs ranging from IT and protein interaction networks to protein structures.	approximation algorithm;graph isomorphism problem;scalability;structure mining	Pranay Anchuri;Mohammed J. Zaki;Omer Barkol;Shahar Golan;Moshe Shamy	2013		10.1145/2487575.2487602	configuration management database;combinatorics;power graph analysis;computer science;machine learning;data mining;subgraph isomorphism problem;mathematics;induced subgraph isomorphism problem;molecule mining	ML	-10.007101566698287	-39.79033940051714	71461
f9ec34d23a595c8382d9ee5afbd709304e9256f3	conflict graph based community detection	community detection;complex networks transforms detection algorithms image edge detection measurement wireless mesh networks;complex networks;ctcd;clustering coefficient networks community structure analysis conflict graph transform based community detection ctcd real world networks 1 hop influences 2 hop influences;ctcd complex networks community detection surprise average clustering coefficient;radio links complex networks graph theory;average clustering coefficient;surprise	Community is a network's subgraph where vertices share similar properties and reflect interesting characteristics for understanding complex networks more closely. Therefore, community structure analysis is important in understanding and exploring complex networks and helps in describing relationship among nodes in a network. However, efficiently finding communities in a complex network still remains an open problem. Since there exists numerous ways of defining a community, existing strategies have adopted different parameters to reflect varied behavior of a community structure and trying to give a coarser or finer community distribution. In this paper, we propose Conflict graph Transform based Community Detection (CTCD) strategy to improve the quality of community distributions. CTCD focuses on the impact of degree of influence to detect more favorable community partitions. A well known measure, known as Surprise, is used to evaluate and compare the quality of the community distributions obtained using CTCD. Finally, in order to study the performance and usefulness of our strategy, CTCD is applied in real-world networks. Using CTCD, we are able to obtain better community distributions with higher Surprise value in real-world networks. We observe that 1-hop and 2-hop influences improve the Surprise value in higher and lower average clustering coefficient networks, respectively. Moreover, CTCD can efficiently extract the hierarchical nature of communities within networks.	clustering coefficient;complex network;expectation–maximization algorithm;serializability	Priti Singh;Abhishek Chakraborty;B. S. Manoj	2016	2016 8th International Conference on Communication Systems and Networks (COMSNETS)	10.1109/COMSNETS.2016.7439934	evolving networks;computer science;artificial intelligence;machine learning;data mining;clustering coefficient;clique percolation method;community structure;complex network	Web+IR	-15.486149072717993	-42.19888206097845	71557
920a7377936ceaefb31ca7c91ac664704a31afdf	the interplay between dynamics and networks: centrality, communities, and cheeger inequality	graph theory;community detection;cheeger s inequality;spectral clustering;centrality	We study the interplay between a dynamic process and the structure of the network on which it is defined. Specifically, we examine the impact of this interaction on the quality-measure of network clusters and node centrality. This enables us to effectively identify network communities and important nodes participating in the dynamics. As the first step towards this objective, we introduce an umbrella framework for defining and characterizing an ensemble of dynamic processes on a network. This framework generalizes the traditional Laplacian framework to continuous-time biased random walks and also allows us to model some epidemic processes over a network. For each dynamic process in our framework, we can define a function that measures the quality of every subset of nodes as a potential cluster (or community) with respect to this process on a given network. This subset-quality function generalizes the traditional conductance measure for graph partitioning. We partially justify our choice of the quality function by showing that the classic Cheeger's inequality, which relates the conductance of the best cluster in a network with a spectral quantity of its Laplacian matrix, can be extended from the Laplacian-conductance setting to this more general setting.	centrality;conductance (graph);graph partition;laplacian matrix;social inequality	Rumi Ghosh;Shang-Hua Teng;Kristina Lerman;Xiaoran Yan	2014		10.1145/2623330.2623738	conductance;network science;mathematical optimization;combinatorics;discrete mathematics;graph theory;alpha centrality;machine learning;mathematics;centrality;spectral clustering	ML	-15.318352812218034	-40.97157075396542	71854
28b76c185f86cbbb47dcd72813c805f99674df54	community number estimation for community detection in complex networks.		Most current community detection methods for complex networks focus on partition. Community number estimation does not have the due attention it deserves, and the community number is only a by-product of community partition. In fact, knowing the community number in advance can speed up the partition process, especially for large scale and dynamic complex networks. This paper proposes a community number estimation method based on topology potential. In the topology potential field, the potential distribution of nodes shows a natural peak-valley structure, and each community corresponds to a local high potential area. The number of local maximum potential nodes is the estimated community number. Experiments on real world networks and artificial networks show that the proposed method gives very good performance in community number estimation. The more noticeable the peak-valley structure of the corresponding topology potential field is, the closer the estimated community number will be to the ground truth. Furthermore, compared with state-of-the-arts methods, our proposed method is not sensitive to the tuned parameter, and shows good efficiency.	complex network;experiment;ground truth;lan manager;maxima and minima;principle of maximum entropy	Zhixiao Wang;Jingke Xi;Yan Xing;Zhiguo Hu	2017	J. Inf. Sci. Eng.		computer science;distributed computing;complex network	Web+IR	-13.703809970529374	-42.15899478376098	71883
2eb6ea0f258d777f8e701040e70409bd666fd4b3	gorder: an efficient method for knn join processing	range similarity;data mining;block nested loop;high-dimensional data;g-ordering knn;efficient knn-join method;efficient method;cpu cost;expensive primitive operation;similarity search;g-ordered data;nested loops;k nearest neighbor;high dimensional data	An important but very expensive primitive operation of high-dimensional databases is the KNearest Neighbor (KNN) similarity join. The operation combines each point of one dataset with its KNNs in the other dataset and it provides more meaningful query results than the range similarity join. Such an operation is useful for data mining and similarity search. In this paper, we propose a novel KNN-join algorithm, called theGorder(or the G-ordering KNN) join method. Gorder is a block nested loop join method that exploits sorting, join scheduling and distance computation filtering and reduction to reduce both I/O and CPU costs. It sorts input datasets into theG-order and applied thescheduled block nested loop join on the G-ordered data. The distance computation reduction is employed to further reduce CPU cost. It is simple and yet efficient, and handles high-dimensional data efficiently. Extensive experiments on both synthetic cluster and real life datasets were conducted, and the results illustrate that Gorder is an efficient KNN-join method and outperforms existing methods by a wide margin.	(1+ε)-approximate nearest neighbor search;anomaly detection;block nested loop;central processing unit;cluster analysis;computation;control flow;data mining;database;datasheet;dhrystone;experiment;input/output;join (sql);k-nearest neighbors algorithm;nested loop join;real life;scalability;scheduling (computing);similarity search;sorting;statistical classification;synthetic data	Chenyi Xia;Hongjun Lu;Beng Chin Ooi;Jin Hu	2004			hash join;nested loop join;computer science;block nested loop;machine learning;data mining;database;k-nearest neighbors algorithm;clustering high-dimensional data	DB	-4.584728780836549	-39.04726814056985	72038
1b0ec6561d650f30a177727b674ee2e5c52e4443	modeling complex metabolic reactions, ecological systems, and financial and legal networks with miann models based on markov-wiener node descriptors		The use of numerical parameters in Complex Network analysis is expanding to new fields of application. At a molecular level, we can use them to describe the molecular structure of chemical entities, protein interactions, or metabolic networks. However, the applications are not restricted to the world of molecules and can be extended to the study of macroscopic nonliving systems, organisms, or even legal or social networks. On the other hand, the development of the field of Artificial Intelligence has led to the formulation of computational algorithms whose design is based on the structure and functioning of networks of biological neurons. These algorithms, called Artificial Neural Networks (ANNs), can be useful for the study of complex networks, since the numerical parameters that encode information of the network (for example centralities/node descriptors) can be used as inputs for the ANNs. The Wiener index (W) is a graph invariant widely used in chemoinformatics to quantify the molecular structure of drugs and to study complex networks. In this work, we explore for the first time the possibility of using Markov chains to calculate analogues of node distance numbers/W to describe complex networks from the point of view of their nodes. These parameters are called Markov-Wiener node descriptors of order k(th) (W(k)). Please, note that these descriptors are not related to Markov-Wiener stochastic processes. Here, we calculated the W(k)(i) values for a very high number of nodes (>100,000) in more than 100 different complex networks using the software MI-NODES. These networks were grouped according to the field of application. Molecular networks include the Metabolic Reaction Networks (MRNs) of 40 different organisms. In addition, we analyzed other biological and legal and social networks. These include the Interaction Web Database Biological Networks (IWDBNs), with 75 food webs or ecological systems and the Spanish Financial Law Network (SFLN). The calculated W(k)(i) values were used as inputs for different ANNs in order to discriminate correct node connectivity patterns from incorrect random patterns. The MIANN models obtained present good values of Sensitivity/Specificity (%): MRNs (78/78), IWDBNs (90/88), and SFLN (86/84). These preliminary results are very promising from the point of view of a first exploratory study and suggest that the use of these models could be extended to the high-throughput re-evaluation of connectivity in known complex networks (collation).		Aliuska Duardo-Sánchez;Cristian R. Munteanu;Pablo Riera-Fernández;Antonio López-Díaz;Alejandro Pazos;Humberto González Díaz	2014	Journal of chemical information and modeling	10.1021/ci400280n	computer science;bioinformatics;artificial intelligence;theoretical computer science;machine learning	ML	-16.109556614426555	-40.05532139285634	72068
3473dc986e2ccc19348eb2baf6a54c56765d9a48	complex patterns in dynamic attributed graphs		In recent years, there has been huge growth in the amount of graph data generated from various sources. These types of data are often represented by vertices and edges in a graph with real-valued attributes, topological properties, and temporal information associated with the vertices. Until recently, most pattern mining techniques focus solely on vertex attributes, topological properties, or a combination of these in a static sense; mining attribute and topological changes simultaneously over time has largely been overlooked. In this work-in-progress paper, we propose to extend an existing state-of-the-art technique to mine for patterns in dynamic attributed graphs which appear to trigger changes in attribute values.	data mining;vertex (computer graphics);vertex (geometry);vertex (graph theory)	Rina Singh;Jeffrey A. Graves;Douglas A. Talbert	2016		10.1145/2872518.2889374	data mining;topological sorting	DB	-10.747868369565449	-38.80402544668279	72192
daf675cc27dc732fcff1252842b809e2536be5c1	a reward-and-punishment-based approach for concept detection using adaptive ontology rules	concept detection;dynamic correlation;adaptive ontology rules;model;multimedia data mining	Despite the fact that performance improvements have been reported in the last years, semantic concept detection in video remains a challenging problem. Existing concept detection techniques, with ontology rules, exploit the static correlations among primitive concepts but not the dynamic spatiotemporal correlations. The proposed method rewards (or punishes) detected primitive concepts using dynamic spatiotemporal correlations of the given ontology rules and updates these ontology rules based on the accuracy of detection. Adaptively learned ontology rules significantly help in improving the overall accuracy of concept detection as shown in the experimental result.		Chidansh Amitkumar Bhatt;Pradeep K. Atrey;Mohan S. Kankanhalli	2013	TOMCCAP	10.1145/2457450.2457452	computer science;machine learning;data mining;ontology-based data integration;information retrieval;process ontology	DB	-10.890382267292017	-50.856628990354515	72356
b57ae10c06fcb3be38b32261ae655dc1578bc2ff	a model-based analysis of the minimum size of demographically-viable hunter-gatherer populations		A non-spatial agent-based model is used to explore how marriage behaviors and fertility affect the minimum population size required for hunter-gatherer systems to be demographically viable. The model incorporates representations of person- and household-level constraints and behaviors affecting marriage, reproduction, and mortality. Results suggest that, under a variety of circumstances, a stable population size of about 150 persons is demographically viable in the sense that it is largely immune from extinction through normal stochastic perturbations in mortality, fertility, and sex ratio. Less restrictive marriage rules enhance the viability of small populations by making it possible to capitalize on a greater proportion of the finite female reproductive span and compensate for random fluctuations in the balance of males and females.	population	Andrew White	2017	J. Artificial Societies and Social Simulation		population size;hunter-gatherer;small population size;extinction;ecology;fertility;sex ratio;biology	AI	-4.64884576458591	-46.186500267123336	72966
3f06e681174988dc07a459f9154a32fcf6862d5c	factor analysis of data matrices : new theoretical and computational aspects with applications			computational problem;factor analysis	Steffen Unkel	2009				ML	-9.241501802896908	-47.1710968473531	73390
a926fd9daff1e272c09b1fc4c544478126996d65	choose the damping, choose the ranking?	web;pagerank;link analysis;ranking;graph;citeseer;ranking algorithm;damping factor	To what extent can changes in PageRank's damping factor affect node ranking? We prove that, at least on some graphs, the top k nodes assume all possible k! orderings as the damping factor varies, even if it varies within an arbitrarily small interval (e.g. [0.84999,0.85001]). Thus, the rank of a node for a given (finite set of discrete) damping factor(s) provides very little information about the rank of that node as the damping factor varies over a continuous interval. We bypass this problem introducing lineage analysis and proving that there is a simple condition, with a ''natural'' interpretation independent of PageRank, that allows one to verify ''in one shot'' if a node outperforms another simultaneously for all damping factors and all damping variables (informally, time variant damping factors). The novel notions of strong rank and weak rank of a node provide a measure of the fuzziness of the rank of that node, of the objective orderability of a graph's nodes, and of the quality of results returned by different ranking algorithms based on the random surfer model. We deploy our analytical tools on a 41M node snapshot of the .it Web domain and on a 0.7M node snapshot of the CiteSeer citation graph. Among other findings, we show that rank is indeed relatively stable in both graphs; that ''classic'' PageRank (d=0.85) marginally outperforms Weighted In-degree (d->0), mainly due to its ability to ferret out ''niche'' items; and that, for both the Web and CiteSeer, the ideal damping factor appears to be 0.8-0.9 to obtain those items of high importance to at least one (model of randomly surfing) user, but only 0.5-0.6 to obtain those items important to every (model of randomly surfing) user.		Marco Bressan;Enoch Peserico	2010	J. Discrete Algorithms	10.1016/j.jda.2009.11.001	mathematical optimization;combinatorics;link analysis;damping factor;ranking;theoretical computer science;mathematics;distributed computing;graph	Theory	-11.161250855034158	-42.56170421019904	73462
331e100b417847b545110eda61aad6ae1426acef	bayesian block modelling for weighted networks	variational bayes;bayesian approach;block modelling;social network;community networks;social network analysis;applications	This paper presents a Bayesian approach to block modelling weighted networks to identify role assignments. This data arises commonly in many forms of social networks where we have a count of the number of communications between users. By using Variational Bayes techniques, we are able to perform fast approximate posterior inference that allows us to recover the underlying role groups in the network and their interactions. We apply our method to synthetic and real communication networks, in particular the Enron email data set.	approximation algorithm;bayesian network;email;hill climbing;interaction;simulated annealing;social network;stochastic block model;synthetic intelligence;telecommunications network;variational principle;weighted network	Ian Gallagher	2010		10.1145/1830252.1830260	social network analysis;social science;bayesian probability;computer science;machine learning;data mining;information technology;statistics;social network	ML	-17.435098782970734	-43.45988927466388	73758
4382f324979d54568a018c8b0973575e32a5e8d8	an overlapped community partition algorithm based on line graph	community partition;overlapped nodes;spectral analysis;line graph	Overlapped communities detection in complex networks is one of the most intensively investigated problems in recent years. In order to accurately detect the overlapped communities in these networks, an algorithm using edge features, namely SAEC, is proposed. The algorithm transforms topology graph of nodes into line graph of edges and calculates the similarity matrix between nodes, then the edges are clustered using spectral analysis, thus we classify the edges into corresponding communities. According to the attached communities of edges, we cluster the nodes incident with the edges again to find the overlapped nodes among the communities. Experiments on randomly generated and real networks validate the algorithm.	algorithm;line graph	Zhenyu Zhang;Zhen Zhang;Wenzhong Yang;Xiaohong Wu	2013		10.1007/978-3-642-38562-9_28	multiple edges;graph partition;force-directed graph drawing;mixed graph;multigraph;machine learning;path;complement graph;line graph;strength of a graph	Graphics	-14.009699054638554	-42.277215119415075	73808
ec3f4d1856b5e6ec0488602941487d4523037bde	link prediction in graph construction for supervised and semi-supervised learning	pattern classification graph theory learning artificial intelligence;link prediction graph construction graph based classification;cancer;weight measurement image edge detection proteins zinc cancer blood;link prediction;graph based classification;semisupervised classification semisupervised learning flat data sets graph based algorithms graph construction link prediction measures graph structure minimum spanning tree graph edges structural similarity graph connectivity;weight measurement;proteins;image edge detection;conferenceobject;blood;zinc;graph construction	Many real-world domains are relational in nature since they consist of a set of objects related to each other in complex ways. However, there are also flat data sets and if we want to apply graph-based algorithms, it is necessary to construct a graph from this data. This paper aims to: i) increase the exploration of graph-based algorithms and ii) proposes new techniques for graph construction from flat data. Our proposal focuses on constructing graphs using link prediction measures for predicting the existence of links between entities from an initial graph. Starting from a basic graph structure such as a minimum spanning tree, we apply a link prediction measure to add new edges in the graph. The link prediction measures considered here are based on structural similarity of the graph that improves the graph connectivity. We evaluate our proposal for graph construction in supervised and semi-supervised classification and we confirm the graphs achieve better accuracy.	algorithm;analysis of algorithms;attribute-value system;baseline (configuration management);connectivity (graph theory);entity;file spanning;minimum spanning tree;semi-supervised learning;semiconductor industry;social network;sparse matrix;structural similarity;supervised learning;time complexity	Lilian Berton;Jorge Carlos Valverde-Rebaza;Alneu de Andrade Lopes	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280543	graph power;edge-transitive graph;factor-critical graph;combinatorics;geometric graph theory;null graph;graph property;clique-width;simplex graph;machine learning;pattern recognition;zinc;mathematics;voltage graph;distance-hereditary graph;graph;butterfly graph;quartic graph;complement graph;line graph;strength of a graph;cancer	AI	-10.297162910530782	-40.04191210062983	73850
c6e0572181fa661e3eb4000c867a03de726c0c12	music classification by transductive learning using bipartite heterogeneous networks		The popularization of music distribution in electronic format has increased the amount of music with incomplete metadata. The incompleteness of data can hamper some important tasks, such as music and artist recommendation. In this scenario, transductive classification can be used to classify the whole dataset considering just few labeled instances. Usually transductive classification is performed through label propagation, in which data are represented as networks and the examples propagate their labels through their connections. Similarity-based networks are usually applied to model data as network. However, this kind of representation requires the definition of parameters, which significantly affect the classification accuracy, and presents a high cost due to the computation of similarities among all dataset instances. In contrast, bipartite heterogeneous networks have appeared as an alternative to similarity-based networks in text mining applications. In these networks, the words are connected to the documents which they occur. Thus, there is no parameter or additional costs to generate such networks. In this paper, we propose the use of the bipartite network representation to perform transductive classification of music, using a bag-of-frames approach to describe music signals. We demonstrate that the proposed approach outperforms other music classification approaches when few labeled instances are available.	computation;location-based service;semantic network;software propagation;statistical classification;text mining;transduction (machine learning)	Diego Furtado Silva;Rafael Geraldeli Rossi;Solange Oliveira Rezende;Gustavo E. A. P. A. Batista	2014			machine learning;artificial intelligence;heterogeneous network;metadata;computation;transduction (machine learning);computer science;bipartite graph;pattern recognition;text mining	ML	-14.60613526040615	-46.996971749324544	74039
4f180e8a1a2ea4b182719318bd6fe4380aa4f52c	mining outlying aspects on numeric data	subspace search;kernel density estimation;outlyingness degree;outlying aspect	When we are investigating an object in a data set, which itself may or may not be an outlier, can we identify unusual (i.e., outlying) aspects of the object? In this paper, we identify the novel problem of mining outlying aspects on numeric data. Given a query object $$o$$ o in a multidimensional numeric data set $$O$$ O , in which subspace is $$o$$ o most outlying? Technically, we use the rank of the probability density of an object in a subspace to measure the outlyingness of the object in the subspace. A minimal subspace where the query object is ranked the best is an outlying aspect. Computing the outlying aspects of a query object is far from trivial. A naïve method has to calculate the probability densities of all objects and rank them in every subspace, which is very costly when the dimensionality is high. We systematically develop a heuristic method that is capable of searching data sets with tens of dimensions efficiently. Our empirical study using both real data and synthetic data demonstrates that our method is effective and efficient.	heuristic;level of measurement;naivety;synthetic data	Lei Duan;Guanting Tang;Jian Pei;James Bailey;Akiko Campbell;Changjie Tang	2014	Data Mining and Knowledge Discovery	10.1007/s10618-014-0398-2	kernel density estimation;computer science;machine learning;pattern recognition;data mining;mathematics;statistics	ML	-6.851549995381225	-40.82761687016388	74845
9ce23beef40f9a59771ebd2e305d638f0163efdf	on summarizing graph streams		Graph streams, which refer to the graph with edges being updated sequentially in a form of a stream, have wide applications such as cyber security, social networks and transportation networks. This paper studies the problem of summarizing graph streams. Specifically, given a graph stream G, directed or undirected, the objective is to summarize G as SG with much smaller (sublinear) space, linear construction time and constant maintenance cost for each edge update, such that SG allows many queries over G to be approximately conducted efficiently. Due to the sheer volume and highly dynamic nature of graph streams, summarizing them remains a notoriously hard, if not impossible, problem. The widely used practice of summarizing data streams is to treat each element independently by e.g., hashor sampling-based method, without keeping track of the connections between elements in a data stream, which gives these summaries limited power in supporting complicated queries over graph streams. This paper discusses a fundamentally different philosophy for summarizing graph streams. We present gLava, a probabilistic graph model that, instead of treating an edge (a stream element) as the operating unit, uses the finer grained node in an element. This will naturally form a new graph sketch where edges capture the connections inside elements, and nodes maintain relationships across elements. We discuss a wide range of supported graph queries and establish theoretical error bounds for basic queries.	computer security;graph (discrete mathematics);graph theory;sampling (signal processing);social network;suicidegirls	Nan Tang;Prasenjit Mitra	2015	CoRR		null graph;computer science;theoretical computer science;data mining;database;graph;random geometric graph;complement graph;graph database;strength of a graph	DB	-10.804219054783559	-39.34841996436674	75096
be8e2dcbe97f3e405d35da468db85b767bce0e9a	mining top-k sequential patterns in database graphs: a new challenging problem and a sampling-based approach		In many real world networks, a vertex is usually associated with a transaction database that comprehensively describes the behaviour of the vertex. A typical example is the social network, where the behaviour of every user is depicted by a transaction database that stores his daily posted contents. A transaction database is a set of transactions, where a transaction is a set of items. Every path of the network is a sequence of vertices that induces multiple sequences of transactions. The sequences of transactions induced by all of the paths in the network forms an extremely large sequence database. Finding frequent sequential patterns from such sequence database discovers interesting subsequences that frequently appear in many paths of the network. However, it is a challenging task, since the sequence database induced by a database graph is too large to be explicitly induced and stored. In this paper, we propose the novel notion of database graph, which naturally models a wide spectrum of real world networks by associating each vertex with a transaction database. Our goal is to find the top-k frequent sequential patterns in the sequence database induced from a database graph. We prove that this problem is #P-hard. To tackle this problem, we propose an efficient two-step sampling algorithm that approximates the top-k frequent sequential patterns with provable quality guarantee. Extensive experimental results on synthetic and real-world data sets demonstrate the effectiveness and efficiency of our method.	algorithm;data mining;database transaction;experiment;p (complexity);provable security;sampling (signal processing);scalability;sequence database;sequential pattern mining;sharp-p-complete;social network;synthetic data;synthetic intelligence;transaction processing	Mingtao Lei;Lingyang Chu;Zhefeng Wang	2018	CoRR		data mining;database;sequence database;computer science;sampling (statistics);vertex (geometry);social network;graph;database transaction;data set	DB	-9.195124581712426	-38.18495207959249	75118
30077d66811f3bdb3df69c0d087db4d7a3304692	jak-stat signalling as example for a database-supported modular modelling concept	jak-stat signalling;modelling approach;detailed model;non-trivial case study;database-supported modular modelling concept;new database-supported modular modeling;advanced biomodel engineering;numerous option;executable model;individual protein;jak-stat pathway	We present a detailed model of the JAK-STAT pathway in IL-6 signaling as non-trivial case study demonstrating a new databasesupported modular modeling method. A module is a self-contained and autonomous Petri net, centred around an individual protein. The modelling approach allows to easily generate and modify coherent, executable models composed from a collection of modules and provides numerous options for advanced biomodel engineering.	autonomous robot;cell signaling;coherence (physics);executable;gene regulatory network;petri net;stat (system call)	Mary Ann Blätke;Anna Dittrich;Monika Heiner;Fred Schaper;Wolfgang Marwan	2012		10.1007/978-3-642-33636-2_21	simulation;computer science;theoretical computer science	AI	-5.861670972746196	-50.32458082932148	75174
94634508705edb0efad4aa2543c26c9caa5fdf26	dimension reduction with meta object-groups for efficient image retrieval	dimension reduction;algorithm;clustering;image retrieval	Bag-of-Word (BoW) has been a prominent form for representing visual content such as image and video in recent years, as a result of its unique capability of characterizing visual content in a picture-level while still preserving part of the object-level information. However, it is also noticed that the dimensionality of BoW is usually as high as a few hundreds or even thousands, posing a serious challenge for any application that requires efficient processing. In this paper we propose a dimension reduction method called Meta objectGroup Component (MGC) to tackle this problem. MGC aims at discovering the hidden relations of objects by examining the correlations between dimensions in the BoW features and maximizing the relations of the members in a meta object-group. By exchanging message passing between object-groups, meta objectgroups are identified for a dataset. A meta object-group does not only contain visually similar objects, but also includes objects that are likely to co-occur with each other. As the meta object groups are obtained, group-specific dimension reduction is performed to obtain denser representations for efficient retrieval. We evaluate the framework on the NUS-Wide image dataset with approximately 270,000 images represented by BoW features, and demonstrate its advantage over existing method. & 2015 Elsevier B.V. All rights reserved.	dimensionality reduction;high- and low-level;image retrieval;level structure;message passing;real life	Shuo Shang;Jiajun Liu;Kun Zhao;Mingrui Yang;Kai Zheng;Ji-Rong Wen	2015	Neurocomputing	10.1016/j.neucom.2014.08.105	computer vision;image retrieval;computer science;theoretical computer science;machine learning;data mining;mathematics;cluster analysis;dimensionality reduction	Vision	-14.866838689207524	-46.674059995934265	76162
466793f1952867929f402b95367074435c86cfc1	community structure of the physical review citation network	89 65 s;citation network;data analysis;05 40 a;02 50 ey;community structure;physical review;modularity;05 50 q	We investigate the community structure of physics subfields in the citation network of all Physical Review publications between 1893 and August 2007. We focus on well-cited publications (those receiving more than 100 citations), and apply modularity maximization to uncover major communities that correspond to clearly identifiable subfields of physics. While most of the links between communities connect those with obvious intellectual overlap, there sometimes exist unexpected connections between disparate fields due to the development of a widely applicable theoretical technique or by cross fertilization between theory and experiment. We also examine communities decade by decade and also uncover a small number of significant links between communities that are widely separated in time. © 2010 Elsevier Ltd. All rights reserved.	citation network;expectation–maximization algorithm	P. Chen;Sidney Redner	2010	J. Informetrics	10.1016/j.joi.2010.01.001	computer science;bioinformatics;data science;data mining;modularity;mathematics;data analysis;community structure;statistics	AI	-16.57557800654369	-39.74398470844249	76293
454fd3f8eae48aaf5a6402dfefc562ae924b8476	egnat: a fully dynamic metric access method for secondary memory	nearest neighbor searches;storage management data structures;metric space;egnat;storage management;construction industry;data mining;ghost hyperplanes;indexes;dataset analysis;ghost hyperplanes egnat metric access secondary memory data structure;data structures;extraterrestrial measurements data structures costs degradation application software computer science pattern recognition data mining multimedia databases nearest neighbor searches;indexation;cophir dataset;secondary memory;extraterrestrial measurements;access method;metric access;data structure;mpeg 7;visual descriptors	We introduce a novel metric space search data structure called EGNAT, which is fully dynamic and designed for secondary memory. The EGNAT is based on Brin's GNAT static index, and partitions the space according to hyperplanes. The EGNAT implements deletions using a novel technique dubbed Ghost Hyperplanes, which is of independent interest for other metric space indexes. We show experimentally that the EGNAT is competitive with the M-tree, the baseline for this scenario.	auxiliary memory;baseline (configuration management);experiment;gnat;m-tree;search data structure	Roberto Uribe;Gonzalo Navarro	2009	2009 Second International Workshop on Similarity Search and Applications	10.1109/SISAP.2009.20	auxiliary memory;database index;topology;data structure;metric space;computer science;theoretical computer science;machine learning;visual descriptors;data mining;database;mathematics;geometry;programming language;access method;world wide web;algorithm	DB	-4.92189125422256	-41.87258660516317	76720
c1583088c27b83340a59100512ca5a4469df6929	query-specific signature selection for efficient k-nearest neighbour approximation	locality sensitive hashing;k nearest neighbour search;k nearest neighbour graph	Finding k-nearest neighbours (k-NN) is one of the most important primitives of many applications such as search engines and recommendation systems. However, its computational cost is extremely high when searching for k-NN points in a huge collection of highdimensional points. Locality-sensitive hashing (LSH) has been introduced for an efficient k-NN approximation, but none of the existing LSH approaches clearly outperforms others. We propose a novel LSH approach, Signature Selection LSH (S2LSH), which finds approximate k-NN points very efficiently in various datasets. It first constructs a large pool of highly diversified signature regions with various sizes. Given a query point, it dynamically generates a query-specific signature region by merging highly effective signature regions selected from the signature pool. We also suggest S2LSH-M, a variant of S2LSH, which processes multiple queries more efficiently by using query-specific features and optimization techniques. Extensive experiments show the performance superiority of our approaches in diverse settings.	algorithmic efficiency;approximation algorithm;computation;experiment;k-nearest neighbors algorithm;locality of reference;locality-sensitive hashing;mathematical optimization;organizing (structure);recommender system;run time (program lifecycle phase);self-organization;web search engine;lsh	Youngki Park;Heasoo Hwang;Sang-goo Lee	2017	J. Information Science	10.1177/0165551516644176	computer science;recommender system;locality-sensitive hashing;data mining;merge (version control);search engine;hash function;machine learning;artificial intelligence;pattern recognition	DB	-5.139009697649118	-40.21853303319439	77140
a224b890d9b8c116b46eea0aea64400a2e2f0620	centrality measures on big graphs: exact, approximated, and distributed algorithms	streams;closeness;sampling;betweenness;centrality;mapreduce;tutorial	Centrality measures allow to measure the relative importance of a node or an edge in a graph w.r.t. other nodes or edges. Several measures of centrality have been developed in the literature to capture different aspects of the informal concept of importance, and algorithms for these different measures have been proposed. In this tutorial, we survey the different definitions of centrality measures and the algorithms to compute them. We start from the most common measures, such as closeness centrality and betweenness centrality, and move to more complex ones such as spanningedge centrality. In our presentation, we begin from exact algorithms and then progress to approximation algorithms, including sampling-based ones, and to highly-scalable MapReduce algorithms for huge graphs, both for exact computation and for keeping the measures up-to-date on dynamic graphs where edges are inserted or removed over time. Our goal is to show how advanced algorithmic techniques and scalable systems can be used to obtain efficient algorithms for an important graph mining task, and to encourage research in the area by highlighting open problems and possible directions.	approximation algorithm;betweenness centrality;closeness centrality;computation;distributed algorithm;mapreduce;sampling (signal processing);scalability;structure mining	Francesco Bonchi;Gianmarco De Francisci Morales;Matteo Riondato	2016		10.1145/2872518.2891063	network theory;network science;sampling;theoretical computer science;alpha centrality;machine learning;network controllability;centrality;streams;betweenness centrality	DB	-11.18249249034038	-40.10687618176478	77160
0e4116e23184af7ee39c9abc7ec65476ad48f6fc	a new label propagation with dams	social network services;intention behavior gap;complexity theory;standards;detection algorithms;innovation diffusionl;agent based model;image edge detection;dynamic electricity tariffs;algorithm design and analysis;partitioning algorithms	Label propagation is one of the fastest methods for community detection, with a near linear time complexity. It's a local method, where each node interacts with its neighbourhood to change its own label. Unfortunately, this method has two major drawbacks. The first is a bad propagation which can lead to obtain huge communities without sense (monster communities problem). The second is the instability of the method. Each trial of a label propagation algorithm gives rarely the same result. In this paper, we propose to do a study on the label propagation by putting artificial dams on edges of some networks in order to limit the propagation and to observe if this can lead to better results. We then apply an existing method based on a co-occurrence frequency matrix in order to stabilise the algorithm.	fastest;instability;label propagation algorithm;software propagation;time complexity	Jean-Philippe Attal;Maria Malek	2015	2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1145/2808797.2808826	algorithm design;simulation;computer science;artificial intelligence;machine learning;data mining;world wide web;algorithm	ML	-15.619504394428994	-42.83455844603123	77316
508290ff7871ffb22595c1cf01321fc66dc669a3	seq2slate: re-ranking and slate optimization with rnns		Ranking is a central task in machine learning and information retrieval. In this task, it is especially important to present the user with a slate of items that is appealing as a whole. This in turn requires taking into account interactions between items, since intuitively, placing an item on the slate affects the decision of which other items should be placed alongside it. In this work, we propose a sequence-to-sequence model for ranking called seq2slate. At each step, the model predicts the next `bestu0027 item to place on the slate given the items already selected. The sequential nature of the model allows complex dependencies between the items to be captured directly in a flexible and scalable way. We show how to learn the model end-to-end from weak supervision in the form of easily obtained click-through data. We further demonstrate the usefulness of our approach in experiments on standard ranking benchmarks as well as in a real-world recommendation system.	benchmark (computing);end-to-end principle;experiment;information retrieval;interaction;machine learning;pointer (computer programming);random neural network;recommender system;scalability;semantics (computer science);transformer	Irwan Bello;Sayali Kulkarni;Sagar Jain;Craig Boutilier;Ed Huai-hsin Chi;Elad Eban;Xiyang Luo;Alan Mackey;Ofer Meshi	2018	CoRR		data mining;recommender system;computer science;scalability;ranking	AI	-18.201026185510752	-49.361391325881264	77475
47206e9c937a1ae4821eace726b97b8ca266c20e	human motion prediction under social grouping constraints		Accurate long-term prediction of human motion in populated spaces is an important but difficult task for mobile robots and intelligent vehicles. What makes this task challenging is that human motion is influenced by a large variety of factors including the person's intention, the presence, attributes, actions, social relations and social norms of other surrounding agents, and the geometry and semantics of the environment. In this paper, we consider the problem of computing human motion predictions that account for such factors. We formulate the task as an MDP planning problem with stochastic policies and propose a weighted random walk algorithm in which each agent is locally influenced by social forces from other nearby agents. The novelty of this paper is that we incorporate social grouping information into the prediction process reflecting the soft formation constraints that groups typically impose to their members' motion. We show that our method makes more accurate predictions than three state-of-the-art methods in terms of probabilistic and geometrical performance metrics.		Andrey Rudenko;Luigi Palmieri;Achim J. Lilienthal;Kai Oliver Arras	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8594258	novelty;random walk;computer vision;task analysis;semantics;social relation;computer science;probabilistic logic;mobile robot;artificial intelligence;norm (social)	Robotics	-17.267008772134382	-46.42218192411684	77584
468666b542b7b38ed3184d6eba3176273f6f5bdd	molecular computing: from conformational pattern recognition to complex processing networks	informatica;self assembly;dynamique conformationnelle;design tool;biologia molecular;process network;artificial intelligent;structural dynamics;system design;molecular biology;information processing;biochemical network;dinamica conformacional;pattern recognition;self organization;molecular computing;informatique;reconnaissance forme;computer science;reconocimiento patron;conformational dynamics;kinetics;biologie moleculaire	Abs t rac t . Natural biomolecular systems process information in a radically different manner than programmable machines. Conformational interactions, the basis of specificity and self-assembly, are of key importance. A gedanken device is presented that illustrates how the fusion of information through conformational self-organization can serve to enhance pattern processing at the cellular level. The device is used to highfight general features of biomolecular information processing. We briefly outline a simulation system designed to address the manner in which conformational processing interacts with kinetic and higher level structural dynamics in complex biochemical networks. Virtual models that capture features of biomolecular information processing can in some instances have artificial intelligence value in their own right and should serve as design tools for future computers built from real molecules.	artificial intelligence;computer;dna computing;information processing;interaction;pattern recognition;self-assembly;self-organization;sensitivity and specificity;simulation;structural dynamics	Michael Conrad;Klaus-Peter Zauner	1996		10.1007/BFb0033199	structural dynamics;self-organization;simulation;information processing;computer science;artificial intelligence;self-assembly;algorithm;kinetics;systems design	AI	-6.580804837638053	-49.731638328786794	77813
6b1879f57b4e33e01d70085db14dcb6c97f34a9e	disjunctive boolean kernels for collaborative filtering in top-n recommendation		In many personalized recommendation problems available data consists only of positive interactions (implicit feedback) between users and items. This problem is also known as One-Class Collaborative Filtering (OC-CF). Linear models usually achieves state-of-the-art performances on OC-CF problems and many efforts have been devoted to build more expressive and complex representations able to improve the recommendations but with no much success. Recent analysis shows that collaborative filtering (CF) datasets have peculiar characteristics such as high sparsity and a long tailed distribution of the ratings. In this paper we propose a boolean kernel, called Disjunctive Kernel, which is less expressive than the linear one but it is able to alleviate the sparsity issue in CF contexts. The embedding of this kernel is composed by all the combinations of a certain degree d of the input variables, and these combined features are semantically interpreted as disjunctions of the input variables. Experiments on several CF datasets show the effectiveness and the efficiency of the proposed kernel.	boolean algebra;collaborative filtering;data (computing);disjunctive normal form;experiment;interaction;kernel (operating system);linear model;linux;long tail;performance;personalization;sparse matrix	Mirko Polato;Fabio Aiolli	2016	CoRR		theoretical computer science;machine learning;data mining;world wide web;information retrieval;statistics	AI	-16.937911060569114	-47.61719939014607	77952
67fae9020fe2683adba9634110b35deb3d6ffb66	gess: a scalable similarity-join algorithm for mining large data sets in high dimensional spaces	high dimensionality;spatial join;efficient algorithm;profiles;large data sets;data replication;feature space;transaction data;similarity join;high dimensional data;mixture models;em algorithm;cost model;analytical model;principal component	The similarity join is an important operation for mining high-dimensional feature spaces. Given two data sets, the similarity join computes all tuples (x, y) that are within a distance ε.One of the most efficient algorithms for processing similarity-joins is the Multidimensional-Spatial Join (MSJ) by Koudas and Sevcik. In our previous work --- pursued for the two-dimensional case --- we found however that MSJ has several performance shortcomings in terms of CPU and I/O cost as well as memory-requirements. Therefore, MSJ is not generally applicable to high-dimensional data.In this paper, we propose a new algorithm named Generic External Space Sweep (GESS). GESS introduces a modest rate of data replication to reduce the number of expensive distance computations. We present a new cost-model for replication, an I/O model, and an inexpensive method for duplicate removal. The principal component of our algorithm is a highly flexible replication engine.Our analytical model predicts a tremendous reduction of the number of expensive distance computations by several orders of magnitude in comparison to MSJ (factor 107). In addition, the memory requirements of GESS are shown to be lower by several orders of magnitude. Furthermore, the I/O cost of our algorithm is by factor 2 better (independent from the fact whether replication occurs or not). Our analytical results are confirmed by a large series of simulations and experiments with synthetic and real high-dimensional data sets.	algorithm;central processing unit;computation;experiment;gess;input/output;join (sql);microsoft developer network;principal component analysis;replication (computing);requirement;scalability;simulation;synthetic data	Jens Dittrich;Bernhard Seeger	2001		10.1145/502512.502524	feature vector;expectation–maximization algorithm;computer science;theoretical computer science;machine learning;transaction data;mixture model;data mining;database;mathematics;replication;statistics;principal component analysis;clustering high-dimensional data	DB	-4.909920614542419	-39.92181680030902	77958
ca1304a9e1f693b0dd16755cb15f9befc708c5ed	large-scale adversarial sports play retrieval with learning to rank		As teams of professional leagues are becoming more and more analytically driven, the interest in effective data management and access of sports plays has dramatically increased. In this article, we present a retrieval system that can quickly find the most relevant plays from historical games given an input query. To search through a large number of games at an interactive speed, our system is built upon a distributed framework so that each query-result pair is evaluated in parallel. We also propose a pairwise learning to rank approach to improve search ranking based on users’ clickthrough behavior. The similarity metric in training the rank function is based on automatically learnt features from a convolutional autoencoder. Finally, we showcase the efficacy of our learning to rank approach by demonstrating rank quality in a user study.	autoencoder;cluster analysis;display resolution;information retrieval;learning to rank;real-time computing;real-time locating system;search algorithm;usability testing;web search engine	Jennifer Zocca;Diego Klabjan;Long Sha;Patrick Lucey	2018	TKDD	10.1145/3230667	machine learning;learning to rank;data mining;autoencoder;artificial intelligence;data management;adversarial system;pairwise comparison;ranking;computer science	ML	-18.42262918581236	-49.70692345481768	78130
bf981e735bfbac88a162afc9f8cb38cc5311c2ee	retweet prediction using social-aware probabilistic matrix factorization		Retweet prediction is a fundamental and crucial task in social networking websites as it may influence the process of information diffusion. Existing prediction approaches simply ignore social contextual information or dont take full advantage of these potential factors, damaging the performance of prediction. Besides, the sparsity of retweet data also severely disturb the performance of these models. In this paper, we propose a novel retweet prediction model based on probabilistic matrix factorization method by integrating the observed retweet data, social influence and message semantic to improve the accuracy of prediction. Finally, we incorporate these social contextual regularization terms into the objective function. Comprehensive experiments on the real-world dataset clearly validate both the effectiveness and efficiency of our model compared with several state-of the-art baselines.	baseline (configuration management);experiment;loss function;manifold regularization;optimization problem;sparse matrix	Bo Jiang;Zhigang Lu;Ning Li;Jianjun Wu;Zhengwei Jiang	2018		10.1007/978-3-319-93698-7_24	mathematical optimization;social influence;probabilistic logic;machine learning;computer science;matrix decomposition;social network;artificial intelligence	AI	-19.090708126784236	-46.754929728743775	78518
130b00ddcb9c775de02ebd223ce70ab074d5977d	a novel method for information propagation model perceiving	social networking online internet;computational modeling smoothing methods social network services analytical models vectors communities mathematical model;triple exponential smoothing social network information propagation model modularity;internet;social networking online;triple exponential smoothing model information propagation model internet social network modularity	With the rapid development of Internet, we have to face an increasingly important reality regarding the way in which people propagate information and express their thoughts. Meanwhile, social network has caused the tremendous changes of information propagation model. On this premise, perceiving information propagation model effectively is critical in understanding the rules of information propagation. In this paper, we determine the relationship between modularity and information propagation model, and propose a novel method which combines modularity with triple exponential smoothing model to perceive information propagation mechanism. The experimental results demonstrate the effectiveness of our method in information propagation model perceiving. This method can also predict information propagation trends to a certain extent.	internet;smoothing;social network;software propagation;time complexity;time series	Chengqi Yi;Yuanyuan Bao;Siqi Sun;Yibo Xue	2014	2014 International Conference on Computing, Networking and Communications (ICNC)	10.1109/ICCNC.2014.6785295	simulation;computer science;artificial intelligence;machine learning	AI	-18.547054631060544	-42.89015005191223	78636
5e55daac2f353bd5c4a0c64c811926f514a0d3c6	social and semantics analysis via non-negative matrix factorization	graph theory;topic detection;algorithm;cluster analysis;non negative matrix factorization;network model;social network analysis;social media;bipartite graph;semantic analysis	Social media such as Web forum often have dense interactions between user and content where network models are often appropriate for analysis. Joint non-negative matrix factorization model of participation and content data can be viewed as a bipartite graph model between users and media and is proposed for analysis social media. The factorizations allow simultaneous automatic discovery of leaders and sub-communities in the Web forum as well as the core latent topics in the forum. Results on topic detection of Web forums and cluster analysis show that social features are highly effective for forum analysis.	cluster analysis;interaction;non-negative matrix factorization;off topic;social media;world wide web	Zhi-Li Wu;Chi-Wa Cheng;Chun-hung Li	2008		10.1145/1367497.1367748	social network analysis;social media;bipartite graph;computer science;graph theory;theoretical computer science;network model;machine learning;social semantic web;cluster analysis;world wide web;non-negative matrix factorization;information retrieval	Web+IR	-17.183825718876818	-46.822450331937155	78691
af1162fabbde98f3c21e1d60844d92b0924bebc5	logarithmic gravity centrality for identifying influential spreaders in dynamic large-scale social networks		The task of identifying influential spreaders for various big data social network applications plays a crucial role in social networks, and lays the foundation for predictive or recommended applications. Though there are several kinds of methods for this task, most of these methods exploit global computing, and are time-consuming for large-scale social networks. In this paper, by combining the degree centrality with the law of universal gravitation in physics, we present a novel metric called Logarithm Gravity (LG) centrality to quantify the influence of nodes in large-scale social networks, which views the value of the degree centrality as mass for each node and regards the length of the shortest path between a pair of nodes as their distance. In our model, for each node, a local network is generated by obtaining all nodes, which are less than k-hop from it. Then the sum of mutual influence values between the node in question and all other nodes in each local network is figured out as its LG centrality index. Therefore, the complexity of our approach is scalable by adjusting the value of k with efficient local computation. We compare our LG centrality with k-shell, betweenness and degree centralities. Experimental evidence, which has been collected based on the SIR model with four real-world datasets, shows that our approach is more feasible and effective than other state-of-art methods in terms of infection ratios and computational complexity.	betweenness centrality;big data;centrality;computation;computational complexity theory;computer simulation;entropic gravity;scalability;shortest path problem;social network	Jianwei Niu;Haifeng Yang;Lei Wang	2017	2017 IEEE International Conference on Communications (ICC)	10.1109/ICC.2017.7997236	network science;katz centrality;random walk closeness centrality;centrality;complex network;network theory;betweenness centrality;network controllability;machine learning;artificial intelligence;mathematics	AI	-14.753419925378024	-41.33495693369116	79117
bf149231ec7c2864223ef2ba01203872ad6bd92b	diffusion centrality: a paradigm to maximize spread in social networks	logic programming;social networks;quantitative logic;diffusion model	We propose Diffusion Centrality (DC) in which semantic aspects of a social network are used to characterize vertices that are influential in diffusing a property p. In contrast to classical centrality measures, diffusion centrality of vertices varies with the property p, and depends on the diffusion model describing how p spreads. We show that DC applies to most known diffusion models including tipping, cascade, and homophilic models. We present a hypergraph-based algorithm (HyperDC) with many optimizations to exactly compute DC. However, HyperDC does not scale well to huge social networks (millions of vertices, tens of millions of edges). For scaling, we develop methods to coarsen a network and propose a heuristic algorithm called “Coarsened Back and Forth” (CBAF) to compute the topk vertices (having the highest diffusion centrality). We report on experiments comparing DC with classical centrality measures in terms of runtime and the “spread” achieved by the k most central vertices (using 7 real-world social networks and 3 different diffusion models). Our experiments show that DC produces higher quality results and is comparable to several centrality measures in terms of runtime.	algorithm;centrality;experiment;heuristic (computer science);image scaling;programming paradigm;scalability;social network;vertex (geometry)	Chanhyun Kang;Sarit Kraus;Cristian Molinaro;Francesca Spezzano;V. S. Subrahmanian	2016	Artif. Intell.	10.1016/j.artint.2016.06.008	network theory;network science;random walk closeness centrality;combinatorics;computer science;katz centrality;artificial intelligence;theoretical computer science;alpha centrality;machine learning;diffusion;mathematics;centrality;betweenness centrality;logic programming;social network	Web+IR	-12.680553287099357	-42.28818548064066	79296
1c0512dfe2b78c8cd847cf695d48fbe9171eda06	mining distance-based outliers from large databases in any metric space	metric space;metric data;mining;outlier;upper bound;distance metric;information search and retrieval	Let R be a set of objects. An object o ∈ R is an outlier, if there exist less than k objects in R whose distances to o are at most r. The values of k, r, and the distance metric are provided by a user at the run time. The objective is to return all outliers with the smallest I/O cost.This paper considers a generic version of the problem, where no information is available for outlier computation, except for objects' mutual distances. We prove an upper bound for the memory consumption which permits the discovery of all outliers by scanning the dataset 3 times. The upper bound turns out to be extremely low in practice, e.g., less than 1% of R. Since the actual memory capacity of a realistic DBMS is typically larger, we develop a novel algorithm, which integrates our theoretical findings with carefully-designed heuristics that leverage the additional memory to improve I/O efficiency. Our technique reports all outliers by scanning the dataset at most twice (in some cases, even once), and significantly outperforms the existing solutions by a factor up to an order of magnitude.	algorithm;anomaly detection;computation;database;euclidean distance;existential quantification;heuristic (computer science);image scanner;input/output;radio jamming;run time (program lifecycle phase);social inequality	Yufei Tao;Xiaokui Xiao;Shuigeng Zhou	2006		10.1145/1150402.1150447	outlier;mining;metric;metric space;intrinsic metric;machine learning;pattern recognition;data mining;mathematics;upper and lower bounds;statistics	ML	-6.836167077053085	-41.23250590196195	79399
724fe2c0485af70aa24a1f1b5b2b691ac8b6282b	capturing missing edges in social networks using vertex similarity	web of linked data;linked data;information retrieval;link prediction;link analysis;social network;vertex similarity;coauthor network;similarity measure	"""We introduce the graph vertex similarity measure, Relation Strength Similarity (RSS), that utilizes a network's topology to discover and capture similar vertices. The RSS has the advantage that it is asymmetric; can be used in a weighted network; and has an adjustable """"discovery range"""" parameter that enables exploration of friend of friend connections in a social network. To evaluate RSS we perform experiments on a coauthorship network from the CiteSeerX database. Our method significantly outperforms other vertex similarity measures in terms of the ability to predict future coauthoring behavior among authors in the CiteSeerX database for the near future 0 to 4 years out and reasonably so for 4 to 6 years out."""	citeseerx;experiment;friend of a friend;rss;semantic similarity;similarity measure;social network;vertex (graph theory);weighted network	Hung-Hsuan Chen;Liang Gou;Xiaolong Zhang;C. Lee Giles	2011		10.1145/1999676.1999722	link analysis;computer science;machine learning;linked data;data mining;world wide web;social network	Web+IR	-15.35602087444343	-44.60967397393711	79757
31fbcdf83441b3cb8e0f95552ee16d69149f9468	over-time measurement of triadic closure in coauthorship networks	clustering coefficient;transitivity;triadic closure;coauthorship networks	Applying the concept of triadic closure to coauthorship networks means that scholars are likely to publish a joint paper if they have previously coauthored with the same people. Prior research has identified moderate to high (20 to 40%) closure rates; suggesting this mechanism is a reasonable explanation for tie formation between future coauthors. We show how calculating triadic closure based on prior operationalizations of closure, namely Newman’s measure for one-mode networks (NCC) and Opsahl’s measure for two-mode networks (OCC) may lead to higher amounts of closure compared to measuring closure over time via a metric that we introduce and test in this paper. Based on empirical experiments using four large-scale, longitudinal datasets, we find a lower bound of 1–3% closure rates and an upper bound of 4–7%. These results motivate research on new explanatory factors for the formation of coauthorship links.	experiment;neural correlates of consciousness;optimistic concurrency control;triadic closure	Jinseok Kim;Jana Diesner	2017	Social Network Analysis and Mining	10.1007/s13278-017-0428-3	artificial intelligence;mathematics;operations research	Web+IR	-15.863881026949919	-40.68249755090746	79915
325ab6d6a82cb7f837a5905cbf1acee6d5c635e1	cost range and the stable network structures	reseau social;network evolution;journal;stability;cost range;social network;theoreme;network structure;stabilite;structure;network structures	The work of this paper concerns with the stable structures in different cost range identified by Doreian in his paper [Doreian, P., 2006. Actor network utilities and network evolution. Social Networks 28, 137–164]. We point out some problems with his Theorem 4 and present our corrections to that theorem.		Fengjie Xie;Wentian Cui	2008	Social Networks	10.1016/j.socnet.2007.07.002	structure;social science;stability;artificial intelligence;mathematics;sociology;mathematical economics;operations research;statistics;social network	ECom	-17.98908743303674	-38.8005306511526	80244
9d10dd03345f835bd3e0c2f6c68333a96c4d33ab	bilateral neural embedding for collaborative filtering-based multimedia recommendation		As one of the most popular and successfully applied recommendation methods, collaborative filtering aims to extract low-dimensional user and item representation from historic user-item interaction matrix. The similarity between the user and item representation vectors in the same space well measures the degree of interest and thus can be directly used for recommendation. This paper proposes to leverage the emerging deep neural language model to solve the collaborative filtering-based multimedia recommendation problem. By applying the standard Word2Vec model on the user-item interaction data, we can obtain the item embedding representation. Based on this, three strategies are introduced to derive the user embedded representation in the same space, which exploit both the user-item interaction and the correlation among items. Experiments on article recommendation application demonstrate that the proposed deep neural language models achieve superior performance than the traditional collaborative filtering methods based on matrix factorization and topic model.	bilateral filter;collaborative filtering;embedded system;interaction information;language model;long tail;topic model;word2vec	Yang Liu;Linfeng Li;Jun Liu	2017	Multimedia Tools and Applications	10.1007/s11042-017-4902-8	collaborative filtering;recommender system;topic model;computer science;data mining;machine learning;word2vec;matrix decomposition;multimedia;exploit;embedding;language model;artificial intelligence	Web+IR	-18.73929779754946	-48.192140915167194	80374
4fb47bec34a4e7612e5adf6e52e7dcd3833ebd38	research on big data clustering algorithm based on local core categories		A big data clustering algorithm based on the local core category is proposed in order to find the network structure of the big data accurately and quickly in this paper. The concept of local core node and maximal clique are introduced to determine the local core community, and the data sets are divided into the core nodes and the common nodes. Then the core category with the largest scale is selected to discover the whole community according to the improved fitness formula. Finally, the parallel strategy of the whole algorithm is proposed. Experimental results show that the proposed algorithm is feasible and effective and could be applied to find the network structure of large scale data.	algorithm;big data;clique (graph theory);cluster analysis;maximal set;parallel computing;requirement	Xiao-Dong Qian;Yang Cao;J. Y. Fang	2018	2018 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2018.8526940	complex network;machine learning;clique;big data;cluster analysis;artificial intelligence;data set;computer science;pattern recognition	DB	-14.039956621585242	-42.81493107557132	80421
0ad90a11cf7c2ade84ad3193ff35cdee7f534cd5	gb-kmv: an augmented kmv sketch for approximate containment similarity search		In this paper, we study the problem of approximate containment similarity search. Given two records Q and X , the containment similarity between Q and X with respect to Q is |Q∩X| |Q| . Given a query record Q and a set of records S , the containment similarity search finds a set of records from S whose containment similarity regarding Q is not less than the given threshold. This problem has many important applications in commercial and scientific fields such as record matching and domain search. Existing solution relies on the asymmetric LSH method by transforming the containment similarity to well-studied Jaccard similarity. In this paper, we use a inherently different framework by transforming the containment similarity to set intersection. We propose a novel augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can achieve a much better trade-off between the sketch size and the accuracy. We provide a set of theoretical analysis to underpin the proposed augmented KMV sketch technique, and show that it outperforms the state-ofthe-art technique LSH-E in terms of estimation accuracy under practical assumption. Our comprehensive experiments on real-life datasets verify that GB-KMV is superior to LSH-E in terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch construction time. For instance, with similar estimation accuracy (F-1 score), GB-KMV is over 100 times faster than LSH-E on several real-life datasets.	approximation algorithm;data dependency;experiment;jaccard index;minhash;real life;similarity search;sketch;lsh	Yang Yang;Wenjie Zhang;Zengfeng Huang	2018	CoRR		jaccard index;data mining;computer science;sketch;artificial intelligence;pattern recognition;intersection (set theory);nearest neighbor search	DB	-7.055231620757922	-40.61313847756859	80649
6ffa564b8a26789c909e060a2b7ac16b478777fd	an evolutionary and local refinement approach for community detection in signed networks	community detection;evolutionary computation;multiobjective clustering;local search;signed networks	An approach to detect communities in signed networks that combines Genetic Algorithms and local search is proposed. The method optimizes the concepts of modularity and frustration in order to find network divisions far from random partitions, and having positive and dense intra-connections, while sparse and negative inter-connections. A local search strategy to improve the network division is performed by moving nodes having positive connections with nodes of other communities, to neighboring communities, provided that there is an increase in signed modularity. An extensive experimental evaluation on randomly generated networks for which the ground-truth division is known proves that the method is competitive with a state-of-art approach, and it is capable to find accurate solutions. Moreover, a comparison on a real life signed network shows that our approach obtains communities that minimize the positive inter-connections and maximize the negative intra-connections better than the contestant methods.		Alessia Amelio;Clara Pizzuti	2016	International Journal on Artificial Intelligence Tools	10.1142/S0218213016500214	mathematical optimization;computer science;artificial intelligence;local search;modularity;theoretical computer science;machine learning;evolutionary computation	DB	-13.62253426911144	-42.826083428367944	81022
50902322f6c98f40020abbc59bad7e3657394495	an improved optimization of link-based label propagation algorithm		Community Detection has been an important tool for network and overlapping Community exists in real work ubiquitously. In this paper, an improved optimization of link-based label propagation algorithm rather than node called LinkLPAm is proposed to detect overlapping community. We briefly introduce our main work. Firstly, the initialization on edge labels by rough core is presented to speed up the process of detecting overlapping community, which is a big timesaver for the link network that magnified a lot of times compared with original node network. Secondly, an optimization algorithm of label propagation on link is given to update label on edge. Thirdly, in order to restrict the number of communities and the number of nodes in the community, the metric community similarity between communities is defined and greedy mergence algorithm is taken to merge communities according to community similarity. Finally, experimental result shows that our method LinkLPAm is serviceable for find reasonable overlapping community.		Xiaoxiang Zhu;Zhengyou Xia	2018		10.1007/978-3-030-05090-0_42	data mining;node (networking);initialization;merge (version control);speedup;algorithm;computer science;restrict	Vision	-13.933825578155789	-42.915848564382316	81128
9c16bd8d4a668a62d4cefab7738c43251db29236	accelerating spatially explicit simulations of spread of lyme disease	organisms;mice;federate migration;dynamic component substitution;acceleration diseases computational modeling microorganisms mice power system modeling computer simulation spatial resolution distributed control organisms;spatially explicit;spatially explicit simulations;acceleration;medical computing;computer based simulations;sequential simulation;computational modeling;load balancing;diseases;power system modeling;lyme disease;distributed simulation;parallel processing diseases microorganisms medical computing digital simulation;high level architecture;computer simulation;distributed control;microorganisms;parallel processing;parallel simulation;digital simulation;spatially explicit model;sequential simulation spatially explicit simulations lyme disease computer based simulations parallel simulation spatial resolution dynamic component substitution;spatial resolution	The factors influencing spread of Lyme disease are often studied using computer-based simulations and spatially explicit models. However, simulating large and complex models is a time consuming task, even when parallel simulation techniques are employed. In an endeavor to accelerate such simulations, an alternative approach involving dynamic (i.e., during simulation) changes to spatial resolution of the model via a novel methodology called Dynamic Component Substitution (DCS) is proposed. Changes to the resolution are performed such that the total number of interactions between the entities in the model is optimized, thereby improving overall performance but introducing minor (< ±1%) deviations in the results. This paper explores the effectiveness and issues involved in applying DCS to accelerate sequential and parallel simulations of spatially explicit Lyme disease models. The paper also presents a brief description of the simulation environment along with empirical results. Our experiments indicate that performance improvements can be obtained using the proposed approach.	entity;experiment;interaction;lyme (software bundle);parallel computing;simulation;substitution matrix	Dhananjai Madhava Rao;Philip A. Wilsey	2005	38th Annual Simulation Symposium	10.1109/ANSS.2005.10	simulation;computer science;theoretical computer science;computer graphics (images)	HPC	-4.782440131905967	-49.92501034021083	81334
099d629b26b83be9ff44a3e935f1b9da9b793513	collaborative filtering by multi-task learning	e commerce;information filtering;data sparsity;information sharing;boosting;recommender system;collaborative filtering;multi task learning;data sparsity collaborative filtering multitask learning user interest prediction behavior pattern recommender systems e commerce information sharing;behavior pattern;pattern classification;user interest prediction;pattern classification information filtering learning artificial intelligence;boosting collaborative filtering multi task learning;learning artificial intelligence;recommender systems;multitask learning	Collaborative filtering is a technique to predict userspsila interests for items by exploiting the behavior patterns of a group of users with similar preferences. This technique has been widely used for recommender systems and has a number of successful applications in E-commerce. In practice, a major challenge when applying collaborative filtering is that a typical user provides ratings for just a small number of items, thus the amount of training data is sparse with respect to the size of the domain. In this paper, we present a method to address this problem. Our method formulates the collaborative filtering problem in a multi-task learning framework by treating each user rating prediction as a classification problem and solving multiple classification problems together. By doing this, the method allows sharing information among different classifiers and thus reduces the effect of data sparsity.	boosting (machine learning);collaborative filtering;computer multitasking;e-commerce payment system;multi-task learning;multi-user;recommender system;sparse matrix	Nguyen Duy Phuong;Tu Minh Phuong	2008	2008 IEEE International Conference on Research, Innovation and Vision for the Future in Computing and Communication Technologies	10.1109/RIVF.2008.4586360	computer science;collaborative filtering;machine learning;pattern recognition;data mining;recommender system	Robotics	-18.756694309499515	-48.567724512847626	81470
e32df275cc00bb063773a3fac3489b0d4a94ff06	multiscale network reduction methodologies: bistochastic and disparity filtering of human migration flows between 3, 000+ u. s. counties		To control for multiscale effects in networks, one can transform the matrix of (in general) weighted, directed internodal flows to bistochastic (doubly-stochastic) form, using the iterative proportional fitting (Sinkhorn-Knopp) procedure, which alternatively scales row and column sums to all equal 1. The dominant entries in the bistochasticized table can then be employed for network reduction, using strong component hierarchical clustering. We illustrate various facets of this wellestablished, widely-applied two-stage algorithm with the 3, 107 × 3, 107 (asymmetric) 1995-2000 intercounty migration flow table for the United States. We compare the results obtained with ones using the disparity filter, for ”extracting the ”multiscale backbone of complex weighted networks”, recently put forth by Serrano, Boguñá and Vespignani (SBV) (Proc. Natl. Acad. Sci. 106 [2009], 6483), upon which we have briefly commented (Proc. Natl. Acad. Sci. 106 [2009], E66). The performance of the bistochastic filter appears to be superior, in this specific case, in two respects: (1) it requires far fewer links to complete a strongly-connected network backbone; and (2) it ”belittles” small flows and nodes less–a principal desideratum of SBV–in the sense that the correlations of the nonzero raw flows are considerably weaker with the corresponding bistochastized links than with the significance levels yielded by the disparity filter. Further, the disparity filter, in general, relies upon a somewhat arbitrary choice of either AND or OR rules, while the bistochastic filter does not. Additional comparative studies–as called for by SBV–of these two filtering procedures, in particular as regards their topological properties, should be of considerable interest. Relatedly, in its many geographic applications, the two-stage procedure has–with rare exceptions–clustered contiguous areas, often reconstructing traditional regions (islands, for example), even though no contiguity constraints, at all, are imposed beforehand. PACS numbers: Valid PACS 02.10.Ox, 02.10.Yn, 89.65.Cd, 89.75.Hc ∗Electronic address: slater@kitp.ucsb.edu	alessandro vespignani;algorithm;binocular disparity;cluster analysis;hierarchical clustering;internet backbone;iterative method;iterative proportional fitting;picture archiving and communication system;the matrix;weighted network	Paul B. Slater	2009	CoRR		contiguity;combinatorics;iterative proportional fitting;matrix (mathematics);filter (signal processing);contiguity (probability theory);backbone network;hierarchical clustering;mathematics;human migration	ML	-14.31867336798242	-40.15505350413281	81606
6f8a6ac2fdb87577a4d6ec2bfe63ed9de85b190b	analyzing future nodes in a knowledge network	knowledge network;technology forecasting;big data analysis future node analysis knowledge network network analytics pegonet future knowledge prediction;knowledge engineering correlation linear regression encyclopedias electronic publishing internet;network analysis;node prediction;network theory graphs big data data analysis knowledge representation;node prediction knowledge network technology forecasting network analysis	The paper proposes new methods for knowledge prediction using network analytics and introduces pEgonet, sub-networks within knowledge networks consisting of to-beneighbors of new knowledge. Preliminary results show that it is feasible to predict how future knowledge is added in the knowledge network by utilizing basic properties of pEgonet. The paper presents initial work which will be expanded to derive a method to predict labelled future knowledge, with its impact and structures.	experiment;pubchem;text corpus;weight function	Sukhwan Jung;Tuan Manh Lai;Aviv Segev	2016	2016 IEEE International Congress on Big Data (BigData Congress)	10.1109/BigDataCongress.2016.57	organizational network analysis;knowledge base;computer science;data science;machine learning;open knowledge base connectivity;data mining;network simulation;knowledge extraction	Robotics	-18.04673520893953	-41.51230519086017	81661
1e316a23bd37e7f385d44e839ee330dab82777fd	gin: a clustering model for capturing dual heterogeneity in networked data		Networked data often consists of interconnected multityped nodes and links. A common assumption behind such heterogeneity is the shared clustering structure. However, existing network clustering approaches oversimplify the heterogeneity by either treating nodes or links in a homogeneous fashion, resulting in massive loss of information. In addition, these studies are more or less restricted to specific network schemas or applications, losing generality. In this paper, we introduce a flexible model to explain the process of forming heterogeneous links based on shared clustering information of heterogeneous nodes. Specifically, we categorize the link generation process into binary and weighted cases and model them respectively. We show these two cases can be seamlessly integrated into a unified model. We propose to maximize a joint log-likelihood function to infer the model efficiently with Expectation Maximization (EM) algorithms. Experiments on real-world networked data sets demonstrate the effectiveness and flexibility of the proposed method in fully capturing the dual heterogeneity of both nodes and links.	bitwise operation;categorization;cluster analysis;expectation–maximization algorithm;social network;unified model	Jialu Liu;Chi Wang;Jing Gao;Quanquan Gu;Charu C. Aggarwal;Lance M. Kaplan;Jiawei Han	2015		10.1137/1.9781611974010.44	categorization;computer science;pattern recognition;expectation–maximization algorithm;machine learning;generality;artificial intelligence;homogeneous;cluster analysis;schema (psychology);unified model;data set	AI	-15.286070012700124	-46.66716184270283	82299
5cd25c5df9734cc8c6c96e3d9eef6b16eafe2a97	social influence modeling using information theory in mobile social networks		Social influence analysis has become one of the most important technologies in modern information and service industries. Thus, how to measure social influence of one user on other users in a mobile social network is also becoming increasingly important. It is helpful to identify the influential users in mobile social networks, and also helpful to provide important insights into the design of social platforms and applications. However, social influence modeling is an open and challenging issue, and most evaluation models are focused on online social networks, but fail to characterize indirect influence. In this paper, we present a mechanism to quantitatively measure social influence in mobile social networks. We exploit the graph theory to construct a social relationship graph that establishes a solid foundation for the basic understandings of social influence. We present an evaluation model to measure both direct and indirect influence based on the social relationship graph, by introducing friend entropy and interaction frequency entropy to describe the complexity and uncertainty of social influence. Based on the epidemic model, we design an algorithm to characterize propagation dynamics process of social influence, and to evaluate the performance of our solution by using a customized program on the basis of a real-world SMS/MMS-based communication data set. The real world numerical simulations and analysis show that the proposed influence evaluation strategies can characterize the social influence of mobile social networks effectively and efficiently.		Sancheng Peng;Aimin Yang;Lihong Cao;Shui Yu;Dongqing Xie	2017	Inf. Sci.	10.1016/j.ins.2016.08.023	simulation;computer science;dynamic network analysis;artificial intelligence;social heuristics;machine learning;management science	AI	-18.215728793592753	-42.905487505756916	82509
185368736a49f075d10ed16a86367de8b0173dc5	simultaneous community discovery and user interests extraction in social network based on probabilistic model	latent dirichlet allocation;lda;probabilistic generative model;social networks;community discovery	This article addresses the problem of discovering latent communities and topics simultaneously in social network. With the advent of online social networking, the automatic discovering communities is vital for understanding the cooperation and interaction patterns of users in these social networks. In this paper, we propose probabilistic generative models to detect latent communities by incorporating both the information of relationships and the textural content. Different from previous work, topics and user community memberships cannot be generated independently, but have a greater degree of correspondence between them. We assume that community membership is dependent on the user and a subset of topics which the user is really interested in. Furthermore, the heterogeneous relationship strengths were used to improve community discovery. These models treat community and topic as different latent variables but interdependent with each other and mutually reinforcing. Experiments on real-world dataset have shown that our models have the capability to detect well-connected and topically meaningful communities.	algorithm;experiment;generative model;gibbs sampling;interdependence;latent variable;sampling (signal processing);social network;statistical model;virtual community	Juan Bi;Zhiguang Qin;Hu Xiong;Jia Huang	2014	IJIIDS	10.1504/IJIIDS.2014.066637	latent dirichlet allocation;computer science;artificial intelligence;data science;machine learning;data mining;social network	ML	-18.833269824384697	-44.58505353274686	82564
3864f4ce025a2ff725bc52738fdde458273f11eb	a study of link farm distribution and evolution using a time series of web snapshots	strongly connected component;robustness analysis;information retrieval;time series;web spam;link analysis	In this paper, we study the overall link-based spam structure and its evolution which would be helpful for the development of robust analysis tools and research for Web spamming as a social activity in the cyber space. First, we use strongly connected component (SCC) decomposition to separate many link farms from the largest SCC, so called the core. We show that denser link farms in the core can be extracted by node filtering and recursive application of SCC decomposition to the core. Surprisingly, we can find new large link farms during each iteration and this trend continues until at least 10 iterations. In addition, we measure the spamicity of such link farms. Next, the evolution of link farms is examined over two years. Results show that almost all large link farms do not grow anymore while some of them shrink, and many large link farms are created in one year.	connected component (graph theory);cyberspace;evolution;iteration;recursion;spamming;strongly connected component;time series;world wide web	Young-joo Chung;Masashi Toyoda;Masaru Kitsuregawa	2009		10.1145/1531914.1531917	real-time computing;simulation;link analysis;computer science;spamdexing;time series;distributed computing;world wide web;strongly connected component;link farm;statistics	Web+IR	-17.83665463186082	-40.83600670848832	82616
0ea96d65aa134c9c7153f12ab75d324a47ebcc0e	predicting community evolution based on time series modeling	time series analysis feature extraction predictive models social network services detection algorithms computational modeling forecasting;social network services;forecasting;time series social networking online;detection algorithms;computational modeling;attribute based community analysis;time series analysis;event prediction community evolution prediction time series modeling evolving social networks event based framework community behavior patterns time series arima model;feature extraction;social network analysis;community patterns;predictive models	Communities in real life are usually dynamic and community structures evolve over time. Detecting community evolution provides insight into the underlying behavior of the network. A growing body of study is devoted in studying the dynamics of communities in evolving social networks. Most of them provide an event-based framework to characterize and track the community evolution. A part of these studies take a step further and provide a predictive model of the events by exploiting community features. However, the proposed models require the community extraction and computing the community features relevant to the time point to be predicted. In this paper, we proposed a new approach for predicting events by estimating feature values related to the communities in a given network. An event-based framework is used to characterize community behavior patterns. Then, a time series ARIMA model is used to predict how particular community features will change in the following time period. Distinct time windows are examined in constituting and analyzing time series. Our proposed approach efficiently tracks similar communities and identifies events over time. Furthermore, community feature values are forecasted with an acceptable error rate. Event prediction using forecasted feature values substantially match up with actual events.	autoregressive integrated moving average;evolution;microsoft windows;predictive modelling;real life;social network;time series	Nagehan Ilhan;Sule Gündüz Ögüdücü	2015	2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1145/2808797.2808913	social network analysis;forecasting;feature extraction;computer science;data science;machine learning;time series;data mining;predictive modelling;computational model;statistics	DB	-18.57981828760193	-38.39396186986847	82641
7df53948db26e93bd99d5bab4f5464aeb30b8c6f	integrative biology - the way forward				Charlie Hodgman	2007	Briefings in bioinformatics	10.1093/bib/bbm036		Comp.	-8.71385727976905	-47.90835773266076	82721
9d128308d66c912958321dcf18a3cbf83a600734	big data: theoretical aspects [scanning the issue]	special issues and sections;data mining;computational modeling;special issues and sections big data computational modeling statistical analysis information analysis data mining algorithm design and analysis computer applications indexes;statistical analysis;big data;information analysis;algorithm design and analysis	Big data has burst into public awareness over the past few years as people have become more and more aware of the massive amount of data being produced by social and scientific activities, and its potential utilization for good or harm. On the research front, big data has spurred new activity across a range of fields, including statistics, machine learning, and computer systems. Many areas have been profoundly altered by the big data revolution, including wireless communications, speech processing, social networking, online commerce, medical informatics, and finance. In these areas, and in many others, analysis of the data yields valuable information that deepens understanding, improves decision making, and enhances performance of predictive models. This special issue highlights a number of algorithmic approaches that are fundamental to data analysis, both in formulating and solving problems. These methods form part of the core of the field, a set of tools that can be applied to many specific application areas. The issue consists of nine papers covering a variety of topics in formulation and algorithms. We summarize each of them briefly. ‘‘A Review of Relational Machine Learning for Knowledge Graphs’’ by Nickel et al. Relational machine learning studies methods for statistical analysis of relational or graph-structured data. This paper reviews how such statistical models can be trained on large knowledge graphs, and then used to predict new facts, such as prediction of new edges in the graph. Two fundamentally different kinds of statistical relational models are addressed. The first is based on latent feature models (for example, tensor factorization and multiway neural networks), while the second is based on mining observable patterns in the graph. The paper shows how to combine the latent and observable models to improve modeling power while decreasing computational cost. It is shown how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. Google’s Knowledge Vault project provides an example of such combinations. ‘‘Learning to Hash for Indexing Big DataVA Survey’’ by Wang et al. The explosive growth in big data has created a great deal of demand for efficient indexing and searching procedures. In many critical applications, including large-scale search and pattern matching, finding the nearest neighbors to a query is a difficult proposition. The paper looks to the approximate nearest neighbor (ANN) search based on hashing techniques, which has become popular due to its promising efficiency and accuracy. The paper describes new approaches that incorporate data-driven learning methods in the development of advanced hash functions. Such learningto-hash methods exploit information such as data distributions or class labels when optimizing hash codes or functions. Most importantly, the hash This special issue highlights a number of algorithmic approaches that are fundamental to data analysis, both in formulating and solving problems.	algorithmic efficiency;approximation algorithm;artificial neural network;big data;code;computation;e-commerce;graph (abstract data type);hash function;informatics;information extraction;knowledge graph;knowledge vault;machine learning;observable;pattern matching;predictive modelling;speech processing;statistical model;text-based (computing);world wide web	Simon Haykin;Stephen J. Wright;Yoshua Bengio	2016	Proceedings of the IEEE	10.1109/JPROC.2015.2507658	algorithm design;big data;computer science;data science;data mining;data analysis;computational model	ML	-7.151416298254536	-43.47628198410138	82989
4fff4cb2a07a14bf3e41188d094944e9d95a5737	blast: a loosely schema-aware meta-blocking approach for entity resolution		Identifying records that refer to the same entity is a fundamental step for data integration. Since it is prohibitively expensive to compare every pair of records, blocking techniques are typically employed to reduce the complexity of this task. These techniques partition records into blocks and limit the comparison to records co-occurring in a block. Generally, to deal with highly heterogeneous and noisy data (e.g. semi-structured data of the Web), these techniques rely on redundancy to reduce the chance of missing matches. Meta-blocking is the task of restructuring blocks generated by redundancy-based blocking techniques, removing superfluous comparisons. Existing meta-blocking approaches rely exclusively on schema-agnostic features. In this paper, we demonstrate how “loose” schema information (i.e., statistics collected directly from the data) can be exploited to enhance the quality of the blocks in a holistic loosely schema-aware (meta-)blocking approach that can be used to speed up your favorite Entity Resolution algorithm. We call it Blast (Blocking with Loosely-Aware Schema Techniques). We show how Blast can automatically extract this loose information by adopting a LSH-based step for efficiently scaling to large datasets. We experimentally demonstrate, on real-world datasets, how Blast outperforms the state-of-the-art unsupervised meta-blocking approaches, and, in many cases, also the supervised one.	algorithm;blast;blocking (computing);datasource;experiment;holism;image scaling;precision and recall;semi-structured data;semiconductor industry;signal-to-noise ratio;world wide web;lsh	Giovanni Simonini;Sonia Bergamaschi;H. V. Jagadish	2016	PVLDB	10.14778/2994509.2994533	computer science;data mining;database;world wide web	DB	-7.855443478805925	-41.170474981579694	82990
0e7c75fc648ab84a15698921c241795d9ab55f1f	an embedding approach to anomaly detection	data mining algorithm design and analysis image edge detection social network services prediction algorithms optimization couplings;social networking online data analysis;community detection network anomaly detection structurally inconsistent nodes detection social networks network embedding approach dimension reduction technique	Network anomaly detection has become very popular in recent years because of the importance of discovering key regions of structural inconsistency in the network. In addition to application-specific information carried by anomalies, the presence of such structural inconsistency is often an impediment to the effective application of data mining algorithms such as community detection and classification. In this paper, we study the problem of detecting structurally inconsistent nodes that connect to a number of diverse influential communities in large social networks. We show that the use of a network embedding approach, together with a novel dimension reduction technique, is an effective tool to discover such structural inconsistencies. We also experimentally show that the detection of such anomalous nodes has significant applications: one is the specific use of detected anomalies, and the other is the improvement of the effectiveness of community detection.	algorithm;anomaly detection;data mining;dimensionality reduction;experiment;gradient descent;high-level programming language;mathematical optimization;scalability;sensor;social network	Renjun Hu;Charu C. Aggarwal;Shuai Ma;Jinpeng Huai	2016	2016 IEEE 32nd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2016.7498256	theoretical computer science;machine learning;data mining	DB	-14.493566535331802	-42.26816436187233	83286
5c0e5f4a89befc58b305ae6fa0461f7f6301266e	predicting eye fixations on webpage with an ensemble of early features and high-level representations from deep network	webpage saliency deep learning visual attention web viewing;neural nets feature extraction internet;media;visualization;computational modeling;internet;image color analysis;feature extraction;visualization computational modeling feature extraction predictive models media internet image color analysis;predictive models;deep neural network web page high level representation visual information source visual media web viewing pattern low level feature extraction pipeline	In recent decades, webpages are becoming an increasingly important visual information source. Compared with natural images, webpages are different in many ways. For example, webpages are usually rich in semantically meaningful visual media (text, pictures, logos, and animations), which make the direct application of some traditional low-level saliency models ineffective. Besides, distinct web-viewing patterns such as top-left bias and banner blindness suggest different ways for predicting attention deployment on a webpage. In this study, we utilize a new scheme of low-level feature extraction pipeline and combine it with high-level representations from deep neural networks. The proposed model is evaluated on a newly published webpage saliency dataset with three popular evaluation metrics. Results show that our model outperforms other existing saliency models by a large margin and both low- and high-level features play an important role in predicting fixations on webpage.	artificial neural network;banner blindness;color;deep learning;feature extraction;high- and low-level;image;information source;multi-level governance;orientation (graph theory);software deployment;web page	Chengyao Shen;Xun Huang;Qi Zhao	2015	IEEE Transactions on Multimedia	10.1109/TMM.2015.2483370	computer vision;the internet;media;visualization;feature extraction;computer science;machine learning;predictive modelling;multimedia;computational model;world wide web	Vision	-18.186514288566283	-51.3527523808564	83465
a58d05de19622ab81fd445e580bf7256ce700c9f	efficient similarity search in feature spaces with the q-tree	arbre r;multimedia;image databank;information retrieval;metric;feature space;r tree;similitude;arbol r;vecino mas cercano;recherche information;banco imagen;banque image;similarity;multidimensional data base;plus proche voisin;nearest neighbour;metrico;base donnee multidimensionnelle;nearest neighbor search;recuperacion informacion;similitud;access method;experience base;similarity search;metrique	In many database applications similarity search is a typical operation that must be efficiently processed. A class of techniques to implement similarity queries which has demonstrated acceptable results includes the use of a nearest neighbor search algorithm on top of a multidimensional access method. We propose a new algorithm based on the space organization induced by the Q-tree. Specifically, a metric is used to discard internal zones of a cube that have been previously extracted from it. Our experiments based on both synthetic and real data sets show that our approach outperforms other competitive techniques like R*-tree, M-tree, and Hybrid-tree.	similarity search;spaces	Elena Jurado;Manuel Barrena García	2002		10.1007/3-540-45710-0_15	best bin first;computer science;machine learning;pattern recognition;data mining;database;nearest neighbor search	DB	-5.441726662171292	-41.072455750773095	83902
e7551e3157459f1406b6e39257488d460683c416	a tool for linguistic assessment of rehabilitation exercises	linguistic modeling;rehabilitation assessment;fuzzy linguistic summarization	In this paper, human motion analysis is performed by modeling a physical complex exercise in order to provide feedback about the patient’s performance in rehabilitation therapies. The Sun Salutation exercise, which is a flowing sequence of twelve yoga poses, is analyzed. This exercise provides physical benefits as improving the strength and flexibility of the muscles and the alignment of the spinal column. A temporal series of measures that contains a numerical description of this sequence is obtained by using a wearable sensing system for monitoring, which is formed by five high precision tri-axial accelerometer sensors worn by the patient while performing the exercise. Due to the complexity of the exercise and the huge amount of available data, its interpretation is a challenging task. Therefore, this paper describes the design of a computational system able of interpreting and generating linguistic descriptions about this exercise. Previous works on both Granular Linguistic Models of Phenomena and Fuzzy Finite State Machines are used to create a basic linguistic model of the Sun Salutation. This model allows generating human friendly reports focused on the assessment of the exercise quality based on its symmetry, stability and rhythm.	activity recognition;computation;computational model;finite-state machine;interpretation (logic);kinesiology;natural language generation;numerical analysis;performance evaluation;sensor;software release life cycle;triangular function;usability;wearable computer	Lara González-Villanueva;Alberto Alvarez-Alvarez;Luca Ascari;Gracián Triviño	2014	Appl. Soft Comput.	10.1016/j.asoc.2013.07.010	simulation;artificial intelligence;machine learning	AI	-10.524139989056883	-48.018097077088925	83905
9572b8d8e6d466f22fe3258a2b13225e2399c041	temporal representation for scientific data provenance	databases;provenance representation;pattern clustering;workflow management software abstract data types data mining frequency domain analysis meta data pattern classification pattern clustering scientific information systems temporal databases time domain analysis;clocks;frequency domain analysis;abstract data types;data mining;time domain analysis;logical clock;data mining clocks frequency domain analysis databases clustering algorithms partitioning algorithms time domain analysis;scientific workflows temporal representation digital scientific data provenance metadata provenance data representation feature space reduction time domain representation frequency domain representation clustering classification association rule mining abstract representation provenance database;pattern classification;workflow management software;clustering algorithms;meta data;temporal databases;temporal data mining;preprint;scientific information systems;partitioning algorithms	Provenance of digital scientific data is an important piece of the metadata of a data object. It can however grow voluminous quickly because the granularity level of capture can be high. It can also be quite feature rich. We propose a representation of the provenance data based on logical time that reduces the feature space. Creating time and frequency domain representations of the provenance, we apply clustering, classification and association rule mining to the abstract representations to determine the usefulness of the temporal representation. We evaluate the temporal representation using an existing 10 GB database of provenance captured from a range of scientific workflows.	association rule learning;cluster analysis;data lineage;data mining;feature vector;gigabyte;high-level programming language;lineage (evolution);mapreduce;mined;scalability;statistical classification;synthetic data	Peng Chen;Beth Plale;Mehmet S. Aktas	2012	2012 IEEE 8th International Conference on E-Science	10.1109/eScience.2012.6404477	computer science;data mining;database;information retrieval	DB	-7.196075698775068	-38.22258795364647	84081
0ddc35863eb5d7028f4b5be99312f3b20d2a1b01	overlapping clustered graphs: co-authorship networks visualization	network visualization;social network;directed graph	The analysis of scientific articles produced by different groups of authors helps to identify and characterize research groups and collaborations among them. Although this is a quite studied area, some issues, such as quick understanding of groups and visualization of large social networks still pose some interesting challenges. In order to contribute to this study, we present a solution based in Overlapper, a tool for the visualization of overlapping groups that makes use of an enhanced variation of force-directed graphs. For a real case study, the tool has been applied to articles in the DBLP database.	algorithm;biconnected component;chart;dbl-browser;diagram;directed graph;force-directed graph drawing;nl (complexity);scientific literature;social network;sparse matrix;usb hub	Rodrigo Santamaría;Roberto Therón	2008		10.1007/978-3-540-85412-8_17	directed graph;computer science;bioinformatics;data mining;graph drawing;world wide web;social network	Visualization	-16.58517415719516	-40.7964417333455	84101
92877f9c72d00b528b3041c282b76ad5c96e0239	bipartite graph matching for keyframe summary evaluation		A keyframe summary, or “static storyboard”, is a collection of frames from a video designed to summarise its semantic content. Many algorithms have been proposed to extract such summaries automatically. How best to evaluate these outputs is an important but little-discussed question. We review the current methods for matching frames between two summaries in the formalism of graph theory. Our analysis revealed different behaviours of these methods, which we illustrate with a number of case studies. Based on the results, we recommend a greedy matching algorithm due to Kannappan et al.	graph theory;greedy algorithm;key frame;matching (graph theory);semantics (computer science);set cover problem;storyboard;thresholding (image processing)	Iain A. D. Gunn;Ludmila I. Kuncheva;Paria Yousefi	2017	CoRR		computer science;artificial intelligence;storyboard;pattern recognition;machine learning;graph theory;blossom algorithm;formalism (philosophy);bipartite graph	NLP	-12.495790315560471	-50.32113894438555	84402
87bf74549b5f2e45889668b326a523ad1b7b4737	generating pictorial storylines via minimum-weight connected dominating set approximation in multi-view graphs	pictorial storyline generation	This paper introduces a novel framework for generating pictorial storylines for given topics from text and image data on the Internet. Unlike traditional text summarization and timeline generation systems, the proposed framework combines text and image analysis and delivers a storyline containing textual, pictorial, and structural information to provide a sketch of the topic evolution. A key idea in the framework is the use of an approximate solution for the dominating set problem. Given a collection of topic-related objects consisting of images and their text descriptions, a weighted multi-view graph is first constructed to capture the contextual and temporal relationships among these objects. Then the objects are selected by solving the minimum-weighted connected dominating set problem defined on this graph. Comprehensive experiments on real-world data sets demonstrate the effectiveness of the proposed framework.	approximation algorithm;automatic summarization;connected dominating set;experiment;image analysis;internet;mathematical optimization;optimization problem;steiner tree problem;timeline	Dingding Wang;Tao Li;Mitsunori Ogihara	2012			computer science;theoretical computer science;machine learning;data mining	AI	-12.56426701860984	-50.04899801836758	84633
9836a4139857c4ffb271f34985908c8b71a006ee	a top-n recommender model with partially predefined structure	information retrieval;e commerce;parameter estimation top n recommender model partially predefined structure recommender systems user behavioral patterns user behavioral preferences multilabel learning approaches customer transaction learning algorithm generative probabilistic model online resource recommendation complex local correspondence user queries latent random variables;mathematical model data models training correlation testing probabilistic logic context;local influence;recommendation;topic models;information retrieval topic models recommendation user behavior e commerce local influence;user behavior;recommender systems learning artificial intelligence parameter estimation probability query processing random processes	Recommender systems can retrieve appropriate results based on users behavioral patterns and preferences. They may be built based on multi-label learning approaches, as each customer transaction may be labeled with several results that interest him/her. It is therefore useful to model the correlations between labels while controlling complexity of the learning algorithm. This paper presents a generative probabilistic model for online resources (products/URLs) recommendation, by capturing the complex local correspondence between the user's queries and the resources he/she has actually viewed. The structure of our model is partially defined and it is completed according to the observed data. Consequently, several links between observed and/or latent random variables are induced from the training dataset before starting the estimation of parameters. Experiments conducted on real data show the effectiveness of our approach.	algorithm;behavioral pattern;complexity;experiment;local-density approximation;multi-label classification;performance;recommender system;statistical model;user profile;web search engine	El Mehdi Rochd;Mohamed Quafafou	2014	2014 IEEE 11th International Conference on e-Business Engineering	10.1109/ICEBE.2014.29	e-commerce;computer science;machine learning;data mining;database;topic model;world wide web;information retrieval	DB	-17.72929613318153	-48.23878878540664	84665
439976413a08b3683ac760c64d4670b731c23747	joint topic-semantic-aware social recommendation for online voting		Online voting is an emerging feature in social networks, in which users can express their attitudes toward various issues and show their unique interest. Online voting imposes new challenges on recommendation, because the propagation of votings heavily depends on the structure of social networks as well as the content of votings. In this paper, we investigate how to utilize these two factors in a comprehensive manner when doing voting recommendation. First, due to the fact that existing text mining methods such as topic model and semantic model cannot well process the content of votings that is typically short and ambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to learn word and document representation by jointly considering their topics and semantics. Then we propose our Joint Topic-Semantic-aware social Matrix Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates similarity among users and votings by combining their TEWE representation and structural information of social networks, and preserves this topic-semantic-social similarity during matrix factorization. To evaluate the performance of TEWE representation and JTS-MF model, we conduct extensive experiments on real online voting dataset. The results prove the efficacy of our approach against several state-of-the-art baselines.	baseline (configuration management);experiment;feature learning;java topology suite (jts);microsoft word for mac;recommender system;social network;social structure;software propagation;text mining;topic model;word embedding	Hongwei Wang;Jia Wang;Miao Zhao;Jiannong Cao;Minyi Guo	2017		10.1145/3132847.3132889	information retrieval;word embedding;recommender system;semantic data model;artificial intelligence;topic model;machine learning;data mining;computer science;voting;semantics;social network;text mining	AI	-17.48665027229685	-47.01325702395258	84881
057075c7aef81b836ab31722dec80b639cbaacb4	summarizing and understanding large graphs	cs si;physics;minimum description length;physics soc ph;graph visualization;graph summarization;physics and society	How can we succinctly describe a million-node graph with a few simple sentences? Given a large graph, how can we find its most ‘important’ structures, so that we can summarize it and easily visualize it? How can we measure the ‘importance’ of a set of discovered subgraphs in a large graph? Starting with the observation that real graphs often consist of stars, bipartite cores, cliques and chains, our main idea is to find the most succinct description of a graph in these ‘vocabulary’ terms. To this end, we first mine candidate subgraphs using one or more graph partitioning algorithms. Next, we identify the optimal summarization using the Minimum Description Length (MDL) principle, picking only those subgraphs from the candidates that together yield the best lossless compression of the graph—or, equivalently, that most succinctly describe its adjacency matrix. Our contributions are three-fold: (a) formulation: we provide a principled encoding scheme to identify the vocabulary type of a given subgraph for six structure types prevalent in real-world graphs, (b) algorithm: we develop VOG, an efficient method to approximate the MDL-optimal summary of a given graph in terms of local graph structures, and (c) applicability: we report an extensive empirical evaluation on multi-million-edge real graphs, including Flickr and the Notre Dame web graph.	adjacency matrix;approximation algorithm;data structure;distributed computing;effective method;emoticon;flickr;graph (discrete mathematics);graph partition;information theory;line code;lossless compression;mdl (programming language);mapreduce;minimum description length;vocabulary;webgraph;wikipedia	Danai Koutra;U. Kang;Jilles Vreeken;Christos Faloutsos	2015	Statistical Analysis and Data Mining	10.1002/sam.11267	implicit graph;graph power;combinatorics;discrete mathematics;minimum description length;graph bandwidth;null graph;graph property;computer science;clique-width;theoretical computer science;simplex graph;forbidden graph characterization;machine learning;comparability graph;mathematics;voltage graph;distance-hereditary graph;graph;graph drawing;butterfly graph;complement graph;line graph;string graph;strength of a graph;coxeter graph;adjacency matrix;statistics	ML	-10.698644722333919	-39.272174664515376	84896
766c0f1052b9e40fca29ac7da2d33249a254c2bb	overlap community detection using spectral algorithm based on node convergence degree		Community structure is a typical feature of complex networks in cyberspace, and community detection is considered to be crucial to understanding the topology structure, network function and social dynamics of cyberspace. However, some particular nodes may simultaneously belong to several communities in cyberspace. Though there are many algorithms to detect the overlapping communities, most of them are based on the network structure without considering the attributes of the nodes. In this paper, we focus on the convergence characteristic of network and propose an overlap community detection algorithm based on the node convergence degree, which is defined as a combination of attribute convergence degree and structure convergence degree. It combines the network topology with the attributes of the nodes and considers both local and global information of a node. An improved PageRank algorithm is used to get the importance of each node in the global network, while the information of local network is used to measure the structure convergence degree. The overlap communities are thus identified by spectral cluster based on the node convergence degree. Finally, experiment results demonstrate the effectiveness and better performance of our proposed method. We propose an overlapping community detection algorithm based on node convergence degree.We define the structure correlation degree by using the influence of the nodes.We combine the network topology with the attributes of the nodes into the proposed method.The experimental results demonstrate the effectiveness and better performance of our proposed method.	algorithm	Weimin Li;Shu Jiang;Qun Jin	2018	Future Generation Comp. Syst.	10.1016/j.future.2017.08.028	local area network;complex network;network topology;degree distribution;pagerank;distributed computing;computer science;algorithm;community structure;social dynamics;convergence (routing)	Arch	-14.69109959167811	-42.42375467146908	85052
b86b280b638d1b35ee75e377426457bf23722418	structure of mutualistic complex networks	cumulative distribution function;food web;complex network;power law	We consider the structures of six plant-pollinator mutualistic networks. The plants and pollinators are linked by the plant-pollinating relation. We assigned the visiting frequency of pollinators to a plant as a weight of each link. We calculated the cumulative distribution functions of the degree and strength for the networks. We observed a power-law, linear, and stretched exponential dependence of the cumulative distribution function. We also calculated the disparity and the strength of the nodes s(k) with degree k. We observed that the plant-pollinator networks exhibit an disassortative behaviors and nonlinear dependence of the strength on the nodes. In mutualistic networks links with large weight are connected to the neighbors with small degrees.	binocular disparity;nonlinear system;time complexity	Jun Kyung Hwang;Seong Eun Maeng;Moon Yong Cha;Jae Woo Lee	2009		10.1007/978-3-642-02466-5_95	biology;power law;cumulative distribution function;mathematics;ecology;complex network;statistics;food web	ECom	-16.052745618501056	-39.496929322139586	85588
353ded33e67a2b2632dc1b748a540de476b88b8e	instance-based relevance feedback using cluster density	cluster density;instance based;content based image retrieval;relevance feedback	Relevance Feedback is a useful technique to reduce semantic gap which is a bottleneck to a successful Content-Based Image Retrieval system. In this paper, we have discussed an Instance-based relevance feedback method that uses the closeness or similarity of an instance to a class of similar instances. We have achieved improvement in retrieval accuracy by incorporating cluster density of relevant images in addition to the nearness of an image from the relevant and non-relevant image sets. We have experimented with four databases with total image numbers ranging from 1000 to 19511, encompassing both narrow domain and broad domain. With the cluster density approach, we have achieved an improvement of up to 3.7% in averaged precision value as compared to an existing method.	computer cluster;relevance feedback	G. Das;S. Ray	2007			machine learning;pattern recognition;mathematics;information retrieval	Vision	-4.947045374765162	-43.283380839678244	85755
6e822391351c3e15f16b8e37f6c9728946571603	high-dimensional approximate nearest neighbor: k-d generalized randomized forests		We propose a new data-structure, the generalized randomized k -d forest, or k -d GeRaF, for approximate nearest neighbor searching in high dimensions. In particular, we introduce new randomization techniques to specify a set of independently constructed trees where search is performed simultaneously, hence increasing accuracy. We omit backtracking, and we optimize distance computations, thus accelerating queries. We release public domain software GeRaF and we compare it to existing implementations of state-of-the-art methods including BBD-trees, Locality Sensitive Hashing, randomized k -d forests, and product quantization. Experimental results indicate that our method would be the method of choice in dimensions around 1,000, and probably up to 10,000, and pointsets of cardinality up to a few hundred thousands or even one million; this range of inputs is encountered in many critical applications today. For instance, we handle a real dataset of 10 images represented in 960 dimensions with a query time of less than 1sec on average and 90% responses being true nearest neighbors.	approximation algorithm;backtracking;bucket-brigade device;computation;data structure;locality of reference;locality-sensitive hashing;public-domain software;randomized algorithm	Yannis S. Avrithis;Ioannis Z. Emiris;Georgios Samaras	2016	CoRR		theoretical computer science;machine learning;data mining;mathematics;nearest neighbor search	ML	-6.563181661375017	-41.3306867708819	85762
0a5963a599870e562d152709538f43e736de041c	geo-social clustering of places from check-in data	libraries;social network services;standards;social networking online data mining expectation maximisation algorithm iterative methods pattern clustering;social network services clustering algorithms pipelines standards business semantics libraries;semantics;social network;check ins;expectation maximization;pipelines;business;spatio temporal;clustering algorithms;social network dbscan expectation maximization check ins spatio temporal;clustering techniques places geo social clustering check in data location tracking technologies geoscop framework social clustering framework community detection social networks chicken and egg problem iterative procedure expectation maximization dbscan;dbscan	"""In this paper, we develop an algorithm to cluster places not only based on their locations but also their semantics. Specifically, two places are considered similar if they are spatially close and visited by people of similar communities. With the explosion in the availability of location-tracking technologies, it has become easy to track locations and movements of users through user """"check-ins"""". These check-ins provide insights into the community structure of people visiting a place, which is leveraged and integrated into the proposed geo-social clustering framework called GeoScop. While community detection is typically done on social networks, in our problem, we lack any network data. Rather, two people belong to the same community if they visit similar geo-social clusters. We tackle this chicken-and-egg problem through an iterative procedure of expectation maximization and DBSCAN. Extensive experiments on real check-in data demonstrate that GeoScop mines semantically meaningful clusters that cannot be found by using any of the existing clustering techniques. Furthermore, GeoScop is up to 6 times more pure in social quality than the state-of-the-art technique. The executables for the tool are available at http://www.cse.iitm.ac.in/ ~simsayan/software.html."""	cluster analysis;dbscan;entropy maximization;executable;expectation–maximization algorithm;experiment;iterative method;observable;real life;social network	Shivam Srivastava;Shiladitya Pande;Sayan Ranu	2015	2015 IEEE International Conference on Data Mining	10.1109/ICDM.2015.16	expectation–maximization algorithm;computer science;data science;machine learning;data mining;semantics;pipeline transport;cluster analysis;world wide web;dbscan;social network	DB	-15.194613303000525	-44.009307764777446	86324
59aad606950349bd64cc220a0df8eca804229865	learning user attributes via mobile social multimedia analytics	mobile social multimedia analytic;occupation inference;learning user attributes	Learning user attributes from mobile social media is a fundamental basis for many applications, such as personalized and targeting services. A large and growing body of literature has investigated the user attributes learning problem. However, far too little attention has been paid to jointly consider the dual heterogeneities of user attributes learning by harvesting multiple social media sources. In particular, user attributes are complementarily and comprehensively characterized by multiple social media sources, including footprints from Foursqare, daily updates from Twitter, professional careers from Linkedin, and photo posts from Instagram. On the other hand, attributes are inter-correlated in a complex way rather than independent to each other, and highly related attributes may share similar feature sets. Towards this end, we proposed a unified model to jointly regularize the source consistency and graph-constrained relatedness among tasks. As a byproduct, it is able to learn the attribute-specific and attribute-sharing features via graph-guided fused lasso penalty. Besides, we have theoretically demonstrated its optimization. Extensive evaluations on a real-world dataset thoroughly demonstrated the effectiveness of our proposed model.	baseline (configuration management);experiment;ground truth;instagram;inter-process communication;lasso;loss function;mathematical optimization;optimization problem;personalization;social media;unified model	Liqiang Nie;Luming Zhang;Meng Wang;Richang Hong;Aleksandr Farseev;Tat-Seng Chua	2017	ACM TIST	10.1145/2963105	simulation;computer science;artificial intelligence;machine learning;data mining;multimedia	AI	-18.837507864039893	-46.909860478793455	86560
d32342274f0ae4a7c7f0111f8a74c6ec70ca3e68	nearest window cluster queries		In this paper, we study a novel type of spatial queries, namely Nearest Window Cluster (NWC) queries. For a given query location q, NWC (q, l,w,n) retrieves n objects within a window of length l and width w, where the distance between the query location q to these n objects is the shortest. To facilitate efficient NWC query processing, we identify several properties and accordingly develop an NWC algorithm. Moreover, we propose several optimization techniques to further reduce the search cost. To validate our ideas, we conduct a comprehensive performance evaluation using both real and synthetic datasets. Experimental results show that the proposed NWC algorithm, along with the optimization techniques, is very efficient under various datasets and parameter settings.	database;dhrystone;experiment;genetic algorithm;input/output;mathematical optimization;microsoft windows;performance evaluation;window function	Chen-Che Huang;Jiun-Long Huang;Tsung-Ching Liang;Jun-Zhe Wang;Wen-Yuah Shih;Wang-Chien Lee	2016		10.5441/002/edbt.2016.32	data mining;database;computer science;search cost	DB	-5.6516529774779345	-41.06217007112426	86679
7aecd23b926b9e9c99633c5b4ffb1ae3e14a08d6	groupthink and peer pressure: social influence in online social network groups	social network services;sra informations och kommunikationsteknik;social influence social networks;flickr;social sciences computing social networking online;mutually acknowledged relation;sra ict;board of directors;social network services peer to peer computing network topology youtube joining processes data mining facebook educational institutions couplings board of directors;computer and information science;orkut;data mining;peer pressure;area of interest;network topology;social network;youtube;group joining process;friendship groupthink peer pressure online social network groups social influence model orkut youtube livejournal flickr group joining process data sets mutually acknowledged relation;social sciences computing;social influence model;social networks;social networking online;facebook;joining processes;social influence;online social network;couplings;peer to peer computing;data sets;data och informationsvetenskap;online social network groups;livejournal;groupthink;friendship	In this paper, we present a horizontal view of social influence, more specifically a quantitative study of the influence of neighbours on the probability of a particular node to join a group, on four popular Online Social Networks (OSNs), namely Orkut, YouTube, LiveJournal, and Flickr. Neighbours in OSNs have a mutually acknowledged relation, most often defined as friendship, and they are directly connected on a graph of a social network. Users in OSNs can also join groups of users. These groups represent common areas of interest.We present a simple social influence model to describe and explain the group joining process of users on OSNs. To this end, we extract the social influence from data sets of OSNs of a million sample nodes. One of our findings is that a set of neighbours in the OSN is about $100$ times more powerful in influencing a user to join a group than the same number of strangers.	flickr;graph (discrete mathematics);groupthink;social network	Pan Hui;Sonja Buchegger	2009	2009 International Conference on Advances in Social Network Analysis and Mining	10.1109/ASONAM.2009.17	social science;social influence;computer science;social psychology;world wide web;social network	DB	-18.752687748030002	-41.1097769777774	86837
fa1ae980f83886e08e3dffbd0ff708fbd044e0a8	discovering communities in heterogeneous social networks based on non-negative tensor factorization and cluster ensemble approach	community detection;non negative tensor factorization ntf;cluster ensemble;social network analysis;heterogeneous social networks hsns	Identification of the appropriate community structure in social networks is an arduous task. The intricacy of the problem increases with the heterogeneity of multiple types of objects and relationships involved in the analysis of the network. Traditional approaches for community detection focus on the networks comprising of content features and linkage information of the set of single type of entities. However, rich social media networks are usually heterogeneous in nature with multiple types of relationships existing between different types of entities. Cognizant to these requirements, we develop a model for community detection in Heterogeneous Social Networks HSNs employing non-negative tensor factorization method and cluster ensemble approach. Extensive experiments are performed on 20Newsgroup dataset which establish the effectiveness and efficiency of our scheme.	social network	Ankita Verma;Kamal K. Bharadwaj	2015		10.1007/978-3-319-26832-3_15	data science;machine learning;data mining;mathematics	ML	-15.83355302565115	-46.59492169608189	86954
85960acd21b48c4b1dbca36cf03017c6ab481331	synergistic partitioning in multiple large scale social networks	lpa;social network services partitioning algorithms servers distributed databases educational institutions electronic mail parallel processing;metis;baseline independent partitioning method synergistic partitioning multiple large scale social networks sharp expansion network size standalone algorithms parallel computing data intensive information multiple social networks data locality communication overhead multiple large scale network partitioning standalone programs balanced partitions np complete distributed multilevel k way partitioning method mapreduce distributed multilevel partitioning method;social networking online optimisation parallel processing;anchor users;mapreduce;synergistic partitioning;mapreduce synergistic partitioning anchor users metis lpa	Social networks have been part of people's daily life and plenty of users have registered accounts in multiple social networks. Interconnections among multiple social networks add a multiplier effect to social applications when fully used. With the sharp expansion of network size, traditional standalone algorithms can no longer support computing on large scale networks while alternatively, distributed and parallel computing become a solution to utilize the data-intensive information hidden in multiple social networks. As such, synergistic partitioning, which takes the relationships among different networks into consideration and focuses on partitioning the same nodes of different networks into same partitions. With that, the partitions containing the same nodes can be assigned to the same server to improve the data locality and reduce communication overhead among servers, which are very important for distributed applications. To date, there have been limited studies on multiple large scale network partitioning due to three major challenges: 1) the need to consider relationships across multiple networks given the existence of intricate interactions, 2) the difficulty for standalone programs to utilize traditional partitioning methods, 3) the fact that to generate balanced partitions is NP-complete. In this paper, we propose a novel framework to partition multiple social networks synergistically. In particular, we apply a distributed multilevel k-way partitioning method to divide the first network into k partitions. Based on the given anchor nodes which exist in all the social networks and the partition results of the first network, using MapReduce, we then develop a modified distributed multilevel partitioning method to divide other networks. Extensive experiments on two real data sets demonstrate that our method can significantly outperform baseline independent-partitioning method in accuracy and scalability.	algorithm;baseline (configuration management);binary space partitioning;data-intensive computing;distributed computing;equivalence partitioning;experiment;interaction;locality of reference;mapreduce;np-completeness;network partition;non-maskable interrupt;overhead (computing);parallel computing;refinement (computing);scalability;server (computing);social network;synergy	Songchang Jin;Jiawei Zhang;Philip S. Yu;Shuqiang Yang;Aiping Li	2014	2014 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2014.7004243	computer science;theoretical computer science;machine learning;distributed computing	DB	-8.968263803744975	-42.58994942205718	87167
93cef25ad1f94eb9f21342e9187119b54dc951d9	on-line chinese character recognition with attributed relational graph matching	structure methods;state space;attributed relational graph;positional information;character recognition	A structural method for on-line recognition of Chinese characters is proposed, which is stroke order free and allows variations in stroke type and stroke number. Both input characters and the model characters are represented with complete attributed relational graphs (ARGs). An optimal matching measure between two ARGs is defined. Classification of an input character can be implemented by inexactly matching its ARG against every ARG of the model base. The matching procedure is formulated as a search problem of finding the minimum cost path in a state space tree, using the A* algorithm. In order to speed up the search of the A*, besides a heuristic estimate, a novel strategy that utilizes the geometric position information of strokes of Chinese characters to prune the tree is employed. The efficience of our method is demonstrated by the promising experimental results.		Jianzhuang Liu;Michael Ming-Yuen Chang;Wai-kuen Cham	1995		10.1007/3-540-60697-1_102	natural language processing;computer science;state space;artificial intelligence;machine learning;pattern recognition	Vision	-7.391470904155994	-44.942985106110775	87234
41fbaad8d218916bbdc5f2bf6940cfdeefc8d0db	a multi-theoretical framework for social network-based recommendation	social network;non linear multiple kernel learning;recommender systems	Traditional recommender system research often explores customer demographics, product characteristics, and transactions in providing recommendations. This study investigates the recommendation problem based on social network information. In light of the social network theories on the formation of a social network and its impact on human behavior, we present a multi-theoretical framework to model multiple facets of social relations for recommendation. Taking a kernel-based framework, we design and select kernels describing individuals’ similarities projected by social network theories. Moreover, we employ a non-linear multiple kernel learning approach to combine the kernels to increase the dimension of models on assessing individuals’ opinions. We evaluate our proposed framework on a real-world movie review dataset. The experiments show that our framework provides more accurate recommendations than trust-based methods, the collaborative filtering approach, and individual kernels. Further analysis shows that kernels derived from contagion theory and homophily theory contribute a larger portion of the framework.	align (company);analysis of algorithms;association rule learning;collaborative filtering;computation;data mining;experiment;kernel (operating system);math kernel library;multiple kernel learning;nonlinear system;recommender system;scalability;social network;strongly regular graph;theory	Xin Li;Mengyue Wang	2012			social science;computer science;knowledge management;artificial intelligence;marketing;machine learning;data mining;database;world wide web;recommender system;social network	ML	-18.338713319301473	-46.13266992344144	87237
2e3708f69fb25b44184c02fe627bf1ffc8ff3c56	structural vulnerability analysis in complex networks based on core theory	social network services;complex networks;approximation algorithms;biological system modeling;complex systems;sampling methods	Recent studies show that the failures of just a few nodes or edges may disable the operation of the whole network, or even lead to the corruption of the entire system. For a given network and a ratio k, we aim to find out the set with the least number of vulnerable nodes whose removals reduce the connectivity of network structures (suppose to be m) to k*m. We introduce core theory to address this problem. Coritivity is an important measurement to assess the connectivity of network. It is capable of showing not only the difficulty to break down network but also how fragmental the network becomes. We demonstrate that the problem of finding core vertices is an NP-Hard problem. In this case, we propose an approximation algorithm to calculate coritivity of network. The most vulnerable nodes are selected by greedy methods from the set of core nodes found by the previous algorithm. We empirically test our approach on both synthetic and real-world datasets, the results confirm its efficacy compared to random method and three centrality-based algorithms.	approximation algorithm;centrality;greedy algorithm;np-hardness;sampling (signal processing);synthetic intelligence	Kan Zhang;Fei Jiang;Yang Zuo;Yunyun Niu	2016	2016 IEEE First International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2016.101	network science;weighted network;network formation;computer science;dynamic network analysis;theoretical computer science;machine learning;network simulation;distributed computing;interdependent networks;network delay	DB	-14.188430261182662	-41.43012502597502	87273
8aae431a3a67756d92c06a9cbf8d79fe8cc28911	centrality-based bipartite local community detection algorithm		As a kind of typical complex network, bipartite network has already received many specialized research. Local community detection is very useful for bipartite network, but it has not yet been systematically researched. To guarantee equivalent partitioning (obtaining coincident community structure starting from arbitrary node) for two category nodes in bipartite network, a centrality-based bipartite local community detection (CBLCD) algorithm is proposed inspired by water surface undulation. The algorithm first gets local centralized subgraphs/subnetworks according to resource allocation index R defined in this paper, and then expands and combines these subnetworks, until the ultimate local communities are detected. The experimental results on some typical network datasets show that the algorithm achieved both good accuracy and stability.	algorithm;centrality;centralized computing;complex network;undulation of the geoid	Dongming Chen;Wei Zhao;Xinyu Huang;Dongqi Wang;Yanbin Yan	2017		10.1145/3018896.3018958	coincident;complex network;centrality;computer science;local community;bipartite graph;algorithm;resource allocation;community structure	AI	-14.565100196284314	-42.41171046826703	87334
f89ff715b623cf470899b8adf63ffd8d903faedf	natural networks as thermodynamic systems	noncomputable;statistical mechanics;scale free;the principle of least action;entropy;natural process;free energy;power law scaling;evolution	Natural networks are considered as thermodynamic systems that evolve from one state to another by consuming free energy. The least-time consumption of free energy is found to result in ubiquitous scale-free characteristics. The network evolution will yield the scale-independent qualities because the least-time imperative will prefer attachment of nodes that contribute most to the free-energy consumption. The analysis of evolutionary equation of motion, derived from statistical physics of open systems, reveals that evolution of natural networks is a path-dependent and nondeterministic process. Despite the noncomputability of evolution, many mathematical models of networks can be recognized as approximations of the least-time process as well as many measures of networks can be appreciated as practical assessments of the system’s thermodynamic status. 2012 Wiley Periodicals, Inc. Complexity 00: 000–000, 2012	approximation;artificial neural network;attachments;cluster analysis;complex systems;embodied cognition;fermat's principle;imperative programming;john d. wiley;mathematical model;maxima and minima;network theory;nondeterministic algorithm;nonlinear system;one-to-one (data model);open system (computing);path dependence;principle of least action;semantics (computer science);the superficial;transduction (machine learning);universality probability	Tuomo Hartonen;Arto Annila	2012	Complexity	10.1002/cplx.21428	biology;entropy;combinatorics;statistical mechanics;computer science;artificial intelligence;theoretical physics;machine learning;evolution;mathematics;thermodynamics;ecology;physics;algorithm;quantum mechanics;statistics	Robotics	-6.134959084102471	-47.54284570953095	87434
7fe4b870fbc1800323bf3e008575583db04d18ea	correlated couplings and robustness of coupled networks	percolation threshold;giant component;statistical mechanics;information network;data analysis;structure and function;network connectivity;complex system;network structure	Most real-world complex systems can be modelled by coupled networks with multiple layers. How and to what extent the pattern of couplings between network layers may influence the interlaced structure and function of coupled networks are not clearly understood. Here we study the impact of correlated inter-layer couplings on the network robustness of coupled networks using percolation concept. We found that the positive correlated inter-layer coupling enhances network robustness in the sense that it lowers the percolation threshold of the interlaced network than the negative correlated coupling case. At the same time, however, positive inter-layer correlation leads to smaller giant component size in the well-connected region, suggesting potential disadvantage for network connectivity, as demonstrated also with some real-world coupled network strucutres. In the past decade, network has proved to be a useful framework to model structural complexity of complex systems [1, 2]. By abstracting a complex system into nodes (constituents) and links (interactions between them), the resulting graph could be efficiently treated analytically and numerically, through which a large body of new physics of complex systems has been acquired [3, 4]. Most studies until recently have focused on the properties of isolated, single networks, such as the World wide web, coauthorship network of a specific subject, and the metabolic network of a microorganism. In most, if not all, situations, however, a network in question is merely a part of a larger system; world wide web could only function properly in conjuction with physical network of routers, a researcher could take part in multiple social networks via various professional or informal relationships other than coauthorship relations, and a protein in a cell could function both as a metabolic enzyme and by binding with other proteins, thereby taking part in both metabolic and protein networks. Therefore, a more complete description of complex systems would be a system of networks coupled one another. Typical forms of network coupling would be layered structure [5], multi-plexity of links [6], and interaction [7] or interdependency [8] between network layers. Only recently, such coupled network structures have started to be studied actively [7–10]. Leicht and D'Souza [7] studied what they called interacting networks, in which two networks are coupled via inter-network edges, and developed a generating function formalism to study their percolation properties. Buldyrev et al. [8] studied the interdependent networks, in which mutual connec-tivity in two network layers plays an important role, and found that catastrophic …	complex system;complex systems;giant component;graph (discrete mathematics);interaction;interdependence;interdependent networks;interlaced video;numerical analysis;percolation threshold;robustness of complex networks;router (computing);semantics (computer science);social network;structural complexity (applied mathematics);whole earth 'lectronic link;world wide web	Won-kuk Cho;K.-I. Goh;I.-M. Kim	2010	CoRR		combinatorics;statistical mechanics;theoretical computer science;machine learning;mathematics;percolation threshold;interdependent networks;data analysis;giant component	ML	-16.111974717376096	-39.864592086533406	87611
160032f1b11793a40cd20272fb7dad8bef4a26b0	a dynamic modularity based community detection algorithm for large-scale networks: dslm	social network services;complex networks;electronic mail;dslm algorithm dynamic modularity optimizer framework dmo community detection algorithm large scale network dynamic smart local moving algorithm;detection algorithms;distributed computing;optimisation graph theory large scale systems network theory graphs;image edge detection;heuristic algorithms;heuristic algorithms mathematical model social network services detection algorithms electronic mail gsm image edge detection;mathematical model;gsm;graph algorithms	In this work, a new fast dynamic community detection algorithm for large scale networks is presented. Most of the previous community detection algorithms are designed for static networks. However, large scale social networks are dynamic and evolve frequently over time. To quickly detect communities in dynamic large scale networks, we proposed dynamic modularity optimizer framework (DMO) that is constructed by modifying well-known static modularity based community detection algorithm. The proposed framework is tested using several different datasets. According to our results, community detection algorithms in the proposed framework perform better than static algorithms when large scale dynamic networks are considered.	algorithm;mathematical optimization;social network;warhammer 40,000: dark millennium	Riza Aktunc;Ismail Hakki Toroslu;Mert Ozer;Hasan Davulcu	2015	2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1145/2808797.2808822	gsm;computer science;modularity;theoretical computer science;machine learning;mathematical model;distributed computing;complex network;statistics	EDA	-14.581712220337677	-42.7819705709123	87645
1b808061bbec0efcf5eee2cf23f6d33c8dddde84	networks and the epidemiology of infectious disease	biological patents;biomedical journals;text mining;europe pubmed central;qa mathematics;citation search;statistical method;citation networks;information network;research articles;abstracts;analytical method;open access;life sciences;clinical guidelines;ra public aspects of medicine;network theory;full text;infectious disease;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	The science of networks has revolutionised research into the dynamics of interacting elements. It could be argued that epidemiology in particular has embraced the potential of network theory more than any other discipline. Here we review the growing body of research concerning the spread of infectious diseases on networks, focusing on the interplay between network theory and epidemiology. The review is split into four main sections, which examine: the types of network relevant to epidemiology; the multitude of ways these networks can be characterised; the statistical methods that can be applied to infer the epidemiological parameters on a realised network; and finally simulation and analytical methods to determine epidemic dynamics on a given network. Given the breadth of areas covered and the ever-expanding number of publications, a comprehensive review of all work is impossible. Instead, we provide a personalised overview into the areas of network epidemiology that have seen the greatest progress in recent years or have the greatest potential to provide novel insights. As such, considerable importance is placed on analytical approaches and statistical methods which are both rapidly expanding fields. Throughout this review we restrict our attention to epidemiological issues.	communicable diseases;inference;interaction;mathematics;network theory;parkinson disease;simulation;study of epidemiology	Leon Danon;Ashley P. Ford;Thomas House;Chris P. Jewell;Matt J. Keeling;Gareth O. Roberts;Joshua V. Ross;Matthew C. Vernon	2011		10.1155/2011/284909	network theory;biology;text mining;medicine;pathology;infectious disease;computer science;bioinformatics;data science;data mining	ML	-8.458196215608154	-51.07268676096292	87785
18f63d843bc2dadf606e0d95c0d7895ae47c37b5	taming verification hardness: an efficient algorithm for testing subgraph isomorphism	efficient algorithm;indexation;synthetic data;conference proceeding	Graphs are widely used to model complicated data semantics in many applications. In this paper, we aim to develop efficient techniques to retrieve graphs, containing a given query graph, from a large set of graphs. Considering the problem of testing subgraph isomorphism is generally NP-hard, most of the existing techniques are based on the framework of filtering-and-verification to reduce the precise computation costs; consequently various novel feature-based indexes have been developed. While the existing techniques work well for small query graphs, the verification phase becomes a bottleneck when the query graph size increases. Motivated by this, in the paper we firstly propose a novel and efficient algorithm for testing subgraph isomorphism, QuickSI. Secondly, we develop a new feature-based index technique to accommodate QuickSI in the filtering phase. Our extensive experiments on real and synthetic data demonstrate the efficiency and scalability of the proposed techniques, which significantly improve the existing techniques.	algorithm;computation;experiment;futures studies;graph database;np-hardness;scalability;semantic data model;subgraph isomorphism problem;swift (programming language);synthetic data;trie	Haichuan Shang;Xuemin Lin;Jeffrey Xu Yu	2008	PVLDB	10.14778/1453856.1453899	computer science;theoretical computer science;data mining;subgraph isomorphism problem;database;algorithm;statistics;synthetic data	DB	-8.999530358791395	-39.40482817390397	87915
7f55cec7b580daa11f1c1f2bf4bd6014271a99e8	a new algorithm for extracting a small representative subgraph from a very large graph		Many real-world networks are prohibitively large for data r etrieval, storage and analysis of all of its nodes and links. Understanding the structure and dyna mics of these networks entails creating a smaller representative sample of the full graph while pres erving its relevant topological properties. In this report, we show that graph sampling algorithms curre ntly proposed in the literature are not able to preserve network properties even with sample sizes c ontaining as many as 20% of the nodes from the original graph. We present a new sampling algorithm , called Tiny Sample Extractor, with a new goal of a sample size smaller than 5% of the original graph while preserving two key properties of a network, the degree distribution and its clustering coefficient. Our approach is based on a new empirical method of estimating measurement biases in crawl ing algorithms and compensating for them accordingly. We present a detailed comparison of best known graph sampling algorithms, focusing in particular on how the properties of the sample subgraphs con verge to those of the original graph as they grow. These results show that our sampling algorithm extrac ts smaller subgraph than other algorithms while also achieving a closer convergence to the degree dist ribution, measured by the degree exponent, of the original graph. The subgraph generated by the Tiny Sam ple Extractor, however, is not necessarily representative of the full graph with regard to other proper ties such as assortativity. This indicates that the problem of extracting a truly representative small subg raph from a large graph remains unsolved.	algorithm;assortativity;clustering coefficient;degree distribution;naruto shippuden: clash of ninja revolution 3;randomness extractor;sampling (signal processing);the verge;tiny basic	Harish Sethu;Xiaoyu Chu	2012	CoRR		degeneracy;graph power;combinatorics;graph bandwidth;null graph;theoretical computer science;machine learning;mathematics;distance-hereditary graph;random geometric graph;complement graph;strength of a graph	ML	-12.88643645946018	-40.42436218678062	88296
fd2575f08dc121daef8a5a7050c3165d0b4633ea	overlapping community detection in social networks using coalitional games	coalition game;community detection;community detection game;game theory;greedy algorithm;shapley value	Community detection is a significant research problem in various fields such as computer science, sociology and biology. The singular characteristic of communities in social networks is the multimembership of a node resulting in overlapping communities. But dealing with the problem of overlapping community detection is computationally expensive. The evolution of communities in social networks happens due to the self-interest of the nodes. The nodes of the social network acts as self-interested players, who wish to maximize their benefit through interactions in due course of community formation. Game theory provides a systematic framework tox capture the interactions between these selfish players in the form of games. In this paper, we propose a Community Detection Game (CDG) that works under the cooperative game framework. We develop a greedy community detection algorithm that employs Shapley value mechanism and majority voting mechanism in order to disclose the underlying community structure of the given network. Extensive experimental evaluation on synthetic and real-world network datasets demonstrates the effectiveness of CDG algorithm over the state-of-the-art algorithms.	analysis of algorithms;capture the flag;computer science;delay-gradient congestion control;dynamic web page;game theory;greedy algorithm;interaction;social network;synthetic intelligence;tox	Annapurna Jonnalagadda;Lakshmanan Kuppusamy	2017	Knowledge and Information Systems	10.1007/s10115-017-1150-1	game theory;machine learning;artificial intelligence;majority rule;computer science;community structure;shapley value;social network;greedy algorithm	AI	-16.044067621557684	-42.506696679219566	88512
2ff40b1b9286a190663b802ebc571cce6ad837ff	coupon advertising in online social systems: algorithms and sampling techniques		Online social systems have become important platforms for viral marketing where the advertising of products is carried out with the communication of users. After adopting the product, the seed buyers may spread the information to their friends via online messages e.g. posts and tweets. In another issue, electronic coupon system is one of the relevant promotion vehicles that help manufacturers and retailers attract more potential customers. By offering coupons to seed buyers, there is a chance to convince the influential users who are, however, at first not very interested in the product. In this paper, we propose a coupon based online influence model and consider the problem that how to maximize the profit by selecting appropriate seed buyers. The considered problem herein is markedly different from other influence related problems as its objective function is not monotone. We provide an algorithmic analysis and give several algorithms designed with different sampling techniques. In particular, we propose the RA-T and RA-S algorithms which are not only provably effective but also scalable on large datasets. The proposed theoretical results are evaluated by extensive experiments done on large-scale real-world social networks. The analysis of this paper also provides an algorithmic framework for non-monotone submodular maximization problems in social networks.		Guangmo Amo Tong;Weili Wu;Dingzhu Du	2018	CoRR		data mining;computer science;submodular set function;scalability;viral marketing;sampling (statistics);advertising;social system;algorithm;coupon;social network;maximization	Web+IR	-16.820814350181273	-44.06752670687088	88544
e4c41cdce2904845ec01818bbfb078a0046de0e3	a stochastic model for the link analysis of the web	preferential attachment;link analysis;world wide web;stochastic model;power law;steady state	The behavior of inlink and outlink distributions appears to be one of the most studied properties of the web structure. The literature agrees that the inlink distribution follows a power law, but no such agreement exists for the outlink distribution. Accurate observations show that in the low-degree region the link distribution fails to fit a power law with a discrepancy larger for outlinks than for inlinks. Moreover, a power law, as well as any continuous function, does not fit the scattered behavior shared by both the link distributions for large-degree values. The linking model we consider here is a mixed one, based on both the preferential attachment strategy and the uniform attachment strategy. A new approximation technique is devised to detect the parameters of the steady state solution that describe a real data set. A stochastic technique is suggested to describe the scattering of the data. With these techniques the model appears to be well suited for describing both inlink and outlink distribution...		Paola Favati;Grazia Lotti;Ornella Menchi;Francesco Romani	2007	Internet Mathematics	10.1080/15427951.2006.10129132	power law;simulation;link analysis;stochastic modelling;mathematics;steady state;statistics	Web+IR	-18.779060223788672	-41.43547928926308	88738
adcabf9d0febde515b8490e0f4ab23bf804bd3d1	mapreduce-based closed frequent itemset mining with efficient redundancy filtering	itemsets;information filtering;data mining;afopt close;redundancy;mapreduce cloud computing framework mapreduce based closed frequent itemset mining redundancy filtering data mining applications memory requirement computational cost cfi mining algorithm large scale datasets parallelized afopt close algorithm;closed frequent itemset;hadoop closed frequent itemset mapreduce data mining afopt close;clustering algorithms;parallel algorithms cloud computing data mining information filtering;scalability;mapreduce;itemsets data mining algorithm design and analysis redundancy clustering algorithms scalability conferences;hadoop;algorithm design and analysis;conferences;cloud computing;parallel algorithms	Mining closed frequent item set(CFI) plays a fundamental role in many real-world data mining applications. However, memory requirement and computational cost have become the bottleneck of CFI mining algorithms, particularly when confronting with large scale datasets, which herewith makes mining closed frequent item set from large scale datasets a significant and challenging issue. To address the above issue, a parallelized AFOPT-close algorithm is proposed and implemented in this paper based on the cloud computing framework MapReduce, which is widely used to cope with large scale data. Furthermore, an efficient parallelized method for checking if a frequent item set is globally closed is also proposed on the MapReduce platform to further improve the mining performance. Experimental results are then provided and analyzed to verify the efficiency and effectiveness of the proposed methods for mining closed frequent item set.	algorithm;algorithmic efficiency;association rule learning;cloud computing;data mining;mapreduce;parallel computing;scalability	Su-Qi Wang;Yu-Bin Yang;Yang Gao;Guang-Peng Chen;Yao Zhang	2012	2012 IEEE 12th International Conference on Data Mining Workshops	10.1109/ICDMW.2012.24	algorithm design;scalability;cloud computing;computer science;data science;data mining;database;parallel algorithm;cluster analysis;redundancy	ML	-5.120507203677326	-38.97499087741641	88814
7ca3dfdf243e6d04374632ccf650dd0f97072036	click-boosted graph ranking for image retrieval		Graph ranking is one popular and successful technique for image retrieval, but its effectiveness is often limited by the well-known semantic gap. To bridge this gap, one of the current trends is to leverage the click-through data associated with images to facilitate the graph-based image ranking. However, the sparse and noisy properties of the image click-through data make the exploration of such resource challenging. Towards this end, this paper propose a novel click-boosted graph ranking framework for image retrieval, which consists of two coupled components. Concretely, the first one is a click predictor based on matrix factorization with visual regularization, in order to alleviate the sparseness of the click-through data. The second component is a soft-label graph ranker that conducts the image ranking by using the enriched click-through data noise-tolerantly. Extensive experiments for the tasks of click predicting and image ranking validate the effectiveness of the proposed methods in comparison to several existing approaches.	data stream mining;experiment;image retrieval;kerrison predictor;neural coding;sparse matrix;web search engine;whole earth 'lectronic link	Jun Wu;Yu He;Xiaohong Qin;Na Zhao;Yingpeng Sang	2017	Comput. Sci. Inf. Syst.		computer science;ranking (information retrieval);automatic image annotation;visual word;ranking svm;image retrieval;information retrieval;graph database;ranking;graph;pattern recognition;artificial intelligence	Web+IR	-17.242183628515985	-47.643886233023004	88911
4df57994e0ec811a0999998b099c0dc4e730258f	tracing influential nodes in a social network with competing information		We consider the problem of competitive influence maximization where multiple pieces of information are spreading simultaneously in a social network. In this problem, we need to identify a small number of influential nodes as first adopters of our information so that the information can be spread to as many nodes as possible with competition against adversary information. We first propose a generalized model of competitive information diffusion by explicitly characterizing the preferences of nodes. Under this generalized model, we show that the influence spreading process is no longer submodular, which implies that the widely used greedy algorithm does not have performance guarantee. So we propose a simple yet effective heuristic algorithm by tracing the information back according to a properly designed random walk on the network, based on the postulation that all initially inactive nodes can be influenced by our information. Extensive experiments are conducted to evaluate the performance of our algorithm. The results show that our algorithm outperforms many other algorithms in most cases, and is very scalable due to its low running time.	adversary (cryptography);computation;converge;entropy maximization;experiment;greedy algorithm;heuristic (computer science);scalability;social network;submodular set function;time complexity	Bolei Zhang;Zhuzhong Qian;Xiaoliang Wang;Sanglu Lu	2013		10.1007/978-3-642-37456-2_4	computer network	AI	-16.886451847448654	-43.63405599751847	88944
e3095362e4276e22a9b5b143cc82bcbd416b769d	symmetric item set mining based on zero-suppressed bdds	hidden information;efficient algorithm;boolean function;data mining;large scale;binary decision diagram	In this paper, we propose a method for discovering hidden information from large-scale item set data based on the symmetry of items. Symmetry is a fundamental concept in the theory of Boolean functions, and there have been developed fast symmetry checking methods based on BDDs (Binary Decision Diagrams). Here we discuss the property of symmetric items in data mining problems, and describe an efficient algorithm based on ZBDDs (Zero-suppressed BDDs). The experimental results show that our ZBDD-based symmetry checking method is efficiently applicable to the practical size of benchmark databases.	algorithm;benchmark (computing);binary decision diagram;data mining;database;real life;zero-suppressed decision diagram	Shin-ichi Minato	2006		10.1007/11893318_35	discrete mathematics;computer science;theoretical computer science;data mining;mathematics;boolean function;binary decision diagram;algorithm	AI	-7.073591203641942	-38.689652850702224	89018
183c44d2b9ac64e8c795464f91ef98f1e3ba2ea3	efficient ad-hoc search for personalized pagerank	personalized pagerank;graph database;top k search	Personalized PageRank (PPR) has been successfully applied to various applications. In real applications, it is important to set PPR parameters in an ad-hoc manner when finding similar nodes because of dynamically changing nature of graphs. Through interactive actions, interactive similarity search supports users to enhance the efficacy of applications. Unfortunately, if the graph is large, interactive similarity search is infeasible due to its high computation cost. Previous PPR approaches cannot effectively handle interactive similarity search since they need precomputation or approximate computation of similarities. The goal of this paper is to efficiently find the top-k nodes with exact node ranking so as to effectively support interactive similarity search based on PPR. Our solution is Castanet. The key Castanet operations are (1) estimate upper/lower bounding similarities iteratively, and (2) prune unnecessary nodes dynamically to obtain top-k nodes in each iteration. Experiments show that our approach is much faster than existing approaches.	approximation algorithm;computation;experiment;hoc (programming language);iteration;pagerank;personalization;portland pattern repository;precomputation;similarity search	Yasuhiro Fujiwara;Makoto Nakatsuji;Hiroaki Shiokawa;Takeshi Mishima;Makoto Onizuka	2013		10.1145/2463676.2463717	computer science;theoretical computer science;data mining;database;world wide web;graph database	Web+IR	-9.906603683205164	-40.4353563891439	89096
fd1037d3ce03e388ad58f2b934cdc9da7516a3ba	learning geographical hierarchy features for social image location prediction	会议论文	Image location prediction is to estimate the geolocation where an image is taken. Social image contains heterogeneous contents, which makes image location prediction nontrivial. Moreover, it is observed that image content patterns and location preferences correlate hierarchically. Traditional image location prediction methods mainly adopt a single-level architecture, which is not directly adaptable to the hierarchical correlation. In this paper, we propose a geographically hierarchical bi-modal deep belief network model (GHBDBN), which is a compositional learning architecture that integrates multi-modal deep learning model with non-parametric hierarchical prior model. GH-BDBN learns a joint representation capturing the correlations among different types of image content using a bi-modal DBN, with a geographically hierarchical prior over the joint representation to model the hierarchical correlation between image content and location. Experimental results demonstrate the superiority of our model for image location prediction.	bayesian network;blog;deep belief network;deep learning;geolocation;modal logic;multi-level cell;multimodal interaction;network model	Xiaoming Zhang;Xia Hu;Zhoujun Li	2015			computer science;machine learning;pattern recognition;data mining	AI	-17.096099107970627	-47.9851781216627	89099
c6d68c5bac5a35aeae028beadc135fd7e4f86a0f	dimension incremental feature selection approach for vertex cover of hypergraph using rough sets		The minimum vertex cover problem is a well-known optimization problem; it has been used in a wide variety of applications. This paper focuses on rough set-based approach for the minimum vertex cover problem of the dynamic and static hypergraphs. First, we demonstrate the relationship between the attribute reduction of decision table and the minimum vertex cover of hypergraph, and the minimum vertex cover problem is converted to an attribute reduction problem based on this relationship. Then, we discuss the update mechanism of minimum vertex cover from the perspective of attribute reduction, and two types of incremental attribute reduction algorithms are proposed, one is the dynamic increase of single vertex and the other is the dynamic increase of multiple vertices. Our algorithms can quickly update the minimum vertex cover in a dynamic hypergraph and improve the rough sets-based method for the minimum vertex cover problem of a static hypergraph in terms of the computational time and the solution quality. The experimental results show the advantages and limitations of the proposed algorithms compared with the existing algorithms.	algorithm;decision table;feature selection;mathematical optimization;optimization problem;rough set;time complexity;vertex cover	Qian Zhou;Xiaolin Qin;Xiaojun Xie	2018	IEEE Access	10.1109/ACCESS.2018.2868846	graph theory;mathematical optimization;vertex cover;vertex (geometry);hypergraph;feature selection;constraint graph;computer science;distributed computing;optimization problem;rough set	DB	-8.721409273265655	-39.277959967391354	89200
6464c706bdc560da37cd12639207c555d209445d	spectral counting of triangles in power-law networks via element-wise sparsification	eigenvalues and eigenfunctions;graph theory;social networking online data mining graph theory pattern clustering;pattern clustering;complexity theory;social network analysis triangle spectral counting power law network element wise sparsification graph mining clustering coefficient sparsifyingeigentriangle algorithm achlioptas mcsherry sparsification process;achlioptas mcsherry sparsification process;approximation algorithms;computer science social network services density measurement eigenvalues and eigenfunctions spectral analysis statistical analysis statistical distributions matrix converters intrusion detection sparse matrices;usa councils;data mining;eigenvalues;graph mining;triangle spectral counting;social network;clustering coefficient;power law network;social networks;sparsifyingeigentriangle algorithm;social networking online;social network analysis;spectral properties;approximation methods;computer science;power law;eigenvalues triangles social networks;element wise sparsification;triangles	Triangle counting is an important problem in graph mining. The clustering coefficient and the transitivity ratio,two commonly used measures effectively quantify the triangle density in order to quantify the fact that friends of friends tend to be friends themselves. Furthermore, several successful graph mining applications rely on the number of triangles. In this paper, we study the problem of counting triangles in large, power-law networks. Our algorithm, SparcifyingEigenTriangle, relies on the spectral properties of power-law networks and the Achlioptas-McSherry sparsification process. SparcifyingEigenTriangle is easy to parallelize, fast and accurate.We verify the validity of our approach with several experiments in real-world graphs, where we achieve at the same time high accuracy and important speedup versus a straight-forward exact counting competitor.	adjacency matrix;algorithm;clustering coefficient;computer data storage;experiment;low-rank approximation;speedup;structure mining;the matrix;vertex-transitive graph	Charalampos E. Tsourakakis;Petros Drineas;Eirinaios Michelakis;Ioannis Koutis;Christos Faloutsos	2009	2009 International Conference on Advances in Social Network Analysis and Mining	10.1109/ASONAM.2009.32	combinatorics;discrete mathematics;social science;graph theory;theoretical computer science;machine learning;data mining;mathematics;statistics;social network	ML	-11.050101426520433	-41.669452227819086	89405
39a77d02ac47b3edf2b95dd460d185bd243dcd38	optimal constraint-based decision tree induction from itemset lattices	constraint based mining;decision tree learning;decision tree;formal concepts;pattern mining;itemset mining;frequent itemset mining;decision tree induction;decision trees	In this article we show that there is a strong connection between decision tree learning and local pattern mining. This connection allows us to solve the computationally hard problem of finding optimal decision trees in a wide range of applications by post-processing a set of patterns: we use local patterns to construct a global model. We exploit the connection between constraints in pattern mining and constraints in decision tree induction to develop a framework for categorizing decision tree mining constraints. This framework allows us to determine which model constraints can be pushed deeply into the pattern mining process, and allows us to improve the state-of-the-art of optimal decision tree induction.	apriori algorithm;bartpe;categorization;data mining;database;decision tree learning;experiment;formal concept analysis;g.m. nijssen;heuristic;latent class model;lucas sequence;mathematical optimization;preprocessor;structure mining;time complexity;uno;video post-processing	Siegfried Nijssen;Élisa Fromont	2010	Data Mining and Knowledge Discovery	10.1007/s10618-010-0174-x	influence diagram;decision tree model;decision tree learning;computer science;machine learning;decision tree;pattern recognition;alternating decision tree;incremental decision tree;data mining;mathematics;id3 algorithm;decision stump	ML	-5.6974318831730155	-38.30002325655648	89493
99e81b754a042153c4aa0df9c48c51fcdf66ec02	enhance the performance of network computation by a tunable weighting strategy		Networked systems with high computational efficiency are desired in many applications ranging from sociology to engineering. Generally, the performance of the network computation can be enhanced by two ways: rewiring and weighting. In this paper, we proposed a new two-modes weighting strategy based on the concept of communication neighbor graph, which takes use of both the local and global topological properties, e.g., degree centrality, betweenness centrality, and closeness centrality. The weighting strategy includes two modes: In the original mode, it enhances the network synchronizability by increasing the weights of bridge edges; whereas in the inverse version, it increases the significance of community structure by decreasing the weights of bridge edges. The scheme of weighting is controlled by only one parameter, i.e., $\alpha$, which can be easily performed. We test the effectiveness of our model on a number of artificial benchmark networks as well as real-world ones. To the best of our knowledge, the proposed weighting strategy can outperform the existing methods in improving the performance of network computation.	benchmark (computing);betweenness centrality;closeness centrality;computation	Hui-Jia Li;Zhan Bu;Zhen Wang;Jie Cao;Yong Shi	2018	IEEE Transactions on Emerging Topics in Computational Intelligence	10.1109/TETCI.2018.2829906	mathematical optimization;network topology;computation;centrality;ranging;benchmark (computing);community structure;betweenness centrality;weighting	DB	-14.423259201656819	-42.39579099425905	89547
4ef06e4484cc0c367f6254b10a062abc9be52a3f	the influence of search engines on preferential attachment	web graph;search engine;web pages;web community;preferential attachment;degree distribution;social network;barriers to entry;world wide web;power law;power law distribution	"""There is much current interest in the evolution of social networks, especially, the Web graph, through time. """"Preferential attachment"""" and the """"copying model"""" are well-known models which explain the observed degree distribution of the Web graph reasonably closely. We claim that the presence of highly popular search engines like Google substantially mediate the act of hyperlink creation by limiting the author's attention to a small set of """"celebrity"""" URLs. Page authors (who are also Web surfers) frequently (with probability p) locate pages using a search engine. Then they link to popular pages among those they visit. We initiate an analysis of this more realistic process, and show that the celebrity nodes eventually accumulate a constant fraction of all links created whp, and that the degrees of the other nodes still follow a power-law distribution, but with a steeper power: Pr(degree = k) α k-(1+2/(1-p)) Whp. Our analysis adds evidence to the recent concern that search engines offer new Web pages a steep, self-sustaining barrier to entry to well-connected, entrenched Web communities."""	attachments;degree distribution;hyperlink;social network;web page;web search engine;webgraph;world wide web	Soumen Chakrabarti;Alan M. Frieze;Juan Vera	2005	Internet Mathematics	10.1080/15427951.2006.10129129	barriers to entry;power law;degree distribution;computer science;pareto distribution;web page;data mining;mathematics;world wide web;search engine;social network	Theory	-18.726087543408692	-41.45643877795853	89669
62ad2b99c62dd1979f8efa664727dd385a07be85	zoom: ssd-based vector search for optimizing accuracy, latency and memory		"""Minjia Zhang Yuxiong He Microsoft {minjiaz,yuxhe}@microsoft.com Abstract With the advancement of machine learning and deep learning, vector search becomes instrumental to many information retrieval systems, to search and find best matches to user queries based on their semantic similarities. These online services require the search architecture to be both effective with high accuracy and efficient with low latency and memory footprint, which existing work fails to offer. We develop, Zoom, a new vector search solution that collaboratively optimizes accuracy, latency and memory based on a multiview approach. (1) A """"preview"""" step generates a small set of good candidates, leveraging compressed vectors in memory for reduced footprint and fast lookup. (2) A """"fullview"""" step on SSDs reranks those candidates with their full-length vector, striking high accuracy. Our evaluation shows that, Zoom achieves an order of magnitude improvements on efficiency while attaining equal or higher accuracy, comparing with the state-of-the-art."""	deep learning;digital zoom;e-services;information retrieval;lookup table;machine learning;memory footprint;optimizing compiler;page zooming;solid-state drive	Minjia Zhang;Yuxiong He	2018	CoRR		machine learning;memory footprint;deep learning;latency (engineering);latency (engineering);zoom;computer science;search-oriented architecture;artificial intelligence	Web+IR	-12.093063158192413	-49.28141957648441	89705
a4ea4b0d83fa459758fdab2a81e17f03697e1b18	distance-based indexing for high-dimensional metric spaces	distance function;empirical study;metric space;high dimensionality;image database;index structure;object database;garbage collection;indexation;partitions;approximate matching;cyclic garbage	In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces. The mvp-tree uses more than one vantage point to partition the space into spherical cuts at each level. It also utilizes the pre-computed (at construction time) distances between the data points and the vantage points. We have done experiments to compare mvp-trees with vp-trees which have a similar partitioning strategy, but use only one vantage point at each level, and do not make use of the pre-computed distances. Empirical studies show that mvp-tree outperforms the vp-tree 20% to 80% for varying query ranges and different distance distributions.	approximation algorithm;computation;cost efficiency;data domain;data point;dataspaces;experiment;precomputation;similarity search;spaces;top-down and bottom-up design;vantage-point tree;web search query	Tolga Bozkaya;Z. Meral Özsoyoglu	1997		10.1145/253260.253345	vantage-point tree;metric;metric space;computer science;theoretical computer science;intrinsic metric;database;garbage collection;empirical research;distance	DB	-5.6546468874948035	-41.40227082013619	89770
18807562c8ceda0996fb7d85acf11fac58ec4f75	a novel generative encoding for evolving modular, regular and scalable networks	networks;indirect encoding;neuroevolution;regularity;scale free network;generative and developmental representations;scaling up;scale free;pattern recognition;power generation;modularity;scalability	In this paper we introduce the Developmental Symbolic Encoding (DSE), a new generative encoding for evolving networks (e.g. neural or boolean). DSE combines elements of two powerful generative encodings, Cellular Encoding and HyperNEAT, in order to evolve networks that are modular, regular, scale-free, and scalable. Generating networks with these properties is important because they can enhance performance and evolvability. We test DSE's ability to generate scale-free and modular networks by explicitly rewarding these properties and seeing whether evolution can produce networks that possess them. We compare the networks DSE evolves to those of HyperNEAT. The results show that both encodings can produce scale-free networks, although DSE performs slightly, but significantly, better on this objective. DSE networks are far more modular than HyperNEAT networks. Both encodings produce regular networks. We further demonstrate that individual DSE genomes during development can scale up a network pattern to accommodate different numbers of inputs. We also compare DSE to HyperNEAT on a pattern recognition problem. DSE significantly outperforms HyperNEAT, suggesting that its potential lay not just in the properties of the networks it produces, but also because it can compete with leading encodings at solving challenging problems. These preliminary results imply that DSE is an interesting new encoding worthy of additional study. The results also raise questions about which network properties are more likely to be produced by different types of generative encodings.	codi;evolving networks;generative grammar;generative model;hyperneat;pattern recognition;scalability	Marcin Suchorzewski;Jeff Clune	2011		10.1145/2001576.2001781	computer science;artificial intelligence;scale-free network;machine learning;mathematics	ML	-4.561757341357542	-47.037284980214	89806
95d13344ec076d4f0824241455d51c63e0237fdf	species formation in evolving finite state machines	heterogeneous environment;genetics;500 naturwissenschaften und mathematik;finite state machine;evolutionary computing	Since the early beginnings of Evolutionary Computation, Finite State Machines (FSMs) have been applied to model organisms. We present a new approach to evolve such artificial organisms. The FSMs are subject to a difficult navigation and searching task in heterogeneous environments. We give a definition of FSM-species and investigate their formation. The results show that species are formed as the organisms agree on a common 'genetic broadcast language' and take advantage of the fruitful effects of recombination. As observed in natural ecosystems, higher abiotic diversity leads to higher biotic diversity.	finite-state machine	Arno Rasek;Walter Dörwald;Michael Hauhs;Alois Kastner-Maresch	1999		10.1007/3-540-48304-7_20	biology;computer science;bioinformatics;artificial intelligence;machine learning;finite-state machine;genetics;evolutionary computation	Robotics	-5.184155750339355	-48.1238212391372	89924
9be2fabd7e45dc0c91040df8174abb3d0a24295b	handbook of network analysis [konect - the koblenz network collection]		Everything is a network – whenever we look at the interactions between things, a network is formed implicitly. In the areas of data mining, machine learning, information retrieval, etc., networks are modeled as graphs. Many, if not most problem types can be applied to graphs: clustering, classification, prediction, pattern recognition, and others. Networks arise in almost all areas of research, commerce and daily life in the form of social networks, road networks, communication networks, trust networks, hyperlink networks, chemical interaction networks, neural networks, collaboration networks and lexical networks. The content of text documents is routinely modeled as document–word networks, taste as person–item networks and trust as person–person networks. In recent years, whole database systems have appeared specializing in storing networks. In fact, a majority of research projects in the areas of web mining, web science and related areas uses datasets that can be understood as networks. Unfortunately, results from the literature can often not be compared easily because they use different datasets. What is more, different network datasets have slightly different properties, such as allowing multiple or only single edges between two nodes. In order to provide a unified view on such network datasets, and to allow the application of network analysis methods across disciplines, the KONECT project defines a comprehensive network taxonomy and provides a consistent access to network datasets. To validate this approach on real-world data from the Web, KONECT also provides a large number (210+) of network datasets of	artificial neural network;cluster analysis;data mining;database;hyperlink;information retrieval;interaction;machine learning;pattern recognition;social network;statistical classification;telecommunications network;web mining;web of trust;web science;world wide web	Jérôme Kunegis	2014	CoRR		web science;network analysis;complex system;data mining;data science;computer science	ML	-14.778335244966712	-46.924215054752445	90360
2b748fd62dcf88051a538c9765a125a5c184c65d	graph data mining with arabesque	graph exploration;think like an embedding;filter process	Graph data mining is defined as searching in an input graph for all subgraphs that satisfy some property that makes them interesting to the user. Examples of graph data mining problems include frequent subgraph mining, counting motifs, and enumerating cliques. These problems differ from other graph processing problems such as PageRank or shortest path in that graph data mining requires searching through an exponential number of subgraphs. Most current parallel graph analytics systems do not provide good support for graph data mining. One notable exception is Arabesque, a system that was built specifically to support graph data mining. Arabesque provides a simple programming model to express graph data mining computations, and a highly scalable and efficient implementation of this model, scaling to billions of subgraphs on hundreds of cores. This demonstration will showcase the Arabesque system, focusing on the end-user experience and showing how Arabesque can be used to simply and efficiently solve practical graph data mining problems that would be difficult with other systems.	computation;data mining;graph (abstract data type);image scaling;pagerank;programming model;scalability;shortest path problem;time complexity;user experience	Eslam Hussein;Abdurrahman Ghanem;Vinícius Vitor dos Santos Dias;Carlos H. C. Teixeira;Ghadeer AbuOda;Marco Serafini;Georgos Siganos;Gianmarco De Francisci Morales;Ashraf Aboulnaga;Mohammed J. Zaki	2017		10.1145/3035918.3058742	machine learning;pattern recognition;graph	ML	-10.841468161216847	-40.01147095011891	90727
ba65d12957d9ef48d96db010f2bd0e75f8a0b327	a behavioral analysis on the reselection of seed nodes in independent cascade based influence maximization		Influence maximization serves as the main goal of a variety of social network activities such as viral marketing and campaign advertising. The independent cascade model for the influence spread assumes a one-time chance for each activated node to influence its neighbors. This reasonable assumption cannot be bypassed, since otherwise the influence probabilities of the nodes, modeled by the edge weights, would be altered. On the other hand, the manually activated seed set nodes can be reselected without violating the model parameters or assumptions. The reselection of a seed set node, simply means paying extra budget to a previously paid node in order for it to retry its influential skills on its uninfluenced neighbors. This view divides the influence maximization process into two cases: the simple case where the reselection of the nodes is not considered and the reselection case. In this study we will analyze the behavior of real world networks on the difference between these two influence maximization cases. First we will show that the difference between the simple and the reselection cases constitutes a wide spectrum of networks ranging from the reselection-independent ones, where the reselection case has no noticeable advantage to the simple case, to the reselection-friendly ones, where the influence spread in the reselection case is twice the one in the simple case. Then we will correlate this dynamic to other influence maximization dynamics of the network. Finally, a significant entanglement between this dynamic and the network structure is shown and verified by the experiments. In other words, a series of conditions on the network structure is specified whose fulfilment is a sign for a reselection-friendly network. As a result of this entanglement, reselection-friendly networks can be spotted without performing the time consuming influence maximization algorithms.	entropy maximization;expectation–maximization algorithm;experiment;quantum entanglement;retry;seed;social network;the hub (forum);usb hub;writing commons	Ali Vardasbi;Heshaam Faili;Masoud Asadpour	2017	CoRR		artificial intelligence;machine learning;cascade;computer science;viral marketing;quantum entanglement;social network;ranging;maximization	ML	-17.2345547266079	-43.55678513843703	90813
579889fe8d14a130f401c0fd9e3ce82f58e74997	the identification and the elimination of clashes in the structure of an early-stage intermediate in the protein folding process	molecular clash;early stage;protein structure;protein folding	For many years, scientists have been trying to unravel the protein folding process. This paper presents a proposition of an improvement to one of models trying to describe it, the elliptical model, developed by the Department of Bioinformatics and Telemedicine UJ-CM [22]. The model assumes a division of a protein folding process into two stages: the Early-Stage (ES) and the Late-Stage (LS). After the first stage, the second one sometimes occurs unable to perform, because of accidentally created clashes between atoms. This work demonstrates several possible solutions to remove clashes before proceeding to the LS. Additionally, one of presented solutions describes mathematically the precession phenomenon, what might be useful in other than protein folding field of studies such as medical imaging, quantum physics or astronomy.	bioinformatics;least squares;medical imaging;quantum mechanics	Zbigniew Baster	2013	Bio-Algorithms and Med-Systems	10.1515/bams-2013-0113	crystallography;biology;bioinformatics	Comp.	-8.791793560125186	-49.0099800269658	90819
04e1b70e8bd52777d87a39ee43d115d43dda085b	λ-diverse nearest neighbors browsing for multidimensional data	diversity;nearest neighbor searches;computational mechanics;natural neighbors;forestry;query processing;information retrieval;queueing theory;diverse nearest neighbor search;search methods;computational geometry;tree data structures;set theory;trees mathematics;research paper;search problems nearest neighbor searches spatial databases search methods query processing diversity methods information retrieval;spatial databases;gabriel graph diversity diverse nearest neighbor search angular similarity natural neighbors;algorithms;mmr λ diverse nearest neighbors browsing multidimensional data information ranking topic query λ diverse k nearest neighbor search problem geometric methods index based methods natural neighbor based method 2d data 3d data incremental browsing algorithm gabriel graph higher dimensional spaces diverse browsing method distance browsing feature spatial index structures r trees priority queue object mindivdist angular diversity spatial data sets high dimensional data sets factual us points of interest data set k nn search maximal marginal relevance;image analysis;trees mathematics computational geometry query processing queueing theory relevance feedback search problems set theory tree data structures;search problems;nearest neighbor search;decision trees;relevance feedback;angular similarity;diversity methods;gabriel graph;computation;image retrieval	Traditional search methods try to obtain the most relevant information and rank it according to the degree of similarity to the queries. Diversity in query results is also preferred by a variety of applications since results very similar to each other cannot capture all aspects of the queried topic. In this paper, we focus on the \lambda-diverse k-nearest neighbor search problem on spatial and multidimensional data. Unlike the approach of diversifying query results in a postprocessing step, we naturally obtain diverse results with the proposed geometric and index-based methods. We first make an analogy with the concept of Natural Neighbors (NatN) and propose a natural neighbor-based method for 2D and 3D data and an incremental browsing algorithm based on Gabriel graphs for higher dimensional spaces. We then introduce a diverse browsing method based on the distance browsing feature of spatial index structures, such as R-trees. The algorithm maintains a Priority Queue with mindivdist of the objects depending on both relevancy and angular diversity and efficiently prunes nondiverse items and nodes. We experiment with a number of spatial and high-dimensional data sets, including Factual's (http://www.factual.com/) US points-of-interest data set of 13M entries. On the experimental setup, the diverse browsing method is shown to be more efficient (regarding disk accesses) than k-NN search on R-trees, and more effective (regarding Maximal Marginal Relevance (MMR)) than the diverse nearest neighbor search techniques found in the literature.	angularjs;diversification (finance);index (publishing);k-nearest neighbors algorithm;marginal model;maximal set;meet-me room;multi-master replication;nearest neighbor search;priority queue;r-tree;relevance;result set;search problem;spatial database;synthetic data	Onur Küçüktunç;Hakan Ferhatosmanoglu	2013	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2011.251	image analysis;computational geometry;image retrieval;computer science;computational mechanics;theoretical computer science;machine learning;decision tree;computation;data mining;database;mathematics;nearest neighbor search;tree;queueing theory;algorithm;set theory	DB	-5.503947169331261	-41.76388589785339	90851
8881c9be836a42c70389b6530064e96717076330	kernel-mapping recommender system algorithms	structure learning;kernel;maximum margin;qa75 electronic computers computer science;recommender systems;linear operation	Recommender systems apply machine learning techniques for filtering unseen information and can predict whether a user would like a given item. In this paper, we propose kernel based recommender (KBR) algorithms that solve the recommender system problem based on a novel structure learning technique. This paper makes contribution on the followings: we show how (1) user-based and item-based versions of the KBR algorithms can be build; (2) user-based and item-based versions can be combined; (3) more information—features, genre, etc.—can be employed using kernels and how it affects the final results; and (4) to make reliable recommendations under cold-start and long-tail scenarios. By extensive experimental results on five different datasets, we show that the proposed algorithms outperform other state-of-the-art algorithms on large datasets.	algorithm;cold start;kernel (operating system);key-based routing;long tail;machine learning;recommender system	Mustansar Ali Ghazanfar;Adam Prügel-Bennett;Sándor Szedmák	2012	Inf. Sci.	10.1016/j.ins.2012.04.012	kernel;computer science;artificial intelligence;theoretical computer science;machine learning;data mining;recommender system	ML	-18.035254579107587	-49.222784628921424	90928
bf7da46534c40cd54021df72012a562027f943bc	mac/fac retrieval of semantic workflows	retrieval;workflows;similarity;case based reasoning	This paper presents a novel two-step retrieval method for semantic workflow cases, inspired by the MAC/FAC (many are called, but few are chosen) model proposed by Gentner and Forbus. MAC/FAC retrieval is motivated by the computational complexity of graph matching, which is usually involved in the similarity-based retrieval of workflows. An additional computationally efficient retrieval step (MAC stage) is introduced prior to the graph-based retrieval (FAC stage) to perform a pre-selection of potentially relevant cases. The MAC stage is based on a feature representation of the workflows automatically derived from the original graph-based representation. In the paper, we briefly introduce previous work on the semantic workflow retrieval and then we describe the pre-selection step in more detail. A comprehensive evaluation with case bases from the cooking domain is reported with demonstrates that the retrieval time can be significantly reduced without significant negative impact on the retrieval	algorithmic efficiency;computation;computational complexity theory;fly-by-wire;kendall tau distance;matching (graph theory);sql	Ralph Bergmann;Alexander Stromer	2013			workflow;case-based reasoning;similarity;computer science;artificial intelligence;theoretical computer science;data mining;information retrieval	Vision	-7.852395935673746	-40.008765375664666	91018
b83fe4d9504cf95b96d856660d934e011623bfbb	an indexing technique using relative approximation for high-dimensional data	virtual bounding rectangles;relative approximation;relative cells;similarity retrieval;indexation;high dimensional data;high dimensional spaces	Abstract#R##N##R##N#We present a new index structure, the VR-tree, for similarity search in high-dimensional vector space. The motivation of the VR-tree is based on the comparison and analysis of the two best access methods proposed so far: the SR-tree and the VA-File. Since no result on the comparison between the SR-tree and the VA-File is available, we have performed experiments comparing these two access methods. The experiments reveal that these methods each have their problems. The SR-tree offers better performance for nonuniformly distributed data sets. However, as dimensionality increases, large volume of entries in nonleaf nodes causes higher search cost. In contrast, the VA-File is not fit to search for nonuniformly and practical data sets since data skew weakens the effect of filtering by approximation file, which consequently degrades the search performance of the VA-File. Based on analysis of the experimental results, we introduce the VR-tree (Virtual/Real tree), a new index structure which overcomes the problems and achieves higher search performance. The basic idea of the VR-tree is the introduction of Virtual Bounding Rectangle (VBR) and relative cell, which contains and approximates MBR and data object, respectively. We also introduce algorithms based on relative approximation for VBRs and relative cells. Since the volume of entries in nodes is extremely small on tree structures which are constructed by the algorithm of relative approximation, it leads to low search cost. The experimental results demonstrate the effectiveness of the VR-tree. For real data sets, the VR-tree outperforms the SR-tree and the VA-File in all ranges of dimensionality up to 64 dimensions, which is the highest dimension in our experiments. The VR-tree achieves 78.1% (78.6%, resp.) savings in page accesses compared to the SR-tree (the VA-File, resp.) for 64-dimensional real data. © 2003 Wiley Periodicals, Inc. Syst Comp Jpn, 34(12): 77–91, 2003; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1218	approximation	Yasushi Sakurai;Masatoshi Yoshikawa;Shunsuke Uemura;Haruhiko Kojima	2003	Systems and Computers in Japan	10.1002/scj.1218	computer science;artificial intelligence;theoretical computer science;operating system;machine learning;data mining;mathematics;geometry;algorithm;statistics;clustering high-dimensional data	Robotics	-5.755651700671091	-41.19081132750563	91089
39a080c17dec400a6f04af5fe5746dab3a5eb0dc	spade: an efficient algorithm for mining frequent sequences	frequent pattern;efficient algorithm;sequential patterns;frequent patterns;data mining;sequence mining;sequential pattern;knowledge discovery	In this paper we present SPADE, a new algorithm for fast discovery of Sequential Patterns. The existing solutions to this problem make repeated database scans, and use complex hash structures which have poor locality. SPADE utilizes combinatorial properties to decompose the original problem into smaller sub-problems, that can be independently solved in main-memory using efficient lattice search techniques, and using simple join operations. All sequences are discovered in only three database scans. Experiments show that SPADE outperforms the best previous algorithm by a factor of two, and by an order of magnitude with some pre-processed data. It also has linear scalability with respect to the number of input-sequences, and a number of other database parameters. Finally, we discuss how the results of sequence mining can be applied in a real application domain.	algorithm;application domain;computer data storage;database;experiment;guardian service processor;hash table;locality of reference;maximal set;mined;precomputation;relevance;scalability;sequential pattern mining;turing completeness;video post-processing	Mohammed J. Zaki	2001	Machine Learning	10.1023/A:1007652502315	sequential pattern mining;computer science;bioinformatics;data mining;knowledge extraction;algorithm	DB	-6.135959296234601	-38.4470342157593	91265
8b8961e9b89cf67f2620d36b9fa66b840366e103	a unified probabilistic framework for clustering correlated heterogeneous web objects	cluster algorithm;web pages;heterogeneous data;feature space;probabilistic model;mixture model;expectation maximization algorithm;correlated data	Most existing algorithms cluster highly correlated data objects (e.g. web pages and web queries) separately. Some other algorithms, however, do take into account the relationship between data objects, but they either integrate content and link features into a unified feature space or apply a hard clustering algorithm, making it difficult to fully utilize the correlated information over the heterogeneous Web objects. In this paper, we propose a novel unified probabilistic framework for iteratively clustering correlated heterogeneous data objects until it converges. Our approach introduces two latent clustering layers, which serve as two mixture probabilistic models of the features. In each clustering iteration we use EM (Expectation-Maximization) algorithm to estimate the parameters of the mixture model in one latent layer and propagate them to the other one. The experimental results show that our approach effectively combines the content and link features and improves the performance of the clustering.		Guowei Liu;Weibin Zhu;Yong Yu	2005		10.1007/978-3-540-31849-1_9	correlation clustering;statistical model;constrained clustering;data stream clustering;feature vector;k-medians clustering;expectation–maximization algorithm;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;web page;mixture model;data mining;cluster analysis	ML	-15.352230360736016	-47.20129735097225	91696
41ae5e5860ab27e95e9a8e9dd2dabc5a4815b6f0	hierarchical block structures and high-resolution model selection in large networks	websearch;hep	Discovering and characterizing the large-scale topological features in empirical networks are crucial steps in understanding how complex systems function. However, most existing methods used to obtain the modular structure of networks suffer from serious problems, such as being oblivious to the statistical evidence supporting the discovered patterns, which results in the inability to separate actual structure from noise. In addition to this, one also observes a resolution limit on the size of communities, where smaller but well-defined clusters are not detectable when the network becomes large. This phenomenon occurs not only for the very popular approach of modularity optimization, which lacks built-in statistical validation, but also for more principled methods based on statistical inference and model selection, which do incorporate statistical validation in a formally correct way. Here we construct a nested generative model that, through a complete description of the entire network hierarchy at multiple scales, is capable of avoiding this limitation, and enables the detection of modular structure at levels far beyond those possible with current approaches. Even with this increased resolution, the method is based on the principle of parsimony, and is capable of separating signal from noise, and thus will not lead to the identification of spurious modules even on sparse networks. Furthermore, it fully generalizes other approaches in that it is not restricted to purely assortative mixing patterns, directed or undirected graphs, and ad hoc hierarchical structures such as binary trees. Despite its general character, the approach is tractable, and can be combined with advanced techniques of community detection to yield an efficient algorithm that scales well for very large networks.	algorithm;assortative mixing;binary tree;cobham's thesis;complex systems;generative model;graph (discrete mathematics);hoc (programming language);image resolution;mathematical optimization;maximum parsimony (phylogenetics);mixing patterns;model selection;occam's razor;sparse matrix	Tiago P. Peixoto	2013	CoRR	10.1103/PhysRevX.4.011047	physics	ML	-14.51232819201863	-39.695810794127084	92029
1e3cdc0d4668c26ca51dfd41bb85c79ca78ec048	increasing evolvability considered as a large-scale trend in evolution	cultural evolution;theoretical biology;computer model;satisfiability;large scale;direction selectivity;evolutionary computing;evolution	Evolvability is the capacity to evolve. This paper introduces a simple computational model of evolvability and demonstrates that, under certain conditions, evolvability can increase indefinitely, even when there is no direct selection for evolvability. The model shows that increasing evolvability implies an accelerating evolutionary pace. It is suggested that the conditions for indefinitely increasing evolvability are satisfied in biological and cultural evolution. We claim that increasing evolvability is a large-scale trend in evolution. This hypothesis leads to testable predictions about biological and cultural evolution.	computation;computational model	Peter D. Turney	1999	CoRR		mathematical and theoretical biology;computer science;evolution;sociocultural evolution;evolutionary computation;satisfiability	ML	-4.822000841556998	-47.408376005744316	92056
43bb2ea651705e091fa55ede849ff162314c47f5	local community detection based on small cliques		Community detection aims to find dense subgraphs in a network. We consider the problem of finding a community locally around a seed node both in unweighted and weighted networks. This is a faster alternative to algorithms that detect communities that cover the whole network when actually only a single community is required. Further, many overlapping community detection algorithms use local community detection algorithms as basic building block. We provide a broad comparison of different existing strategies of expanding a seed node greedily into a community. For this, we conduct an extensive experimental evaluation both on synthetic benchmark graphs as well as real world networks. We show that results both on synthetic as well as real-world networks can be significantly improved by starting from the largest clique in the neighborhood of the seed node. Further, our experiments indicate that algorithms using scores based on triangles outperform other algorithms in most cases. We provide theoretical descriptions as well as open source implementations of all algorithms used.	benchmark (computing);clique (graph theory);computation;dhrystone;experiment;google compute engine;greedy algorithm;hash table;open-source software;precomputation;social network;synthetic intelligence;weighted network	Michael Hamann;Eike Röhrs;Dorothea Wagner	2017	Algorithms	10.3390/a10030090	artificial intelligence;clique;machine learning;implementation;mathematics;clique percolation method;local community;graph	ML	-11.732597178785532	-41.05929842064174	92105
78e727dcf99d57f6a66fd3121907d49175ad79c2	an improved community partition algorithm integrating mutual information	community partition;social network;mutual information;modularity;integration measure score	The research of community detection can help us analyze kinds of problems in social network, in which the research of community structure is very important. This paper proposes an improved algorithm: An Improved BGLL Integrating Mutual Information (BGLLi), which stabilizes modularity, meanwhile fuses the index of mutual information, so that we can find the optimal threshold of modularity and reduce the running time of community partition effectively. The dataset adopts the Twitter dataset and the public Arxiv dataset, and the related experimental results have verified the effectiveness of the algorithm.	algorithm;mutual information;social network;time complexity	Yang Li;Ying Sha;Jixi Shan;Bo Jiang;Jianjun Wu	2014	JNW	10.4304/jnw.9.9.2289-2298	computer science;data science;machine learning;data mining;modularity;mutual information;statistics;social network	AI	-14.419480724962636	-42.82423392396632	92410
a142e10883b8148a2c9fe1b4790f08ecd894e443	a link-density-based algorithm for finding communities in social networks		Label propagation is a very popular, simple and fast algorithm for detecting communities in a graph such as a social network. However, it known to be non-deterministic, unstable and not very accurate. These shortcoming have attracted much attention by the research community, and many improvements have been suggested. In this paper we propose an new approach for computing preference to stabilize label propagation. The idea is to exploit the structure of the graph at study and use the link density to determine the preference of nodes. Our approach do not require any input parameter aside from the input graph itself. The complexity of propagation-based is slightly increased, but the stabilization and determinism are almost reached. Furthermore, we also propose a fuzzy version of our approach that allows one to detect overlapping communities as common in social networks. We have tested our algorithms with various real-world social networks.	algorithm;social network	Vladivy Poaka;Sven Hartmann;Hui Ma;Dietrich Steinmetz	2016		10.1007/978-3-319-47717-6_7	fuzzy logic;determinism;computer science;social network;algorithm;exploit;graph;aside	ML	-12.926677318947158	-41.77256406547653	92417
3570e0efd4394702347249f2dd33d3c9ee3a3d82	hierarchical change point detection on dynamic networks		This paper studies change point detection on networks with community structures. It proposes a framework that can detect both local and global changes in networks efficiently. Importantly, it can clearly distinguish the two types of changes. The framework design is generic and as such several state-of-the-art change point detection algorithms can fit in this design. Experiments on both synthetic and real-world networks show that this framework can accurately detect changes while achieving up to 800X speedup.	algorithm;emoticon;experiment;global change;global network;social network;speedup;subnetwork;synthetic intelligence;wiki	Yu Wang;Aniket Chakrabarti;David Sivakoff;Srinivasan Parthasarathy	2017		10.1145/3091478.3091493	data mining;machine learning;artificial intelligence;anomaly detection;speedup;change detection;computer science	Vision	-14.918238617827804	-41.729684857332074	92613
5875df2aa9f69e97e4c1be4a91dde539091ba052	deep modeling of social relations for recommendation		Social-based recommender systems have been recently proposed by incorporating social relations of users to alleviate sparsity issue of user-to-item rating data and to improve recommendation performance. Many of these social-based recommender systems linearly combine the multiplication of social features between users. However, these methods lack the ability to capture complex and intrinsic non-linear features from social relations. In this paper, we present a deep neural network based model to learn non-linear features of each user from social relations, and to integrate into probabilistic matrix factorization for rating prediction problem. Experiments demonstrate the advantages of the proposed method over stateof-the-art social-based recommender systems.	artificial neural network;deep learning;nonlinear system;recommender system;sparse matrix	Wenqi Fan;Qing Li;Min Cheng	2018			machine learning;artificial intelligence;computer science;social relation	AI	-18.54239015599503	-47.49393009315414	93090
231edab2b378f5b63c03bf676b7d8b285b9bb80d	a fast similarity join algorithm using graphics processing units	high dimensional dataset;experimental analysis;text mining;sorting;computer graphics throughput text mining multimedia databases computer science automation educational institutions modems arithmetic extraterrestrial measurements;computer graphics;space filling curves graphics processing units fast similarity join algorithm arrays searching sort and search problem;sort and search problem;set theory;space filling curves;fast similarity join algorithm;similarity join;sorting parallel algorithms quadtrees set theory;arrays searching;data structures;feature extraction;graphics processing units;multimedia databases;on the fly;graphic processing unit;hypercubes;arithmetic;parallel machines;modems;space filling curve;computer science;extraterrestrial measurements;multimedia database;quadtrees;algorithm design and analysis;graphics;throughput;parallel algorithms;automation	A similarity join operation A BOWTIEepsiv B takes two sets of points A, B and a value epsiv isin Ropf, and outputs pairs of points p isin A,q isin B, such that the distance D(p, q) les epsiv. Similarity joins find use in a variety of fields, such as clustering, text mining, and multimedia databases. A novel similarity join algorithm called LSS is presented that executes on a graphics processing unit (GPU), exploiting its parallelism and high data throughput. As GPUs only allow simple data operations such as the sorting and searching of arrays, LSS uses these two operations to cast a similarity join operation as a GPU sort-and-search problem. It first creates, on the fly, a set of space-filling curves on one of its input datasets, using a parallel GPU sort routine. Next, LSS processes each point p of the other dataset in parallel. For each p, it searches an interval of one of the space-filling curves guaranteed to contain all the pairs in which p participates. Using extensive theoretical and experimental analysis, LSS is shown to offer a good balance between time and work efficiency. Experimental results demonstrate that LSS is suitable for similarity joins in large high-dimensional datasets, and that it performs well when compared against two existing prominent similarity join methods.	algorithm;cluster analysis;computer graphics;database;graphics processing unit;join (sql);on the fly;parallel computing;search problem;sorting;space-filling curve;text mining;throughput	Michael D. Lieberman;Jagan Sankaranarayanan;Hanan Samet	2008	2008 IEEE 24th International Conference on Data Engineering	10.1109/ICDE.2008.4497520	algorithm design;throughput;text mining;parallel computing;data structure;feature extraction;computer science;sorting;graphics;theoretical computer science;automation;data mining;database;parallel algorithm;computer graphics;algorithm;hypercube;experimental analysis of behavior;set theory	DB	-4.833440980824811	-41.63143139769564	93102
be2f6ce737d14fdb06687690f8625b48cfabf7c6	generalizing generalized cores - an analysis of tag-recommender evaluation procedures		Since the rise of collaborative tagging systems on the web, the tag recommendation task – suggesting suitable tags to users of such systems while they add resources to their collection – has been tackled. However, the (offline) evaluation of tag recommendation algorithms usually suffers from difficulties like the sparseness of the data or the cold start problem for new resources or users. Previous studies therefore often used so-called post-cores (specific subsets of the original datasets) for their experiments. In this paper, we generalize the notion of a core by introducing the new notion of a set-core – that is independent of any graph structure – to overcome a structural drawback in the construction of postcores. We complement the theoretical results with a large-scale experiment in which we analyze different tag recommendation algorithms on different classes of cores on three real-world datasets.	algorithm;cold start;experiment;folksonomy;neural coding;online and offline;post's theorem;recommender system	Stephan Doerfel;Robert Jäschke	2013			machine learning;drawback;cold start;generalization;artificial intelligence;computer science;graph	AI	-16.99681203597609	-47.27680113027793	93316
4359c952d2fa8f559d1c79ddd8ac9420d67a8712	incorporating household structure into a discrete-event simulation model of tuberculosis and hiv	hiv;disease progression;qa mathematics;simulation;human immunodeficiency virus;rb pathology;contact tracing;current control;incidence rate;developing country;health;sub saharan africa;infectious disease;social structure;disease modeling;ra0421 public health hygiene preventive medicine;control strategy;tuberculosis;developing countries;high risk;discrete event simulation	Human immunodeficiency virus (HIV) increases the risks of developing tuberculosis (TB) disease following infection, and speeds up disease progression. This has had a devastating effect on TB epidemics in sub-Saharan Africa, where incidence rates have more than trebled in the past twenty years. Current control methods for TB disease have failed to keep pace with this growth, and there is an urgent need to find TB control strategies that are effective in high-HIV prevalent settings. This article describes a discrete-event simulation model of endemic TB that includes the effects of HIV and of household structure on the transmission dynamics of TB. Incorporating a social structure allows us to compare the effectiveness of contact-tracing interventions with case-finding targeted at high risk groups. We describe the modeling of the household structure in some detail, as this has applications to the modeling of other infectious diseases.	color gradient;incidence matrix;simulation;social structure;terabyte	Georgina R. Mellor;Christine S. M. Currie;Elizabeth L. Corbett	2011	ACM Trans. Model. Comput. Simul.	10.1145/2000494.2000499	simulation;developing country;infectious disease	Graphics	-8.888787758512585	-50.207795480561806	93351
602a55fc8c95973ea408e28d5acb182a1ea5c95c	towards discovery of subgraph bisociations	incollection;bisoziation	The discovery of surprising relations in large, heterogeneous information repositories is gaining increasing importance in real world data analysis. If these repositories come from diverse origins, forming different domains, domain bridging associations between otherwise weakly connected domains can provide insights into the data that are not accomplished by aggregative approaches. In this paper, we propose a first formalization for the detection of such potentially interesting, domaincrossing relations based purely on structural properties of a relational knowledge description.	benchmark (computing);bridging (networking);connectivity (graph theory);domain theory;heuristic (computer science);information retrieval;spaces;universal instantiation	Uwe Nagel;Kilian Thiel;Tobias Kötter;Dawid Piatek;Michael R. Berthold	2012		10.1007/978-3-642-31830-6_18	computer science;bioinformatics;data science;data mining	AI	-12.819385407801242	-39.55637136312729	93764
e96fb94547f02293a64f8473a4be43b196d26be0	efficient pattern matching for graphs with multi-labeled nodes	graph simulation;metropolis hastings;graph matching;qa75 electronic computers computer science;multi labeled graph;qa76 computer software	Graph matching is important for a wide variety of applications in different domains such as social network analysis and knowledge discovery. Despite extensive research over the last few decades, graph matching is still challenging particularly when it comes with new conditions and constraints. In this paper, we focus on a new class of graph matching, in which each node can accept multiple labels instead of one. In particular, we address the problem of finding the topk nodes of a data graph which best match a labeled query node from a given pattern graph. We firstly prove this to be an NP-Complete problem. Then, to address this issue and improve the scalability of our approach, we introduce a more flexible graph simulation, namely surjective simulation . This new graph simulation reduces the unnecessary complexity that is due to the unnecessary constraints imposed by the existing definitions while achieving high-quality matching results. In addition, our approach is associated with an early stop strategy to further boost the performance. To approximate the maximum size of a simulation, our approach utilizes Metropolis Hastings algorithm and ranks the top-k matches after computing the set of surjective simulations. The experimental results over social network graphs demonstrate the efficiency of the proposed approach and superiority over existing approaches. © 2016 Elsevier B.V. All rights reserved.	approximation algorithm;complete (complexity);matching (graph theory);metropolis;metropolis–hastings algorithm;np-completeness;pattern matching;scalability;simulation;social network analysis	Ali Shemshadi;Quan Z. Sheng;Yongrui Qin	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.07.009	metropolis–hastings algorithm;graph bandwidth;null graph;computer science;clique-width;artificial intelligence;theoretical computer science;machine learning;voltage graph;distance-hereditary graph;graph;complement graph;strength of a graph;statistics;matching	DB	-9.08197632561152	-39.55933475463545	93948
1bb2d522c5fdf9f99cf0e47b2d72bb7197f4ddb7	the spreading ability of nodes towards localized targets in complex networks		As an important type of dynamics on complex networks, spread ing is widely used to model many real processes such as the epidemic contagion and i formation propagation. One of the most significant research questions in spreading i s to rank the spreading ability of nodes in the network. To this end, substantial e ffort has been made and a variety of effective methods have been proposed. These methods usually de fine the spreading ability of a node as the number of finally infected nodes given that the spr ading is initialized from the node. However, in many real cases such as advertisin g and medicine science the spreading only aims to cover a specific group of nodes. The refor , it is necessary to study the spreading ability of nodes towards localized targ ets in complex networks. In this paper, we propose a reversed local path algorithm for th is problem. Simulation results show that our method outperforms the existing methods in ide tifying the influential nodes with respect to these localized targets. Moreover, the influ e tial spreaders identified by our method can e ffectively avoid infecting the non-target nodes in the spread ing process.	algorithm;complex network;effective method;integrated development environment;simulation;software propagation;tree rearrangement	Ye Sun;Long Ma;An Zeng;Wen-Xu Wang	2015	CoRR		telecommunications;artificial intelligence;spreading activation	AI	-16.352145596735973	-42.85465766387183	94767
6671f50aec53c8aaf8c651a054fe6707255e0e00	adaptive overlapping community detection with bayesian nonnegative matrix factorization		Overlapping Community Detection from a real network is unsupervised, and it is hard to know the exact community number or quantized strength of every node related to each community. Using Non-negative Matrix Factorization (NMF) for Community Detection, we can find two non-negative matrices from whole network adjacent matrix, and the product of two matrices approximates the original matrix well. With Bayesian explanation in factorizing process, we can not only catch most appropriate count of communities in a large network with Shrinkage method, but also verify good threshold how a node should be assigned to a community in fuzzy situation.	non-negative matrix factorization	Xiaohua Shi;Hongtao Lu;Guanbo Jia	2017		10.1007/978-3-319-55699-4_21	mathematical optimization	AI	-14.10252999192824	-41.7192554402588	94819
a29c33379f101243cc01de026fd3395e9497a2ab	a graph-theoretic clustering methodology based on vertex-attack tolerance		We consider a schema for graph-theoretic clustering of data using a node-based resilience measure called vertex attack tolerance (VAT). Resilience measures indicate worst case (critical) attack sets of edges or nodes in a network whose removal disconnects the graph into separate connected components: the resulting components form the basis for candidate clusters, and the critical sets of edges or nodes form the intercluster boundaries. Given a graph representation G of data, the vertex attack tolerance of G is τ(G) = minS⊂V |S| |V−S−Cmax(V−S)|+1 , where Cmax(V − S) is the largest component remaining in the graph upon the removal of critical node set S. We propose three principal variations of VAT-based clustering methodologies: hierarchical (hier-VAT-Clust), non-hierarchical (VAT-Clust) variations, and variation partial-VAT-Clust. The hierarchical implementation yielded the best results on both synthetic and real datasets. Partial-VAT-Clust is useful in data involving noise, as it attempts to remove the noise while clustering the actual data. We also explored possible graph representations options, such as geometric and k-nearest neighbors, and discuss it in context of clustering efficiency and accuracy. Introduction and Related Work Graph theoretic techniques for clustering are important not only when the data is given in network form, but also due to the established effectiveness of various graph partitioning techniques upon assigning a graph representation to the input data (Xu and Wunsch 2009; 2005). In graph theoretic contexts, the clustering problem is often represented within an optimization framework involving finding a kpartitioning of the vertices of the graph such that the cuts between groups are sparse and there exist additional constraints governing the relative sizes of each group (Shi and Malik 2000; Alpert, Kahng, and Yao 1999). Most commonly, the bi-partitioning problem is considered recursively as a basis for hierarchical clustering: Find the sparsest cut disconnecting the graph (into two groups), and continue finding the sparsest cut within a component until k components result. This variation of the sparsest cut problem Copyright c © 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. is often taken interchangeably with problem of finding the conductance of a graph. Combinatorial conductance or edge based conductance is defined as Φ(G) = minS⊂V,V ol(S)≤V ol(V )/2{ |Cut(S,V−S)| V ol|S| } = minS⊂V,V ol(S)≤V ol(V )/2{ |Cut(S,V−S)| δS |S| } where |Cut(S, V − S)| is the size of the cut separating S from V − S, V ol(S) is the sum of the degrees of vertices in S, and δS is the average degree of vertices in S. Although conductance, multi-way cuts, and sparsest cuts are each hard to approximate to any constant factor (Chawla et al. 2006), relationships between eigenvectors and eigenvalues of the matrix representation of graphs and conductance of graphs has formed the basis of well known spectral approaches to graph partitioning (Shi and Malik 2000; Alpert, Kahng, and Yao 1999). Another popular graph theoretic partitioning algorithm, presented in the context of community detection in social and biological networks, is the Girvan-Newman algorithm (Girvan and Newman 2002). It is based on a greedy removal of the highest betweenness edges until k connected components (clusters) results. The removed edges may be viewed as heuristic approximations of candidate sparse cuts between the resulting components, particularly in extremel scenarios involving edges with very high betweenness centrality. The Girvan-Newman algorithm appears to give meaningful results for certain social and biological networks, and has an advantage of simplicity of computation (if betweenness values are updated accurately and efficiently at each iteration). On the other hand, we are unaware of any previous work in which the Girvan-Newman algorithm has been applied to general datasets that were not originally already in network form. It may be observed that common approaches to graph theoretic clustering, such as those mentioned above, solve a resilience problem on the graph while simultaneously outputting the connected components resulting from the removal of a critical edge set as the set of clusters. Specifically, the types of resilience problems considered thus far in the context of clustering are edge-based resilience problems, such as sparsest cut and conductance, which involve finding a critical edge set whose attack causes the greatest “disconnection” in the network. The duality between clustering and resilience is indicated by noting that critical sets of 404 Proceedings of the Twenty-Eighth International Florida Artificial Intelligence Research Society Conference	approximation algorithm;artificial intelligence;attack tolerance;best, worst and average case;betweenness centrality;binary space partitioning;biological network;cluster analysis;computation;conductance (graph);cut (graph theory);existential quantification;girvan–newman algorithm;graph (abstract data type);graph partition;graph theory;greedy algorithm;heuristic;hierarchical clustering;iteration;k-nearest neighbors algorithm;mathematical optimization;matrix representation;partition problem;recursion;samuel newman;sparse matrix;synthetic intelligence;the matrix;yao graph	Jeffrey Borwey;Darla Ahlert;Tayo Obafemi-Ajayi;Gunes Ercal	2015			correlation clustering;artificial intelligence;cycle graph;machine learning;strength of a graph;vertex (graph theory);computer science;complement graph;clustering coefficient;simplex graph;neighbourhood (graph theory)	AI	-13.64459503704052	-40.92983154632598	95070
24e1bf4fe2bda4e9f56f0903208951c93a38769a	a partition-based approach to graph mining	databases;memory management;databases partitioning algorithms spatiotemporal phenomena bridges tree graphs data structures memory management algorithm design and analysis data engineering;bridges;data engineering;graph mining;tree graphs;dynamic environment;data structures;spatiotemporal phenomena;algorithm design and analysis;partitioning algorithms	Existing graph mining algorithms typically assume that databases are relatively static and can fit into the main memory. Mining of subgraphs in a dynamic environment is currently beyond the scope of these algorithms. To bridge this gap, we first introduce a partition-based approach called PartMiner for mining graphs. The PartMiner algorithm finds the frequent subgraphs by dividing the database into smaller and more manageable units, mining frequent subgraphs on these smaller units and finally combining the results of these units to losslessly recover the complete set of subgraphs in the database. Next, we extend PartMiner to handle updates in the dynamic environment. Experimental results indicate that PartMiner is effective and scalable in finding frequent subgraphs, and outperforms existing algorithms in the presence of updates.	algorithm;computer data storage;database;graph database;lossless compression;scalability;structure mining	Junmei Wang;Wynne Hsu;Mong-Li Lee;Chang Sheng	2006	22nd International Conference on Data Engineering (ICDE'06)	10.1109/ICDE.2006.7	algorithm design;data structure;information engineering;computer science;theoretical computer science;data mining;database;programming language;tree;memory management	DB	-7.692162993949746	-38.2020824539845	95095
2527ec2d94067b6133e7540047beed9c9077eead	modeling topic diffusion in multi-relational bibliographic information networks	multi relational information networks;information diffusion;action prediction	Information diffusion has been widely studied in networks, aiming to model the spread of information among objects when they are connected with each other. Most of the current research assumes the underlying network is homogeneous, i.e., objects are of the same type and they are connected by links with the same semantic meanings. However, in the real word, objects are connected via different types of relationships, forming multi-relational heterogeneous information networks.  In this paper, we propose to model information diffusion in such multi-relational networks, by distinguishing the power in passing information around for different types of relationships. We propose two variations of the linear threshold model for multi-relational networks, by considering the aggregation of information at either the model level or the relation level. In addition, we use real diffusion action logs to learn the parameters in these models, which will benefit diffusion prediction in real networks. We apply our diffusion models in two real bibliographic information networks, DBLP network and APS network, and experimentally demonstrate the effectiveness of our models compared with single-relational diffusion models. Moreover, our models can determine the diffusion power of each relation type, which helps us understand the diffusion process better in the multi-relational bibliographic network scenario.	experiment;threshold model	Huan Gui;Yizhou Sun;Jiawei Han;George Brova	2014		10.1145/2661829.2662000	computer science;artificial intelligence;machine learning;data mining;database;world wide web	ML	-15.413261671086108	-46.444083660657796	95109
700e7ac119a9b479a74829e7afe26809223de064	mining target attribute subspace and set of target communities in large attributed networks		Community detection provides invaluable help for various applications, such as marketing and product recommendation. Traditional community detection methods designed for plain networks may not be able to detect communities with homogeneous attributes inside on attributed networks with attribute information. Most of recent attribute community detection methods may fail to capture the requirements of a specific application and not be able to mine the set of required communities for a specific application. In this paper, we aim to detect the set of target communities in the target subspace which has some focus attributes with large importance weights satisfying the requirements of a specific application. In order to improve the university of the problem, we address the problem in an extreme case where only two sample nodes in any potential target community are provided. A Target Subspace and Communities Mining (TSCM) method is proposed. In TSCM, a sample information extension method is designed to extend the two sample nodes to a set of exemplar nodes from which the target subspace is inferred. Then the set of target communities are located and mined based on the target subspace. Experiments on synthetic datasets demonstrate the effectiveness and efficiency of our method and applications on real-world datasets show its application values.	association rule learning;experiment;extension method;importance sampling;internet backbone;mined;requirement;synthetic intelligence	Peng Wu;Li Pan	2017	CoRR		pattern recognition;data mining;database	ML	-15.788141882524965	-44.94329187329436	95112
63c81c1bc6afced2d89271ed3da2d285e2f2f57f	a generalized tree augmented naive bayes link prediction model		Abstract This paper studies link prediction, a recently emerged hot topic with many important applications, noticeably in complex network analysis. We propose a novel similarity-based approach which improves the well-known naive Bayes method by introducing a new tree augmented naive (TAN) Bayes probabilistic model. It makes better link predictions since the model alleviates the strong independency hypothesis among shared common neighbors to match the real-world situation. To obtain the latent correlation among common neighbors, we exploit mutual information to quantify the influence from neighbors’ neighborhood. This yields a better performance than those methods which employing more local link/triangle structure information. In addition, the TAN model are easily adopted to other common neighbors-based methods such as AA and RA. Experimental results on synthetic and real-world networks show that our algorithms outperform the baseline methods, in terms of both effectiveness and efficiency.	naive bayes classifier	Jiehua Wu	2018	J. Comput. Science	10.1016/j.jocs.2018.04.006	complex network;naive bayes classifier;mathematical optimization;mutual information;computer science;bayes' theorem;pattern recognition;artificial intelligence;statistical model	Theory	-14.569760888726355	-45.85635563479821	95121
fc782692bff0dee70ba50255fda384234fa1dfa7	detecting influence relationships from graphs.		Graphs have been widely used to represent objects and object connections in applications such as the Web, social networks, and citation networks. Mining influence relationships from graphs has gained interests in recent years because providing influence information about the object connections in graphs can facilitate graph exploration, graph search, and connection recommendations. In this paper, we study the problem of detecting influence aspects, on which objects are connected, and influence degree (or influence strength), with which one graph node influences another graph node on a given aspect. Existing techniques focus on inferring either the influence degrees or influence types from graphs. We propose two generative Aspect Influence Models, OAIM and LAIM, to detect both influence aspects and influence degrees. These models utilize the topological structure of the graphs, the text content associated with objects, and the context in which the objects are connected. We compare these two models with one baseline approach which considers only the text content associated with objects. The empirical studies on citation graphs and networks of users from Twitter show that our models can discover more effective results than the baseline approach.	baseline (configuration management);citation graph;graph (discrete mathematics);graph traversal;sensor;social network;world wide web	Chuan Hu;Huiping Cao;Chaomin Ke	2014		10.1137/1.9781611973440.94	generative grammar;artificial intelligence;computer science;machine learning;citation;theoretical computer science;graph;graph node	ML	-16.459466719886283	-45.82952225710649	95222
2a6a2963a6594a149164319c4436ecd41285f9f7	evolutionary progress in heterogenous cellular automata (hetca)		Although very controversial in the field of evolutionary biology, the notion of evolutionary progress is nevertheless generally accepted in the field of Artificial Life. In this article we adopt the definition proposed by Shanahan (2012) to study the existence of evolutionary progress in an evolutionary simulation which we call HetCA. HetCA is a heterogeneous cellular automata characterized by its ability to generate open ended long-term evolution. In this study, we measure evolutionary progress on three criteria: the robustness, size and density of generated genotypes. Our results demonstrate that the oldest genotypes in terms of evolutionary time are frequently the most robust, and that phenotypic density is higher for genotypes collected later in the evolutionary process.	artificial life;automata theory;cellular automaton;evolutionary algorithm;robustness (computer science);simulation	David Medernach;Jeannie Fitzgerald;Simon Carrignon;Conor Ryan	2015		10.7551/978-0-262-33027-5-ch090	evolutionary progress;machine learning;computational biology;artificial intelligence;computer science;cellular automaton	Comp.	-4.920857433409318	-47.292563957826616	95282
f1226e57afae82c6dcc2805ae286536e20d3183e	sql-rank: a listwise approach to collaborative ranking		In this paper, we propose a listwise approach for constructing user-specific rankings in recommendation systems in a collaborative fashion. We contrast the listwise approach to previous pointwise and pairwise approaches, which are based on treating either each rating or each pairwise comparison as an independent instance respectively. By extending the work of (Cao et al., 2007), we cast listwise collaborative ranking as maximum likelihood under a permutation model which applies probability mass to permutations based on a low rank latent score matrix. We present a novel algorithm called SQL-Rank, which can accommodate ties and missing data and can run in linear time. We develop a theoretical framework for analyzing listwise ranking methods based on a novel representation theory for the permutation model. Applying this framework to collaborative ranking, we derive asymptotic statistical rates as the number of users and items grow together. We conclude by demonstrating that our SQL-Rank method often outperforms current state-of-the-art algorithms for implicit feedback such as WeightedMF and BPR and achieve favorable results when compared to explicit feedback algorithms such as matrix factorization and collaborative ranking.		Liwei Wu;Cho-Jui Hsieh;James Sharpnack	2018			mathematics;permutation;machine learning;recommender system;time complexity;missing data;artificial intelligence;matrix decomposition;pattern recognition;pairwise comparison;ranking;pointwise	ML	-18.040818868860487	-47.80405953033947	95499
a7a87d4325f16108b5dae0c42f92662340afae33	blc: private matrix factorization recommenders via automatic group learning		We propose a privacy-enhanced matrix factorization recommender that exploits the fact that users can often be grouped together by interest. This allows a form of “hiding in the crowd” privacy. We introduce a novel matrix factorization approach suited to making recommendations in a shared group (or “nym”) setting and the BLC algorithm for carrying out this matrix factorization in a privacy-enhanced manner. We demonstrate that the increased privacy does not come at the cost of reduced recommendation accuracy.	algorithm;automatic group;privacy;recommender system	Alessandro Checco;Giuseppe Bianchi;Douglas J. Leith	2017	ACM Trans. Priv. Secur.	10.1145/3041760	theoretical computer science;data mining;mathematics;world wide web	Security	-18.76634724015316	-47.885087165922094	95788
2fca2981b5dad40a936b27d1fc6d8e6e3ce8fc43	fraud detection on large scale social networks	fraud detection large scale graphs analysis graph partition and clustering parallel processing;social networking online knowledge representation pattern clustering security of data;pattern clustering;coding model fraud detection social networks data analysis data linking knowledge representation paradigm association mechanism graph analysis data clustering data partitioning mapreduce sql extension;social network services communities algorithm design and analysis clustering algorithms partitioning algorithms joining processes internet;social networking online;knowledge representation;security of data	The incredible growth of the internet use for all kinds of businesses has generated at the same time an increase of fraudulent activities, which calls for developing new methods and tools for detecting fraud and other crimes against banks and customers. Fraud detection needs to analyze and link data, which are gathered from heterogeneous data repositories, and to address problem solving algorithms optimization and parallelization, new knowledge representation paradigms, association mechanisms for linking data, and graph analysis for clustering and partitioning. We present in this paper the motivation of our study and the first steps of the work. We will focus on the emergence of new coding models based on MapReduce and SQL extensions, and on graphs paths issues.	algorithm;cluster analysis;emergence;internet;knowledge representation and reasoning;mapreduce;mathematical optimization;parallel computing;problem solving;sql;sensor;social network	Yaya Sylla;Pierre Morizet-Mahoudeaux;Stephen Brobst	2013	2013 IEEE International Congress on Big Data	10.1109/BigData.Congress.2013.62	computer science;data science;machine learning;data mining;cluster analysis	Metrics	-12.318280047686077	-44.44093861461616	95841
00bdfffebb7df95eeda5e397d645771bad9f1a01	an lsh index for computing kendall's tau over top-k lists		We consider the problem of similarity search within a set of top-k lists under the Kendall’s Tau distance function. This distance describes how related two rankings are in terms of concordantly and discordantly ordered items. As top-k lists are usually very short compared to the global domain of possible items to be ranked, creating an inverted index to look up overlapping lists is possible but does not capture tight enough the similarity measure. In this work, we investigate locality sensitive hashing schemes for the Kendall’s Tau distance and evaluate the proposed methods using two real-world datasets.	inverted index;locality of reference;locality-sensitive hashing;similarity measure;similarity search;lsh	Koninika Pal;Sebastian Michel	2014	CoRR		kendall tau distance;data mining;information retrieval	Web+IR	-5.955571341655845	-41.483138833411644	95893
629cbed1caf59ed3893f619a4a296cb375d0f6d1	distributed k-distance indexing approach for efficient shortest path discovery on large graphs		The emergence of large real life networks such as social networks, web page links, and traffic networks exhibits complex graph structures with millions of vertices and edges. Among many operations for exploiting these graphs, the shortest path discovery is a major and expensive one. Besides the in-memory approaches, many efficient shortest path computation methods have been developed on top of distributed and parallel platforms. Pregel, a bulk synchronous parallel framework, is one of them for processing large graphs. The known shortest path computation approach with Pregel is computation intensive and unable to target real-time services. In this paper, we propose a Pregel based efficient k-distance index technique that allows efficient single pair shortest path discovery. We reduce the network cost and unnecessary operations by transmitting more information in a single superstep. The exten- sive experiments on both real and synthetic datasets reveal the superiority of the proposed approach.		Jihye Hong;Hyunwook Kim;Waqas Nawaz;Kisung Park;Byeong-Soo Jeong;Young-Koo Lee	2014		10.1007/978-3-662-43984-5_6	constrained shortest path first;longest path problem;computer science;theoretical computer science;machine learning;distributed computing;shortest path problem;shortest path faster algorithm	DB	-9.907405981021515	-40.28773156712967	96007
2a3e10b0ff84f36ae642b9431c9be408703533c7	ncdawarerank: a novel ranking method that exploits the decomposable structure of the web	near complete decomposability;link spamming;pagerank;link analysis;ncdawarerank;sparsity;ranking	Research about the topological characteristics of the hyperlink graph has shown that Web possesses a nested block structure, indicative of its innate hierarchical organization. This crucial observation opens the way for new approaches that can usefully regard Web as a Nearly Completely Decomposable(NCD) system; In recent years, such approaches gave birth to various efficient methods and algorithms that exploit NCD from a computational point of view and manage to considerably accelerate the extraction of the PageRank vector. However, very little have been done towards the qualitative exploitation of NCD.  In this paper we propose NCDawareRank, a novel ranking method that uses the intuition behind NCD to generalize and refine PageRank. NCDawareRank considers both the link structure and the hierarchical nature of the Web in a way that preserves the mathematically attractive characteristics of PageRank and at the same time manages to successfully resolve many of its known problems, including Web Spamming Susceptibility and Biased Ranking of Newly Emerging Pages. Experimental results show that NCDawareRank is more resistant to direct manipulation, alleviates the problems caused by the sparseness of the link graph and assigns more reasonable ranking scores to newly added pages, while maintaining the ability to be easily implemented on a large-scale and in a computationally efficient manner.	algorithm;algorithmic efficiency;bigraph;direct manipulation interface;hyperlink;network computing devices;neural coding;pagerank;spamming;world wide web	Athanasios N. Nikolakopoulos;John D. Garofalakis	2013		10.1145/2433396.2433415	link analysis;ranking;theoretical computer science;machine learning;data mining;sparsity-of-effects principle;world wide web;link farm;statistics	Web+IR	-14.042592800670695	-44.393698461747995	96283
13541c5418ee9fcadaccda5da9e0cece3d573434	patenet: pairwise alignment of time evolving networks		Networks that change over time, e.g. functional brain networks that change their structure due to processes such as development or aging, are naturally modeled by time-evolving networks. In this paper we present PATENet, a novel method for aligning time-evolving networks. PATENet offers a mathematically-sound approach to aligning time evolving networks. PATENet leverages existing similarity measures for networks with fixed topologies to define well-behaved similarity measures for time evolving networks. We empirically explore the behavior of PATENet through synthetic time evolving networks under a variety of conditions.		Shlomit Gur;Vasant Honavar	2018		10.1007/978-3-319-96136-1_8	evolving networks;computer science;machine learning;network science;network topology;artificial intelligence;pairwise comparison	ML	-15.336125799456866	-41.547692093525015	96414
bac2e03f00bde2427480c26c707581655d9e9338	enumeration of maximal clique for mining spatial co-location patterns	very large databases data mining grid computing;gridclique algorithm maximal clique enumeration spatial colocation patterns mining sloan digital sky survey data data mining techniques large spatial databases;data mining mesh generation spatial databases information technology australia transaction databases focusing astronomy brightness catalogs;sloan digital sky survey;data mining techniques;large spatial databases;data mining;spatial database;data mining application;sloan digital sky survey data;gridclique algorithm;maximal clique enumeration;very large databases;grid computing;spatial colocation patterns mining	This paper presents a systematic approach to mine co- location patterns in Sloan Digital Sky Survey (SDSS) data. SDSS Data Release 5 (DR5) contains 3.6 TB of data. Availability of such large amount of useful data is an opportunity for application of data mining techniques to generate interesting information. The major reason for the lack of such data mining applications in SDSS is the unavailability of data in a suitable format. This work illustrates a procedure to obtain additional galaxy types from an available attributes and transform the data into maximal cliques of galaxies which in turn can be used as transactions for data mining applications. An efficient algorithm GridClique is proposed to generate maximal cliques from large spatial databases. It should be noted that the full general problem of extracting a maximal clique from a graph is known as NP-Hard. The experimental results show that the GridClique algorithm successfully generates all maximal cliques in the SDSS data and enables the generation of useful co-location patterns.	algorithm;clique (graph theory);data mining;galaxy;maximal independent set;maximal set;np-hardness;spatial database;spatial decision support system;terabyte;transaction data;unavailability	Ghazi Al-Naymat	2008	2008 IEEE/ACS International Conference on Computer Systems and Applications	10.1109/AICCSA.2008.4493526	computer science;data science;data mining;database;data stream mining;spatial database;grid computing	DB	-7.521629616062127	-38.32647913942739	96859
a63ea5180974353ff61a4447c4b775aa029d482a	assembling a network out of ambiguous patches	complex networked systems;graph assembly;network reconstruction;network games and algorithms;data analytics	Many graph mining and network analysis problems rely on the availability of the full network over a set of nodes. But inferring a full network is sometimes non-trivial if the raw data is in the form of many small patches or subgraphs, of the true network, and if there are ambiguities in the identities of nodes or edges in these patches. This may happen because of noise or because of the nature of data; for instance, in social networks, names are typically not unique. Graph assembly refers to the problem of reconstructing a graph from these many, possibly noisy, partial observations. Prior work suggests that graph assembly is essentially impossible in regimes of interest when the true graph is Erdős-Rényi. The purpose of the present paper is to show that a modest amount of clustering is sufficient to assemble even very large graphs. We introduce the G(n, p; q) random graph model, which is the random closure over all open triangles of a G(n, p) Erdős-Rényi, and show that this model exhibits higher clustering than an equivalent Erdős-Rényi. We focus on an extreme case of graph assembly: the patches are small (1-hop egonets) and are unlabeled. We show that in realistic regimes, graph assembly is fundamentally feasible, because we can identify, for every edge e, a subgraph induced by its neighbors that is unique and present in every patch containing e. Using this result, we build a practical algorithm that uses canonical labeling to reconstruct the original graph from noiseless patches. We also provide an achievability result for noisy patches, which are obtained by edge-sampling the original egonets.	algorithm;cluster analysis;erdős number;erdős–rényi model;graph (discrete mathematics);patch (computing);random graph;sampling (signal processing);social network analysis;structure mining	Lyudmila Yartseva;Jefferson Elbert Simães;Matthias Grossglauser	2016	2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/ALLERTON.2016.7852327	graph power;combinatorics;graph bandwidth;null graph;graph property;regular graph;theoretical computer science;forbidden graph characterization;machine learning;mathematics;voltage graph;distance-hereditary graph;graph;critical graph;random geometric graph;complement graph;strength of a graph	Theory	-12.838791763006915	-40.30309057853282	97001
3c8af2a988073500b9488f6022f0f05e21d6b9bc	a simple model to characterize social networks	social network services data models predictive models topology fans complex networks;preferential attachment probability scale free network topology map pajek degree distribution map matlab c mean field theory incident link social network prediction analysis customer relationship data analysis;topology complex networks customer relationship management data analysis probability;mean field theory degree distribution power law distribution anti preferential attachment probability node deletion	For the purpose of prediction analysis of customer relationships in social networks, this paper proposes a simple model that can generate future states of a social network based on relevant data analysis. In this model, nodes and edges of the social network are inserted at the same preferential attachment probabilities, but deleted at different anti-preferential attachment probabilities. In this model, we consider the limit of the network size, the directions of incident links and the factor of time in attractiveness when deleting nodes. Networks generated from this model have a nice property that the degree distribution follows the power-law, which desirably characterizes an essential property of social networks. This property is derived by applying the mean-field theory [7]. It is validated through simulation: we use C++, MATLAB to generate the degree distribution map of our model, and PAJEK to draw the topology map of social networks that was generated by our model. We also show that networks generated from our model can self-organize into scale-free networks. If −C − 1< E < m−2C/2, deleting nodes will not result in destruction of the network.	attachments;c++;degree distribution;essence;matlab;network model;quantum field theory;self-organization;simulation;social network;thematic map	Rui Zeng;Hong Shen;Tianwei Xu	2012	2012 18th IEEE International Conference on Networks (ICON)	10.1109/ICON.2012.6506526	network science;degree distribution;dynamic network analysis;artificial intelligence;scale-free network;machine learning;hierarchical network model;data mining	Metrics	-17.24062497139846	-40.90580982296977	97057
7e9bebaf3c3203035175a8cc1c494625d498927f	accurate path-based methods for influence maximization in social networks	influence maximization;social networks;information diffusion	This paper proposes a novel approach to target-oriented influence estimation, which remedies the drawback of state-ofthe-art, thereby understanding information diffusion more accurately in a social network.	expectation–maximization algorithm;social network	Yun-Yong Ko;Dong-Kyu Chae;Sang Wook Kim	2016		10.1145/2872518.2889407	machine learning;social network	AI	-18.49522883068784	-45.62619229835885	97244
18899010bbe0bb03afb69a18da352a08d6151ec7	a parallel computation of skyline using multiple regression analysis-based filtering on mapreduce	mapreduce based skyline query processing;data sampling;multiple regression analysis based filtering method;histogram-based load balancing	In the last decade, skyline query processing has become widely important because of its usefulness in decision making applications. Since the size of the datasets used for skyline query processing are huge, algorithms for MapReduce-based skyline query processing have been widely studied. However, existing algorithms suffer from low-filtering efficiency for local skyline computation, and unrealistically assume both uniform data distributions and dimensional independence. In this paper, we propose a parallel skyline query processing algorithm for MapReduce using multiple regression analysis. The goal of our algorithm is to efficiently find a set of skylines from a large dataset by reducing the number of candidates prior to the skyline computation. To develop the skyline computation algorithm on anti-correlated datasets, we computed a data filtering threshold line based on a multiple regression analysis of the sampled dataset. To guarantee the accuracy of the skyline result, we considered both a filtering threshold line and a grid-based cell dominance condition. Thus, only relevant data could be computed in the real skyline computation step. For local skyline computation, we utilized an angle-based partitioning of data space that effectively eliminates non-promising points in partitions. For the global skyline computation, we used the dominance relationship among grid-based partitions to prune out unnecessary skyline points. Performance analyses showed that our parallel skyline query processing algorithm outperformed existing algorithms, under various settings.	algorithm;computation;database;dataspaces;experiment;filter (signal processing);mapreduce;parallel computing;pareto efficiency;real-time transcription;scalability;transaction processing	Mi-Young Jang;Youngho Song;Jae-Woo Chang	2017	Distributed and Parallel Databases	10.1007/s10619-017-7202-4	regression analysis;filter (signal processing);computer science;grid;theoretical computer science;skyline;computation	DB	-4.726302323671091	-39.992038560789716	97385
40d2b731161d308c358e4c59eb8591d8b3d0092c	a hierarchical bayesian factorization model for implicit and explicit feedback data		Matrix factorization (MF) is one of the most efficient methods for performing collaborative filtering. An MF-based method represents users and items by latent feature vectors that are obtained by decomposing the rating matrix of users to items. However, MF-based methods suffer from the cold-start problem: if no rating data are available for an item, the model cannot find a latent feature vector for that item, and thus cannot make a recommendation for it. In this paper, we present a hierarchical Bayesian model that can infer the latent feature vectors of items directly from the implicit feedback (e.g., clicks, views, purchases) when they cannot be obtained from the rating data. We infer the full posterior distributions of these parameters using a Gibbs sampling method. We show that the proposed method is strong with overfitting even if the model is very complex or the data are very sparse. Our experiments on real-world datasets demonstrate that our proposed method significantly outperforms competing methods on rating prediction tasks, especially for very sparse datasets.		ThaiBinh Nguyen;Atsuhiro Takasu	2017		10.1007/978-3-319-69179-4_8	machine learning;artificial intelligence;computer science;collaborative filtering;overfitting;factorization;feature vector;matrix decomposition;matrix (mathematics);gibbs sampling;bayesian inference	Robotics	-18.87150303278787	-48.07211230237558	97678
e3c55528c8794228c3e68991f9f65ff451e39872	modeling and simulation on information propagation on instant messaging network based on two-layer scale-free networks with tunable clustering	cluster coefficient;information propagation;pattern clustering;complex networks;instant messaging;probability;modeling and simulation;tunable clustering;stifler value;barium;biological system modeling;scale free network;data mining;stifler value instant messaging network information propagation propagation behavior monitoring scale free network tunable clustering cluster coefficient;propagation behavior monitoring;telecommunication networks complex networks information theory pattern clustering;monitoring;clustering coefficient;clustering;scale free network instant messaging information propagation clustering;mathematical model;clustering algorithms;instant messaging network;information theory;telecommunication networks;monitoring predictive models analytical models cybernetics usa councils intelligent networks programmable logic arrays statistical distributions clustering algorithms performance analysis	Since Instant Messaging Network is one of the most important ways to propagate information, in order to monitor and forecast the propagation behaviors on Instant Messaging Network, it is necessary to study information propagation nature, discipline and methods. Scale-free network with tunable clustering can be used to build instant messaging. Based on it, we propose a new two-layer model of Instant Messaging Network, and formulate its information propagation rules as well. We have simulated this model and observed the diversification of group numbers, clustering coefficient, spreader and stifler. Results show that the more the number of group network is, the larger cluster coefficient and the higher maximum spreader and the higher final value of stifler are. Thus group network and cluster coefficient effect the information propagation.	cluster analysis;clustering coefficient;diversification (finance);instant messaging;simulation;software propagation	Yu Wu;Yanrong Yang;Huanzheng Wu;Gongxiao Wang	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5345997	information theory;computer science;data science;data mining;modeling and simulation;cluster analysis;world wide web;statistics	Robotics	-17.467390012957694	-41.002094130181696	97686
2edb9ee5b68ab500a6363ff72157a244d0c355e9	a two-pass exact algorithm for selection on parallel disk systems	biological patents;biomedical journals;olap queries two pass exact algorithm parallel disk systems deterministic algorithm recursive sampling selection rss out of core selection problem deterministic sampling selection quickselect dss big data handling;parallel databases big data data mining deterministic algorithms;text mining;europe pubmed central;citation search;data mining;citation networks;parallel databases;deterministic algorithms;research articles;big data;abstracts;open access;life sciences;clinical guidelines;full text;algorithm design and analysis decision support systems sorting robustness approximation algorithms gaussian distribution uncertainty;rest apis;orcids;europe pmc;biomedical research;olap queries selection parallel disk system median;bioinformatics;literature search	Numerous OLAP queries process selection operations of “top N”, median, “top 5%”, in data warehousing applications. Selection is a well-studied problem that has numerous applications in the management of data and databases since, typically, any complex data query can be reduced to a series of basic operations such as sorting and selection. The parallel selection has also become an important fundamental operation, especially after parallel databases were introduced. In this paper, we present a deterministic algorithm Recursive Sampling Selection (RSS) to solve the exact out-of-core selection problem, which we show needs no more than (2 + ε) passes (ε being a very small fraction). We have compared our RSS algorithm with two other algorithms in the literature, namely, the Deterministic Sampling Selection and QuickSelect on the Parallel Disks Systems. Our analysis shows that DSS is a (2+ε)-pass algorithm when the total number of input elements N is a polynomial in the memory size M (i.e., N = Mc for some constant c). While, our proposed algorithm RSS runs in (2+ε) passes without any assumptions. Experimental results indicate that both RSS and DSS outperform QuickSelect on the Parallel Disks Systems. Especially, the proposed algorithm RSS is more scalable and robust to handle big data when the input size is far greater than the core memory size, including the case of N ≪ Mc.	big data;dss brand of docusate sodium;deterministic algorithm;exact algorithm;floppy disk;genetic selection;hl7publishingsubsection <operations>;information;magnetic-core memory;online analytical processing;out-of-core algorithm;parallel database;petrosal sinus sampling;polynomial;published database;question (inquiry);quickselect;rss;recursion (computer science);scalability;selection algorithm;sorting	Tian Mi;Sanguthevar Rajasekaran	2013	2013 IEEE Symposium on Computers and Communications (ISCC)	10.1109/ISCC.2013.6755015	text mining;medical research;big data;computer science;theoretical computer science;operating system;data mining;database;statistics	Theory	-5.510184348330413	-39.82450001502436	97730
0a5a49d196e56ddf58c65977eeed1db01b2a5815	diffusion of “following” links in microblogging networks	social networking online information dissemination recommender systems;delay effects;diffusion processes;social network;social network link diffusion triad formation;followee recommendation following links microblogging networks social network neighboring links link diffusion process triadic structures diffusion strength learned diffusion strength follower maximization;diffusion processes twitter integrated circuit modeling linear programming delay effects computer science;integrated circuit modeling;linear programming;triad formation;computer science;twitter;link diffusion	When a “following” link is formed in a social network, will the link trigger the formation of other neighboring links? We study the diffusion phenomenon of the formation of “following” links by proposing a model to describe this link diffusion process. To estimate the diffusion strength between different links, we first conduct an analysis on the diffusion effect in 24 triadic structures and find evident patterns that facilitate the effect. We then learn the diffusion strength in different triadic structures by maximizing an objective function based on the proposed model. The learned diffusion strength is evaluated through the task of link prediction and utilized to improve the applications of follower maximization and followee recommendation, which are specific instances of influence maximization. Our experimental results reveal that incorporating diffusion patterns can indeed lead to statistically significant improvements over the performance of several alternative methods, which demonstrates the effect of the discovered patterns and diffusion model.	baseline (configuration management);c date and time functions;causality;entropy maximization;expectation–maximization algorithm;experiment;generative model;loss function;one-way function;optimization problem;randomized algorithm;social network	Jing Zhang;Zhanpeng Fang;Wei Chen;Jie Tang	2015	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2015.2407351	simulation;computer science;linear programming;artificial intelligence;machine learning;social network	ML	-17.851715592378124	-43.442571587379966	97824
85899f17a9413c2d69cc1d2191274fe0db15ef20	collaborative filtering incorporating review text and co-clusters of hidden user communities and item groups	co clustering;collaborative filtering;user community;item group;topic model	Most collaborative filtering (CF) algorithms only make use of the rating scores given by users for items. However, it is often the case that each rating score is associated with a piece of review text. Such review texts, which are capable of providing us valuable information to reveal the reasons why users give a certain rating, have not been exploited and they are usually ignored by most CF algorithms. Moreover, the underlying relationship buried in users and items has not been fully exploited. Items we would recommend can often be characterized into hidden groups (e.g. comedy, horror movie and action movie), and users can also be organized as hidden communities. We propose a new generative model to predict user's ratings on previously unrated items by considering review texts as well as hidden user communities and item groups relationship. Regarding the rating scores, traditional algorithms would not perform well on uncovering the community and group information of each user and each item since the user-item rating matrix is dyadic involving the mutual interactions between users and items. Instead, co-clustering, which is capable of conducting simultaneous clustering of two variables, is able to take advantage of such user-item relationships to better predict the rating scores. Additionally, co-clustering would be more effective for modeling the generation of review texts since different user communities would discuss different topics and vary their own wordings or expression patterns when dealing with different item groups. Besides, by modeling as a mixed membership over community and group respectively, each user or item can belong to multiple communities or groups with varying degrees. We have conducted extensive experiments to predict the missing rating scores on 22 real word datasets. The experimental results demonstrate the superior performance of our proposed model comparing with the state-of-the-art methods.	algorithm;biclustering;cluster analysis;collaborative filtering;dyadic transformation;experiment;generative model;interaction	Yinqing Xu;Wai Lam;Tianyi Lin	2014		10.1145/2661829.2662059	computer science;collaborative filtering;machine learning;data mining;database;multimedia;topic model;world wide web;biclustering;information retrieval	AI	-18.63139055373026	-47.6924644843714	97936
94a6b4c79505429f5d98c7bfeb6aa9ecf0a14587	a cultural algorithm for the representation of mitochondrial population	atp production;mitochondrial population;autonomous culture;mitochondrial dynamic;combinatorial representation;mitochondrial dysfunctions;mammalian cell;alternative theoretical framework;particle-based brownian dynamics simulation;bioinspired algorithm;cultural algorithm	We propose a novel Cultural Algorithm for the representation of mitochondrial population in mammalian cells as an autonomous culture. While mitochondrial dysfunctions are highly associated with neurodegenerative diseases and related disorders, an alternative theoretical framework is described for the representation of mitochondrial dynamics. A new perspective of bioinspired algorithm is produced, combining the particle-based Brownian dynamics simulation and the combinatorial representation of mitochondrial population in the lattice, involving the optimization problem of ATP production in mammalian cells.	cultural algorithm	Athanasios Alexiou;Panayiotis M. Vlamos	2012	Adv. Artificial Intellegence	10.1155/2012/457351	bioinformatics	NLP	-4.997390565373632	-48.4810138962431	98056
67db3218de0ab576c7ec8a9fa54c06423ce4f26b	scalable functional dependencies discovery from big data	big data functional dependencies discovering functional dependencies knowledge discovery;functional dependencies;lattices;distributed databases big data lattices algorithm design and analysis partitioning algorithms knowledge discovery;resource allocation big data data mining distributed processing relational databases;big data;distributed databases;load balance scalable functional dependencies discovery fd relational databases functional dependencies mining distributed big data;discovering functional dependencies;algorithm design and analysis;partitioning algorithms;knowledge discovery	Functional dependencies (FDs) represent potentially novel and interesting patterns existent in relational databases. The discovery of functional dependencies has a wide range of applications such as database design, knowledge discovery, data quality assessment, etc. There has been growing interest in the problem of functional dependencies discovery in the last ten years. However, existing functional dependencies discovery algorithms are mainly applied to centralized small data. It is far more challenging to discover functional dependencies from big data. In this paper, we propose an efficient functional dependencies discovery algorithm, for mining functional dependencies from distributed big data. We prune candidate FDs at each node by local fragmented data and batch verify candidate FDs in parallel. Load balance is taken into account when discovering functional dependencies. Experiments show that the proposed algorithm is effective on real dataset and synthetic dataset.	big data;functional dependency	Shouzhong Tu;Minlie Huang	2016		10.1109/BigMM.2016.63	dependency theory;computer science;data science;data mining;database	ML	-5.032890729212458	-38.800581648896774	98184
7363b12fb9563bbf4026ad11375e9ffb7728ac0f	the diffusion of viral content in multi-layered social networks	diffusion of information;multiplex networks;multi layered social networks;clustering algorithms;social network analysis	Modelling the diffusion of information is one of the key areas related to activity within social networks. In this field, there is recent research associated with the use of community detection algorithms and the analysis of how the structure of communities is affecting the spread of information. The purpose of this article is to examine the mechanisms of diffusion of viral content with particular emphasis on cross community diffusion.	algorithm;social network	Jaroslaw Jankowski;Michal Kozielski;Wojciech Filipowski;Radoslaw Michalski	2013		10.1007/978-3-642-40495-5_4	social network analysis;social science;simulation;computer science;dynamic network analysis;machine learning;cluster analysis	ML	-17.7858343638209	-41.63864519802398	98327
709307caf91f613b1908caabf5aefdd44886e507	the category structure in wikipedia: to analyze and know its quality using k-core decomposition	complex network;k-core;overall topology;quality;wikipedia	Wikipedia is a famous and free encyclopedia. A network based on its category structure is built and then analyzed from various aspects, such as the connectivity distribution, evolution of the overall topology. As an innovative point of our paper, the model that is on the base of the k-core decomposition is used to analyze evolution of the overall topology and test the quality (that is, the error and attack tolerance) of the structure when nodes are removed. The model based on removal of edges is compared. Our results offer useful insights for the growth and the quality of the category structure, and the methods how to better organize the category structure.	wikipedia	Qishun Wang;Xiaohua Wang;Zhiqun Chen	2013		10.1007/978-3-642-39787-5_37	data science;data mining;information retrieval	NLP	-16.616550446332166	-41.02246239088489	98344
264027e65712cff361943fbed0076a6309260b93	supergraph based periodic pattern mining in dynamic social networks		In dynamic networks, periodically occurring interactions express especially significant meaning. However, these patterns also could occur infrequently, which is why it is difficult to detect while working with mass data. To identify such periodic patterns in dynamic networks, we propose single pass supergraph based periodic pattern mining SPPMiner technique that is polynomial unlike most graph mining problems. The proposed technique stores all entities in dynamic networks only once and calculate common sub-patterns once at each timestamps. In this way, it works faster. The performance study shows that SPPMiner method is time and memory efficient compared to others. In fact, the memory efficiency of our approach does not depend on dynamic network’s lifetime. By studying the growth of periodic patterns in social networks, the proposed research has potential implications for behavior prediction of intellectual communities.	data mining;social network	Sajal Halder;Md. Samiullah;Young-Koo Lee	2017	Expert Syst. Appl.	10.1016/j.eswa.2016.10.033	simulation;computer science;machine learning;data mining	ML	-11.43307566047309	-38.21847281921152	98587
8446459d3b31f73541a5184e89ec608e6f99c2e5	ranking websites: a probabilistic view	computational complexity;web search;error bound;markov chain	In this paper we suggest evaluating the importance of a website with the mean frequency of visiting the website for the Markov chain on the Internet Graph describing random surfing. We show that this mean frequency is equal to the sum of the PageRanks of all the webpages in that website (hence is referred to as PageRankSum ), and propose a novel algorithm ’AggregateRank’ based on the theory of stochastic complement to calculate the rank of a website. The AggregateRank Algorithm gives a good approximation of the PageRankSum accurately, while the corresponding computational complexity is much lower than PageRankSum. By constructing return-time Markov chains restricted to each website, we describe also the probabilistic relation between PageRank and AggregateRank. The complexity of the AggregateRank Algorithm, the error bound of the estimation, and experiments are discussed at the end of the paper.	algorithm;approximation;computational complexity theory;experiment;markov chain;pagerank	Ying Bao;Guang Feng;Tie-Yan Liu;Zhiming Ma;Ying Wang	2007	Internet Mathematics	10.1080/15427951.2006.10129125	markov chain;combinatorics;computer science;theoretical computer science;data mining;mathematics;computational complexity theory;world wide web;statistics	ML	-10.889101984610141	-42.765258305584226	98766
8141773f8afd93d493733c30bab0424ea5a0fc57	learning from social media network	social media network	Recent years have witnessed the popularity of Web 2.0 content. Examples include Flickr, YouTube, Facebook, MySpace, etc. The proliferation of such applications on social web and social networks have produced a new type of multimedia content, termed as ‘‘social media’’ here as it is created by people using highly accessible and scalable publishing technologies for sharing via the web. The intrinsic attributes of social media is to facilitate interactive information sharing, interoperability and collaboration on the internet. By virtue of that, web images, videos and audios are generally accompanied by user-contributed contextual information such as, tag, category, title, metadata, comments, and viewer ratings, etc. Massive emerging social media data offer new opportunities for resolving the long-standing challenges such as how can we build video indexing and search benefit from the shared videos and other metadata? How can we deal with the large scale web videos by leveraging the video content and the user contributed information? Furthermore, this new media also introduces many challenging and new research problems and many exciting real-world applications (e.g. social image search, social group recommendation, etc.). This special issue is organized with the purpose of introducing novel research work on learning from social media network, i.e., how to use learning technology to facilitate the analysis of social media network and further make application under the scenario of Web 2.0 benefit from that. Submissions have come from an open call for paper. With the assistance of professional referees, 14 papers are selected after at least two rounds of rigorous reviews. These papers cover widely subtopics of learning from social media network, including social image search, image and video concept detection, geo-information mining based on social media network, social media data mining, and so on. The first part of the special issue contains four papers on the image and video concept detection. In the first paper ‘‘Constructing Visual Tag Dictionary by Mining Community-Contributed Images’’, Wang et al. construct a corpus named visual tag dictionary by mining community-contributed images. With this fully automatically constructed dictionary, tags and images are connected via visual words and many applications can be facilitated such as tag-based image search, tag ranking, image annotation and tag graph construction. In the second paper ‘‘Exploring Multi-Modality Structure for Cross Domain Adaptation in Video Concept Annotation’’, Xu et al. leverage multi-modality knowledge generalized by auxiliary classifier in the source domains to assist multi-graph optimization in the target domain for video concept annotation. The third paper ‘‘Collaborative Visual Modeling for Automatic Image Annotation via Sparse Model Coding’’ focuses on the exploiting the visual relatedness information among different	automatic image annotation;data mining;dictionary;digital video;domain adaptation;flickr;image retrieval;interoperability;mathematical optimization;modality (human–computer interaction);new media;scalability;social media;social network;sparse;video clip;visual modeling;web 2.0	Richang Hong;Ling Shao	2012	Neurocomputing	10.1016/j.neucom.2012.02.025	educational technology	Web+IR	-17.52746053698173	-47.34921458176854	99368
8fb06bea25790284ee56b4081e46157a0806a8fc	inferring anchor links based on social network structure		Nowadays, people usually participate in multiple social networks simultaneously, e.g., Facebook and Twitter. Formally, the correspondences of the accounts that belong to the same user are defined as anchor links, and the networks aligned by anchor links can be denoted as aligned networks. In this paper, we study the problem of anchor link prediction (ALP) across a pair of aligned networks based on social network structure. First, three similarity metrics (CPS, CCS, and CPS+) are proposed. Different from the previous works, we focus on the theoretical guarantees of our metrics. We prove mathematically that the node pair with the maximum CPS or CPS+ should be an anchor link with high probability and a correctly predicted anchor link must have a high value of CCS. Second, using the CPS+ and CCS, we present a two-stage iterative algorithm CPCC to solve the problem of the ALP. More specifically, we present an early termination strategy to make a tradeoff between precision and recall. At last, a series of experiments are conducted on both synthetic and real-world social networks to demonstrate the effectiveness of the CPCC.	algorithm;anchor text;experiment;iterative method;precision and recall;social network;synthetic intelligence;with high probability	Shuo Feng;Derong Shen;Tiezheng Nie;Yue Kou;Jingrui He;Ge Yu	2018	IEEE Access	10.1109/ACCESS.2018.2814000	iterative method;precision and recall;distributed computing;social network;computer science	DB	-16.196908677722167	-44.10017131362093	99551
6b94b933a25fef2682d7bdfe4507906333886665	several remarks on mining frequent trajectories in graphs	frequent trajectory;path support;graph structure;relevant support;mining technique;traditional frequent item set;market basket data;simple path;simple algorithm;path tree	We apply techniques that originate in the analysis of market basket data sets to the study of frequent trajectories in graphs. Tr ajectories are defined as simple paths through a directed graph, and we put forth som e definitions and observations about the calculation of supports of paths in t his context. A simple algorithm for calculating path supports is introduced and a alyzed, but we explore an algorithm which takes advantage of traditional fre quent item set mining techniques, as well as constraints placed on supports by the graph structure, for optimizing the calculation of relevant supports. To this en d, the notion of the path tree is introduced, as well as an algorithm for producing suc h path trees.	algorithm;computation;directed graph;waist–hip ratio	Henry Z. Lo;Dan A. Simovici;Wei Ding	2012		10.1007/978-3-642-31087-4_9	longest path problem;algorithm	DB	-8.346565209299225	-38.51739951903358	99683
4d5f4d6785d550383e3f3afb04c3015bf0d28405	meta-graph based hin spectral embedding: methods, analyses, and insights		Heterogeneous information network (HIN) has drawn significant research attention recently, due to its power of modeling multi-typed multi-relational data and facilitating various downstream applications. In this decade, many algorithms have been developed for HIN modeling, including traditional similarity measures and recent embedding techniques. Most algorithms on HIN leverage meta-graphs or meta-paths (special cases of meta-graphs) to capture various semantics. Given any arbitrary set of meta-graphs, existing algorithms either consider them as equally important or study their different importance through supervised learning. Their performance largely relies on prior knowledge and labeled data. While unsupervised embedding has shown to be a fundamental solution for various homogeneous network mining tasks, for HIN, it is a much harder problem due to such a presence of various meta-graphs. In this work, we propose to study the utility of different meta-graphs, as well as how to simultaneously leverage multiple meta-graphs for HIN embedding in an unsupervised manner. Motivated by prolific research on homogeneous networks, especially spectral graph theory, we firstly conduct a systematic empirical study on the spectrum and embedding quality of different meta-graphs on multiple HINs, which leads to an efficient method of meta-graph assessment. It also helps us to gain valuable insight into the higher-order organization of HINs and indicates a practical way of selecting useful embedding dimensions. Further, we explore the challenges of combining multiple meta-graphs to capture the multi-dimensional semantics in HIN through reasoning from mathematical geometry and arrive at an embedding compression method of autoencoder with l2,1-loss, which finds the most informative meta-graphs and embeddings in an end-to-end unsupervised manner. Finally, empirical analysis suggests a unified workflow to close the gap between our meta-graph assessment and combination methods. To the best of our knowledge, this is the first research effort to provide rich theoretical and empirical analyses on the utility of meta-graphs and their combinations, especially regarding HIN embedding. Extensive experimental comparisons with various state-of-the-art neural network based embedding methods on multiple real-world HINs demonstrate the effectiveness and efficiency of our framework in finding useful meta-graphs and generating high-quality HIN embeddings.		Carl Yang;Y. Feng;Pan Li;Yu Shi;Jiawei Han	2018	2018 IEEE International Conference on Data Mining (ICDM)	10.1109/ICDM.2018.00081	machine learning;autoencoder;artificial neural network;artificial intelligence;graph theory;supervised learning;empirical research;spectral graph theory;data modeling;computer science;embedding	ML	-14.11713644463639	-46.96961057885897	100300
910fa7ddf244204483c473aae38e1e1ab67104f0	optimisation using natural language processing: personalized tour recommendation for museums	social sciences computer science museums natural language processing optimisation;tourist attraction;saturation magnetization;personalization;museums;turystyczne atrakcje;social sciences optimisation natural language processing personalized tour recommendation museums artwork importance automatic extraction museum information textual energy computer sciences;tour recommendation;artificial neural networks;computational modeling;vectors;muzea;operation reseach;optimization natural language processing saturation magnetization vectors computational modeling numerical models artificial neural networks;optimization;optimal paths;numerical models;personalizacja;natural language processing	This paper proposes a new method to provide personalized tour recommendation for museum visits. It combines an optimization of preference criteria of visitors with an automatic extraction of artwork importance from museum information based on Natural Language Processing using textual energy. This project includes researchers from computer and social sciences. Some results are obtained with numerical experiments. They show that our model clearly improves the satisfaction of the visitor who follows the proposed tour. This work foreshadows some interesting outcomes and applications about on-demand personalized visit of museums in a very near future.	algorithm;artificial intelligence;experiment;graph theory;integer programming;linear programming;mathematical optimization;natural language processing;numerical analysis;personalization;recommender system;routing;spatial organization	Mayeul Mathias;Assema Moussa;Juan-Manuel Torres-Moreno;Fen Zhou;Marie-Sylvie Poli;Didier Josselin;Marc El-Bèze;Andréa Carneiro Linhares;Françoise Rigat	2014	2014 Federated Conference on Computer Science and Information Systems	10.15439/2014F336	simulation;saturation;computer science;artificial intelligence;machine learning;personalization;multimedia;computational model;artificial neural network	AI	-17.691016139382285	-50.8627200725425	100421
ef6450a791ebf795f1cc376f5eba194ef57f5cff	modeling data correlations in recommendation		In the field of recommender systems, the Beer & Nappies is a famous story, which reveals the latent relationships between different categories of items. Though matrix factorization (MF) has demonstrated its great effectiveness in most previous work, it neglects the co-occurrences of items selected by individuals. In most MF-based models, the latent preferences of users (or the latent categories of items) are assumed independent, which thereby leads to the weak correlation between the Beer and the Nappies. It also greatly limits the models’ ability in recommendation. In this paper, we propose a pure probabilistic generative model, which applies a Gaussian prior to capture the semantic correlations between the latent factors. We also show that our model theoretically achieves better expressive power than traditional MF-based models. We derive efficient inference and learning algorithms based on variational EM methods. The effectiveness of our proposed model is comprehensively verified on three different public data sets. Experimental results show that our approach achieves significant improvements on prediction quality compared with the current state of the art.	algorithm;expressive power (computer science);generative model;information privacy;latent variable;machine learning;recommender system;variational principle	Yuan He;Cheng Wang;Changjun Jiang	2017	IEEE Access	10.1109/ACCESS.2017.2712196	recommender system;computer science;data modeling;approximation algorithm;generative model;probabilistic logic;probabilistic latent semantic analysis;data set;machine learning;inference;artificial intelligence;pattern recognition	AI	-18.846047129775325	-47.46637400550752	100509
2e554fc23b02b9674cf4986f7960f2a8babb4c03	predicting perceived emotions in animated gifs with 3d convolutional neural networks	perceived emotion;animated gifs;emotion detection;3d convolutional neural network	Animated GIFs are widely used on the Internet to express emotions, but their automatic analysis is largely unexplored before. To help with the search and recommendation of GIFs, we aim to predict their emotions perceived by humans based on their contents. Since previous solutions to this problem only utilize image-based features and lose all the motion information, we propose to use 3D convolutional neural networks (CNNs) to extract spatiotemporal features from GIFs. We evaluate our methodology on a crowd-sourcing platform called GIFGIF with more than 6000 animated GIFs, and achieve a better accuracy then any previous approach in predicting crowd-sourced intensity scores of 17 emotions. It is also found that our trained model can be used to distinguish and cluster emotions in terms of valence and risk perception.	artificial neural network;computer animation;convolutional neural network;crowdsourcing;gif;internet	Weixuan 'Vincent' Chen;Rosalind W. Picard	2016	2016 IEEE International Symposium on Multimedia (ISM)	10.1109/ISM.2016.0081	computer vision;machine learning	SE	-18.335966216201992	-51.38214256395681	100642
72d60b7585ad51c1ef7617e404f360eb465a8c07	identification of group changes in blogosphere	social network services;group identification;social community;dynamics analysis;size measurement;evolution events group change identification polish blogosphere service social group evolution sgci method stable group discovery ged method;heuristic algorithms;social networking online;merging;communities;blogs communities merging heuristic algorithms size measurement algorithm design and analysis social network services;blogs;algorithm design and analysis;dynamics analysis group identification group changes blogoshpere social community;blogoshpere;group changes	The paper addresses a problem of change identification in social group evolution. A new SGCI method for discovering of stable groups was proposed and compared with existing GED method. The experimental studies on a Polish blogosphere service revealed that both methods are able to identify similar evolution events even though both use different concepts. Some differences were demonstrated as well.	blogosphere	Bogdan Gliwa;Stanislaw Saganowski;Anna Zygmunt;Piotr Bródka;Przemyslaw Kazienko;Jaroslaw Kozlak	2012	2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining	10.1109/ASONAM.2012.207	algorithm design;computer science;data mining;social psychology;world wide web;algorithm	EDA	-15.671471057362531	-42.44929991385318	100755
a191068347510535099a372145a8140fe10e3180	enhancing community detection for big sensor data clustering via hyperbolic network embedding		In this paper we present a novel big data clustering approach for measurements obtained from pervasive sensor networks. To address the potential very large scale of such datasets, we map the problem of data clustering to a community detection one. Datasets are cast in the form of graphs, representing the relations among individual observations and data clustering is mapped to node clustering (community detection) in the data graph. We propose a novel computational approach for enhancing the traditional Girvan-Newman (GN) community detection algorithm via hyperbolic network embedding. The data dependency graph is embedded in the hyperbolic space via Rigel embedding, making it possible to compute more efficiently the hyperbolic edge-betweenness centrality (HEBC) needed in the modified GN algorithm. This allows for more efficient clustering of the nodes of the data graph without significantly sacrificing accuracy. We demonstrate the efficacy of our approach with artificial network and data topologies, and real benchmark datasets. The proposed methodology can be used for efficient clustering of datasets obtained from massive pervasive smart city/building sensor networks, such as the FIESTA-IoT platform, and exploited in various applications such as lower-cost sensing.	benchmark (computing);betweenness centrality;big data;cluster analysis;computation;data dependency;embedded system;energy minimization;girvan–newman algorithm;graph (discrete mathematics);grid north;image scaling;mathematical optimization;sensor;smart city;synthetic intelligence	Vasileios Karyotis;Konstantinos Tsitseklis;Konstantinos Sotiropoulos;Symeon Papavassiliou	2018	2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)	10.1109/PERCOMW.2018.8480134	wireless sensor network;distributed computing;data dependency;big data;computer science;centrality;network topology;embedding;cluster analysis;hyperbolic space	ML	-12.116358227738106	-41.69859126273229	100883
cf7b54eb76dae3664710ebfb8450c9b415fa0a4d	skyline community search in multi-valued networks		Given a scientific collaboration network, how can we find a group of collaborators with high research indicator (e.g., h-index) and diverse research interests? Given a social network, how can we identify the communities that have high influence (e.g., PageRank) and also have similar interests to a specified user? In such settings, the network can be modeled as a multi-valued network where each node has d ($d \ge 1$) numerical attributes (i.e., h-index, diversity, PageRank, similarity score, etc.). In the multi-valued network, we want to find communities that are not dominated by the other communities in terms of d numerical attributes. Most existing community search algorithms either completely ignore the numerical attributes or only consider one numerical attribute of the nodes. To capture d numerical attributes, we propose a novel community model, called skyline community, based on the concepts of k-core and skyline. A skyline community is a maximal connected k-core that cannot be dominated by the other connected k-cores in the d-dimensional attribute space. We develop an elegant space-partition algorithm to efficiently compute the skyline communities. Two striking advantages of our algorithm are that (1) its time complexity relies mainly on the size of the answer s (i.e., the number of skyline communities), thus it is very efficient if s is small; and (2) it can progressively output the skyline communities, which is very useful for applications that only require part of the skyline communities. Extensive experiments on both synthetic and real-world networks demonstrate the efficiency, scalability, and effectiveness of the proposed algorithm.	community search;degeneracy (graph theory);embnet.journal;experiment;maximal set;numerical analysis;pagerank;scalability;scientific collaboration network;search algorithm;social network;synthetic intelligence;time complexity;virtual community	Rong-Hua Li;Lu Qin;Fanghua Ye;Jeffrey Xu Yu;Xiaokui Xiao;Nong Xiao;Zibin Zheng	2018		10.1145/3183713.3183736	time complexity;computer science;data mining;skyline;scientific collaboration network;scalability;pagerank;search algorithm;distributed computing;social network	DB	-11.527754143358834	-41.56604785566966	100976
786e1ae9101d98d2027470e762772dc171dca13e	situation goodness method for weighted centroid-based wi-fi aps localization	institute;image;init;imaging;new;technologies	Knowing the location of Wi-Fi antennas may be critical for indoor localization. However, in a real environment, their positions may be unknown since they can be managed by external entities. This paper introduces a new method for evaluating the suitability of using the weighted centroid method for the 2D localization of a Wi-Fi AP. The method is based on the idea that the weighted centroid method provides its best results when there are fingerprints taken around the AP. In order to find the probability of being in the presence of such situations, a natural neighbor interpolation method is used to find the regions with the highest signal strengths. A geometrical method is then used to characterize that probability based on the distribution of those regions in relation to the AP position estimation given by the weighted centroid method. The paper describes the testing location and the used Wi-Fi fingerprints database. That database is used to create new databases that recreate different sampling possibilities through a samples deletion strategy. The original database and the newly created ones are then used to evaluate the localG.M. Mendoza-Silva (✉) ⋅ J. Torres-Sospedra ⋅ J. Huerta ⋅ R. Montoliu ⋅ F. Benítez ⋅ O. Belmonte Institute of New Imaging Technologies, Universitat Jaume I, Avda, Vicente Sos Baynat S/N, 12071 Castellón de la Plana, Spain e-mail: gmendoza@uji.es J. Torres-Sospedra e-mail: jtorres@uji.es J. Huerta e-mail: huerta@uji.es R. Montoliu e-mail: montoliu@uji.es F. Benítez e-mail: benitezm@uji.es O. Belmonte e-mail: belfern@uji.es © Springer International Publishing AG 2017 G. Gartner and H. Huang (eds.), Progress in Location-Based Services 2016, Lecture Notes in Geoinformation and Cartography, DOI 10.1007/978-3-319-47289-8_2 27 ization results of several AP localization methods and the new method proposed in this paper. The evaluation results have shown that the proposed method is able to provide a proper probability for the suitability of using the weighted centroid method for localizing a Wi-Fi AP.	baseline (configuration management);cartography;database;email;entity;fingerprint;geographic information system;internationalization and localization;interpolation;linear algebra;location-based service;sampling (signal processing);springer (tank)	German M. Mendoza-Silva;Joaquín Torres-Sospedra;Joaquín Huerta;Raúl Montoliu;Fernando Benitez-Paez;Oscar Belmonte	2016		10.1007/978-3-319-47289-8_2	simulation;computer science;data science;data mining	Vision	-11.477376596769764	-48.78298099773026	101043
3e45c147c5e48277af5be362037788aa439b2c1b	an automated approach to translate a biological process from odes into graphical hybrid functional petri nets		The study of biological systems is growing rapidly, and can be considered as an intrinsic task in biological research, and a prerequisite for diagnosing diseases and drug development. The integration of biological studies with computer technologies led to noticeable developments in biology with the appearance of many powerful modeling and simulation techniques and tools. The help of computers in biology resulted in deeper knowledge about complex biological systems and biopathways behaviors. Among modeling tools, the Petri Net formalism plays an important role. Petri Net is a powerful computerized and graphical modeling technique originally developed by Carl Adam Petri in 1960 to model discrete event systems. With its various extensions, Petri Nets find applications in many other fields including Biology. The extension known under the name Hybrid Functional Petri Net (HFPN) was developed specifically to model biological systems. Traditionally, biological processes are captured as systems of ordinary differential equations (ODEs). However, HFPNs offer a much more elegant and versatile approach to represent these processes more accurately. In fact, HFPNs allow to capture phenomena which are impossible to capture with ODES, while being more intuitive and easy to understand and model with. In this work we propose an approach to translate a system of ODEs representing a biological process into a HFPN. The resulting HFPN, not only preserves the semantics of the original model, but is also more humanly readable thanks to the use of a novel technique to connect its components in a smart way. To validate our approach, we implemented it as an extension to the tool Real Time Studio (an integrated environment for modeling, simulation and automatic verification of real-time systems), and compared our simulation results with those obtained by simulating systems of ODEs using MATLAB.	biological system;comparison of command shells;computer;graphical user interface;human-readable medium;hybrid functional;matlab;petri net;real-time clock;real-time computing;semantics (computer science);simulation	Imene Mecheter;Rachid Hadjidj;Sebti Foufou	2017	2017 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)	10.1109/CIBCB.2017.8058558	matlab;bioinformatics;machine learning;biological process;artificial intelligence;hybrid functional;petri net;ordinary differential equation;computer science;theoretical computer science;modeling and simulation;formalism (philosophy)	Comp.	-7.161728838609575	-50.85719597158021	101089
a9c0875363e18acc35e4f1f905165d3325131584	analyzing query optimization process: portraits of join enumeration algorithms	graph theory;generic algorithm;query processing;costing;search space;sql;resource allocation;storage management;storage management data mining data visualisation graph theory query processing relational databases resource allocation sql statistical analysis;query optimization;data mining;sql statement optimization query optimization process analysis join enumeration algorithm cost model critical decision join enumeration viewer tool search space visualization search space mining search space comparison sybase sql anywhere relational database management system optimization log logical plan graph physical plan graph resource consumption statistics optimization time breakdown logical join enumeration costing physical plan memory allocation optimization structure sql anywhere optimizer highly adaptable algorithm self managing algorithm search space generation algorithm ordering technique pruning technique;data visualisation;optimization partitioning algorithms heuristic algorithms algorithm design and analysis visualization costing conferences;visualization;statistical analysis;heuristic algorithms;relational database management system;optimization;relational databases;memory allocation;algorithm design;algorithm design and analysis;cost model;heuristic algorithm;conferences;partitioning algorithms	Search spaces generated by query optimizers during the optimization process encapsulate characteristics of the join enumeration algorithms, the cost models, as well as critical decisions made for pruning and choosing the best plan. We demonstrate the Join Enumeration Viewer which is a tool designed for visualizing, mining, and comparing plan search spaces generated by different join enumeration algorithms when optimizing same SQL statement. We have enhanced Sybase SQL Anywhere relational database management system to log, in a very compact format, its search space during an optimization process. Such optimization log can then be analyzed by the Join Enumeration Viewer which internally builds the logical and physical plan graphs representing complete and partial plans considered during the optimization process. The optimization logs also contain statistics of the resource consumption during the query optimization such as optimization time breakdown, for example, for logical join enumeration versus costing physical plans, and memory allocation for different optimization structures. The SQL Anywhere Optimizer implements a highly adaptable, self-managing, search space generation algorithm by having several join enumeration algorithms to choose from, each enhanced with different ordering and pruning techniques. The emphasis of the demonstration will be on comparing and contrasting these join enumeration algorithms by analyzing their optimization logs. The demonstration scenarios will include optimizing SQL statements under various conditions which will exercise different algorithms, pruning and ordering techniques. These search spaces will then be visualized and compared using the Join Enumeration Viewer.	algorithm;mathematical optimization;optimizing compiler;query optimization;relational database management system;sql anywhere;self-management (computer science)	Anisoara Nica;Ian Charlesworth;Maysum Panju	2012	2012 IEEE 28th International Conference on Data Engineering	10.1109/ICDE.2012.132	hash join;recursive join;algorithm design;computer science;graph theory;theoretical computer science;data mining;database;sort-merge join	DB	-7.429161848040028	-38.3772054148039	101175
907b5111078035c6bfc7cceccdf4dc7624696f0d	a space-based layout algorithm for the drawing of co-citation networks	social network services;graph theory;social networking online graph theory;social network services computer networks layout displays filling data visualization spine information analysis algorithm design and analysis design methodology;graphically cocitation networks;pediatrics;complexity theory;social networks space based layout algorithm graphically cocitation networks network scaling algorithm edge crossings avoidance kamada kawai drawing algorithm;graph drawing;space based layout algorithm;kamada kawai drawing algorithm;social network analysis visualization graph drawing algorithm;distance measurement;visualization;shape;social networks;social networking online;edge crossings avoidance;social network analysis;network scaling algorithm;graph drawing algorithm;algorithm design and analysis	We present in this paper a drawing algorithm to represent graphically co-citation networks (scientograms). These networks have some interesting and unusual topological properties which are often valuable to be visualized. In general, these networks are pruned with a network scaling algorithm, then visualized using a drawing algorithm \cite{Chen98a}. However, typical drawing algorithms do not work properly, especially when the size of the networks grows. Edge crossings appear while the drawing space is not adequately filled resulting in an unsightly display. The approach presented in this paper is able to print the networks filling all the available space in an aesthetic way, while avoiding edge crossings. The algorithm is detailed and compared with the classical Kamada-Kawai drawing algorithm on two maps.	algorithm;co-citation;decision tree learning;experiment;force-directed graph drawing;image scaling;internet backbone;metaheuristic;printing;recursion;relocation (computing);simulated annealing;tree (data structure)	Arnaud Quirin;Oscar Cordón	2009	2009 International Conference on Computational Aspects of Social Networks	10.1109/CASoN.2009.25	algorithm design;combinatorics;social network analysis;social science;visualization;shape;computer science;artificial intelligence;graph theory;theoretical computer science;machine learning;graph drawing;line drawing algorithm;social network	ML	-14.085775377114103	-39.28670988593656	101279
0efedf8a4d8119e8ffe8294aab7c1c832326a46e	easing embedding learning by comprehensive transcription of heterogeneous information networks		Heterogeneous information networks (HINs) are ubiquitous in real-world applications. In the meantime, network embedding has emerged as a convenient tool to mine and learn from networked data. As a result, it is of interest to develop HIN embedding methods. However, the heterogeneity in HINs introduces not only rich information but also potentially incompatible semantics, which poses special challenges to embedding learning in HINs. With the intention to preserve the rich yet potentially incompatible information in HIN embedding, we propose to study the problem of comprehensive transcription of heterogeneous information networks. The comprehensive transcription of HINs also provides an easy-to-use approach to unleash the power of HINs, since it requires no additional supervision, expertise, or feature engineering. To cope with the challenges in the comprehensive transcription of HINs, we propose the HEER algorithm, which embeds HINs via edge representations that are further coupled with properly-learned heterogeneous metrics. To corroborate the efficacy of HEER, we conducted experiments on two large-scale real-words datasets with an edge reconstruction task and multiple case studies. Experiment results demonstrate the effectiveness of the proposed HEER model and the utility of edge representations and heterogeneous metrics. The code and data are available at https://github.com/GentleZhu/HEER.	algorithm;experiment;feature engineering;loss function;medical transcription;outer product;software incompatibility;transcription (software)	Yu Shi;Qi Zhu;Fang Guo;Chao Zhang;Jiawei Han	2018		10.1145/3219819.3220006	computer science;machine learning;artificial intelligence;semantics;feature engineering;embedding;feature learning	ML	-14.35185954562677	-46.83629082835992	101434
acbbe7b3bc83be406c3379e846525d23d4e73104	modelling plant variation through growth	local features;growth mechanism;feedback control;environmental factor	This paper introduces a method for creating naturally varied plants from a given basic plant model. Previous techniques create variation in plants by introducing local randomness to the plant model’s description. However, randomness is restricted by the model’s parameterization and lack of correlation between local features making varying global properties (e.g. branch and stem curvature) difficult. We present a biologically-based method which mimics the underpinnings of variation in real plants. This method uses a feedback control system to simulate the biological growth mechanism by which a plant naturally responds to environmental factors. We show that our technique creates more realistically varied models by modelling growth responses to stimuli, and provides a method for quickly creating numerous similar models, none of which are exactly alike.	approximation algorithm;control system;feedback;geometry instancing;level of detail;randomness;simulation	Lisa Streit;Pavol Federl;Mario Costa Sousa	2005	Comput. Graph. Forum	10.1111/j.1467-8659.2005.00875.x	simulation;computer science;artificial intelligence;feedback	Graphics	-4.54583295936463	-46.92375259650302	101485
0b8f8c9782d99f93ba5f426102c9248ed1c7c7a7	detecting core-periphery structure in spatial networks		The core-periphery structure, which decompose a network into a densely-connected core and a sparsely-connected periphery, constantly emerges from spatial networks such as traffic, biological and social networks. In this paper, we propose a random network model for spatial networks with core-periphery structure, which is inspired by the Kleinberg small-world model. In this model, we use a vertex core score to indicate the “coreness” of each vertex, and we connect each pair of vertices with a probability parameterized by their distance and core scores. We compute the optimal vertex core scores in a network by fitting it to our model using a maximum likelihood estimation. Results in real-world networks indicate that the fitted vertex core scores are informative machine learning features for vertex metadata prediction and network classification. Furthermore, we develop near linear-time algorithms for network generation and model inference by using the fast multipole method, which allow us to scale to networks with millions of vertices with minor tradeoffs in accuracy. 1 NETWORK CORE-PERIPHERY STRUCTURE Networks are widely used to model the interacting components of complex systems emerging from biology, ecosystems, economics, and social organizations [4, 27, 54]. A typical network consists of a set of vertices V and a set of edges E, where the vertices represent discrete objects (e.g., people, cities) and the edges represent pairwise connections (e.g., friendships, highways). Networks are often described in terms of local properties such as vertex degree, or local clustering coefficients and global properties such as diameter or the number of connected components. At the same time, a number of mesoscale proprieties are consistently observed in real-world networks, which often reveal important structural information of the underlying complex systems; arguably the most well-known is community structure, and a tremendous amount of effort has been devoted to its explanation and algorithmic identification [28, 43, 52, 66]. Another important mesoscale structure is core-periphery structure, although such structure has received relatively little attention. In contrast to community detection, which separates the vertices into multiple internally well-connected modules, core-periphery identification involves finding sets of cohesive core vertices and periphery vertices loosely connected to both each other and the cores. This type of structure is common in traffic [41, 44, 63, 64, 71], economic [6, 14, 20, 30, 36, 44, 48, 64], social [13, 22, 31, 35, 41, 42, 44, 49, 63, 64, 73, 77], and biological [7, 23, 37] networks. Oftentimes, spatial information is a driving factor in the core-periphery structure [67, 69, 70]. For example, in the C. elegans neural network shown in Fig. 1, most long-distance neural connections are between tightly connected early-born neurons, which serve as hubs and constitute the core [67]. Identifying core-periphery structure not only provides us with a new perspective to study the mesoscale A B	algorithm;artificial neural network;cluster analysis;clustering coefficient;complex systems;core-periphery structure;degree (graph theory);ecosystem;erdős–rényi model;fast multipole method;geographic information system;interaction;machine learning;network model;random graph;sensor;social network;spatial network;time complexity	Junteng Jia;Austin R. Benson	2018	CoRR		fast multipole method;maximum likelihood;metadata;vertex (geometry);inference;parameterized complexity;social network;machine learning;random graph;mathematics;artificial intelligence	ML	-15.036646574046603	-40.15931421182586	101525
812f75faeffeb50fc63a14502499d257bfbd1ef7	coupling logical analysis of data and shadow clustering for partially defined positive boolean function reconstruction	pattern clustering;complexity theory;boolean functions;data analysis boolean functions computational efficiency algorithm design and analysis clustering algorithms kernel computational modeling analytical models circuit synthesis network synthesis;information retrieval;probability density function;shadow clustering positive boolean function logic synthesis logical analysis of data;training;data mining;indexes;data analysis;positive boolean function;logic synthesis;logic gates;tree searching boolean functions data analysis information retrieval logic gates pattern clustering;logical analysis of data;shadow clustering;asymptotically linear memory occupation shadow clustering partially defined positive boolean function reconstruction and or expression pdpbf lsc logical analysis of data information retrieval depth first approach;tree searching;computational efficiency	The problem of reconstructing the and-or expression of a partially defined positive Boolean function (pdpBf) is solved by adopting a novel algorithm, denoted by LSC, which combines the advantages of two efficient techniques, Logical Analysis of Data (LAD) and Shadow Clustering (SC). The kernel of the approach followed by LAD consists in a breadth-first enumeration of all the prime implicants whose degree is not greater than a fixed maximum d. In contrast, SC adopts an effective heuristic procedure for retrieving the most promising logical products to be included in the resulting and-or expression. Since the computational cost required by LAD prevents its application even for relatively small dimensions of the input domain, LSC employs a depth-first approach, with asymptotically linear memory occupation, to analyze the prime implicants having degree not greater than d. In addition, the theoretical analysis proves that LSC presents almost the same asymptotic time complexity as LAD. Extensive simulations on artificial benchmarks validate the good behavior of the computational cost exhibited by LSC, in agreement with the theoretical analysis. Furthermore, the pdpBf retrieved by LSC always shows a better performance, in terms of complexity and accuracy, with respect to those obtained by LAD.	algorithm;algorithmic efficiency;artificial neural network;asymptotic computational complexity;best, worst and average case;breadth-first search;computation;computational complexity theory;correctness (computer science);degree (graph theory);depth-first search;heuristic;least absolute deviations;missing data;neural networks;procedural generation;reconstruction conjecture;regular language description for xml;run time (program lifecycle phase);semi-continuity;signal-to-noise ratio;simulation;time complexity;tomographic reconstruction;xfig	Marco Muselli;Enrico Ferrari	2011	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2009.206	database index;probability density function;discrete mathematics;logic synthesis;logic gate;computer science;theoretical computer science;machine learning;data mining;database;mathematics;boolean function;data analysis;algorithm;statistics	DB	-5.297952030449482	-38.12383232105481	101611
c62f9e334da190fae5678795bb5ad4c8946ebf6b	learning hierarchical feature influence for recommendation by recursive regularization	hierarchy;feature based recommendation;recommender systems	Existing feature-based recommendation methods incorporate auxiliary features about users and/or items to address data sparsity and cold start issues. They mainly consider features that are organized in a flat structure, where features are independent and in a same level. However, auxiliary features are often organized in rich knowledge structures (e.g. hierarchy) to describe their relationships. In this paper, we propose a novel matrix factorization framework with recursive regularization -- ReMF, which jointly models and learns the influence of hierarchically-organized features on user-item interactions, thus to improve recommendation accuracy. It also provides characterization of how different features in the hierarchy co-influence the modeling of user-item interactions. Empirical results on real-world data sets demonstrate that ReMF consistently outperforms state-of-the-art feature-based recommendation methods.	cold start;interaction;matrix regularization;recursion (computer science);sparse matrix	Jie Yang;Zhu Sun;Alessandro Bozzon;Jie Zhang	2016		10.1145/2959100.2959159	computer science;machine learning;pattern recognition;data mining;hierarchy;recommender system	AI	-18.01676097398267	-47.64819594172143	102297
8fd56eb59c11fceeefd0dc72c9e8a4be5fec4e16	fatbird: a tool for flight and trajectories analyses of birds		"""Abstract-Analyzing flyways of birds is one approach ornithologists pursue e.g. to be able to detect potential risks during the animal's migration. But this analysis is not trivial and the functionalities of existing supporting tools are neither perfect nor all-encompassing. In this paper, we introduce our new FATBIRD Tool, which not only visualizes flyways or arbitrary trajectories, but also helps the researchers in several aspects of the analysis. Similarities between all trajectories of the individual birds are calculated via Dynamic Time Warping distances, which is to the best of our knowledge the first usage in this field and delivers promising results. We show the functionalities of our tool on a use case based on real data of a GPS/GSM telemetry study of Eurasian curlews of the """"Bavarian Society for the Protection of Birds"""". The similarities are shown in an intuitively understandable heat map colored distance matrix as well as a hierarchical clustering dendrogram. The clustering of all data points is performed and shown, and the data can be filtered by several parameters. With that, potential stop-over and wintering areas can be detected very fast and easily. After having obtained the similarities and differences of the trajectories in an automatic way, the researchers can focus on the biological reasons of the generated results of the FATBIRD Tool. These can lead to a better understanding of e.g. why certain birds die on their flyways and thus to new approaches to develop optimized conservation measures for the specific species."""		Daniyal Kazempour;Anna Beer;Friederike Herzog;Daniel Kaltenthaler;Johannes-Y. Lohrer;Thomas Seidl	2018	2018 IEEE 14th International Conference on e-Science (e-Science)	10.1109/eScience.2018.00023	data mining;data point;computer science;cluster analysis;assisted gps;hierarchical clustering;telemetry;dynamic time warping;dendrogram;artificial intelligence;pattern recognition;distance matrix	DB	-8.000319423011748	-51.72633315671752	102350
49cdf7afe28e821a0d93cbd50780835aa26da803	transfer learning from minimal target data by mapping across relational domains	navegacion informacion;navigation information;information browsing;user preferences;partage des ressources;educational resource;internet;estudio caso;comportement utilisateur;resource sharing;particion recursos;etude cas;user behavior;reseau neuronal;red neuronal;comportamiento usuario;artificial neural network;neural network	A central goal of transfer learning is to enable learning when training data from the domain of interest is limited. Yet, work on transfer across relational domains has so far focused on the case where there is a significant amount of target data. This paper bridges this gap by studying transfer when the amount of target data is minimal and consists of information about just a handful of entities. In the extreme case, only a single entity is known. We present the 2 algorithm that finds an effective mapping of predicates from a source model to the target domain in this setting and thus renders preexisting knowledge useful to the target task. We demonstrate 2’s effectiveness in three benchmark relational domains on social interactions and study its behavior as information about an increasing number of entities becomes available.	algorithm;baseline (configuration management);benchmark (computing);entity;experiment;ibm notes;interaction;missing data;open-source software;rendering (computer graphics)	Lilyana Mihalkova;Raymond J. Mooney	2003		10.1007/11539117_82	shared resource;the internet;simulation;computer science;artificial intelligence;machine learning;world wide web;computer security;artificial neural network	AI	-15.725800255247307	-48.97071512282819	102871
7f867576e172d6fed8c0714893231c1ea1c5fd46	a temporal tree decomposition for generating temporal graphs		Discovering the underlying structures present in large real world graphs is a fundamental scientic problem. Recent work at the intersection of formal language theory and graph theory has found that a Hyperedge Replacement Grammar (HRG) can be extracted from a tree decomposition of any graph. is HRG can be used to generate new graphs that share properties that are similar to the original graph. Because the extracted HRG is directly dependent on the shape and contents of the of tree decomposition, it is unlikely that informative graph-processes are actually being captured with the extraction algorithm. To address this problem, the current work presents a new extraction algorithm called temporal HRG (tHRG) that learns HRG production rules from a temporal tree decomposition of the graph. We observe problems with the assumptions that are made in a temporal HRG model. In experiments on large real world networks, we show and provide reasoning as to why tHRG does not perform as well as HRG and other graph generators. CCS CONCEPTS •Mathematics of computing→Hypergraphs;Graph algorithms; •eory of computation →Random network models;	algorithm;computation;experiment;formal language;graph isomorphism;graph rewriting;graph theory;hemispherical resonator gyroscope;information;tree decomposition	Corey Pennycuff;Salvador Aguiñaga;Tim Weninger	2017	CoRR		computer science;data mining;graph theory;theoretical computer science;tree decomposition;formal language;graph;grammar	ML	-10.939992478188046	-38.747342802664384	103105
2878bc043339dd0c767811df89d934dc0896fb8d	fast frequent free tree mining in graph databases	graph database;frequent free tree mining;candidate generation;pruning technique;frequent free tree;graph databases;special graph;free tree;xml databases;automorphism-based pruning;computationally efficient algorithm f3tm;computational biology;xml database;pattern recognition;false positive;connected graph;computer network	Free tree, as a special graph which is connected, undirected and acyclic, is extensively used in domains such as computational biology, pattern recognition, computer networks, XML databases, etc. In this paper, we present a computationally efficient algorithm F3TM (Fast Frequent Free Tree Mining) to discover all frequent free trees in a graph database. We focus ourselves on how to reduce the cost of candidate generation and minimize the number of candidates being generated. We prove a theorem that the completeness of frequent free trees can be guaranteed by growing vertices from a limited range of vertices in a free tree. Two pruning techniques, automorphism-based pruning and pruning based on canonical mapping are proposed which significantly reduce the cost of candidate generation. We conducted experimental studies on a real application dataset and we show that our F3TM outperforms the upto- date algorithms by an order of magnitude.	graph database;structure mining	Peixiang Zhao;Jeffrey Xu Yu	2006		10.1109/ICDMW.2006.79	type i and type ii errors;computer science;connectivity;graph theory;theoretical computer science;machine learning;data mining;k-ary tree;xml database;database;trémaux tree;tree;graph database	ML	-7.31832604185018	-39.073379459698614	103441
604e08540c6ef3532257d81f75c92f4057a48d5e	natural document clustering by clique percolation in random graphs	busqueda informacion;document clustering;modelizacion;graph theory;disjunction;random graph;appel procedure;analyse amas;teoria grafo;distribution donnee;grouping;loi probabilite;ley probabilidad;information retrieval;grafo aleatorio;percolacion;graph clique;graphe aleatoire;classification;theorie graphe;data distribution;modelisation;multigraph;disyuncion;cluster analysis;llamada procedimiento;percolation;recherche information;multigrafo;disjonction;probability distribution;estructura datos;analisis cluster;structure donnee;agrupamiento;clique graphe;multigraphe;modeling;data structure;procedure call;clustered data;distribucion dato;clasificacion;groupage	Document clustering techniques mostly depend on models that impose explicit and/or implicit priori assumptions as to the number, size, disjunction characteristics of clusters, and/or the probability distribution of clustered data. As a result, the clustering effects tend to be unnatural and stray away more or less from the intrinsic grouping nature among the documents in a corpus. We propose a novel graph-theoretic technique called Clique Percolation Clustering (CPC). It models clustering as a process of enumerating adjacent maximal cliques in a random graph that unveils inherent structure of the underlying data, in which we unleash the commonly practiced constraints in order to discover natural overlapping clusters. Experiments show that CPC can outperform some typical algorithms on benchmark data sets, and shed light on natural document clustering.	percolation theory;random graph	Wei Gao;Kam-Fai Wong	2006		10.1007/11880592_10	clique;probability distribution;random graph;correlation clustering;constrained clustering;data stream clustering;systems modeling;document clustering;data structure;fuzzy clustering;biological classification;computer science;graph theory;canopy clustering algorithm;multigraph;machine learning;cure data clustering algorithm;percolation;clique percolation method;cluster analysis;single-linkage clustering;brown clustering;logical disjunction;statistics;clustering high-dimensional data	Theory	-9.01495347392901	-44.27282536351425	103553
76ee4ffb219a98446491a7b5e25479c6f6380392	distributed social graph embedding	embedding;link prediction;gossip;social network;recommender system;community structure;social networks;euclidean space;graph embedding;synthetic data;peer to peer;distributed algorithm;coordinate system	Distributed recommender systems are becoming increasingly important for they address both scalability and the Big Brother syndrome. Link prediction is one of the core mechanism in recommender systems and relies on extracting some notion of proximity between entities in a graph. Applied to social networks, defining a proximity metric between users enable to predict potential relevant future relationships. In this paper, we propose SoCS (Social Coordinate Systems}, a fully distributed algorithm that embeds any social graph in an Euclidean space, which can easily be used to implement link prediction. To the best of our knowledge, SoCS is the first system explicitly relying on graph embedding. Inspired by recent works on non-isomorphic embeddings, the SoCS embedding preserves the community structure of the original graph, while being easy to decentralize. Nodes thus get assigned coordinates that reflect their social position. We show through experiments on real and synthetic data sets that these coordinates can be exploited for efficient link prediction.	distributed algorithm;entity;experiment;graph embedding;recommender system;scalability;social graph;social network;synthetic data;system on a chip	Anne-Marie Kermarrec;Vincent Leroy;Gilles Trédan	2011		10.1145/2063576.2063751	distributed algorithm;artificial intelligence;theoretical computer science;machine learning;data mining;database;distributed computing;random geometric graph;world wide web;statistics;recommender system;social network	ML	-15.493298241126553	-44.870230933144	103701
714544b7cf35a3b8bdc12fb1967624a38f257a42	deep content-based music recommendation	technology and engineering;deep learning;convolutional neural networks;content based recommendation;music recommendation;recommender systems	Automatic music recommendation has become an increasingly relevant problem in recent years, since a lot of music is now sold and consumed digitally. Most recommender systems rely on collaborative filtering. However, this approach suffers from the cold start problem: it fails when no usage data is available, so it is not effective for recommending new and unpopular songs. In this paper, we propose to use a latent factor model for recommendation, and predict the latent factors from music audio when they cannot be obtained from usage data. We compare a traditional approach using a bag-of-words representation of the audio signals with deep convolutional neural networks, and evaluate the predictions quantitatively and qualitatively on the Million Song Dataset. We show that using predicted latent factors produces sensible recommendations, despite the fact that there is a large semantic gap between the characteristics of a song that affect user preference and the corresponding audio signal. We also show that recent advances in deep learning translate very well to the music recommendation setting, with deep convolutional neural networks significantly outperforming the traditional approach.	artificial neural network;bag-of-words model;cold start;collaborative filtering;convolutional neural network;deep learning;latent variable;recommender system;usage data	Aäron van den Oord;Sander Dieleman;Benjamin Schrauwen	2013			speech recognition;computer science;machine learning;deep learning;multimedia;world wide web	AI	-18.916047004600365	-50.980487801618494	103749
a4bde29e1a3762d4334ba094c0f6a6354056c41b	mining contrast subspaces	likelihood contrast;kernel density estimation;contrast subspace	In this paper, we tackle a novel problem of mining contrast subspaces. Given a set of multidimensional objects in two classes C+ and C− and a query object o, we want to find top-k subspaces S that maximize the ratio of likelihood of o in C+ against that in C−. We demonstrate that this problem has important applications, and at the same time, is very challenging. It even does not allow polynomial time approximation. We present CSMiner, a mining method with various pruning techniques. CSMiner is substantially faster than the baseline method. Our experimental results on real data sets verify the effectiveness and efficiency of our method.	actor-based concurrent language;approximation algorithm;baseline (configuration management);experiment;heuristic;polynomial;time complexity	Lei Duan;Guanting Tang;Jian Pei;James Bailey;Guozhu Dong;Akiko Campbell;Changjie Tang	2014		10.1007/978-3-319-06608-0_21	kernel density estimation;computer vision;computer science;variable kernel density estimation;statistics	ML	-6.873101471840508	-40.711711785895886	103770
2e404628c5f3b3e5d8b7326cdb7941fd5264c6ee	nucleus decompositions for identifying hierarchy of dense subgraphs		Finding dense substructures in a graph is a fundamental graph mining operation, with applications in bioinformatics, social networks, and visualization to name a few. Yet most standard formulations of this problem (like clique, quasi-clique, densest at-least-k subgraph) are NP-hard. Furthermore, the goal is rarely to find the “true optimum” but to identify many (if not all) dense substructures, understand their distribution in the graph, and ideally determine relationships among them. Current dense subgraph finding algorithms usually optimize some objective and only find a few such subgraphs without providing any structural relations.  We define the nucleus decomposition of a graph, which represents the graph as a forest of nuclei. Each nucleus is a subgraph where smaller cliques are present in many larger cliques. The forest of nuclei is a hierarchy by containment, where the edge density increases as we proceed towards leaf nuclei. Sibling nuclei can have limited intersections, which enables discovering overlapping dense subgraphs. With the right parameters, the nucleus decomposition generalizes the classic notions of k-core and k-truss decompositions.  We present practical algorithms for nucleus decompositions and empirically evaluate their behavior in a variety of real graphs. The tree of nuclei consistently gives a global, hierarchical snapshot of dense substructures and outputs dense subgraphs of comparable quality with the state-of-the-art solutions that are dense and have non-trivial sizes. Our algorithms can process real-world graphs with tens of millions of edges in less than an hour. We demonstrate how proposed algorithms can be utilized on a citation network. Our analysis showed that dense units identified by our algorithms correspond to coherent articles on a specific area. Our experiments also show that we can identify dense structures that are lost within larger structures by other methods and find further finer grain structure within dense groups.	algorithm;bioinformatics;citation network;clique (graph theory);coherence (physics);degeneracy (graph theory);dense subgraph;experiment;graph (discrete mathematics);induced subgraph;snapshot (computer storage);social network;structure mining;the forest	Ahmet Erdem Sariyüce;Seshadhri Comandur;Ali Pinar;Ümit V. Çatalyürek	2017	TWEB	10.1145/3057742	clique;computer science;discrete mathematics;nucleus;data mining;citation network;hierarchy;theoretical computer science;dense graph;graph	Web+IR	-13.197552209779555	-40.59423496231529	103903
088e35bae645593621fe0a8113d2c0656906c2a7	degree correlations in signed social networks	structural balance;assortativity;disassortativity;mixing patterns by degree;signed networks;positive and negative subnetworks	We investigate degree correlations in two online social networks where users are connected through different types of links. We find that, while subnetworks in which links have a positive connotation, such as endorsement and trust, are characterized by assortative mixing by degree, networks in which links have a negative connotation, such as disapproval and distrust, are characterized by disassortative patterns. We introduce a class of simple theoretical models to analyze the interplay between network topology and the superimposed structure based on the sign of links. Results uncover the conditions that underpin the emergence of the patterns observed in the data, namely the assortativity of positive subnetworks and the disassortativity of negative ones. We discuss the implications of our study for the analysis of signed complex networks. Preprint submitted to Elsevier December 3, 2014 ar X iv :1 41 2. 10 24 v1 [ ph ys ic s. so cph ] 2 D ec 2 01 4	assortative mixing;assortativity;complex network;distrust;emergence;network topology;social network	Valerio Ciotti;Ginestra Bianconi;Andrea Capocci;Francesca Colaiori;Pietro Panzarasa	2014	CoRR	10.1016/j.physa.2014.11.062	assortative mixing;mathematics;mixing patterns;assortativity	ML	-17.12929646396937	-40.51523984354556	103906
40e0e3ba841bda5f71999deddaa8d43c23002a6c	enhancing community discovery and characterization in vcop using topic models	social network services;digital documents;community detection;text analysis data mining semantic web social networking online;social networking services;text mining;virtual community;semantics;text analysis;latent dirichlet allocation web intelligence community discovery social network analysis text mining;data mining;latent dirichlet allocation;network analysis;semantic information;web intelligence;social networking online;mathematical model;semantic web;social network analysis;community discovery;communities social network services mathematical model equations semantics text mining;communities;structural properties;virtual community vcop topic models social network analysis community discovery problem digital documents text mining community detection latent semantic information	The identification of communities in social networks is a common problem that researchers have been dealing using network analysis properties. However, in environments where community members are connected by digital documents, most researchers have either emphasize to solve the community discovery problem computing structural properties of networks, ignoring the underlying semantic information from digital documents. In this paper, we propose a novel approach to combine traditional network analysis methods for community detection with text mining techniques. This way, extracted communities can be labeled according to latent semantic information within documents, called topics. Our proposal was evaluated in Plexilandia, a virtual community of practice with more than 2,500 members and 9 years of commentaries.	network theory;social network;text mining;virtual community	Lautaro Cuadra;Sebastián A. Ríos;Gaston L'Huillier	2011	2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2011.97	latent dirichlet allocation;text mining;social network analysis;network analysis;computer science;artificial intelligence;data science;semantic web;mathematical model;data mining;world wide web	Web+IR	-18.26600831188902	-42.09783489357232	104105
6b098b93119ae7cec6bbe2083ddb94891d1736c2	processing distance join queries with constraints	ikee lib auth gr;spatial constraints;βκπ;spatial data;ικee;websearch;bkp;auth;βιβλιοθήκη και κέντρο πληροφόρησης;ιδρυματικό καταθeτήριο;απθ;closest pair queries;library and information center;aristotle university of thessaloniki ικee;ikee;institutional repository	Distance join queries are used in many modern applications, such as spatial databases, spatiotemporal databases and data mining. One of the most common distance join queries is the closestpair query (CPQ). Given two datasets DA and DB the CPQ retrieves the pair (a, b), where a 2 DA and b 2DB, having the smallest distance between all pairs of objects. An extension to this problem is to generate the k closest pairs of objects (k-CPQ). In several cases spatial constraints are applied, and object pairs that are retrieved must also satisfy these constraints. Although the application of spatial constraints seems natural towards a more focused search, only recently they have been studied for the CPQ problem with the restriction that DA 1⁄4 DB. In this work, we focus on constrained closest-pair queries, between two distinct datasetsDA andDB, where objects fromDA must be enclosed by a spatial region R. Several algorithms are presented and evaluated using real-life and synthetic datasets. Among them, a heap-based method enhanced with batch capabilities outperforms the other approaches as it is demonstrated by an extensive performance evaluation.	algorithm;closest pair of points problem;data mining;decibel;emoticon;ordered pair;performance evaluation;real life;spatial database;spatiotemporal database;synthetic intelligence	Apostolos N. Papadopoulos;Alexandros Nanopoulos;Yannis Manolopoulos	2006	Comput. J.	10.1093/comjnl/bxl002	discrete mathematics;data mining;database;mathematics;spatial analysis;statistics	DB	-5.95379057391137	-41.62314304242819	104138
a1352c428a5a9c2cf92f3bd2475075b2b20f5de0	link communities detection via local approach	community detection;link community;local community	The traditional community detection algorithms were always focusing on the node community, while some recent studies have shown great advantage of link community approach which partitions links instead of nodes into communities. Here, we proposed a novel algorithm LBLC (local based link community) to detect link communities in networks based on some local information. A local link community can be detected by maximizing a local link fitness function from a seed link, which was ranked by another algorithm previously. The proposed LBLC algorithm has been tested on both synthetic and real world networks. The experimental results showed LBLC achieves meaningful link community structure.		Lei Pan;Chong-Jun Wang;Junyuan Xie	2012		10.1007/978-3-642-31900-6_36	machine learning;data mining	Vision	-14.383199350035063	-42.651915858781614	104162
649dfed8d7cb652ad8aa9c0316c137727e09e54f	performance evaluations of graph spectra on evolving systems	eigenvalues and eigenfunctions;complex networks;measurement;lattices;complex network;spectral radius;evolving system;laplace equations;internet;normalized laplacian spectrum;natural connectivity;bipartite graph	Evolving complex networks are abundant in the real world, i.e., Networks with different sizes (number of nodes) may originate from the same evolving system. In this paper, we use some typical evolving models of complex networks to evaluate the performances of graph spectra. The experimental results verify that the normalized Laplacian spectrum is a good indicator to distinguish between different evolving systems.	complex network;emergence;image scaling;laplacian matrix;performance;time complexity	Bo Jiao;Yuan-Ping Nie;Jing Du;Ying Zhou;Chengdong Huang;Xun-Long Pang	2015	2015 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery	10.1109/CyberC.2015.12	mathematical optimization;complex network	Robotics	-14.989719953080835	-40.49864341447588	104255
228956cdaf4a1f166e0848d709bd08e924ff3345	what kind of network are you? - using local and global characteristics in network categorisation tasks	directed graphs;content based filtering;local characteristics network models directed undirected graphs weighted binary graphs network classification small world network scale free network clustering coefficient power law node degree distribution small world phenomenon real world networked systems network categorisation tasks global characteristics;network theory graphs directed graphs;interpersonal influence;recommender system;electronic mail artificial neural networks integrated optics;network theory graphs	"""The amount of research done in the area of real--world networked systems is rapidly growing. Everybody knows what six degrees of separation or small--world phenomenon are. Scientists very easily give labels to the networks they analyse. If it has power law node degree distribution then it has to be scale--free network or if there is high clustering coefficient then it must be small--world network. These simplifications, although convenient, are not always very useful from the perspective of understanding phenomena existing within the network. In this paper we decided to go back to the basics and investigate whether analysis of one single measure is enough to describe a network. We analyse both local and global characteristics in order to discover the """"true"""" nature of a network. Not only using local and/or global measures can lead to different classification of a network but we also show how significantly different interpretation can result from analysing the same data by building network models as directed/undirected and/or weighted/binary graphs."""	categorization;clustering coefficient;degree distribution;graph (discrete mathematics);six degrees of separation;small-world experiment	Katarzyna Musial;Bogdan Gabrys;Marcin Buczko	2013	2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)	10.1145/2492517.2500258	network science;combinatorics;weighted network;degree distribution;directed graph;network theory in risk assessment;network formation;computer science;dynamic network analysis;artificial intelligence;machine learning;data mining;network simulation;world wide web;complex network;recommender system	ML	-16.366498176102453	-40.734705419420685	104273
1abc1342c407e3135bb1f87cee3e7713492ab0cd	influential neighbours selection for information diffusion in online social networks	initial activated neighbour influential neighbours selection information diffusion online social network real world network topologies node degree information naive random selection;electronic mail;social networking online random processes;network topology;random processes;integrated circuit modeling;social networking online;facebook;correlation;blogs;radio access networks;radio access networks facebook electronic mail blogs network topology integrated circuit modeling correlation	The problem of maximizing information diffusion through a network is a topic of considerable recent interest. A conventional problem is to select a set of any arbitrary k nodes as the initial influenced nodes so that they can effectively disseminate the information to the rest of the network. However, this model is usually unrealistic in online social networks since we cannot typically choose arbitrary nodes in the network as the initial influenced nodes. From the point of view of an individual user who wants to spread information as much as possible, a more reasonable model is to try to initially share the information with only some of its neighbours rather than a set of any arbitrary nodes; but how can these neighbours be effectively chosen? We empirically study how to design more effective neighbours selection strategies to maximize information diffusion. Our experimental results through intensive simulation on several real- world network topologies show that an effective neighbours selection strategy is to use node degree information for short-term propagation while a naive random selection is also adequate for long-term propagation to cover more than half of a network. We also discuss the effects of the number of initial activated neighbours. If we particularly select the highest degree nodes as initial activated neighbours, the number of initial activated neighbours is not an important factor at least for long-term propagation of information.	network topology;simulation;social network;software propagation	Hyoungshick Kim;Eiko Yoneki	2012	2012 21st International Conference on Computer Communications and Networks (ICCCN)	10.1109/ICCCN.2012.6289230	stochastic process;computer science;machine learning;data mining;world wide web;correlation;network topology;computer network	ECom	-17.271158909245152	-43.33497834258122	104325
089ba96484f6bd49ff4588a9c0b4e154c2ce119f	bitwise parallel association rule mining for web page recommendation		For many real-life web applications, web surfers would like to get recommendation on which collections of web pages that would be interested to them or that they should follow. In order to discover this information and make recommendation, data mining---and specially, association rule mining or web mining---is in demand. Since its introduction, association rule mining has drawn attention of many researchers. Consequently, many association rule mining algorithms have been proposed for finding interesting relationships---in the form of association rules---among frequently occurring patterns. These algorithms include level-wise Apriori-based algorithms, tree-based algorithms, hyperlinked array structure based algorithms, and vertical mining algorithms. While these algorithms are popular, they suffer from some drawbacks. Moreover, as we are living in the era of big data, high volumes of a wide variety of valuable data of different veracity collected at a high velocity post another challenges to data science and big data analytics. To deal with these big data while avoiding the drawbacks of existing algorithms, we present a bitwise parallel association rule mining system for web mining and recommendation in this paper. Evaluation results show the effectiveness and practicality of our parallel algorithm---which discovers popular pages on the web, which in turn gives the web surfers recommendation of web pages that might be interested to them---in real-life web applications.	apriori algorithm;association rule learning;big data;bitwise operation;data mining;data science;parallel algorithm;real life;velocity (software development);veracity;web application;web mining;web page	Carson Kai-Sang Leung;Fan Jiang;Adam G. M. Pazdor	2017		10.1145/3106426.3106542	data web;web page;web mining;web intelligence;data mining;data stream mining;k-optimal pattern discovery;web analytics;association rule learning;computer science	ML	-11.020479748689604	-38.142952443238556	104424
7fbcbaaf476cb498ef649f371382a257d8f3c460	computing cross associations for attack graphs and other applications	graph theory;computer science information security bipartite graph application software data mining information retrieval information analysis computer networks information systems business;information security;information retrieval;e commerce;data mining;heuristic algorithm attack graphs information security data mining e commerce information retrieval network management optimal cross association finding np complete problem;security of data computational complexity graph theory;computational complexity;network management;attack graph;security of data;heuristic algorithm	Applications in information security, data mining, e-commerce, information retrieval and network management require the analysis of large graphs in order to discover homogeneous groupings of rows and columns, called cross associations. We show that finding an optimal cross association is NP-complete. Furthermore, we give a heuristic algorithm with an O(n4) running time for finding good cross associations	algorithm;column (database);data mining;e-commerce;graph (discrete mathematics);heuristic (computer science);information retrieval;information security;karp's 21 np-complete problems;time complexity	Mohammad Hossain Heydari;Linda Morales;C. O. Shields;Ivan Hal Sudborough	2007	2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)	10.1109/HICSS.2007.141	network management;heuristic;computer science;information security;graph theory;theoretical computer science;data mining;database;computational complexity theory	DB	-7.55272336345583	-38.57131734493004	104515
6cadb82f9cd0daf03c39155fccc0435c55e690b0	state of the art of graph-based data mining	tree;graph based data mining;discrete mathematics;data mining;graph;path;data structure;structured data	The need for mining structured data has increased in the past few years. One of the best studied data structures in computer science and discrete mathematics are graphs. It can therefore be no surprise that graph based data mining has become quite popular in the last few years.This article introduces the theoretical basis of graph based data mining and surveys the state of the art of graph-based data mining. Brief descriptions of some representative approaches are provided as well.	computer science;data mining;data structure;discrete mathematics	Takashi Washio;Hiroshi Motoda	2003	SIGKDD Explorations	10.1145/959242.959249	concept mining;data structure;data model;computer science;data science;machine learning;data mining;graph;data stream mining;tree;path;graph;molecule mining;graph database	ML	-10.3959215460192	-38.92299307880525	104752
5bbd6278c93be51db9f843ddd5dbc001186ed7e3	fast top-k graph similarity search via representative matrices		Graph similarity search is a crucial problem in many applications, such as cheminformatics, data mining, and pattern recognition. Top-k graph similarity search aims to find the most similar  $k$  graphs to a query graph in graph databases. In this paper, we present a fast top-k graph similarity search algorithm with high classification accuracy. We introduce a new graph similarity measure based upon the number of occurrences of subtree patterns in graphs. In order to accelerate search, we also construct hierarchical representative matrices for graph databases, where each row of the matrices represents a graph set. Using representative matrices, we can derive a similarity upper bound of a query graph and the graph set so as to reduce search space. Comprehensive experiments on real data sets demonstrate that our algorithm has a better performance than compared methods on classification accuracy and query time, and it also can scale to large data sets including 15 million chemical structure graphs.	cheminformatics;data mining;experiment;graph database;pattern recognition;search algorithm;similarity measure;similarity search;tree (data structure)	Zhigang Sun;Hongwei Huo;Xiaoyang Chen	2018	IEEE Access	10.1109/ACCESS.2018.2819426	chemical structure;cheminformatics;tree (data structure);similarity measure;graph database;computer science;matrix (mathematics);distributed computing;data set;artificial intelligence;nearest neighbor search;pattern recognition	DB	-6.805663467590608	-38.97758444780109	104846
230674fd5156f8c7392538c4fb5b6be992e0a306	axiomatic ranking of network role similarity	similarity metric;complex network;triangle inequality;graph automorphism;computational method;satisfiability;information network;social network;ranking;role similarity;vertex similarity;automorphic equivalence;similarity measure	A key task in analyzing social networks and other complex networks is role analysis: describing and categorizing nodes by how they interact with other nodes. Two nodes have the same role if they interact with equivalent sets of neighbors. The most fundamental role equivalence is automorphic equivalence. Unfortunately, the fastest algorithm known for graph automorphism is nonpolynomial. Moreover, since exact equivalence is rare, a more meaningful task is measuring the role similarity between any two nodes. This task is closely related to the link-based similarity problem that SimRank addresses. However, SimRank and other existing simliarity measures are not sufficient because they do not guarantee to recognize automorphically or structurally equivalent nodes. This paper makes two contributions. First, we present and justify several axiomatic properties necessary for a role similarity measure or metric. Second, we present RoleSim, a role similarity metric which satisfies these axioms and which can be computed with a simple iterative algorithm. We rigorously prove that RoleSim satisfies all the axiomatic properties and demonstrate its superior interpretative power on both synthetic and real datasets.	algorithm;categorization;complex network;fastest;graph automorphism;iterative method;neighbourhood (graph theory);simrank;similarity measure;social network;synthetic intelligence;turing completeness	Ruoming Jin;Victor E. Lee;Hui Hong	2011		10.1145/2020408.2020561	combinatorics;discrete mathematics;social science;topology;ranking;machine learning;graph automorphism;triangle inequality;mathematics;complex network;statistics;social network;satisfiability	ML	-13.507725639174199	-41.34018390982965	105005
8263956acdebef8691c75b621db187505dd9af0c	quantifying social network dynamics	graph edit distance social network changes graph differential tuple dynamics of the social network sna;graph theory;social networking online graph theory;diverse distance measures social network dynamics dynamic character complex analysis network snapshots graph differential tuple;social network services electronic mail time measurement vectors educational institutions analytical models complex networks;social networking online	The dynamic character of most social networks requires to model evolution of networks in order to enable complex analysis of theirs dynamics. The following paper focuses on the definition of differences between network snapshots by means of Graph Differential Tuple. These differences enable to calculate the diverse distance measures as well as to investigate the speed of changes. Four separate measures are suggested in the paper with experimental study on real social network data.	experiment;social network	Radoslaw Michalski;Piotr Bródka;Przemyslaw Kazienko;Krzysztof Juszczyszyn	2012	2012 Fourth International Conference on Computational Aspects of Social Networks (CASoN)	10.1109/CASoN.2012.6412380	organizational network analysis;network science;combinatorics;computer science;dynamic network analysis;artificial intelligence;graph theory;theoretical computer science;network dynamics;machine learning;spatial network;mathematics;world wide web	DB	-15.971853999698292	-41.90415869804096	105306
619cdd400f94702638fbb64eca63f36289b78d81	a linear-time graph kernel	graph theory;logic arrays;kernel;complexity theory;compounds;benchmark data sets linear time graph kernel knowledge discovery graph structured data binary arrays logical operations linear time complexity;probability density function;data mining;kernel data mining logic arrays scalability chemical compounds polynomials computational efficiency time measurement machine learning informatics;binary arrays;linear time graph kernel;benchmark data sets;logical operations;machine learning;computational complexity;linear time;graph structured data;linear time complexity;graph theory computational complexity data mining;structured data;knowledge discovery	The design of a good kernel is fundamental for knowledge discovery from graph-structured data. Existing graph kernels exploit only limited information about the graph structures but are still computationally expensive. We propose a novel graph kernel based on the structural characteristics of graphs. The key is to represent node labels as binary arrays and characterize each node using logical operations on the label set of the connected nodes. Our kernel has a linear time complexity with respect to the number of nodes times the average number of neighboring nodes in the given graphs. The experimental result shows that the proposed kernel performs comparable and much faster than a state-of-the-art graph kernel for benchmark data sets and shows high scalability for new applications with large graphs.	analysis of algorithms;anomaly detection;benchmark (computing);computation;exclusive or;feature hashing;graph (abstract data type);graph (discrete mathematics);graph kernel;kernel (operating system);logical connective;scalability;time complexity	Shohei Hido;Hisashi Kashima	2009	2009 Ninth IEEE International Conference on Data Mining	10.1109/ICDM.2009.30	1-planar graph;time complexity;pathwidth;probability density function;kernel;string kernel;kernel embedding of distributions;graph bandwidth;graph product;null graph;data model;computer science;clique-width;graph theory;theoretical computer science;machine learning;comparability graph;data mining;graph kernel;mathematics;voltage graph;distance-hereditary graph;graph;modular decomposition;tree kernel;computational complexity theory;complement graph;graph operations	DB	-7.076034834768202	-39.01671124587633	105414
51589facbe83d3feb275e52b3aba41b944855182	an approach to chaos and self-organizing behaviors in symbiotic relationships between human and robots	symbiosis;chaos;robotics;self organization;behavior		robot	Mitsuo Wada;Sadayoshi Mikami	1996	JRM	10.20965/jrm.1996.p0318	biology;simulation;communication;ecology	Robotics	-5.472596993772826	-48.2049565265236	105415
aa8ca31f18f641e7da554c1aa7a90b3f2c2186ac	collaborative filtering by pso-based mmmf	recommender systems collaborative filtering conjugate gradient methods matrix decomposition mean square error methods particle swarm optimisation;latent factor pso based mmmf matrix factorization technique mf techniques recommender systems collaborative filtering prediction cf movie recommender system user preference partially observed rating matrix matrix completion problem partially observed data matrix maximum margin matrix factorisation mae rmse performance metrics gradient stochastic gradient search gradient information pso search particle swarm optimisation search hinge loss objective function multilevel discrete rating matrix maximum margin matrix factorization stochastic gradient descent conjugate gradient;collaborative filtering particle swarm optimization matrix factorization;sparse matrices collaboration optimization fasteners training motion pictures linear programming	Matrix factorization (MF) techniques are one of the most succesful realisations of recommender systems based on collaborative filtering/prediction (CF). For instance, in a movie recommender system based on CF, the inputs to the system are user ratings on movies (items) the users have already seen. To predict user preferences on movies they have not yet watched one needs to understand the patterns in the partially observed rating matrix. It is possible to visualize this setting as a matrix completion problem, i.e., completing entries in a partially observed data matrix. Then the objective is to compute user latent factor and item latent factor such that the rating matrix is completed. The factorization is usually accomplished by minimizing an objective function using gradient descent or its variants such as conjugate gradient or stochastic gradient descent. In this paper we make use of a particular MF technique called Maximum Margin Matrix Factorization (MMMF) and show that it is suitable for multi-level discrete rating matrix. The factorization is accomplished by minimizing the hinge loss objective function. We propose to improve the gradient search by combining a component of particle Swarm Optimisation (PSO) search. Though earlier attempts of improving PSO search by adding gradient information exist, the main objective of the present work is to improvise gradient/stochastic-gradient search. Our proposed algorithm finds better minimizing points early (fewer number of iterations) not only for the loss function but also for other performance metrics of collaborative filtering such as RMSE and MAE. There has not been any earlier attempt to combine particle swarm optimisation with maximum margin matrix factorisation for collaborative filtering.	collaborative filtering;conjugate gradient method;experiment;hinge loss;iteration;loss function;mathematical optimization;monoid factorisation;particle swarm optimization;recommender system;search algorithm;stochastic gradient descent;user (computing)	V. Devi SowminiDevi;Venkateswara Rao Kagita;Arun K. Pujari;Vineet Padmanabhan	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6973968	mathematical optimization;sparse matrix;machine learning;pattern recognition;mathematics;non-negative matrix factorization	ML	-17.19919025519954	-48.81317954453752	105624
4fba513f6fdc9e044e3e78c28d281759e2e82a6e	region proximity in metric spaces and its use for approximate similarity search	nearest neighbor queries;metric trees;metric space;approximate algorithm;performance evaluation;metric data;approximation algorithms;search algorithm;probabilistic approach;metricdata;approximate similarity search;similarity search;analytical model	"""Similarity search structures for metric data typically bound object partitions by ball regions. Since regions can overlap, a relevant issue is to estimate the proximity of regions in order to predict the number of objects in the regions' intersection. This paper analyzes the problem using a probabilistic approach and provides a solution that effectively computes the proximity through realistic heuristics that only require small amounts of auxiliary data. An extensive simulation to validate the technique is provided. An application is developed to demonstrate how the proximity measure can be successfully applied to the approximate similarity search. Search speedup is achieved by ignoring data regions whose proximity to the query region is smaller than a user-defined threshold. This idea is implemented in a metric tree environment for the similarity range and """"nearest neighbors"""" queries. Several measures of efficiency and effectiveness are applied to evaluate proposed approximate search algorithms on real-life data sets. An analytical model is developed to relate proximity parameters and the quality of search. Improvements of two orders of magnitude are achieved for moderately approximated search results. We demonstrate that the precision of proximity measures can significantly influence the quality of approximated algorithms."""	approximation algorithm;heuristic (computer science);metric tree;real life;search algorithm;similarity search;simulation;speedup	Giuseppe Amato;Fausto Rabitti;Pasquale Savino;Pavel Zezula	2003	ACM Trans. Inf. Syst.	10.1145/763693.763696	beam search;mathematical optimization;metric space;computer science;machine learning;cover tree;nearest neighbor search;approximation algorithm;search algorithm	DB	-5.762511074301833	-40.84164919531677	105846
0e2ed4bf57df1bc3c77276ddb78a2b5842c9526c	efficient influential individuals discovery on service-oriented social networks: a community-based approach		With the rapid development of Internet and mobile Internet, service-oriented social networks gain increasing popularity. Discovering a small subset of influential individuals on service-oriented social networks is beneficial for both users and service providers. This issue is formally referred to the influence maximization problem. In this paper, through exploiting the community structures of social networks, we propose two novel community-based approximation algorithms BCAA and ICAA, which have high performance guarantee as well as high efficiency, to address the influence maximization problem. Both BCAA and ICAA discover influential individuals within each individual community rather than the entire network. We further provide performance guarantee analysis of BCAA and ICAA. Finally, extensive experiments are conducted to demonstrate the efficiency and effectiveness of the proposed algorithms.	service-oriented device architecture;social network	Fanghua Ye;Jiahao Liu;Chuan Chen;Guohui Ling;Zibin Zheng;Yuren Zhou	2017		10.1007/978-3-319-69035-3_44	data mining;the internet;service provider;computer science;approximation algorithm;maximization;popularity;social network	ML	-16.46911900412997	-44.500175395592215	105854
c8211bc846a42de016fc6b31e17bda2d33016ef2	causal embeddings for recommendation		Many current applications use recommendations in order to modify the natural user behavior, such as to increase the number of sales or the time spent on a website. This results in a gap between the final recommendation objective and the classical setup where recommendation candidates are evaluated by their coherence with past user behavior, by predicting either the missing entries in the user-item matrix, or the most likely next event. To bridge this gap, we optimize a recommendation policy for the task of increasing the desired outcome versus the organic user behavior. We show this is equivalent to learning to predict recommendation outcomes under a fully random recommendation policy. To this end, we propose a new domain adaptation algorithm that learns from logged data containing outcomes from a biased recommendation policy and predicts recommendation outcomes according to random exposure. We compare our method against state-of-the-art factorization methods, in addition to new approaches of causal recommendation and show significant improvements.	algorithm;association rule learning;causal filter;discrepancy function;domain adaptation;fits;mary tsingou;recommender system	Stephen Bonner;Flavian Vasile	2018		10.1145/3240323.3240360	recommender system;artificial neural network;data mining;machine learning;artificial intelligence;causality;domain adaptation;computer science;factorization	ML	-19.08600656400368	-49.947318322042534	105885
013d13c1b1a2b65c4ea469e66770c629c1b8f771	accurate and scalable social recommendation using mixed-membership stochastic block models		With increasing amounts of information available, modeling and predicting user preferences-for books or articles, for example-are becoming more important. We present a collaborative filtering model, with an associated scalable algorithm, that makes accurate predictions of users' ratings. Like previous approaches, we assume that there are groups of users and of items and that the rating a user gives an item is determined by their respective group memberships. However, we allow each user and each item to belong simultaneously to mixtures of different groups and, unlike many popular approaches such as matrix factorization, we do not assume that users in each group prefer a single group of items. In particular, we do not assume that ratings depend linearly on a measure of similarity, but allow probability distributions of ratings to depend freely on the user's and item's groups. The resulting overlapping groups and predicted ratings can be inferred with an expectation-maximization algorithm whose running time scales linearly with the number of observed ratings. Our approach enables us to predict user preferences in large datasets and is considerably more accurate than the current algorithms for such large datasets.	book;collaborative filtering;expectation–maximization algorithm;inference;recommender system;scalability;stochastic block model;time complexity;user (computing);mixture	Antonia Godoy-Lorite;Roger Guimerà;Cristopher Moore;Marta Sales-Pardo	2016	Proceedings of the National Academy of Sciences of the United States of America		probability distribution;collaborative filtering;scalability;machine learning;data mining;matrix decomposition;artificial intelligence;computer science	ML	-18.617012290824587	-47.81746492467071	105975
19fad531abe45f2434bb4df24ac98442e26fba6d	a model for the analysis of neighbor finding in pointer-based quadtrees	analytical models;hierarchical data structure;image processing;application software;predictive models image processing image analysis computer graphics data structures tree graphs algorithm design and analysis inspection costs application software;computer graphics;hierarchical data structures;inspection;computer graphic;arrays;tree graphs;distance measurement;computational modeling;data structures;pixel;cartography;predictive models;image analysis;quadtrees cartography computer graphics hierarchical data structures image processing neighbor finding;floods;quadtrees;algorithm design and analysis;neighbor finding	A natural byproduct of the tree-like nature of the quadtree is that many basic image processing operations can be implemented as tree traversals which differ in the nature of the computation that is performed at each node. Some of these computations involve the inspection of a node's adjacent neighbors (termed neighbor finding). A new model is developed for images represented by quadtrees, and it is used to analyze various neighbor-finding techniques. The model's predicted costs for neighbor finding correlate very closely with empirical results and it is superior to the model that was used previously.	anatomic node;computation;hl7publishingsubsection <operations>;image processing;node - plant part;pointer <dog>;quadtree;tree traversal	Hanan Samet;Clifford A. Shaffer	1985	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.1985.4767729	algorithm design;computer vision;application software;image analysis;inspection;image processing;computer science;theoretical computer science;machine learning;data mining;predictive modelling;computer graphics;computational model;tree;pixel	Visualization	-13.631728972224966	-38.700575395128105	106018
34b5c976ff9afd243989bbd6680191cceb34f670	collaboration over time: characterizing and modeling network evolution	stochastic network modeling;poisson process;network evolution;large dataset;support vector regression;small world;degree distribution;statistical properties;social network;large scale;scale free;tree structure;network model;social network analysis;stochastic model;social network analysis sna;poisson model	A formal type of scientific and academic collaboration is coauthorship which can be represented by a coauthorship network. Coauthorship networks are among some of the largest social networks and offer us the opportunity to study the mechanisms underlying large-scale real world networks. We construct such a network for the Computer Science field covering research collaborations from 1980 to 2005, based on a large dataset of 451,305 papers authored by 283,174 distinct researchers. By mining this network, we first present a comprehensive study of the network statistical properties for a longitudinal network at the overall network level as well as for the intermediate community level. Major observations are that the database community is the best connected while the AI community is the most assortative, and that the Computer Science field as a whole shows a collaboration pattern more similar to Mathematics than to Biology. Moreover, the small world phenomenon and the scale-free degree distribution accompany the growth of the network. To study the individual collaborations, we propose a novel stochastic model, Stochastic Poisson model with Optimization Tree (Spot)to efficiently predict any increment of collaboration based on the local neighborhood structure. Spot models the non-stationary Poisson process by maximizing the log-likelihood with a tree structure. Empirical results show that Spot outperforms Support Vector Regression by better fitting collaboration records and predicting the rate of collaboration	assortativity;computer science;degree distribution;semantic network;small-world experiment;social network;stationary process;support vector machine;tree structure	Jian Huang;Ziming Zhuang;Jia Li;C. Lee Giles	2008		10.1145/1341531.1341548	support vector machine;social network analysis;social science;degree distribution;poisson process;network formation;computer science;stochastic modelling;data science;scale-free network;network model;machine learning;data mining;poisson regression;tree structure;world wide web;statistics;social network	ML	-17.523066494501798	-40.929443018892705	106093
979e38f6679d01878d5f16b062ab47c02c5d08f7	aspects of biomolecular computing	dna;molecular computation;search problem;modelizacion;parallelisme;linea montaje;structure cellulaire;calcul membrane;branching;biologia molecular;membrane system;interpretacion abstracta;problema investigacion;calcul biologique;modelisation;biomolecular computing;gene assembly;cell structure;parallelism;calculo molecular;paralelismo;calculo biologico;calculo membrana;ramificacion;molecular biology;assembly line;dna computation;ramification;estructura producto;dna computing;membrane computing;interpretation abstraite;estructura celular;abstract interpretation;biological computation;unconventional computing;probleme recherche;modeling;structure produit;calcul moleculaire;calculo adn;product structure;chaine montage;calcul adn;biologie moleculaire	This paper is intended as a survey of the state of the art of some branches of Biomolecular Computing. Biomolecular Computing aims to use biological hardware (biomare), rather than chips, to build a computer. We discuss the following three main research directions: DNA computing, membrane systems, and gene assembly in ciliates. DNA computing combines practical results together with theoretical algorithm design. Various search problems have been implemented using DNA strands. Membrane systems are a family of computational models inspired by the membrane structure of living cells. The process of gene assembly in ciliates has been formalized as an abstract computational model. Biomolecular Computing is a field in full development, with the promise of important results from the perspective of both Computer Science (models of computation) and Biology (understanding biological processes).		Naya Nagy;Selim G. Akl	2007	Parallel Processing Letters	10.1142/S012962640700296X	natural computing;biological computation;systems modeling;branching;search problem;computer science;bioinformatics;membrane computing;artificial intelligence;ramification;dna;dna computing;algorithm;unconventional computing	HPC	-6.722481789988923	-49.790888278873105	106127
0a888fadf7f8ac8eb0f194f7979e421ce072bcc4	fast reliability search in uncertain graphs		Uncertain, or probabilistic, graphs have been increasingl y used to represent noisy linked data in many emerging application sc enarios, and have recently attracted the attention of the databa se research community. A fundamental problem on uncertain graph s is reliability, which deals with the probability of nodes being reachable one from another. Existing literature has exclusively focused on reliability detection, which asks to compute the probability that two given nodes are connected. In this paper we studyreliability search on uncertain graphs, which we define as the problem of computing all nodes reachable from a set of query nodes with probability no less than a given threshold. Existing reliability-detection approac hes are not well-suited to efficiently handle the reliability-search p roblem. We proposeRQ-tree, a novel index which is based on a hierarchical clustering of the nodes in the graph, and further optimized u sing a balanced-minimum-cut criterion. Based on RQ-tree, we define a fast filtering-and-verification online query-evaluation s trategy that relies on a maximum-flow-based candidate-generation phase , followed by a verification phase consisting of either a lower-bo unding method or a sampling technique. The first verification method returns no incorrect nodes, thus guaranteeing perfect precis ion, completely avoids sampling, and is more efficient. The second ve rification method ensures instead better recall. Extensive experiments on real-world uncertain graphs show t at our methods are very efficient—over state-of-the-art relia bilitydetection methods, we obtain a speed-up up to five orders of ma gnitude; as well as accurate—our techniques achieve precision > 0.95 and recall usually higher than 0.75.	call of duty: black ops;cluster analysis;experiment;hierarchical clustering;linked data;minimum cut;multi-source;precision and recall;sampling (signal processing);web search query	Arijit Khan;Francesco Bonchi;Aristides Gionis;Francesco Gullo	2014		10.5441/002/edbt.2014.48	theoretical computer science;machine learning;data mining;database	DB	-8.754457531151235	-40.04577823504381	106179
797fd6e1378facc913b28a356c0ec565175f4da3	group impact: local influence maximization in social networks		Influence maximization defined as the problem of selecting influential small set of nodes that maximize influence spread over the social network. Influence maximization considered in number of domains, emergence situations, viral marketing, education, collaborative activities and political elections. In this paper, we propose Local Information Maximization LIM, considering group impact in terms of local propagation where the influencer(s) of each community has a direct effect on the nodes in the same community. We conduct experiments on synthetic data set and compare the performance of the LIM to various heuristics.	expectation–maximization algorithm;social network	Ragia A. Ibrahim;Hesham A. Hefny;Aboul Ella Hassanien	2016		10.1007/978-3-319-48308-5_43	mathematical optimization;viral marketing;small set;heuristics;political elections;social network;synthetic data;greedy algorithm;computer science;maximization	DB	-16.915953096138967	-43.5175169083381	106194
fdb39279a5fb948c4921c6f142058177681f6733	resampling-based gap analysis for detecting nodes with high centrality on large social network	node centrality;error estimation;gap analysis;resampling	We address a problem of identifying nodes having a high centrality value in a large social network based on its approximation derived only from nodes sampled from the network. More specifically, we detect gaps between nodes with a given confidence level, assuming that we can say a gap exists between two adjacent nodes ordered in descending order of approximations of true centrality values if it can divide the ordered list of nodes into two groups so that any node in one group has a higher centrality value than any one in another group with a given confidence level. To this end, we incorporate confidence intervals of true centrality values, and apply the resampling-based framework to estimate the intervals as accurately as possible. Furthermore, we devise an algorithm that can efficiently detect gaps by making only two passes through the nodes, and empirically show, using three real world social networks, that the proposed method can successfully detect more gaps, compared to the one adopting a standard error estimation framework, using the same node coverage ratio, and that the resulting gaps enable us to correctly identify a set of nodes having a high centrality value.	algorithm;approximation;approximation error;betweenness centrality;centrality;computation;computational complexity theory;experiment;gap analysis;resampling (statistics);sensor;social network;sorting	Kouzou Ohara;Kazumi Saito;Masahiro Kimura;Hiroshi Motoda	2015		10.1007/978-3-319-18038-0_11	resampling;machine learning;data mining;mathematics;centrality;betweenness centrality;statistics	Web+IR	-14.533745389060783	-40.872918339462274	106241
3b8c8c101f80a8c564c50794bd2a6238949271a4	a diversity-dependent measure for discovering influencers in social networks		In this paper, a diversity-dependent influence measure, considering social diversity and transition probability, is proposed for detecting the influencers by evaluating the influence of users across the social networks. Two models are then proposed to evaluate this measure. Comparative analyses on synthetic social networks and a real Twitter data suggest that the social diversities of the influenced people may play an important role in the identification of various influence levels of influencers. Comparative analysis between our proposed methods shows that the weighted spread strategy performs best. It implies that the pattern of the influence propagation would be beneficial to discover influencers. Our proposed scheme is therefore practical and feasible to be deployed in the real world.	social network	Pei-Ying Huang;Hsin-Yu Liu;Chun-Ting Lin;Pu-Jen Cheng	2013		10.1007/978-3-642-45068-6_32	data mining;social network;computer science;influencer marketing	ML	-19.04679895060377	-43.206867206855	106980
218d3e893c9fd242d50f28cfca832bc6afb57741	heigen: spectral analysis for billion-scale graphs	eigenvalues and eigenfunctions;graph theory;eigenvalues and eigenfunctions twitter algorithm design and analysis sparse matrices symmetric matrices linkedin web pages;h;mr;web pages;linkedin;data mining;graph mining;symmetric matrices;pattern recognition;parallel machines;hadoop platform heigen spectral analysis billion scale graphs matrices m45 cluster supercomputers real world graphs twitter social network yahooweb data set publicly available graphs graph mining pattern discovery eigensolver open source mapreduce framework;spectral analysis data mining graph theory parallel machines pattern recognition;mapreduce;twitter;spectral analysis;hadoop;graph mining spectral analysis mapreduce hadoop heigen;sparse matrices;algorithm design and analysis;heigen	Given a graph with billions of nodes and edges, how can we find patterns and anomalies? Are there nodes that participate in too many or too few triangles? Are there close-knit near-cliques? These questions are expensive to answer unless we have the first several eigenvalues and eigenvectors of the graph adjacency matrix. However, eigensolvers suffer from subtle problems (e.g., convergence) for large sparse matrices, let alone for billion-scale ones. We address this problem with the proposed HEIGEN algorithm, which we carefully design to be accurate, efficient, and able to run on the highly scalable MAPREDUCE (HADOOP) environment. This enables HEIGEN to handle matrices more than 1;000 × larger than those which can be analyzed by existing algorithms. We implement HEIGEN and run it on the M45 cluster, one of the top 50 supercomputers in the world. We report important discoveries about nearcliques and triangles on several real-world graphs, including a snapshot of the Twitter social network (56 Gb, 2 billion edges) and the “YahooWeb” data set, one of the largest publicly available graphs (120 Gb, 1.4 billion nodes, 6.6 billion edges).	adjacency matrix;algorithm;apache hadoop;graph (discrete mathematics);mapreduce;scalability;snapshot (computer storage);social network;sparse matrix;supercomputer	U. Kang;Brendan Meeder;Evangelos E. Papalexakis;Christos Faloutsos	2014	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2012.244	algorithm design;haplogroup h;sparse matrix;computer science;graph theory;data science;theoretical computer science;machine learning;web page;data mining;database;world wide web;intellectual disability;symmetric matrix	DB	-9.968820619417395	-41.85098266655251	106997
d79558d40ac5ba1ee212fab674a42f10024c742e	special issue on neural and sensory information processing	motor drives;information processing neurons motor drives computational modeling adaptation models mathematical model brain modeling;brain modeling;computational modeling;information processing;mathematical model;neurons;adaptation models	IN RECENT YEARS, experimental and theoretical progress has rekindled interest in the study of organizational and computational principles underlying neural systems. Neural systems are both complex and intriguing, and the relation of neural and sensory information processing to communications, control, and behavior in living organisms continues to pose fundamental research issues in a variety of disciplines. This Special Issue on Neural and Sensory Information Processing is intended to provide a timely sampling of current views and progress in these studies.	information processing;sampling (signal processing)	Arthur C. Sanderson;Yehoshua Y. Zeevi	1983	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1983.6313058	information processing;computer science;artificial intelligence;machine learning;mathematical model;mathematics;computational model;cognitive science;statistics	ML	-6.9465276191364875	-46.79335271277633	107021
2ac5e25bc14aa9e3223ac95c78a7823362f261f9	hiding sequences	data mining;security of data	The process of discovering relevant patterns holding in a database, was first indicated as a threat to database security by O' Leary in [20]. Since then, many different approaches for knowledge hiding have emerged over the years, mainly in the context of association rules and frequent itemsets mining. Following many real-world data and applications demands, in this paper we shift, the problem of knowledge hiding to contexts where both the data arid the extracted knowledge have a sequential structure. We provide problem statement, some theoretical issues including NP-hardness of the problem, a polynomial sanitization algorithm and an experimental evaluation. Finally we discuss possible extensions that will allow to use this work as a basic building block for more complex kinds of patterns and applications.	algorithm;association rule learning;autocorrelation;data mining;database security;distortion;experiment;heuristic (computer science);maxima and minima;np-hardness;polynomial;regular expression;sanitization (classified information);sequential pattern mining	Osman Abul;Maurizio Atzori;Francesco Bonchi;Fosca Giannotti	2007	2007 IEEE 23rd International Conference on Data Engineering Workshop	10.1109/ICDEW.2007.4400985	computer science;data science;data mining;database	DB	-7.633458979133409	-38.34370818885217	107307
a34bd488d845ee64082aa6251d2170e909867d15	detection of fuzzy duplicates in high dimensional datasets	fuzzy duplicates;psnm;data cleaning;pb;improved indexing technique;record linkage	Record duplicate detection is the most crucial task in the industry today. The record contains misspelled words, different formats of text and also repeated data. To overcome this issue of fuzzy duplicates, progressive methods, namely Progressive Sorted Neighborhood Method (PSNM), Progressive Blocking (PB) and record linkage are proposed. However, they face issues with respect to scalability and this is solved using Improved Indexing Technique (IIT) for fast duplicate detection. The result obtained shows the accuracy, precision and recall for each of these techniques and proves that Improved Indexing Technique provides maximum efficiency.	algorithm;integrated information theory;linkage (software);precision and recall;scalability;sensor;web page;window function	N. Raksha;Raj Alankar	2016	2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2016.7732247	record linkage;computer science;data mining;database;information retrieval	DB	-5.8499237186100785	-38.9232309061521	107376
321984cbfb9c83456a3e0f4fdccb11cdb630c0b0	golap: graph-based online analytical processing		Graph-based Online Analytical Processing (GOLAP) extends Online Analytical Processing (OLAP) to address graph-based problems that involve object attributes. Based on graph data, GOLAP can answer user queries related to combinatorial optimization, structural analytics, and influence analytics. Besides, since a GOLAP system is an online interactive system that requires fast response time, the execution time for graph-problem queries is essentially critical. Thus, how to speed up the execution time of specific graph problems becomes a challenge in GOLAP. In this paper, we show several methods to speed up the running time, including graph data reduction and approximation. In this paper, we survey classes of graph-based queries, challenges for GOLAP, and solutions that GOLAP provides.		Chung-Hsien Chou;Masahiro Hayakawa;Atsushi Kitazawa;Phillip C.-Y. Sheu	2018	Int. J. Semantic Computing	10.1142/S1793351X18500071	online analytical processing;data mining;response time;data reduction;computer science;speedup;combinatorial optimization;graph;analytics	HPC	-10.31802316549791	-38.99962849149126	107591
a37a4865464fe8c50d5ab8e3b50bb9c9f2e2ff34	expanding network communities from representative examples	graph theory;news analysis;discrete mathematics;gold standard;artificial intelligent;social network;reliable communication;general methods;seed set;social networks;artificial intelligence;community discovery;subjective evaluation;random partition	We present an approach to leverage a small subset of a coherent community within a social network into a much larger, more representative sample. Our problem becomes identifying a small conductance subgraph containing many (but not necessarily all) members of the given seed set. Starting with an initial seed set representing a sample of a community, we seek to discover as much of the full community as possible.  We present a general method for network community expansion, demonstrating that our methods work well in expanding communities in real world networks starting from small given seed groups (20 to 400 members). Our approach is marked by incremental expansion from the seeds with retrospective analysis to determine the ultimate boundaries of our community. We demonstrate how to increase the robustness of the general approach through bootstrapping multiple random partitions of the input set into seed and evaluation groups.  We go beyond statistical comparisons against gold standards to careful subjective evaluations of our expanded communities. This process explains the causes of most disagreement between our expanded communities and our gold-standards—arguing that our expansion methods provide more reliable communities than can be extracted from reference sources/gazetteers such as Wikipedia.	bootstrapping (compilers);coherence (physics);conductance (graph);social network;web standards;wikipedia	Andrew Mehler;Steven Skiena	2009	TKDD	10.1145/1514888.1514890	social science;computer science;artificial intelligence;graph theory;machine learning;data mining;mathematics;statistics;social network	ML	-16.567303200015928	-41.159328693767264	107692
5dcc2a6673e1d2b691fb07451816eae14d2a5dd1	approximate high-dimensional nearest neighbor queries using r-forests	high dimensionality;r forest;approximate nearest neighbor queries;r trees	Highly efficient query processing on high-dimensional data, while important, is still a challenge nowadays -- as the curse of dimensionality makes efficient solution very difficult. On the other hand, there have been suggestions that it is better off if one can return a solution quickly, that is close enough, to be sufficient. In this paper we will introduce the concept R-Forest, comprised of a set of disjoint R-trees built over the domain of the search space. Each R-tree will store a sub-set of points in a non-overlapping space, which is maintained throughout the life of the forest. Also included are several new features, Median point used for ordering and searching a pruning parameter, as well as restricted access. When all of these are combined together they can be used to answer Approximate Nearest Neighbor queries, returning a result that is an improvement over alternative methods, such as Locality Sensitive Hashing B-Tree (LSB-tree) with the same amount of IO. With our approach to this difficult problem, we are able to handle different data distribution, even taking advantage of the distribution without any additional parameter tuning, scales with increasing dimensionality and most importantly provides the user with some feedback, in terms of lower bound as to the quality of the results.	b-tree;curse of dimensionality;database;least significant bit;locality of reference;locality-sensitive hashing;r language;r-tree;the forest	Michael Nolen;King-Ip Lin	2013		10.1145/2513591.2513652	r-tree;best bin first;computer science;machine learning;pattern recognition;data mining;database;nearest neighbor search	DB	-5.918849349306913	-41.209680010197594	108107
4fdcc3889866452a17d2607d034de5f974a8819a	near neighbor search in large metric spaces	metric space;high dimensionality;data mining;databases and the web;geometric model;data structure;image similarity	Given user data, one often wants to find approximate matches in a large database. A good example of such a task is finding images similar to a given image in a large collection of images. We focus on the important and technically difficult case where each data element is high dimensional, or more generally, is represented by a point in a large metric spaceand distance calculations are computationally expensive. In this paper we introduce a data structure to solve this problem called a GNAT Geometric Near-neighbor Access Tree. It is based on the philosophy that the data structure should act as a hierarchical geometrical model of the data as opposed to a simple decomposition of the data that does not use its intrinsic geometry. In experiments, we find that GNAT’s outperform previous data structures in a number of applications.	analysis of algorithms;approximation algorithm;data element;data structure;database;experiment;gnat;spaces	Sergey Brin	1995			data structure;metric space;computer science;theoretical computer science;geometric modeling;intrinsic metric;machine learning;data mining;database;cover tree;programming language	DB	-5.694540137039693	-41.75603650507225	108138
14cca964af51745ea989669622986c428fda7b4b	evaluation of tensor-based algorithms for real-time bidding optimization		In this paper we evaluate tensor-based approaches to the Real-Time Bidding (RTB) Click-Through Rate (CTR) estimation problem. We propose two new tensor-based CTR prediction algorithms. We analyze the evaluation results collected from several papers – obtained with the use of the iPinYou contest dataset and the Area Underneath the ROC curve measure. We accompany these results with analogical results of our experiments – conducted with the use of our implementations of tensor-based algorithms and approaches based on the logistic regression. In contrast to the results of other authors, we show that biases – in particular those being low-order expectation value estimates – are at least as useful as outcomes of high-order components’ processing. Moreover, on the basis of Average Precision results, we postulate that ROC curve should not be the only characteristic used to evaluate RTB CTR estimation performance.	algorithm;optimizing compiler;real-time bidding	Andrzej Szwabe;Pawel Misiorek;Michal Ciesielczyk	2017		10.1007/978-3-319-54472-4_16	real-time bidding;artificial intelligence;tensor;machine learning;computer science;big data;display advertising;logistic regression;bidding;algorithm;demand-side platform;expectation value	EDA	-17.86257010415457	-51.52163484504381	108198
6b0a8d5702d9529007def856fa5eac16af2b924b	centrality, gossip, and diffusion of information in networks	social learning;gossip;centrality;social networks;communication;diffusion;microfinance	How can we identify the most influential nodes in a network for initiating diffusion? Are people able to easily identify those people in their communities who are best at spreading information, and if so How? Using theory and recent data, we will examine these questions and see how the structure of social networks affects information transmission ranging from gossip to the diffusion of new products. In particular, the concept of diffusion centrality from Banerjee, Chandrasekhar, Duflo, and Jackson (2013) will be considered and shown to nest degree centrality, eigenvector centrality, and other measures of centrality as extreme special cases. Then it will be shown that by tracking gossip within a network, nodes can easily learn to rank the centrality of other nodes without knowing anything about the network itself. Finally, the theoretical predictions will be tested with data. The results are presented in Banerjee, Chandrasekhar, Duflo, and Jackson (2014).	eigenvector centrality;gossip protocol;jackson;learning to rank;social network	Matthew O. Jackson	2014		10.1145/2600057.2602958	network theory;gossip;network science;random walk closeness centrality;microfinance;social network analysis;social learning;katz centrality;artificial intelligence;alpha centrality;diffusion;centrality;betweenness centrality	AI	-18.01085255713441	-41.00038179536394	108266
346f95e5be0364bed78d276aaf59de2b2b92af9e	group topic model: organizing topics into groups	document clustering;topic modeling;online learning;latent dirichlet allocation;variational inference;期刊论文;group	Latent Dirichlet allocation defines hidden topics to capture latent semantics in text documents. However, it assumes that all the documents are represented by the same topics, resulting in the “forced topic” problem. To solve this problem, we developed a group latent Dirichlet allocation (GLDA). GLDA uses two kinds of topics: local topics and global topics. The highly related local topics are organized into groups to describe the local semantics, whereas the global topics are shared by all the documents to describe the background semantics. GLDA uses variational inference algorithms for both offline and online data. We evaluated the proposed model for topic modeling and document clustering. Our experimental results indicated that GLDA can achieve a competitive performance when compared with state-of-the-art approaches.	algorithm;cluster analysis;document;latent dirichlet allocation;online and offline;organizing (structure);topic model;variational principle	Ximing Li;Jihong OuYang;You Lu;Xiaotang Zhou;Tian Tian	2014	Information Retrieval Journal	10.1007/s10791-014-9244-9	latent dirichlet allocation;dynamic topic model;document clustering;computer science;machine learning;pattern recognition;data mining;group;topic model	Web+IR	-16.69653812721163	-47.08060735699383	108492
7b628ac2106fddea1b54bcdd6a590347bb3c3694	freme: a pattern partition based engine for fast and scalable regular expression matching in practice	pattern partition;deterministic finite automata dfa;regular expression matching;deep inspection	Regular expression matching has been widely used in modern content-aware network devices, where the content of interest (i.e., patterns) is often specified by regular expressions. Due to the ever-increasing number of patterns, implementing fast and scalable regular expression matching becomes a big challenge. Practical solutions rely mainly on a variety of deterministic finite automata (DFA) deflation techniques, but cannot guarantee both high speed and linear scalability simultaneously.#R##N##R##N#To fully address the problem, in this paper, we present a fundamentally different design: (1) following principles to partition all regular expression patterns (in the given pattern set) into segments, so that state explosion never occurs when converting these segments to DFA, and (2) compiling the resulting segments and their syntagmatic relations, respectively, into DFA and relation mapping table (RMT), which together make up the final matching engine named FREME.#R##N##R##N#Despite the pattern partition, FREME does not sacrifice any matching correctness with the aid of RMT. Evaluation based on real-world pattern sets (open source and commercial) shows that FREME scales linearly with the size of pattern set, meanwhile keeps fast matching based on nonexplosive DFA. In contrast, FREME outperforms state-of-the-art matching engines up to two orders of magnitude.	regular expression;scalability	Kai Wang;Jun Li	2015	J. Network and Computer Applications	10.1016/j.jnca.2015.05.012	theoretical computer science;dfa minimization;algorithm	Networks	-9.060945281394769	-39.46286035142758	108821
15fe57b15ddbc2cade2423d7aa337a20e37d979e	who is the best connected ec researcher?	scientometrics;complex networks;complex network;multi objective optimization;social network analysis	The coauthorship graph (that is, the graph of authors linked by coauthorship of papers) is a complex network, which expresses the dynamics of a complex system. Only recently its study has started to draw interest from the EC community, the first paper dealing with it being published two years ago. In this paper we study the coauthorship network of EC at a microscopic level. Our objective is ascertaining which are the most relevant nodes (i.e. authors) in it. We have defined our network using data taken from the DBLP. The network comprises 7712 authors, linked if they have coauthored a paper. The importance –i.e., centrality– of a node can be measured in different ways. The metrics we have considered are betweenness (the relative fraction of geodesics -i.e., shortest pathsbetween any two nodes i, j that pass through a node k), closeness (average distance to other nodes), Bonacich’s power (a measure related to having many neighbors whose power is high too), and eigenvector (the centrality coefficients taken from the eigenvector associated with the dominant eigenvalue of the adjacency matrix). We have analyzed them both in isolation and combined within a Pareto-dominance approach (the first time this is done, to the best of our knowledge). The result of our analysis indicates that there are some well-known researchers that appear systematically in top rankings. We also note that eigenvector centrality is likely to promote some authors due to “hitchhiking” effects. Computing the successive Pareto-fronts resulting from betweenness, closeness, and Bonacich’s power, we obtain the following results: (front #1) K. Deb, D.E. Goldberg, (front #2) Z. Michalewicz, M. Schoenauer, (front #3) T. Bäck, A.E. Eiben, H. de Garis, D. Keymeulen, B. Paechter, M. Tomassini, X. Yao, (front #4) D.B. Fogel, J.J. Merelo, T.	adjacency matrix;betweenness;coefficient;complex network;complex system;dbl-browser;david b. fogel;dynamic energy budget;eigenvector centrality;pareto efficiency;yao graph	Juan Julián Merelo Guervós;Carlos Cotta	2007		10.1145/1276958.1277255	computer science;artificial intelligence;data mining;management science;complex network	ML	-15.506464616646538	-40.59599945647575	108986
4f1ef8cd3c90f7d5de960abe17b517ad2441dd49	energy saving mechanisms, collective behavior and the variation range hypothesis in biological systems: a review	bioconvection;peloton;energy saving mechanism;drafting;flock;herd;school;evolution	Energy saving mechanisms are ubiquitous in nature. Aerodynamic and hydrodynamic drafting, vortice uplift, Bernoulli suction, thermoregulatory coupling, path following, physical hooks, synchronization, and cooperation are only some of the better-known examples. While drafting mechanisms also appear in non-biological systems such as sedimentation and particle vortices, the broad spectrum of these mechanisms appears more diversely in biological systems that include bacteria, spermatozoa, various aquatic species, birds, land animals, semi-fluid dwellers like turtle hatchlings, as well as human systems. We present the thermodynamic framework for energy saving mechanisms, and we review evidence in favor of the variation range hypothesis. This hypothesis posits that, as an evolutionary process, the variation range between strongest and weakest group members converges on the equivalent energy saving quantity that is generated by the energy saving mechanism. We also review self-organized structures that emerge due to energy saving mechanisms, including convective processes that can be observed in many systems over both short and long time scales, as well as high collective output processes in which a form of collective position locking occurs.	approximation algorithm;aquatic ecosystem;bernoulli polynomials;biological system;hydrodynamics;lock (computer science);quantity;rem sleep behavior disorder;self-organization;semiconductor industry;suction drainage;thermodynamics;turtle;vortex	Hugh Trenchard;Matjaž Perc	2016	Bio Systems	10.1016/j.biosystems.2016.05.010	biology;flock;simulation;herd;evolution;ecology;genetics	HPC	-5.399175478412727	-46.71055404892448	109012
2614619595b3bd903eaf9750dc9f681210b7053c	active clustering of document fragments using information derived from both images and catalogs	pattern clustering;complexity theory;history;computer model;catalogs;data model;pattern clustering document image processing history;visualization;computational modeling;graphical models;document image processing;graphical model;handwriting pairwise similarities active clustering document fragments image information historical corpora multipage documents semiautomatic clustering original document clustering graphical model catalog information;humans;catalogs computational modeling graphical models complexity theory humans data models visualization;data models	Many significant historical corpora contain leaves that are mixed up and no longer bound in their original state as multi-page documents. The reconstruction of old manuscripts from a mix of disjoint leaves can therefore be of paramount importance to historians and literary scholars. Previously, it was shown that visual similarity provides meaningful pair-wise similarities between handwritten leaves. Here, we go a step further and suggest a semiautomatic clustering tool that helps reconstruct the original documents. The proposed solution is based on a graphical model that makes inferences based on catalog information provided for each leaf as well as on the pairwise similarities of handwriting. Several novel active clustering techniques are explored, and the solution is applied to a significant part of the Cairo Genizah, where the problem of joining leaves remains unsolved even after a century of extensive study by hundreds of scholars.	cluster analysis;graphical model;handwriting recognition;text corpus;cairo	Lior Wolf;Lior Litwak;Nachum Dershowitz;Roni Shweka;Yaacov Choueka	2011	2011 International Conference on Computer Vision	10.1109/ICCV.2011.6126428	computer simulation;computer science;data science;machine learning;data mining;graphical model;information retrieval	Vision	-10.987724657923707	-46.664219625885075	109022
6a02cb8716527496894ddbf32e814e574049af78	an adaptive approximation algorithm for community detection in dynamic scale-free networks	optimisation;complex networks;social networks adaptive approximation algorithm community structure modularity;approximation theory;social networking online approximation theory complex networks computational complexity optimisation;computational complexity;social networking online;communities heuristic algorithms approximation methods adaptive algorithms approximation algorithms social network services time complexity;facebook social networks dynamic scale free networks adaptive approximation algorithm community detection a3cs adaptive framework community structure modularity q power law distribution property np hard modularity maximization problem synthesized networks arxiv e print citation	We introduce A3CS, an adaptive framework with approximation guarantees for quickly identifying community structure in dynamic networks via maximizing Modularity Q. Our framework explores the advantages of power-law distribution property, is scalable for very large networks, and more excitingly, possesses approximation factors to ensure the quality of its detected community structure. To the best of our knowledge, this is the first framework that achieves approximation guarantees for the NP-hard modularity maximization problem, especially on dynamic networks. To certify our approach, we conduct extensive experiments in comparison with other adaptive methods on both synthesized networks with known community structures and real-world traces including ArXiv e-print citation and Facebook social networks. Excellent empirical results not only confirm our theoretical results but also promise the practical applicability of A3CS in a wide range of dynamic networks.	approximation algorithm;entropy maximization;experiment;np-hardness;scalability;social network;tracing (software)	Thang N. Dinh;Nam P. Nguyen;My T. Thai	2013	2013 Proceedings IEEE INFOCOM	10.1109/INFCOM.2013.6566734	computer science;modularity;theoretical computer science;machine learning;distributed computing;clique percolation method;computational complexity theory;complex network;approximation theory	DB	-16.02475007407141	-43.367281541233645	109075
64e4290e7b46cbce8dafc3b5a2589b3337827d8f	multi-resolution sketches and locality sensitive hashing for fast trajectory processing		Searching for similar GPS trajectories is a fundamental problem that faces challenges of large data volume and intrinsic complexity of trajectory comparison. In this paper, we present a suite of sketches for trajectory data that drastically reduce the computation costs associated with near neighbor search, distance estimation, clustering and classification, and subtrajectory detection. Apart from summarizing the dataset, our sketches have two uses. First, we obtain simple provable locality sensitive hash families for both the Hausdorff and Fréchet distance measures, useful in near neighbour queries. Second, we build a data structure called MRTS (Multi Resolution Trajectory Sketch), which contains sketches of varying degrees of detail. The MRTS is a user-friendly, compact representation of the dataset that allows to efficiently answer various other types of queries. Moreover, MRTS can be used in a dynamic setting with fast insertions of trajectories into the database.  Experiments on real data show effective locality sensitive hashing substantially improves near neighbor search time. Distances defined on the skteches show good correlation with Fréchet and Hausdorff distances.	acclimatization;biological system;clinical act of insertion;cluster analysis;computation;data structure;embedded system;embedding;face;financial cost;fréchet distance;global positioning system;hausdorff dimension;locality of reference;locality-sensitive hashing;marijuana abuse;provable security;randomized algorithm;robotics;silo (dataset);sketch;statistical classification;usability;statistical cluster	Jacob Burgoon;Paul Cesaretti;Panagiota Katsikouli;Mayank Goswami;Rik Sarkar	2018		10.1145/3274895.3274943	data mining;locality-sensitive hashing;algorithm;hash function;data compression;fréchet distance;locality;data structure;cluster analysis;trajectory;computer science	ML	-4.6533080624805745	-42.28504902464052	109169
2983f4dc99ae2d7c85f81ad53040f6e478d0b85d	a link prediction approach to recommendations in large-scale user-generated content systems	flickr;large dataset;link prediction;user generated content systems;large scale;collaborative filtering;recommendation;k nearest neighbor;online social network;user generated content	Recommending interesting and relevant content from the vast repositories of User-Generated Content systems (UGCs) such as YouTube, Flickr and Digg is a significant challenge. Part of this challenge stems from the fact that classical collaborative filtering techniques – such as k-Nearest Neighbor – cannot be assumed to perform as well in UGCs as in other applications. Such technique has severe limitations regarding data sparsity and scalability that are unfitting for UGCs. In this paper, we employ adaptations of popular Link Prediction algorithms that were shown to be effective in massive online social networks for recommending items in UGCs. We evaluate these algorithms on a large dataset we collect from Flickr. Our results suggest that Link Prediction algorithms are a more scalable and accurate alternative to classical collaborative filtering in the context of UGCs. Moreover, our experiments show that the algorithms considering the immediate neighborhood of users in an user-item graph to recommend items outperform the algorithms that use the entire graph structure for the same. Finally, we find that, contrary to intuition, exploiting explicit social links among users in the recommendation algorithms improves only marginally their performance.	algorithm;baseline (configuration management);bookmark (world wide web);collaborative filtering;experiment;flickr;nearest-neighbor interpolation;pagerank;precision and recall;recommender system;scalability;social network;sparse matrix;user-generated content	Nitin Chiluka;Nazareno Andrade;Johan A. Pouwelse	2011		10.1007/978-3-642-20161-5_19	computer science;collaborative filtering;machine learning;data mining;user-generated content;world wide web;k-nearest neighbors algorithm;information retrieval	ML	-19.10827170021117	-48.662666815147325	109534
5a82b60422e94984ad2fd83634588205003404ee	prediction of subscriber churn using social network analysis	selected works;bepress	In today's world, mobile phone penetration has reached a saturation point. As a result, subscriber churn has become an important issue for mobile operators as subscribers switch operators for a variety of reasons. Mobile operators typically employ churn prediction algorithms based on service usage metrics, network performance indicators, and traditional demographic information. A newly emerging technique is the use of social network analysis (SNA) to identify potential churners. Intuitively, a subscriber who is churning will have an impact on the churn propensity of his social circle. Call detail records are useful to understand the social connectivity of subscribers through call graphs but do not directly provide the strength of their relationship or have enough information to determine the diffusion of churn influence. In this paper, we present a way to address these challenges by developing a new churn prediction algorithm based on a social network analysis of the call graph. We provide a formulation that quantifies the strength of social ties between users based on multiple attributes and then apply an influence diffusion model over the call graph to determine the net accumulated influence from churners. We combine this influence and other social factors with more traditional metrics and apply machine-learning methods to compute the propensity to churn for individual users. We evaluate the performance of our algorithm over a real data set and quantify the benefit of using SNA in churn prediction.	algorithm;call graph;gene prediction;gradient boosting;machine learning;mobile phone;network performance;social media analytics;social network analysis;software propagation;text mining	Chitra Phadke;Hüseyin Uzunalioglu;Veena B. Mendiratta;Dan Kushnir;Derek Doran	2013	Bell Labs Technical Journal	10.1002/bltj.21575	simulation;computer science;engineering;data mining;computer network	ML	-18.09295111761659	-42.32801981147016	109885
a9504fd450f5205623bfad96cf142a20ab9d852d	inhibiting diffusion of complex contagions in social networks: theoretical and experimental results	blocking;social networks;complex contagions	We consider the problem of inhibiting undesirable contagions (e.g. rumors, spread of mob behavior) in social networks. Much of the work in this context has been carried out under the 1-threshold model, where diffusion occurs when a node has just one neighbor with the contagion. We study the problem of inhibiting more complex contagions in social networks where nodes may have thresholds larger than 1. The goal is to minimize the propagation of the contagion by removing a small number of nodes (called critical nodes) from the network. We study several versions of this problem and prove that, in general, they cannot even be efficiently approximated to within any factor $$\rho \ge 1$$ ρ ≥ 1 , unless P = NP. We develop efficient and practical heuristics for these problems and carry out an experimental study of their performance on three well known social networks, namely epinions, wikipedia and slashdot. Our results show that these heuristics perform significantly better than five other known methods. We also establish an efficiently computable upper bound on the number of nodes to which a contagion can spread and evaluate this bound on many real and synthetic networks.	anatomic node;apollonian network;approximation algorithm;blocking (computing);computable function;computational complexity theory;deletion mutation;experiment;heuristics;inhibition;large;mental blocking;node - plant part;p versus np problem;partial;rem sleep behavior disorder;slashdot;social network;software propagation;synthetic intelligence;time-varying network;version;weight;wikipedia	Chris J. Kuhlman;V. S. Anil Kumar;Madhav V. Marathe;S. S. Ravi;Daniel J. Rosenkrantz	2014	Data Mining and Knowledge Discovery	10.1007/s10618-014-0351-4	artificial intelligence;machine learning;blocking;statistics;social network	ML	-16.167174538847878	-43.1307541611863	110376
868e3d01289b7cce532532be6207bc6ce121a0f6	graph compartmentalization		This article introduces a concept and measure of graph compartmentalization. This new measure allows for principled comparison between graphs of arbitrary structure, unlike existing measures such as graph modularity. The proposed measure is invariant to graph size and number of groups and can be calculated analytically, facilitating measurement on very large graphs. I also introduce a block model generative process for compartmentalized graphs as a benchmark on which to validate the proposed measure. Simulation results demonstrate improved performance of the new measure over modularity in recovering the degree of compartmentalization of graphs simulated from the generative model. I also explore an application to the measurement of political polarization.	benchmark (computing);compartmentalization (information security);generative model;network compartment;polarization (waves);simulation	Matthew J. Denny	2014	CoRR			ML	-15.037330593992207	-39.82506553771887	110466
ce1e6d26e0bc11fd350113d9018747d12308689c	distributed simulation of metabolic networks with model variants	metabolic network;distributed simulation		simulation	Marc Daniel Haunschild;Bernd Freisleben;Wolfgang Wiechert;Ralf Takors	2002			bioinformatics;distributed computing	ECom	-5.040293998299707	-51.6978662118361	110807
98e5990847c587cfd9df90cc2450f661e7f86dce	detecting large cohesive subgroups with high clustering coefficients in social networks	cohesive subgroups;clustering coefficient;optimization;clique relaxations	Clique relaxations  are used in classical models of cohesive subgroups in social network analysis.  Clustering coefficient  was introduced more recently as a structural feature characterizing small-world networks. Noting that cohesive subgroups tend to have high clustering coefficients, this paper introduces a new clique relaxation,  α - cluster , defined by enforcing a lower bound  α  on the clustering coefficient in the corresponding induced subgraph. Two variations of the clustering coefficient are considered, namely, the local and global clustering coefficient. Certain structural properties of  α -clusters are analyzed and mathematical optimization models for determining  α -clusters of the largest size in a network are developed and validated using several real-life social networks. In addition, a network clustering algorithm based on local  α -clusters is proposed and successfully tested.	cluster analysis;coefficient;sensor;social network	Zeynep Ertem;Alexander Veremyev;Sergiy Butenko	2016	Social Networks	10.1016/j.socnet.2016.01.001	correlation clustering;mathematical optimization;combinatorics;discrete mathematics;k-medians clustering;fuzzy clustering;flame clustering;clustering coefficient;mathematics;cluster analysis;single-linkage clustering	ML	-14.294555177478149	-41.354117423591845	110991
5eed00a268e8ca27df03f00c6a0c44e5ef6370e1	towards a more realistic evaluation: testing the ability to predict future tastes of matrix factorization-based recommenders	matrix factorization;temporal dynamics;recommender system;time aware recommender systems;time aware evaluation	The use of temporal dynamic terms in Matrix Factorization (MF) models of recommendation have been proposed as a means to obtain better accuracy in rating prediction task. However, the way such models have been tested may not be a realistic setting for recommendation. In this paper, we evaluated rating prediction and top-N recommendation tasks using a MF model with and without temporal dynamic terms under two evaluation settings. Our experiments show that the addition of dynamic parameters do not necessarily yield to better results on these tasks when a more strict time-aware separation of train/test data is performed, and moreover, results may vary notably when different evaluation schemes are used.	experiment;test data	Pedro G. Campos;Fernando Díez;Manuel A. Sánchez-Montañés	2011		10.1145/2043932.2043990	simulation;computer science;machine learning;data mining;multimedia;matrix decomposition;world wide web;recommender system	Web+IR	-19.007359835844554	-47.61843788467819	111172
8c855ef0e089073c9880379d7c13216012614139	fca for common interest communities discovering	social network services;semantics;social networks common interest communities formal concept analysis;social networking online data mining formal concept analysis;system on chip;clustering algorithms;formal concept analysis fca common interest communities discovering social networks long standing problem automatic communities extraction cic soc miner communities detection;communities;blogs;context;communities system on chip social network services blogs context clustering algorithms semantics	Major scientific and industrial issues related to social networks have led many researchers to focus on the long-standing problem of automatic communities extraction. The vast majority of the proposed methods tend to make a partition of the entities from the initial graph of observed relationships. The semantics of these relationships is rarely considered. The lack of information about the elements that connect or repel individuals course, is the main cause. In this article, we focus our interest on the detection of Common Interest Communities (CIC) in social networks. In this respect, we introduce a new approach called SOC Miner for communities detection based on the use of both Formal Concept Analysis (FCA) techniques and data from social networks. Carried out experiments over real-world data sets emphasize the relevance of our proposal and open many issues.	cluster analysis;entity;experiment;formal concept analysis;internationalization and localization;list of code lyoko episodes;louvain modularity;relevance;social network	Soumaya Guesmi;Chiraz Trabelsi;Cherif Chiraz Latiri	2014	2014 International Conference on Data Science and Advanced Analytics (DSAA)	10.1109/DSAA.2014.7058111	computer science;data science;data mining;world wide web	DB	-17.687177348155096	-41.82134701739303	111502
667fef5ba1dafd6960831ece2ca4932ad0380076	top-cop: mining top-k strongly correlated pairs in large databases	pearson correlation coefficient top cop top k strongly correlated pair mining large databases minimum correlation threshold 2d monotone property;database management systems data mining;pearson correlation coefficient;top k strongly correlated pair mining;database management systems;top cop;efficient algorithm;data mining;large databases;upper bound;2d monotone property;data mining transaction databases upper bound marketing and sales algorithm design and analysis public healthcare bioinformatics promotion marketing computational efficiency books;minimum correlation threshold;correlation coefficient	Recently, there has been considerable interest in computing strongly correlated pairs in large databases. Most previous studies require the specification of a minimum correlation threshold to perform the computation. However, it may be difficult for users to provide an appropriate threshold in practice, since different data sets typically have different characteristics. To this end, we propose an alternative task: mining the top-k strongly correlated pairs. In this paper, we identify a 2-D monotone property of an upper bound of Pearson's correlation coefficient and develop an efficient algorithm, called TOP-COP to exploit this property to effectively prune many pairs even without computing their correlation coefficients. Our experimental results show that the TOP-COP algorithm can be orders of magnitude faster than brute-force alternatives for mining the top-k strongly correlated pairs.	algorithm;coefficient;computation;database;experiment;hereditary property;tree traversal;monotone	Hui Xiong;Mark Brodie;Sheng Ma	2006	Sixth International Conference on Data Mining (ICDM'06)	10.1109/ICDM.2006.161	pearson product-moment correlation coefficient;computer science;data science;data mining;mathematics;upper and lower bounds;statistics	DB	-5.9688760145246595	-40.13470868922286	111596
357e2836df58af0006ae2b52c2e5c48715d41f67	estimating local information trustworthiness via multi-source joint matrix factorization	reliability cities and towns joints matrix decomposition convergence algorithm design and analysis optimization;trustworthiness;recommendation system;user interfaces data privacy matrix decomposition recommender systems;data privacy;matrix decomposition;new york city local information trustworthiness estimation multisource joint matrix factorization information source latent matrix space user review system recommendation system customer rating item rating reliability score computation group membership matrix group rating matrix orbitz priceline trip advisor las vegas;joint matrix factorization;recommendation system trustworthiness joint matrix factorization;user interfaces;recommender systems	We investigate how to estimate information trustworthiness by considering multiple information sources jointly in a latent matrix space. We particularly focus on user review and recommendation systems, as there are multiple platforms where people can rate items and services that they have purchased, and many potential customers rely on these opinions to make decisions. Information trustworthiness is a serious problem because ratings are generated freely by end-users so that many stammers take advantage of freedom of speech to promote their business or damage reputation of competitors. We propose to simply use customer ratings to estimate each individual source's reliability by exploring correlations among multiple sources. Ratings of items are provided by users of diverse tastes and styles, and thus may appear noisy and conflicting across sources, however, they share some underlying common behavior. Therefore, we can group users based on their opinions, and a source is reliable on an item if its opinions given by latent groups are consistent across platforms. Inspired by this observation, we solve the problem by a two-step model -- a joint matrix factorization procedure followed by reliability score computation. We propose two effective approaches to decompose rating matrices as the products of group membership and group rating matrices, and then compute consistency degrees from group rating matrices as source reliability scores. We conduct experiments on both synthetic data and real user ratings collected from Orbitz, Priceline and Trip Advisor on all the hotels in Las Vegas and New York City. Results show that the proposed method is able to give accurate estimates of source reliability and thus successfully identify inconsistent, conflicting and unreliable information.	computation;coordinate descent;experiment;mathematical optimization;non-negative matrix factorization;recommender system;review site;synthetic data;time complexity;trust (emotion)	Liang Ge;Jing Gao;Xiao Yu;Wei Fan;Aidong Zhang	2012	2012 IEEE 12th International Conference on Data Mining	10.1109/ICDM.2012.151	trustworthiness;simulation;computer science;machine learning;data mining;matrix decomposition;user interface;world wide web;recommender system	ML	-18.941450677854565	-46.59679597988001	111641
1dcccbd4d2e7a1551450c1c5081e1f16759208e9	evaluating performance of recommender systems: an experimental comparison	institutional repositories;groupware;performance recommender systems evaluating;fedora;computed tomography;circuit faults;motion pictures;current transformers;kernel based algorithm recommender system performance evaluation confidence metrics decision making collaborative recommendation method;performance;information filtering;fault currents;vital;evaluating;accuracy;recommender system;information retrieval system evaluation;robustness;sparse data;information retrieval system evaluation decision making groupware information filtering information filters;vtls;information filters;recommender systems time measurement robustness intelligent agent conference management technology management management information systems collaborative work algorithm design and analysis particle measurements;recommender systems;ils	"""Much early evaluation work focused specifically on the """"accuracy"""" of recommendation algorithms. Good recommendation (in terms of accuracy) has, however, to be coupled with other considerations. This work suggests measures aiming at evaluating other aspects than accuracy of recommendation algorithms. Other considerations include (1) coverage, which measures the percentage of a data set that a recommender system is able to provide recommendation for, (2) confidence metrics that can help users make more effective decisions, (3) computing time, which measures how quickly an algorithm can produce good recommendations, (4) novelty/serendipity, which measure whether a recommendation is original, and (5) robustness which measure the ability of the algorithm to make good predictions in the presence of noisy or sparse data. Six collaborative recommendation methods are investigated. Results on artificial data sets (for robustness) or on the real MovieLens data set (for accuracy, novelty, and computing time) are included and analyzed, showingthat kernel-based algorithms provide the best results overall."""	algorithm;cybernetic serendipity;movielens;recommender system;sparse matrix	François Fouss;Marco Saerens	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.252	current transformer;sparse matrix;performance;computer science;machine learning;data mining;accuracy and precision;world wide web;information retrieval;robustness;recommender system	ML	-16.50941523008872	-51.1349308336226	111707
1f559f4f808a3856ae2c684f4b95f4a4dce1dff0	how social network is evolving?: a preliminary study on billion-scale twitter network	degree of separation;diameter;degree distribution;social network analysis;twitter;reciprocity	Recently, social network services such as Twitter, Facebook, MySpace, LinkedIn have been remarkably growing. There are various studies about social networks analysis. Haewoon Kwak performed the analysis of the Twitter network on 2009 and shows the degree of separation. However, the number of users on 2009 is about 41.7 million, the graph scale is not very large compared with the current graph. In this paper, we conduct a Twitter network analysis in terms growth by region, scale-free, reciprocity, degree of separation and diameter using Twitter user data with 469.9 million users and 28.7 billion relationships. We report that the value of degree of separation is 4.59 in current Twitter network through our experiments.	diameter (protocol);experiment;six degrees of separation;social network analysis	Masaru Watanabe;Toyotaro Suzumura	2013		10.1145/2487788.2487988	six degrees of separation;social network analysis;degree distribution;computer science;diameter;internet privacy;reciprocity;world wide web	Web+IR	-17.313296140919178	-41.144537600066194	111769
937e452351a836a24ad310d8b018cfa6a00d91f7	learning deep representations in large integrated network for graph clustering		Social communities, which are closely related groups based on some characteristics, are common structures hidden in social networks. In general, users tend to cluster in a group because of similar interests or frequent interactions. The identification of such communities in the social networks is of methodological and practical value. Existing methods are limited in that user network needs learning from heterogeneous networks, the complete feature expression of each user is ignored. To address this challenge, we propose a novel model, called DeepInNet. This method obtains a comprehensive deep representation by learning the information of different modes, which are presented by the corresponding network structure and represent the heterogeneous information in the social network. To perform the task, DeepInNet first computes the various diffusion state of each node from the heterogeneous network as features. Given the integrated network representation, we introduced a stacked auto-encoder model to form the deep neural network and learn the deep representation. Such low-dimensional representations could be used to cluster interest communities in the social network quickly. DeepInNet has been tested with four real-world datasets include two large-scale datasets. It also has been compared with several common approaches to social network clustering. The experimental results show that the integrated deep representation found by DeepInNet may match well with the known social communities and it is able to outperform the state-of-the-art approaches to analyzing the large-scale social network.	artificial neural network;autoencoder;cluster analysis;clustering coefficient;computational complexity theory;deep learning;encoder;experiment;interaction;k-means clustering;scalability;semantic network;social network	Pengwei Hu;Zhaomeng Niu;Tiantian He;Keith C. C. Chan	2018	2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)	10.1109/AIKE.2018.00022	machine learning;artificial neural network;social network;clustering coefficient;cluster analysis;heterogeneous network;artificial intelligence;computer science	ML	-14.74493105033938	-46.09383464574477	111867
74950e3ff624058cca544376fbdb56a679743da2	an efficient homophilic model and algorithms for community detection using nash dynamics		The problem of community detection is important as it helps in understanding the spread of information in a social network. All real complex networks have an inbuilt structure which captures and characterizes the network dynamics between its nodes. Linkages are more likely to form between similar nodes, leading to the formation of some community structure which characterizes the network dynamic. The more friends they have in common, the more the influence that each person can exercise on the other. People use their attributes to assess the similarity of the other people with them, similarly, the attributes are also influenced with the people they interact and the network structure. Hence, we assume that communities capture homophily as people of the same community share a lot of similar features. The contributions of my thesis are as follows: • We propose a disjoint community detection algorithm, NashDisjoint that detects disjoint communities in any given network. • We evaluate the algorithm NashDisjoint against the best state of the art algorithms so far, and we find that our algorithm works at least as good as that of the state of the art algorithms on the standard LFR Benchmarks for the mixing factors less than 0.55 in all the cases. • On Real Social Networks, we observe that the modularity values for the community structure detected by our algorithm NashDisjoint is in comparison to that of one of the best modularity optimization algorithms so far. • We propose an overlapping community detection algorithm NashOverlap to detect the overlapping communities in any given network. • We evaluate the algorithm NashOverlap against the best state of the art algorithms so far and we find that our algorithm works far better than the state of the art algorithms on the standard LFR benchmarks in around 140 different scenarios, generated by varying the number of nodes, mixing factor and overlapping membership. • The algorithm NashDisjoint is modeled as a sequence of weighted potential games. As a part of the first game, the algorithm computes the closeness values for every edge. Worst case time for the first game to converge is O(k·k1 ·m) where k1 is the number of iterations it takes for each of the k subgames to converge. As a part of the second game, the algorithm picks the closeness values returned by the first game and computes the disjoint community structure. Its time complexity is O(k2 · m) where k2 is the number of iterations it takes for the second game to converge. • Of all the community structures possible in the given network, our algorithm NashDisjoint detects the community structure which has a minimal value for the sum of the probability strengths of the cut edges of that community structure. 1 ar X iv :1 50 6. 05 65 9v 1 [ cs .S I] 1 8 Ju n 20 15 • Similarly, the overlapping community detection algorithm NashOverlap can be formulated as a sequence of weighted potential games, where the value of the potential function for a given cover is constantly proportional to the sum of the probability strengths of the cut edges of that cover. For a given cover, duplicate the vertex for each community that it is a part of, and the cut edges are the edges between the vertices between two different communities. The time complexity is O(k · k1 ·m) for the first game and O(k2 ·m) for the second game where k1 and k2 are respectively the number of iterations it takes for both first game and the second game to converge and k is the number of subgames. • We run our algorithm NashOverlap on DBLP dataset to detect the top collaboration groups. We have identified a giant component as a top collaboration group and the second largest collaboration group is much smaller than the giant component. This is due to the percolating regime characteristic of such networks, there exist very small connected clusters, each with very few edges in them, and of which most of them group to form a giant component with the increase in the number of edges in the network, leaving behind small clusters, which only group later into the giant component, with the further increase in connections. This leads to a reasoning which says that the scientific collaboration network is highly connected and there is a possibility of more interdisciplinary work in the future which is a good sign for the development of science. The diameter of the largest collaboration group for both the datasets is between 9 and 11 and has the average path length between 4 and 5, which strengthens the above argument. Also, the average number of intermediate researchers that connect any pair of researchers in the giant component is quite less. • These results of our algorithm on DBLP collaboration network are compared with the results of the COPRA algorithm and OSLOM. We find that our algorithm has detected more collaboration groups of significant size each of which carries an identity, compared to that of COPRA or OSLOM. Also, the communities detected by our algorithm are much bigger in size with a least number of outlier nodes. The community structure is likely to have more mixing factor than that of detected by COPRA and OSLOM. It means, that the collaboration groups are big enough in size with several researchers in each, so that almost no researcher who concern this collaboration group is ignored.	algorithm;average path length;bridge (graph theory);centrality;complex network;converge;dbl-browser;data mining;embnet.journal;existential quantification;experiment;game theory;giant component;grid north;interaction network;iteration;louvain modularity;machine learning;mathematical optimization;nash equilibrium;parallel computing;percolation theory;scalability;scientific collaboration network;sensor;social network;theoretical computer science;time complexity	Radhika Arava	2015	CoRR		simulation;computer science;artificial intelligence;machine learning;data mining;world wide web	Web+IR	-15.75007628815907	-42.481049013885446	111993
81234c68063adeb3dc50f4f60bec23521b1b2809	synergetic approach and modeling of fish population dynamics	marine ecosystem;environment variability;marine environment;population dynamic;number of factors;population dynamics;stochastic model;simulation model;synergetics	In the last few years it has become increasingly obvious that one of the obstacles in the way of constructing good simulation models of the global ocean ecosystem is a poor understanding of the general principles of marine ecosystems processes. A great number of factors and relationships acting in the marine environment, in combination with the random character of change in many of them, call for the development of new approaches in modeling. In this paper a synergetic approach is proposed. A new paradigm for this approach is discussed. As an example a population with logistic natural growth under different conditions of exploitation is considered. It is shown that the simplest mechanism, that principally changes the behavior of a population in a fluctuating environment, includes fishing and migration. This mechanism explains catastrophic changes in population abundance in cases when no one factor may be seen as exclusive. It is shown that the characteristic level of population number does not correspond to the average balance between input (migration), output (fishing) and the growth of the population. The environment variability leads to stabilization far from equilibrium. This totally conforms to one of the fundamental results in Synergetics which assert that non-equilibrium in the presence of fluctuations may serve as a source of new order.	heart rate variability;marine ecosystem;population dynamics;programming paradigm;simulation;synergetics (haken);synergy;system migration	Vyacheslav Navrotsky	2000	Annals OR	10.1023/A:1018981519768	simulation;stochastic modelling;marine ecosystem;simulation modeling;population dynamics	Theory	-5.205898694802921	-46.60741630676587	112278
bf2b5e7abd5e267d78e86b75b2b47476485859dc	using non-zero dimensions and lengths of vectors for the tanimoto similarity search among real valued vectors	text mining;sparse high dimensional data;information retrieval;data mining;chemical informatics;the tanimoto similarity;bioinformatics	The Tanimoto similarity measure finds numerous applications e.g. in chemical informatics, bioinformatics, information retrieval, text and web mining. Recently, two efficient methods for reducing the number of candidates for Tanimoto similar real valued vectors have been offered: the one using lengths of vectors and the other using their non-zero dimensions. In this paper, we offer new theoretical results on combined usage of lengths of real valued vectors and their non-zero dimensions for more efficient reduction of candidates for Tanimoto similar vectors. In particular, we derive more restrictive bounds on lengths of such candidate vectors.		Marzena Kryszkiewicz	2014		10.1007/978-3-319-05476-6_18	text mining;computer science;data science;data mining;information retrieval	DB	-4.78954568905238	-44.034986267925326	112432
93c1bf2a3c06664b9c9dcf98f336dfbcf49dff4f	power laws in biology: between fundamental regularities and useful interpolation rules	useful interpolation rule;power law;fundamental regularity	Why live larger mammals longer than smaller ones? Why is the energy consumption per body mass of a mouse six times higher than that of a human? These questions and many others dealing with biological allometry kept and keep biologists busy since the second half of nineteenth century and as it seems, the ultimate answers have not yet been given. Analyzing allometry is particularly attractive since the biomass of organisms varies over more than twenty orders of magnitude from approximately 1 pg = 10 g (mycoplasma, a very small bacterium) to 2  10 g (blue whale), and in case of mammals the lower limit by mass is provided by the Etruscan Shrew with about 1 g thus still leaving eight orders of magnitude variation in body mass. The wide range of animal sizes makes body mass related properties an ideal test ground for scaling relations, in particular for power laws, and this is the reason why body mass allometry is chosen here as a representative and data rich example for other power laws. The number of papers dealing with attempt to scale body mass dependent relations in log/log-plots is indeed enormous.	human body weight;image scaling;interpolation	Peter Schuster	2011	Complexity	10.1002/cplx.20366	calculus;pure mathematics;mathematics	Networks	-7.681416871855941	-47.04955219469495	112807
0ffef3e1ecba4dc6995fbe38a62f56eb1e7ba6f3	eliminating the redundancy in blocking-based entity resolution methods	optimal solution;digital library;heterogeneous data;redundancy based blocking;space complexity;entity resolution;data cleaning;block method	Entity resolution is the task of identifying entities that refer to the same real-world object. It has important applications in the context of digital libraries, such as citation matching and author disambiguation. Blocking is an established methodology for efficiently addressing this problem; it clusters similar entities together, and compares solely entities inside each cluster. In order to effectively deal with the current large, noisy and heterogeneous data collections, novel blocking methods that rely on redundancy have been introduced: they associate each entity with multiple blocks in order to increase recall, thus increasing the computational cost, as well.  In this paper, we introduce novel techniques that remove the superfluous comparisons from any redundancy-based blocking method. They improve the time-efficiency of the latter without any impact on the end result. We present the optimal solution to this problem that discards all redundant comparisons at the cost of quadratic space complexity. For applications with space limitations, we also present an alternative, lightweight solution that operates at the abstract level of blocks in order to discard a significant part of the redundant comparisons. We evaluate our techniques on two large, real-world data sets and verify the significant improvements they convey when integrated into existing blocking methods.	algorithmic efficiency;blocking (computing);cartesian closed category;computation;dspace;digital library;entity;library (computing);mapreduce;parallel computing;requirement;word-sense disambiguation	George Papadakis;Ekaterini Ioannou;Claudia Niederée;Themis Palpanas;Wolfgang Nejdl	2011		10.1145/1998076.1998093	digital library;name resolution;computer science;theoretical computer science;data mining;dspace;world wide web;algorithm	DB	-8.07847407844915	-40.86339944046848	112907
f334b60436c0b8d5c442375f954eb7d0a1292184	minmax circular sector arc for external plagiarism's heuristic retrieval stage		Abstract Heuristic Retrieval (HR) task aims to retrieve a set of documents from which the External Plagiarism detection identifies plagiarized pieces of text. In this context, we present Minmax Circular Sector Arcs ( MinmaxCSA ) algorithms that treats HR task as an approximate k -nearest neighbor search problem. Moreover, MinmaxCSA algorithms aim to retrieve the set of documents with greater amounts of plagiarized fragments, while reducing the amount of time to accomplish the HR task. Our theoretical framework is based on two aspects: (i) a triangular property to encode a range of sketches on a unique value; and (ii) a Circular Sector Arc property which enables (i) to be more accurate. Both properties were proposed for handling high-dimensional spaces, hashing them to a lower number of hash values. Our two MinmaxCSA methods, Minmax Circular Sector Arcs Lower Bound ( CSA L ) and Minmax Circular Sector Arcs Full Bound ( CSA ), achieved Recall levels slightly more imprecise than Minmaxwise hashing in exchange for a better Speedup in document indexing and query extraction and retrieval time in high-dimensional plagiarism-related datasets.	heuristic;minimax	Fellipe Duarte;Danielle Caled;Geraldo Xexéo	2017	Knowl.-Based Syst.	10.1016/j.knosys.2017.08.013	data mining;locality-sensitive hashing;artificial intelligence;machine learning;computer science;approximation algorithm;search engine indexing;hash function;plagiarism detection;speedup;circular sector;nearest neighbor search	NLP	-5.70701759058431	-41.61288349165575	113245
2e64075837749b943db49e8c2e8bb5bfaeb48e94	patterns of interactions in complex social networks based on coloured motifs analysis	metabolic network;food web;building block;complex network;network motif;gene network;social network;local structure;social science;network structure;communication pattern;biological network;transcriptional regulatory network	Coloured network motifs are small subgraphs that enable to discover and interpret the patterns of interaction within the complex networks. The analysis of three-nodes motifs where the colour of the node reflects its high – white node or low – black node centrality in the social network is presented in the paper. The importance of the vertices is assessed by utilizing two measures: degree prestige and degree centrality. The distribution of motifs in these two cases is compared to mine the interconnection patterns between nodes. The analysis is performed on the social network derived from email communication.	centrality;complex network;email;interaction;interconnection;sequence motif;social network	Katarzyna Musial;Krzysztof Juszczyszyn;Bogdan Gabrys;Przemyslaw Kazienko	2008		10.1007/978-3-642-03040-6_74	network science;gene regulatory network;biological network;weighted network;network formation;computer science;bioinformatics;dynamic network analysis;network motif;machine learning;data mining;complex network;metabolic network;social network;food web	ML	-16.323290049554792	-40.350335047287174	113426
57052e77022f00c254e154f7c7f06f21e80eeba8	small-world topology of functional connectivity in randomly connected dynamical systems	graph theory;nonlinear dynamical systems;small world networks;grupo de excelencia;ciencias basicas y experimentales;matematicas	Characterization of real-world complex systems increasingly involves the study of their topological structure using graph theory. Among global network properties, small-world property, consisting in existence of relatively short paths together with high clustering of the network, is one of the most discussed and studied. When dealing with coupled dynamical systems, links among units of the system are commonly quantified by a measure of pairwise statistical dependence of observed time series (functional connectivity). We argue that the functional connectivity approach leads to upwardly biased estimates of small-world characteristics (with respect to commonly used random graph models) due to partial transitivity of the accepted functional connectivity measures such as the correlation coefficient. In particular, this may lead to observation of small-world characteristics in connectivity graphs estimated from generic randomly connected dynamical systems. The ubiquity and robustness of the phenomenon are documented by an extensive parameter study of its manifestation in a multivariate linear autoregressive process, with discussion of the potential relevance for nonlinear processes and measures.	anatomy, regional;autoregressive model;cluster analysis;coefficient;complex network;complex systems;document completion status - documented;dynamical system;estimated;generic drugs;global network;graph - visual representation;graph theory;interpretation process;nonlinear system;population parameter;random graph;randomness;relevance;resting state fmri;small-world experiment;smallworld;time series;ursidae family;vertex-transitive graph;statistical cluster	Jaroslav Hlinka;David Hartman;Milan Paluš	2012	Chaos	10.1063/1.4732541	algebraic connectivity;combinatorics;discrete mathematics;topology;connectivity;graph theory;mathematics;small-world network	ML	-15.30046830614461	-40.410460523841785	113525
27c31c2af67bbca62df7d5927d8d45ca96f7b6f9	core2vec: a core-preserving feature learning framework for networks		Recent advances in the field of network representation learning are mostly attributed to the application of the skip-gram model in the context of graphs. State-of-the-art analogues of skip-gram model in graphs define a notion of neighbourhood and aim to find the vector representation for a node, which maximizes the likelihood of preserving this neighborhood. In this paper, we take a drastic departure from the existing notion of neighbourhood of a node by utilizing the idea of coreness. More specifically, we utilize the well-established idea that nodes with similar core numbers play equivalent roles in the network and hence induce a novel and an organic notion of neighbourhood. Based on this idea, we propose core2vec, a new algorithmic framework for learning low dimensional continuous feature mapping for a node. Consequently, the nodes having similar core numbers are relatively closer in the vector space that we learn. We further demonstrate the effectiveness of core2vec by comparing word similarity scores obtained by our method where the node representations are drawn from standard word association graphs 11In linguistics, such networks built from various linguistic units are known to have a core-periphery structure (see [3] and the references therein), against scores computed by other state-of-the-art network representation techniques like node2vec, Deep-Walk and LINE. Our results always outperform these existing methods, in some cases achieving improvements as high as 46 % on certain ground-truth word similarity datasets. We make all codes used in this paper available in the public domain: https://github.com/Sam131112/Core2vec_test.	baseline (configuration management);code;core-periphery structure;feature learning;ground truth;map;n-gram;neighbourhood (graph theory);sensor;whole earth 'lectronic link	Soumya Sarkar;Aditya Bhagwat;Animesh Mukherjee	2018	2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1109/ASONAM.2018.8508693	machine learning;public domain;artificial intelligence;computer science;word association;graph;vector space;neighbourhood (mathematics);feature learning	AI	-14.099783312162808	-46.297685251115524	113841
0a5af324c3b44aa7e48036369850d370080c2848	selecting information diffusion models over social networks for behavioral analysis	model selection;learning algorithm;time delay;social network;prediction accuracy;information diffusion;behavior analysis;diffusion model	We investigate how well different information diffusion models can explain observation data by learning their parameters and discuss which model is better suited to which topic. We use two models (AsIC, AsLT), each of which is an extension of the well known Independent Cascade (IC) and Linear Threshold (LT) models and incorporates asynchronous time delay. The model parameters are learned by maximizing the likelihood of observation, and the model selection is performed by choosing the one with better predictive accuracy. We first show by using four real networks that the proposed learning algorithm correctly learns the model parameters both accurately and stably, and the proposed selection method identifies the correct diffusion model from which the data are generated. We next apply these methods to behavioral analysis of topic propagation using the real blog propagation data, and show that although the relative propagation speed of topics that are derived from the learned parameter values is rather insensitive to the model selected, there is a clear indication as to which topic better follows which model. The correspondence between the topic and the model selected is well interpretable.	application-specific integrated circuit;blog;broadcast delay;consistency model;experiment;highly accelerated life test;information theory;linear logic;model selection;selection algorithm;simulation;social network analysis;software propagation;whole earth 'lectronic link	Kazumi Saito;Masahiro Kimura;Kouzou Ohara;Hiroshi Motoda	2010		10.1007/978-3-642-15939-8_12	computer science;artificial intelligence;machine learning;diffusion;data mining;model selection;statistics;social network	ML	-17.378937715828545	-44.8137490619059	113979
56412a6e04bf09b184911e00e729865185de229d	unsupervised discovery of activity correlations using latent topic models	multinomial distribution;gibbs sampling;latent dirichlet allocation;dynamic bayesian network;human action recognition;visual features;pictorial structure;probabilistic latent semantic analysis	Topic models such as probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA) have been successfully used to discover individual activities in a scene. However these methods do not discover group activities which are commonly observed in real life videos of public places. In this paper we address the problem of discovering activities and their associations as a group activity in an unsupervised manner. We propose a method that uses a two layer hierarchical latent structure to correlate individual activities in lower layer with group activity in higher layer. Our model considers each scene to be composed of a mixture of group activities. Each group activity is in turn composed as a mixture of individual activities represented as multinomial distributions. Each individual activity is represented as a distribution over local visual features. We use a Gibbs sampling based algorithm to infer these activities. Our method can summarize not only the individual activities but also the common group activities in a video. We demonstrate the strength of our method by mining activities and the salient correlation amongst them in real life videos of crowded public scenes.	algorithm;gibbs sampling;latent dirichlet allocation;multinomial logistic regression;probabilistic latent semantic analysis;real life;sampling (signal processing);unsupervised learning	Tanveer A. Faruquie;Subhashis Banerjee;Prem Kumar Kalra	2010		10.1145/1924559.1924563	latent class model;latent dirichlet allocation;dynamic topic model;geography;machine learning;pattern recognition;data mining;probabilistic latent semantic analysis	ML	-15.30414444901266	-47.831690349305966	113991
234fbbd65a23ebca6a4c7ea37c0f0574c8dee43e	overlapping community extraction: a link hypergraph partitioning based method	overlapping community extraction;hypergraph social network overlapping community extraction link partitioning local structure;hmetis link hypergraph partitioning based method overlapping community extraction real life social networks local link structure mining pattern mining problem;social network;local structure;hypergraph;link partitioning;social sciences computing data mining graph theory social networking online;communities educational institutions google proteins itemsets partitioning algorithms social network services	Real-world networks often contain communities with pervasive overlaps such that nodes simultaneously belong to several groups. Community extraction, emerging in recent years, is considered to be a promising solution for finding meaningful communities from social networks. In this paper, we explore overlapping community extraction from a link partitioning perspective. First, we define the local link structure composed of a set of closely interrelated links, by extending the similarity of link-pairs to that of a group of links. Second, based upon our prior work, we transform the problem of mining local link structures into a pattern mining problem, and thus present an efficient mining algorithm. Third, we propose to use the hypergraph to assemble all local link structures, and employ hMETIS for hypergraph partitioning. Finally, based on extracted link communities, we restore the membership of nodes in the original graph owing to its links. Experimental results on various real-life social networks validate the effectiveness of the proposed method.	algorithm;data mining;graph partition;pervasive informatics;real life;social network	Haicheng Tao;Zhiang Wu;Jin Shi;Jie Cao;Xiaofeng Yu	2014	2014 IEEE International Conference on Services Computing	10.1109/SCC.2014.25	theoretical computer science;machine learning;data mining;mathematics	Robotics	-14.723838098012278	-42.9255769915802	114157
04447928078b125af5993c8e019c84c77dfce038	a distance measure approach to exploring the rough set boundary region for attribute reduction	occupation time;evaluation performance;text;dk atira pure researchoutput researchoutputtypes contributiontojournal article;fuzzy set;performance evaluation;boundary region;distance measure;time measurement;transparence;dimension reduction;uncertainty;information retrieval;rough set boundary region;rough set theory;evaluacion prestacion;conjunto difuso;useful information;metric;size measurement;ensemble flou;informacion util;data mining set theory runtime time measurement humans data visualization uncertainty size measurement fuzzy sets data models;set theory;data mining;runtime;classification;transparencia;fuzzy set theory;fuzzy sets;information gathering;reduction dimension;approximation theory;attribute reduction;dimensionality reduction;temps occupation;classification accuracy distance measure approach rough set boundary region attribute reduction feature selection dimensionality reduction data visualization data transparency lower approximation dependency value;theorie ensemble approximatif;distance measure approach;data transparency;feature extraction;data visualization;classification rough sets fuzzy sets attribute reduction boundary region;tiempo ocupacion;pattern classification;distance metric;reduccion dimension;rough sets;transparency;metrico;visualisation donnee;feature selection;humans;data reduction;classification accuracy;rough set;dimensional reduction;clasificacion;lower approximation dependency value;metrique;information utile;data models;rough set theory approximation theory data reduction feature extraction fuzzy set theory information retrieval pattern classification	Feature Selection (FS) or Attribute Reduction techniques are employed for dimensionality reduction and aim to select a subset of the original features of a data set which are rich in the most useful information. The benefits of employing FS techniques include improved data visualization and transparency, a reduction in training and utilization times and potentially, improved prediction performance. Many approaches based on rough set theory up to now, have employed the dependency function, which is based on lower approximations as an evaluation step in the FS process. However, by examining only that information which is considered to be certain and ignoring the boundary region, or region of uncertainty, much useful information is lost. This paper examines a rough set FS technique which uses the information gathered from both the lower approximation dependency value and a distance metric which considers the number of objects in the boundary region and the distance of those objects from the lower approximation. The use of this measure in rough set feature selection can result in smaller subset sizes than those obtained using the dependency function alone. This demonstrates that there is much valuable information to be extracted from the boundary region. Experimental results are presented for both crisp and real-valued data and compared with two other FS techniques in terms of subset size, runtimes, and classification accuracy.	approximation;dvd region code;data visualization;dimensionality reduction;feature selection;rough set;runtime system;set theory;transparency (graphic)	Neil MacParthalain;Qiang Shen;Richard Jensen	2010	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2009.119	rough set;computer science;machine learning;pattern recognition;data mining;mathematics;fuzzy set;feature selection;data visualization;statistics	DB	-4.621590061364476	-41.16318967907961	114202
2594ea5f343007917a909b882664988ec677ca57	bacterially inspired evolving system with an application to time series prediction	natural computing;synthetic biology;genetic programming;bio inspired computation	This paper explores the synergies between evolutionary computation and synthetic biology, developing an in silico evolutionary system that is inspired by the behavior of bacterial populations living in continuously changing environments. This system creates a 3D environment seeded with a simulated population of bacteria that eat, reproduce, interact with each other and with the environment and eventually die. This provides a 3D framework implementing an evolutionary process. The subject of the evolution is each bacterium's internal process, defining its interactions with the environment. The evolutionary goal is the survival of the population under successive, continuously changing environmental conditions. The key advantage of this bacterial evolutionary system is its decentralized, asynchronous, parallel and self-adapting general-purpose evolutionary process. We describe this system and present the results of an application to the evolution of a bacterial population that learns how to predict the presence or absence of food in the environment by analyzing three input signals from the environment. The resulting populations successfully evolve by continuously improving their fitness under different environmental conditions, demonstrating their adaptability to a fluctuating medium.	time series	Dolores Barrios Rolanía;José María Font;Daniel Manrique	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.10.012	evolutionary programming;genetic programming;natural computing;interactive evolutionary computation;human-based evolutionary computation;computer science;bioinformatics;artificial intelligence;machine learning;evolutionary algorithm;synthetic biology	HCI	-5.242142086241386	-48.10035337955849	114401
b5da9f8d4941cb5b9c2bd864fb30492bdcc9ea0e	an efficient approach for the generation of allen relations.		Event data is increasingly being represented according to the Linked Data principles. The need for large-scale machine learning on data represented in this format has thus led to the need for efficient approaches to compute RDF links between resources based on their temporal properties. Time-efficient approaches for computing links between RDF resources have been developed over the last years. However, dedicated approaches for linking resources based on temporal relations have been paid little attention to. In this paper, we address this research gap by presenting AEGLE, a novel approach for the efficient computation of links between events according to Allen’s interval algebra. We study Allen’s relations and show that we can reduce all thirteen relations to eight simpler relations. We then present an efficient algorithm with a complexity of O(n logn) for computing these eight relations. Our evaluation of the runtime of our algorithms shows that we outperform the state of the art by up to 4 orders of magnitude while maintaining a precision and a recall of 1.	algorithm;allen's interval algebra;blocking (computing);computation;dynamic problem (algorithms);experiment;incremental computing;linked data;load balancing (computing);machine learning;scalability;sorting	Kleanthi Georgala;Mohamed Ahmed Sherif;Axel-Cyrille Ngonga Ngomo	2016		10.3233/978-1-61499-672-9-948	artificial intelligence;computer science;machine learning	AI	-9.339159982238733	-38.19577735145206	114561
9146857bf19832369554468e9cd53628748b0191	detecting communities in social networks by local affinity propagation with grey relational analysis	community detection;grey relational analysis;social network	Purpose – The purpose of this paper is to discover social communities from the social networks by propagating affinity messages among members in a localized way. The affinity between any two members is computed by grey relational analysis method. Design/methodology/approach – First, the responsibility messages and the availability messages are restricted to be broadcasted only among a node and its neighbours, i.e. the nodes that connected to it directly. In this way, both the time complexity and the space complexity can be reduced to be near linear to the network size. The near-linear time and space complexity is quite important for social network analysis because social networks are generally very large. Second, instead of the widely used Euclidean distance, the grey relational degree is adopted in the calculation of node similarity, because the latter is more suitable for the discovery of the hidden relations among the nodes. On the basis of the two improvements, a new social community detection algorit...	affinity analysis;affinity propagation;grey relational analysis;sensor;social network;software propagation	Kun Guo;Qishan Zhang	2015	Grey Systems: T&A	10.1108/GS-11-2014-0039	artificial intelligence;machine learning;data mining;mathematics	Vision	-14.953774116436444	-43.14267813785656	114993
32e0813fa8030061d751809f2f3f20046f351278	a modeling approach using multiple graphs for semi-supervised learning	semi supervised learning;classification accuracy	Most graph-based semi-supervised learning methods model the structure of a dataset as a single  k -NN graph. Although graph construction is an important task, many existing graph-based methods build a graph from a dataset directly and naively. While the resulting  k -NN graph provides relatively a good representation of the dataset,it generally produces inappropriate shortcuts on cluster boundaries. In this paper, we propose a novel approach for modeling and combining multiple graphs with different edge weights to avoid such undesirable behavior. Using the combination of those graphs, we can systematically reduce the effect of noise in conceptually similar fashion to an ensemble approach. Experimental results demonstrate that our approach improves classification accuracy on both benchmark and artificial datasets.	semi-supervised learning;semiconductor industry;supervised learning	Akihiko Izutani;Kuniaki Uehara	2008		10.1007/978-3-540-88411-8_28	null model;computer science;machine learning;pattern recognition;data mining	ML	-13.386165521556848	-47.36456575670789	115220
fd04fb605a97f3e13d0fd14d6a42736e4570398e	leadersrank: towards a new approach for community detection in social networks	measurement;image edge detection;lead;three dimensional displays;complex networks leader node graph theory community detection social network centrality clustering classification influential nodes;social networking online complex networks graph theory information dissemination;contagious disease detection leadersrank community detection social networks leader nodes information dissemination information spreading infectious diseases product exposure;buildings;lead three dimensional displays image edge detection buildings measurement	Social networks play an important role in the dissemination of information and the spread of influence. Identifying the most influential individuals spreading information or infectious diseases can assist or hinder information dissemination, product exposure, and contagious disease detection. The leader or influential members may be even more critical to product diffusion and the formation of widespread contagions. This paper proposes a new algorithm LeadersRank, which use the network structure to identify influential members, i.e. the leaders' nodes, for community detection in social networks, without a priori knowledge of k number of communities in the network.	algorithm;social network	Sara Ahajjam;Mohamed El Haddad;Hassan Badir	2015	2015 IEEE/ACS 12th International Conference of Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2015.7507215	lead;computer security;measurement	DB	-15.883552309817965	-42.527376120296246	115505
308760971f996b378703011eb2465f302e36c15b	xtile: an error-correction package for dna self-assembly	self assembly;wang tile;coding theory;error correction;information processing;error control;molecular computing;dna computing;compact scheme	Self-assembly is a process by which supramolecular species form spontaneously from their components. This process is ubiquitous throughout the life chemistry and is central to biological information processing. It has been predicted that in future self-assembly will become an important engineering discipline by combining the fields of bio-molecular computation, nano-technology and medicine. However error-control is a key challenge in realizing the potential of self-assembly. Recently many authors have proposed several combinatorial error correction schemes to control errors which have a close analogy with the coding theory such as Winfree’s proofreading scheme and its generalizations by Chen and Goel and compact scheme of Reif, Sahu and Yin. In this work, we present an error-correction computational tool XTile that can be used to create input files to the Xgrow simulator of Winfree by providing the design logic of the tiles and it also allows the user to apply proofreading, snake and compact error correction schemes.	british informatics olympiad;coding theory;computation;entity–relationship model;error detection and correction;gnu nano;information processing;john reif;self-assembly;simulation	Anshul Chaurasia;Sudhanshu Dwivedi;Prateek Jain;Manish K. Gupta	2009	CoRR		error detection and correction;information processing;telecommunications;wang tile;computer science;theoretical computer science;mathematics;dna computing;algorithm;statistics;coding theory		-8.01215270018666	-48.161146255331786	115683
b596542e8529e8ec1e6580cdbd29965ae775cf7c	current research in artificial intelligence and molecular biology	artificial intelligent;molecular biology	Molecular biology has enjoyed phenomenal success since the 1950s both as an experimental and theoretical science. One manifestation of this is that the ability to generate low level data such as DNA sequences has accelerated beyond all expectations in the last decade or so. Part of this success has led to the instigation of the Human Genome Project (HGP) whose aim is to sequence the entire human genome by the year 2000, thereby creating the opportunity for major advances in our understanding of the molecular and genetic basis of disease. Overall, the complexity of biological systems and the rapid expansion of the amount of data makes molecular biology an area of unparalleled challenge and opportunity for AI and informatics in general.	artificial intelligence;biological system;informatics	Dominic A. Clark;Christopher J. Rawlings;Simon Parsons	1993	Knowledge Eng. Review	10.1017/S0269888900000345	computational biology;computer science;artificial intelligence	Comp.	-6.903298857310866	-48.36399421063408	115753
d07129daf961384d7c1de0aeacbba5043cdf75ea	systems for big graph analytics		Due to the growing need of analyzing big graphs such as online social networks and web graphs, recent years have witnessed a surge in developing distributed graph analytics systems. Among these systems, the think-like-a-vertex computation framework pioneered by Google’s Pregel gains particular interest. In Pregel, a user only needs to specify the behavior of one generic vertex when developing parallel graph algorithms. This talk introduces the latest development in the field of big graph analytics systems. It starts with a brief review on Pregel, followed by an introduction on how to develop Pregel algorithms for various graph problems with performance guarantees. We then proceed to identify the weaknesses of the existing Pregel-like systems, and introduce a few novel ideas and designs in improving the basic model of Pregel. These techniques often provide orders of magnitude performance improvements. This talk will cover important topics including computation model, communication mechanism, ondemand querying, out-of-core support, fault tolerance, etc. Many of the introduced works have been published in first-tier conferences, including four recent VLDB papers and one recent WWW paper. The talk will also provide a vision on future research directions on big graph analytics.	fault tolerance;list of algorithms;model of computation;multitier architecture;out-of-core algorithm;social network;vldb;www	Da Yan;Yuanyuan Tian;James Cheng	2017		10.1007/978-3-319-58217-7	analytics;software analytics;semantic analytics	DB	-10.815846799993404	-39.060308327784035	115898
8f9ca607568a0782fab9568c8255158e7c962e1b	uncovering relations between traffic classifiers and anomaly detectors via graph theory	benchmarking;graph theory;anomaly detection;common ground;general methods;graph community detection;ground truth;cross validation;internet traffic classification	Network traffic classification and anomaly detection have received much attention in the last few years. However, due to the the lack of common ground truth, proposed methods are evaluated through diverse processes that are usually neither comparable nor reproducible. Our final goal is to provide a common dataset with associated ground truth resulting from the cross-validation of various algorithms. This paper deals with one of the substantial issues faced in achieving this ambitious goal: relating outputs from various algorithms. We propose a general methodology based on graph theory that relates outputs from diverse algorithms by taking into account all reported information. We validate our method by comparing results of two anomaly detectors which report traffic at different granularities. The proposed method succesfully identified similarities between the outputs of the two anomaly detectors although they report distinct features of the traffic.	abstraction layer;algorithm;anomaly detection;coherence (physics);cross-validation (statistics);graph (discrete mathematics);graph theory;ground truth;internet backbone;sensor;traffic classification	Romain Fontugne;Pierre Borgnat;Patrice Abry;Kensuke Fukuda	2010		10.1007/978-3-642-12365-8_8	anomaly detection;ground truth;computer science;artificial intelligence;graph theory;machine learning;data mining;computer security;cross-validation;benchmarking	Metrics	-13.41522548701923	-45.24143202598178	116118
678beea182875013076ef4daf274503f79e9c23a	graphscape: integrated multivariate network visualization	graph theory;graphscape method;pattern clustering;2d plane;graph clustering;network visualization;computational geometry;proteins data visualization clustering algorithms social network services shape computer networks computer graphics application software algorithm design and analysis;layout algorithm;pattern clustering computational geometry data visualisation graph theory network theory graphs;layout algorithm graphscape method integrated multivariate network visualization 2d plane multivariate graph clustering;data visualisation;surface model;multivariate graph clustering;network structure;network theory graphs;multivariate visualization;graph visualization;integrated multivariate network visualization;multivariate data	In this paper, we introduce a new method, GraphScape, to visualize multivariate networks, i.e., graphs with multivariate data associated with their nodes. GraphScape adopts a landscape metaphor with network structure displayed on a 2D plane and the surface height in the third dimension represents node attribute. More than one attribute can be visualized simultaneously by using multiple surfaces. In addition, GraphScape can be easily combined with existing methods to further increase the total number of attributes visualized. One of the major goals of GraphScape is to reveal multivariate graph clustering, which is based on both network structure and node attributes. This is achieved by a new layout algorithm and an innovative way of constructing attribute surface, which also allows visual clustering at different scales through interaction. A simplified attribute surface model is also proposed to reduce computation requirement when visualizing large networks. GraphScape is applied to networks of three different size (20, 100, and 1500) to demonstrate its effectiveness.	algorithm;cluster analysis;computation;force-directed graph drawing;response time (technology)	Kai Xu;Andrew Cunningham;Seok-Hee Hong;Bruce H. Thomas	2007	2007 6th International Asia-Pacific Symposium on Visualization	10.1109/APVIS.2007.329306	attribute domain;computer science;theoretical computer science;machine learning;data mining	Visualization	-13.71530352715685	-38.85903504377667	116169
adc5bf04240b0eb06002152ce2bfd05ba10bf90d	efficient processing of shortest path queries in evolving graph sequences		Abstract In many applications, information is best represented as graphs. In a dynamic world, information changes and so the graphs representing the information evolve with time. We propose that historical graph-structured data be maintained for analytical processing. We call a historical evolving graph sequence an EGS. We observe that in many applications, graphs of an EGS are large and numerous, and they often exhibit much redundancy among them. We study the problem of efficient shortest path query processing on an EGS and put forward a solution framework called FVF. Two algorithms, namely, FVF-F and FVF-H, are proposed. While the FVF-F algorithm works on a sequence of flat graph clusters, the FVF-H algorithm works on a hierarchy of such clusters. Through extensive experiments on both real and synthetic datasets, we show that our FVF framework is highly efficient in shortest query processing on EGSs. Comparing FVF-F and FVF-H, the latter gives a larger speedup, is more flexible in terms of memory requirements, and is far less sensitive to parameter values.	shortest path problem	Chenghui Ren;Eric Lo;Ben Kao;Xinjie Zhu;Reynold Cheng;David Wai-Lok Cheung	2017	Inf. Syst.	10.1016/j.is.2017.05.004	redundancy (engineering);database;data mining;longest path problem;theoretical computer science;computer science;shortest path faster algorithm;hierarchy;speedup;k shortest path routing;shortest path problem;graph	DB	-8.21051422086512	-38.28075341015841	116310
4574e9facfb43a14a1d986ad3661a1143e21a358	a spline-based cost model for metric trees		Abstract. Whenever two (or more) access methods are alternatives for the execution of a query, how to choose which one is the best for the task? Such a decision is made by the DBMS optimizer module, which models the query costs according to the distribution of the data space. Cost modeling of similarity searches, however, requires the representation of distances’ rather than data distribution. In this paper, we propose the Stockpile model for cost estimation of similarity queries on metric trees by using pivot-based distance histograms that represent the local densities around the query elements. By combining the local densities to the probability of traversing the tree nodes, Stockpile provides a fair estimation of both disk accesses (I/O costs) and distance calculations (CPU costs). We compared Stockpile and two literature models regarding similarity queries in real-world data sources and our model was up to 85% more precise than the competitors.	analysis of algorithms;central processing unit;dataspaces;distance matrix;input/output;locality of reference;mathematical optimization;metric tree;semantic similarity;spline (mathematics)	Marcos V. N. Bedo;Agma J. M. Traina;Caetano Traina	2017			data mining;computer science;spline (mathematics)	DB	-5.65805763485306	-41.62081293236779	116342
32fc17245616dc4e20e69160ad19ef2a39199568	alleviating the sparsity problem of collaborative filtering using trust inferences	modelizacion;reseau social;tratamiento transaccion;confiance;psychologie sociale;incertidumbre;information source;uncertainty;source information;securite informatique;recommandation;computer security;modelisation;trusted computing;social network;confidence;fichier log;transaction data;fichero actividad;confianza;collaborative filtering;seguridad informatica;inferencia;psicologia social;representacion parsimoniosa;recomendacion;recommendation;social psychology;incertitude;transaction processing;sparse representation;modeling;red social;inference;log file;traitement transaction;fuente informacion;representation parcimonieuse	Collaborative Filtering (CF), the prevalent recommendation approach, has been successfully used to identify users that can be characterized as “similar” according to their logged history of prior transactions. However, the applicability of CF is limited due to the sparsity problem, which refers to a situation that transactional data are lacking or are insufficient. In an attempt to provide high-quality recommendations even when data are sparse, we propose a method for alleviating sparsity using trust inferences. Trust inferences are transitive associations between users in the context of an underlying social network and are valuable sources of additional information that help dealing with the sparsity and the cold-start problems. A trust computational model has been developed that permits to define the subjective notion of trust by applying confidence and uncertainty properties to network associations. We compare our method with the classic CF that does not consider any transitive associations. Our experimental results indicate that our method of trust inferences significantly improves the quality performance of the classic CF method.	algorithm;cold start;collaborative filtering;computation;computational model;computational trust;dynamic data;experiment;horner's method;recommender system;social network;sparse matrix;transitive dependency;trust management (information system)	Manos Papagelis;Dimitris Plexousakis;Themistoklis Kutsuras	2005		10.1007/11429760_16	systems modeling;uncertainty;transaction processing;computer science;artificial intelligence;collaborative filtering;machine learning;transaction data;sparse approximation;data mining;database;distributed computing;confidence;trustworthy computing;computer security;algorithm;statistics;social network	AI	-15.639873522653808	-49.38149139723775	116584
41a6d0e4499c78727f3ed95bdf3cbf470d0219a7	collaborative recommendation with multiclass preference context	matrix factorization;collaboration;prediction algorithms;collaborative recommendation;training data;intelligent systems;multiclass preference context;mathematical model;predictive models;context modeling;context;data models	Factorization- and neighborhood-based methods have been recognized as state-of-the-art approaches for collaborative recommendation tasks. In this article, the authors take user ratings as categorical multiclass preferences and propose a novel method called matrix factorization with multiclass preference context (MF-MPC), which integrates an enhanced neighborhood based on the assumption that users with similar past multiclass preferences (instead of one-class preferences in SVD++) will have similar tastes in the future. The main merit of MF-MPC is its ability to make use of the multiclass preference context in the factorization framework in a fine-grained manner and thus inherit the advantages of those two methods. Experimental results on three real-world datasets show that their solution can perform significantly better than factorization-based methods, neighborhood-based methods, and integrated methods with a one-class preference context.		Weike Pan;Zhong Ming	2017	IEEE Intelligent Systems	10.1109/MIS.2017.30	data modeling;training set;prediction;intelligent decision support system;computer science;machine learning;pattern recognition;mathematical model;data mining;predictive modelling;context model;matrix decomposition;collaboration	AI	-18.5365899435012	-48.48767617937185	116732
2ca1b75ba1a36d89c5322d36a690244e4fc024b1	on k-nearest neighbor searching in non-ordered discrete data spaces	database indexing;dna;genomics;sequences;electronic commerce;k nearest neighbor searching;discrete data;query processing;distance measure;numerical technique;information retrieval;granularity enhanced hamming distance;k nearest neighbor query retrieval;multidimensional database;hamming distance;indexing;multidimensional database indexing k nearest neighbor searching k nearest neighbor query retrieval nonordered discrete data space coarse granularity granularity enhanced hamming distance;multidimensional database indexing;indexation;hamming distance bioinformatics indexing genomics sequences information retrieval multidimensional systems multimedia databases dna electronic commerce;multimedia databases;k nearest neighbor;nonordered discrete data space;multidimensional systems;coarse granularity;bioinformatics;query processing database indexing	A k-nearest neighbor (k-NN) query retrieves k objects from a database that are considered to be the closest to a given query point. Numerous techniques have been proposed in the past for supporting efficient k-NN searches in continuous data spaces. No such work has been reported in the literature for k-NN searches in a non-ordered discrete data space (NDDS). Performing k-NN searches in an NDDS raises new challenges. The Hamming distance is usually used to measure the distance between two vectors (objects) in an NDDS. Due to the coarse granularity of the Hamming distance, a k-NN query in an NDDS may lead to a large set of candidate solutions, creating a high degree of non-determinism for the query result. We propose a new distance measure, called granularity-enhanced Hamming (GEH) distance, that effectively reduces the number of candidate solutions for a query. We have also considered using multidimensional database indexing for implementing k-NN searches in NDDSs. Our experiments on synthetic and genomic data sets demonstrate that our index-based k-NN algorithm is effective and efficient in finding k-NNs in NDDSs.	dataspaces;discrete mathematics;experiment;hamming distance;k-nearest neighbors algorithm;online analytical processing;spaces;synthetic intelligence;window function	Dashiell Kolbe;Qiang Zhu;Sakti Pramanik	2007	2007 IEEE 23rd International Conference on Data Engineering	10.1109/ICDE.2007.367888	database index;search engine indexing;genomics;hamming distance;multidimensional systems;computer science;data mining;sequence;database;k-nearest neighbors algorithm;dna;information retrieval	DB	-5.653035174596716	-40.84123498115078	116871
5efb2cd71f9161aff8bf94543d23f697c6055878	community detection in temporal multilayer networks, with an application to correlation networks	94c15;91c20;modularity maximization;62h30;community structure;90c35;financial correlation networks;temporal networks;multilayer networks	Networks are a convenient way to represent complex systems of interacting entities. Many networks contain “communities” of nodes that are more densely connected to each other than to nodes in the rest of the network. In this paper, we investigate the detection of communities in temporal networks represented as multilayer networks. As a focal example, we study time-dependent financialasset correlation networks. We first argue that the use of the “modularity” quality function—which is defined by comparing edge weights in an observed network to expected edge weights in a “null network”—is application-dependent. We differentiate between “null networks” and “null models” in our discussion of modularity maximization, and we highlight that the same null network can correspond to different null models. We then investigate a multilayer modularity-maximization problem to identify communities in temporal networks. Our multilayer analysis only depends on the form of the maximization problem and not on the specific quality function that one chooses. We introduce a diagnostic to measure persistence of community structure in a multilayer network partition. We prove several results that describe how the multilayer maximization problem measures a trade-off between static community structure within layers and larger values of persistence across layers. We also discuss some computational issues that the popular “Louvain” heuristic faces with temporal multilayer networks and suggest ways to mitigate them.	adjacency matrix;apollonian network;authorization;boundary case;cluster analysis;command & conquer:yuri's revenge;complex systems;dual ec drbg;entity;entropy maximization;expectation–maximization algorithm;experiment;focal (programming language);heuristic (computer science);interaction;local optimum;louvain modularity;modularity (networks);nl (complexity);network partition;null model;numerical analysis;numerical aperture;ordinal data;persistence (computer science);ut-vpn	Marya Bazzi;Mason A. Porter;Stacy Williams;Mark McDonald;Daniel J. Fenn;Sam D. Howison	2016	Multiscale Modeling & Simulation	10.1137/15M1009615	combinatorics;evolving networks;network motif;modularity;machine learning;data mining;mathematics;clique percolation method;interdependent networks;community structure;complex network;statistics	AI	-15.94886793851606	-40.55705294411121	117042
9b0ce9f51dc6b6ed3cb07b276fac70d41e992d58	improving collaborative filtering recommendations using external data	aggregates recommender systems information filtering information filters information analysis international collaboration motion pictures constraint theory collaborative work taxonomy;external ratings recommender systems collaborative filtering predictive models olap ratings;large dataset;heuristic method;information filtering;collaboration;model based approach;data mining;statistical properties;recommender system;estimation;collaborative filtering;aggregates;mathematical model;external ratings;predictive models;prediction model;olap ratings;estimation procedure collaborative filtering recommendations heuristic item based collaborative filtering model based collaborative filtering datasets aggregate rating information;recommender systems;noise;covariance matrix	This paper describes an approach for incorporating externally specified aggregate ratings information into certain types of collaborative filtering (CF) methods. For a statistical model-based CF approach, we formally showed that this additional aggregated information provides more accurate recommendations of individual items to individual users. Furthermore, theoretical insights gained from the analysis of this model-based method suggested a way to incorporate aggregate information into the heuristic item-based CF method. Both the model-based and the heuristic item-based CF methods were empirically tested on several datasets, and the experiments uniformly confirmed that the aggregate rating information indeed improves CF recommendations. These results also show the power of theory by demonstrating how the insights gained from theoretical developments can shed light on proper selection of good heuristic methods. We also showed the way to introduce scalability and parallelization into the estimation procedure and reported the running time for steps of the estimation procedure for large datasets.	aggregate data;bottom-up proteomics;collaborative filtering;experiment;heuristic;image scaling;netflix prize;online analytical processing;parallel computing;recommender system;scalability;sparse matrix;statistical model;time complexity;top-down and bottom-up design	Akhmed Umyarov;Alexander Tuzhilin	2008	2008 Eighth IEEE International Conference on Data Mining	10.1109/ICDM.2008.44	computer science;data science;machine learning;data mining;predictive modelling;information retrieval;statistics;recommender system	DB	-17.29840448203977	-48.535664857947296	117488
64e50feab6fc25ffcba5bc74b2bd3cad111fb938	nodes' evolution diversity and link prediction in social networks		Recently, social networks have witnessed a massive surge in popularity. A key issue in social network research is network evolution analysis, which assumes that all the autonomous nodes in a social network follow uniform evolution mechanisms. However, different nodes in a social network should have different evolution mechanisms to generate different edges. This is proposed as the underlying idea to ensure the nodes’ evolution diversity in this paper. Our approach involves identifying the micro-level node evolution that generates different edges by introducing the existing link prediction methods from the perspectives of nodes. We also propose the edge generation coefficient to evaluate the extent to which an edge's generation can be explained by a link prediction method. To quantify the nodes’ evolution diversity, we define the diverse evolution distance. Furthermore, a diverse node adaption algorithm is proposed to indirectly analyze the evolution of the entire network based on the nodes’ evolution diversity. Extensive experiments on disparate real-world networks demonstrate that the introduction of the nodes’ evolution diversity is important and beneficial for analyzing the network evolution. The diverse node adaption algorithm outperforms other state-of-the-art link prediction algorithms in terms of both accuracy and universality. The greater the nodes’ evolution diversity, the more obvious its advantages.	algorithm;autonomous robot;coefficient;evolution;experiment;graph (discrete mathematics);scinet consortium;slashdot;social network;universal turing machine;weighted network	Huan Wang;Wenbin Hu;Zhenyu Qiu;Bo Du	2017	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2017.2728527	cultural diversity;artificial intelligence;computer science;machine learning;fitness model;algorithm design;dynamic network analysis;social network;popularity	AI	-17.019730626789553	-42.02822913078021	117995
1e4c1d76313af41932ec3eae475b209ffbc6f598	reconciling long-term cultural diversity and short-term collective social behavior	dna;self assembly;soft matter;nanoparticles;time scale;theoretical model;human interaction;empirical distribution;high dimensionality;models theoretical;large dataset;cultural diversity;hard matter;information network;social ties;collective behavior;colloids;social support;social network;public opinion;large scale;piero;time factors;phase transition;social behavior;long term culture;symmetry breaking;baglioni;csgi;social influence;university;humans;long range;questionnaires;gel;hierarchy social;databases factual;europe;florence;surfactants;gels;microemulsion;nanorestore;microemulsions	An outstanding open problem is whether collective social phenomena occurring over short timescales can systematically reduce cultural heterogeneity in the long run, and whether offline and online human interactions contribute differently to the process. Theoretical models suggest that short-term collective behavior and long-term cultural diversity are mutually excluding, since they require very different levels of social influence. The latter jointly depends on two factors: the topology of the underlying social network and the overlap between individuals in multidimensional cultural space. However, while the empirical properties of social networks are intensively studied, little is known about the large-scale organization of real societies in cultural space, so that random input specifications are necessarily used in models. Here we use a large dataset to perform a high-dimensional analysis of the scientific beliefs of thousands of Europeans. We find that interopinion correlations determine a nontrivial ultrametric hierarchy of individuals in cultural space. When empirical data are used as inputs in models, ultrametricity has strong and counterintuitive effects. On short timescales, it facilitates a symmetry-breaking phase transition triggering coordinated social behavior. On long timescales, it suppresses cultural convergence by restricting it within disjoint groups. Moreover, ultrametricity implies that these results are surprisingly robust to modifications of the dynamical rules considered. Thus the empirical distribution of individuals in cultural space appears to systematically optimize the coexistence of short-term collective behavior and long-term cultural diversity, which can be realized simultaneously for the same moderate level of mutual influence in a diverse range of online and offline settings.	anatomy, regional;coexist (image);collective intelligence;convergence (action);interaction;online and offline;phase transition;rule (guideline);silo (dataset);social network;societies;specification	Luca Valori;Francesco Picciolo;Agnes Allansdottir;Diego Garlaschelli	2012	Proceedings of the National Academy of Sciences of the United States of America	10.1073/pnas.1109514109	soft matter;microemulsion	ML	-16.790198379803574	-39.700005485873135	118366
1ba7863d514c3f05268954253c17a74587b601bf	rws-diff: flexible and efficient change detection in hierarchical data	change detection;tree edit distance;hierarchical data;tree edit script;random walk;xml;tree diff;similarity search	The problem of generating a cost-minimal edit script between two trees has many important applications. However, finding such a cost-minimal script is computationally hard, thus the only methods that scale are approximate ones. Various approximate solutions have been proposed recently. However, most of them still show quadratic or worse runtime complexity in the tree size and thus do not scale well either. The only solutions with log-linear runtime complexity use simple matching algorithms that only find corresponding subtrees as long as these subtrees are equal. Consequently, such solutions are not robust at all, since small changes in the leaves which occur frequently can make all subtrees that contain the changed leaves unequal and thus prevent the matching of large portions of the trees. This problem could be avoided by searching for similar instead of equal subtrees but current similarity approaches are too costly and thus also show quadratic complexity. Hence, currently no robust log-linear method exists.  We propose the random walks similarity (RWS) measure which can be used to find similar subtrees rapidly. We use this measure to build the RWS-Diff algorithm that is able to compute an approximately cost-minimal edit script in log-linear time while having the robustness of a similarity-based approach. Our evaluation reveals that random walk similarity indeed increases edit script quality and robustness drastically while still maintaining a runtime comparable to simple matching approaches.	approximation algorithm;hierarchical database model;log-linear model;robustness (computer science);time complexity;tree (data structure)	Jan Finis;Martin Raiber;Nikolaus Augsten;Robert Brunel;Alfons Kemper;Franz Färber	2013		10.1145/2505515.2505763	xml;computer science;machine learning;pattern recognition;data mining;database;search tree;change detection;random walk;hierarchical database model;statistics	Web+IR	-7.161749728194258	-41.402275610243144	118952
1e39ae897cb36103c2848c64c809a0fcdbfd114f	invited talk: the many wonders of the web graph	web graph;web pages;rate of change;information retrieval;web spam	The Web graph, meaning the graph induced by Web pages as nodes and their hyperlinks as directed edges, has become a fascinating object of study for many people: physicists, sociologists, mathematicians, computer scientists, and information retrieval specialists.#R##N##R##N#Recent results range from theoretical (e.g.: models for the graph, semi-external algorithms), to experimental (e.g.: new insights regarding the rate of change of pages, new data on the distribution of degrees), to practical (e.g.: improvements in crawling technology, uses in information retrieval, web spam prevention).#R##N##R##N#The goal of this talk is to convey an introduction to the state of the art in this area and to sketch the current issues in collecting, representing, analyzing, and modeling this graph.	webgraph;world wide web	Andrei Z. Broder	2004		10.1007/11527954_14	web mining;web development;web modeling;web mapping;web design;web search engine;computer science;web navigation;web page;semantic web stack;multimedia;web intelligence;web 2.0;world wide web;graph database;information retrieval	Theory	-18.425576483621033	-40.947349963504344	119143
645d35c8913adcd3540b1efc3af252968384f532	holistic influence maximization: combining scalability and efficiency with opinion-aware models	influence maximization;efficiency;opinion;social networks;greedy algorithm;scalability;viral marketing	The steady growth of graph data from social networks has resulted in wide-spread research in finding solutions to the influence maximization problem. In this paper, we propose a holistic solution to the influence maximization (IM) problem. (1) We introduce an opinion-cum-interaction (OI) model that closely mirrors the real-world scenarios. Under the OI model, we introduce a novel problem of Maximizing the Effective Opinion (MEO) of influenced users. We prove that the MEO problem is NP-hard and cannot be approximated within a constant ratio unless P=NP. (2) We propose a heuristic algorithm OSIM to efficiently solve the MEO problem. To better explain the OSIM heuristic, we first introduce EaSyIM - the opinion-oblivious version of OSIM, a scalable algorithm capable of running within practical compute times on commodity hardware. In addition to serving as a fundamental building block for OSIM, EaSyIM is capable of addressing the scalability aspect - memory consumption and running time, of the IM problem as well. Empirically, our algorithms are capable of maintaining the deviation in the spread always within 5% of the best known methods in the literature. In addition, our experiments show that both OSIM and EaSyIM are effective, efficient, scalable and significantly enhance the ability to analyze real datasets.	approximation algorithm;commodity computing;entropy maximization;expectation–maximization algorithm;experiment;heuristic (computer science);holism;np-hardness;p versus np problem;scalability;social network;time complexity	Sainyam Galhotra;Akhil Arora;Shourya Roy	2016		10.1145/2882903.2882929	mathematical optimization;greedy algorithm;scalability;social science;simulation;computer science;machine learning;opinion;viral marketing;efficiency;social network	DB	-16.3999724594306	-44.1417199739666	119186
351c8d072d6ff01dfb3096793455bddcabe897b7	probabilistic logic methods and some applications to biology and medicine	probabilistic graphical models;hierarchical bayesian networks;probabilistic boolean networks;machine learning;propositional logic;markov logic networks;first order logic;bayesian networks	For the computational analysis of biological problems-analyzing data, inferring networks and complex models, and estimating model parameters-it is common to use a range of methods based on probabilistic logic constructions, sometimes collectively called machine learning methods. Probabilistic modeling methods such as Bayesian Networks (BN) fall into this class, as do Hierarchical Bayesian Networks (HBN), Probabilistic Boolean Networks (PBN), Hidden Markov Models (HMM), and Markov Logic Networks (MLN). In this review, we describe the most general of these (MLN), and show how the above-mentioned methods are related to MLN and one another by the imposition of constraints and restrictions. This approach allows us to illustrate a broad landscape of constructions and methods, and describe some of the attendant strengths, weaknesses, and constraints of many of these methods. We then provide some examples of their applications to problems in biology and medicine, with an emphasis on genetics. The key concepts needed to picture this landscape of methods are the ideas of probabilistic graphical models, the structures of the graphs, and the scope of the logical language repertoire used (from First-Order Logic [FOL] to Boolean logic.) These concepts are interlinked and together define the nature of each of the probabilistic logic methods. Finally, we discuss the initial applications of MLN to genetics, show the relationship to less general methods like BN, and then mention several examples where such methods could be effective in new applications to specific biological and medical problems.	6-bromo-2-naphthyl sulfate;algorithm;artificial neural network;bayesian network;boolean algebra;boolean network;complex systems;computation;computational biology;estimated;first-order logic;first-order predicate;fuzzy logic;graph - visual representation;graphical model;hidden markov model;inference;logical connective;machine learning;markov chain;markov logic network;mathematics;mean squared error;mucocutaneous lymph node syndrome;neural network simulation;parallel computing;rigor - temperature-associated observation;score;weakness;executing - querystatuscode	Nikita A. Sakhanenko;David J. Galas	2012	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2011.0234	probabilistic ctl;variable-order bayesian network;bayesian programming;probabilistic relevance model;computer science;artificial intelligence;machine learning;bayesian network;first-order logic;data mining;graphical model;propositional calculus;probabilistic logic;probabilistic argumentation;probabilistic logic network;algorithm;statistics;subjective logic	ML	-6.474588987167083	-52.06093172095495	119308
28f7f4f7bc518aeeb5c060dfe89ff402e51b6f8f	influential node detection in implicit social networks using multi-task gaussian copula models	mathematics;computing;and information science	Influential node detection is a central research topic in social network analysis. Many existing methods rely on the assumption that the network structure is completely known a priori. However, in many applications, network structure is unavailable to explain the underlying information diffusion phenomenon. To address the challenge of information diffusion analysis with incomplete knowledge of network structure, we develop a multi-task low rank linear influence model. By exploiting the relationships between contagions, our approach can simultaneously predict the volume (i.e. time series prediction) for each contagion (or topic) and automatically identify the most influential nodes for each contagion. The proposed model is validated using synthetic data and an ISIS twitter dataset. In addition to improving the volume prediction performance significantly, we show that the proposed approach can reliably infer the most influential users for specific contagions.	algorithm;computer multitasking;isis;social network analysis;synthetic data;synthetic intelligence;time series	Qunwei Li;Bhavya Kailkhura;Jayaraman J. Thiagarajan;Zhenliang Zhang;Pramod K. Varshney	2016			computing;computer science;data science;machine learning;data mining;world wide web;statistics	ML	-18.619206183214892	-43.8493467806024	119527
efb20fb4615a6405ef621de7064d3e0fe2f8acc8	overlapping community detection in heterogeneous social networks via the user model		Abstract Clustering users with more common interests who interact frequently on social networking sites has attracted much attention from researchers due to the high economic value and further application prospects. Community detection is a widely accepted means of dealing with the challenge of clustering users, but conventional methods are inadequate since there are billions of vertices and various relations in social media. Through the user model, a heterogeneous network containing both undirected and directed edges is built in this study to exactly simulate a social network. A novel approach for overlapping community detection in a heterogeneous social network (OCD-HSN) is proposed, which contains seed selecting and community initializing and expanding to accurately and efficiently unfold modules in parallel. Experimental results on artificial and real-world social networks demonstrate the higher accuracy and lower time consumption of the proposed scheme compared with other existing state-of-the-art algorithms.	social network	Mingqing Huang;Guobing Zou;Bofeng Zhang;Yue Liu;Yajun Gu;Keyuan Jiang	2018	Inf. Sci.	10.1016/j.ins.2017.11.055	user modeling;artificial intelligence;mathematics;machine learning;cluster analysis;initialization;heterogeneous network;social network;social media	AI	-15.756722088515177	-44.59061950700284	119763
4a8d97172382144b9906e2cec69d3decb4188fb7	taming evasions in machine learning based detection pipelines	computer science taming evasions in machine learning based detection pipelines university of california berkeley anthony d joseph;j d tygar kantchelian alex;computer science	This thesis presents and evaluates three mitigation techniques for evasion attacks against machine learning based detection pipelines. Machine learning based detection pipelines provide much of the security in modern computerized system. For instance, these pipelines are responsible for the detection of undesirable content on computing platforms and Internet-based services, such as malicious software and email spam. By its adversarial nature, the security application domain exhibits a permanent arms race between attackers who aim to avoid, or evade, detection and the pipelineu0027s maintainers whose aim is to catch all undesirable content.The first part of this thesis examines a defense technique for the concrete application domain of comment spam on social media. We propose content complexity, a compression-based normalized measure of textual redundancy that is mostly insensitive to the underlying language used and adversarial word spelling variations. We demonstrate on a real dataset of tens of millions of comments that content complexity alone achieves 15 percentage points higher precision than a state-of-the-art detection system.The second part of this thesis takes a quantitative approach to evasion and introduces one machine learning algorithm and one learning framework for building hardened detection pipelines. Both techniques are generic and suitable for a large class of application domains. We propose the convex polytope machine, a non-linear large-scale learning algorithm which aims at finding a large-margin polytope separator and thereby decrease the effectiveness of evasion attacks. We show that as a general purpose machine learning algorithm, the convex polytope machine displays an outstanding trade-off between classification accuracy and computational efficiency. We also demonstrate on a benchmark handwritten digit recognition task that the convex polytope machine is quantitatively as evasion-resistant as a classic neural network.We finally introduce adversarial boosting, a boosting-inspired framework for iteratively building ensemble classifiers that are hardened against evasion attacks. Adversarial boosting operates by repeatedly constructing evasion attacks and adding the corresponding corrective sub-classifiers to the ensemble. We implement this technique for decision tree sub-classifiers by constructing the first exact and approximate automatic evasion algorithms for tree ensembles. For our benchmark task, the adversarially boosted tree ensemble is respectively five times and two times less evasion-susceptible than regular tree ensembles and the convex polytope machine.	machine learning;pipeline (computing)	Alex Kantchelian	2016			computer science;artificial intelligence;data science	ML	-10.4088088951668	-51.89523434625426	119896
570689c91d8f663458319a53e4a48759e10263e9	inter-media hashing for large-scale retrieval from heterogeneous data sources	hashing;inter media retrieval;indexing;heterogeneous data source	In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users' demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query's results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.	code;data point;exclusive or;experiment;hamming space;hash function;programming paradigm;scalability	Jingkuan Song;Yang Yang;Yi Yang;Zi Xuan Huang;Heng Tao Shen	2013		10.1145/2463676.2465274	search engine indexing;hash function;computer science;data mining;database;information retrieval	DB	-14.814705442586533	-50.44183941189626	120053
360cf6ee979ddf8f33786c65573b5b865103e0e6	the pursuit of a good possible world: extracting representative instances of uncertain graphs	uncertain graph;representative;possible world	Data in several applications can be represented as an uncertain graph, whose edges are labeled with a probability of existence. Exact query processing on uncertain graphs is prohibitive for most applications, as it involves evaluation over an exponential number of instantiations. Even approximate processing based on sampling is usually extremely expensive since it requires a vast number of samples to achieve reasonable quality guarantees. To overcome these problems, we propose algorithms for creating deterministic representative instances of uncertain graphs that maintain the underlying graph properties. Specifically, our algorithms aim at preserving the expected vertex degrees because they capture well the graph topology. Conventional processing techniques can then be applied on these instances to closely approximate the result on the uncertain graph. We experimentally demonstrate, with real and synthetic uncertain graphs, that indeed the representative instances can be used to answer, efficiently and accurately, queries based on several properties such as shortest path distance, clustering coefficient and betweenness centrality.	approximation algorithm;betweenness centrality;clustering coefficient;database;directed graph;experiment;graph property;possible world;sampling (signal processing);shortest path problem;synthetic intelligence;time complexity;topological graph theory	Panos Parchas;Francesco Gullo;Dimitris Papadias;Francesco Bonchi	2014		10.1145/2588555.2593668	machine learning;possible world	DB	-11.306483581422398	-40.227403273449255	120089
1fa07577fa6323b1e81e3ce221067f761709c689	modeling the small-world phenomenon with local network flow	average distance;small world;hybrid model;degree distribution;graph model;power law;network flow	The small-world phenomenon includes both small average distance and the clustering effect. Randomly generated graphs with a power law degree distribution are widely used to model large real-world networks, but while these graphs have small average distance, they generally do not exhibit the clustering effect. We introduce an improved hybrid model that combines a global graph (a random power law graph) with a local graph (a graph with high local connectivity defined by network flow). We present an efficient algorithm that extracts a local graph from a given realistic network. We show that the underlying local graph is robust in the sense that when our extraction algorithm is applied to a hybrid graph, it recovers the original local graph with a small error. The proof involves a probabilistic analysis of the growth of neighborhoods in the hybrid graph model.	algorithm;butterfly effect;cluster analysis;degree distribution;flow network;mcgurk effect;probabilistic analysis of algorithms;randomness;small-world experiment	Reid Andersen;Fan Chung Graham;Linyuan Lu	2005	Internet Mathematics	10.1080/15427951.2005.10129109	lattice graph;graph power;power law;combinatorics;discrete mathematics;flow network;degree distribution;graph bandwidth;null graph;degree;regular graph;clique-width;simplex graph;machine learning;mathematics;voltage graph;distance-hereditary graph;butterfly graph;random geometric graph;climate graph;quartic graph;complement graph;strength of a graph;statistics	Theory	-14.905206992821945	-40.17007865228748	120256
44b391d024a6be1c301a618e24e0012f8129c545	fast graph drawing algorithm revealing networks cores	hierarchical clustering;graph theory data visualisation;graph drawing;clustering algorithms layout visualization clutter image edge detection vegetation force;edge visual clutter graph drawing algorithm networks cores automatic visualization methods node links diagram glyph node edge node layout algorithm hierarchical coreness decomposition clusters topologies area aware drawing algorithms node overlap free drawings network communities;network community visualization graph drawing hierarchical clustering;net work community visualization;network community visualization	Graph is a powerful tool to model relationships between elements and has been widely used in different research areas. Size and complexity of newly acquired graphs prohibit manual representations and urge a need for automatic visualization methods. We are interested with the node-links diagram which represents each node as a glyph and edge as a line between the corresponding nodes. % We present a novel layout algorithm that emphasizes the cores of very large networks (up to several hundred thousand of nodes and million of edges) in few seconds or minutes. Our method uses a hierarchical coreness decomposition of the graph and a combination of existing layout algorithms according to the clusters topologies. Area-aware drawing algorithms which produce node overlap-free drawings are used to reduce the visual clutter. Edges are bundled along the hierarchy of clusters to highlight the network communities and reduce edge visual clutter. % We validated our approach by comparing our method against one of the fastest method of the state of the art on a benchmark of 23 large graphs extracted from various sources. We have statistically proved that our method performs faster while providing meaningful results.	aggregate data;algorithm;algorithmic efficiency;baseline (configuration management);benchmark (computing);cluster analysis;clutter;computation;data structure;diagram;edge dominating set;edge enhancement;fastest;force-directed graph drawing;glyph;hierarchical clustering;hyperlink;quotient graph;recursion;time complexity;tree network;turing completeness	Romain Giot;Romain Bourqui	2015	2015 19th International Conference on Information Visualisation	10.1109/iV.2015.54	computer science;theoretical computer science;force-directed graph drawing;machine learning;distributed computing;graph drawing	Visualization	-13.790336884314748	-39.66112406340277	120629
1eca746ae5189aaa41873996da483129fa3b44c5	robust vision-based robot localization using combinations of local feature region detectors	robot localization;afﬁne covariant region detectors;articulo;vision based localization;panoramic vision;local features;indoor environment;mobile robot localization;affine covariant region detectors;topological localization	HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. Robust vision-based robot localization using combinations of local feature region detectors Arnau Ramisa, Adriana Tapus, David Aldavert, Ricardo Toledo, Ramon Lopez de Mantaras	adriana de barros;archive;comefrom;hal;linear algebra;robotic mapping;sensor	Arnau Ramisa;Adriana Tapus;David Aldavert;Ricardo Toledo;Ramón López de Mántaras	2009	Auton. Robots	10.1007/s10514-009-9136-9	monte carlo localization;computer vision;simulation	Robotics	-9.832068309732898	-49.054611835638745	120706
cad9b685d44bb29e96b2e2d8c982d889c54cc43d	type disciplines for systems biology		Systems Biology is a discipline that aims to study complex biological systems by means of computational models. Because of the complexity of biological behaviors, the formalisms used in this field are usually designed ad-hoc for the biological topic of interest, or they need to be tuned by a long set of evolution rules. Here we present a different approach: we define biological properties through a type discipline, leaving the formalisms as general as possible. We explore three different kinds of Type Systems: a static one, that limits the model that can be written by modelers; a dynamic one, that limits the evolution of the model at run-time; and an hybrid combination of the previous ones.	computational model;hoc (programming language);systems biology;type system	Livio Bioglio	2014			computer science;ecology and evolutionary biology;discrete mathematics;structure of the disciplines;theoretical computer science;computational model;systems biology;scientific method;computer cluster;natural computing;object-oriented programming	PL	-5.717671608202436	-49.64112019784641	121019
896db629dd8b2d5d0efa0e8f85422b8b78a1f741	the incremental mining of constrained cube gradients	data cube;condensed cube;incremental mining;association rule mining;data warehouse;constrained cube gradients	The mining of cube gradients is an extension of traditional association rules mining in data cube and has broad applications. In this paper, we consider the problem of mining constrained cube gradients for partially materialized data cubes. Its purpose is to extract interesting gradient-probe cell pairs from partially materialized cubes while adding or deleting cells. Instead of directly searching the new data cubes from scratch, an incremental mining algorithm IncA is presented, which sufficiently uses the mined cube gradients from old data cubes. In our algorithms, the condensed cube structure is used to reduce the sizes of materialized cubes. Moreover, some efficient methods are presented in IncA to optimize the comparison process of cell pairs. The performance studies show the incremental mining algorithm IncA is more efficient and scalable than the directed mining algorithm DA with different constraints and sizes of materialized data cubes.	algorithm;association rule learning;data cube;image gradient;materialized view;mined;olap cube;scalability	Yubao Liu;Jianlin Feng;Jian Yin	2007	International Journal of Information Technology and Decision Making	10.1142/S0219622007002502	association rule learning;computer science;data warehouse;data mining;klee–minty cube;database;mathematics;data cube	DB	-5.904082981916114	-38.24863845209344	121133
0614b4612dbb1ff548aed2ab7571140e64b0ac2b	finding influencers in networks using social capital	search engines;social networking (online);dblp network;f1-measure;myerson's allocation function;pmia;pagerank;socap;allocation rule;fair share;information diffusion;maximum information spread;network collaboration;real-life data set;social capital;value-allocation model;weighted degree baselines;wikipedia	The existing methods for finding influencers use the process of information diffusion to discover the nodes with maximum information spread. These models capture only the process of information diffusion and not the actual social value of collaborations in the network. We have proposed a method for finding influencers using the idea that people generate more value for their work by collaborating with peers of high influence. The social value generated through such collaborations denotes the notion of individual social capital. We hypothesize and show that players with high social capital are often key influencers in the network. We propose a value-allocation model to compute the social capital and allocate the fair share of this capital to each individual involved in the collaboration. We show that our allocation satisfies several axioms of fairness and falls in the same class as the Myerson's allocation function. We implement our allocation rule using an efficient algorithm SoCap and show that our algorithm outperforms the baselines in several real-life data sets. Specifically, in DBLP network, our algorithm outperforms PageRank, PMIA and Weighted Degree baselines up to 8% in terms of precision, recall and F1-measure.	algorithm;baseline (configuration management);f1 score;fairness measure;pagerank;real life;social capital;value (ethics)	Karthik Subbian;Dhruv Sharma;Zhen Wen;Jaideep Srivastava	2013	2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)	10.1007/s13278-014-0219-z	public relations;knowledge management;political science;data mining	AI	-18.254112680994158	-44.90921985982698	121222
0c186c7a89739f293554e7dc98ae391178e7648e	context-aware path ranking for knowledge base completion.		Knowledge base (KB) completion aims to infer missing facts from existing ones in a KB. Among various approaches, path ranking (PR) algorithms have received increasing attention in recent years. PR algorithms enumerate paths between entitypairs in a KB and use those paths as features to train a model for missing fact prediction. Due to their good performances and high model interpretability, several methods have been proposed. However, most existing methods suffer from scalability (high RAM consumption) and feature explosion (trains on an exponentially large number of features) problems. This paper proposes a Context-aware Path Ranking (C-PR) algorithm to solve these problems by introducing a selective path exploration strategy. C-PR learns global semantics of entities in the KB using word embedding and leverages the knowledge of entity semantics to enumerate contextually relevant paths using bidirectional random walk. Experimental results on three large KBs show that the path features (fewer in number) discovered by C-PR not only improve predictive performance but also are more interpretable than existing baselines.	algorithm;baseline (configuration management);computational resource;entity;enumerated type;knowledge base;pr/sm;performance;random-access memory;scalability;word embedding	Sahisnu Mazumder;Bing Liu	2017		10.24963/ijcai.2017/166	machine learning;scalability;artificial intelligence;word embedding;random walk;knowledge base;semantics;computer science;interpretability;pattern recognition;ranking	AI	-13.520632687021427	-45.9447572789154	121533
13644ed54e278caa6e75a4661f73d0e11d494697	indirectly encoding running and jumping sodarace creatures for artificial life	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;sodarace;morphologies;research articles;abstracts;open access;hyperneat;life sciences;clinical guidelines;ambulating creatures;novelty search;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search;evolution	This article presents a lightweight platform for evolving two-dimensional artificial creatures. The aim of providing such a platform is to reduce the barrier to entry for researchers interested in evolving creatures for artificial life experiments. In effect the novel platform, which is inspired by the Sodarace construction set, makes it easy to set up creative scenarios that test the abilities of Sodarace-like creatures made of masses and springs. In this way it allows the researcher to focus on evolutionary algorithms and dynamics. The new indirectly encoded Sodarace (IESoR) system introduced in this article extends the original Sodarace by enabling the evolution of significantly more complex and regular creature morphologies. These morphologies are themselves encoded by compositional pattern-producing networks (CPPNs), an indirect encoding previously shown effective at encoding regularities and symmetries in structure. The capability of this lightweight system to facilitate research in artificial life is then demonstrated through both walking and jumping domains, in which IESoR discovers a wide breadth of strategies through novelty search with local competition.	artificial life;biological evolution;compositional pattern-producing network;creatures;evolutionary algorithm;experiment;inspiration function;natural springs;preparation	Paul A. Szerlip;Kenneth O. Stanley	2015	Artificial Life	10.1162/ARTL_a_00185	biology;text mining;simulation;computer science;bioinformatics;artificial intelligence;machine learning;evolution;genetics	AI	-6.018945131559295	-46.96225644002257	121580
9492576585b5206b09dc2c2d427fd950ea3d49e4	oinduced: an efficient algorithm for mining induced patterns from rooted ordered trees	trees mathematics data mining pattern clustering tree codes;tree encoding;cm coding oinduced induced pattern mining rooted ordered trees indexing tree encoding;pattern clustering;labeled tree;tree encoding breadth first candidate generation frequency counting frequent tree pattern induced subtree rooted ordered labeled tree;tree codes;frequency counting;induced subtree;efficient algorithm;frequent patterns;trees mathematics;data mining;data mining encoding indexing data structures algorithm design and analysis;breadth first candidate generation;general methods;indexing;data structures;indexation;frequent tree pattern;synthetic data;encoding;data structure;algorithm design;algorithm design and analysis;rooted ordered labeled tree	Frequent tree patterns have many practical applications in different domains, such as Extensible Markup Language mining, Web usage analysis, etc. In this paper, we present OInduced , which is a novel and efficient algorithm for finding frequent ordered induced tree patterns. OInduced uses a breadth-first candidate generation method and improves it by means of an indexing scheme. We also introduce frequency counting using tree encoding. For this purpose, we present two novel tree encodings, namely, m-coding and cm-coding, and show how they can restrict nodes of input trees and compute frequencies of generated candidates. We perform extensive experiments on both real and synthetic data sets to show the efficiency and scalability of OInduced.	algorithm;breadth-first search;experiment;markup language;scalability;synthetic data;usage analysis;xml	Mostafa Haghir Chehreghani;Morteza Haghir Chehreghani;Caro Lucas;Masoud Rahgozar	2011	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2010.2096808	algorithm design;data structure;computer science;theoretical computer science;data mining;interval tree;mathematics;search tree;tree;algorithm	DB	-6.997246695125492	-38.204230652198305	121891
fa23e43bc1af76c71ecf407e0bdc92c274e2c8f2	group selection vs multi-level selection: some example models using evolutionary games	dna;organisms;topology;genomics;sequences;evolutionary computation;game theory;biological system modeling;group theory biology evolutionary computation game theory;qa 76 software;biology;biology group selection multilevel selection evolutionary game;psychology;group theory;multilevel selection;evolution biology;computer programming;computational modeling;adaptation model;evolutionary game;games;production;costs sequences microorganisms dna organisms evolution biology psychology game theory production topology;high frequency;microorganisms;group selection;bioinformatics	We present a model of multi-level selection. By this we mean the idea that there are multiple units of selection each of which operates on a different hierarchical level. Concretely we consider here a model of 3 hierarchical levels and various selection scenarios of adaptive conflict between levels. The main finding of this contribution is that in order for selection at higher level units to be effective, it has to occur at a high frequency compared to low level selection. From this we conclude that multi-level selection is biologically not very plausible.	backup;computational model;experiment;interaction;population;simulation;the evolution of cooperation	Dominique Chu;David J. Barnes	2009	2009 IEEE Congress on Evolutionary Computation	10.1109/CEC.2009.4983028	games;game theory;group selection;genomics;computer science;bioinformatics;artificial intelligence;machine learning;high frequency;computer programming;group theory;dna;evolutionary computation	SE	-4.693626668336409	-48.487781159292574	121907
0e80a25b12641468da87a5417fa128020a4dd2ef	annexml: approximate nearest neighbor search for extreme multi-label classification		"""Extreme multi-label classification methods have been widely used in Web-scale classification tasks such as Web page tagging and product recommendation. In this paper, we present a novel graph embedding method called """"AnnexML"""". At the training step, AnnexML constructs a k-nearest neighbor graph of label vectors and attempts to reproduce the graph structure in the embedding space. The prediction is efficiently performed by using an approximate nearest neighbor search method that efficiently explores the learned k-nearest neighbor graph in the embedding space. We conducted evaluations on several large-scale real-world data sets and compared our method with recent state-of-the-art methods. Experimental results show that our AnnexML can significantly improve prediction accuracy, especially on data sets that have larger a label space. In addition, AnnexML improves the trade-off between prediction time and accuracy. At the same level of accuracy, the prediction time of AnnexML was up to 58 times faster than that of SLEEC, which is a state-of-the-art embedding-based method."""	approximation algorithm;artificial neural network;association rule learning;deep learning;graph embedding;k-nearest neighbors algorithm;kernel method;multi-label classification;nearest neighbor search;nearest neighbour algorithm;nonlinear system;web analytics;web page	Yukihiro Tagami	2017		10.1145/3097983.3097987	graph embedding;artificial intelligence;cover tree;data mining;machine learning;computer science;best bin first;ball tree;large margin nearest neighbor;r-tree;pattern recognition;nearest neighbor graph;nearest neighbor search	AI	-17.546584765993256	-49.18118736138855	121985
eaaf3953567df5ccd2c58cb4b18c07c1bb65e068	three-dimensional visualization and animation of emerging patterns by the process of self-organization in collaboration networks	complementarities;visualization;animation;mathematical model;social network analysis;self organization;3 d computer graphs;co authorship	The “Social Gestalt” model is a new parametric model visualizing 3-D graphs, using animation to show these graphs from different points of view. A visible 3-D graph image is the emerging pattern at the macro level of a system of co-authorships by the process of self-organization. Well-ordered 3-D computer graphs are totally rotatable and their shapes are visible from all possible points of view. The objectives of this paper are the description of several methods for three-dimensional modelling and animation and the application of these methods to two co-authorship networks selected for demonstration of varying 3-D graph images. This application of the 3-D graph modelling and animation shows for both the journal “NATURE” and the journal “Psychology of Women Quarterly” that at any time and independently on the manifold visible results of rotation, the empirical values nearly exactly match the theoretical distributions (Called “Social Gestalts”) obtained by regression analysis. In addition the emergence of different shapes between the 3-D graphs of “NATURE” and “Psychology of Women Quarterly” is explained.	emergence;gestalt psychology;parametric model;self-organization	Hildrun Kretschmer;Donald de B. Beaver;Theo Kretschmer	2015	Scientometrics	10.1007/s11192-015-1579-5	anime;social network analysis;self-organization;social science;simulation;visualization;computer science;artificial intelligence;theoretical computer science;mathematical model;data mining;mathematics;world wide web;statistics	Graphics	-17.842701803598843	-40.35667149693781	122224
5424b20ce48c3373c96fdefe0afb0d65b4a7c855	feature based task recommendation in crowdsourcing with implicit observations		Existing research in crowdsourcing has investigated how to recommend tasks to workers based on which task the workers have already completed, referred to as implicit feedback. We, on the other hand, investigate the task recommendation problem, where we leverage both implicit feedback and explicit features of the task. We assume that we are given a set of workers, a set of tasks, interactions (such as the number of times a worker has completed a particular task), and the presence of explicit features of each task (such as, task location). We intend to recommend tasks to the workers by exploiting the implicit interactions, and the presence or absence of explicit features in the tasks. We formalize the problem as an optimization problem, propose two alternative problem formulations and respective solutions that exploit implicit feedback, explicit features, as well as similarity between the tasks. We compare the efficacy of our proposed solutions against multiple state-of-the-art techniques using two large scale real world datasets.	crowdsourcing;interaction;latent variable;mathematical optimization;optimization problem;stack exchange	Habibur Rahman;Lucas Joppa;Senjuti Basu Roy	2016	CoRR		simulation;computer science;machine learning;data mining	ML	-18.357252741542947	-47.70802135362349	122504
507c9267a9c614115e0daf0c45373ab30c33a3ad	anonymizing subsets of social networks with degree constrained subgraphs	graph theory;efficient algorithm;social network;data privacy;k subset anonymization;network theory graphs;degree constrained subgraphs;privacy	In recent years, concerns of privacy have become more prominent for social networks. Anonymizing a graph meaningfully is a challenging problem, as the original graph properties must be preserved as well as possible. We introduce a generalization of the degree anonymization problem posed by Liu and Terzi. In this problem, our goal is to anonymize a given subset of nodes while adding the fewest possible number of edges. The main contribution of this paper is an efficient algorithm for this problem by exploring its connection with the degree-constrained subgraph problem. Our experimental results show that our algorithm performs very well on many instances of social network data.	algorithm;anonymous web browsing;graph property;privacy;social network	Sean Chester;Jared Gaertner;Ulrike Stege;Srinivas Karthik Venkatesh	2012	2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining	10.1109/ASONAM.2012.74	combinatorics;discrete mathematics;computer science;graph theory;theoretical computer science;mathematics;privacy;computer security;social network	DB	-12.584229491718961	-43.509283879574	122611
6d91dd631300c91e584ddf28a60e86c370186169	open networks from within: input or output betweenness centrality of nodes in directed networks	directed networks;betweenness centrality;category theory	New betweenness centralities of nodes in a directed network are proposed based on the idea that nodes in a network are processes rather than things. They are called input and output betweenness centralities. They measure importance of nodes as input and output for gluing arcs together as interface between processes, respectively. We demonstrate their use and discuss their meaning by calculating them in two toy directed networks and one real-world network. We also compare them with the existing centrality measures that reflect asymmetry of links in directed networks: outand in-degrees and Hub and Authority scores. We found that input and output betweenness centralities behave differently from these measures in some nodes. It is suggested that they can effectively identify nodes that are less important in terms of existing measures but are noteworthy from the viewpoint that nodes are processes.	betweenness centrality;directed graph;input/output;process (computing);usb hub	Taichi Haruna	2018	Applied Network Science	10.1007/s41109-018-0076-1	category theory;distributed computing;centrality;betweenness centrality;mathematics;input/output	Web+IR	-16.962509483478943	-39.42596684281503	122637
15b8b8970eaa2b8eec80500656d5eac4182a4b87	generating uncertain networks based on historical network snapshots	data mining;snapshot;social networks;uncertain graph	Imprecision, incompleteness and dynamic exist in wide range of network applications. It is difficult to decide the uncertainty relationship among nodes since traditional models do not make sense on uncertain networks, and the inherent computational complexity of problems with uncertainty is always intractable. In this paper, we study how to capture the uncertainty in networks by modeling a series snapshots of networks to an uncertain graph. Since the large number of possible instantiations of an uncertain network, a novel sampling scheme is proposed which enables the development of efficient algorithm to measure in uncertain networks; considering the practical of neighborhood relationship in real networks, a framework is introduced to transform the uncertain networks into deterministic weight networks where the weights on edges can be measured as Jaccard-like index. The comprehensive experimental evaluation on real data demonstrates the effectiveness and efficiency of our algorithms.	algorithm;computational complexity theory;jaccard index;sampling (signal processing)	Meng Han;Mingyuan Yan;Jinbao Li;Shouling Ji;Yingshu Li	2013		10.1007/978-3-642-38768-5_68	mathematical optimization;machine learning;data mining;mathematics	ML	-11.367513945237862	-40.27574322232649	122659
345c4f93cb14a357598b8548f16cce80a61abed9	mining direct antagonistic communities in explicit trust networks	trust network;selected works;efficient algorithm;social network;signed social network;bepress;social networking sites;direct antagonistic community;mining maximal bi cliques	There has been a recent increase of interest in analyzing trust and friendship networks to gain insights about relationship dynamics among users. Many sites such as Epinions, Facebook, and other social networking sites allow users to declare trusts or friendships between different members of the community. In this work, we are interested in extracting direct antagonistic communities (DACs) within a rich trust network involving trusts and distrusts. Each DAC is formed by two subcommunities with trust relationships among members of each sub-community but distrust relationships across the sub-communities. We develop an efficient algorithm that could analyze large trust networks leveraging the unique property of direct antagonistic community. We have experimented with synthetic and real data-sets (myGamma and Epinions) to demonstrate the scalability of our proposed solution.	algorithm;british national vegetation classification;distrust;scalability;social network;synthetic intelligence;web of trust	David Lo;Didi Surian;Kuan Zhang;Ee-Peng Lim	2011		10.1145/2063576.2063722	world wide web;social network	Metrics	-17.395695737424692	-41.470723175654875	123051
457757df5c57de5e6698bd80ec20293ba537237e	the (black) art of runtime evaluation: are we comparing algorithms or implementations?	implementation matters;efficiency evaluation;methodology;runtime experiments	Any paper proposing a new algorithm should come with an evaluation of efficiency and scalability (particularly when we are designing methods for “big data”). However, there are several (more or less serious) pitfalls in such evaluations. We would like to point the attention of the community to these pitfalls. We substantiate our points with extensive experiments, using clustering and outlier detection methods with and without index acceleration. We discuss what we can learn from evaluations, whether experiments are properly designed, and what kind of conclusions we should avoid. We close with some general recommendations but maintain that the design of fair and conclusive experiments will always remain a challenge for researchers and an integral part of the scientific endeavor.	algorithm;anomaly detection;big data;cluster analysis;data mining;expect;experiment;floor and ceiling functions;no free lunch in search and optimization;no free lunch theorem;runtime system;scalability;vldb;vertex-transitive graph	Hans-Peter Kriegel;Erich Schubert;Arthur Zimek	2016	Knowledge and Information Systems	10.1007/s10115-016-1004-2	simulation;computer science;methodology;data mining;management science	DB	-12.153715299388951	-41.269641519969454	123280
1ec41c4e6be845a1090d0d43ae07d1c0e979daf8	querying web-scale information networks through bounding matching scores	distributed system;query processing;index free;billion node graphs;subgraph matching;graph similarity	Web-scale information networks containing billions of entities are common nowadays. Querying these networks can be modeled as a subgraph matching problem. Since information networks are incomplete and noisy in nature, it is important to discover answers that match exactly as well as answers that are similar to queries. Existing graph matching algorithms usually use graph indices to improve the efficiency of query processing. For web-scale information networks, it may not be feasible to build the graph indices due to the amount of work and the memory/storage required. In this paper, we propose an efficient algorithm for finding the best k answers for a given query without precomputing graph indices. The quality of an answer is measured by a matching score that is computed online. To speed up query processing, we propose a novel technique for bounding the matching scores during the computation. By using bounds, we can efficiently prune the answers that have low qualities without having to evaluate all possible answers. The bounding technique can be implemented in a distributed environment, allowing our approach to efficiently answer the queries on web-scale information networks. We demonstrate the effectiveness and the efficiency of our approach through a series of experiments on real-world information networks. The result shows that our bounding technique can reduce the running time up to two orders of magnitude comparing to an approach that does not use bounds.	algorithm;computation;database;entity;experiment;matching (graph theory);precomputation;subgraph isomorphism problem;time complexity	Jiahui Jin;Samamon Khemmarat;Lixin Gao;Junzhou Luo	2015		10.1145/2736277.2741131	computer science;theoretical computer science;machine learning;data mining;database	DB	-9.682992767869186	-40.29621002939604	123349
39b4d14b4e8c05afd0e13b7962cf9bf36a7be95e	learning on partial-order hypergraphs		Graph-based learning methods explicitly consider the relations between two entities (i.e., vertices) for learning the prediction function. They have been widely used in semi-supervised learning, manifold ranking, and clustering, among other tasks. Enhancing the expressiveness of simple graphs, hypergraphs formulate an edge as a link to multiple vertices, so as to model the higher-order relations among entities. Typically, hyperedges in a hypergraph are used to encode certain kinds of similarity among vertices. To the best of our knowledge, all existing hypergraph structures represent the hyperedge as an unordered set of vertices, without considering the possible ordering relationship among vertices. In real-world data, the ordering relationships commonly exist, such as in graded categorical features (e.g., users’ ratings on movies) and numerical features (e.g., monthly income of customers). When constructing a hypergraph, ignoring such ordering relationships among entities will lead to severe information loss, resulting in suboptimal performance of the subsequent learning algorithms. In this work, we address the inherent limitation of existing hypergraphs by proposing a new data structure named Partial-Order Hypergraph, which specifically injects the partially ordering relationships of vertices into a hyperedge. We develop regularization-based learning theories for partial-order hypergraphs, generalizing conventional hypergraph learning by incorporating logical rules that encode the partial-order relations. We apply our proposed method to two applications: university ranking from Web data and popularity prediction of online content. Extensive experiments demonstrate the superiority of our proposed partial-order hypergraphs, which consistently improve over conventional hypergraphs.	algorithm;artificial neural network;cluster analysis;data structure;deep learning;encode;entity;experiment;machine learning;numerical analysis;semi-supervised learning;semiconductor industry;supervised learning;time complexity;unordered associative containers (c++);vertex (geometry);web content	Fuli Feng;Xiangnan He;Yiqun Liu;Liqiang Nie;Tat-Seng Chua	2018		10.1145/3178876.3186064	computer science;artificial intelligence;machine learning;vertex (geometry);categorical variable;cluster analysis;hypergraph;constraint graph;data structure;generalization;ranking	Web+IR	-16.3537152448292	-47.54081201184423	124297
ad7f339acb70d0b70959a2de486b33589a552120	maintaining sliding-window neighborhood profiles in interaction networks		Large networks are being generated by applications that keep track of relationships between different data entities. Examples include online social networks recording interactions between individuals, sensor networks logging information exchanges between sensors, and more. There is a large body of literature on computing exact or approximate properties on large networks, although most methods assume static networks. On the other hand, in most modern real-world applications, networks are highly dynamic and continuous interactions along existing connections are generated. Furthermore, it is desirable to consider that old edges become less important, and their contribution to the current view of the network diminishes over time. We study the problem of maintaining the neighborhood profile of each node in an interaction network. Maintaining such a profile has applications in modeling network evolution and monitoring the importance of the nodes of the network over time. We present an online streaming algorithm to maintain neighborhood profiles in the sliding-window model. The algorithm is highly scalable as it permits parallel processing and the computation is node centric, hence it scales easily to very large networks on a distributed system, like Apache Giraph. We present results from both serial and parallel implementations of the algorithm for different social networks. The summary of the graph is maintained such that query of any window length can be performed.		Rohit Kumar;Toon Calders;Aristides Gionis;Nikolaj Tatti	2015		10.1007/978-3-319-23525-7_44	evolving networks;computer science;dynamic network analysis;artificial intelligence;theoretical computer science;operating system;machine learning;data mining;spatial network;database;distributed computing;interdependent networks;computer security	ML	-11.662002064828346	-39.43372053883521	124441
9633a8eb05296c0da6fe79f9cf2fe04f16ac9d62	influence maximization in human-intervened social networks		Recently there has been tremendous research on influence analysis in social networks: how to find initial topics or users to maximize the word-of-mouth effect that may be significant for advertising, viral marketing and other applications. Many researchers focus on the problem of influence maximization on the static structure of the network and find a subset of early adopters which activate the influence diffusion across the network. Despite the progress in modeling and techniques, how the incentives improve the network structure to enlarge the influence diffusion has been largely overlooked. In this paper, we introduce a novel problem which extends the influence maximization to the situation that the network structure can be varied in case of some incentives such as fans trading by compensating the web users to be fans in social networks. Providing that the presented problem is NP-hard, we propose two approximate approaches to solve the problem of influence maximization in dynamic networks. The first is a two-stage approach which separates the problem into two sub problems and solves them respectively. The second is a joint influence diffusion algorithm so as to repair the network structure and find the corresponding initial subset of the individuals in the repaired social network simultaneously to maximize the influence. We performed experiments on social network data to provide evidence of the effectiveness of the proposed methods.	amazon mechanical turk;approximation algorithm;entropy maximization;expectation–maximization algorithm;experiment;np-hardness;real life;social network;the turk;timeline	Qiang You;Weiming Hu;Ou Wu	2015			simulation;computer science;dynamic network analysis;artificial intelligence;social psychology	Web+IR	-16.92921405249263	-43.76571238279297	124623
74bafb5649aa78b78cc2e37194938e1b7f6f2416	on computational approaches to trust evaluation in large-scale social networks		In this paper, we consider the problem of evaluating trust between individuals in social networks. The previously introduced three-valued subjective logic (3VSL) provides a set of useful tools in measuring interpersonal trust in social human networks. However, the number of required operations for such measurements grows exponentially with the size of network. Moreover, the correlations between different paths connecting the two individuals caused by common edges are not considered. In this paper, we show that the operators in 3VSL can still be used to give a lower-bound on trust even if such correlations are taken into account. We introduce a low complexity scalable algorithm to obtain this lower-bound. The numerical experiment results are represented and compared with the 3VSL.	algorithm;complex network;computation;distrust;numerical analysis;scalability;social network	Shahriar Etemadi Tajbakhsh;Gaojie Chen;Justin P. Coon	2017	2017 IEEE International Black Sea Conference on Communications and Networking (BlackSeaCom)	10.1109/BlackSeaCom.2017.8277691	operator (computer programming);machine learning;scalability;subjective logic;social network;computer science;artificial intelligence;interpersonal communication	Mobile	-16.014542022980365	-43.348240150353575	124624
dd1e4296c2dc8479b83907728dbfe50585b9062a	learning heterogeneous coupling relationships between non-iid terms	non iid;vector representation;coupled similarity	With the rapid proliferation of social media and online community, a vast amount of text data has been generated. Discovering the insightful value of the text data has increased its importance, a variety of text mining and process algorithms have been created in the recent years such as classification, clustering, similarity comparison. Most previous research uses a vector-space model for text representation and analysis. However, the vector-space model does not utilise the information about the relationships between the term to term. Moreover, the classic classification methods also ignore the relationships between each text document to another. In other word, the traditional text mining techniques assume the relation between terms and between documents are independent and identically distributed iid. In this paper, we will introduce a novel term representation by involving the coupled relations from term to term. This coupled representation provides much richer information that enables us to create a coupled similarity metric for measuring document similarity, and a coupled document similarity based K-Nearest centroid classifier will be applied to the classification task. Experiments verify the proposed approach outperforming the classic vector-space based classifier, and show potential advantages and richness in exploring the other text mining tasks.		Mu Li;Jinjiu Li;Yuming Ou;Ya Zhang;Dan Luo;Maninder Bahtia;Longbing Cao	2013		10.1007/978-3-642-55192-5_7	text mining;computer science;artificial intelligence;machine learning;pattern recognition;data mining	DB	-17.62649313335601	-46.94056698673814	124703
8f893a77321620357f7f4d44953a17191574b423	revealing hierarchical structure through simultaneous community detection across multiscale networks	community detection complex networks hierarchical topology;community detection;complex networks;intercommunity relationship hierarchical structure community detection multiscale networks social networks artificial networks natural networks hierarchical topology hierarchical organization synthetic networks real networks community structure formation;topology network theory graphs;communities organizations educational institutions couplings network topology topology visualization;hierarchical topology	One common feature shared by many natural, artificial and social networks is the presence of hierarchical topology. In this paper we propose a method to reveal the hierarchical organization inherent in a network by simultaneously detecting communities across a series of multiscale networks. It has been tested on both synthetic and real networks. Results show that by visualizing the formation of community structures at different scales at the same time, a better understanding of hierarchical organization of a network and inter-community relationship can be obtained.	network topology;semantic network;sensor;social network;synthetic intelligence;tree network	Haiying Wang;Huiru Zheng	2013	2013 Ninth International Conference on Natural Computation (ICNC)	10.1109/ICNC.2013.6818116	geography;bioinformatics;machine learning;hierarchical network model;data mining;clique percolation method;complex network	DB	-16.24568625320095	-41.14083112450142	124982
7836bb9c4a431c878fe6c640292d362d9d6fb198	exploring neural principles with si elegans, a neuromimetic representation of the nematode caenorhabditis elegans		Biological neural systems are powerful, robust and highly adaptive computational entities that outperform conventional computers in almost all aspects of sensory-motor integration. Despite dramatic progress in information technology, there is a big performance discrepancy between artificial computational systems and brains in seemingly simple orientation and navigation tasks. In fact, no system exists that can faithfully reproduce the rich behavioural repertoire of the tiny worm Caenorhabditis elegans which features one of the simplest nervous systems in nature made of 302 neurons and about 8000 connections. The Si elegans project aims at providing this missing link. This article is sketching out the main platform components.	blueprint;computation;computer;control system;discrepancy function;emulator;entity;formal verification;holism;list of code lyoko episodes;open-source software;parallel computing;robotics;signal processing;testbed;wetware computer	Axel W. Blau;Frank Callaly;Seamus Cawley;Aedan Coffey;Alessandro De Mauro;Gorka Epelde;Lorenzo Ferrara;Finn Krewer;Carlo Liberale;Pedro Machado;Gregory Maclair;T. Martin McGinnity;Fearghal Morgan;Andoni Mujika;Alexey Petrushin;Gautier Robin;John J. Wade	2014		10.5220/0005190701890194	biology;bioinformatics;anatomy	NLP	-6.135330822916345	-48.14033027454049	125416
eaf22288233b43eb073bc18577d1a04cbdc4713c	a simplification algorithm for visualizing the structure of complex graphs	graph theory;complex graphs structure visualization;citation analysis;simplification;complex;graph theory data visualisation;cancer;importance metrics;pruning process;computer networks;network topology;tree graphs;data visualisation;visualization;graph overall connectivity;visual representation;graph overall connectivity simplification algorithm complex graphs structure visualization importance metrics topological graph properties pruning process;displays;graph;data visualization;properties complex graph visualization simplification;clustering algorithms;topological graph properties;computer science;properties;sampling methods;simplification algorithm	Complex graphs, ones containing thousands of nodes of high degree, are difficult to visualize. Displaying all of the nodes and edges of these graphs can create an incomprehensible cluttered output. This paper presents a simplification algorithm that may be applied to a complex graph in order to produce a controlled thinning of the graph. Using importance metrics, the simplification process removes nodes from the graph, leaving the central structure for visualization and evaluation. The simplification algorithm consists of two steps, calculation of the importance metrics and pruning. Several metrics based on various topological graph properties are described. The metrics are then used in a pruning process to simplify the graph. Nodes, along with their corresponding edges, are removed from the graph, while maintaining the graph's overall connectivity. This simplified graph provides a cleaner, more meaningful visual representation of the graph's structure; thus aiding the analysis of the graph's underlying data.	algorithm;complex system;graph (discrete mathematics);graph property;importance sampling;level of detail;text simplification;thinning;topological graph	Daniel Hennessey;Daniel Brooks;Alex Fridman;David E. Breen	2008	2008 12th International Conference Information Visualisation	10.1109/IV.2008.37	lattice graph;combinatorics;geometric graph theory;graph bandwidth;null graph;graph property;computer science;clique-width;theoretical computer science;simplex graph;machine learning;comparability graph;voltage graph;distance-hereditary graph;graph;butterfly graph;quartic graph;complement graph;graph operations;strength of a graph	Visualization	-13.893495489764335	-39.628304001507466	125659
b0290ac1f8dc9938a578afb7535b7b275e40dbde	window-chained longest common subsequence: common event matching in sequences	sequences approximation theory big data graph theory;sensors;approximation algorithms;algorithm design and analysis semantics companies approximation algorithms sensors diabetes heuristic algorithms;semantics;diabetes;companies;common event matching synthetic datasets real world datasets systematic experimental evaluation approximation algorithm graph problem wclcs semantics window chained longest common subsequence semantics event subsequences big data event processing sequence data;heuristic algorithms;algorithm design and analysis	Sequence data is prevalent, and event processing over sequences is increasingly important in this Big Data era, drawing much attention from both research and industry. In this paper, we address a novel problem, which is to find common event subsequences from two long sequences. This problem is well motivated, with applications in diverse domains. We propose the window-chained longest common subsequence (WCLCS) semantics, and argue that the traditional longest common subsequence (LCS) cannot serve this need. We then devise efficient algorithms to solve this problem by reducing it to a graph problem. We also propose two more methods to improve the performance: one is based on informed search and exploration, and the other is an approximation algorithm with accuracy guarantees. We finally carry out a systematic experimental evaluation using two real-world datasets and some synthetic datasets.	approximation algorithm;big data;complex event processing;graph theory;longest common subsequence problem;search algorithm;synthetic intelligence	Chunyao Song;Tingjian Ge	2015	2015 IEEE 31st International Conference on Data Engineering	10.1109/ICDE.2015.7113331	algorithm design;hunt–mcilroy algorithm;longest increasing subsequence;computer science;sensor;theoretical computer science;longest common subsequence problem;data mining;semantics;algorithm	DB	-9.44296619662406	-38.08612786889795	125759
23d3bd7f0f3006f17b9015f6d0eb2e860fb32b89	maximizing influence spread in a new propagation model	influence maximization;information propagation;social network;network model	Study on information propagation in social networks has a long history. The influence maximization problem has become a popular research area for many scholars. Most of algorithms to solve the problem are based on the basic greedy algorithm raised by David Kempe etc. However, these algorithms seem to be ineffective for the large-scaled networks. On seeing the bottleneck of these algorithms, some scholars raised some heuristic algorithms. However, these heuristic algorithms just consider local information of networks and cannot get good results. In this paper, we studied the procedure of information propagation in layered cascade model, a new propagation model in which we can consider the global information of networks. Based on the analysis on layered cascade model, we developed heuristic algorithms to solve influence maximization problem, which perform well in experiments.		Hongchao Yang;Chong-Jun Wang;Junyuan Xie	2012		10.1007/978-3-642-31900-6_37	computer science;artificial intelligence;network model;machine learning;management science;social network	NLP	-16.8249802359834	-43.494716123039765	125947
8611ef2076f3330b76e7f70a4f27cd9623542784	an evolutionary algorithm approach to link prediction in dynamic social networks	complex networks;link prediction;data mining;social networks;complex systems;algorithms;twitter	Many real world, complex phenomena have an underlying structure of evolving networks where nodes and links are added and removed over time. A central scientific challenge is the description and explanation of network dynamics, with a key test being the prediction of short and long term changes. For the problem of short-term link prediction, existing methods attempt to determine neighborhood metrics that correlate with the appearance of a link in the next observation period. Here, we provide a novel approach to predicting future links by applying an evolutionary algorithm (Covariance Matrix Evolution) to weights which are used in a linear combination of sixteen neighborhood and node similarity indices. We examine reciprocal reply networks of Twitter users constructed at the time scale of weeks, both as a test of our general method and as a problem of scientific interest in itself. Our evolved predictors exhibit a thousand-fold improvement over random link prediction, to our knowledge strongly outperforming all extant methods. Based on our findings, we suggest possible factors which may be driving the evolution of Twitter reciprocal reply networks.	evolutionary algorithm;evolving networks;social network	Catherine A. Bliss;Morgan R. Frank;Christopher M. Danforth;Peter Sheridan Dodds	2014	J. Comput. Science	10.1016/j.jocs.2014.01.003	complex systems;evolving networks;computer science;artificial intelligence;machine learning;data mining;complex network;social network	ML	-16.661572419513437	-41.72264114577778	126135
c67c02ba026db6e03e225671133404126a706ed8	outcome aware ranking in interaction networks	interaction networks;modeling technique;interaction network;public domain;centrality measures;ranking algorithm;social network analysis;value creation;network structure;service delivery;node ranking	In this paper, we present a novel ranking technique that we developed in the context of an application that arose in a Service Delivery setting. We consider the problem of ranking agents of a service organization. The service agents typically need to interact with other service agents to accomplish the end goal of resolving customer requests. Their ranking needs to take into account two aspects: firstly, their importance in the network structure that arises as a result of their interactions, and secondly, the value generated by the interactions involving them. We highlight several other applications which have the common theme of ranking the participants of a value creation process based on the network structure of their interactions and the value generated by their interactions. We formally present the problem and describe the modeling technique which enables us to encode the value of interaction in the graph. Our ranking algorithm is based on extension of eigen value methods. We present experimental results on real-life, public domain datasets from the Internet Movie DataBase. This makes our experiments replicable and verifiable.	algorithm;baseline (configuration management);computation;encode;eigen (c++ library);experiment;formal verification;heuristic;itil;interaction network;internet movie database (imdb);numerical linear algebra;power iteration;real life;the matrix	Sampath Kameshwaran;Vinayaka Pandit;Sameep Mehta;Nukala Viswanadham;Kashyap Dixit	2010		10.1145/1871437.1871470	interaction network;social network analysis;public domain;ranking;computer science;artificial intelligence;service delivery framework;machine learning;data mining;database;world wide web;information retrieval	ML	-18.65199367258138	-46.52080538072682	126364
427416b3c6631d14e61de9fa1b03563eee9b5e12	mining associations in large graphs for dynamically incremented marked nodes		The edges between nodes of a graph describe some sort of relationship between the two nodes. In this paper, we would like to efficiently determine the relationship between specific nodes of importance, which we call marked nodes, in a large graph. These relationships obtained must be optimal, which requires us to segregate the marked nodes from the less important nodes and group them together using partitioning algorithms. We introduce an improved algorithm which allows for the efficient addition of new marked nodes to a partition without rerunning the algorithm on previously marked nodes.	algorithm;computation;correctness (computer science);dspace;polynomial;real-time clock;recommender system;social network;splay tree;time complexity	Anshul Rai;Zackary Crosley;Srivignessh Pacham Sri Srinivasan	2018		10.1007/978-3-319-96136-1_4	graph theory;pattern recognition;artificial intelligence;computer science;sort;graph;partition (number theory)	ML	-13.298384543536429	-42.486374602230725	126410
a477d17c3fe7e34a83029adb2f7e8031b2f1532e	cascading transitions in coupled complex ecosystems		The Earth system is arguably one of the most complex systems in the known universe. Over 4.5 billion years it has self-organised into a state which features complex differentiated life that covers its surface and penetrates its crust. This widespread biosphere requires stability in the sense of maintenance of temperature and pressures on the surface that allow liquid water. Within this range there is significant scope for change, some of it dramatic. Glaciation to inter-glacial cycles are classic examples of planetary-scale transitions. This paper examines the role of biological feedback on planetaryscale transitions. We present a conceptual model in which stability emerges as a consequence of interactions between environment and life. These mechanisms not only lead to stable ecosystem configurations, but can also produce critical transitions which we characterise as cascading transitions. Such failures can interact and produce system-wide transitions. Our results would be of interest to those studying realworld ecosystems, tipping elements in the Earth system as well as theoretical studies on complex artificial life systems.	artificial life;biosphere;complex systems;earth system science;ecosystem;interaction;planetary scanner;self-organization	Iain S. Weaver	2015		10.7551/978-0-262-33027-5-ch106	earth system science;conceptual model;artificial intelligence;computer science;artificial life;earth science;complex system;biological feedback;universe;biosphere;ecosystem	AI	-5.859911787173505	-47.11870036516673	126580
0e19aa67a4f8a6d97f50adab705206dceffac1fc	most influential community search over large social networks	electronic mail;companies;indexes;search problems electronic mail australia twitter indexes companies;search problems;twitter;australia	Detecting social communities in large social networks provides an effective way to analyze the social media users' behaviors and activities. It has drawn extensive attention from both academia and industry. One essential aspect of communities in social networks is outer influence which is the capability to spread internal information of communities to external users. Detecting the communities of high outer influence has particular interest in a wide range of applications, e.g., Ads trending analytics, social opinion mining and news propagation pattern discovery. However, the existing detection techniques largely ignore the outer influence of the communities. To fill the gap, this work investigates the Most Influential Community Search problem to disclose the communities with the highest outer influences. We firstly propose a new community model, maximal kr-Clique community, which has desirable properties, i.e., society, cohesiveness, connectivity, and maximum. Then, we design a novel tree-based index structure, denoted as C-Tree, to maintain the offline computed r-cliques. To efficiently search the most influential communities, we also develop four advanced index-based algorithms which improve the search performance of non-indexed solution by about 200 times. The efficiency and effectiveness of our solution have been extensively verified using six real datasets and a small case study.	community search;degeneracy (graph theory);group cohesiveness;maximal set;online and offline;search algorithm;search problem;sensor;social media;social network;software propagation	Jianxin Li;Xinjue Wang;Ke Deng;Xiaochun Yang;Timos Sellis;Jeffrey Xu Yu	2017	2017 IEEE 33rd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2017.136	database index;computer science;data mining;database;world wide web	DB	-17.179500424924374	-42.70059692237018	126796
e02aad52db67a026b916b938d78705fb87f30884	user-centered probabilistic models for content diffusion in the blogosphere		Abstract Predicting the diffusion of information in social networks is a key problem for applications like Opinion Leader Detection, Buzz Detection or Viral Marketing. Many diffusion models are direct extensions of the Cascade and Threshold nmodels, initially proposed for epidemiology and social studies. In such models, the diffusion process is based on the dynamics of interactions between neighbor nodes in the network (the social pressure), and largely ignores important dimensions as the content diffused and the active/passive role users tend to have in social networks. We propose here a new family of models that aims at predicting how a content diffuses in a network by making use of additional dimensions: the content diffused, user’s profile and willingness to diffuse. In particular, we show how to integrate these dimensions into simple feature functions, and propose a probabilistic modeling to account for the diffusion process. These models are then illustrated and compared with other approaches on two blog datasets. The experimental results obtained on these datasets show that taking into account the content diffused is important to accurately model the diffusion process. Lastly, we study the influence maximization problem with these models and prove that it is NP-hard, prior to propose an adaptation of the greedy algorithm to approximate the optimal solution.	blogosphere	Cédric Lagnier;Éric Gaussier;François Kawala	2018	Online Social Networks and Media	10.1016/j.osnem.2018.01.001	probabilistic logic;blogosphere;viral marketing;machine learning;diffusion process;marketing buzz;social network;maximization;artificial intelligence;greedy algorithm;computer science	ML	-17.642559509320336	-44.12905117937673	126812
a738697dfb6153572249c929534aa7cfb2db1699	material quality assessment of silk nanofibers based on swarm intelligence		In this paper, we propose a novel approach for texture analysis based on artificial crawler model. Our method assumes that each agent can interact with the environment and each other. The evolution process converges to an equilibrium state according to the set of rules. For each textured image, the feature vector is composed by signatures of the live agents curve at each time. Experimental results revealed that combining the minimum and maximum signatures into one increase the classification rate. In addition, we pioneer the use of autonomous agents for characterizing silk fibroin scaffolds. The results strongly suggest that our approach can be successfully employed for texture analysis. keywords: agent-based model, texture analysis, silk fibroin scaffolds 1 ar X iv :1 30 3. 31 52 v1 [ cs .C V ] 1 3 M ar 2 01 3	agent-based model;antivirus software;autonomous robot;feature vector;swarm intelligence;type signature;web crawler	Bruno Brandoli Machado;Wesley Nunes Gonçalves;Odemir Martinez Bruno	2013	CoRR	10.1088/1742-6596/410/1/012163	nanotechnology	AI	-6.331970859758846	-46.32714519407681	126845
5c03251e8bc68f90ac4250132324422d42a65869	identifying user attributes through non-i.i.d. multi-instance learning	transportation network;network centrality;support vector machines kernel conferences social network services learning systems accuracy educational institutions;social networking online;social network services non i i d multiinstance learning personalized recommendation targeted advertising sns postings machine learning methods automatic user attribute identification performance improvement;learning artificial intelligence;social networking online learning artificial intelligence	User attribute is an essential factor for personalized recommendation and targeted advertising. Therefore, there have been a number of studies to identify user attributes automatically from SNS postings, since the postings reveal various attributes of writers. Many kinds of machine learning methods have been applied to automatic identification of user attributes as a candidate solution, but they suffer from two major problems. First, there are many postings in SNS that do not deliver any information about writers. Then, learning from SNS postings results in a biased model by these irrelevant postings. Second, the postings of a SNS user are somewhat related one another. However, most machine learning methods ignore this information, since they assume that data are independently and identically distributed. In order to solve these problems in user attribute identification, this paper proposes a novel method based on non-i.i.d. multi-instance learning. Since multi-instance learning treats all postings by a user as a bag and learns user attribute identification with such bags, not with postings, the first problem is solved. In addition, the proposed method assumes that the postings by a single user have a structure. By incorporating this assumption into the multi-instance learning, the second problem is solved. Our experimental results show that consideration of these two problems in automatic user attribute identification results in performance improvement.	automatic identification and data capture;machine learning;personalization;regular expression;relevance	Hyun-Je Song;Jeong Woo Son;Seong-Bae Park	2013	2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)	10.1145/2492517.2492597	flow network;computer science;artificial intelligence;machine learning;data mining;world wide web	AI	-18.69897126043745	-49.14517041929522	126869
5562fe46262a6849c149a445035ee9fc3fc118a0	probabilistic model for discovering topic based communities in social networks	community detection;probabilistic method;probabilistic methods;probabilistic model;social network;large scale;social networks;interaction pattern;bayesian model	Social graphs have received renewed interest as a research topic with the advent of social networking websites. These online networks provide a rich source of data to study user relationships and interaction patterns on a large scale. In this paper, we propose a generative Bayesian model for extracting latent communities from a social graph. We assume that community memberships depend on topics of interest between users and the link relationships between them in the social graph topology. In addition, we make use of the nature of interaction to gauge user interests. Our model allows communities to be related to multiple topics and each user in the graph can be a member of multiple communities. This gives an insight into user interests and topical distribution in communities. We show the effectiveness of our model using a real world data set and also compare our model with existing community discovery methods.	bayesian network;social network;statistical model;topological graph theory	Mrinmaya Sachan;Danish Contractor;Tanveer A. Faruquie;L. Venkata Subramaniam	2011		10.1145/2063576.2063963	data science;probabilistic method;machine learning;data mining;database;world wide web;statistics;social network	ML	-18.70984972920695	-44.01257355465546	126934
995400a5cab4a1520e1a34c608d6ae9485baf2d3	community detection in scale-free networks: approximation algorithms for maximizing modularity	communities approximation algorithms approximation methods algorithm design and analysis social network services vectors internet;complex networks;approximation theory;internet;modularity maximization algorithm community detection scale free networks approximation algorithms scale free architecture network science;social networks network science approximation algorithm community structure modularity;internet approximation theory complex networks	Many networks, indifferent of their function and scope, converge to a scale-free architecture in which the degree distribution approximately follows a power law. Meanwhile, many of those scale-free networks are found to be naturally divided into communities of densely connected nodes, known as community structure. Finding this community structure is a fundamental but challenging topic in network science. Since Newman's suggestion of using modularity as a measure to qualify the strength of community structure, many efficient methods that find community structure based on maximizing modularity have been proposed. However, there is a lack of approximation algorithms that provide provable quality bounds for the problem. In this paper, we propose polynomial-time approximation algorithms for the modularity maximization problem together with their theoretical justifications in the context of scale-free networks. We prove that the solutions of the proposed algorithms, even in the worst-case, are optimal up to a constant factor for scale-free networks with either bidirectional or unidirectional links. Even though our focus in this work is not on designing another empirically good algorithms to detect community structure, experiments on real-world networks suggest that the proposed algorithm is competitive with the state-of-the-art modularity maximization algorithm.	approximation algorithm;best, worst and average case;complex network;context-free grammar;converge;degree distribution;entropy maximization;expectation–maximization algorithm;experiment;mathematical optimization;network science;polynomial;provable security;semantic network;time complexity	Thang N. Dinh;My T. Thai	2013	IEEE Journal on Selected Areas in Communications	10.1109/JSAC.2013.130602	mathematical optimization;the internet;evolving networks;modularity;theoretical computer science;machine learning;mathematics;clique percolation method;approximation algorithm;complex network;approximation theory	ML	-16.02095104403821	-43.35044637441992	126981
af36dbd55085dcca2f8a268386b0dec6113f6dbf	qubais reed ghazala: circuit-bending: build your own alien instruments	general			Ilias Anagnostopoulos	2008	Computer Music Journal	10.1162/comj.2008.32.4.84	speech recognition;alien;computer science;circuit bending	DB	-10.53501397842644	-50.26424558945801	127367
62c95eb8ccc24aa83202af8a04516594fbc645e9	a graph summarization: a survey		While advances in computing resources have made processing enormous amounts of data possible, human ability to identify patterns in such data has not scaled accordingly. Thus, efficient computational methods for condensing and simplifying data are becoming vital for extracting actionable insights. In particular, while data summarization techniques have been studied extensively, only recently has summarizing interconnected data, or graphs, become popular. This survey is a structured, comprehensive overview of the state-of-the-art methods for summarizing graph data. We first broach the motivation behind and the challenges of graph summarization. We then categorize summarization approaches by the type of graphs taken as input and further organize each category by core methodology. Finally, we discuss applications of summarization on real-world graphs and conclude by describing some open problems in the field.	adjacency matrix;anomaly detection;approximation algorithm;automatic summarization;categorization;computation;data mining;deep learning;general-purpose modeling;graph (discrete mathematics);layer (electronics);lazy evaluation;lossless compression;moore neighborhood;natural language processing;open research;orientation (graph theory);real life;real-time web;social network;sparse matrix;surround sound;taxonomy (general);time series;wang tile;web 2.0;word lists by frequency	Yike Liu;Abhilash Dighe;Tara Safavi;Danai Koutra	2016	CoRR		text graph;multi-document summarization;computer science;data science;theoretical computer science;automatic summarization;data mining;world wide web;information retrieval	ML	-11.07722630157893	-39.293678950226315	127624
f9b7d9de8634b60d43711d33180b7bcf618da1cf	community evolution in dynamic social networks -- challenges and problems	community evolution;graph theory;dynamic social network;evolutionary analysis dynamic social network community mining community evolution;information network;community evolution graph structure information networks dynamic social network analysis iteration pattern;dynamic social networks;iterative methods;social network;evolutionary analysis;community mining;social networking online;social network analysis;structural properties;social networking online graph theory iterative methods;communities social network services heuristic algorithms accuracy measurement predictive models data mining	Information networks that describe the relationship between individuals are called social networks and are usually modeled by a graph structure. Social network analysis is the study of these information networks which leads to uncover patterns of interaction among the entities. Most social networks are dynamic, and studying the evolution of these networks over time could provide insight into the changes that occurred in the iteration patterns and also the future trends of the networks. Furthermore, in a dynamic scenario, communities, which are groups of densely interconnected nodes, are affected by changes in the underlying population. The analysis of communities and their evolutions can help determine the characteristics and structural properties of the network. Here, we provide a brief overview of the existing research in the area of dynamic social network analysis, their limitations, and the challenges that are exists for further analysis.	entity;evolution;iteration;social network analysis	Mansoureh Takaffoli	2011	2011 IEEE 11th International Conference on Data Mining Workshops	10.1109/ICDMW.2011.52	organizational network analysis;social network analysis;evolving networks;computer science;dynamic network analysis;graph theory;machine learning;hierarchical network model;data mining;management science;iterative method;interdependent networks;social network	DB	-17.114824976962492	-41.738571894734676	127800
0bcb01ee32f8a82545186886561a64a0ffada114	infusing collaborative recommenders with distributed representations	distributed representations;neural networks;collaborative filtering;recommender systems	Recommender systems assist users in navigating complex information spaces and focus their attention on the content most relevant to their needs. Often these systems rely on user activity or descriptions of the content. Social annotation systems, in which users collaboratively assign tags to items, provide another means to capture information about users and items. Each of these data sources provides unique benefits, capturing different relationships.  In this paper, we propose leveraging multiple sources of data: ratings data as users report their affinity toward an item, tagging data as users assign annotations to items, and item data collected from an online database. Taken together, these datasets provide the opportunity to learn rich distributed representations by exploiting recent advances in neural network architectures. We first produce representations that subjectively capture interesting relationships among the data. We then empirically evaluate the utility of the representations to predict a user's rating on an item and show that it outperforms more traditional representations. Finally, we demonstrate that traditional representations can be combined with representations trained through a neural network to achieve even better results.	affinity analysis;artificial neural network;recommender system	Greg Zanotti;Miller Horvath;Lucas Nunes Barbosa;Venkata Trinadh Kumar Gupta Immedisetty;Jonathan Gemmell	2016		10.1145/2988450.2988455	computer science;collaborative filtering;machine learning;data mining;world wide web;information retrieval;artificial neural network	ML	-17.93046163634951	-48.08144718746884	127874
0a65ea2877c9d8538e5ca5e8bf46182b16eba600	understanding spreading patterns on social networks based on network topology	analytical models;discussion forums;role analysis;biological system modeling;expertise networks;receivers;internet;diseases;twitter	Ever since the introduction of the first epidemic model, scientists have tried extrapolating the damage caused by a contagious disease, given its spreading pattern in the premature stage. However, understanding epidemiology remains an elusive mystery to researchers specifically because of the unavailability of large amount of data. We utilise the study of diffusion of memes in a social networking website to solve this problem. In this paper, we analyse the impact of specific meso-scale properties of a network on a meme traversing over it. We have employed SCCP (Scale free, Communities, Core Periphery structure) networks for analysis purpose. We propose a new meme propagation model for real world social networks and observe the cause of virality of a meme. We have tested and validated our model with the real world information spreading pattern.	extrapolation;meme;mesoscopic physics;natural language understanding;network topology;social network;software propagation;sparse conditional constant propagation;unavailability	Akrati Saxena;Sudarshan Iyengar;Yayati Gupta	2015	2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1145/2808797.2809360	the internet;social science;simulation;artificial intelligence;machine learning;sociology;operations research;law;world wide web	Metrics	-19.113004584100892	-42.3594716642698	127910
782b6774d46da068f4691bcd083ef1fc248865a1	feasibility study of multi-agent simulation at the cellular level with flame gpu		Multi-Agent Systems (MAS) are a common approach to simulating biological systems. Multi-agent modelling provides a natural method for describing individual level behaviours of cells. However, the computation cost of simulating behaviours at an individual level is considerably larger than top down equation based modelling approaches. A recent possibility to improve computational performance is the use of Graphics Processing Units (GPUs) to provide the necessary parallel computing power. In this paper we show that multiagent models describing biological systems at cellular level are well suited to GPU acceleration. Cellular level systems are characterised by vast numbers of agents that intensively communicate, indirectly through diffusion of chemical substances, or directly, through connection of chemical receptors. We present a study which utilises the FLAME GPU software to target a MAS model of a generic pathogen induced infection to validate the suitability of the GPU for simulation of a broader class of cellular level systems.	agent-based model;biological system;central processing unit;computation;flame (malware);graphics processing unit;image scaling;instant messaging;lisp machine;multi-agent system;parallel computing;requirement;simulation;top-down and bottom-up design;x-machine	Alcione de Paiva Oliveira;Paul Richmond	2016			theoretical computer science;software;graphics;computation;computer science;top-down and bottom-up design;multi-agent system	Metrics	-5.463943813153795	-50.702931942614924	128123
75bb5c595402b03174b42adf54be51833c49df32	species coexistence of communities with intraguild predation: the role of refuges used by the resource and the intraguild prey	invasion analysis;intraguild predation;coexistence;refuges	In this paper, we develop a three-species intraguild predation model which incorporates refuges used by the resource and the intraguild prey, and focus on the effects of refuges on the three species coexistence. The invasion condition and parameter region for coexistence are obtained using invasion analysis. The new invasion condition requires that all boundary states with one missing species can be invaded by the missing species. Numerical simulations show that refuges have a major influence on species coexistence of intraguild predation system, and the results strongly depend on the types of refuges introduced into the model. Our study also shows that prey's refuges are detrimental to species coexistence except the resource using refuges. In contrast to previous research, we find that spatial structure may play an important role in effects of refuges on species coexistence of intraguild predation systems. Our results may shed new light on understanding the mechanisms and the persistence of multi-species predators-prey system.		Zhiguang Liu;Fengpan Zhang	2013	Bio Systems	10.1016/j.biosystems.2013.07.010	biology;ecology	AI	-5.499737259533132	-45.866258952970334	128138
9fa3e53b5937a0ec92499ed415e339ede6c92010	deepinf: social influence prediction with deep learning		Social and information networking activities such as on Facebook, Twitter, WeChat, and Weibo have become an indispensable part of our everyday life, where we can easily access friendsu0027 behaviors and are in turn influenced by them. Consequently, an effective social influence prediction for each user is critical for a variety of applications such as online recommendation and advertising. Conventional social influence prediction approaches typically design various hand-crafted rules to extract user- and network-specific features. However, their effectiveness heavily relies on the knowledge of domain experts. As a result, it is usually difficult to generalize them into different domains. Inspired by the recent success of deep neural networks in a wide range of computing applications, we design an end-to-end framework, DeepInf, to learn usersu0027 latent feature representation for predicting social influence. In general, DeepInf takes a useru0027s local network as the input to a graph neural network for learning her latent social representation. We design strategies to incorporate both network structures and user-specific features into convolutional neural and attention networks. Extensive experiments on Open Academic Graph, Twitter, Weibo, and Digg, representing different types of social and information networks, demonstrate that the proposed end-to-end model, DeepInf, significantly outperforms traditional feature engineering-based approaches, suggesting the effectiveness of representation learning for social applications.	artificial neural network;deep learning;end-to-end principle;experiment;feature engineering;feature learning;machine learning;subject-matter expert	Jiezhong Qiu;Jian Tang;Hao Ma;Yuxiao Dong;Kuansan Wang;Jie Tang	2018		10.1145/3219819.3220077	machine learning;social representation;computer science;artificial neural network;deep learning;social influence;feature engineering;social network;artificial intelligence;feature learning;everyday life	ML	-17.992369252355214	-46.62217039855762	128883
79d7fb18b5af1c88e447ee7a3969ed2f77c4b13b	two birds with one stone: an efficient hierarchical framework for top-k and threshold-based string similarity search	silicon;threshold based string similarity search pruning techniques hs tree hierarchical segment tree index data integration data cleaning top k string similarity search;会议论文;indexes heuristic algorithms search problems blogs partitioning algorithms upper bound silicon;upper bound;indexes;heuristic algorithms;tree searching data integration string matching;search problems;blogs;partitioning algorithms	String similarity search is a fundamental operation in data cleaning and integration. It has two variants, threshold-based string similarity search and top-k string similarity search. Existing algorithms are efficient either for the former or the latter; most of them can't support both two variants. To address this limitation, we propose a unified framework. We first recursively partition strings into disjoint segments and build a hierarchical segment tree index (HS-Tree) on top of the segments. Then we utilize the HS-Tree to support similarity search. For threshold-based search, we identify appropriate tree nodes based on the threshold to answer the query and devise an efficient algorithm (HS-Search). For top-k search, we identify promising strings with large possibility to be similar to the query, utilize these strings to estimate an upper bound which is used to prune dissimilar strings, and propose an algorithm (HS-Topk). We also develop effective pruning techniques to further improve the performance. Experimental results on real-world datasets show our method achieves high performance on the two problems and significantly outperforms state-of-the-art algorithms.	hs algorithm;ibm notes;plasma cleaning;recursion;regular expression;search algorithm;segment tree;similarity search;string metric;unified framework;web search query;work breakdown structure	Zhongjing Wang;Guoliang Li;Dong Deng;Yong Zhang;Jianhua Feng	2015	2015 IEEE 31st International Conference on Data Engineering	10.1109/ICDE.2015.7113311	beam search;database index;commentz-walter algorithm;computer science;theoretical computer science;machine learning;boyer–moore string search algorithm;data mining;database;incremental heuristic search;search tree;silicon;upper and lower bounds;ternary search tree;string metric;string searching algorithm;search algorithm	DB	-8.250917155334111	-39.6258440130656	128893
7055cfa57eead059da47d117005dfc9e722e9f02	a diffusion model for maximizing influence spread in large networks		Influence spread is an important phenomenon that occurs in many social networks. Influence maximization is the corresponding problem of finding the most influential nodes in these networks. In this paper, we present a new influence diffusion model, based on pairwise factor graphs, that captures dependencies and directions of influence among neighboring nodes. We use an augmented belief propagation algorithm to efficiently compute influence spread on this model so that the direction of influence is preserved. Due to its simplicity, the model can be used on large graphs with high-degree nodes, making the influence maximization problem practical on large, real-world graphs. Using large Flixster and Epinions datasets, we provide experimental results showing that our model predictions match well with ground-truth influence spreads, far better than other techniques. Furthermore, we show that the influential nodes identified by our model achieve significantly higher influence spread compared to other popular models. The model parameters can easily be learned from basic, readily available training data. In the absence of training, our approach can still be used to identify influential seed nodes.		Tu-Thach Quach;Jeremy D. Wendt	2016		10.1007/978-3-319-47880-7_7	data mining;belief propagation;machine learning;computer science;factor graph;marginal distribution;artificial intelligence;diffusion (business);social network;pairwise comparison;phenomenon;maximization	ML	-17.039685206414674	-43.56188783690103	129154
45510240dbe291d098e709673585212ae21e3533	tracing community genealogy: how new communities emerge from the old		The process by which new communities emerge is a central research issue in the social sciences. While a growing body of research analyzes the formation of a single community by examining social networks between individuals, we introduce a novel community-centered perspective. We highlight the fact that the context in which a new community emerges contains numerous existing communities. We reveal the emerging process of communities by tracing their early members’ previous community memberships. Our testbed is Reddit, a website that consists of tens of thousands of user-created communities. We analyze a dataset that spans over a decade and includes the posting history of users on Reddit from its inception to April 2017. We first propose a computational framework for building genealogy graphs between communities. We present the first large-scale characterization of such genealogy graphs. Surprisingly, basic graph properties, such as the number of parents and max parent weight, converge quickly despite the fact that the number of communities increases rapidly over time. Furthermore, we investigate the connection between a community’s origin and its future growth. Our results show that strong parent connections are associated with future community growth, confirming the importance of existing community structures in which a new community emerges. Finally, we turn to the individual level and examine the characteristics of early members. We find that a diverse portfolio across existing communities is the most important predictor for becoming an early member		Chenhao Tan	2018			graph property;tracing;social network;testbed;computer science;genealogy;graph;portfolio	Web+IR	-17.73066662343174	-40.82669162322214	129590
756ce01cb01c08356f589982a840968967554bf4	detecting change processes in dynamic networks by frequent graph evolution rule mining	databases;topology;frequency measurement;data mining;network topology;heuristic algorithms;conferences	The analysis of the temporal evolution of dynamic networks is a key challenge for understanding complex processes hidden in graph structured data. Graph evolution rules capture such processes on the level of small subgraphs by describing frequently occurring structural changes within a network. Existing rule discovery methods make restrictive assumptions on the change processes present in networks. We propose EvoMine, a frequent graph evolution rule mining method that, for the first time, supports networks with edge insertions and deletions as well as node and edge relabelings. EvoMine defines embedding-based and event-based support as two novel measures to assess the frequency of rules. These measures are based on novel mappings from dynamic networks to databases of union graphs that retain all evolution information relevant for rule mining. Using these mappings the rule mining problem can be solved by frequent subgraph mining. We evaluate our approach and two baseline algorithms on several real datasets. To the best of our knowledge, this is the first empirical comparison of rule mining algorithmsfor dynamic networks.	algorithm;association rule learning;baseline (configuration management);database;evolution;graph (abstract data type);sensor	Erik Scharwächter;Emmanuel Müller;Jonathan F. Donges;Marwan Hassani;Thomas Seidl	2016	2016 IEEE 16th International Conference on Data Mining (ICDM)	10.1109/ICDM.2016.0158	computer science;machine learning;pattern recognition;data mining;mathematics;molecule mining;network topology	DB	-8.312518147311206	-38.91080217167898	129629
8e820679fcac844dce8691a92b5f332b44ee2145	efficiently extracting frequent subgraphs using mapreduce	graph theory;tree searching data mining graph theory;data mining;isomorphism testing free approach mapreduce frequent subgraph extraction data mining breadth first search strategy;tree searching;data mining encoding chemical compounds indexes testing educational institutions search problems	Frequent subgraph extraction from a large number of small graphs is a primitive operation for many data mining applications. To extract frequent subgraphs, existing techniques need to enumerate a large number of subgraphs which is superlinear with the cardinality of the dataset. Given the rapid growing volume of graph data, it is difficult to perform the frequent subgraph extraction on a centralized machine efficiently. In this paper, we investigate how to efficiently perform this extraction over very large datasets using MapReduce. Parallelizing existing techniques directly using MapReduce does not yield good performance as it is difficult to balance the workload among the compute nodes. We therefore propose a framework that adopts the breadth first search strategy to iteratively extract frequent subgraphs, i.e., all frequent size-(i+1) subgraphs are generated based on frequent size-i subgraphs at the ith iteration using a single MapReduce job. To efficiently extract frequent subgraphs, we propose an isomorphism-testing-free approach by properly maintaining how frequent subgraphs are mapped within each graph. Extensive experiments conducted on our in-house clusters demonstrate the superiority of our proposed solution in comparison with the baseline approach.	apache hadoop;baseline (configuration management);breadth-first search;centralized computing;data mining;enumerated type;experiment;iteration;mapreduce;parallel computing;scalability	Wei Lu;Gang Chen;Anthony K. H. Tung	2013	2013 IEEE International Conference on Big Data	10.1109/BigData.2013.6691633	theoretical computer science;data mining;database;mathematics	DB	-8.429546574395467	-40.96063154900828	129684
32faf612e008938af3b793b458db20487e50f979	learning to predict opinion share and detect anti-majority opinionists in social networks	social networks;opinion dynamics;parameter learning	We address the problem of detecting anti-majority opinionists using the value-weighted mixture voter (VwMV) model. This problem is motivated by the fact that 1) each opinion has its own value and an opinion with a higher value propagates more easily/rapidly and 2) there are always people who have a tendency to disagree with any opinion expressed by the majority. We extend the basic voter model to include these two factors with the value of each opinion and the anti-majoritarian tendency of each node as new parameters, and learn these parameters from a sequence of observed opinion data over a social network. We experimentally show that it is possible to learn the opinion values correctly using a short observed opinion propagation data and to predict the opinion share in the near future correctly even in the presence of anti-majoritarians, and also show that it is possible to learn the anti-majoritarian tendency of each node if longer observation data is available. Indeed, the learned model can predict the future opinion share much more accurately than a simple polynomial extrapolation can do. Ignoring these two factors substantially degrade the performance of share prediction. We also show theoretically that, in a situation where the local opinion share can be approximated by the average opinion share, 1) when there are no anti-majoritarians, the opinion with the highest value eventually takes over, but 2) when there are a certain fraction of anti-majoritarians, it is not necessarily the case that the opinion with the highest value prevails and wins, and further, 3) in both cases, when the opinion values are uniform, the opinion share prediction problem becomes ill-defined and any opinion can win. The simulation results support that this holds for typical real world social networks. These theoretical results help understand the long term behavior of opinion propagation.	approximation algorithm;experiment;extrapolation;heuristic;numerical analysis;online and offline;polynomial;quantum field theory;sensor;simulation;social network;software propagation;statistical model;voter model	Masahiro Kimura;Kazumi Saito;Kouzou Ohara;Hiroshi Motoda	2012	Journal of Intelligent Information Systems	10.1007/s10844-012-0222-7	data mining	ML	-17.737982558817457	-44.14055320712981	129786
c09e9047e8d3d01b91c7b6a9c8a00ecf317fa251	scalable rfm-enriched representation learning for churn prediction		Most of the recent studies on churn prediction in telco utilize social networks built on top of the call (and/or SMS) graphs to derive informative features. However, extracting features from large graphs, especially structural features, is an intricate process both from a methodological and computational perspective. Due to the former, feature extraction in the current literature has mainly been addressed in an ad-hoc and hand-crafted manner. Due to the latter, the full potential of the structural information is unexploited. In this work, we incorporate both interaction and structural information by devising two different ways of enriching original graphs with interaction information, delineated by the well-known RFM model. We circumvent the process of extensive manual feature engineering by enriching the networks and improving the scalability of the renowned node2vec approach to learn node representations. The obtained results demonstrate that our enriched network outperforms baseline RFM-based methods.	baseline (configuration management);computation;experiment;feature engineering;feature extraction;gene prediction;hoc (programming language);interaction information;machine learning;scalability;social network;turing completeness;unsupervised learning;whole earth 'lectronic link	Sandra Mitrovic;Gaurav Singh;Bart Baesens;Wilfried Lemahieu;Jochen De Weerdt	2017	2017 IEEE International Conference on Data Science and Advanced Analytics (DSAA)	10.1109/DSAA.2017.42	data mining;scalability;feature extraction;social network;feature engineering;machine learning;wireless ad hoc network;artificial intelligence;graph;feature learning;interaction information;computer science	DB	-14.33573903384175	-45.39550538272321	129856
317a571f5a41a24537605f6d52764945dd32bdde	clustering evolving networks		Roughly speaking, clustering evolving networks aims at detecting structurally dense subgroups in networks that evolve over time. This implies that the subgroups we seek for also evolve, which results in many additional tasks compared to clustering static networks. We discuss these additional tasks and difficulties resulting thereof and present an overview on current approaches to solve these problems. We focus on clustering approaches in online scenarios, i.e., approaches that incrementally use structural information from previous time steps in order to incorporate temporal smoothness or to achieve low running time. Moreover, we describe a collection of real world networks and generators for synthetic data that are often used for evaluation.	algorithm;benchmark (computing);cluster analysis;computer cluster;evolving networks;sensor;synthetic data;time complexity	Tanja N Hartmann;Andrea Kappes;Dorothea Wagner	2016		10.1007/978-3-319-49487-6_9	evolving networks;artificial intelligence;machine learning;data mining;mathematics	AI	-12.055520605552902	-40.6990865027288	130021
9fb9a578c5d7434bb09b0a59c4f3d1eb5344a777	network application profiling with traffic causality graphs	network application profiling;graph mining;identification;traffic causality graph	A network application profiling framework is proposed that is based on traffic causality graphs (TCGs), representing temporal and spatial causality of flows to identify application programs. The proposed framework consists of three modules: the feature vector space construction using discriminative patterns extracted from TCGs by a graph-mining algorithm; a feature vector supervised learning procedure in the constructed vector space; and an application identification program using a similarity measure in the feature vector space. Accuracy of the proposed framework for application identification is evaluated, making use of ground truth packet traces from seven peer-to-peer (P2P) application programs. It is demonstrated that this framework achieves an overall 90:0% accuracy in application identification. Contributions are twofold: (1) using a graph-mining algorithm, the proposed framework enables automatic extraction of discriminative patterns serving as identification features; 2) high accuracy in application identification is achieved, notably for P2P applications that are more difficult to identify because of their using random ports and potential communication encryption. Copyright © 2014 John Wiley & Sons, Ltd	algorithm;causality;encryption;feature vector;ground truth;john d. wiley;network packet;peer-to-peer;similarity measure;simple features;stemming;structure mining;supervised learning;tracing (software)	Hirochika Asai;Kensuke Fukuda;Patrice Abry;Pierre Borgnat;Hiroshi Esaki	2014	Int. Journal of Network Management	10.1002/nem.1865	identification;computer science;theoretical computer science;machine learning;data mining	ML	-13.961217579377747	-45.42902615685785	130139
f7d8724844a0875cbfb52ed46c61a5ac6c9962c0	modeling social networks with overlapping communities using hypergraphs and their line graphs	preferential attachment;information network;degree distribution;social network;network model;average path length;network theory;weighted graph;power law;line graph	We propose that hypergraphs can be used to model social networks with overlapping communities. The nodes of the hypergraphs represent the communities. The hyperlinks of the hypergraphs denote the individuals who may participate in multiple communities. The hypergraphs are not easy to analyze, however, the line graphs of hypergraphs are simple graphs or weighted graphs, so that the network theory can be applied. We define the overlapping depth k of an individual by the number of communities that overlap in that individual, and we prove that the minimum adjacency eigenvalue of the corresponding line graph is not smaller than −kmax, which is the maximum overlapping depth of the whole network. Based on hypergraphs with preferential attachment, we establish a network model which incorporates overlapping communities with tunable overlapping parameters k and w. By comparing with the Hyves social network, we show that our social network model possesses high clustering, assortative mixing, power-law degree distribution and short average path length.	assortative mixing;attachments;average path length;cluster analysis;degree distribution;hyperlink;line graph;network model;network theory;social network analysis	Dajie Liu;Norbert Blenn;Piet Van Mieghem	2010	CoRR		network theory;power law;combinatorics;discrete mathematics;degree distribution;average path length;network model;machine learning;mathematics;line graph;social network	Theory	-16.467903809858935	-40.57032645300609	130313
37af4020a2f6cbb3485dd1d8e1c74ba9da0cbb58	an empirical study of the structure of relevant keywords in a search engine using the minimum spanning tree	online marketplace;keyword network;search keyword;pearson correlation;minimum spanning tree;relevant keyword;cosine similarity	"""This paper provides a comprehensive study of the structure of relevant keywords in a search engine using the minimum spanning tree (MST) approach. In the process of constructing MST's, we introduce a novel metric to measure a distance between keywords by applying an integration of the Pearson correlation and the query-based cosine similarity. From this work, we made several meaningful observations about the networks of relevant keywords. First, keyword networks in a search engine exhibit the small-world effect and the scale-free property. Second, only a few among relevant keywords in the order of popularity are positively correlated and there is no significantly positive or negative relationship for the rest of relevant keywords. Third, the degree of searching activity for relevant keywords varies depending on whether they are branded keywords or non-branded keywords as well as the characteristics of product categories. Fourth, the mean correlation coefficient for keyword impressions during slow season increases. Finally, both k""""m""""a""""x and the betweenness centrality for high-involvement products are higher than those for low-involvement products."""	file spanning;minimum spanning tree;web search engine	Cookhwan Kim;Sungsik Park;Kwiseok Kwon;Woojin Chang	2012	Expert Syst. Appl.	10.1016/j.eswa.2011.09.147	pearson product-moment correlation coefficient;computer science;minimum spanning tree;machine learning;pattern recognition;data mining;mathematics;vector space model;information retrieval	Theory	-16.82690482122427	-40.888492215074855	130457
136bd3374e2c8cf2170762af3968c46c14dcada3	distributed methods for high-dimensional and large-scale tensor factorization	memory management;tensile stress;conference;tensile stress scalability matrix decomposition memory management optimization distributed databases tin;distributed computing;tensor factorization;recommender system;matrix decomposition;coordinate descent for tensor factorization high dimensional tensor factorization large scale tensor factorization distributed tensor factorization methods sals cdtf factor matrix mapreduce optimization techniques local disk caching greedy row assignment subset alternating least square;parallel processing data handling least squares approximations matrix decomposition;distributed databases;optimization;scalability;mapreduce;tin;mapreduce tensor factorization recommender system distributed computing	Given a high-dimensional and large-scale tensor, how can we decompose it into latent factors? Can we process it on commodity computers with limited memory? These questions are closely related to recommendation systems exploiting context information such as time and location. They require tensor factorization methods scalable with both the dimension and size of a tensor. In this paper, we propose two distributed tensor factorization methods, SALS and CDTF. Both methods are scalable with all aspects of data, and they show an interesting trade-off between convergence speed and memory requirements. SALS updates a subset of the columns of a factor matrix at a time, and CDTF, a special case of SALS, updates one column at a time. On our experiment, only our methods factorize a 5-dimensional tensor with 1B observable entries, 10M mode length, and 1K rank, while all other state-of-the-art methods fail. Moreover, our methods require several orders of magnitude less memory than the competitors. We implement our methods on MapReduce with two widely applicable optimization techniques: local disk caching and greedy row assignment.	column (database);computer;distributed algorithm;greedy algorithm;latent variable;mapreduce;mathematical optimization;observable;recommender system;requirement;scalability;southern adirondack library system	Kijung Shin;U. Kang	2014	2014 IEEE International Conference on Data Mining	10.1109/ICDM.2014.78	mathematical optimization;discrete mathematics;scalability;tin;computer science;theoretical computer science;mathematics;stress;matrix decomposition;recommender system;memory management	DB	-8.711896126156441	-42.66608267705478	130538
84dc1a1081caa4fb157fd60bb490338a0a8bdbd9	neural information processing		Recently, recommendation algorithms have been widely used to improve the benefit of businesses and the satisfaction of users in many online platforms. However, most of the existing algorithms generate intermediate output when predicting ratings and the error of intermediate output will be propagated to the final results. Besides, since most algorithms predict all the unrated items, some predicted ratings may be unreliable and useless which will lower the efficiency and effectiveness of recommendation. To this end, we propose a Low-rank and Sparse Matrix Completion (LSMC) method which recovers rating matrix directly to improve the quality of rating prediction. Following the common methodology, we assume the structure of the predicted rating matrix is low-rank since rating is just connected with some factors of user and item. However, different from the existing methods, we assume the matrix is sparse so some unreliable predictions will be removed and important results will be retained. Besides, a slack variable will be used to prevent overfitting and weaken the influence of noisy data. Extensive experiments on four real-world datasets have been conducted to verify that the proposed method outperforms the state-of-the-art recommendation algorithms.	algorithm;experiment;information processing;low-rank approximation;overfitting;recommender system;signal-to-noise ratio;slack variable;sparse matrix;the matrix	Derong Liu;Shengli Xie;Yuanqing Li	2017		10.1007/978-3-319-70139-4		AI	-16.494801290287455	-48.754556122631904	130639
9c667bb99e494ef65c1c5c979f43d041cb51ea55	the network-untangling problem: from interactions to activity timelines		In this paper we study a problem of determining when entities are active based on their interactions with each other. More formally, we consider a set of entities V and a sequence of time-stamped edges E among the entities. Each edge (u, v, t) ∈ E denotes an interaction between entities u and v that takes place at time t. We view this input as a temporal network. We then assume a simple activity model in which each entity is active during a short time interval. An interaction (u, v, t) can be explained if at least one of u or v are active at time t. Our goal is to reconstruct the activity intervals, for all entities in the network, so as to explain the observed interactions. This problem, which we refer to as the network-untangling problem, can be applied to discover timelines of events from complex interactions among entities. We provide two formulations for the network-untangling problem: (i) minimizing the total interval length over all entities, and (ii) minimizing the maximum interval length. We show that the sum problem is NP-hard, while, surprisingly, the max problem can be solved optimally in linear time, using a mapping to 2-SAT. For the sum problem we provide efficient and effective algorithms based on realistic assumptions. Furthermore, we complement our study with an evaluation on synthetic and real-world datasets, which demonstrates the validity of our concepts and the good performance of our algorithms.	2-satisfiability;academy;approximation algorithm;decision problem;entity;function model;graph (discrete mathematics);interaction;iterative method;nestor (encryption);np-hardness;rich internet application;synthetic intelligence;temporal logic;time complexity;timeline	Polina Rozenshtein;Nikolaj Tatti;Aristides Gionis	2017		10.1007/978-3-319-71249-9_42	discrete mathematics;vertex cover;complex network;timeline;linear programming;mathematics	ML	-13.244022318359116	-40.5721447647926	130694
8533292488f7ebe5a22ead7d151d52a07524c426	social network privacy for attribute disclosure attacks	ubiquitous computing computer crime data privacy graphs greedy algorithms social networking online;anonymization;privacy greedy algorithms partitioning algorithms diseases communities facebook;attribute disclosure;greedy algorithms;computer crime;attribute disclosure data privacy social networks anonymization algorithms;graphs;social network;data privacy;social networks;social networking online;greedy algorithm social network privacy attribute disclosure attack α proximity label distribution;greedy algorithm;algorithms;ubiquitous computing	Increasing research on social networks stresses the urgency for producing effective means of ensuring user privacy. Represented ubiquitously as graphs, social networks have a myriad of recently developed techniques to prevent identity disclosure, but the equally important attribute disclosure attacks have been neglected. To address this gap, we introduce an approach to anonymize social networks that have labeled nodes, $\alpha$-proximity, which requires that the label distribution in every neighbourhood of the graph be close to that throughout the entire network. We present an effective greedy algorithm to achieve $\alpha$-proximity and experimentally validate the quality of the solutions it derives.	experiment;greedy algorithm;social network	Sean Chester;Gautam Srivastava	2011	2011 International Conference on Advances in Social Networks Analysis and Mining	10.1109/ASONAM.2011.105	greedy algorithm;social science;computer science;data mining;internet privacy;world wide web;ubiquitous computing;social network	DB	-12.559696400744867	-43.62835727845585	130893
4cf950aeb88fc6abe6a23ff2b5957a3d364708fa	on the evolvability of different computational architectures using a common developmental genome		Artificial organisms comprise a method that enables the construction of complex systems with structural and/or computational properties. In this work we investigate whether a common developmental genome can favor the evolvability of different computational architectures. This is rather interesting, especially when limited computational resources is the case. The commonly evolved genome showed ability to boost the evolvability of the different computational architectures requiring fewer resources and in some cases, finding better solutions.	complex systems;computational resource	Konstantinos Antonakopoulos;Gunnar Tufte	2012			bioinformatics	NLP	-4.613905397709185	-47.55006936140495	130919
33337e801c57fa1345943fcfab7dfbe80920b1e1	scalability of local image descriptors: a comparative study	high dimensionality;query processing;stopping rule;pvs framework;local image descriptors;computer vision;large scale;indexation;comparative study;rank aggregation;scalability;high dimensional indexing;median rank aggregation	Computer vision researchers have recently proposed several local descriptor schemes. Due to lack of database support, however, these descriptors have only been evaluated using small image collections. Recently, we have developed the PvS-framework, which allows efficient querying of large local descriptor collections. In this paper, we use the PvSframework to study the scalability of local image descriptors. We propose a new local descriptor scheme and compare it to three other well known schemes. Using a collection of almost thirty thousand images, we show that the new scheme gives the best results in almost all cases. We then give two stop rules to reduce query processing time and show that in many cases only a few query descriptors must be processed to find matching images. Finally, we test our descriptors on a collection of over three hundred thousand images, resulting in over 200 million local descriptors, and show that even at such a large scale the results are still of high quality, with no change in query processing time.	approximation algorithm;cpu cache;computer vision;conjunctive query;database;display resolution;response time (technology);scalability;scale-invariant feature transform;search algorithm;tor messenger;visual descriptor	Herwig Lejsek;Friðrik Heiðar Ásmundsson;Björn Þór Jónsson;Laurent Amsaleg	2006		10.1145/1180639.1180760	computer vision;scalability;computer science;theoretical computer science;comparative research;data mining;information retrieval	Vision	-5.449783099030467	-41.241828759457796	131115
02346726af3cdb0c73ca16105f06f57c80ebe3aa	reciprocal versus parasocial relationships in online social networks		Many online social networks are fundamentally directed, i.e., they consist of both reciprocal edges (i.e., edges that have already been linked back) and parasocial edges (i.e., edges that have not been linked back). Thus, understanding the structures and evolutions of reciprocal edges and parasocial ones, exploring the factors that influence parasocial edges to become reciprocal ones, and predicting whether a parasocial edge will turn into a reciprocal one are basic research problems. However, there have been few systematic studies about such problems. In this paper, we bridge this gap using a novel large-scale Google+ dataset (available at http://www.cs.berkeley.edu/~stevgong/dataset.html/ ) crawled by ourselves as well as one publicly available social network dataset. First, we compare the structures and evolutions of reciprocal edges and those of parasocial edges. For instance, we find that reciprocal edges are more likely to connect users with similar degrees while parasocial edges are more likely to link ordinary users (e.g., users with low degrees) and popular users (e.g., celebrities). However, the impacts of reciprocal edges linking ordinary and popular users on the network structures increase slowly as the social networks evolve. Second, we observe that factors including user behaviors, node attributes, and edge attributes all have significant impacts on the formation of reciprocal edges. Third, in contrast to previous studies that treat reciprocal edge prediction as either a supervised or a semi-supervised learning problem, we identify that reciprocal edge prediction is better modeled as an outlier detection problem. Finally, we perform extensive evaluations with the two datasets, and we show that our proposal outperforms previous reciprocal edge prediction approaches.	anomaly detection;directed graph;flickr;google+;graph (discrete mathematics);network model;semi-supervised learning;semiconductor industry;social network;supervised learning	Neil Zhenqiang Gong;Wenchang Xu	2014	Social Network Analysis and Mining	10.1007/s13278-014-0184-6	artificial intelligence;machine learning;mathematics	Web+IR	-17.38955847649754	-42.32886014877194	131152
7a1d73b9229b2bb18044075aae4db6d70d9a6502	efficient search ranking in social networks	shortest path;query processing;graphs;social network;social networks;time use	In social networks such as Orkut, www.orkut.com, a large portion of the user queries refer to names of other people. Indeed, more than 50% of the queries in Orkut are about names of other users, with an average of 1.8 terms per query. Further, the users usually search for people with whom they maintain relationships in the network. These relationships can be modelled as edges in a friendship graph, a graph in which the nodes represent the users. In this context, search ranking can be modelled as a function that depends on the distances among users in the graph, more specifically, of shortest paths in the friendship graph. However, application of this idea to ranking is not straightforward because the large size of modern social networks (dozens of millions of users) prevents efficient computation of shortest paths at query time. We overcome this by designing a ranking formula that strikes a balance between producing good results and reducing query processing time. Using data from the Orkut social network, which includes over 40 million users, we show that our ranking, augmented by this new signal, produces high quality results, while maintaining query processing time small.	computation;database;display resolution;experience;friendship graph;graph (discrete mathematics);random permutation;ranking (information retrieval);scalability;search algorithm;shortest path problem;social network;software deployment;text-based (computing);world wide web	Monique V. Vieira;Bruno M. Fonseca;Rodrigo Damazio;Paulo Braz Golgher;Davi de Castro Reis;Berthier A. Ribeiro-Neto	2007		10.1145/1321440.1321520	web query classification;ranking;computer science;theoretical computer science;machine learning;data mining;database;world wide web;social network	DB	-9.934738787156787	-40.24700039534965	131434
8e7958a75fac8e045864e8192ff383ee909f2837	query driven algorithm selection in early stage retrieval		Large scale retrieval systems often employ cascaded ranking architectures, in which an initial set of candidate documents are iteratively refined and re-ranked by increasingly sophisticated and expensive ranking models. In this paper, we propose a unified framework for predicting a range of performance-sensitive parameters based on minimizing end-to-end effectiveness loss. The framework does not require relevance judgments for training, is amenable to predicting a wide range of parameters, allows for fine tuned efficiency-effectiveness trade-offs, and can be easily deployed in large scale search systems with minimal overhead. As a proof of concept, we show that the framework can accurately predict a number of performance parameters on a query-by-query basis, allowing efficient and effective retrieval, while simultaneously minimizing the tail latency of an early-stage candidate generation system. On the 50 million document ClueWeb09B collection, and across 25,000 queries, our hybrid system can achieve superior early-stage efficiency to fixed parameter systems without loss of effectiveness, and allows more finely-grained efficiency-effectiveness trade-offs across the multiple stages of the retrieval system.	algorithm selection;end-to-end encryption;experiment;hybrid system;intel developer zone;multiple encryption;overhead (computing);relevance;unified framework	Joel Mackenzie;J. Shane Culpepper;Roi Blanco;Matt Crane;Charles L. A. Clarke;Jimmy Lin	2018		10.1145/3159652.3159676	latency (engineering);proof of concept;data mining;computer science;hybrid system;ranking;algorithm selection;data structure	Web+IR	-12.368011391055541	-49.229928575153906	131510
1417a79a3ae7d8a9edfe9652b0d23380b220703d	k-degree anonymity and edge selection: improving data utility in large networks		The problem of anonymization in large networks and the utility of released data are considered in this paper. Although there are some anonymization methods for networks, most of them cannot be applied in large networks because of their complexity. In this paper, we devise a simple and efficient algorithm for k-degree anonymity in large networks. Our algorithm constructs a k-degree anonymous network by the minimum number of edge modifications. We compare our algorithm with other well-known k-degree anonymous algorithms and demonstrate that information loss in real networks is lowered. Moreover, we consider the edge relevance in order to improve the data utility on anonymized networks. By considering the neighbourhood centrality score of each edge, we preserve the most important edges of the network, reducing the information loss and increasing the data utility. An evaluation of clustering processes is performed on our algorithm, proving that edge neighbourhood centrality increases data utility. Lastly, we apply our algorithm to different large real datasets and demonstrate their efficiency and practical utility.	algorithm;centrality;cluster analysis;data anonymization;relevance	Jordi Casas-Roma;Jordi Herrera-Joancomartí;Vicenç Torra	2016	Knowledge and Information Systems	10.1007/s10115-016-0947-7	computer science;theoretical computer science;machine learning;data mining	ML	-12.427925064401974	-43.32465844679522	131612
c6edc694e8a5af421981385da21545ec99d8fd65	epidemic thresholds in directed complex networks	complex network;dynamic model;information network;scale free;computer virus;steady state	The spread of a disease, a computer virus or information is discussed in a directed complex network. We are concerned with a steady state of the spread for the SIR and SIS dynamic models. In a scale-free directed network it is shown that the threshold of its outbreak in both models approaches zero under a high correlation between nodal indegrees and outdegrees.	complex network;computer virus;steady state	Shinji Tanimoto	2011	CoRR		combinatorics;simulation;telecommunications;computer science;artificial intelligence;scale-free network;mathematics;steady state;complex network;computer virus	Theory	-15.888616007832262	-39.73069158969486	132082
8a26b4a6f5642466604ed1f811450b6661ad981f	analysis of network by generalized mutual entropies	complex network;scale free network;random networks;network analysis;clustering;entropy;computer simulation;information	29208 Generalized mutual entropy is defined for networks and applied for analysis of complex network structures. The method is tested for the case of computer simulated scale free networks, random networks, and their mixtures. The possible applications for real network analysis are discussed.	complex network;computer simulation;flow network;mutual information;social network analysis	Vladimir Gudkov;V. Montealegre	2007	CoRR	10.1016/j.physa.2008.01.005	computer simulation;network science;entropy;information;network analysis;theoretical computer science;scale-free network;hierarchical network model;data mining;network simulation;cluster analysis;complex network	Theory	-16.037315162577226	-40.85050075503449	132117
8fb7e35bb768c653ea9866e3333116436238b996	on optimal link creation for facilitation of consensus in social networks	consensus;alternating direction method of multipliers;small world networks;sparsity;social networks;opinion dynamics;social network services equations optimization eigenvalues and eigenfunctions transient analysis steady state symmetric matrices;optimization;stochastic matrices alternating direction method of multipliers consensus degroot model opinion dynamics optimization small world networks social networks sparsity;optimal link creation consensus reaching efficiency link creation budget regular graph small world network sparse long range link optimization problem public opinion transient behavior degroot model social network;stochastic matrices;social sciences graph theory optimisation small world networks;degroot model	We consider the problem of reaching consensus in a social network of agents described by the DeGroot model. We develop a measure for the efficiency with which consensus is reached, where the measure quantifies the transient behavior of public opinion around the consensus value. We then propose an optimization problem that maximizes consensus-reaching efficiency via the creation of new social links, subject to a total link-creation budget. We employ the alternating direction method of multipliers, an algorithm well-suited to large optimization problems, to find the optimal location and weights of the new links. We demonstrate the utility of our results through an example, where we observe that for a social network described by a regular graph the addition of new links leads to an augmented graph that resembles a small-world network characterized by sparse long-range links.	algorithm;augmented lagrangian method;mathematical optimization;optimization problem;social network;sparse matrix	Makan Fardad;Fu Lin;Mihailo R. Jovanovi&#x0107;	2014	2014 American Control Conference	10.1109/ACC.2014.6859126	mathematical optimization;combinatorics;consensus;computer science;machine learning;mathematics;small-world network;sparsity-of-effects principle;social network	AI	-16.954981462201797	-43.080472709609516	132182
52eff07e095e2b9a531fb06f77d807471c04b1f0	méthodes qualitatives pour la construction et l'analyse des réseaux moléculaires sbgn. (qualitative methods for the construction and the analysis of sbgn molecular networks)		Two fundamental tasks of Systems Biology are the construction of molecular networks from experimental data, and their analysis with a view to discovering their emergent properties. With the increase of available experimental data, these two tasks can no longer be realized by hand. Based on this observation, numerous bioinformatics methods aiming at the automation of these two task have been developped. In parallel, standards aiming at defining and organizing terms of systems biology, or representing networks and mathematical models, have been developped. Among these standards, the Standard Biology Graphical Notation is composed of three languages that allow the representation of molecular networks. The two main SBGN languages are SBGN-PD for the representation of reaction networks, and SBGN-AF for the representation of influence graphs. The SBGN notation not only standardizes the representation of networks, but also gives the concepts of systems biology that are most often used to express knowledge of the field. Our work takes its root in this general background. We have developped a number of methods to construct molecular networks and analyze their dynamics. All the methods that we propose are based on qualitative formalisms, such as logics or automata networks. These formalisms have solid theoretical bases and can be used by numerous pieces of software. All our methods also rely on the biological concepts given by the SBGN standard, and can therefore be blended in the same theoretical framework. First, we introduce two sets of predicates that allow to translate any SBGN-PD or SBGN-AF network into a set of ground atoms. Then, we show how these sets of predicates can be used to reason on networks, by proposing a transformation method of SBGN-PD signaling networks into SBGN-AF influence graphs. Second, we present a first-order logic based method to construct signaling networks from experimental results. This method formalizes and automatizes biologists’ reasoning using explicit reasoning rules. On the contrary to existing methods, it allows to take into account numerous types of experimental results while reconstructing precise molecular mecanisms. Third, we show a new method to compute the finite traces and attractor points of Boolean networks that model SBGN-AF networks and that are parameterized using general principles. Finally, we introduce two new qualitative semantics for the computation of the dynamics of SBGN-PD reaction networks. These semantics are expressed using automata networks. The first semantics extends the classical Boolean semantics by taking into account inhibitions. As to the second one, it relies on the concept of story which introduces a new point of view on reaction networks. Indeed, it allows to model different physical states of the same molecular entity using a unique variable. All the methods that we have developped show how qualitative formalisms can be used to reason on the relations represented by molecular networks in order to discorver new knowledge in systems biology. Université Paris-Saclay Espace Technologique / Immeuble Discovery Route de l’Orme aux Merisiers RD 128 / 91190 Saint-Aubin, France 3	anisotropic filtering;automata theory;automaton;bioinformatics;boolean network;computation;espace;emergence;first-order logic;first-order predicate;graphical user interface;linear algebra;mathematical model;organizing (structure);point of view (computer hardware company);ruby document format;systems biology graphical notation;tracing (software)	Adrien Rougny	2016				Comp.	-7.118805642898667	-51.175163633453316	132213
1b945f12404e8f4cbf5b53ed02054a9e1fe0b50e	community detection in bibsonomy using data clustering		Community detection aims to extract the related groups of nodes from complex networks, by exploiting the network topology. Different approaches have been proposed for community detection, where most of them are based on clustering algorithms. In this paper we investigate how we can use the clustering for the community detection in the academic social bookmarking website: Bibsonomy. Our goal is to determine the most suitable clustering algorithm for similar user detection in Bibsonomy. To realize that, we have compared three clustering algorithms: The k-means, the k-medoids and the Agglomerative clustering algorithms. Experimental results demonstrate that k-means performs better than the other algorithms, for community detection in Bibsonomy.	algorithm;cluster analysis;complex network;k-means clustering;k-medoids;medoid;network topology	Zakaria Saoud;Jan Platos	2017		10.1007/978-3-319-67220-5_14	machine learning;knowledge management;data visualization;computer science;complex network;network topology;cluster analysis;hierarchical clustering;artificial intelligence	AI	-14.659361807466542	-42.82363329213827	132398
de5e4c789209dfed61d427675e20ccf7362b9375	influence maximization on social graphs: a survey		"""Influence Maximization (IM), which selects a set of <inline-formula><tex-math notation=""""LaTeX"""">$k$</tex-math> <alternatives><inline-graphic xlink:href=""""li-ieq1-2807843.gif""""/></alternatives></inline-formula> users (called seed set) from a social network to maximize the expected number of influenced users (called influence spread), is a key algorithmic problem in social influence analysis. Due to its immense application potential and enormous technical challenges, IM has been extensively studied in the past decade. In this paper, we survey and synthesize a wide spectrum of existing studies on IM from an <italic>algorithmic perspective</italic>, with a special focus on the following key aspects: (1) a review of well-accepted diffusion models that capture the information diffusion process and build the foundation of the IM problem, (2) a fine-grained taxonomy to classify existing IM algorithms based on their design objectives, (3) a rigorous theoretical comparison of existing IM algorithms, and (4) a comprehensive study on the applications of IM techniques in combining with novel context features of social networks such as topic, location, and time. Based on this analysis, we then outline the key challenges and research directions to expand the boundary of IM research."""	expectation–maximization algorithm;social network;xlink	Yuchen Li;Ju Fan;Yanhao Wang;Kian-Lee Tan	2018	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2018.2807843	machine learning;artificial intelligence;computer science;algorithm design;design objective;statistical classification;social network;social influence;diffusion process;stochastic process;maximization	DB	-17.08274315803226	-44.56402934837265	132464
b3f91a8f8c07da54ab661876aac6d5c2e1c13352	a clustering approach for structural k-anonymity in social networks using genetic algorithm	k anonymity;social network;genetic algorithm;structural information loss	With an abundance of social network data being released, the need to protect sensitive information within these networks has become an important concern of data publishers. One of privacy preserving approaches often used is anonymization. This paper focuses on the popular notion of k-anonymization, where k is the required threshold of structural anonymity. Given a social network graph, it transforms G to G', such that structural property of each node in G' is attained by at least k --- 1 other nodes in G'. The nodes are clustered together into supernodes of size at least k. The above being NP-hard optimization problem, a genetic algorithm is proposed to optimize it's structural k-anonymity. Edge generalization is then employed, based on their relationships to achieve indistinguishable nodes.	cluster analysis;collaboration graph;genetic algorithm;information sensitivity;mathematical optimization;np-hardness;optimization problem;social network	Vikas Kumar Sihag	2012		10.1145/2381716.2381850	computer science;artificial intelligence;machine learning;data mining	DB	-12.557297842216945	-43.45477860158346	132549
0c932ba0c8606377d3c028416a30ad5589135866	similarity searching in databases of flexible 3d structures using autocorrelation vectors derived from smoothed bounded distance matrices	3d structure;similarity search	This paper presents an exploratory study of a novel method for flexible 3-D similarity searching based on autocorrelation vectors and smoothed bounded distance matrices. Although the new approach is unable to outperform an existing 2-D similarity searching in terms of enrichment factors, it is able to retrieve different compounds at a given percentage of the hit-list and so may be a useful adjunct to other similarity searching methods.	autocorrelation;databases;gene ontology term enrichment;smoothing (statistical technique);thrombocytopenia	Nicholas Rhodes;David E. Clark;Peter Willett	2006	Journal of chemical information and modeling	10.1021/ci0503863	combinatorics;discrete mathematics;chemistry;machine learning;mathematics;nearest neighbor search;statistics	DB	-4.814367393951531	-44.216604631023166	132571
ccaab12c26dca37d066c496f068d7214ca679b9a	distributed community detection in complex networks	social network services;computers;distributed algorithms;community detection;complex networks;complexity theory;communities image edge detection social network services complexity theory computers measurement heuristic algorithms;measurement;social networking services;edge detection;complex network;similar nodes distributed community detection complex network analysis multiple community detection time complexity local modularity measure similarity measurement;distributed algorithms complex networks computational complexity;local modularity;image edge detection;computational complexity;heuristic algorithms;modularity;communities;modularity complex networks community detection distributed algorithms local modularity;distributed algorithm;heuristic algorithm	Network analysis is an important and interesting area of research with many applications in different domains. One of the challenges in network analysis is community detection. Community detection is the process of partitioning the network into some groups in such a way that there exist many interactions in the groups and few interactions among them. Toward improving time complexity and precision of community detection, a novel method is proposed in this paper. This method is able to detect multiple communities simultaneously. No prior knowledge is required about the number of communities or the structure of network in proposed algorithm. This algorithm is evaluated by modularity measure on different networks and the result shows improvements over existing methods. We use local modularity as the similarity measurement to collect similar nodes in one community. Our method is a kind of agglomerative approach.	algorithm;existential quantification;interaction;modularity (networks);social network analysis;time complexity	Zahra Masdarolomoor;Sadegh Aliakbary;Reza Azmi;Noushin Riahi	2011	2011 Third International Conference on Computational Intelligence, Communication Systems and Networks	10.1109/CICSyN.2011.66	distributed algorithm;computer science;modularity;theoretical computer science;machine learning;distributed computing;clique percolation method;community structure;complex network	AI	-14.613691226236261	-42.65712090602511	132690
18bb2bfbf33408573ab89443c580a35640b4aeb6	collaborative topic regression with denoising autoencoder for content and community co-representation		Personalized recommendation of items frequently faces scenarios where we have sparse observations on users' adoption of items. In the literature, there are two promising directions. One is to connect sparse items through similarity in content. The other is to connect sparse users through similarity in social relations. We seek to integrate both types of information, in addition to the adoption information, within a single integrated model. Our proposed method models item content via a topic model, and user communities via an autoencoder model, while bridging a user's community-based preference to her topic-based preference. Experiments on public real-life data showcase the utility of the model, particularly when there is significant compatibility between communities and topics.	autoencoder;baseline (configuration management);bridging (networking);latent variable;noise reduction;real life;sparse matrix;topic model	Trong T. Nguyen;Hady W. Lauw	2017		10.1145/3132847.3133128	autoencoder;information retrieval;topic model;data mining;social relation;computer science;bridging (networking)	Web+IR	-18.791629796503145	-46.802230937541594	132781
67dc19da49fb853308733413407aacf06333bb44	influence maximization in online social networks		Starting with the earliest studies showing that the spread of new trends, information, and innovations is closely related to the social influence exerted on people by their social networks, the research on social influence theory took off, providing remarkable evidence on social influence induced viral phenomena. Fueled by the extreme popularity of online social networks and social media, computational social influence has emerged as a subfield of data mining whose goal is to analyze and optimize social influence using computational frameworks such as algorithm design and theoretical modeling. One of the fundamental problems in this field is the problem of influence maximization, primarily motivated by the application of viral marketing. The objective is to identify a small set of users in a social network who, when convinced to adopt a product, shall influence others in the network in a manner that leads to a large number of adoptions.  In this tutorial, we extensively survey the research on social influence propagation and maximization, with a focus on the recent algorithmic and theoretical advances. To this end, we provide detailed reviews of the latest research effort devoted to (i) improving the efficiency and scalability of the influence maximization algorithms; (ii) context-aware modeling of the influence maximization problem to better capture real-world marketing scenarios; (iii) modeling and learning of real-world social influence; (iv) bridging the gap between social advertising and viral marketing.	algorithm design;bridging (networking);computation;computational complexity theory;data mining;entropy maximization;expectation–maximization algorithm;scalability;social advertising (social relationships);social media;social network;software propagation	Çigdem Aslay;Laks V. S. Lakshmanan;Wei Lu;Xiaokui Xiao	2018		10.1145/3159652.3162007	data mining;management science;viral marketing;scalability;computer science;bridging (networking);social media;social network;social influence;popularity;maximization	ML	-17.072970425968787	-44.34601498281028	132822
325d5606c874bfc3b652d7924e2d7bedcca3d96e	a reinforcement learning and recurrent neural network based dynamic user modeling system		With the exponential growth in areas of machine intelligence, the world has witnessed promising solutions to the personalized content recommendation. The ability of interactive learning agents to take optimal decisions in dynamic environments has been very well conceptualized and proven by Reinforcement Learning (RL). The learning characteristics of Deep-Bidirectional Recurrent Neural Networks (DBRNN) in both positive and negative time directions has shown exceptional performance as generative models to generate sequential data in supervised learning tasks. In this paper, we harness the potential of the said two techniques and strive to create personalized video recommendation through emotional intelligence by presenting a novel context-aware collaborative filtering approach where intensity of usersu0027 spontaneous non-verbal emotional response towards recommended video is captured through system-interactions and facial expression analysis for decision-making and video corpus evolution with real-time data streams. We take into account a useru0027s dynamic nature in the formulation of optimal policies, by framing up an RL-scenario with an off-policy (Q-Learning) algorithm for temporal-difference learning, which is used to train DBRNN to learn contextual patterns and generate new video sequences for the recommendation. Evaluation of our system with real users for a month shows that our approach outperforms state-of-the-art methods and models a useru0027s emotional preferences very well with stable convergence.	algorithm;artificial intelligence;artificial neural network;baseline (configuration management);bidirectional recurrent neural networks;collaborative filtering;contextual advertising;framing (world wide web);interaction;neural network software;personalization;q-learning;random neural network;real-time clock;real-time data;real-time transcription;recurrent neural network;reinforcement learning;spontaneous order;supervised learning;temporal difference learning;time complexity;user modeling	Abhishek Tripathi;S AshwinT.;Ram Mohana Reddy Guddeti	2018	2018 IEEE 18th International Conference on Advanced Learning Technologies (ICALT)	10.1109/ICALT.2018.00103	supervised learning;user modeling;multimedia;collaborative filtering;machine learning;reinforcement learning;data stream mining;recurrent neural network;computer science;emotional intelligence;artificial intelligence;interactive learning	AI	-16.60763992011723	-49.79863784905156	133540
92b845459f6a64a79addd2b46efc6c99c4f6b264	enhanced nearest neighbour search on the r-tree	index structure;indexing method;nearest neighbour search;nearest neighbor search;multimedia database	Multimedia databases usually deal with huge amounts of data and it is necessary to have an indexing structure such that efficient retrieval of data can be provided. R-Tree with its variations, is a commonly cited indexing method. In this paper we propose an improved nearest neighbor search algorithm on the R-tree and its variants. The improvement lies in the removal of two hueristics that have been used in previous R*-tree work, which we prove cannot improve on the pruning power during a search.	database;nearest neighbor search;r-tree;search algorithm	King Lum Cheung;Ada Wai-Chee Fu	1998	SIGMOD Record	10.1145/290593.290596	nearest-neighbor chain algorithm;r-tree;ball tree;best bin first;computer science;pattern recognition;data mining;cover tree;nearest neighbor search;information retrieval	DB	-5.65992395800998	-41.11584224860996	133600
dc0a2a30751e988ca8889061f12aead8b90f2c40	a link-based method for propositionalization		Propositionalization, a popular technique in Inductive Logic Programming, aims at converting a relational problem into an attributevalue one. An important facet of propositionalization consists in building a set of relevant features. To this end we propose a new method, based on a synthetic representation of the database, modeling the links between connected ground atoms. Comparing it to two state-of-the-art logicbased propositionalization techniques on three benchmarks, we show that our method leads to good results in supervised classification.	inductive logic programming;literal (mathematical logic);machine learning;supervised learning;synthetic intelligence	Quang-Thang Dinh;Christel Vrain;Matthieu Exbrayat	2012			machine learning;theoretical computer science;facet (geometry);computer science;relational problem;artificial intelligence;inductive logic programming	AI	-7.8826143039057985	-39.967646033549386	133836
c4843cf123f3411dd0d6635531b5ac12cab18a6c	de novo assembly of high-throughput sequencing data with cloud computing and new operations on string graphs	dna;directed graphs;parallel algorithms bioinformatics cloud computing directed graphs dna genomics;de novo sequence assembly;genomics;bioinformatics cloud;e coli genomes de novo assembly high throughput sequencing data cloud computing next generation sequencing technologies dna sequencing scalable commodity servers parallel algorithm mapreduce framework myers bidirected string graphs graph construction graph simplification prefix and extend algorithm conventional operations path compression tip removal bubble removal similar neighbour edge adjustment error topology structure removal a statistics cloudbrush assembler contrail edena;bioinformatics cloud de novo sequence assembly map reduce;decision support systems;map reduce;decision support systems conferences cloud computing;conferences;cloud computing;bioinformatics;parallel algorithms	"""The next-generation sequencing technologies dramatically accelerate the throughput of DNA sequencing in a much faster rate than the growth rate of computer speed as predicted by the """"Moore's Law."""" It is a problem even to load and run these sequencing data in memory. There is an urgent need for de novo assemblers to efficiently handle the huge amount of sequencing data using scalable commodity servers in the clouds. In this paper, we present CloudBrush, a parallel algorithm that runs on the MapReduce framework of cloud computing for de novo assembly of high-throughput sequencing data. The algorithm uses Myers's bi-directed string graphs as its basis and consists of two main stages: graph construction and graph simplification. First, a vertex is defined for each non-redundant sequence read. We present a prefix-and-extend algorithm to identify overlaps between a pair of reads and to reduce transitive edges. The graph is further simplified by using conventional operations including path compression, tip removal and bubble removal. We also present a new operation, Similar Neighbour Edge Adjustment, to remove error topology structures in string graphs. Besides, we also disconnect repeat regions by revised A-statistics. The goal is to partition the string graph so that all paths in each connected subgraph correspond to similar subsequences of the underlying genome. We then traverse each connected subgraph to find a long path supported by a sufficient amount of reads to represent the subgraph. Preliminary results show that the CloudBrush assembler, compared with Contrail and Edena on the sequencing data of E. coli genomes, may yield longer contigs."""	assembly language;cloud computing;contrail;de novo transcriptome assembly;disjoint-set data structure;high-throughput computing;level of detail;mapreduce;moore's law;parallel algorithm;scalability;string graph;traverse;throughput;vertex (graph theory)	Yu-Jung Chang;Chien-Chih Chen;Jan-Ming Ho;Chuen-Liang Chen	2012	2012 IEEE Fifth International Conference on Cloud Computing	10.1109/CLOUD.2012.123	genomics;directed graph;cloud computing;computer science;bioinformatics;theoretical computer science;operating system;k-mer;distributed computing;parallel algorithm;dna	Visualization	-7.015903540580681	-39.52015965039871	134410
1b20811a18373e42f4936012d3119e1503776278	biomass: a biological multi-agent simulation system	organisms;biology computing;predator prey systems;biological system modeling;ecology;environmental science computing;multi agent systems;computational modeling;multi agent systems biology computing ecology environmental science computing;interdisciplinary individual based modeling methodology biomass biological multiagent simulation system agent based model functional groups ecological complexity;ecosystems;biomass;biological system modeling computational modeling organisms biomass ecosystems predator prey systems mathematical model;mathematical model	This article presents an agent based model for the simulation of biological systems. The approach consists mainly of providing individual based models for each of the functional groups that conform an ecosystem. Functional groups (a term commonly used by ecologists) may represent a group of individuals (from the same or from different species) that share relevant attributes. This provides flexibility to configure different kinds of populations by parametrization without the need of programming, something useful for biologists. Additionally, a simulation tool implemented as a multi-agent system facilitates the analysis and understanding of ecological complexity. Multi-agent systems are proposed to address heterogeneity and autonomy demanded by the interdisciplinary individual-based modeling methodology. The objective of the system is to explore the intricate relationships among population and individuals, in an ecosystem approach. The main difference with other tools is the ability of incorporating individual decisions, based on metabolism and environmental conditions.	agent-based model;biological system;ecology;ecosystem;entity–relationship model;experiment;graphical user interface;lotka–volterra equations;multi-agent system;population;prey;simulation;usability	Candelaria E. Sansores;Flavio Reyes-Ramírez;Héctor F. Gómez;Juan Pavón;Luis Calderon	2011	2011 Federated Conference on Computer Science and Information Systems (FedCSIS)		organism;ecosystem;biomass;bioinformatics;multi-agent system;mathematical model;management science;computational model	AI	-5.425444823452136	-48.32189194092266	134499
be8ade7b954190f0df842350f10cef9b42c32a9f	artificial consciousness algorithm for an autonomous system	learning algorithm;generic model;neural nets;brain models;autonomic system;external models artificial consciousness algorithm autonomous system conscious behavior neural architecture brain one dimensional feedback control system learning algorithm functional relations predictive simulation example system;feedback;learning artificial intelligence brain models neural nets feedback;learning artificial intelligence;biological system modeling predictive models control systems physics feedback control prediction algorithms brain modeling pressing biological systems evolution biology;feedback control	"""Conscious behavior is hypothesized to be governed by the dynamics of the neural architecture of the brain. A general model of an artificial consciousness algorithm is presented, and applied to a one-dimensional feedback control system. A new learning algorithm for learning functional relations is presented and shown to be biologically grounded. The consciousness algorithm uses predictive simulation and evaluation to let the example system relearn new internal and external models after it is damaged. Introduction. Brains evolved to help their owners solve a pressing problem no tree or chair ever has: Answer the question """"What should I do next?"""". Artificial consciousness (AC) likewise is a system that makes choices of what to do next. Rather than being concerned with the control of dynamical behavior as in classical control systems, it is concerned with the best choice of which dynamical behavior to use. It is a command system rather than a control system. In biological systems evolution will favor the brains that produce answers which allow the entity to survive, reproduce, and bring its offspring to reproductive age. The route to consciousness does not pass through the route to increasing brilliance. Very smart systems can be unconscious and very stupid systems can be conscious. We argue that consciousness exists to serve a purpose no merely smart system could ever serve: It solves the problem of how to solve problems. It is a solution -the only one we can imagineto the problem of preparing the entity to deal with we know not what. Some of the things brains contain to accomplish these goals are reflexes, instincts and the ability to learn. Reflexes are very short, direct stimulus-response rules. They are useful precisely because they are evoked without thought. They are hardwired rules. Instincts are also stimulus-response behaviors. Their difference is that these rules pass through a filter of situation recognition. They are situation-dependent behavior patterns. In an artificial system, natural instincts are called """"expert systems"""" and biological ancestors are called """"experts."""" Third, some animals can learn during their lifetimes. Learned behaviors are also stimulusresponse rules, but are more dependent on the particular environment (world model) and the particular individual (self model). Learned behavior in humans becomes faster and more reliable than the conscious behavior from which it derived. Athletes, pilots, musicians, etc. practice to retain good subconscious functions that bypass the rather slow clumsy consciousness. Like control systems, an AC system uses feedback. It uses prior knowledge and past experience. Where it differs from conventional control systems is that it deals with unexpected situations. In design it is an adaptive, attentional, conflict-detection cueing algorithm that evaluates and executes the best available moves based on its capabilities, experience and objectives. 635 0-7695-06 19-4/00 $10.00"""	algorithm;artificial consciousness;autonomous system (internet);biological system;brilliance;control system;expert system;feedback;simulation;smart system;tree (data structure)	John L. Johnson;H. John Caulfield;Jaime R. Taylor	2000		10.1109/IJCNN.2000.861540	wake-sleep algorithm;computer science;artificial intelligence;machine learning;feedback;artificial intelligence system;artificial neural network	AI	-7.872588240351298	-45.98696329283849	134545
2ce037c9b5d77972af6892c170396c82d883dab9	a high-performance approach to string similarity using most frequent k characters		The amount of available data has been growing significantly over the last decades. Thus, linking entries across heterogeneous data sources such as databases or knowledge bases, becomes an increasingly difficult problem, in particular w.r.t. the runtime of these tasks. Consequently, it is of utmost importance to provide time-efficient approaches for similarity joins in the Web of Data. While a number of scalable approaches have been developed for various measures, the Most Frequent k Characters (MFKC) measure has not been tackled in previous works. We hence present a sequence of filters that allow discarding comparisons when executing bounded similarity computations without losing recall. Therewith, we can reduce the runtime of bounded similarity computations by approximately 70%. Our experiments with a single-threaded, a parallel and a GPU implementation of our filters suggest that our approach scales well even when dealing with millions of potential comparisons.	algorithm;complete (complexity);computation;database;experiment;graphics processing unit;knowledge base;linkage (software);most frequent k characters;scalability;string metric;thread (computing);turing completeness;world wide web	Andre Valdestilhas;Tommaso Soru;Axel-Cyrille Ngonga Ngomo	2017			most frequent k characters;string metric;mathematics;pattern recognition;artificial intelligence	ML	-6.458758635135826	-38.9423610462955	134671
66ecfada13105f2f869a736ffa96d22e4274e656	uncovering and predicting the dynamic process of information cascades with survival model	social network;information cascades;dynamic processes prediction	Cascades are ubiquitous in various network environments. Predicting these cascades is decidedly nontrivial in various important applications, such as viral marketing, epidemic prevention, and traffic management. Most previous works have focused on predicting the final cascade sizes. As cascades are dynamic processes, it is always interesting and important to predict the cascade size at any given time, or to predict the time when a cascade will reach a certain size (e.g., the threshold for an outbreak). In this paper, we unify all these tasks into a fundamental problem: cascading process prediction. That is, given the early stage of a cascade, can we predict its cumulative cascade size at any later time? For such a challenging problem, an understanding of the micromechanism that drives and generates the macrophenomena (i.e., the cascading process) is essential. Here, we introduce behavioral dynamics as the micromechanism to describe the dynamic process of an infected node’s neighbors getting infected by a cascade (i.e., one-hop sub-cascades). Through data-driven analysis, we find out the common principles and patterns lying in the behavioral dynamics and propose the novel NEtworked WEibull Regression model for modeling it. We also propose a novel method for predicting cascading processes by effectively aggregating behavioral dynamics and present a scalable solution to approximate the cascading process with a theoretical guarantee. We evaluate the proposed method extensively on a large-scale social network dataset. The results demonstrate that the proposed method can significantly outperform other state-of-the-art methods in multiple tasks including cascade size prediction, outbreak time prediction, and cascading process prediction.	approximation algorithm;information cascade;scalability;social network	Linyun Yu;Peng Cui;Fei Wang;Chaoming Song;Shiqiang Yang	2016	Knowledge and Information Systems	10.1007/s10115-016-0955-7	simulation;computer science;artificial intelligence;machine learning;information cascade;social network	Web+IR	-17.095442247750405	-42.3508237629499	134934
543789a1f6930398bf11f4be7aae7163680f37a5	evolving control metabolism for a robot	genetic program;robot navigation;genetic programming artificial chemistry chemical information processing autonomous robots;genetic programming;autonomous robots;information processing;biological systems;chemical information processing;metabolic pathway;artificial chemistry	This article demonstrates a new method of programming artificial chemistries. It uses the emerging capabilities of the system's dynamics for information-processing purposes. By evolution of metabolisms that act as control programs for a small robot one achieves the adaptation of the internal metabolic pathways as well as the selection of the most relevant available exteroceptors. The underlying artificial chemistry evolves efficient information-processing pathways with most benefit for the desired task, robot navigation. The results show certain relations to such biological systems as motile bacteria.	acclimatization;admissible numbering;algorithm;artificial chemistry;artificial life;bacteria;biological system;chemotaxis;computation;computational problem;control system;dynamical system;emergence;genetic programming;graph - visual representation;hereditary diseases;information processing;interconnection;metabolism;navigation;robot;robot control;robotic mapping	Jens Ziegler;Wolfgang Banzhaf	2001	Artificial Life	10.1162/106454601753138998	genetic programming;artificial chemistry;metabolic pathway;simulation;information processing;computer science;artificial intelligence	Robotics	-5.493083087576548	-48.419140062046246	134935
177a0a5bdf540a88689237d189b3c7e5c5ebf052	strong social component-aware trust sub-network extraction in contextual social networks	sub network extraction;trust prediction;trsut	In Online Social Networks (OSNs), the important participants, the trust relations between participants, and the interaction contexts between participants greatly impact a participant's decision-making in many applications, such as service provider selection and crowdsourcing service invocation. However, predicting the trust between two unknown participants based on the whole large-scale social network can lead to very high computation costs. Thus, prior to trust prediction, extracting a small-scale sub-network containing the important participants and the corresponding contextual information with a high density could make the trust prediction more efficient and effective. However, extracting such a sub-network has been proved to be an NP-Complete problem. To address this challenging problem, we propose a strong social component-aware trust sub-network extraction model, So-BiNet, to search for near-optimal solutions effectively and efficiently. Our method can extract a trust sub-network without any decompression, which can in turn greatly save the search time of trust sub-network extraction. The experiments, conducted on four social network datasets, demonstrate that our approach can efficiently extract sub-networks covering important participants and contextual information while keeping a high density. Our approach is superior to the state-of-the-art approaches in terms of the quality of the sub-networks extracted within the same execution time.	algorithm;complete (complexity);computation;crowdsourcing;data compression;experiment;np-completeness;run time (program lifecycle phase);social network;subnetwork	Guanfeng Liu;Yan Wang;Mehmet A. Orgun;Xiaoming Zheng;An Liu;Zhixu Li;Kai Zheng	2016	2016 IEEE International Conference on Web Services (ICWS)	10.1109/ICWS.2016.22	socioeconomics;social psychology	Web+IR	-18.49271343656613	-45.10240105147871	135091
0a6a81a80d82428187084d7f3cdc5db1bb9624b2	a unified framework for web video topic discovery and visualization	star structured k partite graph;web video;co clustering;topic visualization;topic discovery;linked cluster network	0167-8655/$ see front matter 2011 Elsevier B.V. A doi:10.1016/j.patrec.2011.07.026 ⇑ Corresponding author. E-mail address: jshao@cs.zju.edu.cn (J. Shao). Together with the explosive growth of web video in sharing sites like YouTube, automatic topic discovery and visualization have become increasingly important in helping to organize and navigate such largescale videos. Previous work dealt with the topic discovery and visualization problem separately, and did not take fully into account of the distinctive characteristics of multi-modality and sparsity in web video features. This paper tries to solve web video topic discovery problem with visualization under a single framework, and proposes a Star-structured K-partite Graph based co-clustering and ranking framework, which consists of three stages: (1) firstly, represent the web videos and their multi-model features (e.g., keyword, near-duplicate keyframe, near-duplicate aural frame, etc.) as a Star-structured K-partite Graph; (2) secondly, group videos and their features simultaneously into clusters (topics) and organize the generated clusters as a linked cluster network; (3) finally, rank each type of nodes in the linked cluster network by ‘‘popularity’’ and visualize them as a novel interface to let user interactively browse topics in multi-level scales. Experiments on a YouTube benchmark dataset demonstrate the flexibility and effectiveness of our proposed framework. 2011 Elsevier B.V. All rights reserved.	a* search algorithm;benchmark (computing);biclustering;browsing;cluster analysis;experiment;information;interactivity;key frame;mathematical optimization;modality (human–computer interaction);multi-source;software framework;sparse matrix;unified framework;usability testing;video clip	Jian Shao;Shuai Ma;Weiming Lu;Yueting Zhuang	2012	Pattern Recognition Letters	10.1016/j.patrec.2011.07.026	computer science;data mining;multimedia;world wide web;biclustering	Vision	-16.781771495969227	-47.105027936236354	135137
23e6517b8ed4dd2ff5dd452f25fd5dafdf905906	scalable multimodal search with distributed indexing by sparse hashing	scalable search;sparse hashing;distributed search;rank fusion	Multimedia search systems must deal with an increasingly large and heterogeneous amount of data. Several challenges exist when deploying real-world search engines for such data. Existing literature does not properly tackle the many efficiency issues that such task requires. In this paper, we address several of the key efficiency aspects required to deploy a distributed search engine, capable of handling several millions of multimedia documents. The search engine builds on a framework designed to: first, ease the distribution of documents and queries across cluster-nodes, second, index media efficiently for fast similarity search and third aggregate ranked results from several heterogeneous sources. Moreover, the proposed framework is flexible enough to support several state-of-the-art indexing and aggregation techniques.  At the heart of the indexing architecture lies an inverse index structure optimized for sparse hashes, that speeds up the retrieval of similar descriptors. To leverage the distributed nature of the search framework, the proposed aggregation technique offers a low temporal complexity overhead and it is agnostic to the index type (a key aspect to support simultaneous modalities). A comprehensive evaluation with both general IR metrics and efficiency metrics, provides a unique assessment of the several efficiency bottlenecks faced by a search engine. In addition, we test the scalability of the search framework to multiple index sizes, i.e., up to 5 million documents per cluster-node.	aggregate data;algorithmic efficiency;cryptographic hash function;distributed web crawling;inverted index;multimodal interaction;overhead (computing);scalability;similarity search;sparse matrix;web search engine	André Mourão;João Magalhães	2015		10.1145/2671188.2749310	beam search;search engine indexing;computer science;theoretical computer science;machine learning;data mining;database;world wide web;search engine	Web+IR	-14.483874259469701	-50.50851856332224	135194
150c5e4846d16986f8b5eb56ebd22ee5dfcba591	generalizing pagerank: damping functions for link-based ranking algorithms	web graph;algorithm analysis;link based ranking;ranking function;link analysis;web graphs;ranking algorithm;exponential decay	This paper introduces a family of link-based ranking algorithms that propagate page importance through links. In these algorithms there is a damping function that decreases with distance, so a direct link implies more endorsement than a link through a long path. PageRank is the most widely known ranking function of this family.The main objective of this paper is to determine whether this family of ranking techniques has some interest per se, and how different choices for the damping function impact on rank quality and on convergence speed. Even though our results suggest that PageRank can be approximated with other simpler forms of rankings that may be computed more efficiently, our focus is of more speculative nature, in that it aims at separating the kernel of PageRank, that is, link-based importance propagation, from the way propagation decays over paths.We focus on three damping functions, having linear, exponential, and hyperbolic decay on the lengths of the paths. The exponential decay corresponds to PageRank, and the other functions are new. Our presentation includes algorithms, analysis, comparisons and experiments that study their behavior under different parameters in real Web graph data.Among other results, we show how to calculate a linear approximation that induces a page ordering that is almost identical to PageRank's using a fixed small number of iterations; comparisons were performed using Kendall's τ on large domain datasets.	approximation algorithm;experiment;inline linking;iteration;linear approximation;pagerank;ranking (information retrieval);software propagation;speculative execution;time complexity;webgraph	Ricardo A. Baeza-Yates;Paolo Boldi;Carlos Castillo	2006		10.1145/1148170.1148225	mathematical optimization;link analysis;computer science;machine learning;ranking svm;world wide web;exponential decay;statistics	Web+IR	-11.03459805617874	-42.645242960873276	135260
fcf3f74251b3b117c34902eb18c3909a323557e3	community detection by label propagation with compression of flow		The label propagation algorithm (LPA) has been proved to be a fast and effective method for detecting communities in large complex networks. However, its performance is subject to the non-stable and trivial solutions of the problem. In this paper, we propose a modified label propagation algorithm LPAf to efficiently detect community structures in networks. Instead of the majority voting rule of the basic LPA, LPAf updates the label of a node by considering the compression of a description of random walks on a network. A multi-step greedy agglomerative strategy is employed to enable LPAf to escape the local optimum. Furthermore, an incomplete update condition is also adopted to speed up the convergence. Experimental results on both synthetic and real-world networks confirm the effectiveness of our algorithm. PACS. 89.75.Fb Structures and organization in complex systems – 89.75.Hc Networks and genealogical trees	complex network;complex systems;conductance (graph);dhrystone;effective method;experiment;graph (discrete mathematics);greedy algorithm;label propagation algorithm;local optimum;louvain modularity;maxima and minima;non-maskable interrupt;sensor;software propagation;sparse matrix;synthetic intelligence;time complexity;vergence;weighted network	Jihui Han;Wei Li;Zhu Su;Longfeng Zhao;Weibing Deng	2016	CoRR		physics	ML	-13.586595721118522	-42.27398214108556	135350
9f903ebba079ab7c62904d8c738aa14e1196b2ba	prefix-suffix trees: a novel scheme for compact representation of large datasets	cluster algorithm;pattern count pc tree;large dataset;prefix suffix trees;abstraction;large data sets;incremental mining;data mining;suffix tree;compact representation;clustering;decision making process	An important goal in data mining is to generate an abstraction of the data. Such an abstraction helps in reducing the time and space requirements of the overall decision making process. It is also important that the abstraction be generated from the data in small number of scans. In this paper we propose a novel scheme called Prefix-Suffix trees for compact storage of patterns in data mining, which forms an abstraction of the patterns, and which is generated from the data in a single scan. This abstraction takes less amount of space and hence forms a compact storage of patterns. Further, we propose a clustering algorithm based on this storage and prove experimentally that this type of storage reduces the space and time. This has been established by considering large data sets of handwritten numerals namely the OCR data, the MNIST data and the USPS data. The proposed algorithm is compared with other similar algorithms and the efficacy of our scheme is thus established.		Radhika M. Pai;V. S. Ananthanarayana	2007		10.1007/978-3-540-77046-6_40	decision-making;computer science;theoretical computer science;machine learning;data mining;abstraction;cluster analysis	NLP	-4.708301238232674	-38.81357722026502	136622
5dbe794dd7af30c293ab952b6dcec21fe899e6c9	influence maximization on correlated networks through community identification		The identification of the minimal set of nodes that maximizes the propagation of information is one of the most important problems in network science. Here we investigate the effect of degree-degree correlation on the influence maximization problem. In contrast to what is expected, we verify that increasing the number of initial spreaders does not improve the propagation in assortative network. Moreover, we introduce a new method for identification of the most influential spreaders through the community organization of networks. Our simulation results show that our approach is statistically similar, in terms of the information reach, to the technique based on greedy optimization. However, our method is more suitable in practice, because it is much less time consuming than the greedy approach.	expectation–maximization algorithm;greedy algorithm;mathematical optimization;network science;semantic network;simulation;software propagation	Didier Augusto Vega-Oliveros;Luciano da Fontoura Costa;Francisco Aparecido Rodrigues	2017	CoRR		network science;artificial intelligence;machine learning;complex network;mathematics;mathematical optimization;maximization;greedy algorithm;correlation	AI	-16.631199646803797	-43.20776678659077	136764
41344dcd8ad2f32f9853a9464c13e57db24ec49c	a query-sensitive cost model for similarity queries with m-tree	indexation;access method;metric space	We introduce a cost model for the M-tree access method [Ciaccia et al., 1997] which provides estimates of CPU (distance computations) and I/O costs for the execution of similarity queries as a function of each single query. This model is said to be query-sensitive, since it takes into account, by relying on the novel notion of “witness”, the “position” of the query point inside the metric space indexed by the M-tree. We describe the basic concepts underlying the model along with different methods which can be used for its implementation; finally, we experimentally validate the model over both real and synthetic datasets.	algorithm;analysis of algorithms;best, worst and average case;central processing unit;computation;experiment;input/output;m-tree;mathematical optimization;metric tree;query language;synthetic intelligence	Paolo Ciaccia;A. Nanni;Marco Patella	1999			data mining;access method;database;witness;computer science;m-tree;computation;indexation;metric space	DB	-5.860259135202815	-42.12209872051836	136772
75ca87ed7a59e1c3855f93000935468335ac04aa	the minimum information dominating set for opinion sampling in social networks	social network services;graph theory;time complexity;information dominating set;power system faults;social networks;opinion polling;information management;np complete;correlation;sampling methods;graph sampling	We consider the problem of sampling a node-valued graph. The objective is to infer the values of all nodes from that of a minimum subset of nodes by exploiting correlations in node values. We first introduce the concept of information dominating set (IDS). A subset of nodes in a given graph is an IDS if the values of these nodes are sufficient to infer the values of all nodes. We focus on two fundamental algorithmic problems: (i) how to determine whether a given subset of nodes is an IDS; (ii) how to construct a minimum IDS. Assuming binary node values and the local majority rule for information correlation, we first show that in acyclic graphs, both problems admit linear-complexity solutions by establishing a connection between the IDS problems and the vertex cover problem. We then show that in a general graph, the first problem is co-NP-complete and the second problem is NP-hard. We develop two approaches to solve the IDS problems: one reduces the problems to a hitting set problem based on the concept of essential difference set, the other a gradient-based approach with a tunable parameter that trades off performance with time complexity. The concept of IDS finds applications in opinion sampling such as political polling and market survey, identifying critical nodes in information networks, and inferring epidemics and cascading failures in communication and infrastructure networks.	cascading failure;co-np;co-np-complete;directed acyclic graph;dominating set;gradient;np-completeness;np-hardness;sampling (signal processing);set cover problem;social network;time complexity;vertex cover	Jianhang Gao;Qing Zhao;Ananthram Swami	2016	IEEE Transactions on Network Science and Engineering	10.1109/TNSE.2016.2600030	time complexity;sampling;mathematical optimization;combinatorics;discrete mathematics;np-complete;computer science;graph theory;machine learning;mathematics;information management;correlation;algorithm;social network	Theory	-16.37503145732213	-42.89760972702655	136785
0436f605937267cd177edffa4815a89f948a9ee7	communities detection and the analysis of their dynamics in collaborative networks	community detection;dynamic graph analysis;diffusion of information;community identification;random walk;collaborative networks;social network analysis;co authorship analysis;information diffusion;community finding;dynamic networks;web based communities	The analysis of graphs like collaborative networks aims at studying the relationships between individuals, instead of individual attributes or properties. One of the interesting substructures of such a graph is a community structure, which is a subset of nodes that are more densely linked when compared with the rest of the network. Such dense subgraphs gather individuals who share similar interests depending on the type of relation encoded in the graph. In this paper we tackle the problem of identifying communities in dynamic networks. We propose an approach based on the random walk to identify communities in evolving graphs like collaborative networks. We apply this approach to the Infocom co-authorship network to determine stable collaborations and evolving communities. We use such information, combined with other Digital Bibliography & Library Project (DBLP) co-authorship network topology features, to analyse the formation of the programme committee board of a conference.	collaborative network	Céline Robardet;Eric Fleury	2009	IJWBC	10.1504/IJWBC.2009.023965	network science;social network analysis;social science;evolving networks;computer science;dynamic network analysis;data science;data mining;sociology;world wide web;random walk;complex network	ML	-17.14557136492097	-41.007090916966014	136809
f0bc5b1bbefc3c878cefdd307aac591ef8d29fc9	node seniority ranking		Recent advances in graph theory suggest that is possible to identify the oldest nod es of a network using only the graph topology. Here we repo t on applications to heterogeneous real world networks. To this end, and in order to gain new ins ight , we propose the theoretical framework of the Estrada communicability. We apply it to two technol ogical networks (an underground, the diffusion of a software worm in a LAN) and to a third network representing a cho lera outbreak. In spite of errors introduced in the adjacency matrix of their graphs, t e identification of the oldest nodes is feasible , within a small margin of error, and extremely simple. Util izations include the search of the initial diseasespreader (patient zero problem), rumors in social n etworks, malware in computer networks, triggering events in blackouts, oldest urban sites recognition . We investigate the growth over time of graphs, identifying the source nodes w ho started the growth on a pure topological basis. Is it feasible to classify nodes according to their age without measurements ? The common sense answer a few years ago would be, predictably, puzzling. Nevertheless, recently (1) it has been shown how to trace the oldest node sour ces of an evolving graph, using only the eigenvalues and eigenvectors of the Laplacian matrix. Such an identification task is to be regarded as a difficul t one, but advantages to be gained in many fields of science are so relevant to justify the efforts o f a large number of researchers. The interest of this inverse problem lies in the variety of applica tions in IT security, medicine, pharmacology, archaeology, finance, engineering, biology, but til l a few years ago solutions were not foreseen. We show that is possible to identify oldest nodes o f heterogeneous real world technological networks or of an epidemic spreading graph, namely: the underground of Paris during the period 1900-1949, the diffusion of a software worm in a co mputer LAN, a cholera outbreak. Moreover, we suggest a necessary condition to recognize the networks suita ble of the age-analysis and a rough estimator of the algorithm performance. Pinto (2) has developed a procedure to estimate the location of the epidemic source from measurements collected by sparsely placed observers by means of a maximum probability estimator. Each observer (about 20% of the nodes we re monitored) measures from which neighbour and at what time has received the contagi on. The collected data are used to produce the estimate, whose complexity is O(N), N number of nodes. Results of the validation test on the Kwa Zulu cholera outbreak in South Africa in 2000 s how the estimation errors is below 4 hops . In this paper we consider the “patient zero” as the old st node of the cholera outbreak graph, thus we see no difference among the three different netw orks and consequently we can apply the same methodology. Zhu (1) instead has developed a deterministic spectral str tegy based only on the topology of the network (solving de facto an inverse problem) at the same computational cost O(N), applying his method to the Santa Fè co-authorship s ocial network (3) and to the protein-protein interaction network. Here we present the solution to both problems ap plying a similar, very simple methodology. Our main goal is to study some heterogeneous protot ypical real-world networks in order to provide tools for practical applications. The graph (is the mathematical counterpart network, but the two words are almost equivalent) to be analyzed results from a growth intended as an evolution over time, depending generally on stable, non-stochastic, “smooth“ transformations. When its topology is known, Zhu claims that the eig envalue spectrum of the connectivity matrix or preferably of the Laplacian matrix is related cl osely to the age of nodes. The correlation between eigenvalues and age is strictly required; m oreover, if no evolutionary process was developed in the past, the method is not applicable . For graphs following the preferential attachment rule (“rich get richer”) the correlation is clear, because the probability for a node to acquire new links is proportional to its degree, th erefore a strong correlation between the node degree and its life-time is sure, but real world ne tworks are much more complicated (4 -6). For a given eigenvalue, the lifetime of the associated ei genvector is the average age of all nodes contained in the vector, weighted by the respective components of the eigenvector. The first step is to build the Laplacian matrix L = D – A, where D is the degree diagonal matrix and A the adjacency matrix ( aij = 1 if the link i-j exists, 0 otherwise ). The second step is the standardization of each eigenvector components: vi = | vi / max (vi) |, with i = 1, 2, ... N. The third step is the seniority ranking. Nodes with sta nd rdized component values larger than a threshold are clustered in a certain age subset and related to the associated eigenvalues, thus the largest eigenvalues is associated to the oldest nod e an so on. This method, tested on the Santa Fè Institute co-authorship of scientific papers soc ial network (3), is able to classify the age of nodes (1) completely. For example, the first three larger eig envalues of the Laplacian, related to the nodes corresponding to the eigenvectors selecte d by the thresholding procedure, indicate the three oldest nodes of the network of Fig. 1: λ76>λ75>λ74 ↔ 40, 7, 67 where 40 is the oldest node, and the λN>λN-1 > ... >λ1 is the descending eigenvalue spectrum. The Zhu’ pro cedure is due to the observation that the eigenvector size in networks, such as the protein-protein interactions, do not seems to increase, while the corresponding (accordi ng to the threshold procedure) eigenvalue does. No suggestions about the characteristics of e volving networks suitable to be age-analyzed or how to choose the threshold’s value are given. N ow we note that has been discovered independently (7), in many social networks, how in large non random graphs changes over time result in a continuous variation of the adjacency m atrix eigenvalues, while the eigenvectors stay (relatively) constants, therefore, the correlation between the node ages and the largest eigenvalues comes as a direct consequence. Although a thors of (7) apparently were not aware of the relation eigenspectrum-age, they sketch a de monstration for a necessary condition on the eigenvectors, that we consider a sound approach to explain the age – eigenvalue correlation, as follows. Starting from the standard eigenvalue deco mp sition of a graph: A(ti) = V(ti) Λ(ti) V(ti) i = 0, 1, 2, ... , N where A is the adjacency matrix, V the eigenvectors matrix, V’ its transpose, Λ the eigenvalues matrix, at time ti .If the eigenvectors remain constants, we can write: A(ti+1) ≈ V(ti) Λ(ti+1) V(ti)’ where Λ(ti+1) = Δ(A, V, Λ) + Λ(ti). Since V has orthogonal columns we can compute the best fit of Δ in a least-squares sense Δ’ (A, V, Λ) ≈ V(ti)( A(ti+1) – A(ti) )V(ti)’ and since the calculation requires Δ’ to be diagonal, any deviation from this condition indicates a deviation from the type of graph evolution over tim e required, deteriorating the age evaluation. Actually, the diagonality condition may be relaxed to a diagonal dominance. A note of caution: for random graphs, such as Erdos-Renyi graphs, eige nvectors increase faster than eigenvalues (7), hence the age analysis is unfeasible. Now we introd uce our alternative procedure. We have seen A(ti+1) ≈ V(ti) Λ(ti+1) V(ti)’ then tr(e A(ti+1 ) = tr(V(ti)e ) V(ti)’) = Σj e, for λj=λj(ti+1). If eigenvectors stay almost constants, most of the var iation of the trace from time ti to ti+1 depends from the eigenvalues; in particular, each node i contributes with the quantity SCi = Σj (vj)e, where λj=λj(ti+1) and vj denote the i-th component of the eigenvector vj. The SCi parameter (8) called sub-graph centrality, is closely related to the communicability index ECI defined (11) as:	acceptance testing;adjacency matrix;algorithm;attachments;column (database);computation;computational complexity theory;computer networks (journal);curve fitting;diagonally dominant matrix;economic complexity index;eigenvector centrality;emoticon;ernesto estrada;graph (discrete mathematics);interaction network;laplacian matrix;least squares;malware;multidimensional digital pre-distortion;random graph;scientific literature;social network;thresholding (image processing);topological graph theory;utility;util-linux	Vincenzo Fioriti;Marta Chinnici	2013	CoRR		combinatorics;computer science;artificial intelligence;theoretical computer science;data mining;mathematics	ML	-16.610611607562618	-38.20646178186345	137124
17461819b7027151188f64189b64e3cc10b8a0c1	sequential analyses of foraging behavior and attack speed in ambush and widely foraging lizards	foraging;bepress selected works;sequential analysis;foraging behavior;locomotion;lizard foraging sequential analysis locomotion;prey capture;lizard	Food acquisition mode in lizards (i.e., ambush vs. widely searching) has been intensely scrutinized for the past decade to identify correlations between food acquisition mode, diet, sprint speed, and other aspects of phenotypic diversity. To begin to understand these correlations, we studied foraging mode variation in natural foraging behavior and attack speed in three ambush predators and two widely foraging species in the field. Sequential analyses revealed considerable variation in the temporal structure of behavioral repertoires associated with acquiring food. Ambush and wide-foraging species use unique combinations of behaviors prior to prey attack with differences among and between food acquisition modes. Attack speeds were well below maximum sprint speed for these species. Thus, the widely demonstrated correlation between food acquisition mode and sprint speed is not related to prey capture per se. The striking variation in prey capture repertoires in these model ambush and wide foragers shows that we have a long way to go before we will understand the ecological relevance of many performance and phenotypic traits that are related to foraging mode in lizards.	prey;relevance;sprint (software development);way to go	Eric J. McElroy;Lance D. McBrayer;Steven C. Williams;Roger A. Anderson;Stephen M. Reilly	2012	Adaptive Behaviour	10.1177/1059712311426800	foraging;optimal foraging theory;communication	HCI	-4.557108134897199	-47.146344637108356	137749
2f230a46b4939fa75f266b8419f907dc0d4fb5db	graph structure in the web	web graph;diameter;graph structure;web measurement	The study of the web as a graph is not only fascinating in its own right, but also yields valuable insight into web algorithms for crawling, searching and community discovery, and the sociological phenomena which characterize its evolution. We report on experiments on local and global properties of the web graph using two Altavista crawls each with over 200 million pages and 1.5 billion links. Our study indicates that the macroscopic structure of the web is considerably more intricate than suggested by earlier experiments on a smaller scale.	algorithm;experiment;webgraph;world wide web	Andrei Z. Broder;Ravi Kumar;Farzin Maghoul;Prabhakar Raghavan;Sridhar Rajagopalan;Raymie Stata;Andrew Tomkins;Janet L. Wiener	2000	Computer Networks	10.1016/S1389-1286(00)00083-9	computer science;data science;theoretical computer science;diameter;world wide web;graph database	Web+IR	-18.035645050532494	-41.06915322757512	137774
6cf3a89cd0352a7724f280f3fe07266c5ec810ee	the minimum step linkage algorithm and cumulative cost model for time-dependent shortest paths		Real-word road networks are typically time-dependent, but research on time-dependent shortest paths (TDSPs) is not as rich as that on static shortest paths (SSPs). For large networks, some speed-up techniques are usually applied to compute SSPs. Unfortunately, many of these techniques are impractical for TDSPs, especially the techniques requiring a precomputation because the number of time instances may be incredibly large or even infinite. Therefore, this study introduced a new TDSP technique that is able to overcome the issue of infinite time instances by utilizing network geometry alone for the pre-computation, namely the Minimum Step Linkage (MSL) algorithm. It works with a data model called Cumulative Cost Model (CCM). The theoretical concepts of the new data model and algorithm are described, and its performances were tested and compared with existing algorithms through a series of simulations.	a* search algorithm;analysis of algorithms;computation;data model;distance matrix;euclidean distance;greedy algorithm;linkage (software);performance;precomputation;recommender system;scalability;shortest path problem;simulation	Vini Indriasari;Denis Dean	2016	Trans. GIS	10.1111/tgis.12215	mathematical optimization;computer science;machine learning;algorithm	DB	-9.743022049599606	-40.17209417484598	138010
9d17539c17648a3c994f1de85fa5a5160a5bb553	collaboration networks analysis: combining structural and keyword-based approaches		This paper proposes a method for the analysis of the characteristics of collaboration networks. The method uses social network analysis metrics which are especially applicable to directed and weighted collaboration networks. By using the proposed method it is possible to investigate the global structure of the collaboration networks, such as density, centralisation, assortativity and the dynamics of network growth. Furthermore, the method proposes appropriate network centrality measures (degree and its variations for directed and weighted networks) for ranking the nodes. In addition the proposed method combines a keyword-based approach and Louvain algorithm for the community detection task. Next, the paper describes a case study in which the proposed method is applied to the collaboration networks emerged from STSMs on the KEYSTONE COST Action.		Ana Mestrovic	2017		10.1007/978-3-319-74497-1_11	social network analysis;centrality;machine learning;centralisation;assortativity;ranking;computer science;artificial intelligence	Logic	-16.4019736575294	-41.290453551064346	138100
7f90a086140f2ca2ff282e3eedcf8c51ee2db674	optimizing factorization machines for top-n context-aware recommendations		Context-aware Collaborative Filtering (CF) techniques such as Factorization Machines (FM) have been proven to yield high precision for rating prediction. However, the goal of recommender systems is often referred to as a top-N item recommendation task, and item ranking is a better formulation for the recommendation problem. In this paper, we present two collaborative rankers, namely, Ranking Factorization Machines (RankingFM) and Lambda Factorization Machines (LambdaFM), which optimize the FM model for the item recommendation task. Specifically, instead of fitting the preference of individual items, we first propose a RankingFM algorithm that applies the cross-entropy loss function to the FM model to estimate the pairwise preference between individual item pairs. Second, by considering the ranking bias in the item recommendation task, we design two effective lambda-motivated learning schemes for RankingFM to optimize desired ranking metrics, referred to as LambdaFM. The two models we propose can work with any types of context, and are capable of estimating latent interactions between the context features under sparsity. Experimental results show its superiority over several state-of-the-art methods on three public CF datasets in terms of two standard ranking metrics.	algorithm;collaborative filtering;cross entropy;experiment;fm broadcasting;hinge loss;interaction;loss function;optimizing compiler;recommender system;sampling (signal processing);sparse matrix;spelling suggestion;time complexity;web search engine	Fajie Yuan;Guibing Guo;Joemon M. Jose;Long Chen;Haitao Yu;Weinan Zhang	2016		10.1007/978-3-319-48740-3_20	computer science;machine learning;data mining;database;world wide web;information retrieval	AI	-18.59592550360027	-48.90991662327361	138954
bd9bc4b343d70695d4b84730749a40e723d913f4	full body: the importance of the phenotype in evolution	phenotypic plasticity;sustained evolution;agent based model;agent based modeling;complexity;niche construction;natural selection;phenotype plasticity;agent based modeling evolution complexity causality sustained evolution phenotype plasticity niche construction;causality;evolution	This is a position paper on phenotype-based evolution modeling. It argues that evolutionary complexity is essentially a functional kind of complexity, and for it to evolve, a full body, or, in other words, a dynamically defined, deeply structured, and plasticity-bound phenotype is required. In approaching this subject, we ask and answer some key questions, which we think are interrelated. The questions we discuss and the answers we propose are: (a) How should complexity growth be measured or operationalized in natural and artificial systems? Evolutionary complexity is akin to that of machines, and to operationalize it, we need to study how machinelike organismic functions work and develop. Inspired by studies on causality, we propose the notion of mechanism. A mechanism is a simplified causal system that carries out a function. A growth of functional complexity involves interconversions between a deep (or unused) process and that of a mechanism. (b) Are the principles of natural selection, as they are currently understood, sufficient to explain the evolution of complexity? Our answer is strongly negative. Natural selection helps adapting mechanisms to carry out a given task, but will not generate a task. Hence there is a tradeoff between available tasks and mechanisms fulfilling them. To escape, we argue that competition avoidance is required for new complexity to emerge. (c) What are the environmental constraints on complexity growth in living systems? We think these constraints arise from the structure of the coevolving ecological system, and the basic frames are given by the niche structure. We consider the recently popular idea of niche construction and relate it to the plasticity of the phenotype. We derive a form of phenotype plasticity from the hidden (unused) and explicit (functional) factors discussed in the causality part. (d) What are the main hypotheses about complexity growth that can actually be tested? We hypothesize that a rich natural phenotype that supports causality-function conversions is a necessary ingredient of complexity growth. We review our work on the FATINT system, which incorporates similar ideas in a computer simulation, and shows that full-body phenotypes are sufficient for achieving functional evolution. (e) What language is most appropriate for speaking about the evolution of complexity in living systems? FATINT is developed using advanced agent-based modeling techniques, and we discuss the general relevance of this methodology for understanding and simulating the phenomena discussed.	agent-based model;biological evolution;cns disorder;causal system;causality;complexity;computer simulation;ecosystem;emoticon;frame (physical object);living systems;multi-agent system;natural selection;niche blogging;phenotype;relevance;speaking (activity)	George Kampis;László Gulyás	2008	Artificial Life	10.1162/artl.2008.14.3.14310	biology;natural selection;complexity;causality;artificial intelligence;evolution;niche construction;ecology;genetics;phenotypic plasticity	AI	-4.837817467623973	-47.07094838442636	139027
3df246b5ce470b7ce799bc28577c65bcf9c5b610	discriminative distance-based network indices and the tiny-world property		Distance-based indices, including closeness centrality, average path length, eccentricity and average eccentricity, are important tools for network analysis. In these indices, the distance between two vertices is measured by the size of shortest paths between them. However, this measure has shortcomings. A well-studied shortcoming is that extending it to disconnected graphs (and also directed graphs) is controversial. The second shortcoming is that when this measure is used in real-world networks, a huge number of vertices may have exactly the same closeness/eccentricity scores. This restricts the applicability of these indices as they cannot distinguish vertices. The third shortcoming is that in many applications, the distance between two vertices not only depends on the size of shortest paths, but also on the number of shortest paths between them. In this paper, we develop a new distance measure between vertices of a graph that yields discriminative distance-based centrality indices. This measure is proportional to the size of shortest paths and inversely proportional to the number of shortest paths. We present algorithms for exact computation of the proposed discriminative indices. We then develop randomized algorithms that precisely estimate average discriminative path length and average discriminative eccentricity and show that they give (ε, δ )-approximations of these indices (ε ∈ R and δ ∈ (0, 1)). Finally, we preform extensive experiments over several real-world networks from different domains. We first show that compared to the traditional indices, discriminative indices have usually much more discriminability. We then show that our randomized algorithms can very precisely estimate average discriminative path length and average discriminative eccentricity, using only few samples. Our experiments reveal that real-world networks have usually a tiny average discriminative path length, bounded by a constant (e.g., 2). We refer to this property as the tiny-world property.	approximation;average path length;closeness centrality;computation;directed graph;discriminative model;distance (graph theory);existential quantification;experiment;network theory;randomized algorithm;shortest path problem;vertex (graph theory)	Mostafa Haghir Chehreghani;Albert Bifet;Talel Abdessalem	2017	CoRR		combinatorics;discrete mathematics;graph center;machine learning;mathematics;distance	Theory	-11.37188004474823	-41.032488520640065	139208
0d0c71bcd770d6702e5182950ed21f47dfd2c35c	reconstruction of a social network graph from incomplete call detail records	social network services;graph theory;cdr;social networking online graph theory network theory graphs;data mining;cliques;affiliation graph;social network;shape;network dynamics social network graph reconstruction incomplete call detail record call detail data telecommunication operator customer affiliation network graph construction algorithm network cliques;social networking online;joining processes;communications technology;communities;cliques social network affiliation graph bipartite graph parallel processing cdr;network theory graphs;bipartite graph;algorithm design and analysis;parallel processing;social network services communications technology communities data mining joining processes algorithm design and analysis shape	Real-life call detail data (CDR) are used to build a graph of a social network of telecommunication operator customers. Affiliation network is used in graph construction since CDR data are partially kept anonymous. A number of the resulting network properties are examined to prove the correctness of the graph construction algorithm. Cliques in the network and network dynamics are analyzed; suggestions are given about possible utilization of the obtained information in the operation of a telecommunication operator.	algorithm;clique (graph theory);collaboration graph;correctness (computer science);directed graph;graph (discrete mathematics);graph theory;interaction;internet;social network;tracing (software)	Mariusz Kamola;Ewa Niewiadomska-Szynkiewicz;Bartlomiej Cezary Piech	2011	2011 International Conference on Computational Aspects of Social Networks (CASoN)	10.1109/CASON.2011.6085932	network science;parallel processing;algorithm design;information and communications technology;bipartite graph;shape;computer science;connectivity;graph theory;theoretical computer science;machine learning;network simulation;distributed computing;graph;moral graph;random geometric graph;social network	DB	-16.398808447180404	-41.73011323176494	139277
8041cdc2c1033efb88a0d15ae26a4afc5a5e470e	persistent clustered main memory index for accelerating k-nn queries on high dimensional datasets	high dimensional dataset;high dimensionality;query processing;curse of dimensionality;euclidean distance;dimensionality curse;high dimensional data analysis;face recognition;outliers;indexation;k nearest neighbor;content based image retrieval;similarity search	Similarity search implemented via  k -Nearest-Neighbor ( k -NN) queries is an extremely useful paradigm in  content based image retrieval  (CBIR), which is costly on high-dimensional indices due to the curse of dimensionality. We improve  k -NN query processing by utilizing the double filtering effect of clustering and indexing on a persistent version of the Ordered-Partition tree (OP-tree) index, which is highly efficient in processing  k -NN queries. The OP-tree is made persistent by writing it onto disk after serialization, i.e. arranging its nodes into contiguous memory locations, so that the high transfer rate of modern disk drives is exploited. We first report experimental results to optimize OP-tree parameters. We then compare OP-trees and sequential scans with options for the Karhunen-Loeve transform and Euclidean distance calculation. Comparisons against OMNI-based sequential scan are also reported. We finally compare a clustered and persistent version of the OP-tree against a clustered version of the SR-tree and the VA-File method. It is observed that the OP-tree index outperforms the other two methods and that the improvement increases with the number of dimensions.	cluster analysis;computer data storage;content-based image retrieval;curse of dimensionality;database;euclidean distance;full table scan;k-nearest neighbors algorithm;nearest neighbor search;programming paradigm;serialization;similarity search	Lijuan Zhang;Alexander Thomasian	2005		10.1145/1160939.1160955	computer science;machine learning;pattern recognition;data mining	DB	-5.193127490578127	-41.20832579880919	139348
b2c2e51cb0cf5d1ff5f837b8a6f53a382d110408	a life-log search model based on bayesian network	belief networks;information retrieval belief networks probability uncertainty handling multimedia computing;bayesian network;probability;information retrieval;uncertainty handling;engineering all;multimedia computing;bayesian methods electronic mail wearable sensors tv uncertainty global positioning system space technology web pages art information science;search model;data consistency;multimedia life log data search model bayesian network probability density function uncertainty handling	The integration of life-log data in different media enables to represent the same activities from various viewpoints. Integrated life-log data represent contexts each other that are not able to represent in single. Each media has its own characteristic features, and the limitation on its content representation ability. Life-log data consists of records of activities. Examples of life-log data are the e-mail massages that he/she sent and received, TV programs that he/she watched, and photographs that he/she took, and so on. One of the most important characteristic of life-log is that the same activities are represented in different media. This paper focuses on the integrate life-logs in different media and to search them based on their contexts. A problem of the integration is that it is difficult to make correspondences between life-logs in different media types strictly and the correspondences consists uncertainty. In this paper, we introduce a framework based on Bayesian Network.	bayesian network;email;unified framework	Taketoshi Ushiama;Toyohide Watanabe	2004	IEEE Sixth International Symposium on Multimedia Software Engineering	10.1109/MMSE.2004.11	computer vision;computer science;artificial intelligence;data science;machine learning;probability;bayesian network;data mining;data consistency;world wide web;statistics	SE	-15.485906270768194	-51.85311906194054	139486
197ac3c0c57339cf4b6ed2b4d0a32f89ff9f1618	leaders in social networks, the delicious case	search engine;information retrieval;noisy data;search algorithm;leadership;information foraging;websearch;graphs;online community;social support;social network;internet;social networks;random walk;social influence;world wide web;algorithms;source code;humans;computational linguistics;bibliotheque numerique rero doc	Finding pertinent information is not limited to search engines. Online communities can amplify the influence of a small number of power users for the benefit of all other users. Users' information foraging in depth and breadth can be greatly enhanced by choosing suitable leaders. For instance in delicious.com, users subscribe to leaders' collection which lead to a deeper and wider reach not achievable with search engines. To consolidate such collective search, it is essential to utilize the leadership topology and identify influential users. Google's PageRank, as a successful search algorithm in the World Wide Web, turns out to be less effective in networks of people. We thus devise an adaptive and parameter-free algorithm, the LeaderRank, to quantify user influence. We show that LeaderRank outperforms PageRank in terms of ranking effectiveness, as well as robustness against manipulations and noisy data. These results suggest that leaders who are aware of their clout may reinforce the development of social networks, and thus the power of collective search.	anatomy, regional;choose (action);community;information foraging;pagerank;population parameter;relevance;search algorithm;signal-to-noise ratio;social network;web search engine;world wide web	Linyuan Lu;Yi-Cheng Zhang;Chi Ho Yeung;Tao Zhou	2011		10.1371/journal.pone.0021202	bioinformatics;computational linguistics;social network	Web+IR	-17.933457289029565	-41.849640324949846	139633
1a4edf228e648b54fab3f25df82817cfd0a5bb42	slashburn: graph compression and mining beyond caveman communities	graph theory;cost function;hubs and spokes;super hubs graph compression data mining graph mining caveman community clique like community cavemen graph block diagonal mental image hubs collection slashburn method good cuts problem matrix vector operations graph processing tools;data mining;graph mining;vectors;matrix decomposition;graph theory cost function matrix decomposition data mining encoding;communities;graph theory data compression data mining;graph compression;encoding	Given a real world graph, how should we lay-out its edges? How can we compress it? These questions are closely related, and the typical approach so far is to find clique-like communities, like the cavemen graph', and compress them. We show that the block-diagonal mental image of the cavemen graph' is the wrong paradigm, in full agreement with earlier results that real world graphs have no good cuts. Instead, we propose to envision graphs as a collection of hubs connecting spokes, with super-hubs connecting the hubs, and so on, recursively. Based on the idea, we propose the SLASHBURN method to recursively split a graph into hubs and spokes connected only by the hubs. We also propose techniques to select the hubs and give an ordering to the spokes, in addition to the basic SLASHBURN. We give theoretical analysis of the proposed hub selection methods. Our view point has several advantages: (a) it avoids the no good cuts' problem, (b) it gives better compression, and (c) it leads to faster execution times for matrix-vector operations, which are the back-bone of most graph processing tools. Through experiments, we show that SLASHBURN consistently outperforms other methods for all data sets, resulting in better compression and faster running time. Moreover, we show that SLASHBURN with the appropriate spokes ordering can further improve compression while hardly sacrificing the running time.	adjacency matrix;clique (graph theory);disk space;emoticon;experiment;graph (abstract data type);greedy algorithm;inner loop;matrix multiplication;optical disc authoring;pagerank;programming paradigm;recursion;requirement;structure mining;the hub (forum);time complexity;usb hub	Yongsub Lim;U. Kang;Christos Faloutsos	2014	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2014.2320716	graph power;factor-critical graph;combinatorics;directed graph;graph bandwidth;null graph;graph property;regular graph;clique-width;graph theory;theoretical computer science;simplex graph;machine learning;data mining;mathematics;voltage graph;distance-hereditary graph;graph;moral graph;matrix decomposition;butterfly graph;complement graph;line graph;strength of a graph;encoding	ML	-9.524332889577435	-41.00332873912979	139989
cb69de9c6c1527c69d729d54423987e819589c85	nearest neighbors graph construction: peer sampling to the rescue	k nearest neighbors;randomness;sampling;clustering	In this paper, we propose an efficient KNN service, called KPS (KNN-Peer-Sampling). The KPS service can be used in various contexts e.g. recommendation systems, information retrieval and data mining. KPS borrows concepts from P2P gossip-based clustering protocols to provide a localized and efficient KNN computation in large-scale systems. KPS is a sampling-based iterative approach, combining ran-domness, to provide serendipity and avoid local minimum, and clustering , to ensure fast convergence. We compare KPS against the state of the art KNN centralized computation algorithm NNDescent, on multiple datasets. The experiments confirm the efficiency of KPS over NNDescent: KPS improves significantly on the computational cost while converging quickly to a close to optimal KNN graph. For instance, the cost, expressed in number of pairwise similarity computations, is reduced by ≈ 23% and ≈ 49% to construct high quality KNN graphs for Jester and MovieLens datasets, respectively. In addition, the randomized nature of KPS ensures eventual convergence, not always achieved with NNDescent.		Yahya Benkaouz;Mohammed Erradi;Anne-Marie Kermarrec	2016		10.1007/978-3-319-46140-3_4	computer science;theoretical computer science;machine learning;data mining	Theory	-9.016306036666649	-41.86507441060703	140270
d9cf0ba8f32245ba5ff51b7a05fa6711914e960d	opinion dynamics with varying susceptibility to persuasion		A long line of work in social psychology has studied variations in peopleu0027s susceptibility to persuasion -- the extent to which they are willing to modify their opinions on a topic. This body of literature suggests an interesting perspective on theoretical models of opinion formation on social networks: in addition to considering interventions that directly modify peopleu0027s intrinsic opinions, it is also natural to consider those that modify peopleu0027s susceptibility to persuasion. Here, we adopt a popular model for social opinion dynamics, and formalize the opinion maximization and minimization problems where interventions happen at the level of susceptibility. We show that modeling interventions at the level of susceptibility leads to an interesting family of new questions in network opinion dynamics. We find that the questions are quite different depending on whether there is an overall budget constraining the number of agents we can target or not. We give a polynomial-time algorithm for finding the optimal target-set to optimize the sum of opinions when there are no budget constraints on the size of the target-set. We show that this problem is NP-hard when there is a budget, and that the objective function is neither submodular nor supermodular. Finally, we propose a heuristic for the budgeted opinion optimization problem and show its efficacy at finding target-sets that optimize the sum of opinions on real world networks, including a Twitter network with real opinion estimates.	approximation algorithm;decision problem;entropy maximization;expectation–maximization algorithm;greedy algorithm;heuristic (computer science);keneth alden simons;local search (optimization);long line (telecommunications);mathematical optimization;np-hardness;optimization problem;scalability;search algorithm;simulation;social network;submodular set function;supermodular function;synthetic intelligence;time complexity	Rediet Abebe;Jon M. Kleinberg;David C. Parkes;Charalampos E. Tsourakakis	2018		10.1145/3219819.3219983	submodular set function;artificial intelligence;machine learning;computer science;persuasion;management science;psychological intervention;social network;maximization;heuristic;optimization problem;budget constraint	ML	-16.798621665129772	-43.476092005450525	140320
d69236504260502207f4ae81f37f343bcce53285	information filtering based on personalized topology information	topology;motion pictures;training;information filtering;prediction algorithms;会议论文;data mining;temporal information;topology information;network topology;recommender systems	In the recommendation of the bipartite networks, researchers have mainly dedicated to improve the accuracy of the recommendation, but neglected the fact that the entire history information which can be redundant or even misleading to the performance of recommendation. In this paper, we set unique weight to every link according to their temporal information and topology information. Then, we remove the links according to their weight. Experimental results on a number of real networks show that the algorithm improves the recommendation accuracy, meanwhile, largely shortens the computational time complexity and reduces the data storage space.	algorithm;computer data storage;computer science;information filtering system;lattice boltzmann methods;personalization;time complexity	Bolun Chen;Ling Chen	2015	2015 Third International Conference on Advanced Cloud and Big Data	10.1109/CBD.2015.38	computer science;theoretical computer science;information filtering system;machine learning;data mining	Robotics	-12.9071322377777	-42.46392325852988	140431
32c221c7d27972523976bcb3cd670d2a3b47ccf4	containment of competitive influence spread in social networks	sub modularity;influence containment;competitive influence spread;linear threshold model;greedy algorithm;diffusion containment model	To contain the competitive influence spread in social networks is to maximize the influence of one participant and contain the influence of its opponent. It is desirable to develop effective strategies for influence spread of the participants themselves instead of blocking the influence spread of their opponents. In this paper, we extend the linear threshold model to establish the diffusion-containment model, abbreviated as D-C model, by incorporating the realistic specialties and characteristics of the containment of competitive influence spread. Then, we discuss the influence spread mechanism for the D-C model, and give the algorithm for the propagation of the diffusion influence (D-influence) and containment influence (C-influence). Further, we define the sub-modular set function of the C-influence in the D-C model and consequently give a greedy algorithm for solving the problem of maximizing the competitive influence containment approximately. Experimental results show the feasibility of our method.	social network	Weiyi Liu;Kun Yue;Hong Wu;Jin Li;Donghua Liu;Duanping Tang	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.07.008	greedy algorithm;simulation;computer science;operations research	ECom	-16.877317889989314	-43.71979246322273	140460
a6926a9c75f3a5018761a11627a93390ee95984c	detecting communities using social ties	social network services;hierarchical clustering;community detection;pattern clustering;measurement;disjoint communities;cluster quality measurement;hierarchical clustering method;social ties;social networking online data analysis pattern clustering;clustering coefficient social network analysis community detection hierarchical clustering;social network;data analysis;clustering coefficient notion;image edge detection;clustering coefficient;community quality measurement;information dissemination;data visualization;social networking online;joining processes;communities social network services image edge detection clustering algorithms measurement data visualization joining processes;clustering algorithms;social network analysis;communities;cluster quality measurement community detection social network analysis hierarchical clustering method disjoint communities clustering coefficient notion community quality measurement;viral marketing	Many internet-based applications such as social networking websites, online viral marketing, and recommendation network based applications, use social network analysis to improve performance in terms of user-specific information dissemination. The notion of community in a social network is a key concept in such analyses and there has been significant work recently in identifying communities within a social network. In this paper, we formally define the notion of strength of a link, which was informally introduced by Granovetter, and present a divisive hierarchical clustering method to divide the nodes of a social network into disjoint communities. We also introduce the notion of clustering coefficient as a measure of the quality of a community or cluster. Our experimental results using some well-known benchmark social networks show that our method determines communities with better clustering coefficient than the well known Girvan-Newman method.	algorithm;benchmark (computing);cluster analysis;clustering coefficient;experiment;grid north;hierarchical clustering;internet;iteration;sensor;social network analysis;whole earth 'lectronic link	Partha Basuchowdhuri;Jianhua Chen	2010	2010 IEEE International Conference on Granular Computing	10.1109/GrC.2010.141	interpersonal ties;social network analysis;weighted network;computer science;dynamic network analysis;machine learning;hierarchical network model;data mining;clustering coefficient;viral marketing;hierarchical clustering;cluster analysis;data analysis;world wide web;data visualization;measurement;statistics;social network	DB	-15.138366665280174	-42.590777883547	140635
0505d1d9cb95bb420ec34fc029358aa9cdda5cfc	planarity as a driver of spatial network structure	qa75 electronic computers computer science	In this paper we introduce a new model of spatial network growth in which nodes are placed at randomly selected locations in space over time, forming new connections to old nodes subject to the constraint that edges do not cross. The resulting network has a power law degree distribution, high clustering and the small world property. We argue that these characteristics are a consequence of two features of our mechanism, growth and planarity conservation. We further propose that our model can be understood as a variant of random Apollonian growth. We then investigate the robustness of our findings by relaxing the planarity. Specifically, we allow edges to cross with a defined probability. Varying this probability demonstrates a smooth transition from a power law to an exponential degree distribution.	algorithm;apollonian network;assortativity;baseline (configuration management);cluster analysis;clustering coefficient;degree distribution;graph drawing;left-right planarity test;planarity;randomness;small-world experiment;spatial network;stepwise regression;time complexity;triune continuum paradigm	Garvin Haslett;Markus Brede	2015		10.7551/978-0-262-33027-5-ch075	degree distribution;computer science	Web+IR	-16.510305806219403	-39.96663253054638	140676
5c1a875fb9fc982e7b387ccc61738b9fdc66b948	a semantic-rich similarity measure in heterogeneous information networks		Abstract Most of the existing similarity metrics in heterogeneous information networks depend on the pre-specified meta-path or meta-structure. This dependency may cause them to be sensitive to different meta-paths or meta-structures. In this paper, we propose a stratified meta-structure-based similarity measure named SMSS in heterogeneous information networks. The stratified meta-structure can be constructed automatically and capture rich semantics.Then, we define the commuting matrix of the stratified meta-structure by virtue of the commuting matrices of meta-paths and meta-structures. As a result, the SMSS is defined by virtue of this commuting matrix. Experimental evaluations show that the existing metrics are sensitive to different meta-paths or meta-structures and that the proposed SMSS outperforms the state-of-the-art metrics in terms of ranking and clustering.	cluster analysis;meta element;recommender system;similarity measure;web search engine	Yu Zhou;Jianbin Huang;Heli Sun	2018	Knowl.-Based Syst.	10.1016/j.knosys.2018.05.010	machine learning;artificial intelligence;computer science;cluster analysis;similarity measure;matrix (mathematics);ranking	ML	-16.021838521386645	-47.28099010517012	140931
6c28950713bbf6419a253d4e3c256693cc41cbc7	extracting diffusion channels from real-world social data: a delay-agnostic learning of transmission probabilities	social network services;social networking online diffusion information theory learning artificial intelligence;question routing;expertise estimation;integrated circuit modeling probability distribution social network services delay effects data models predictive models;delay effects;information diffusion diffusion channel extraction real world social data delay agnostic learning relaxed learning process independent cascade model infection probabilities transmission probabilities;probability distribution;integrated circuit modeling;question recommendation;predictive models;community question answering;non qa data;data models	Probabilistic cascade models consider information diffusion as an iterative process in which information transits from users to others in a network. The problem of diffusion modeling then comes down to learning transmission probability distributions, depending on hidden influence relationships between users, in order to discover the main diffusion channels of the network. Various learning models have been proposed in the literature, but we argue that the diffusion mechanisms defined in most of these models are too complex for real social networks, where transmissions of content occur between human users. Classical models usually have some difficulties for extracting the main regularities in such real-world settings. In this paper, we propose a relaxed learning process of the well-known Independent Cascade model that, rather than attempting to explain exact timestamps of users' infections, focus on infection probabilities knowing sets of previously infected users. Experiments show the effectiveness of our proposals, by considering the learned models for real-world prediction tasks.	experiment;iteration;social network;whole earth 'lectronic link	Sylvain Lamprier;Simon Bourigault;Patrick Gallinari	2015	2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1145/2808797.2808865	probability distribution;data modeling;computer science;artificial intelligence;machine learning;data mining;predictive modelling;world wide web;statistics	ML	-18.162973802651063	-43.693743639360854	141434
14485e6e745d8e46f4d4e0523d16f180fd974f3f	social playlists and bottleneck measurements: exploiting musician social graphs using content-based dissimilarity and pairwise maximum flow values.	maximum flow;social network;earth mover s distance;minimum cut	We have sampled the artist social network of Myspace and to it applied the pairwise relational connectivity measure Minimum cut/Maximum flow. These values are then compared to a pairwise acoustic Earth Mover’s Distance measure and the relationship is discussed. Further, a means of constructing playlists using the maximum flow value to exploit both the social and acoustic distances is realized.	acoustic cryptanalysis;maximum flow problem;minimum cut;social network	Benjamin Fields;Christophe Rhodes;Michael A. Casey;Kurt Jacobson	2008			earth mover's distance;maximum flow problem;mathematical optimization;minimum cut;computer science;artificial intelligence;machine learning;mathematics;social network	AI	-11.478039736803078	-42.36643703695802	141999
acde597e9a3c47612acadb9b8c0b1daf74e54054	multi-competitive viruses over static and time-varying networks		Epidemic processes are used commonly for modeling and analysis of biological networks, computer networks, and human contact networks. The idea of competing viruses has been explored recently, motivated by the spread of different ideas along different social networks. Previous studies of competitive viruses have focused only on two viruses and on static networks. In this paper, we consider multiple competing viruses over static and dynamic graph structures, and investigate the eradication and propagation of diseases in these systems. Stability analysis for the class of models we consider is performed and an antidote control technique is proposed.	biological network;centralized computing;computer virus;social network;software propagation;time-varying network	Philip E. Pare;Ji Liu;Carolyn L. Beck;Angelia Nedic;Tamer Basar	2017	2017 American Control Conference (ACC)	10.23919/ACC.2017.7963195	computer science;mathematical optimization;biological network;theoretical computer science;social network;distributed computing;graph		-17.06370863154346	-41.74744106018149	142101
9f2a9384a111ebfb369d55c62b684e2c1a961d9c	egoclustering: overlapping community detection via merged friendship-groups		There has been considerable interest in identifying communities within large collections of social networking data. Existing algorithms will classify an actor (node) into a single group, ignoring the fact that in real-world situations people tend to belong concurrently to multiple (overlapping) groups. Our work focuses on the ability to find overlapping communities. We use egonets to form friendship-groups. A friendship-group is a localized community as seen from an individual’s perspective that allows an actor to belong to multiple communities. Our algorithm finds overlapping communities and identifies key members that bind communities together. Additionally, we will highlight the parallel feature of the algorithm as a means of improving runtime performance, and the ability of the algorithm to run within a database and not be constrained by system memory.		Bradley S. Rees;Keith Brian Gallagher	2013		10.1007/978-3-7091-1346-2_1	friendship;dense graph;business;distributed computing;betweenness centrality;social network	HCI	-15.610309366008822	-42.01973046139054	142200
2150a4461bdaac271b260cc740f8a46ad8f03901	learning sparse models at scale	zipf distribution;optimizing hyper parameters;recommendations;sparse models;scalable machine learning;advertising	Recently, learning deep models from dense data has received a lot of attention in tasks such as object recognition and signal processing. However, when dealing with non-sensory data about real-world entities, data is often sparse; for example people interaction with products in e-Commerce, people interacting with each other in social networks or word sequences in natural language. In this talk, I will share lessons learned over the past 10 years when learning predictive models based on sparse data: 1) how to scale the inference algorithms to distributed data setting, 2) how to automate the learning process by reducing the amount of hyper-parameters to zero, 3) how to deal with Zipf distributions when learning resource-constrained models, and 4) how to combine dense and sparse-learning algorithms. The talk will be drawing from many real-world experiences I gathered over the past decade in applications of the techniques in gaming, search, advertising and recommendations of systems developed at Microsoft, Facebook and Amazon.	algorithm;e-commerce;entity;experience;interaction;machine learning;natural language;outline of object recognition;predictive modelling;signal processing;social network;sparse matrix;zipf's law	Ralf Herbrich	2016		10.1145/2939672.2945362	zipf's law;computer science;data science;machine learning;data mining;statistics	ML	-18.782580381555828	-50.36959177940685	142293
38170a0523ccef8a3298391c9d02d7a7a0c4f707	inferring the strength of social ties: a community-driven approach		Online social networks are growing and becoming denser.The social connections of a given person may have very high variability: from close friends and relatives to acquaintances to people who hardly know. Inferring the strength of social ties is an important ingredient for modeling the interaction of users in a network and understanding their behavior. Furthermore, the problem has applications in computational social science, viral marketing, and people recommendation. In this paper we study the problem of inferring the strength of social ties in a given network. Our work is motivated by a recent approach by Sintos et. al [24], which leverages the Strong Triadic Closure} STC principle, a hypothesis rooted in social psychology. To guide our inference process, in addition to the network structure, we also consider as input a collection of tight communities. Those are sets of vertices that we expect to be connected via strong ties. Such communities appear in different situations, e.g., when being part of a community implies a strong connection to one of the existing members. We consider two related problem formalizations that reflect the assumptions of our setting: small number of STC violations and strong-tie connectivity in the input communities. We show that both problem formulations are NP-hard. We also show that one problem formulation is hard to approximate, while for the second we develop an algorithm with approximation guarantee. We validate the proposed method on real-world datasets by comparing with baselines that optimize STC violations and community connectivity separately.		Polina Rozenshtein;Nikolaj Tatti;Aristides Gionis	2017		10.1145/3097983.3098199	computer science;social network analysis;computational sociology;artificial intelligence;machine learning;triadic closure;data mining;approximation algorithm;interpersonal ties;viral marketing;social network;inference	ML	-17.536717041775173	-42.986759224207745	142369
06a7c2f746480ab00036393df311a2eda17a14c5	conceptual descriptions from monitoring and watching image sequences	surveillance;scene interpretation;behaviour modelling;image sequence;action understanding;high level vision;dynamic scenes	This paper contrasts two ways of forming conceptual descriptions from images. The first, called “monitoring”, just follows the flow of data from images to interpretation, having little need for top-level control. The second, called “watching”, emphasizes the use of top-level control and actively selects evidence for task-based descriptions of the dynamic scenes. Here we look at the effect this has on forming conceptual descriptions. First, we look at how motion verbs and the perception of events contribute to an effective representational scheme. Then we go on to discuss illustrated examples of computing conceptual descriptions from images in our implementations of the monitoring and watching systems. Finally, we discuss future plans and related work. q 2000 Elsevier Science B.V. All rights reserved.	dataflow	Richard J. Howarth;Hilary Buxton	2000	Image Vision Comput.	10.1016/S0262-8856(99)00025-6	computer vision;computer science;artificial intelligence	AI	-11.873245557349001	-52.064933275840765	142388
88e8cf04464e11df674919be2e1afe56e658eec2	community mining using three closely joint techniques based on community mutual membership and refinement strategy		Abstract Community structure has become one of the central studies of the topological structure of complex networks in the past decades. Although many advanced approaches have been proposed to identify community structure, those state-of-the-art methods still lack efficiency in terms of a balance between stability, accuracy and computation time. Here, we propose an algorithm with different stages, called TJA-net, to efficiently identify communities in a large network with a good balance between accuracy, stability and computation time. First, we propose an initial labeling algorithm, called ILPA, combining K-nearest neighbor (KNN) and label propagation algorithm (LPA). To produce a number of sub-communities automatically, ILPA iteratively labels a node in a network using the labels of its adjacent nodes and their index of closeness. Next, we merge sub-communities using the mutual membership of two communities. Finally, a refinement strategy is designed for modifying the label of the wrongly clustered nodes at boundaries. In our approach, we propose and use modularity density as the objective function rather than the commonly used modularity. This can deal with the issue of the resolution limit for different network structures enhancing the result precision. We present a series of experiments with artificial and real data set and compare the results obtained by our proposed algorithm with the ones obtained by the state-of-the-art algorithms, which shows the effectiveness of our proposed approach. The experimental results on large-scale artificial networks and real networks illustrate the superiority of our algorithm.	refinement (computing)	Ronghua Shang;Huan Liu;Licheng Jiao;Amir Masoud Ghalamzan Esfahani	2017	Appl. Soft Comput.	10.1016/j.asoc.2017.08.050	machine learning;mathematical optimization;merge (version control);artificial intelligence;computation;community structure;complex network;modularity;closeness;computer science	Robotics	-13.685730636270673	-42.8255130925379	142946
3b43a9ec2708a2911eb14e8c3f94832aac6c5ff9	of toasters and molecular ticker tapes	software;animals;models neurological;brain;sequence analysis dna;molecular biology;neurosciences;humans;computational biology;biomedical research	Experiments in systems neuroscience can be seen as consisting of three steps: (1) selecting the signals we are interested in, (2) probing the system with carefully chosen stimuli, and (3) getting data out of the brain. Here I discuss how emerging techniques in molecular biology are starting to improve these three steps. To estimate its future impact on experimental neuroscience, I will stress the analogy of ongoing progress with that of microprocessor production techniques. These techniques have allowed computers to simplify countless problems; because they are easier to use than mechanical timers, they are even built into toasters. Molecular biology may advance even faster than computer speeds and has made immense progress in understanding and designing molecules. These advancements may in turn produce impressive improvements to each of the three steps, ultimately shifting the bottleneck from obtaining data to interpreting it.	biomedical tape;bottleneck (engineering);computer;computers;microprocessor;molecular biology;neuroscience discipline;systems neuroscience;timer	Konrad P. Körding	2011		10.1371/journal.pcbi.1002291	biology;medical research;computer science;bioinformatics	Theory	-7.511450148917796	-48.64570410421177	143088
4cc8e7f3fd7de2e888b9dfc6f55aa6f53b011697	ring: an integrated method for frequent representative subgraph mining	usa councils data mining multidimensional systems social network services pattern analysis testing monte carlo methods spatial databases indexing distributed computing;graph theory;trees mathematics data mining graph theory;pattern mining process;empirical study;frequent pattern;ring algorithm;trees mathematics;data mining;runtime;pattern mining;invariant vectors;proteins;indexing;frequent representative subgraph mining;r trees frequent representative subgraph mining ring algorithm invariant vectors pattern mining process;r trees;subgraph mining representative;gallium	We propose a novel representative based subgraph mining model. A series of standards and methods are proposed to select invariants. Patterns are mapped into invariant vectors in a multidimensional space. To find qualified patterns, only a subset of frequent patterns is generated as representatives, such that every frequent pattern is close to one of the representative patterns while representative patterns are distant from each other. We devise the RING algorithm, integrating the representative selection into the pattern mining process. Meanwhile, we use R-trees to assist this mining process. Last but not least, a large number of real and synthetic datasets are employed for the empirical study, which show the benefits of the representative model and the efficiency of the RING algorithm.	algorithm;data mining;invariant (computer science);r* tree;synthetic intelligence	Shijie Zhang;Jiong Yang;Shirong Li	2009	2009 Ninth IEEE International Conference on Data Mining	10.1109/ICDM.2009.96	r-tree;search engine indexing;computer science;graph theory;data science;machine learning;data mining;mathematics;empirical research;gallium	DB	-6.709951024728571	-39.25465400391726	143529
7b29548b5b81498a99f8ed3efc8f0cd6e7919f48	dealing with the best attachment problem via heuristics		Ordering nodes by rank is a benchmark used in several contexts, from recommendation-based trust networks to e-commerce, search engines and websites ranking. In these scenarios, the node rank depends on the set of links the node establishes, hence it becomes important to choose appropriately the nodes to connect to. The problem of finding which nodes to connect to in order to achieve the best possible rank is known as the best attachment problem. Since in the general case the best attachment problem is NP-hard, in this work we propose heuristics that produce near-optimal results while being computable in polynomial time; simulations on different networks show that our proposals preserve both effectiveness and feasibility in obtaining the best rank.	attachments;heuristic (computer science)	Marco Buzzanca;Vincenza Carchiolo;Alessandro Longheu;Michele Malgeri;Giuseppe Mangioni	2016		10.1007/978-3-319-48829-5_20	mathematical optimization;time complexity;degree distribution;search engine;heuristics;mathematics;ranking	NLP	-16.186168414195645	-44.09428331087945	144005
13a20be39269d37984617e13bbcba3f6be5027d0	reverse engineering an agent-based hidden markov model for complex social systems	model identification;learning process;driving force;agent based;hidden markov model;social system;optimization problem;machine learning;social groups;synthetic data;social evolution;state transition;reverse engineering	The power of social values that helps to shape or formulate ou r behavior patterns is not only inevitable, but also how we have surr eptitiously responded to the hidden curriculum that derives from such social value s in our decision making can be just as significant. Through a machine learning app ro ch, we are able to discover the agent dynamics that drives the evolution of t he social groups in a community. By doing so, we set up the problem by introducing a agent-based hidden Markov model, in which the acts of an agent are determi ned bymicrolaws with unknown parameters. To solve the problem, we develop a m ultistage learning process for determining the micro-laws of a community based on observed set of communications between actors without the sem antic contents. We present the results of extensive experiments on synthetic d a a as well as some results on real communities, e.g., Enron email and movie newsgroups.	agent-based model;algorithm;email;experiment;heuristic (computer science);hidden markov model;machine learning;markov chain;multistage amplifier;reverse engineering;social system;synthetic data;synthetic intelligence;value (ethics)	Hung-Ching Chen;Mark K. Goldberg;Malik Magdon-Ismail;William A. Wallace	2007		10.1007/978-3-540-77226-2_94	social group;optimization problem;simulation;system identification;computer science;social evolution;artificial intelligence;machine learning;data mining;social system;hidden markov model;reverse engineering;synthetic data	AI	-18.59753865274131	-42.91788842775539	144068
914f68b9c73e0700ccb69c5006fb069d59185ebd	artificial life as a tool for biological inquiry	artificial life;new google;function drawchart;load alert2;var data;function loadalert2;function letemknow;load alert;function loadalert;biological inquiry function settab;function testthis;emergent properties;natural selection;origin of life;cultural evolution;evolution	Artificial life embraces those human-made systems that possess some of the key properties of natural life. We are specifically interested in artificial systems that serve as models of living systems for the investigation of open questions in biology. First we review some of the artificial life models that have been constructed with biological problems in mind, and classify them by medium (hardware, software, or wetware) and by level of organization (molecular, cellular, organismal, or population). We then describe several grand challenge open problems in biology that seem especially good candidates to benefit from artificial life studies, including the origin of life and self-organi- zation, cultural evolution, origin and maintenance of sex, shifting balance in evolution, the relation between fitness and adaptedness, the structure of ecosystems, and the nature of mind.	artificial intelligence;artificial life;ecosystem;grand challenges;living systems;wetware (brain)	Charles E. Taylor;David Jefferson	1993	Artificial Life	10.1162/artl.1993.1.1_2.1	biology;zoology;natural selection;computer science;artificial intelligence;evolution;artificial creation;artificial life;sociocultural evolution		-6.301885074749649	-47.79642431391014	144661
96cb0dc27f9b9fc827aff76f1925473b6d66d5d9	topological analysis of knowledge maps	complex network;learning dependency;topological analysis;journal;topological property;期刊论文;knowledge map	A knowledge map can be viewed as a directed graph, in which each node is a knowledge unit (KU), and each edge is a learning-dependency between two KUs. Understanding the topological properties of knowledge map can help us gain better insights into human cognition structure and its mechanism, design better knowledge map construction algorithms, and guide learners' navigational learning through knowledge map. In this paper, we perform topological analysis on 12 knowledge maps from computer science, mathematics, and physics. We discover that they exhibit small-world and scale-free properties like many other networks. Specifically, we show the locality of learning-dependency and hierarchical modular structure in the 12 knowledge maps. In addition, we study how KUs affect the network efficiency by removing KUs based on different centrality measures. We find that the importance of KUs varies greatly.	cognitive map	Jun Liu;Jincheng Wang;Qinghua Zheng;Wei Zhang;Lu Jiang	2012	Knowl.-Based Syst.	10.1016/j.knosys.2012.07.011	computer science;knowledge management;artificial intelligence;machine learning;complex network	NLP	-16.58940836050544	-39.20624924162498	144890
474f44e58989504713efe0c6cadabec94593d41d	jtav: jointly learning social media content representation by fusing textual, acoustic, and visual features		Learning social media content is the basis of many real-world applications, including information retrieval and recommendation systems, among others. In contrast with previous works that focus mainly on single modal or bi-modal learning, we propose to learn social media content by fusing jointly textual, acoustic, and visual information (JTAV). Effective strategies are proposed to extract fine-grained features of each modality, that is, attBiGRU and DCRNN. We also introduce cross-modal fusion and attentive pooling techniques to integrate multi-modal information comprehensively. Extensive experimental evaluation conducted on real-world datasets demonstrates our proposed model outperforms the state-of-the-art approaches by a large margin.	acoustic cryptanalysis;attentive user interface;information retrieval;list of content management frameworks;modal logic;modality (human–computer interaction);recommender system;social media	Hongru Liang;Haozheng Wang;Jun Wang;Shaodi You;Zhe Sun;Jin-Mao Wei;Zhenglu Yang	2018			recommender system;natural language processing;machine learning;artificial intelligence;computer science;social media;pooling	AI	-18.315553812842516	-48.75541351945747	145172
bcfe5ee5ba42439f3a1b8fb1395d178d3b5783cb	"""quantitative analysis of robot-environment interaction - towards """"scientific mobile robotics"""""""	scientific mobile robotics;chaos theory applied to mobile robotics;mobile robot;computer model;scientific method;mobile robotics research;quantitative descriptions of robot environment interaction;quantitative analysis;natural science;chaos theory	Quantitative descriptions of a physical system’s behaviour form the backbone of the scientific method used in the natural sciences. They allow the principled determination of experimental parameters, a clear and unambiguous representation of experiments, and independent replication and verification of experimental results. In mobile robotics to date, quantitative descriptions of robot-environment interaction remain the exception, chiefly due to the lack of those descriptions. Instead, qualitative descriptions of experiments and existence proofs (i.e. unvalidated experimental results) are the norm. This paper discusses this problem, and presents a novel method of describing robot-environment interaction quantitatively — a first step towards scientific mobile robotics. The application of this novel method is illustrated on an example taken from mobile robotics: the comparison between a Nomad 200 mobile robot and its computer model.	amiga walker;computer simulation;experiment;internet backbone;mobile robot;nomad 200;numerical weather prediction;robot learning;robotics	Ulrich Nehmzow	2003	Robotics and Autonomous Systems	10.1016/S0921-8890(03)00010-1	computer simulation;mobile robot;natural science;simulation;scientific method;computer science;quantitative analysis;artificial intelligence;chaos theory	Robotics	-7.391606117794065	-50.81095321092507	145531
1f53c5b566564e6d59325d9b18f4308f6ea78705	content to node: self-translation network embedding		This paper concerns the problem of network embedding (NE), whose aim is to learn low-dimensional representations for nodes in networks. Such dense vector representations offer great promises for many network analysis problems. However, existing NE approaches are still faced with challenges posed by the characteristics of complex networks in real-world applications. First, for many real-world networks associated with rich content information, previous NE methods tend to learn separated content and structure representations for each node, which requires a post-processing of combination. The empirical and simple combination strategies often make the final vector suboptimal. Second, the existing NE methods preserve the structure information by considering short and fixed neighborhood scope, such as the first- and/or the second-order proximities. However, it is hard to decide the scope of the neighborhood when facing a complex problem. To this end, we propose a novel sequence-to-sequence model based NE framework which is referred to as Self-Translation Network Embedding (STNE) model. With the sequences generated by random walks on a network, STNE learns the mapping that translates each sequence itself from the content sequence to the node sequence. On the one hand, the bi-directional LSTM encoder of STNE fuses the content and structure information seamlessly from the raw input. On the other hand, high-order proximity can be flexibly learned with the memories of LSTM to capture long-range structural information. By such self-translation from content to node, the learned hidden representations can be adopted as node embeddings. Extensive experimental results based on three real-world datasets demonstrate that the proposed STNE outperforms the state-of-the-art NE approaches. To facilitate reproduction and further study, we provide Internet access to the code and datasets\footnotehttp://dm.nankai.edu.cn/code/STNE.rar.	complex network;context awareness;encoder;end-to-end principle;experiment;internet access;long short-term memory;neural machine translation;video post-processing	Jie Liu;Zhicheng He;Lai Wei;Yalou Huang	2018		10.1145/3219819.3219988	complex network;machine learning;internet access;random walk;computer science;network analysis;encoder;artificial intelligence;embedding;feature learning	AI	-14.356293825372518	-46.462009837108845	145904
9aba099a14ba4e770e22e644ad09cfb66c68c543	exploring social influence for recommendation - a probabilistic generative model approach	generic model;information retrieval;information network;unified model;large scale;single machine;expectation maximization;collaborative filtering;social influence;parallel implementation;em algorithm	In this paper, we propose a probabilistic generative model, called unified model, which naturally unifies the ideas of social influence, collaborative filtering and content-based methods for item recommendation. To address the issue of hidden social influence, we devise new algorithms to learn the model parameters of our proposal based on expectation maximization (EM). In addition to a single-machine version of our EM algorithm, we further devise a parallelized implementation on the Map-Reduce framework to process two large-scale datasets we collect. Moreover, we show that the social influence obtained from our generative models can be used for group recommendation. Finally, we conduct comprehensive experiments using the datasets crawled from last.fm and whrrl.com to validate our ideas. Experimental results show that the generative models with social influence significantly outperform those without incorporating social influence. The unified generative model proposed in this paper obtains the best performance. Moreover, our study on social influence finds that users in whrrl.com are more likely to get influenced by friends than those in last.fm. The experimental results also confirm that our social influence based group recommendation algorithm outperforms the state-of-the-art algorithms for group recommendation.	collaborative filtering;expectation–maximization algorithm;experiment;generative model;last.fm;mapreduce;parallel computing;unified model	Mao Ye;Xingjie Liu;Wang-Chien Lee	2011	CoRR		expectation–maximization algorithm;computer science;data science;machine learning;data mining;world wide web;statistics	ML	-18.48761050648902	-47.30529428205074	145984
60af0d7e6c0a987e2e51ea6ad31a664915bd41ae	an iterative hypothesis-testing strategy for pattern discovery	uncertainty;large data sets;data mining;statistical models;statistical model;outlier detection;pattern discovery;data storage;residual analysis;hypothesis test	Pattern discovery has emerged as a direct result of increased data storage and analytic capabilities available to the data analyst. Without a massive amount of data, we do not have the evidence to support the discovery of the local deterministic structures that we call patterns. As such, pattern discovery is one of the few areas of data mining that cannot be considered simply as a 'scaling-up' of current statistical methodology to analyze large data sets. However, the philosophies of hypothesis testing and modeling in traditional statistics do lend themselves to forming a framework for pattern discovery, and we can also draw from ideas relating to outlier discovery and residual analysis to discover patterns. We illustrate an iterative strategy in a statistical framework by way of its application to one simulated and two real data sets.	computer data storage;data mining;image scaling;iterative method	Richard J. Bolton;Niall M. Adams	2003		10.1145/956750.956760	statistical model;anomaly detection;computer science;data science;discovery science;data mining;exploratory data analysis;k-optimal pattern discovery;statistics	ML	-12.898411016119743	-38.68042487271398	146228
7c2639f1caea2352f2e09c562bfff051a5174134	rlcf: a collaborative filtering approach based on reinforcement learning with sequential ratings	ensemble learning;q learning;markov decision process;recommender systems	We present a novel approach for collaborative filtering, RLCF, that considers the dynamics of user ratings. RLCF is based on reinforcement learning applied to the sequence of ratings. First, we formalize the collaborative filtering problem as a Markov Decision Process. Then, we learn the connection between the temporal sequences of user ratings using Q-learning. Experiments demonstrate the feasibility of our approach and a tight relationship between the past and the current ratings. We also suggest an ensemble learning in RLCF and demonstrate its improved performance. © 2016 tSI® press	collaborative filtering;ensemble learning;markov chain;markov decision process;q-learning;reinforcement learning;time-slot interchange	Jung-Kyu Lee;Byonghwa Oh;Jihoon Yang;Unsang Park	2017	Intelligent Automation & Soft Computing	10.1080/10798587.2016.1231510	q-learning;recommender system;machine learning;collaborative filtering;reinforcement learning;computer science;artificial intelligence;markov decision process;ensemble learning;pattern recognition	AI	-16.722521826476292	-49.69640613487302	146311
288366387becf5a15eda72c94a18e5b4f179578f	persistence and periodicity in a dynamic proximity network		The topology of social networks can be understood as being inherently dynamic, with edges having a distinct position in time. Most characterizations of dynamic networks discretize time by converting temporal information into a sequence of network “snapshots” for further analysis. Here we study a highly resolved data set of a dynamic proximity network of 66 individuals. We show that the topology of this network evolves over a very broad distribution of time scales, that its behavior is characterized by strong periodicities driven by external calendar cycles, and that the conversion of inherently continuous-time data into a sequence of snapshots can produce highly biased estimates of network structure. We suggest that dynamic social networks exhibit a natural time scale ∆nat, and that the best conversion of such dynamic data to a discrete sequence of networks is done at this natural rate.	discretization;dynamic data;persistence (computer science);quasiperiodicity;social network	Aaron Clauset;Nathan Eagle	2007	CoRR		real-time computing;simulation;telecommunications;dynamic network analysis	ML	-11.89907865329592	-39.31314732483454	146315
6c02053805434162e0fed26e1d5e035eb1071249	autorec: autoencoders meet collaborative filtering	conference paper;collaborative filtering;autoencoders;recommender systems	This paper proposes AutoRec, a novel autoencoder framework for collaborative filtering (CF). Empirically, AutoRec's compact and efficiently trainable model outperforms state-of-the-art CF techniques (biased matrix factorization, RBM-CF and LLORMA) on the Movielens and Netflix datasets.	autoencoder;collaborative filtering;movielens;restricted boltzmann machine	Suvash Sedhain;Aditya Krishna Menon;Scott Sanner;Lexing Xie	2015		10.1145/2740908.2742726	computer science;collaborative filtering;data mining;world wide web;information retrieval;recommender system	AI	-18.21114752352316	-48.99230027338093	146549
19a5a497a4d30cef0d802df53f1d4734e7553213	benchmarking link analysis ranking methods in assistive environments	link analysis ranking;computer science and information systems;american sign language;network analysis;social networks	Several assistive applications exhibit a network structure. Characterizing the structure of such networks is critical in many assistive applications. Existing methods in the of social network analysis aim to detect, analyze, and summarize interesting or surprising components and trends in the network. In this paper, we provide a benchmark of two graph ranking methods: pagerank and HITS. The methods are tested on real social network data from three different domains: citation graphs, road networks, and a subgraph of Google. Our findings suggest that the quality of the ranking as well as the speed of convergence of both algorithms highly depends on the underlying network structure.	algorithm;benchmark (computing);citation graph;link analysis;pagerank;rate of convergence;social network analysis	Konstantinos Georgatzis;Panagiotis Papapetrou	2012		10.1145/2413097.2413154	organizational network analysis;network science;network analysis;computer science;artificial intelligence;machine learning;data mining;world wide web;social network	ML	-12.84167288191665	-41.62220061211267	146631
8d825f130b92bddecd412b652d84e2c633516639	efficient unique column combinations discovery based on data distribution		Discovering all unique column combinations in a relation is a fundamental research problem for modern data management and knowledge discovery applications. With the rapid growth of data volume and popularity of distributed platform, some algorithms are trying to discover uniques in large-scale datasets. However, the performance is not always satisfactory for some datasets which have few unique values in each column. This paper proposes a parallel algorithm to discover unique column combinations in large-scale datasets on Hadoop. We first construct a prefix tree to depict all unique candidates. Then we parallelize the verification of candidates in the same layer of the prefix tree. Two parallel strategies can be chosen: one is parallelizing across all subtrees, the other is parallelizing only in a single subtree. The parallel strategies and pruning methods are self-adaptive based on the data distribution. Eventually, experimental results demonstrate the advantages of the method we proposed.		Chao Wang;Shupeng Han;Xiangrui Cai;Haiwei Zhang;Yanlong Wen	2016		10.1007/978-3-319-39937-9_35	knowledge extraction;machine learning;tree (data structure);computer science;artificial intelligence;trie;parallel algorithm;data management	ML	-5.020810330101301	-38.9640981386422	146641
d93adabc5aaf3c94b8a125cae190b66360762a16	determining cosine similarity neighborhoods by means of the euclidean distance		Cosine similarity measure is often applied in the area of information retrieval, text classification, clustering, and ranking, where documents are usually represented as term frequency vectors or its variants such as tf-idf vectors. In these tasks, the most time-consuming operation is the calculation of most similar vectors or, alternatively, least dissimilar vectors. This operation has been commonly believed to be inefficient for large high-dimensional datasets. However, using the triangle inequality to determine neighborhoods based on a distance metric, offered recently, makes this operation feasible for such datasets. Although the cosine similarity measure is not a distance metric and, in particular, violates the triangle inequality, in this chapter, we present how to determine cosine similarity neighborhoods of vectors by means of the Euclidean distance applied to (α − )normalized forms of these vectors and by using the triangle inequality. We address three types of sets of cosine similar vectors: all vectors, the similarity of which to a given vector is not less than an e threshold value, and two variants of the k-nearest neighbors of a given vector.	cosine similarity;euclidean distance	Marzena Kryszkiewicz	2013		10.1007/978-3-642-30341-8_17	minkowski distance;euclidean distance;euclidean distance matrix;similarity	Vision	-5.212066593634907	-42.739693999166484	146681
127e02c7581a215d8e171b1774dc493fd56eb7ab	applications and design of cooperative multi-agent arn-based systems	artificial biochemical network abn;artificial reaction network arn;swarm robotics;artificial chemistry;artificial neural network ann	The Artificial Reaction Network (ARN) is an Artificial Chemistry inspired by Cell Signalling Networks (CSNs). Its purpose is to represent chemical circuitry and to explore the computational properties responsible for generating emergent high-level behaviour. In previous work, the ARN was applied to the simulation of the chemotaxis pathway of E. coli and to the control of quadrupedal robotic gaits. In this paper, the design and application of ARN-based cell-like agents termed Cytobots are explored. Such agents provide a facility to explore the dynamics and emergent properties of multicellular systems. The Cytobot ARN is constructed by combining functional motifs found in real biochemical networks. By instantiating this ARN, multiple Cytobots are created, each of which is capable of recognizing environmental patterns, stigmergic communication with others and controlling its own trajectory. Applications in biological simulation and robotics are investigated by first applying the agents to model the life-cycle phases of the cellular slime mould D. discoideum and then to simulate an oil-spill clean-up operation. The results demonstrate that an ARN based approach provides a powerful tool for modelling multi-agent biological systems and also has application in swarm robotics.	artificial chemistry;artificial intelligence;biological system;cell (microprocessor);cell signaling;connectionism;electronic circuit;emergence;experiment;gene regulatory network;high- and low-level;modular programming;multi-agent system;programming paradigm;robot;sequence motif;simulation;stigmergy;swarm robotics	Claire Gerrard;John A. W. McCall;Christopher MacLeod;George Macleod Coghill	2015	Soft Comput.	10.1007/s00500-014-1330-9	swarm robotics;artificial chemistry;simulation;computer science;artificial intelligence	AI	-5.784211755154137	-49.00245576782507	146707
28e7682a62d2f6a873f539f4f58837525a019ed0	a preferential attachment paradox: how preferential attachment combines with growth to produce networks with log-normal in-degree distributions		Every network scientist knows that preferential attachment combines with growth to produce networks with power-law in-degree distributions. How, then, is it possible for the network of American Physical Society journal collection citations to enjoy a log-normal citation distribution when it was found to have grown in accordance with preferential attachment? This anomalous result, which we exalt as the preferential attachment paradox, has remained unexplained since the physicist Sidney Redner first made light of it over a decade ago. Here we propose a resolution. The chief source of the mischief, we contend, lies in Redner having relied on a measurement procedure bereft of the accuracy required to distinguish preferential attachment from another form of attachment that is consistent with a log-normal in-degree distribution. There was a high-accuracy measurement procedure in use at the time, but it would have have been difficult to use it to shed light on the paradox, due to the presence of a systematic error inducing design flaw. In recent years the design flaw had been recognised and corrected. We show that the bringing of the newly corrected measurement procedure to bear on the data leads to a resolution of the paradox.	attachments;berry paradox;degree distribution;directed graph;epr paradox;flaw hypothesis methodology;image resolution;journal citation reports;resolution (logic);negative regulation of mitotic attachment of spindle microtubules to kinetochore	Paul Sheridan;Taku Onodera	2018		10.1038/s41598-018-21133-2		Theory	-16.649362773601815	-38.10743743036396	146737
2666933266ffd11ae56593f71e4f25ab9349eee6	community detection in complex networks: the role of node metadata			complex network	Darko Hric	2017			world wide web;complex network;metadata;computer network;computer science	AI	-18.366336009793418	-40.38877477466932	146753
4f507d88733fd9536c6a36b7b8d71246c1bd15e0	using self-organizing maps for identification of roles in social networks	role identification social networks self organizing map;neural nets;dblp dataset self organizing maps node role identification social networks link structure pattern som artificial neural networks role discovery data preprocessing methods large scale co authorship network;social networking online neural nets;lead;social networking online	In social networks the participants may be characterized by their roles. We understand roles as different patterns of link structure in the network. These roles describe the node and its activity in the network over time. Self-organizing maps (SOMs) - type of artificial neural-networks, are used for node's role identification and for discovery of all the roles present in the network. Different data preprocessing methods allow us to capture different aspects of roles. We show results of the experiment with a large scale co-authorship network constructed from a DBLP dataset.	artificial neural network;dbl-browser;data pre-processing;f1 score;organizing (structure);preprocessor;randomness;self-organizing map;social network;social structure;unbalanced circuit	Sarka Zehnalova;Zdenek Horak;Milos Kudelka;Václav Snásel	2013	2013 Fifth International Conference on Computational Aspects of Social Networks	10.1109/CASoN.2013.6622598	lead;computer science;artificial intelligence;machine learning;data mining;artificial neural network	ML	-17.82903699742383	-41.61867573357842	146914
a0fd2393b6aa4491cb145d14e570548c3031ed6b	varying retrieval categoricity using hyperbolic geometry	busqueda informacion;geometria euclidiana;categorisation;geometria hiperbolica;algoritmo busqueda;vector space model;ranking order preservation;information retrieval;algorithme recherche;hyperbolic geometry;geometrie euclidienne;search algorithm;vector space;indexing terms;similitude;modelo;modele cayley klein;categorizacion;hyperbolic space;computer experiment;recherche information;geometrie hyperbolique;normal weight;similarity;similarity measures;euclidean geometry;cayley klein model;modele;espace vectoriel;similitud;espacio vectorial;similarity measure;models;information search and retrieval;categorization	The paper proposes a Vector Space Model over the Cayley-Klein Hyperbolic Geometry (referred to as Hyperbolic Information Retrieval = HIR) using a similarity measure derived from the hyperbolic distance. It is shown that the proposed model is equivalent with the classical Vector Space Model using Cosine measure with normalized weighting scheme. It is also shown that the categoricity of the new retrieval system can be varied by only modifying the radius of the hyperbolic space and without using a different weighting scheme and similarity measure, which is not the case in the VSM, where the same effect can only be obtained by both changing the weighting scheme and similarity measure at the expense of a more costly computation. Experiments are also reported to demonstrate and support the ideas, and they show that categoricity in HIR can be varied more than O(n) faster, where n is the number of index terms, than in the VSM.	computation;experiment;information retrieval;magma;similarity measure;viable system model	Júlia Góth;Adrienn Skrop	2005	Information Retrieval	10.1007/s10791-005-5662-z	euclidean geometry;computer experiment;index term;similarity;hyperbolic space;vector space;computer science;similitude;hyperbolic geometry;vector space model;statistics;categorization;search algorithm	ML	-6.252032755593675	-44.032655430300515	147018
fbc7f219fee3cbd76f2c2f155c6fcbb32a40294e	overlapping community detection in social networks using a quantum-based genetic algorithm		One of the main research areas in social networks is community detection among them. In this research, the structure of communities of a social network considering their overlap has been detected with reliance on the quantum-based genetic algorithm (QIGA). This is a new dual-phase algorithm for community detection as an optimization problem. This algorithm has taken advantage of the modularity function as the target function. In order to detect communities considering their overlap, Jaccard's coefficient and the Jaccard median have been used in preprocessing stage. Results indicate that different amounts of overlap and modularity are achievable by incorporating a coefficient into the existing correlation between Jaccard's coefficient and the Jaccard median. This coefficient has been regulated using the three networks of karate, dolphin, and soccer. The huge air traffic control network has also benefitted from the mentioned regulated number. Implementation outcomes show that the suggested algorithm is able to detect community structure in overlapping networks with pinpoint accuracy.	coefficient;dolphin;genetic algorithm;jaccard index;mathematical optimization;optimization problem;preprocessor;social network	Alireza Saleh Sedghpour;Amin Nikanjam	2017		10.1145/3067695.3076000	jaccard index;machine learning;genetic algorithm;computer science;artificial intelligence;clique percolation method;community structure;social network;optimization problem	Web+IR	-13.764318005729514	-42.86062599006326	147130
8fa9cb5dac394e30e4089bf5f4ffecc873d1da96	personalized clothing recommendation combining user social circle and fashion style consistency	clothing recommendation;social circle;fashion;collaborative filtering	With the rapid expansion of social networks and fashion websites, clothing recommendation has attracted more attention of researchers, since various web data bring opportunities for recommender systems to solve the problems of cold start and sparsity. For clothing recommender system, user social circle and fashion style consistency of clothing items are two important factors, which have critical impacts on user decision making. In this paper, two practical problems are considered: how to visually analyze fashion style consistency between clothing items and how to implement personalized clothing recommendation by combining user social circle and fashion style consistency. To address the first problem, a Siamese Convolutional Neural Network (SCNN) with a novel sampling strategy is employed to measure the fashion style consistency of clothing items. It can learn a feature transformation from clothing images to a latent feature space, where the representations of clothing items with similar styles locate closer than those with different styles. For the second problem, three social factors (i.e., personal interest, interpersonal interest similarity and interpersonal influence) and fashion style consistency are fused into a unified personalized recommendation model based on probabilistic matrix factorization (PMF). To comprehensively evaluate our model, extensive experiments have been conducted on two real world datasets collected from a popular social fashion website, which demonstrate the effectiveness of the proposed method for personalized clothing recommendation.	cold start;convolutional neural network;experiment;feature vector;personalization;recommender system;sampling (signal processing);social network;sparse matrix	Guang-Lu Sun;Zhi-Qi Cheng;Xiao Wu;Qiang Peng	2017	Multimedia Tools and Applications	10.1007/s11042-017-5245-1	recommender system;artificial intelligence;computer science;data mining;collaborative filtering;probabilistic logic;clothing;multimedia;pattern recognition;feature vector;social network;social circle;interpersonal communication	AI	-18.792033373780104	-46.77809145625071	147221
787b5a0101a78a0511d5c16593caa12d7f7c6d75	gateway finder in large graphs: problem definitions and fast solutions	sub modularity;gateway;graph mining;social network;scalability	Given a graph, how to find a small group of ‘gateways’, that is a small subset of nodes that are crucial in connecting the source to the target? For instance, given a social network, who is the best person to introduce you to, say, Chris Ferguson, the poker champion? Or, given a network of people and skills, who is the best person to help you learn about, say, wavelets? We formally formulate this problem in two scenarios: Pair-Gateway and Group-Gateway. For each scenario, we show that it is sub-modular and thus it can be solved near-optimally. We further give fast, scalable algorithms to find such gateways. Extensive experimental evaluations on real data sets demonstrate the effectiveness and efficiency of the proposed methods.	algorithm;graph (discrete mathematics);scalability;social network;wavelet	Hanghang Tong;Spiros Papadimitriou;Christos Faloutsos;Philip S. Yu;Tina Eliassi-Rad	2012	Information Retrieval	10.1007/s10791-012-9190-3	scalability;simulation;computer science;machine learning;data mining;distributed computing;world wide web;social network	ML	-12.592077030983477	-41.89931585536219	147231
ce56940ee0aee286cea5f0f00fca613c5944d9be	layout-expectation-based model for image search re-ranking	ct;layout vectors prediction algorithms computed tomography search engines measurement image edge detection;search engines;drntu engineering computer science and engineering;image re ranking;search engines image retrieval;conference paper;image layouts image search re ranking layout expectation web search users expectation;image re ranking layout expectation based model ct;layout expectation based model;image retrieval	In this paper, a new algorithm is proposed for image re-ranking in Web search applications. The proposed algorithm introduces a new layout expectation model for improving the image search results. The motivation for using the expectation model is that users may often have potential expectations about the desired image during the search process. By including the layout expectation model to describe users' expectation on image layouts, the re-ranked search results can become more satisfactory to users. Experimental results demonstrate that our proposed algorithm can significantly improve the re-ranking precision compared with the state-of-the-art algorithms.	algorithm;image retrieval;web search engine	Bin Jin;Weiyao Lin;Jianxin Wu;Tianhao Wu;Jun Huang;Chongyang Zhang	2012	2012 IEEE International Conference on Multimedia and Expo Workshops	10.1109/ICMEW.2012.28	haplogroup ct;image retrieval;computer science;machine learning;data mining;information retrieval;search engine	Vision	-16.501383199330267	-51.113050541859195	147255
6e62bef712728370be59484f29d28413d063b303	concise representation of hypergraph minimal transversals: approach and application on the dependency inference problem	graph theory;boolean functions data mining redundancy inference algorithms data structures electronic mail databases;functional dependencies hypergraph minimal transversal extraction dependency inference problem moderately sized hypergraphs irred engine concise representation	The problem of extracting the minimal transversals from a hypergraph is known to be particularly difficult. Given that the number of minimal transversals can be exponential even for moderately sized hypergraphs, we propose, in this paper, a concise representation of minimal transversals in order to optimize the computation time and we introduce a new algorithm, called IRRED-ENGINE, which extracts all the minimal transversals from a subset. Experiments carried out on several types of hypergraphs, showed that IRRED-ENGINE obtains very interesting results evaluated through a compactness measure. To illustrate the benefit of our approach, we show how our concise representation can be used to solve the dependency inference problem by computing a concise cover of functional dependencies.	algorithm;computation;functional dependency;performance;relational database;time complexity	Mohamed Nidhal Jelassi;Christine Largeron;Sadok Ben Yahia	2015	2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)	10.1109/RCIS.2015.7128905	graph theory;algorithm	DB	-8.846626290129938	-39.242018602974085	147959
6f288eb8a72e5ba61f85c03d8df40eb6e08949a4	using network science to estimate the cost of architectural growth	blockmodeling;community detection;cosysmo;text;preferential attachment;electronic dissertation;dodaf;systems industrial engineering;network science		network science	Matthew Francis Dabkowski	2016			simulation;engineering;data science	DB	-18.074512311401968	-40.31322241364703	148288
48b44e7fbb07cbb621f466f5e6aaa86cd2c2af44	fast computation of simrank for static and dynamic information networks	simrank;information network;network analysis;dynamic information;graph;iteration method;similarity measure;kronecker product	"""Information networks are ubiquitous in many applications and analysis on such networks has attracted significant attention in the academic communities. One of the most important aspects of information network analysis is to measure similarity between nodes in a network. SimRank is a simple and influential measure of this kind, based on a solid theoretical """"random surfer"""" model. Existing work computes SimRank similarity scores in an iterative mode. We argue that the iterative method can be infeasible and inefficient when, as in many real-world scenarios, the networks change dynamically and frequently. We envision non-iterative method to bridge the gap. It allows users not only to update the similarity scores incrementally, but also to derive similarity scores for an arbitrary subset of nodes. To enable the non-iterative computation, we propose to rewrite the SimRank equation into a non-iterative form by using the Kronecker product and vectorization operators. Based on this, we develop a family of novel approximate SimRank computation algorithms for static and dynamic information networks, and give their corresponding theoretical justification and analysis. The non-iterative method supports efficient processing of various node analysis including similarity tracking and centrality tracking on evolving information networks. The effectiveness and efficiency of our proposed methods are evaluated on synthetic and real data sets."""	approximation algorithm;automatic vectorization;centrality;computation;iterative method;rewrite (programming);simrank;social network analysis;synthetic intelligence	Cuiping Li;Jiawei Han;Guoming He;Xin Jin;Yizhou Sun;Yintao Yu;Tianyi Wu	2010		10.1145/1739041.1739098	network analysis;computer science;theoretical computer science;machine learning;database;iterative method;kronecker product;graph	ML	-10.576285543699221	-41.30797014474262	148486
8bfcd206bd2370b36a89894eb87c03caee6c6168	a comparative evaluation of big data frameworks for graph processing		A lot of big data applications are using the graph model for structuring data. Examples are digital social networks, traffic information systems, and the hypertext connection between web pages. In order to process large scale graph structures, various frameworks have been proposed. In this paper, we evaluate and compare GraphX based on Spark and Gelly based on Flink as two prominent graph processing frameworks. We focus on the scalability of GraphX and Gelly with respect to increasing data volumes and their ability to distribute work between multiple processing nodes in a cluster. We perform experiments with different graph algorithms and both real world data and artificially generated data. For this, we implement a new algorithm using both Flink and Gelly. Our experiments show that while both frameworks are capable of processing big data, they substantially differ in non–functional features like resource consumption. Additionally, our experimental data show that choosing between different computing models offered by the frameworks can significantly influence the performance of big data graph computations.	algorithm;big data;computation;experiment;graph (abstract data type);graph theory;hypertext;information system;scalability;social network;web page	Marc Kaepke;Olaf Zukunft	2018	2018 4th International Conference on Big Data Innovations and Applications (Innovate-Data)	10.1109/Innovate-Data.2018.00012	data mining;experimental data;web page;information system;scalability;theoretical computer science;big data;computation;data modeling;social network;computer science	DB	-12.379803752916844	-41.26775802220005	148514
9e58f49d88a47b402758e5ae8b2bf2e2b6505807	similarity measures for sequential data	text mining;data concepts;biological data mining;key design issues in data mining	Expressive comparison of strings is a prerequisite for analysis of sequential data in many areas of computer science. However, comparing strings and assessing their similarity is not a trivial task and there exists several contrasting approaches for defining similarity measures over sequential data. In this article, we review three major classes of such similarity measures: edit distances, bag-of-word models and string kernels. Each of these classes originates from a particular application domain and models similarity of strings differently. We present these classes and underlying comparison concepts in detail, highlight advantages and differences as well as provide basic algorithms for practical application.	algorithm;application domain;bag-of-words model;computer science;data mining and knowledge discovery;edit distance;john d. wiley;machine learning;string (computer science);word lists by frequency	Konrad Rieck	2011	Wiley Interdiscip. Rev. Data Min. Knowl. Discov.	10.1002/widm.36	text mining;computer science;artificial intelligence;data science;machine learning;data mining;statistics	ML	-7.655678279248834	-44.210917438628016	148561
007c1d97ae76b853a8986092196f7c965ce3bd5e	using strong triadic closure to characterize ties in social networks	strong triadic closure;approximation algorithms;social networks	In the past few years there has been an explosion of social networks in the online world. Users flock these networks, creating profiles and linking themselves to other individuals. Connecting online has a small cost compared to the physical world, leading to a proliferation of connections, many of which carry little value or importance. Understanding the strength and nature of these relationships is paramount to anyone interesting in making use of the online social network data. In this paper, we use the principle of Strong Triadic Closure to characterize the strength of relationships in social networks. The Strong Triadic Closure principle stipulates that it is not possible for two individuals to have a strong relationship with a common friend and not know each other. We consider the problem of labeling the ties of a social network as strong or weak so as to enforce the Strong Triadic Closure property. We formulate the problem as a novel combinatorial optimization problem, and we study it theoretically. Although the problem is NP-hard, we are able to identify cases where there exist efficient algorithms with provable approximation guarantees. We perform experiments on real data, and we show that there is a correlation between the labeling we obtain and empirical metrics of tie strength, and that weak edges act as bridges between different communities in the network. Finally, we study extensions and variations of our problem both theoretically and experimentally.	algorithm;approximation;combinatorial optimization;complexity;existential quantification;experiment;flock;linker (computing);mathematical optimization;np-hardness;optimization problem;provable security;social network;triadic closure	Stavros Sintos;Panayiotis Tsaparas	2014		10.1145/2623330.2623664	computer science;artificial intelligence;machine learning;mathematics;approximation algorithm;triadic closure;social network	ML	-17.201014342502713	-42.974601623900796	148672
0260abc04cfd3ab853fcb857f0ea9e2b63d9a293	a probability based algorithm for influence maximization in social networks	influence maximization;social network;influence diffusion;modeling;diffusion model	In a social network, information runs from word-of-mouth based on the relationship of the users. The influence maximization is to find a limited number of initial users (nodes) to spread the information, so that the maximum number of other users could accept the information, which is a useful technique for marketing, information monitoring and advertising in a social network. Diffusion model of social networks imitates the process of information spreading in social networks, and Independent Cascade (IC) Model and Linear Threshold (LT) Model, are well-known stochastic information influence models. In this paper, we extend the classical IC model according to the observation of users' behaviors in social networks and propose an effective influence maximization algorithm based on this extended IC model. This novel algorithm calculates the influence probability of each node in sub-graphs that other nodes can engendered to it iteratively. The simulation experiments on real social network datasets show that our algorithm is much faster than the greedy hill-climbing algorithm, while the results are very close to the greedy algorithm and out-perform the other heuristic algorithms.	expectation–maximization algorithm;experiment;greedy algorithm;heuristic;hill climbing;linear logic;simulation;social network	Zhen Wang;Zhuzhong Qian;Sanglu Lu	2013		10.1145/2532443.2532455	mathematical optimization;computer science;artificial intelligence;machine learning	AI	-17.285497511124483	-43.87711536613673	148698
b66753a9086d8102188b179f9c0d8bfe874b5492	an upper bound on community size in scalable community detection	community detection;information network;upper bound;objective function	It is well-known that community detection methods based on modularity optimization often fails to discover small communities. Several objective functions used for community detection therefore involve a resolution parameter that allows the detection of communities at different scales. We provide an explicit upper bound on the community size of communities resulting from the optimization of several of these functions. We also show with a simple example that the use of the resolution parameter may artificially force the complete disaggregation of large and densely connected communities.	mathematical optimization;scalability	Gautier Krings;Vincent D. Blondel	2011	CoRR		machine learning;data mining;mathematics;upper and lower bounds	ML	-13.085254210118118	-40.95225662713863	148759
3ff28d6fdc375d141ad88c5baa9f0fc304cf8ee6	most central or least central? how much modeling decisions influence a node's centrality ranking in multiplex networks	europe	To understand a node's centrality in a multiplex network, its centrality values in all the layers of the network can be aggregated. This requires a normalization of the values, to allow their meaningful comparison and aggregation over networks with different sizes and orders. The concrete choices of such preprocessing steps like normalization and aggregation are almost never discussed in network analytic papers. In this paper, we show that even sticking to the most simple centrality index (the degree) but using different, classic choices of normalization and aggregation strategies, can turn a node from being among the most central to being among the least central. We present our results by using an aggregation operator which scales between different, classic aggregation strategies based on three multiplex networks. We also introduce a new visualization and characterization of a node's sensitivity to the choice of a normalization and aggregation strategy in multiplex networks. The observed high sensitivity of single nodes to the specific choice of aggregation and normalization strategies is of strong importance, especially for all kinds of intelligence-analytic software as it questions the interpretations of the findings.	categorization;centrality;database normalization;multiplexing;preprocessor;software documentation;turned a	Sude Tavassoli;Katharina Anna Zweig	2016	2016 Third European Network Intelligence Conference (ENIC)	10.1109/ENIC.2016.012	computer science;artificial intelligence;data mining;mathematics;centrality;statistics	Web+IR	-17.66272037841968	-39.131192512883096	148777
94cf552bd8a51a7c95ed1f2fe165ced2e124e454	finding influential seed successors in social networks	influence maximization;influential successors;influence maximizations;dynamic social networks;social network;social networks;social networking online;world wide web;coauthorship	In a dynamic social network, nodes can be removed from the network for some reasons, and consequently affect the behaviors of the network. In this paper, we tackle the challenge of finding a successor node for each removed seed node to maintain the influence spread in the network. Given a social network and a set of seed nodes for influence maximization, who are the best successors to be transferred the jobs of initial influence propagation if some seeds are removed from the network. To tackle this problem, we present and discuss five neighborhood-based selection heuristics, including degree, degree discount, overlapping, community bridge, and community degree. Experiments on DBLP co-authorship network show the effectiveness of devised heuristics.	dbl-browser;expectation–maximization algorithm;heuristic (computer science);seed;social network;software propagation	Cheng-Te Li;Hsun-Ping Hsieh;Shou-de Lin;Man-Kwan Shan	2012		10.1145/2187980.2188125	network formation;dynamic network analysis;management science;social network	ECom	-16.770377916087778	-43.507614914544575	148829
4306bfe8fa275371b0a5156c09d92d573a46b004	a hierarchical ensemble method for dag-structured taxonomies		Structured taxonomies characterize several real world problems, ranging from text categorization, to video annotation and protein function prediction. In this context “flat” learning methods may introduce inconsistent predictions, while structured output-aware learning methods can improve the accuracy of the predictions by exploiting the hierarchical relationships between classes. We propose a novel hierarchical ensemble method able to provide theoretically guaranteed consistent predictions for any Direct Acyclic Graph (DAG)-structured taxonomy, and consequently also for any taxonomy structured according to a tree. Results with a complex real-world DAG-structured taxonomy involving about one thousand classes and twenty thousand of examples show that the proposed hierarchical ensemble approach significantly improves flat methods, especially in terms of precision/recall curves.	categorization;directed acyclic graph;document classification;experiment;human phenotype ontology;kernel method;multiclass classification;protein function prediction;taxonomy (general);web page	Peter N. Robinson;Marco Frasca;Sebastian Köhler;Marco Notaro;Matteo Ré;Giorgio Valentini	2015		10.1007/978-3-319-20248-8_2	computer science;machine learning;pattern recognition;data mining	ML	-13.929923585323532	-47.444276226349366	148899
d1e4c44470c13c13548233129826885849ed2801	sign inference for dynamic signed networks via dictionary learning		Mobile online social network (mOSN) is a burgeoning research area. However, most existing works referring to mOSNs deal with static network structures and simply encode whether relationships among entities exist or not. In contrast, relationships in signed mOSNs can be positive or negative and may be changed with time and locations. Applying certain global characteristics of social balance, in this paper, we aim to infer the unknown relationships in dynamic signed mOSNs and formulate this sign inference problem as a low-rank matrix estimation problem. Specifically, motivated by the Singular Value Thresholding (SVT) algorithm, a compact dictionary is selected from the observed dataset. Based on this compact dictionary, the relationships in the dynamic signed mOSNs are estimated via solving the formulated problem. Furthermore, the estimation accuracy is improved by employing a dictionary self-updating mechanism.	dictionary;machine learning	Yi Cen;Rentao Gu;Yuefeng Ji	2013	J. Applied Mathematics	10.1155/2013/708581	mathematical optimization;machine learning;data mining;mathematics;statistics	Vision	-17.509659553435903	-45.69914489797942	148955
c1931e2c251c420d1372a6f620bc5454034b91df	chemical reaction kinetics is back: attempts to deal with complexity in biology: developing a quantitative molecular view to understanding life	kinetics;chemical reaction	N ew strategies to handle complexity in biology have been and are developed under the catch phrase “systems biology.” What stands in the core of this recent field of research is the concept to understand and model cells and organisms as high-dimensional dynamical systems and to determine the necessary input parameters by experiment. Regulation of gene activities and metabolic functions are encapsulated in differential equations that have their origin in chemical reaction kinetics. Needless to say, this approach has to envisage enormous complexity. On the other hand, solution of large numbers of kinetic equations, up to one thousand and more commonly rather stiff equations, is routine in combustion chemistry and flame modeling. What’s new, however, is the fact that cellular reaction networks have a number of unique properties unknown in physics and chemistry. They are not only selfregulated but they are also capable of reproduction, they are robust and don’t change their state under often not so small changes in the environment, and they can tolerate loss of one or the other constituent without loosing function. Both the experimental [1] and the computational approach to systems biology [2, 3] have made substantial progress within the last few years. Somehow, the mathematical analysis of the basic properties of genetic and metabolic networks is lagging behind [4, 5]. Despite undoubted success [6], many fundamental questions are still unanswered. Over many decades, molecular biology has been extraordinarily successful in applying a qualitative molecular view to understanding life. This qualitative image of nature is based on yes-or-no answers rather than the conventional quantitative	belousov–zhabotinsky reaction;computation;dynamical system;input lag;kinetics internet protocol;systems biology	Peter Schuster	2004	Complexity	10.1002/cplx.20056	biophysics;chemical reaction;computer science;mathematics;physical chemistry;kinetics	Comp.	-6.925318874331771	-48.26332116284647	148960
ac740bca5c056c29fdd02971012bd1775088804b	considering data-mining techniques in user preference learning	random access memory;user preferences preference learning data mining;query processing;multilayer perceptrons;training;user preferences;data mining;training data;machine learning;preference learning;mean square error methods;learning artificial intelligence;decision trees;rank correlations data mining techniques user preference learning top k query highest ranked objects learning objective utility functions data mining methods rmse;intelligent agent software engineering computer science testing learning systems data mining abstracts human computer interaction;conferences;query processing data mining learning artificial intelligence mean square error methods	In this paper we deal with the problem of learning user preferences from user’s scoring of a small sample of objects with labels from a very small linearly ordered set. The main task of this process is to use these preferences for a top-k query, which delivers the user with an ordered list of k highest ranked objects. We deal with a problem of many ties in the highest score. Two algorithms for learning objective and utility functions are presented. We experiment and compare them to some classical data-mining methods. We use several measures (RMSE and rank correlations …) to evaluate efficiency of these methods.	algorithm;data mining;preference learning;user (computing)	Peter Vojtás;Alan Eckhardt	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.53	preference learning;training set;computer science;artificial intelligence;data science;machine learning;decision tree;data mining;world wide web	DB	-17.564897476283505	-49.97084344084853	149438
359d6c4d30862f67ff682ea7dafd5e895b79110d	designing a multi-modal association graph for music objects		Music object features are complex and multifaceted, ranging from short-term/low-level features to long-term/high-level features, in which the semantic gap in between has not been properly resolved yet. In this paper, we introduce a graph-based approach to organize different aspects of features in a unified way. Based on the graph, various kinds of features could be related to and associated associated with each other. However, by further investigating the graph structure, we observe that the node degree distribution asymptotically follows a power law. As a result, some hubs (i.e., high-degree nodes) will dominate most metrics; the representation of graph semantics could be degenerated. Therefore, we introduce the graph projection operator to reduce the graph complexity and “compress” the graph accordingly. The graph projection is a method of refactoring edge weights, in which only a particular set of nodes are reserved to show the intrinsic structure of graph. To demonstrate the feasibility of graph-based approach, we introduce two applications (music clustering and auto-tagging); and perform experiments. According to our experiment study, the performance of projected graph is better than that of unprojected graph.	modal logic	Jia-Lien Hsu;Chiu-Yuan Ho	2014		10.1007/978-3-319-07293-7_69	theoretical computer science;machine learning;mathematics;graph;multimedia;graph database	Vision	-13.636812162482626	-46.73143092793123	149522
0158e8636c49bb55fa21aa1b3d4f4218595aee65	community detection in multidimensional networks	community detection;evolutionary computation;social networks;multidimensional networks;multiobjective genetic algorithm	The paper proposes a new approach to detect shared community structure in multidimensional networks based on the combination of multiobjective genetic algorithms, local search, and the concept of temporal smoothness, coming from evolutionary clustering. A multidimensional network is clustered by running on each slice a multiobjective genetic algorithm that maximizes the modularity on such a slice and, at the same time, minimizes the difference between the community structure obtained for the current layer and that found on the already considered dimensions. Experiments on synthetic and real-world datasets show the ability of the approach in discovering latent shared clustering of objects.	cluster analysis;clustering coefficient;experiment;genetic algorithm;heuristic;local search (optimization);multidimensional network;software propagation;synthetic intelligence	Alessia Amelio;Clara Pizzuti	2014	2014 IEEE 26th International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2014.60	mathematical optimization;computer science;artificial intelligence;machine learning;data mining;mathematics;social network;evolutionary computation	SE	-13.764846409646687	-43.45537855782784	149636
4022d038edf09fd60096caa3f484e7b7b7f5782c	high quality graph-based similarity search	conference paper;link analysis;high quality search;graph based similarity	"""SimRank is an influential link-based similarity measure that has been used in many fields of Web search and sociometry. The best-of-breed method by Kusumoto et. al., however, does not always deliver high-quality results, since it fails to accurately obtain its diagonal correction matrix D. Besides, SimRank is also limited by an unwanted """"connectivity trait"""": increasing the number of paths between nodes a and b often incurs a decrease in score s(a,b). The best-known solution, SimRank++, cannot resolve this problem, since a revised score will be zero if a and b have no common in-neighbors. In this paper, we consider high-quality similarity search. Our scheme, SR#, is efficient and semantically meaningful: (1) We first formulate the exact D, and devise a """"varied-D"""" method to accurately compute SimRank in linear memory. Moreover, by grouping computation, we also reduce the time of from quadratic to linear in the number of iterations. (2) We design a """"kernel-based"""" model to improve the quality of SimRank, and circumvent the """"connectivity trait"""" issue. (3) We give mathematical insights to the semantic difference between SimRank and its variant, and correct an argument: """"if D is replaced by a scaled identity matrix, top-K rankings will not be affected much"""". The experiments confirm that SR# can accurately extract high-quality scores, and is much faster than the state-of-the-art competitors."""	computation;emoticon;experiment;iteration;kernel (operating system);simrank;similarity measure;similarity search;so.cl	Weiren Yu;Julie A. McCann	2015		10.1145/2766462.2767720	link analysis;computer science;theoretical computer science;machine learning;data mining;world wide web;information retrieval;statistics	Web+IR	-10.411908987549177	-41.52179292103202	149988
150aef806708abc2dd6a94c93f3f0d30d1695355	inferring the structure of social contacts from demographic data in the analysis of infectious diseases spread	schools;data interpretation statistical;incidence;interpersonal relations;proportional hazards models;influenza;contact tracing;population surveillance;risk factors;infectious diseases;models statistical;risk assessment;humans;europe;disease transmission infectious;social epidemiology;demography;computer simulation;age groups;infectious disease epidemiology;disease outbreaks	Social contact patterns among individuals encode the transmission route of infectious diseases and are a key ingredient in the realistic characterization and modeling of epidemics. Unfortunately, the gathering of high quality experimental data on contact patterns in human populations is a very difficult task even at the coarse level of mixing patterns among age groups. Here we propose an alternative route to the estimation of mixing patterns that relies on the construction of virtual populations parametrized with highly detailed census and demographic data. We present the modeling of the population of 26 European countries and the generation of the corresponding synthetic contact matrices among the population age groups. The method is validated by a detailed comparison with the matrices obtained in six European countries by the most extensive survey study on mixing patterns. The methodology presented here allows a large scale comparison of mixing patterns in Europe, highlighting general common features as well as country-specific differences. We find clear relations between epidemiologically relevant quantities (reproduction number and attack rate) and socio-demographic characteristics of the populations, such as the average age of the population and the duration of primary school cycle. This study provides a numerical approach for the generation of human mixing patterns that can be used to improve the accuracy of mathematical models in the absence of specific experimental data.	attack rate;censuses;communicable diseases;communicable diseases, emerging;display resolution;encode;mathematical model;mathematics;mixing patterns;numerical analysis;population;quantity;synthetic intelligence	Laura Fumanelli;Marco Ajelli;Piero Manfredi;Alessandro Vespignani;Stefano Merler	2012		10.1371/journal.pcbi.1002673	computer simulation;risk assessment;biology;interpersonal relationship;incidence;simulation;social epidemiology;contact tracing;risk factor;demographic profile	ML	-8.801610698239111	-50.764684928272786	150656
015655da50c32774c222098c01bdc96a5df7534e	characters and patterns of communities in networks		A community can be seen as a group of vertices with strong cohesion among themselves and weak cohesion between each other. Community structure is one of the most remarkable features of many complex networks. There are various kinds of algorithms for detecting communities. However it is widely open for the question: what can we do with the communities? In this paper, we propose some new notions to characterize and analyze the communities. The new notions are general characters of the communities or local structures of networks. At first, we introduce the notions of internal dominating set and external dominating set of a community. We show that most communities in real networks have a small internal dominating set and a small external dominating set, and that the internal dominating set of a community keeps much of the information of the community. Secondly, based on the notions of the internal dominating set and the external dominating set, we define an internal slope (ISlope, for short) and an external slope (ESlope, for short) to measure the internal heterogeneity and external heterogeneity of a community respectively. We show that the internal slope (ISlope) of a community largely determines the structure of the community, that most communities in real networks are heterogeneous, meaning that most of the communities have a core/periphery structure, and that both ISlopes and ESlopes (reflecting the structure of communities) of all the communities of a network approximately follow a normal distribution. Therefore typical values of both ISolpes and ESoples of all the communities of a given network are in a narrow interval, and there is only a small number of communities having ISlopes or ESlopes out of the range of typical values of the ISlopes and ESlopes of the network. Finally, we show that all the communities of the real networks we studied, have a three degree separation phenomenon, that is, the average distance of communities is approximately 3, implying a general property of true communities for many real networks, and that good community finding algorithms find communities that amplify clustering coefficients of the networks, for many real networks.	algorithm;character encoding;cluster analysis;coefficient;cohesion (computer science);complex network;dominating set;sensor;vertex (graph theory)	Jiankou Li;Angsheng Li	2013	CoRR		combinatorics;discrete mathematics;mathematics	Theory	-15.889681499681506	-39.8372793222583	150659
e07f43f454bcac894e51cb2739c64c3b04128fab	heterogeneous translated hashing: a scalable solution towards multi-modal similarity search	heterogeneous translated hashing;scalability;hash function learning;similarity search	Multi-modal similarity search has attracted considerable attention to meet the need of information retrieval across different types of media. To enable efficient multi-modal similarity search in large-scale databases recently, researchers start to study multi-modal hashing. Most of the existing methods are applied to search across multi-views among which explicit correspondence is provided. Given a multi-modal similarity search task, we observe that abundant multi-view data can be found on the Web which can serve as an auxiliary bridge. In this paper, we propose a Heterogeneous Translated Hashing (HTH) method with such auxiliary bridge incorporated not only to improve current multi-view search but also to enable similarity search across heterogeneous media which have no direct correspondence. HTH provides more flexible and discriminative ability by embedding heterogeneous media into different Hamming spaces, compared to almost all existing methods that map heterogeneous data in a common Hamming space. We formulate a joint optimization model to learn hash functions embedding heterogeneous media into different Hamming spaces, and a translator aligning different Hamming spaces. The extensive experiments on two real-world datasets, one publicly available dataset of Flickr, and the other MIRFLICKR-Yahoo Answers dataset, highlight the effectiveness and efficiency of our algorithm.	algorithm;database;experiment;flickr;hamming space;hash function;information retrieval;mathematical optimization;mobile computing;modal logic;modality (human–computer interaction);similarity search;social media;window function;world wide web;yahoo! answers	Ying Wei;Yangqiu Song;Yi Zhen;Bo Liu;Qiang Yang	2016	TKDD	10.1145/2744204	scalability;computer science;machine learning;data mining;world wide web;information retrieval;locality-sensitive hashing	ML	-14.897838585440761	-50.43197264227442	150773
7ee02e514fc082c421a2135c0a5000ba7db26e15	a drive-reinforcement neural network model of simple instrumental conditioning	neural nets;learning systems;neural nets learning systems;learning system;neural network model;classical conditioning;instrumental conditioning;simple instrumental conditioning t maze active effector alternative actions classically conditionable drive reinforcement neurons cues drive reinforcement neural network model instrumental response intentional neuron learning system negative reinforcement center negative reinforcer	A network of classically conditionable drive-reinforcement neurons learned to choose an appropriate instrumental response to cues in a T maze when the cues could be utilized to anticipate the presentation of a positive or negative reinforcer or the absence of a reinforcer. To prove generality, it was shown that a network trained to respond to one configuration of reinforcers in the maze could learn to respond appropriately when the configuration is reversed. When the learning system turned toward the one arm of the maze it was negatively reinforced. When this happened the opposing intentional neuron corresponding to the active effector became active. The opposing intentional neuron corresponding to the inactive effector remained inactive because its high threshold precluded activity unless both the negative reinforcement center and its corresponding effector were active. When either opposing intentional neuron became active, it inhibited the currently active effector neuron and excited the other. A negatively reinforcing event therefore had the effect of inhibiting the responses that led to it. causing the connection strength from the T sensor to that effector neuron to decrease and also excited alternative actions	artificial neural network;network model	James S. Morgan;Eugene Patterson;A. Harry Klopf	1990		10.1109/IJCNN.1990.137719	classical conditioning;computer science;artificial intelligence;machine learning;operant conditioning;artificial neural network	ML	-7.927673451552251	-45.96790689612955	151112
43895e1003406754504917f875b959bf2b16784d	a recommendation system algorithm based on large scale internet environment	silicon;collaboration;prediction algorithms;internet;classification algorithms;algorithm design and analysis;partitioning algorithms	With the growing scale of the Internet, the amount of data is increasing rapidly as well. In order to improve the user experience, the recommendation system came into being. It recommends products to the user by analyzing the user's behavior. In the recommendation system, collaborative filtering algorithm is one of the most widely used algorithms. While the traditional collaborative filtering is no longer suitable for large-scale network, where the algorithm efficiency is low, as well as, it has the extremely sparse problem. To solve those problems, we designed an improved collaborative filtering algorithm for network segmentation. The algorithm uses a segmentation rules to partition the large-scale network, and decompose the problem into sub-problems. Ultimately, it can help us to meet the purpose of optimizing the algorithm.	algorithm;algorithmic efficiency;collaborative filtering;foreach loop;internet;item unique identification;mathematical optimization;network partition;network segmentation;neural coding;recommender system;sparse matrix;user experience	Xifeng Liu;Zhijian Wang;Feng Ye	2016	2016 13th Web Information Systems and Applications Conference (WISA)	10.1109/WISA.2016.31	weighted majority algorithm;computer science;theoretical computer science;collaborative filtering;machine learning;data mining;fsa-red algorithm;algorithmics	Web+IR	-12.493066329083595	-43.10969671880237	151721
752361474d91626dfd5725a91efa96b26ec1cb6d	ego-centric network sampling in viral marketing applications	social network services;egocentric network sampling;viral marketing ego centric networks network sampling;long period;marketing data processing;stanford berkeley network;data collection;network sampling;web sites marketing data processing social sciences computing software engineering;information services;data mining;software engineering;stanford berkeley network egocentric network sampling viral marketing application social network internet data collection web sites;social network;sampling methods internet social network services facebook ip networks information services web sites computer networks particle measurements time measurement;internet;estimation;social sciences computing;ego centric networks;web sites;viral marketing application;network structure;sampling methods;internet data collection;viral marketing	Marketing is most successful when people spread the message within their social network. The Internet can serve as an approximation of the spread of messages, particularly marketing campaigns, to both measure marketing effectiveness and provide data for influencing future efforts. However, to measure the network of Web sites spreading the marketing message potentially requires a massive amount of data collection over a long period of time. Additionally, collecting data from the Internet is very noisy and can create a false sense of precision. Therefore, we propose to use ego-centric network sampling to both reduce the amount of data required to collect as well as handle the inherent uncertainty of the data collected. In this paper, we study whether the proposed ego-centric network sampling accurately captures the network structure. We use the Stanford-Berkeley network to show that the approach can capture the underlying structure with a minimal amount of data.	arpack;approximation;coefficient;internet;pagerank;sampling (signal processing);shortest path problem;social network;steady state	Huaiyu Ma;Steven Gustafson;Abha Moitra;David B. Bracewell	2009	2009 International Conference on Computational Science and Engineering	10.1109/CSE.2009.419	sampling;estimation;the internet;computer science;artificial intelligence;machine learning;data mining;viral marketing;database;world wide web;computer security;information system;statistics;computer network;social network;data collection	DB	-18.66650414037377	-43.5636327038391	151739
af7830e47382befde934905207aa5855df5e4823	search engine drives the evolution of social networks		"""The search engine is tightly coupled with social networks and is primarily designed for users to acquire interested information. Specifically, the search engine assists the information dissemination for social networks, i.e., enabling users to access interested contents with keywords-searching and promoting the process of contents-transferring from the source users directly to potential interested users. Accompanying such processes, the social network evolves as new links e-merge between users with common interests. However, there is no clear understanding of such a """"chicken-and-egg"""" problem, namely, new links encourage more social interactions, and vice versa. In this paper, we aim to quantitatively characterize the social network evolution phenomenon driven by a search engine. First, we propose a search network model for social network evolution. Second, we adopt two performance metrics, namely, degree distribution and network diameter. Theoretically, we prove that the degree distribution follows an intensified power-law, and the network diameter shrinks. Third, we quantitatively show that the search engine accelerates the rumor propagation in social networks. Finally, based on four real-world data sets (i.e., CDBLP, Facebook, Weibo, and P2P), we verify our theoretical findings. Furthermore, we find that the search engine dramatically increases the speed of rumor propagation."""	degree distribution;evolution;interaction;network model;propagation delay;social network;software propagation;web search engine	Cai Fu;Chenchen Peng;Xiao-Yang Liu	2017		10.1145/3063955.3064807	machine learning;artificial intelligence;computer science;network science;network model;dynamic network analysis;degree distribution;search engine;rumor;social network;distributed computing;phenomenon	Web+IR	-18.47163528756833	-41.889007940162806	153020
4ee8987bc986ee99c8d0ed44f4cdefbacc8eed54	linking content information with bayesian personalized ranking via multiple content alignments		In many application domains of recommender systems, content-based information are available for users, objects or both. Such information can be processed during recommendation and significantly decrease the cold-start problem. However, content information may come from several, possibly external, sources. Some sources may be incomplete, less reliable or less relevant for the purpose of recommendation. Thus, each content source or attribute possess different level of informativeness, which should be taken into consideration during the process of recommendation. In this paper, we propose a multiple content alignments extension to the Bayesian Personalized Ranking Matrix Factorization (BPR-MCA). The proposed method incorporates multiple sources of content information in the form of user-to-user or object-to-object similarity matrices and aligns users' and items' latent factors ac-cording to these similarities. During the training phase, BPR-MCA also learns the relevance weight of each similarity matrix. BPR-MCA was evaluated on the MovieLens 1M dataset, extended by the content information from IMDB, DBTropes and ZIP code statistics. The experiment shows that BPR-MCA can help to significantly improve recommendation w.r.t. nDCG and AUPR over standard BPR under several cold-start scenarios.	artificial neural network;bridging (networking);cold start;convolutional neural network;data descriptor;information;interaction;internet movie database (imdb);latent variable;mathematical optimization;micro channel architecture;movielens;negative base;online and offline;recommender system;relevance;similarity measure;user (computing)	Ladislav Peska	2017		10.1145/3078714.3078732	data mining;computer science;recommender system;learning to rank;cold start;world wide web;movielens;similarity matrix;matrix decomposition;ranking;bayesian probability	Web+IR	-18.94950754649802	-48.108234126516855	153059
1150b18c6b3ceb03dcd7005f87c60c65f27a1e27	on power law growth of social networks		"""What is the growth dynamics of social networks, like Facebook or WeChat? Does it truly exhibit exponential early-growth, as predicted by the celebrated models, like the Bass model? How about the dynamics of links, for which there are few published models? For the first time, we examine the growth of WeChat which is the largest online social network in China, together with several other real social networks. We observe Power-Law growth dynamics for both nodes and links, a fact that breaks the textbook models featuring Sigmoid curves. We propose <sc>NetTide</sc>, along with differential equations for the growth of nodes and links. Our model fits the growth dynamics of real social networks well; it encompasses many traditional growth dynamics as special cases, while remaining parsimonious in parameters. The <sc>NetTide</sc> for link growth is the first one of its kind, accurately fitting real data, and capturing densification phenomenon. We further formulate two stochastic generators, which interpret the growth of nodes and links through survival analysis and micro-level interactions within a social network, respectively. The proposed generators reproduce realistic growth dynamics of social networks. When applied on the WeChat data, our <sc> NetTide</sc> forecasted <inline-formula><tex-math notation=""""LaTeX"""">$\geq$</tex-math><alternatives> <inline-graphic xlink:href=""""zang-ieq1-2801844.gif""""/></alternatives></inline-formula> 730 days ahead with 3 percent error."""	approximation error;beneath a steel sky;fits;interaction;moore's law;occam's razor;sigmoid function;social network;time complexity;xlink	Chengxi Zang;Peng Cui;Christos Faloutsos;Wenwu Zhu	2018	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2018.2801844	artificial intelligence;computer science;machine learning;differential equation;survival analysis;social network;power law;sigmoid function;phenomenon;social dynamics;stochastic process	ML	-18.693876472669835	-39.277812981189356	153252
9a289efb3adacf5331edd7e501599407016d983e	selection rules in alliance formation: strategic decisions or abundance of choice?		We study how firms select partners using a large database of publicly announced R&D alliances over a period of 25 years. We identify, for the first time, two distinct behavioral strategies of firms in forming these alliances. By reconstructing and analysing the temporal R&D network of 14,000 international firms and 21.000 publicly announced alliances, we find a “universal” behavior in firms changing between these strategies. In the first strategy, newcomers and nodes of low centrality initially establish links to nodes of similar or higher centrality. After these firms have consolidated their position and increased their centrality, they switch to the second strategy, and preferably form links to less central nodes. In addition, we show that k-core centrality can be established as a measure of firm’s success that correlates e.g. with the number of patents (obtained from a dataset of 3 Mio patents). To synthezise our findings, we provide a network growth model based on k-core centrality which reproduces the strategic behavior of firms, as well as other properties of the empirical network. 1/25 ar X iv :1 40 3. 32 98 v2 [ ph ys ic s. so cph ] 1 4 M ar 2 01 4 Antonios Garas, Mario V. Tomasello, and Frank Schweitzer: Selection rules in alliance formation: strategic decisions or abundance of choice? The growth of real networks is often explained by the preferential attachment rule [1], which only assumes that newcomers in a network prefer to connect to the node with the highest degree. This allows to reproduce the power-law distribution of node degrees and the emergence of nodes with profound importance, but it fails to explain saturated growth, or even decline [2], of real networks. In real networks strategic considerations with whom to link govern the growth process [3]. For example, firms searching for partners to form an R&D alliance have to consider complementarities in their knowledge base, but also the network position of their counterparts. This, however, is an important limitation, as it requires full knowledge of the network connections of all counterparts. Hence, in many economic and social networks nodes have to find other ways to improve their position in a competitive environment. Here, we use the k-core centrality [4] to quantify the importance of nodes in an R&D network, and we show that this measure is highly correlated (more than other widely used centrality measures) with the number of patents filed by the nodes. This way, we link an external –publicly available– source of information to internal properties that are used for link formation. Additionally, we identify two different strategies in choosing partners dependent on the importance of the node itself. These apply either to newcomers or to established nodes. Newcomers, or nodes of little importance, usually establish links to nodes of similar or higher importance, very much like new PhD students team up with fellow students or postdocs in their group, but rarely with famous professors. After these new nodes have established their position and gained considerably in importance, they switch their strategy and preferably establish links to nodes of less importance. In the example at hand, the reputed professor is less likely to restrict his contacts to other professors of similar reputation and more likely to younger graduate students. Likewise, established firms rarely focus their R&D collaboration on other established firms which often have become their competitors. Instead, they are more likely to search for, and to team up with, new start-up companies with fresh ideas. To empirically verify these two strategies, we analyze the formation of R&D alliances between firms, using a database of 21,572 publicly announced alliances between 14,000 international firms from different economic sectors during 1984 2009. In addition, given that the purpose of an R&D collaboration is to create new products, we measure their output by the number of patents filled by every firm, using a database of about three million patents granted in the U.S.A., as described in the Materials and Methods section. By representing firms with nodes and alliances with links connecting two nodes, we map the alliance formation process to a growing complex network. Nodes have to choose between the establishment of a new link with another node (alliance with a new partner) and the increase of the weight of an already existing link (alliance with an existing partner). Hence, at every time step the degree of a node represents its mere number of R&D alliances with distinct partners, while the weight of a link between two nodes represents the number of times these two nodes formed an R&D alliance.	attachments;centrality;complementarity theory;complex network;database;degeneracy (graph theory);emergence;hyperlink;information source;knowledge base;population dynamics;quantum number;selection rule;social network	Antonios Garas;Mario Vincenzo Tomasello;Frank Schweitzer	2014	CoRR		marketing;operations management;commerce	AI	-16.277110191678606	-38.7940648101282	153343
aa96a257c28225901006502266aa65877897dc5d	sybil defense in crowdsourcing platforms		Crowdsourcing platforms have been widely deployed to solve many computer-hard problems, e.g., image recognition and entity resolution. Quality control is an important issue in crowdsourcing, which has been extensively addressed by existing quality-control algorithms, e.g., voting-based algorithms and probabilistic graphical models. However, these algorithms cannot ensure quality under sybil attacks, which leverages a large number of sybil accounts to generate results for dominating answers of normal workers. To address this problem, we propose a sybil defense framework for crowdsourcing, which can help crowdsourcing platforms to identify sybil workers and defense the sybil attack. We develop a similarity function to quantify worker similarity. Based on worker similarity, we cluster workers into different groups such that we can utilize a small number of golden questions to accurately identify the sybil groups. We also devise online algorithms to instantly detect sybil workers to throttle the attacks. Our method also has ability to detect multi-attackers in one task. To the best of our knowledge, this is the first framework for sybil defense in crowdsourcing. Experimental results on real-world datasets demonstrate that our method can effectively identify and throttle sybil workers.	baseline (configuration management);computer vision;crowdsourcing;graphical model;ibm notes;online algorithm;similarity measure;sybil attack	Dong Yuan;Guoliang Li;Qi Li;Yudian Zheng	2017		10.1145/3132847.3133039	online algorithm;data mining;sybil attack;name resolution;small number;computer science;crowdsourcing;graphical model	ML	-11.444014417439755	-45.54127702549227	153347
819997f1aa7c2eaa1363837d2669b256b1fe5642	selecting key person of social network using skyline query in mapreduce framework	mapreduce framework skyline query social network online communications channels;social networking online data handling parallel processing query processing;skyline query;social networks;facebook media databases measurement algorithm design and analysis electronic mail;mapreduce;key person;mapreduce social networks key person skyline query	This paper considers a problem of selecting small number of important person from social networks. We applied skyline query for selecting the key person of social network. Skyline query selects persons from social network those are not dominated by another person. Different from general skyline query, selecting key person from social networks is more complicated because we need to consider different metrics of social network. In addition, social networks is a container of massive data and each and every hour it is increasing in gigantic amount. It is the collective of online communications channels dedicated to community-based input, interaction, content-sharing, and collaboration. We use MapReduce framework to speed up the computation and parallelism. An extensive set of experiments shows that the analysis of social activities, social relationships, and socially shared contents helps to find key person.	computation;experiment;mapreduce;parallel computing;pareto efficiency;social network	Asif Zaman;Md. Anisuzzaman Siddique;Annisa;Yasuhiko Morimoto	2015	2015 Third International Symposium on Computing and Networking (CANDAR)	10.1109/CANDAR.2015.84	computer science;data mining;database;world wide web	DB	-9.247903382988502	-42.73614051692735	153385
9223a4242909694028038210fc8fb38bf25c0b4d	a model of artificial genotype and norm of reaction in a robotic system		The genes of living organisms serve as large stores of information for replicating their behavior and morphology over generations. The evolutionary view of genetics that has inspired artificial systems with a Mendelian approach does not take into account the interaction between species and with the environment to generate a particular phenotype. In this paper, a genotype model is suggested to shape the relationship with the phenotype and the environment in an artificial system. A method to obtain a genotype from a population of a particular robotic system is also proposed. Finally, we show that this model presents a similar behavior to that of living organisms in what regards the concept of norm of reaction.		Angel Juan Duran;Angel P. Del Pobil	2016		10.1007/978-3-319-43488-9_24	simulation;artificial intelligence	Robotics	-5.2600683139267606	-48.25750257916194	153394
c2840ac966280f8770ca047bb1124b00754d2232	memetics of the computer universe based on the quran		The emerging concept of computer universe has the potential to bring about a radical change in our perception of the world. Energy is word of God that carries His commands. It derives its properties in accordance with the divine instructions immanent in it. Memetics is the science of information carried by the entity called energy. Living and non-living systems represent two different languages in which information content of energy exists. These may be distinguished as bioprogram (biological information) and abioprogram (chemical information) respectively. While abioprogram can be explained in terms of a structure-code concept, the bioprogram is intangible to human beings and is stored on the chromosomes of the cell. It is non-physical in nature but requires a physical medium for its storage like the computer program. An organism is natural biocomputer or biorobot. All organisms except Homo sapiens are totally programmed unconscious systems like man-made robot. Man is the only conscious, freewilled robot (abd in Arabic) of God. Consciousness and freewill are the attributes of the unique processor of human being, the mind (qalb in Arabic). The computer model of the organism helps us to define and explain the phenomena of life and death. The phenomenon of life is the manifestation of the execution of the biosoftware while death is the result of deletion of the biosoftware. A dead body is like a computer without software. Man-made computer, robot, etc., which run on man-made software are forms of “artificial life”. The basic change that the computer concept of the universe brings into our present knowledge of the universe and cosmology is that it is the divine information carried in energy that represents the underlying reality of the universe.	artificial life;biorobotics;cheminformatics;computer program;computer simulation;consciousness;life & death;living systems;memetics;numerical weather prediction;robot;self-information	Pallacken Abdul Wahid	2010	JSEA	10.4236/jsea.2010.37084	computer science;engineering;artificial intelligence;management	AI	-6.461613939016914	-47.767497099186876	153431
2c69fe8f97fd8f52747345a8c884c012f55522a6	dominating sets and ego-centered decompositions in social networks	cs si;epj special topics;topical issues;journal;epj;publication;physics soc ph	Our aim here is to address the problem of decomposing a whole network into a minimal number of ego–centered subnetworks. For this purpose, the network egos are picked out as the members of a minimum dominating set of the network. However, to find such an efficient dominating ego–centered construction, we need to be able to detect all the minimum dominating sets and to compare all the corresponding dominating ego–centered decompositions of the network. To find all the minimum dominating sets of the network, we are developing a computational heuristic, which is based on the partition of the set of nodes of a graph into three subsets, the always dominant vertices, the possible dominant vertices and the never dominant vertices, when the domination number of the network is known. To compare the ensuing dominating ego–centered decompositions of the network, we are introducing a number of structural measures that count the number of nodes and links inside and across the ego–centered subnetworks. Furthermore, we are applying the techniques of graph domination and ego–centered decomposition for six empirical social networks.	algorithm;bibliometrics;directed graph;dominating set;edge dominating set;graph (discrete mathematics);heuristic;social media mining;social network;theory;user-centered design;vertex (geometry)	Moses A. Boudourides;Sergios T. Lenis	2016	CoRR	10.1140/epjst/e2016-02673-0	dominating set;publication;advertising;law;domatic number;economic growth	AI	-16.592545800916223	-40.49104319868713	153588
821382dd92c0303c3e41904fa6d6872b0bc81e5e	spatial ordering and encoding for geographic data mining and visualization	hierarchical clustering;linear order;geographic information;linear ordering;data mining;space filling curves;journal article;data distribution;spatio temporal visualization;spatial data mining;spatial relation;nearest neighbor;multidimensional scaling;space filling curve;spatial locality;weighted graph;spatial clustering;temporal data mining;rules;multivariate data	Geographic information (e.g., locations, networks, and nearest neighbors) are unique and different from other aspatial attributes (e.g., population, sales, or income). It is a challenging problem in spatial data mining and visualization to take into account both the geographic information and multiple aspatial variables in the detection of patterns. To tackle this problem, we present and evaluate a variety of spatial ordering methods that can transform spatial relations into a one-dimensional ordering and encoding which preserves spatial locality as much possible. The ordering can then be used to spatially sort temporal or multivariate data series and thus help reveal patterns across different spaces. The encoding, as a materialization of spatial clusters and neighboring relations, is also amenable for processing together with aspatial variables by any existing (non-spatial) data mining methods. We design a set of measures to evaluate nine different ordering/encoding methods, including two space-filling curves, six hierarchical clustering based methods, and a one-dimensional Sammon mapping (a multidimensional scaling approach). Evaluation results with various data distributions show that the optimal ordering/encoding with the complete-linkage clustering consistently gives the best overall performance, surpassing well-known space-filling curves in preserving spatial locality. Moreover, clustering-based methods can encode not only simple geographic locations, e.g., x and y coordinates, but also a wide range of other spatial relations, e.g., network distances or arbitrarily weighted graphs.	central processing unit;cluster analysis;complete-linkage clustering;computation;computational complexity theory;data mining;dataspaces;desktop computer;encode;hierarchical clustering;hilbert curve;image scaling;linkage (software);locality of reference;multidimensional scaling;pentium 4;random-access memory;randomness;sammon mapping;scalability;self-organized criticality;single-linkage clustering;space-filling curve	Diansheng Guo;Mark Gahegan	2006	Journal of Intelligent Information Systems	10.1007/s10844-006-9952-8	spatial relation;multivariate statistics;multidimensional scaling;computer science;machine learning;pattern recognition;data mining;hierarchical clustering;k-nearest neighbors algorithm;total order	DB	-5.292253267692238	-42.31273149513939	153954
c56de91eae24c8aaaaf6e12efda52fd4b3767a87	sampler design for bayesian personalized ranking by leveraging view data		Bayesian Personalized Ranking (BPR) is a representative pairwise learning method for optimizing recommendation models. It is widely known that the performance of BPR depends largely on the quality of negative sampler. In this paper, we make two contributions with respect to BPR. First, we find that sampling negative items from the whole space is unnecessary and may even degrade the performance. Second, focusing on the purchase feedback of E-commerce, we propose an effective sampler for BPR by leveraging the additional view data. In our proposed sampler, users’ viewed interactions are considered as an intermediate feedback between those purchased and unobserved interactions. The pairwise rankings of user preference among these three types of interactions are jointly learned, and a user-oriented weighting strategy is considered during learning process, which is more effective and flexible. Compared to the vanilla BPR that applies a uniform sampler on all candidates, our view-enhanced sampler enhances BPR with a relative improvement over 37.03% and 16.40% on two real-world datasets. Our study demonstrates the importance of considering users’ additional feedback when modeling their preference on different items, which avoids sampling negative items indiscriminately and inefficiently.	bayesian programming;collaborative filtering;e-commerce payment system;feedback;interaction;recommender system	Jingtao Ding;Guanghui Yu;Xiangnan He;Yong Li;Depeng Jin	2018	CoRR		computer science;data mining;sampling (statistics);business process reengineering;pairwise comparison;ranking;weighting;bayesian probability	Web+IR	-18.97738558264221	-48.26369455512358	154079
361638fa4eeb1679798c76cc3921a946ae3f7b46	long-term evolutionary dynamics in heterogeneous cellular automata	artificial ecosystem;genetic programming;cellular automata;open ended;evolution	"""In this work we study open-ended evolution through the analysis of a new model, HetCA, for """"heterogeneous cellular automata"""". Striving for simplicity, HetCA is based on classical two-dimensional CA, but differs from them in several key ways: cells include properties of """"age"""", """"decay"""", and """"quiescence""""; cells utilize a heterogeneous transition function, one inspired by genetic programming; and there exists a notion of genetic transfer between adjacent cells. The cumulative effect of these changes is the creation of an evolving ecosystem of competing cell colonies. To evaluate the results of our new model, we define a measure of phenotypic diversity on the space of cellular automata. Via this measure, we contrast HetCA to several controls known for their emergent behaviours---homogeneous CA and the Game of Life---and several variants of our model. This analysis demonstrates that HetCA has a capacity for long-term phenotypic dynamics not readily achieved in other models. Runs exceeding one million time steps do not exhibit stagnation or even cyclic behaviour. Further, we show that the design choices are well motivated, as the exclusion of any one of them disrupts the long-term dynamics."""	automata theory;cellular automaton;conway's game of life;ecosystem;emergence;genetic programming;nonlinear gameplay;quiescence search	David Medernach;Taras Kowaliw;Conor Ryan;René Doursat	2013		10.1145/2463372.2463395	cellular automaton;genetic programming;computer science;artificial intelligence;evolution;mathematics;mobile automaton;algorithm	ML	-4.770614187291453	-47.10957582296538	154168
1bd6c258463d2b3e088cf30b14b67023d61fab21	canonical consistent weighted sampling for real-value weighted min-hash	electronic mail;complexity theory;approximation algorithms;quantization signal;indexes;big data;conference proceeding;australia	Min-Hash, as a member of the Locality Sensitive Hashing (LSH) family for sketching sets, plays an important role in the big data era. It is widely used for efficiently estimating similarities of bag-of-words represented data and has been extended to dealing with multi-sets and real-value weighted sets. Improved Consistent Weighted Sampling (ICWS) has been recognized as the state-of-the-art for real-value weighted Min-Hash. However, the algorithmic implementation of ICWS is flawed because it violates the uniformity of the Min-Hash scheme. In this paper, we propose a Canonical Consistent Weighted Sampling (CCWS) algorithm, which not only retains the same theoretical complexity as ICWS but also strictly complies with the definition of Min-Hash. The experimental results demonstrate that the proposed CCWS algorithm runs faster than the state-of-the-arts while achieving similar classification performance on a number of real-world text data sets.	algorithm;bag-of-words model;big data;canonical quantization;circuit complexity;computation;experiment;gibbs sampling;hash function;international conference on web services;locality of reference;locality-sensitive hashing;maxima and minima;sampling (signal processing);text corpus;lsh	Wei Wu;Bin Li;Ling Chen;Chengqi Zhang	2016	2016 IEEE 16th International Conference on Data Mining (ICDM)	10.1109/ICDM.2016.0174	database index;big data;computer science;theoretical computer science;machine learning;data mining;mathematics;algorithm;statistics	DB	-7.048801743410258	-40.92443007772656	154307
2b186b8f78dc181d8aeba867b61fd7309bfd9d6a	future systems and control research in synthetic biology		Synthetic biology is the application of engineering principles to the fundamental components of biology, with the aim of creating systems with novel functionalities that can be used for energy, environment, and medical applications. While the potential impact of this new technology is enormous, there are challenges that we need to overcome before the impact of synthetic biology can be fully realized. Many of these challenges fall beyond the scope of molecular biology and are indeed “system-level” problems, where very little research is being performed. This paper identifies pressing challenges in synthetic biology that can be formulated as systems and control theoretic problems and outlines potentially new systems and control theories/tools that are required to tackle such problems. The aim is to attract more systems and control theorists to collaborate with molecular biologists and biophysicists and help synthetic biology reach its promise. At the same time, engaging the systems and control community more broadly into the rich research opportunities and life-changing applications of synthetic biology may provide added visibility to the field of systems and controls. © 2018 Elsevier Ltd. All rights reserved.	synthetic biology;synthetic intelligence;theory	Domitilla Del Vecchio;Yili Qian;Richard M. Murray;Eduardo D. Sontag	2018	Annual Reviews in Control	10.1016/j.arcontrol.2018.04.007	data science;control engineering;engineering;synthetic biology	Comp.	-8.130903656781063	-49.29997377305878	154349
456440c6a5ec99857d4597c5a6cf0a4f9cc6e1fb	identifying modular flows on multilayer networks reveals highly overlapping organization in social systems	annan fysik;other physics topics	Unveiling the community structure of networks is a powerful methodology to comprehend interconnected systems across the social and natural sciences. To identify different types of functional modules in interaction data aggregated in a single network layer, researchers have developed many powerful methods. For example, flow-based methods have proven useful for identifying modular dynamics in weighted and directed networks that capture constraints on flow in the systems they represent. However, many networked systems consist of agents or components that exhibit multiple layers of interactions. Inevitably, representing this intricate network of networks as a single aggregated network leads to information loss and may obscure the actual organization. Here we propose a method based on compression of network flows that can identify modular flows in nonaggregated multilayer networks. Our numerical experiments on synthetic networks show that the method can accurately identify modules that cannot be identified in aggregated networks or by analyzing the layers separately. We capitalize on our findings and reveal the community structure of two multilayer collaboration networks: scientists affiliated to the Pierre Auger Observatory and scientists publishing works on networks on the arXiv. Compared to conventional aggregated methods, the multilayer method reveals smaller modules with more overlap that better capture the actual organization.	experiment;interaction;numerical analysis;semantic network;social system;synthetic intelligence	Manlio De Domenico;Andrea Lancichinetti;Alexandre Arenas;Martin Rosvall	2014	CoRR	10.1103/PhysRevX.5.011027	physics	ML	-16.38959613371991	-40.909548839445165	154415
08c4d4fade7eb08741631992edd07565bda9a06a	gaze aware deep learning model for video summarization		Video summarization is an ideal tool for skimming videos. Previous computational models extract explicit information from the input video, such as visual appearance, motion or audio information, in order to generate informative summaries. Eye gaze information, which is an implicit clue, has proved useful for indicating important content and the viewer’s interest. In this paper, we propose a novel gaze-aware deep learning model for video summarization. In our model, the position and velocity of the observers’ raw eye movements are processed by the deep neural network to indicate the users’ preferences. Experiments on two widely used video summarization datasets show that our model is more proficient than state-of-the-art methods in summarizing video for characterizing general preferences as well as for personal preferences. The results provide an innovative and improved algorithm for using gaze information in video summarization.	deep learning	Jiaxin Wu;Sheng-hua Zhong;Zheng Ma;Stephen J. Heinen;Jianmin Jiang	2018		10.1007/978-3-030-00767-6_27	gaze;automatic summarization;convolutional neural network;computer vision;artificial intelligence;computational model;computer science;artificial neural network;deep learning;eye tracking;visual appearance	AI	-17.49298808348021	-51.894859462816555	155042
0e2e230a2a727966391180310db405de91117b7b	entropy and selection: life as an adaptation for universe replication		Natural selection is the strongest known antientropic process in the universe when operating at the biological level and may also operate at the cosmological level. Consideration of how biological natural selection creates adaptations may illuminate the consequences and significance of cosmological natural selection. An organismal trait is more likely to constitute an adaptation if characterized by more improbable complex order, and such order is the hallmark of biological selection. If the same is true of traits created by selection in general, then the more improbably ordered something is (i.e., the lower its entropy), the more likely it is to be a biological or cosmological adaptation. By this logic, intelligent life (as the least-entropic known entity) is more likely than black holes or anything else to be an adaptation designed by cosmological natural selection.This view contrasts with Smolin’s suggestion that black holes are an adaptation designed by cosmological natural selection and that life is the by-product of selection for black holes. Selection may be the main or only ultimate antientropic process in the universe/multiverse; that is, much or all observed order may ultimately be the product or by-product of biological and cosmological selection.	cns;multiverse;universe	Michael E. Price	2017	Complexity	10.1155/2017/4745379	physical cosmology;astrophysics;multiverse;trait;natural selection;universe;black hole;theoretical physics;physics	Comp.	-5.720665954293317	-47.195473863297956	155206
951b5ee8b13c71e66f27b22210b6df07652339eb	fractional greedy and partial restreaming partitioning: new methods for massive graph partitioning	partitioning algorithms runtime adaptation models linear programming np hard problem silicon load modeling;greedy algorithms computational complexity graph theory;running time cost fractional greedy partitioning partial restreaming partitioning massive graph partitioning computation tasks large distributed graphs heuristics hybrid streaming model graph restreaming data stream reldg refennel memory cost	Graph partitioning is an important challenging problem when performing computation tasks over large distributed graphs; the reason is that a good partitioning leads to faster computations. In this work, we first introduce a new heuristic for streaming partitioning and show that it outperforms the state-of-the-art heuristics for streaming partitioning, leading to exact balance and lower cut. Secondly, we introduce the partial restreaming partitioning which is a hybrid streaming model allowing only several portions of the graph to be restreamed while the rest is to be partitioned on a single pass of the data stream. We show that our method yields partitions of similar quality than those provided by methods restreaming the whole graph (e.g ReLDG, ReFENNEL), while incurring lower cost in running time and memory since only several portions of the graph will be restreamed.	binary space partitioning;computation;graph partition;greedy algorithm;heuristic (computer science);time complexity	Ghizlane Echbarthi;Hamamache Kheddouci	2014	2014 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2014.7004368	mathematical optimization;combinatorics;graph bandwidth;computer science;graph partition;theoretical computer science;strength of a graph	DB	-8.68697897525772	-40.99982425856075	155272
377ae49c77987cb7ab51f6083a9a0eb38ec8e79c	a multidimensional heuristic for social routing in peer-to-peer networks	decentralized social application social routing peer to peer network decentralized search social network small world property;telecommunication network routing;social networking online;livejournal social graph multidimensional heuristic social routing peer to peer networks fundamental problem decentralized social social peer to peer networks small world phenomenon social networks smallworld phenomenon unstructured social overlays real world social graph dataset online social networking website decentralized social routing algorithm;peer to peer computing;routing peer to peer computing social network services cities and towns algorithm design and analysis electronic mail;telecommunication network routing peer to peer computing social networking online	A fundamental problem encountered in designing decentralized social applications is the issue of efficiently locating target nodes in social peer-to-peer networks based on local information only. The unique “small-world” phenomenon of social networks shows that a typical pair of nodes is connected by very short chains of intermediate friends, and individuals are able to collectively discover such short paths. In this paper, we propose a decentralized algorithm that exploits this “small-world” phenomenon to discover efficient routes to reach target nodes in unstructured social overlays. In order to test the performance of this algorithm, we simulate it on a real-world social graph dataset crawled from a large online social networking website (LiveJournal). Compared with other related works, the simulation results show that our proposed decentralized social routing algorithm not only shortens the median length of the search path, but also increases the success rate of finding such routes between arbitrary pair of nodes in the LiveJournal social graph.	algorithm;geographic coordinate system;greedy algorithm;heuristic;path (variable);peer-to-peer;routing;simulation;small-world experiment;social graph;social network	Shuo Jia;Pierre St. Juste;Renato J. O. Figueiredo	2013	2013 IEEE 10th Consumer Communications and Networking Conference (CCNC)	10.1109/CCNC.2013.6488466	computer science;distributed computing;world wide web;social computing;computer network	Metrics	-16.87498900621819	-43.00442865824177	155358
ca0386d32f3869dd86452e3c61061804d1854664	using novel bio-inspired principles to improve adaptability of evolutionary robots in dynamically changing environments		An important goal in the field of evolutionary computation and robotics is the development of systems that show self-adaptation in dynamically changing environments. However, adaptation to a changing or fluctuating environment is challenging and usually requires a dynamic solution. Here, we present a bio-inspired, agent-based controller driven by an artificial gene regulatory network that enhances the adaptability of robots in dynamically changing environments. Artificial genome The Genome consists of two kinds of genes. The ‘regulatory’ gene is in charge of the regulatory functions while the ‘structural’ gene determines the phenotype of the robot through its gene expression product. The interaction between genes and the environment is based on the transcription factor binding process. Agent based GRN The dynamics of the GRN, encoded by the artificial genome, and its interaction with the environment, are modeled by an agentbased system. Figure 2. Agent based system modeling the condition dependent instantiation of the GRN encoded by the artificial genome. Three different kinds of agents are distinguished, namely signaling agents modeling the interaction between the environment and the artificial genome, regulatory agents that constitute the active part of the GRN encoded by the artificial genome, and structural agents that translate the encoded information of a structural gene to an output signal, which drives the actuators (e.g. wheel) of the robot. The agents based level is also essential in establishing the feedback from the environment to the system through the agents adaptability values which affect both the agents lifetime, genomic encoding, and mutation rate. Figure 1. Artificial genome encoding the regulatory network. Any gene, irrespective of its type, consists of the following components: a transcription start site, a gene identifier (type), a gene length region, a binding site region, an expression level region (default and gene-specific expression region), and a content region which is different for structural, regulatory and signaling genes. Results & Conclusion To demonstrate the advantages of our bio-inspired controller, we also developed an artificial life simulation platform that can provide a dynamically changing environment. Simulation results show that the bio-inspired GRN based controller has a better average adaptability than an evolutionary artificial neural network (ANN) based controller and a signal-based controller when placed in the same dynamically changing environments. Based on the experimental results, we believe the dynamics and the feedback loops of the GRN have great potential to enhance the adaptability of individual organisms in a changing environment. Figure 3. Average energy levels for the three different controllers during runtime. The Y-axis represents the average energy level of the robots while the X-axis represent running time measured in time steps. The red line shows the result for a single simulation experiment based on the GRN controller; the blue curve shows the result for the experiment based on the ANN controller, and the purple curve shows the result for the experiment based on the simple non-evolvable signal-based controller. Late Breaking Papers	agent-based model;apache axis;artificial gene synthesis;artificial life;artificial neural network;british informatics olympiad;dna binding site;energy level;evolutionary computation;evolutionary robotics;feedback;gene regulatory network;identifier;memory controller;robot;simulation;systems modeling;time complexity;transcription (software);universal instantiation	Yao Yao;Kathleen Marchal;Yves Van de Peer	2013		10.7551/978-0-262-31709-2-ch191	biology;bioinformatics;genetics	Robotics	-5.512151132272755	-48.910973091871966	155732
d16541c1a71edd9f8186662d0b768f83778aa62f	discovering user access pattern based on probabilistic latent factor model	log files;latent factor model;knowledge extraction;probabilistic model;user profile;semantic model;web linkage information;web usage mining;web mining;em algorithm;probabilistic latent semantic analysis;conference proceeding;probabilistic latent semantic model	There has been an increased demand for characterizing user access patterns using web mining techniques since the informative knowledge extracted from web server log files can not only offer benefits for web site structure improvement but also for better understanding of user navigational behavior. In this paper, we present a web usage mining method, which utilize web user usage and page linkage information to capture user access pattern based on Probabilistic Latent Semantic Analysis (PLSA) model. A specific probabilistic model analysis algorithm, EM algorithm, is applied to the integrated usage data to infer the latent semantic factors as well as generate user session clusters for revealing user access patterns. Experiments have been conducted on real world data set to validate the effectiveness of the proposed approach. The results have shown that the presented method is capable of characterizing the latent semantic factors and generating user profile in terms of weighted page vectors, which may reflect the common access interest exhibited by users among same session cluster.	data logger;data mining;expectation–maximization algorithm;experiment;information;linkage (software);probabilistic latent semantic analysis;server (computing);server log;statistical model;usage data;user profile;web mining;web server	Guandong Xu;Yanchun Zhang;Jiangang Ma;Xiaofang Zhou	2005			semantic data model;statistical model;web mining;data web;expectation–maximization algorithm;computer science;social semantic web;data mining;database;probabilistic latent semantic analysis;world wide web	Web+IR	-17.573467611284357	-48.149478103507306	155740
7a096bef3509c83c602c1470d9b804b122f01835	abstract representations and generalized frequent pattern discovery		We discuss the frequent pattern mining problem in a general setting. From an analysis of abstract representations, summarization and frequent pattern mining, we arrive at a generalization of the problem. Then, we show how the problem can be cast into the powerful language of algorithmic information theory. We formulate and prove a universal pruning theorem analogous to the well-known Downward Closure Lemma in data mining. This result allows us to formulate a simple algorithm to mine all frequent patterns given an appropriate compressor to recognize patterns.	apriori algorithm;bit array;cluster analysis;data mining;high- and low-level;information theory;kolmogorov complexity;maximal set;sed;synthetic data	Eray Özkural	2017		10.1007/978-3-319-63703-7_7	artificial intelligence;automatic summarization;machine learning;simple algorithm;lemma (mathematics);computer science;algorithmic information theory	ML	-10.427168921938819	-38.82847196197032	155770
0da46a3a064903bbaf0dc1c5c95f00d2ffbf8f2c	adaptive cluster-distance bounding for nearest neighbor search in image databases	pattern clustering;multi dimensional indexing;high dimensionality;retrieval;image database;indexing terms;multi dimensional;vector quantization;visual databases image retrieval indexing pattern clustering vector quantisation;indexing;clustering;indexation;image retrieval adaptive cluster distance bounding nearest neighbor search image database vector quantization indexing;vector quantizer;nearest neighbor search;vector quantisation;nearest neighbor searches image databases multimedia databases information retrieval spatial databases vector quantization indexing search engines biomedical imaging image storage;similarity search;clustering similarity search multi dimensional indexing retrieval vector quantization;visual databases;image retrieval	We consider approaches for exact similarity search in a high dimensional space of correlated features representing image datasets, based on principles of clustering and vector quantization. We develop an adaptive cluster distance bound based on separating hyperplanes, that complements our index in selectively retrieving clusters that contain data entries closest to the query. Experiments conducted on real data-sets confirm the efficiency of our approach with random disk IOs reduced by 100X, as compared with the popular vector approximation-file (VA-File) approach, when allowed (roughly) the same number of sequential disk accesses, with relatively low preprocessing storage and computational costs.	approximation;cluster analysis;nearest neighbor search;preprocessor;similarity search;vector quantization	Sharadh Ramaswamy;Kenneth Rose	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379601	image retrieval;computer science;machine learning;pattern recognition;data mining;mathematics;nearest neighbor search;information retrieval	DB	-5.0335927727046155	-41.64170318492092	155822
e07f9646c8c14146c9a6493e25bd0c6cb53a27a0	influence maximization in social networks based on non-backtracking random walk	social network services;greedy algorithms;computational modeling;heuristic algorithms;optimization;adaptation models;algorithm design and analysis	Influence maximization is an optimization problem that aims at finding a small set of nodes as seed nodes to maximize the spread of influence in a social network. In this paper, we introduce a new technique called Non-backtracking Random Walk into the influence maximization problem. An optimized algorithm, called NBRW, is devised through sampling nodes when applying restricted non-backtracking walk in network communities. Node's traversing number gives a good estimate of its influence. Our algorithm can be executed in linear time, which beats other influence maximization algorithms. We give extensive experiments under various diffusion models against other state-of-the-art node-selection algorithms. It demonstrates that in terms of influence range and convergent speed our algorithm gives a superior performance under most diffusion models, especially for linear threshold model.	backtracking;entropy maximization;expectation–maximization algorithm;experiment;mathematical optimization;optimization problem;sampling (signal processing);social network;threshold model;time complexity	Jingzhi Pan;Fei Jiang;Jin Xu	2016	2016 IEEE First International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2016.114	mathematical optimization;theoretical computer science;machine learning;mathematics	DB	-16.13844716295496	-43.47709057065653	155875
2b058f0abddfe638559c07222b435576f331af4a	indexing evolving events from tweet streams	event detection;event indexing;event evolution event indexing multi layer inverted list;upper bound;monitoring;indexing;heuristic algorithms;system;multi layer inverted list;incremental index maintenance event indexing structure tweet text stream social event detection single pass incremental clustering create operation absorb operation split operation merge operation multilayer inverted list mil nearest neighbour search upper bound pruning;indexing monitoring event detection heuristic algorithms upper bound twitter;twitter;text analysis feature extraction pattern clustering search problems social networking online;event evolution	Tweet streams provide a variety of real-time information on dynamic social events. Although event detection has been actively studied, most of the existing approaches do not address the issue of efficient event monitoring in the presence of a large number of events detected from continuous tweet streams. In this paper, we capture the dynamics of events using four event operations: creation, absorption, split and merge.We also propose a novel event indexing structure, named Multi-layer Inverted List (MIL), for the acceleration of large-scale event search and update. We thoroughly study the problem of nearest neighbour search using MIL based on upper bound pruning. Extensive experiments have been conducted on a large-scale tweet dataset. The results demonstrate the promising performance of our method in terms of both efficiency and effectiveness.	event monitoring;experiment;inverted index;nearest neighbor search;real-time data	HongYun Cai;Zi Xuan Huang;Divesh Srivastava;Qing Zhang	2015	2016 IEEE 32nd International Conference on Data Engineering (ICDE)	10.1109/TKDE.2015.2445773	search engine indexing;computer science;artificial intelligence;machine learning;pattern recognition;data mining;database;system;upper and lower bounds;world wide web	DB	-8.189373653999825	-39.469309762973126	156141
67bdb50b834d4ea13fedc255c03962daa8f51cbe	multi-layered social networks		A social network (SN) 1 is defined as a tuple <V,E>, where: V – is a not-empty set of actors representing social entities: humans, organizations, departments etc., called also vertices, nodes, vertexes, members, cases, agents, instances or points; E – is a set of directed edges (relations between actors called also arcs, connections, or ties) where a single edge is represented by a tuple <x,y>, x,yεV, x≠y and for two edges <x,y> and <x’,y’> if <x,y>≠<x’,y’> and x=x’ then y≠y’. Note that <x,y>≠<y,x> because we consider here directed social networks. Since social networks usually represent one kind of relationships they are also called singlelayered social network SSN [Magnani 11].	dmz (computing);entity;social network	Piotr Bródka;Przemyslaw Kazienko	2012	CoRR		social science;artificial intelligence;mathematics;sociology;social psychology	DB	-17.2728586589932	-39.213494307345506	156366
c3e2e3d4c9bb88dd28e9a7c9168e7cf0b5aff0ad	ranking influencers in social collaboration networks		The emergence of social collaboration platforms has created new ways for people or groups to interact and work collaboratively to achieve mutual goals. The research community has highlighted the importance of measuring influence in such social collaboration networks to recognize key contributors and better facilitate collaboration. However, classic graph-based approaches often fail to elaborate on relations between people and the items they collaborate on, which is the essence of such social collaboration platforms. Moreover, with traditional methods it is usually difficult to incorporate auxiliary information about users and items into the modeling process. Therefore, in this paper we draw an analogy between recommender systems and social collaboration networks, and propose a method to harness information about users and items to model influence diffusion in collaboration networks. With this analogy, we quantify social influence by measuring the latent influence of each user-item pair under a unified framework based on factorization machines. Experiments conducted on a real-world dataset collected from GitHub attest the effectiveness of the proposed framework in providing better rankings for top influencers than several baseline methods. In addition, interesting analyses and discussions on the results are also provided in the paper.	baseline (configuration management);emergence;recommender system;social collaboration;unified framework	Chih-Wei Chang;Chia-Yu Tsui;Zhe-Li Lin;Chao-Ching Chiang;Chuan-Ju Wang;Ming-Feng Tsai	2017	2017 Conference on Technologies and Applications of Artificial Intelligence (TAAI)	10.1109/TAAI.2017.21	management science;recommender system;influencer marketing;analogy;social influence;social collaboration;ranking;graph;computer science	Web+IR	-18.812098596059137	-46.46339737152008	156712
0519e43bf25b5065312fa50683e8a7f633293513	governing the swarm: controlling a bio-hybrid society of bees & robots with computational feedback loops		Bio-hybrid systems in which living organisms interact and self-organize with multi-robot systems are a novel approach in engineering and biology. We show here how a group of honeybees and robots can interact in collective decision making and how computer code that adds feedback loops to the system may affect the global system properties. This study contains a series of experiments with living honeybees and robots as well as a cellular-automaton inspired model that is simple, yet still in good agreement with the empirical findings presented here. Using this model, we explore the most likely candidates for local parameters in the proximate mechanisms of the animals, thus further the understanding of this natural system in a context that is relevant for such bio-hybrid manifestations. We identify positive feedback based on bee-to-bee collision and temperature as an important factor governing collective decision making and found the stopping probability after close-encounters among bees as a crucial local parameter. This study is the first step towards using computer and robotic technology to monitor and control complex animal societies like honeybees.	autonomous robot;british informatics olympiad;cellular automaton;computer;experiment;hybrid system;positive feedback;self-organization;swarm	Martin Stefanec;Martina Szopek;Thomas Schmickl;Rob Mills	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8285346	swarm behaviour;complex system;swarm intelligence;machine learning;robot;positive feedback;group decision-making;source code;swarm robotics;artificial intelligence	Robotics	-5.4688175927178415	-48.09308672208264	156811
f76aeae94fd03b810a53546004f315871db3555e	on democratic evaluation of nodes representativity		The characterization of representative nodes has great relevance in several contexts, such as semi-supervised machine learning, contagion process, rumor spreading, among others. We propose a method to nodes characterization in networks based on a voting process, a method other than the several methods in the literature, which works on the active participation of the network nodes, by considering the point of view of the nodes, in such a democratic evaluation of the network. The proposed method is studied through computational simulations on real-world and synthetic networks. Results are presented by considering the influence of the free parameters, in comparison with results of reference methods, and in relation to ground-truth knowledge about the networks, revealing consistency of the results, practicality, and some advantages. Several insights on opened issues are provided.	algorithmic efficiency;apollonian network;centrality;computation;computer simulation;ground truth;lazy evaluation;machine learning;relevance;semi-supervised learning;semiconductor industry;supervised learning;synthetic intelligence	Serguei Damián Rico-Esenaro;Hillary Shah	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489512	democracy;knowledge engineering;machine learning;node (networking);rumor;artificial intelligence;voting;computer science	AI	-14.747015664013382	-44.88366567461034	156826
de628e41761e669b2e24a5da00a25aaa6905928a	detecting spammer communities using network structural features		Spammers generate fake reviews to influence the reputation of products. By grouping together, spammers can dramatically alter how products are perceived. Different from previous research, which has mostly used behavioral indicators and structural indicators, we propose a new perspective on spammer detection. In our approach, we portray reviewers as a comment-based reviewer network through a new collusion similarity measure, divide reviewers into different communities using an effective community detection method and separate spammer communities from normal reviewer communities through network structure. We find that spammer communities have different network structural features from normal reviewer communities, a high clustering coefficient and high self-similarity. In our experiments, we show that our method achieves a detection accuracy of 94.59% - substantially higher than the current state-of-the-art methods which achieve an 80.00% accuracy.		Wen Zhou;Meng Liu;Yajun Zhang	2017		10.1007/978-3-030-00916-8_61	data mining;collusion;similarity measure;computer science;distributed computing;spamming;clustering coefficient;reputation	Robotics	-16.016216303860286	-41.76133735922329	156876
0710099aee18aabc0605c52628c29ac5ad94ec60	asymmetric transitivity preserving graph embedding	high order proximity;directed graph;graph embedding;asymmetric transitivity	Graph embedding algorithms embed a graph into a vector space where the structure and the inherent properties of the graph are preserved. The existing graph embedding methods cannot preserve the asymmetric transitivity well, which is a critical property of directed graphs. Asymmetric transitivity depicts the correlation among directed edges, that is, if there is a directed path from u to v, then there is likely a directed edge from u to v. Asymmetric transitivity can help in capturing structures of graphs and recovering from partially observed graphs. To tackle this challenge, we propose the idea of preserving asymmetric transitivity by approximating high-order proximity which are based on asymmetric transitivity. In particular, we develop a novel graph embedding algorithm, High-Order Proximity preserved Embedding (HOPE for short), which is scalable to preserve high-order proximities of large scale graphs and capable of capturing the asymmetric transitivity. More specifically, we first derive a general formulation that cover multiple popular high-order proximity measurements, then propose a scalable embedding algorithm to approximate the high-order proximity measurements based on their general formulation. Moreover, we provide a theoretical upper bound on the RMSE (Root Mean Squared Error) of the approximation. Our empirical experiments on a synthetic dataset and three real-world datasets demonstrate that HOPE can approximate the high-order proximities significantly better than the state-of-art algorithms and outperform the state-of-art algorithms in tasks of reconstruction, link prediction and vertex recommendation.	approximation algorithm;directed graph;experiment;graph (discrete mathematics);graph embedding;mean squared error;path (graph theory);scalability;synthetic intelligence;vertex-transitive graph	Mingdong Ou;Peng Cui;Jian Pei;Ziwei Zhang;Wenwu Zhu	2016		10.1145/2939672.2939751	topological graph theory;combinatorics;discrete mathematics;feedback arc set;graph embedding;directed graph;topology;null graph;graph property;comparability graph;asymmetric graph;planar straight-line graph;mathematics;voltage graph;graph;complement graph;book embedding;line graph;planar graph	ML	-11.702468187224804	-41.1568672469946	156913
b56b9f17808b29cf9138df4b3f4e1d5c7debe549	label-dependent node classification in the network	classification in networks;gibbs sampling;classification;label dependent features;ldbootstrapping;node classification;bootstrapping;label dependent classification;ldgibbs;collective classification	Relations between objects in various systems, such as hyperlinks connecting web pages, citations of scientific papers, conversations via email or social interactions in Web 2.0 portals are commonly modeled by networks. One of many interesting problems currently studied for such domains is node classification. Due to the nature of the networked data and the unavailability of collection of nodes’ broad representation for training in majority of environments, only a very limited data may remain useful for classification. Therefore, there is a need for accurate and efficient algorithms that are able to perform good classification based only on scanty knowledge of network nodes. A new approach of sampling algorithm—LDGibbs, used in the context of collective classification with application of label-dependent features, is proposed in the paper in order to provide more accurate generalization for sparse datasets. Additionally, a new LDBootstrapping algorithm based on label-dependent features has been developed. Both new algorithms include additional steps to extract new input features based on graph structures but limited only to the nodes of a given label. It means that a separate set of structural features is provided for each label. The comparison with the other approaches, in particular with standard Gibbs Sampling and bootstrapping provided satisfactory results and revealed LDGibbs’s superiority. & 2011 Elsevier B.V. All rights reserved.	algorithm;artificial intelligence;email;feature extraction;gibbs sampling;graph (discrete mathematics);hyperlink;interaction;iterative method;portals;sampling (signal processing);scheme;scientific literature;sparse matrix;statistical classification;unavailability;web 2.0;web page	Przemyslaw Kazienko;Tomasz Kajdanowicz	2012	Neurocomputing	10.1016/j.neucom.2011.04.047	web query classification;gibbs sampling;biological classification;computer science;machine learning;pattern recognition;data mining;one-class classification;bootstrapping;statistics;library classification	AI	-18.7697746147511	-44.69348162442839	157354
637aa0c0214cb6783e4ae4d076eb5fadcfcd82e7	scalable and parallelizable processing of influence maximization for large-scale social networks?	influence maximization;parallelizable processing openmp meta programming expression influence evaluation unit independent cascade diffusion model ipa independent path algorithm scalable influence approximation algorithm polynomial time p hard viral marketing social network service influence maximization;social networking online approximation theory computational complexity message passing parallel processing;approximation theory;integrated circuit modeling greedy algorithms social network services approximation methods approximation algorithms mathematical model;computational complexity;social networks;social networking online;message passing;parallel processing influence maximization social networks;parallel processing	As social network services connect people across the world, influence maximization, i.e., finding the most influential nodes (or individuals) in the network, is being actively researched with applications to viral marketing. One crucial challenge in scalable influence maximization processing is evaluating influence, which is #P-hard and thus hard to solve in polynomial time. We propose a scalable influence approximation algorithm, Independent Path Algorithm (IPA) for Independent Cascade (IC) diffusion model. IPA efficiently approximates influence by considering an independent influence path as an influence evaluation unit. IPA are also easily parallelized by simply adding a few lines of OpenMP meta-programming expressions. Also, overhead of maintaining influence paths in memory is relieved by safely throwing away insignificant influence paths. Extensive experiments conducted on large-scale real social networks show that IPA is an order of magnitude faster and uses less memory than the state of the art algorithms. Our experimental results also show that parallel versions of IPA speeds up further as the number of CPU cores increases, and more speed-up is achieved for larger datasets. The algorithms have been implemented in our demo application for influence maximization (available at http://dm.postech.ac.kr/ipa demo), which efficiently finds the most influential nodes in a social network.	approximation algorithm;central processing unit;entropy maximization;expectation–maximization algorithm;experiment;fastest;greedy algorithm;metaprogramming;monte carlo;multi-core processor;openmp;overhead (computing);p (complexity);parallel computing;scalability;sharp-p;social network;time complexity	Jinha Kim;Seung-Keol Kim;Hwanjo Yu	2013	2013 IEEE 29th International Conference on Data Engineering (ICDE)	10.1109/ICDE.2013.6544831	parallel processing;message passing;computer science;theoretical computer science;machine learning;data mining;database;distributed computing;programming language;computational complexity theory;social network;approximation theory	DB	-10.135556130628789	-41.70864130997245	157534
a31891a69e7d6a17f239a5ff0975c27af24c950a	distributed efficient provenance-aware regular path queries on large rdf graphs		With the proliferation of knowledge graphs, massive RDF graphs have been published on the Web. As an essential type of queries for RDF graphs, Regular Path Queries (RPQs) have been attracting increasing research efforts. However, the existing query processing approaches mainly focus on the standard semantics of RPQs, which cannot provide provenance of the answer sets. We propose dProvRPQ that is a distributed approach to evaluating provenance-aware RPQs over big RDF graphs. Our Pregel-based method employs Glushkov automata to keep track of matching processes of RPQs in parallel. Meanwhile, four optimization strategies are devised, including edge filtering, candidate states, message compression, and message selection, which can reduce the intermediate results of the basic dProvRPQ algorithm dramatically and overcome the counting-paths problem to some extent. The proposed algorithms are verified by extensive experiments on both synthetic and real-world datasets, which show that our approach can efficiently answer the provenance-aware RPQs over large RDF graphs.		Yueqi Xin;Xin Wang;Di Jin;Simiao Wang	2018		10.1007/978-3-319-91452-7_49	rdf;computer science;database;theoretical computer science;semantics;filter (signal processing);graph	DB	-8.931401877442449	-39.85545691210255	157783
8dafeed5cc1f50311ccb186c05ff99b1cf77283a	a generic framework for interesting subspace cluster detection in multi-attributed networks		Detection of interesting (e.g., coherent or anomalous) clusters has been studied extensively on plain or univariate networks, with various applications. Recently, algorithms have been extended to networks with multiple attributes for each node in the real-world. In a multi-attributed network, often, a cluster of nodes is only interesting for a subset (subspace) of attributes, andthis type of clusters is called subspace clusters. However, in the current literature, few methods are capable of detecting subspace clusters, which involves concurrent feature selection and network cluster detection. These relevant methods are mostly heuristic-driven and customized for specific application scenarios. In this work, we present a generic and theoretical framework for detection of interesting subspace clusters in large multi-attributed networks. Specifically, we propose a subspace graph-structured matching pursuit algorithm, namely, SG-Pursuit, to address a broad class of such problems for different scorefunctions (e.g., coherence or anomalous functions) and topology constraints (e.g., connected subgraphs and dense subgraphs). We prove that our algorithm 1) runs in nearly-linear time on the network size and the total number of attributes and 2) enjoys rigorous guarantees (geometrical convergence rate and tight error bound) analogous to those of the state-of-the-art algorithms for sparse feature selection problems and subgraph detection problems. As a case study, we specialize SG-Pursuit to optimizea number of well-known score functions for two typical tasks, including detection of coherent dense and anomalous connected subspace clusters in real-world networks. Empirical evidence demonstrates that our proposed generic algorithm SG-Pursuit is superior over state-of-the-art methods that are designed specifically for these two tasks.	algorithm;basis pursuit denoising;coherence (physics);computer cluster;feature selection;generic programming;heuristic;matching pursuit;pursuit-evasion;rate of convergence;sensor;sparse matrix;suicidegirls;time complexity	Feng Chen;Baojian Zhou;Adil Alim;Liang Zhao	2017	2017 IEEE International Conference on Data Mining (ICDM)	10.1109/ICDM.2017.13	machine learning;rate of convergence;mathematics;genetic algorithm;artificial intelligence;feature extraction;algorithm design;cluster analysis;feature selection;subspace topology;univariate	ML	-12.722396482856402	-41.85093859225887	157870
aa39e845a8dc4af95d16f93f3cd904a91f4564a4	breaking cycles in noisy hierarchies		Taxonomy graphs that capture hyponymy or meronymy relationships through directed edges are expected to be acyclic. However, in practice, they may have thousands of cycles, as they are often created in a crowd-sourced way. Since these cycles represent logical fallacies, they need to be removed for many web applications. In this paper, we address the problem of breaking cycles while preserving the logical structure (hierarchy) of a directed graph as much as possible. Existing approaches for this problem either need manual intervention or use heuristics that can critically alter the taxonomy structure. In contrast, our approach infers graph hierarchy using a range of features, including a Bayesian skill rating system and a social agony metric. We also devise several strategies to leverage the inferred hierarchy for removing a small subset of edges to make the graph acyclic. Extensive experiments demonstrate the effectiveness of our approach.	adjacency matrix;chomsky hierarchy;crowdsourcing;directed acyclic graph;directed graph;distributed file system (microsoft);experiment;graph embedding;heuristic (computer science);international conference on services computing;open road tolling;recommender system;scalability;sports rating system;transitive closure;web application	Jiankai Sun;Deepak Ajwani;Patrick K. Nicholson;Alessandra Sala;Srinivasan Parthasarathy	2017		10.1145/3091478.3091495	feedback arc set;directed acyclic graph;moral graph;wait-for graph;closure problem;acyclic dependencies principle;machine learning;directed graph;artificial intelligence;computer science;directed acyclic word graph	DB	-13.103187393371577	-45.85954963018643	158013
2fa9c668ecf6204832f994307f527395fdd79ef4	community detection for emerging networks		Nowadays, many new social networks offering specific services spring up overnight. In this paper, we want to detect communities for emerging networks. Community detection for emerging networks is very challenging as information in emerging networks is usually too sparse for traditional methods to calculate effective closeness scores among users and achieve good community detection results. Meanwhile, users nowadays usually join multiple social networks simultaneously, some of which are developed and can share common information with the emerging networks. Based on both link and attribution information across multiple networks, a new general closeness measure, intimacy, is introduced in this paper. With both micro and macro controls, an effective and efficient method, CAD (Cold stArt community Detector), is proposed to propagate information from developed network to calculate effective intimacy scores among users in emerging networks. Extensive experiments conducted on real-world social networks demonstrate that CAD can perform very well in addressing the emerging network community detection problem.	centrality;computer-aided design;experiment;social network;sparse matrix	Jiawei Zhang;Philip S. Yu	2015		10.1137/1.9781611974010.15	artificial intelligence;computer science;machine learning;closeness;macro;social network;distributed computing	AI	-16.122099619863747	-45.085537716900106	158126
b670d651fbbb7e87f3eb58735640c0a974c42a8f	estimating the degrees of neighboring nodes in online social networks	distributed computation;online social networks;degree estimations	We propose an agent centric algorithm that each agent (i.e., node) in a social network can use to estimate each of its neighbor’s degree. The knowledge about the degrees of neighboring nodes is useful for many existing algorithms in social networks studies. For example, algorithms to estimate the diffusion rate of information spread need such information. In many studies, either such degree information is assumed to be available or an overall probabilistic distribution of degrees of nodes is presumed. Furthermore, most of these existing algorithms facilitate a macro-level analysis assuming the entire network is available to the researcher although sampling may be required due to the size of the network. In this paper, we consider the case that the network topology is unknown to individual nodes and therefore each node must estimate the degrees of its neighbors. In estimating the degrees, the algorithm correlates observable activities of neighbors to Bernoulli trials and utilize a power-law distribution to infer unobservable activities. Our algorithm was able to estimate the neighbors’ degrees in 92% accuracy for the 60867 number of nodes. We evaluate the mean squared error of accuracy for the proposed algorithm on a real and a synthetic networks.		Jooyoung Lee;Jae C. Oh	2014		10.1007/978-3-319-13191-7_4	degree distribution;evolving networks;computer science;theoretical computer science;machine learning;distributed computing	ECom	-17.595470800189585	-43.38659125231307	158185
24aa3763eaef6a68cf43ba77d8e89be97decb2e0	density-based and transport-based core-periphery structures in networks	networks;pedestrian safety;nodes networks;poison control;injury prevention;safety literature;traffic safety;injury control;home safety;injury research;safety abstracts;urban areas;human factors;occupational safety;transportation;safety;financial analysis;safety research;accident prevention;violence prevention;bicycle safety;density;poisoning prevention;falls;ergonomics;suicide prevention	Networks often possess mesoscale structures, and studying them can yield insights into both structure and function. It is most common to study community structure, but numerous other types of mesoscale structures also exist. In this paper, we examine core-periphery structures based on both density and transport. In such structures, core network components are well-connected both among themselves and to peripheral components, which are not well-connected to anything. We examine core-periphery structures in a wide range of examples of transportation, social, and financial networks-including road networks in large urban areas, a rabbit warren, a dolphin social network, a European interbank network, and a migration network between counties in the United States. We illustrate that a recently developed transport-based notion of node coreness is very useful for characterizing transportation networks. We also generalize this notion to examine core versus peripheral edges, and we show that the resulting diagnostic is also useful for transportation networks. To examine the properties of transportation networks further, we develop a family of generative models of roadlike networks. We illustrate the effect of the dimensionality of the embedding space on transportation networks, and we demonstrate that the correlations between different measures of coreness can be very different for different types of networks.	anatomic node;core-periphery structure;dolphin;embedding;generative model;peripheral;sinoatrial node;social network;tursiops truncatus;warren abstract machine	Sang Hoon Lee;Mihai Cucuringu;Mason A. Porter	2014	Physical review. E, Statistical, nonlinear, and soft matter physics	10.1103/PhysRevE.89.032810	transport;financial analysis;evolving networks;density;suicide prevention;human factors and ergonomics;injury prevention;thermodynamics;physics	ML	-16.029124279922563	-38.339723203004326	158415
007d23aa75f13862843caa5f446fe00619dfb727	incremental graph pattern matching	affected area;social network;pattern matching;linear time;bounded incremental matching algorithms;incremental algorithm;synthetic data	Graph pattern matching has become a routine process in emerging applications such as social networks. In practice a data graph is typically large, and is frequently updated with small changes. It is often prohibitively expensive to recompute matches from scratch via batch algorithms when the graph is updated. With this comes the need for incremental algorithms that compute changes to the matches in response to updates, to minimize unnecessary recomputation. This paper investigates incremental algorithms for graph pattern matching defined in terms of graph simulation, bounded simulation and subgraph isomorphism. (1) For simulation, we provide incremental algorithms for unit updates and certain graph patterns. These algorithms are optimal: in linear time in the size of the changes in the input and output, which characterizes the cost that is inherent to the problem itself. For general patterns we show that the incremental matching problem is unbounded, i.e., its cost is not determined by the size of the changes alone. (2) For bounded simulation, we show that the problem is unbounded even for unit updates and path patterns. (3) For subgraph isomorphism, we show that the problem is intractable and unbounded for unit updates and path patterns. (4) For multiple updates, we develop an incremental algorithm for each of simulation, bounded simulation and subgraph isomorphism. We experimentally verify that these incremental algorithms significantly outperform their batch counterparts in response to small changes, using real-life data and synthetic data.	approximation algorithm;dynamic problem (algorithms);experiment;graph isomorphism problem;input/output;mapreduce;matching (graph theory);pattern matching;real life;simulation;social network;subgraph isomorphism problem;synthetic data;time complexity	Wenfei Fan;Jianzhong Li;Jizhou Luo;Zijing Tan;Xin Wang;Yinghui Wu	2011		10.1145/1989323.1989420	time complexity;mathematical optimization;computer science;machine learning;pattern matching;subgraph isomorphism problem;programming language;social network;synthetic data	DB	-9.974952066604988	-40.231575174197566	158739
20bf74f43a8dc8283c74a17aa2186f2327c21404	op-dci: a riskless k-means clustering for influential user identification in mooc forum		Massive Open Online Courses (MOOCs) have recently been highly popular among worldwide learners, while it is challenging to manage and interpret the large-scale discussion forum which is the dominant channel of online communication. K-Means clustering, one of the famous unsupervised learning algorithms, could help instructors identify influential users in MOOC forum, to better understand and improve online learning experience. However, traditional K-Means suffers from bias of outliers and risk of falling into local optimum. In this paper, OP-DCI, an optimized K-Means algorithm is proposed, using outlier post-labeling and distant centroid initialization. Outliers are not solely filtered out but extracted as distinct objects for post-labeling, and distant centroid initialization eliminates the risk of falling into local optimum. With OP-DCI, learners in MOOC forum are clustered efficiently with satisfactory interpretation, and instructors can subsequently design personalized learning strategies for different clusters.	algorithm;cluster analysis;computer-mediated communication;data, context and interaction;digitally controlled impedance;k-means clustering;machine learning;personalization	Xiangyu Hou;Chi-Un Lei;Yu-Kwong Kwok	2017	2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2017.00-34	artificial intelligence;computer science;local optimum;machine learning;initialization;outlier;cluster analysis;algorithm design;k-means clustering;unsupervised learning;personalized learning	ML	-14.843321478966963	-48.97620394220904	158983
f436505eaf94e0bc5d434132a16625c39fab9b3c	time aware index for link prediction in social networks	link prediction;ssocial networks;time stamps;temporal behavior	Link prediction in social networks such as collaboration networks and friendship networks have recently attracted a great deal of attention. There have been numerous attempts to address this problem through diverse approaches. In the present paper, we focus on the temporal behavior of the link strength, particularly the relationship between the time stamps of interactions or links and the temporal behavior of link strength and how link strength affects future link evolution. Most of the previous studies neglected the impact of time stamps of the interactions and of the links on link evolution. The gap between the current time and the time stamps of the interactions or links is also important to link evolution. In the present paper, we introduced a new time aware index, referred to as time score, that captures the important aspects of time stamps of interactions and the temporality of the link strengths. We apply time score to two social network data sets, namely, a coauthorship network data set and a Facebook friendship network data set. The results reveal a significant improvement in predicting future links.	social network	Lankeshwara Munasinghe;Ryutaro Ichise	2011		10.1007/978-3-642-23544-3_26	telecommunications;artificial intelligence	ML	-18.94012706170968	-43.285920484468676	158993
e93d6ca87899aaa290375900a9773e0c4af24ee7	challenges in streaming graph analysis	parallel algorithms data models graph theory;analytical models;graph theory;theoretical model;electronic mail;efficient algorithm;computer model;laboratories aging computational modeling usa councils algorithm design and analysis electronic mail analytical models;aging;usa councils;parallel system streaming graph analysis data streaming cyber analysis cyber data modelling fundamental graph problems;computational modeling;parallel systems;graph algorithm;connected component;algorithm design;algorithm design and analysis;analytical model;data models;parallel algorithms	The volume of streaming data for cyber analysis is increasing at a rate much greater than any organization's ability to hire human analysts. As a preliminary step to automating significant portions of analysis workload, we consider the problem of modeling cyber data. Since the latter tends to be relational in nature, graphs are a natural abstraction. This motivates future research into efficient algorithms for fundamental graph problems in a high-volume, streaming environment. Algorithms designed using current theoretical models for streaming graph algorithms are not directly suitable for operations. In this talk, we propose a new streaming model that can be implemented on a parallel system with extremely simple topology. Our model assumes an infinite stream which must be analyzed using finite resources. We illustrate the associated challenges by giving an algorithm for maintaining and querying the connected components of a graph in which edges may be expired periodically.	algorithm;connected component (graph theory);graph theory;streaming media	Jonathan W. Berry;Matthew Oster;Cynthia A. Phillips;Steven J. Plimpton	2011	2011 - MILCOM 2011 Military Communications Conference	10.1109/MILCOM.2011.6127489	computer science;theoretical computer science;distributed computing	EDA	-10.737714892367313	-39.6365275992074	159004
2cc2f71e1ded405347034b2964ea9fe66d71ca6d	multi-role event organization in social networks		Abstract Recently, event-based social networks (EBSNs) have become popular, hence how to organize a social event has received significant attention. Most of prior studies about social events organization usually consider the willingness of attendees and their relationships. However, they ignore the roles of attendees. In fact, many social events have requirement of attendees roles in the real world. In this paper, we propose to study the problem of Multi-Role Social Event Organization (MRSEO). Our goal is to maximize the overall harmony of the social event while considering multiple factors, such as attendees’ roles, willingness and their relationships. To solve the problem, we propose two algorithms. Firstly, we propose a continuous relaxation technique based algorithm, called MRSEO-CRA. It converts the problem of MRSEO to an equivalent unconstrained continuous problem, and then employs RatioDCA algorithm to solve the converted one. Secondly, to better trade off between performance and running time, we further propose the other algorithm based on improved PageRank, called MRSEO-IPR. We conduct extensive experiments on real-world datasets to evaluate these two proposed algorithms and experimental results show that our algorithms outperform the state-of-the-art algorithm in terms of performance and running time.	social network	Siyuan Gao;Zhongbao Zhang;Sen Su;Muhammad Azam Zia	2018	Inf. Sci.	10.1016/j.ins.2018.03.017	harmony (color);artificial intelligence;machine learning;mathematics;pagerank;trade-off;social network	AI	-16.670065646053796	-44.833280681209544	159052
ca014f775868191c22b6f7b5847ef4ecafacff8f	quick and easy implementation of approximate similarity search with lucene		Similarity search technique has been proved to be an effective way for retrieving multimedia content. However, as the amount of available multimedia data increases, the cost of developing from scratch a robust and scalable system with content-based image retrieval facilities is quite prohibitive.	similarity search	Giuseppe Amato;Paolo Bolettieri;Claudio Gennaro;Fausto Rabitti	2012		10.1007/978-3-642-35834-0_17	information retrieval;scalability;computer science;image retrieval;scratch;nearest neighbor search	NLP	-13.813422223849184	-50.46613535683119	159581
828cacf277c3d7e0720cca3b8434aca66ba690af	efficient non-spatial and spatial simulation of biochemical reaction networks			simulation	Matthias Jeschke	2010			computer science	ML	-8.728937361853838	-47.89310026481861	160208
ca3eb256b4dcafeca53f47c3e4ee3584c6e1b2c1	worldlets: 3-d thumbnails for wayfinding in large virtual worlds	virtual worlds	Finding one's way to sites of interest on the Web can be problematic, and this difficulty has been recently exacerbated by widespread development of 3-D Web content and virtual-world browser technology using the Virtual Reality Modeling Language (VRML). Whereas travelers can often navigate 2-D Web sites based on textual and 2-D thumbnail image representations of the sites' content, finding one's way to destinations in 3-D environments is notoriously troublesome. Wayfinding literature provides clear support for the importance of landmarks in building a cognitive map and then using that map to navigate in a 3-D environment, be it real or virtual. Textual and 2-D image landmark representations, however, lack the depth and context needed for travelers to reliably recognize 3-D landmarks. This paper describes a novel 3-D thumbnail landmark affordance called a worldlet. Containing a 3-D fragment of a virtual world, worldlets offer travelers first-person, multi-viewpoint experience with faithful representations of potential destinations. To facilitate an investigation into the comparative advantages of landmark affordances for wayfinding, worldlet capture algorithms were designed, implemented, and incorporated into two VRML-based virtual environment browsers. Findings from a psychological experiment using one of these browsers revealed that, compared to textual and image guidebook usage, worldlet guidebook usage: nearly doubled the time subjects spent studying the landmarks in the guidebook, significantly reduced the time required for subjects to reach landmarks, and reduced backtracking to almost zero. These results support the hypothesis that worldlets facilitate traveler landmark knowledge, expedite wayfinding in large virtual environments, and enable skilled wayfinding.	algorithm;backtracking;cognitive map;experiment;modeling language;self-information;social affordance;thumbnail;vrml;virtual reality;virtual world;web content;world wide web	T. Todd Elvins;David R. Nadeau;Rina Schul;David Kirsh	2001	Presence: Teleoperators & Virtual Environments	10.1162/105474601753272835	simulation;computer science;artificial intelligence;metaverse;multimedia	HCI	-13.64030834004603	-50.3066037790373	160306
91eccc6e5257cb5e8295454bbcb0cf646a013978	onset synchronization in weighted complex networks: the effect of weight-degree correlation	oscillations;complex networks;coupled oscillator;nonlinear dynamical systems;complex network;weight distribution;grupo de excelencia;synchronisation;numerical analysis;ciencias basicas y experimentales;matematicas;network dynamics;nonlinear dynamic system;numerical simulation	By numerical simulations, we investigate the onset of synchronization of networked phase oscillators under two different weighting schemes. In scheme-I, the link weights are correlated to the product of the degrees of the connected nodes, so this kind of networks is named as the weight-degree correlated (WDC) network. In scheme-II, the link weights are randomly assigned to each link regardless of the node degrees, so this kind of networks is named as the weight-degree uncorrelated (WDU) network. Interestingly, it is found that by increasing a parameter that governs the weight distribution, the onset of synchronization in WDC network is monotonically enhanced, while in WDU network there is a reverse in the synchronization performance. We investigate this phenomenon from the viewpoint of gradient network, and explain the contrary roles of coupling gradient on network synchronization: gradient promotes synchronization in WDC network, while deteriorates synchronization in WDU network. The findings highlight the fact that, besides the link weight, the correlation between the weight and the node degree is also important to the network dynamics.	anatomic node;computer simulation;gradient;minimum-weight triangulation;name;node - plant part;numerical analysis;onset (audio);personnameuse - assigned;population parameter;published comment;randomness;wdu (software);weight	Menghui Li;Xingang Wang;Ying Fan;Zengru Di;Choy Heng Lai	2011	Chaos	10.1063/1.3597646	computer simulation;control theory;mathematics;distributed computing;complex network	Metrics	-16.03603575287386	-39.20916506658983	160334
7d7eaf544fc7cba6159cefd0e11c7b6dc1bf443f	clustering cubes with binary dimensions in one pass	olap;clustering;dbms	Finding aggregations of records with high dimensionality in large data warehouses is a crucial and costly task. These groups of similar records are the result of partitions obtained with GROUP BYs. In this research, we focus on obtaining aggregations of groups of similar records by turning the problem into efficient binary clustering of a fact table as a relaxation of a GROUP BY clause. We present an efficient window-based Incremental K-Means algorithm in a relational database system implemented as a user-defined function. This variant is based on the Incremental K-Means algorithm. The speed up is achieved through the computation of sufficient statistics, multithreading, efficient distance computation and sparse matrix operations. Finally, the performance of our algorithm is compared against multiple variants of the K-Means algorithm. Our experiments show that our incremental K-Means algorithm achieves similar or even better results more quickly than the traditional K-Means algorithm.	aggregate function;algorithm;cluster analysis;computation;experiment;k-means clustering;linear programming relaxation;multithreading (computer architecture);olap cube;relational database management system;sparse matrix;thread (computing);user-defined function	Carlos Garcia-Alvarado;Carlos Ordonez	2013		10.1145/2513190.2513192	online analytical processing;computer science;theoretical computer science;canopy clustering algorithm;data mining;database;cluster analysis;population-based incremental learning	ML	-5.32177131165545	-38.674391538233344	160402
13ded645888b4287f5dc4248bd6f22459d5d5185	unveiling hidden communities through cascading detection on network structures		Community detection is the process of assigning nodes and links in significant communities (e.g. clusters, function modules) and its development has led to a better understanding of complex networks. When applied to sizable networks, we argue that most detection algorithms correctly identify prominent communities, but fail to do so across multiple scales. As a result, a significant fraction of the network is left uncharted. We show that this problem stems from larger or denser communities overshadowing smaller or sparser ones, and that this effect accounts for most of the undetected communities and unassigned links. We propose a generic cascading approach to community detection that circumvents the problem. Using real network datasets with two widely used community detection algorithms, we show how cascading detection allows for the detection of the missing communities and results in a significant drop of the fraction of unassigned links.	algorithm;complex network;computer cluster	Jean-Gabriel Young;Antoine Allard;Laurent Hébert-Dufresne;Louis J. Dubé	2012	CoRR		machine learning;data mining;world wide web	ML	-15.551368456687857	-41.746989538117134	160828
f9f5ad867061ea3ea1af25cf45b29eaf8f755e85	discrete simulations of biochemical dynamics	computer model;discrete biological models;p systems;metabolic grammars;p system;discrete dynamics;discrete simulation;metabolism	Metabolic P systems, shortly MP systems, are a special class of P systems, introduced for expressing biological metabolism. Their dynamics are computed by metabolic algorithms which transform populations of objects according to a mass partition principle, based on suitable generalizations of chemical laws. The definition of MP system is given and a new kind of regulation mechanism is outlined, for the construction of computational models from experimental data of given metabolic processes.	algorithm;computational model;computer simulation;p system;population	Vincenzo Manca	2007		10.1007/978-3-540-77962-9_24	discrete mathematics;computer science;theoretical computer science;discrete system	Logic	-5.72880645797921	-49.58982873510428	160913
26c4e0dd4787c52935523ef8caef3b03e37a5e8b	practical message-passing framework for large-scale combinatorial optimization	message passing framework approximation ratio independent set vertex cover maximum weight matching hybrid damping bp belief propagation parallel processing distributed processing big data analytics gm graphical model combinatorial optimization;convergence;parallel algorithm;approximation algorithms;maximum weighted matching combinatorial optimization belief propagation parallel algorithm;optimization algorithm design and analysis approximation algorithms approximation methods convergence big data belief propagation;big data;belief propagation;optimization;approximation methods;combinatorial optimization;algorithm design and analysis;set theory approximation theory belief networks big data data analysis graph theory mathematics computing message passing optimisation parallel processing pattern matching;maximum weighted matching	Graphical Model (GM) has provided a popular framework for big data analytics because it often lends itself to distributed and parallel processing by utilizing graph-based `local' structures. It models correlated random variables where in particular, the max-product Belief Propagation (BP) is the most popular heuristic to compute the most-likely assignment in GMs. In the past years, it has been proven that BP can solve a few classes of combinatorial optimization problems under certain conditions. Motivated by this, we explore the prospect of using BP to solve generic combinatorial optimization problems. The challenge is that, in practice, BP may converge very slowly and even if it does converge, the BP decision often violates the constraints of the original problem. This paper proposes a generic framework that enables us to apply BP-based algorithms to compute an approximate feasible solution for an arbitrary combinatorial optimization task. The main novel ingredients include (a) careful initialization of BP messages, (b) hybrid damping on BP updates, and (c) post-processing using BP beliefs. Utilizing the framework, we develop parallel algorithms for several large-scale combinatorial optimization problems including maximum weight matching, vertex cover and independent set. We demonstrate that our framework delivers high approximation ratio, speeds up the process by parallelization, and allows large-scale processing involving billions of variables.	approximation algorithm;backpropagation;belief propagation;big data;combinatorial optimization;converge;emoticon;graphical model;heuristic;independent set (graph theory);matching (graph theory);mathematical optimization;message passing;parallel algorithm;parallel computing;vertex cover;video post-processing	Inho Cho;Soya Park;Sejun Park;Dongsu Han;Jinwoo Shin	2015	2015 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2015.7363737	optimization problem;mathematical optimization;combinatorial optimization;computer science;theoretical computer science;machine learning	ML	-9.665514034153533	-41.9393746092442	161171
b6ac64622d9c344df780091a667e47f9009af6d4	social temporal collaborative ranking for context aware movie recommendation	context awareness;user feedback;collaborative filtering;recommender systems	Most existing collaborative filtering models only consider the use of user feedback (e.g., ratings) and meta data (e.g., content, demographics). However, in most real world recommender systems, context information, such as time and social networks, are also very important factors that could be considered in order to produce more accurate recommendations. In this work, we address several challenges for the context aware movie recommendation tasks in CAMRa 2010: (1) how to combine multiple heterogeneous forms of user feedback? (2) how to cope with dynamic user and item characteristics? (3) how to capture and utilize social connections among users? For the first challenge, we propose a novel ranking based matrix factorization model to aggregate explicit and implicit user feedback. For the second challenge, we extend this model to a sequential matrix factorization model to enable time-aware parametrization. Finally, we introduce a network regularization function to constrain user parameters based on social connections. To the best of our knowledge, this is the first study that investigates the collective modeling of social and temporal dynamics. Experiments on the CAMRa 2010 dataset demonstrated clear improvements over many baselines.	aggregate data;baseline (configuration management);collaborative filtering;experiment;recommender system;social network	Nathan Nan Liu;Luheng He;Min Zhao	2013	ACM TIST	10.1145/2414425.2414440	computer science;collaborative filtering;machine learning;data mining;multimedia;world wide web;recommender system	AI	-18.90436254175954	-47.34271821597814	161471
423e0f595365640b653c1195749e01394cbcd937	web-scale responsive visual search at bing		In this paper, we introduce a web-scale general visual search system deployed in Microsoft Bing. The system accommodates tens of billions of images in the index, with thousands of features for each image, and can respond in less than 200 ms. In order to overcome the challenges in relevance, latency, and scalability in such large scale of data, we employ a cascaded learning-to-rank framework based on various latest deep learning visual features, and deploy in a distributed heterogeneous computing platform. Quantitative and qualitative experiments show that our system is able to support various applications on Bing website and apps.	algorithm;central processing unit;cloud computing;data mart;deep learning;definite clause grammar;experiment;field-programmable gate array;graphics processing unit;heterogeneous computing;inverted index;learning to rank;mathematical optimization;mobile app;relevance;scalability;software deployment;test set;web search engine	Houdong Hu;Yan Wang;Linjun Yang;Pavel Komlev;Li Huang;Xi Chen;Jiapei Huang;Ye Wu;Meenaz Merchant;Arun Sacheti	2018		10.1145/3219819.3219843	machine learning;computer science;latency (engineering);visual search;symmetric multiprocessor system;artificial intelligence;deep learning;scalability;object detection;content-based image retrieval	Web+IR	-14.014606516129513	-51.565018870172956	161702
329a1d631ff04442e52d12075cabf294a7c660b2	a bio-inspired algorithm for searching relationships in social networks	social network services;graph theory;topology;cluster algorithm;optimisation;ant colony optimization;social networking services;aco;graph theory bioinspired algorithm searching relationships social networks information exchange sense of smell ant colony optimization sos aco;social network services clustering algorithms topology proposals network topology algorithm design and analysis ant colony optimization;network topology;large graphs;social network;internet;social networks;information exchange;social networking online;clustering algorithms;path search large graphs social networks aco dijkstra;proposals;algorithm design;algorithm design and analysis;dijkstra;social networking online graph theory internet optimisation;path search	Nowadays the Social Networks are experiencing a growing importance. The reason of this is that they enable the information exchange among people, meeting people in the same field of work or establishing collaborations with other research groups. In order to manage social networks and to find people inside them, they are usually represented as graphs with persons as nodes and relationships between them as edges. Once this is done, establishing contact with anyone involves searching the chain of people to reach him/her, that is, the search of the path inside the graph which joins two nodes. In this paper, a new algorithm based on nature is proposed to realize this search: SoS-ACO (Sense of Smell - Ant Colony Optimization). This algorithm improves the classical ACO algorithm when it is applied in huge graphs.	algorithm;ant colony optimization algorithms;apple sos;british informatics olympiad;information exchange;social network	Jessica Rivero-Espinosa;Dolores Cuadra;Francisco Javier Calle-Gómez;Pedro Isasi Viñuela	2011	2011 International Conference on Computational Aspects of Social Networks (CASoN)	10.1109/CASON.2011.6085919	algorithm design;computer science;artificial intelligence;graph theory;machine learning;distributed computing;algorithm;social network	Robotics	-14.821482894779706	-43.0486320008133	161943
b22af4f74746b2575cb1024ca473f9b797540b21	rank-to-engage: new listwise approaches to maximize engagement		For many internet businesses, presenting a given list of items in an order that maximizes a certain metric of interest (e.g., click-through-rate, average engagement time etc.) is crucial. We approach the aforementioned task from a learning-to-rank perspective which reveals a new problem setup. In traditional learning-to-rank literature, it is implicitly assumed that during the training data generation one has access to the best or desired order for the given list of items. In this work, we consider a problem setup where we do not observe the desired ranking. We present two novel solutions: the first solution is an extension of already existing listwise learning-to-rank technique–Listwise maximum likelihood estimation (ListMLE)–while the second one is a generic machine learning based framework that tackles the problem in its entire generality. We discuss several challenges associated with this generic framework, and propose a simple item-payoff and positional-gain model that addresses these challenges. We provide training algorithms, inference procedures, and demonstrate the effectiveness of the two approaches over traditional ListMLE on synthetic as well as on real-life setting of ranking news articles for increased dwell time.	algorithm;internet;learning to rank;machine learning;real life;sample complexity;synthetic intelligence;yet another	Swayambhoo Jain;Akshay Soni;Nikolay Laptev;Yashar Mehdad	2017	CoRR		simulation;computer science;machine learning;data mining;management science;statistics	ML	-18.38365514547662	-48.082316271264034	162005
4a2292d08673fc66112808d29f2859a730be0f11	advances in knowledge discovery and data mining		Information diffusion, which addresses the issue of how a piece of information spreads and reaches individuals in or between networks, has attracted considerable research attention due to its widespread applications, such as viral marketing and rumor control. However, the process of information diffusion is complex and its underlying mechanism remains unclear. An important reason is that social influence takes many forms and each form may be determined by various factors. One of the major challenges is how to capture all the crucial factors of a social network such as users’ interests (which can be represented as topics), users’ attributes (which can be summarized as roles), and users’ reposting behaviors in a unified manner to model the information diffusion process. To address the problem, we propose the joint information diffusion model (TRM) that integrates user topical interest extraction, role recognition, and information diffusion modeling into a unified framework. TRM seamlessly unifies the user topic role extraction, role recognition, and modeling of information diffusion, and then translates the calculations of individual level influence to the role-topic pairwise influence, which can provide a coarse-grained diffusion representation. Extensive experiments on two real-world datasets validate the effectiveness of our approach under various evaluation indices, which performs superior than the state-of-the-art models by a large margin.	data mining;experiment;federal enterprise architecture;social network;unified framework	Dinh Phung;Vincent S. Tseng;Geoffrey I. Webb;Bao Ho;Mohadeseh Ganji;Lida Rashidi	2018		10.1007/978-3-319-93037-4	deductive reasoning;knowledge extraction;data science;semantic query;data mining;cluster analysis;inductive logic programming;expert system;association rule learning;bayesian network;computer science	AI	-18.950811595961465	-45.75552592120346	162236
6b4be758cb6878da0fcbe671f84d01bdbeabea97	topic scientific community in science: a combined perspective of scientific collaboration and topics	scientific community;scientific collaboration;research topic;network;author topic model;05c82;68u15	Scientific communities are clusters of researchers and play important roles in modern science. Studying different forms of scientific communities that either physically or virtually exist is a feasible way to disclose underlying mechanisms of science. From the perspective of complex networks, topology-based communities and topic-based communities reflect scientific collaboration and topical features of science respectively. However, the two features are not isolated but intertwined in scientific practice. This study proposes an approach to detect Topical Scientific Communities (TSCs) with both topology and topic features by applying machine learning techniques and network theory. As an example, the TSCs of the informetrics field are detected, and then the characteristics of these TSCs are analyzed. It is shown that collaboration patterns on the topic level can be revealed by analyzing the static network structure and dynamics of TSCs. Furthermore, cross-topic collaborations at multiple levels could be investigated through TSCs. In addition, TSCs can effectively organize researchers in terms of productivity. Future work will further explore and generalize characteristics of TSCs, and the applications of TSCs to other tasks of studying science.	complex network;informetrics;machine learning;network theory	Jin Mao;Yujie Cao;Kun Lu;Gang Li	2017	Scientometrics	10.1007/s11192-017-2418-7	data mining;complex network;network theory;computer science;informetrics;knowledge management	HPC	-16.613587464003707	-41.13314962484759	162342
8845d0db5bd8946fd223b1f382448c9e35fd13a8	factorization machine based service recommendation on heterogeneous information networks		With the wide adoption of SOA (Service Oriented Architecture), a massive amount of innovative applications emerge in the Internet. One of the popular representations is mashup. It is a new application created by combining different kinds of services. There exist multiple typed objects (e.g., mashup, service, category, tag, provider and description) and relations (e.g., compose and composed by relation between mashups and services, provide and provided by relation between services and providers), which constitute a heterogeneous information network (HIN) naturally. Several approaches already exist for recommending services for users but they are limited to consider only one or two kinds of relations between mashups and services. To apply the rich semantics and enhance recommendation performance, in this paper, we propose a Factorization Machine based service Recommendation approach, called FMRec, on HIN. Specifically, we firstly apply counting-based similarities for meta paths to capture the multiple semantic meanings between mashups and services. And then, we employ matrix factorization to the similarity matrices built by different kinds of meta paths to obtain the mashup latent features and service latent features. Finally, we leverage factorization machine model with a group lasso regularization term to learn the ratings between mashups and services. Comprehensive experiments are conducted on a real-world dataset, indicating that our proposed service recommendation approach significantly improves the quality of the recommendation results compared with existing methods.	experiment;lasso;mashup (web application hybrid);service-oriented architecture;type system	Fenfang Xie;Liang Chen;Yongjian Ye;Zibin Zheng;Xiaola Lin	2018	2018 IEEE International Conference on Web Services (ICWS)	10.1109/ICWS.2018.00022	mashup;computer science;lasso (statistics);the internet;data mining;semantics;matrix (mathematics);service-oriented architecture;factorization;matrix decomposition	Web+IR	-17.205027468498628	-47.368548537459425	162349
f39aec65adba416b52733f1ebad072415c25a40f	solving the contamination minimization problem on networks for the linear threshold model	optimization problem;social network;computer viruses;information diffusion;threshold model	We address the problem of minimizing the spread of undesirable things, such as computer viruses and malicious rumors, by blocking a limited number of links in a network. This optimization problem called the contamination minimization problem is, not only yet another approach to the problem of preventing the spread of contamination by removing nodes in a network, but also a problem that is converse to the influence maximization problem of finding the most influential nodes in a social network for information di ffusion. We adapted the method which we developed for the independent cascade model, known for a model for the spread of epidemic disease, to the contamination minimization problem under the linear threshold model, a model known for the propagation of innovation which is considerably di fferent in nature. Using large real networks, we demonstrate experimentally that the proposed method significantly outperforms conventional link-removal methods.	betweenness;blocking (computing);blog;computer virus;directed graph;entropy maximization;expectation–maximization algorithm;experiment;heuristic (computer science);malware;mathematical optimization;optimization problem;social network;software propagation;threshold model;wikipedia;yet another	Masahiro Kimura;Kazumi Saito;Hiroshi Motoda	2008		10.1007/978-3-540-89197-0_94	optimization problem;mathematical optimization;multi-commodity flow problem;computer science;artificial intelligence;machine learning;threshold model;computer virus;social network	ML	-16.83512550140412	-43.53908339526455	162388
2a0d6670b77cc29dae86b56b58ea26d6169c05d7	on approximate algorithms for distance-based queries using r-trees	approximate algorithm	In modern database applications the similarity or dissimilarity of complex objects is examined by performing distance-based queries (DBQs) on data of high dimensionality. The R-tree and its variations are commonly cited multidimensional access methods that can be used for answering such queries. Although the related algorithms work well for low-dimensional data spaces, their performance degrades as the number of dimensions increases (dimensionality curse). In order to obtain acceptable response time in high-dimensional data spaces, algorithms that obtain approximate solutions can be used. Approximation techniques, like N-consider (based on the tree structure), α-allowance and ε-approximate (based on distance), or Time-consider (based on time) can be applied in branch-and-bound algorithms for DBQs in order to control the trade-off between cost and accuracy of the result. In this paper, we improve previous approximate DBQ algorithms by applying a combination of the approximation techniques in the same query algorithm (hybrid approximation scheme). We investigate the performance of these improvements for one of the most representative DBQs (the K-closest pairs query, K-CPQ) in high-dimensional data spaces, as well as the influence of the algorithmic parameters on the control of the trade-off between the response time and the accuracy of the result. The outcome of the experimental evaluation, using synthetic and real datasets, is the derivation of the outperforming DBQ approximate algorithm for large high-dimensional point datasets.	acorn archimedes;approximation algorithm;branch and bound;closest pair of points problem;curse of dimensionality;dataspaces;experiment;gamma correction;nl (complexity);polynomial-time approximation scheme;r-tree;recursion;response time (technology);synthetic intelligence;the computer journal;tree structure	Antonio Corral;Michael Vassilakopoulos	2005	Comput. J.	10.1093/comjnl/bxh060	computer science;theoretical computer science;machine learning;data mining;mathematics	DB	-5.751640944971592	-40.96493564898454	162481
0f6eedd8fa98f244c725ffef7549b43af918eae9	stroke order and stroke number free on-line chinese character recognition using attributed relational graph matching	stroke number free online chinese character recognition;search problem;graph theory;structure methods;state space tree;state space methods;handwriting recognition;personal communication networks;optical character recognition;model characters;optimal matching;a algorithm;set theory;stroke order free online chinese character recognition;character recognition personal communication networks optimal matching state space methods writing search problems costs microcomputers handwriting recognition shape;model base;shape;state space;minimum cost path;attributed relational graph matching;attributed relational graph;writing;positional information;search problems;character recognition;microcomputers;a algorithm stroke number free online chinese character recognition stroke order free online chinese character recognition attributed relational graph matching model characters model base search problem minimum cost path state space tree	A structural method for on-line recognition of Chinese characters is proposed, which is stroke order and stroke number free. Both input characters and the model characters are represented with complete attributed relational graphs (ARGs). A new optimal matching measure between two ARGs is defined. Classification of an input character can be implemented by matching its ARG against every ARG of the model base. The matching procedure is formulated as a search problem of finding the minimum cost path in a state space tree, using the A* algorithm. In order to speed up the search of the A*, besides a heuristic estimate, a novel strategy that utilizes the geometric position information of stroke segments of Chinese characters to prune the tree is employed. The efficiency of our method is demonstrated by the promising experimental results.	matching (graph theory);online and offline;optical character recognition	Jianzhuang Liu;Wai-kuen Cham;Michael Ming-Yuen Chang	1996		10.1109/ICPR.1996.546950	arithmetic;a* search algorithm;search problem;shape;computer science;state space;graph theory;machine learning;optimal matching;pattern recognition;mathematics;microcomputer;handwriting recognition;optical character recognition;writing;set theory	Vision	-7.387170859216594	-44.95263471043677	162709
87048df918c34b662bc0d28894efa430d70a9206	crowdsourced clustering: querying edges vs triangles		We consider the task of clustering items using answers from non-expert crowd workers. In such cases, the workers are often not able to label the items directly, however, it is reasonable to assume that they can compare items and judge whether they are similar or not. An important question is what queries to make, and we compare two types: random edge queries, where a pair of items is revealed, and random triangles, where a triple is. Since it is far too expensive to query all possible edges and/or triangles, we need to work with partial observations subject to a fixed query budget constraint. When a generative model for the data is available (and we consider a few of these) we determine the cost of a query by its entropy; when such models do not exist we use the average response time per query of the workers as a surrogate for the cost. In addition to theoretical justification, through several simulations and experiments on two real data sets on Amazon Mechanical Turk, we empirically demonstrate that, for a fixed budget, triangle queries uniformly outperform edge queries. Even though, in contrast to edge queries, triangle queries reveal dependent edges, they provide more reliable edges and, for a fixed budget, many more of them. We also provide a sufficient condition on the number of observations, edge densities inside and outside the clusters and the minimum cluster size required for the exact recovery of the true adjacency matrix via triangle queries using a convex optimization-based clustering algorithm.	adjacency matrix;algorithm;amazon mechanical turk;cluster analysis;convex optimization;crowdsourcing;edge enhancement;experiment;generative model;mathematical optimization;response time (technology);simulation;the turk	Ramya Korlakai Vinayak;Babak Hassibi	2016			combinatorics;discrete mathematics;machine learning;data mining;mathematics;spatial query	ML	-10.95754464024367	-41.633650812248206	162894
a31fde3d4c7102dbe14a311078662dab07d9bd23	a new approach for mobile advertising click-through rate estimation based on deep belief nets		In recent years, with the rapid development of mobile Internet and its business applications, mobile advertising Click-Through Rate (CTR) estimation has become a hot research direction in the field of computational advertising, which is used to achieve accurate advertisement delivery for the best benefits in the three-side game between media, advertisers, and audiences. Current research on the estimation of CTR mainly uses the methods and models of machine learning, such as linear model or recommendation algorithms. However, most of these methods are insufficient to extract the data features and cannot reflect the nonlinear relationship between different features. In order to solve these problems, we propose a new model based on Deep Belief Nets to predict the CTR of mobile advertising, which combines together the powerful data representation and feature extraction capability of Deep Belief Nets, with the advantage of simplicity of traditional Logistic Regression models. Based on the training dataset with the information of over 40 million mobile advertisements during a period of 10 days, our experiments show that our new model has better estimation accuracy than the classic Logistic Regression (LR) model by 5.57% and Support Vector Regression (SVR) model by 5.80%.		Jiehao Chen;Ziqian Zhao;Jiyun Shi;Chong Zhao	2017		10.1155/2017/7259762	machine learning;click-through rate;the internet;artificial intelligence;support vector machine;pattern recognition;linear model;logistic regression;feature extraction;computer science;advertising;external data representation	ML	-19.114829311513674	-50.53130973201743	163005
af12e593c6289dbb2c499d35712494014c3cebd4	visual sentiment analysis for social images using transfer learning approach	flickr;neural networks;visualization;feature extraction;sentiment analysis;twitter	Visual sentiment analysis framework can predict the sentiment of an image by analyzing the image contents. Nowadays, people are uploading millions of images in social networks such as Twitter, Facebook, Google Plus, and Flickr. These images play a crucial part in expressing emotions of users in online social networks. As a result, image sentiment analysis has become important in the area of online multimedia big data research. Several research works are focusing on analyzing the sentiment of the textual contents. However, little investigation has been done to develop models that can predict sentiment of visual content. In this paper, we propose a novel visual sentiment analysis framework using transfer learning approach to predict sentiment. We use hyper-parameters learned from a very deep convolutional neural network to initialize our network model to prevent overfitting. We conduct extensive experiments on a Twitter image dataset and prove that our model achieves better performance than the current state-of-the-art.	artificial neural network;big data;convolutional neural network;experiment;flickr;google+;network model;overfitting;sentiment analysis;social network;upload	Jyoti Islam;Yanqing Zhang	2016	2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)	10.1109/BDCloud-SocialCom-SustainCom.2016.29	visualization;feature extraction;computer science;data mining;internet privacy;world wide web;sentiment analysis	Web+IR	-18.730279925605945	-50.915814420835694	163516
fd9d329724c30c3c26536624fe64f8c7e481ba44	fast spatial co-location mining without cliqueness checking	automatic generation;spatial data mining;co location mining	In this paper, we propose a novel, spatial co-location mining algorithm which automatically generates co-located spatial features without generating any non-clique candidates at each level. Subsequently our algorithm is more efficient than other existing level-wise co-location algorithms because no cliqueness checking is performed in our algorithm. In addition, our algorithm produces a smaller number of co-location candidates than the other existing algorithms.	algorithm	Zhongshan Lin;Seungjin Lim	2008		10.1145/1458082.1458333	computer science;data science;machine learning;data mining;fsa-red algorithm	EDA	-6.600316991682482	-38.11909377901567	163679
097d6bbb830e878ebd3d2aab7bb64d20b49132f7	the promise of big data technologies and challenges for image and video analytics in healthcare	big health data;medical image and video analysis;big data;mapreduce;medical video databases;medical video processing	There is a growing need for developing fast and scalable methods for storing and processing large databases of healthcare images and videos. The paper reviews current medical image analysis techniques and the recent emergence, promise, and challenges associated with large scale video analysis methods. Furthermore, the paper describes large-scale video processing paradigms and provides a summary of recently published methods.	big data;database;emergence;image analysis;medical image computing;medical imaging;scalability;video content analysis;video processing	Andreas S. Panayides;Constantinos S. Pattichis;Marios S. Pattichis	2016	2016 50th Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2016.7869579	computer science;data science;data mining;multimedia	Vision	-14.663083113174677	-51.74690538702526	163757
2f7e9f7d9a6769bd3a287c62b4c2d4bd1f8d11de	a generalized markov graph model: application to social network analysis	social network services;social network classification;graph theory;network generation;pattern clustering;complex networks;markov graph model complex networks pattern recognition classification;social network synthesis;degree distribution statistics;social sciences;clustering coefficient distribution;classification;social network services markov processes mathematical model signal processing algorithms signal processing adaptation models physics;physics;statistical distributions;social network graph;signal processing;pattern classification;mathematical model;pattern recognition;generalized markov graph model;social network analysis;markov processes;crowding coefficient distribution;signal processing algorithms;network theory graphs;adaptation models;social network characterization;markov graph model	In this paper we propose a generalized Markov Graph model for social networks and evaluate its application in social network synthesis, and in social network classification. The model reveals that the degree distribution, the clustering coefficient distribution as well as a newly discovered feature, a crowding coefficient distribution, are fundamental to characterizing a social network. The application of this model to social network synthesis leads to a capacity to generate networks dominated by the degree distribution and the clustering coefficient distribution. Another application is a new social network classification method based on comparing the statistics of their degree distributions and clustering coefficient distributions as well as their crowding coefficient distributions. In contrast to the widely held belief that a social network graph is solely defined by its degree distribution, the novelty of this paper consists in establishing the strong dependence of social networks on the degree distribution, the clustering coefficient distribution and the crowding coefficient distribution, and in demonstrating that they form minimal information to classify social networks as well as to design a new social network synthesis tool. We provide numerous experiments with published data and demonstrate very good performance on both counts.	clustering coefficient;collaboration graph;crowding;degree distribution;experiment;markov chain;markov random field;network synthesis filters;social network analysis	Tian Wang;Hamid Krim;Yannis Viniotis	2013	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2013.2246767	probability distribution;network science;social network analysis;degree distribution;biological classification;computer science;dynamic network analysis;graph theory;scale-free network;machine learning;hierarchical network model;signal processing;pattern recognition;mathematical model;clustering coefficient;mathematics;markov process;complex network;statistics	Metrics	-14.83097198847264	-41.55040549886275	164061
5ee15839844522bdec4c2fb36818b39be2759a2f	a proposal on the error bound of collaborative filtering recommender system	generative probability;probability;classification functions;pervasive computing;storage management;information filtering;statistical test;testing;data engineering;accuracy;storage management information filtering information filters probability statistical testing;recommender system;filtering algorithms;collaborative filtering;proposals recommender systems accuracy data engineering international collaboration computer science pervasive computing filtering algorithms probability testing;prediction accuracy;collaborative filtering recommender system;memory based collaborative filtering algorithm;statistical tests;statistical testing;computer science;error bound;information filters;statistical tests collaborative filtering recommender system memory based collaborative filtering algorithm eda approach prediction accuracy generative probability classification functions;proposals;eda approach;recommender systems;error bound recommender system collaborative filtering	We predict accuracy of user's preferences by using memory-based collaborative filtering algorithm in recommender system, and then analyze the results through the EDA approach. The possibilities are presented that prediction accuracy can be evaluated before prediction process by analyzing the results. The classification functions using the generative probability of specific ratings are made, and users are classified by using the classification functions. The prediction accuracies of each classified group are analyzed through statistical tests. The method of setting the Error Bound of users who have high probabilities in low prediction accuracy will be presented.	algorithm;collaborative filtering;recommender system	Uk-Pyo Han;Gil-Mo Yang;Jae Soo Yoo;Young-Jun Chung;Hee-Choon Lee	2008	2008 International Conference on Multimedia and Ubiquitous Engineering (mue 2008)	10.1109/MUE.2008.72	statistical hypothesis testing;computer science;machine learning;data mining;information retrieval;statistics;recommender system	ML	-17.286472999650538	-50.06167214123012	164289
68f0bf26cb6e1f0722de0afa3f8e7c97b9bb1c95	temporal kernel descriptors for learning with time-sensitive patterns		Detecting temporal patterns is one of the most prevalent challenges while mining data. Often, timestamps or information about when certain instances or events occurred can provide us with critical information to recognize temporal patterns. Unfortunately, most existing techniques are not able to fully extract useful temporal information based on the time (especially at different resolutions of time). They miss out on 3 crucial factors: (i) they do not distinguish between timestamp features (which have cyclical or periodic properties) and ordinary features; (ii) they are not able to detect patterns exhibited at different resolutions of time (e.g. different patterns at the annual level, and at the monthly level); and (iii) they are not able to relate different features (e.g. multimodal features) of instances with different temporal properties (e.g. while predicting stock prices, stock fundamentals may have annual patterns, and at the same time factors like peer stock prices and global markets may exhibit daily patterns). To solve these issues, we offer a novel multiple-kernel learning view and develop Temporal Kernel Descriptors which utilize Kernel functions to comprehensively detect temporal patterns by deriving relationship of instances with the time features. We automatically learn the optimal kernel function, and hence the optimal temporal similarity between two instances. We formulate the optimization as a Multiple Kernel Learning (MKL) problem. We empirically evaluate its performance by solving the optimization using Online MKL.	kernel (operating system);kernel method;math kernel library;mathematical optimization;modal logic;multimodal interaction;multiple kernel learning;norm (social);online machine learning;quasiperiodicity;sensor;temporal logic;trusted timestamping	Doyen Sahoo;Abhishek Sharma;Steven C. H. Hoi;Peilin Zhao	2016		10.1137/1.9781611974348.61	pattern recognition	ML	-13.331896947638393	-48.35551211650245	164385
c0a49d78bfedc0fc6e22057315565ce76c7e5af3	on learning community-specific similarity metrics for cold-start link prediction		This paper studies a cold-start problem of inferring new edges between vertices with no demonstrated edges but vertex content by learning vertex-based similarity metrics. Existing metric learning methods for link prediction fail to consider communities which can be observed in real-world social networks. Because communities imply the existence of local homogeneities, learning a global similarity metric is not appropriate. In this paper, we thus learn community-specific similarity metrics by proposing a community-weighted formulation of metric learning model. To better illustrate the community-weighted formulation, we instantiate it in two models, which are community-weighted ranking (CWR) model and community-weighted probability (CWP) model. Experiments on three real-world networks show that community-specific similarity metrics are meaningful and that both models perform better than those leaning global metrics in terms of prediction accuracy.	cold start;computing with words and perceptions;experiment;social network	Linchuan Xu;Xiaokai Wei;Jiannong Cao;Philip S. Yu	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489683	machine learning;learning community;artificial neural network;pattern recognition;artificial intelligence;vertex (geometry);computer science;cold start (automotive);social network;ranking	ML	-17.562248663572657	-46.646719905244396	164463
3f7060aa21f5087cb0a83b4f6576fad5b2bf9055	catching synchronized behaviors in large networks: a graph mining approach	anomaly detection;graph mining;suspicious behavior;connectivity pattern	Given a directed graph of millions of nodes, how can we automatically spot anomalous, suspicious nodes judging only from their connectivity patterns? Suspicious graph patterns show up in many applications, from Twitter users who buy fake followers, manipulating the social network, to botnet members performing distributed denial of service attacks, disturbing the network traffic graph. We propose a fast and effective method, CatchSync, which exploits two of the tell-tale signs left in graphs by fraudsters: (a) synchronized behavior: suspicious nodes have extremely similar behavior patterns because they are often required to perform some task together (such as follow the same user); and (b) rare behavior: their connectivity patterns are very different from the majority. We introduce novel measures to quantify both concepts (“synchronicity” and “normality”) and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots. Thanks to careful design, CatchSync has the following desirable properties: (a) it is scalable to large datasets, being linear in the graph size; (b) it is parameter free; and (c) it is side-information-oblivious: it can operate using only the topology, without needing labeled data, nor timing information, and the like., while still capable of using side information if available. We applied CatchSync on three large, real datasets, 1-billion-edge Twitter social graph, 3-billion-edge, and 12-billion-edge Tencent Weibo social graphs, and several synthetic ones; CatchSync consistently outperforms existing competitors, both in detection accuracy by 36% on Twitter and 20% on Tencent Weibo, as well as in speed.	algorithm;botnet;denial-of-service attack;directed graph;effective method;emoticon;network topology;network traffic control;scalability;social graph;social network;structure mining;synchronicity;synthetic intelligence;tell-tale	Meng Jiang;Peng Cui;Alex Beutel;Christos Faloutsos;Shiqiang Yang	2016	TKDD	10.1145/2746403	anomaly detection;computer science;machine learning;data mining;world wide web	ML	-15.171904417741493	-41.549214737728626	164988
03d96efaa7e5d7e446d12e8539b37417fd70186d	on triangular versus edge representations --- towards scalable modeling of networks		In this paper, we argue for representing networks as a bag of triangular motifs, particularly for important network problems that current model-based approaches handle poorly due to computational bottlenecks incurred by using edge representations. Such approaches require both 1-edges and 0-edges (missing edges) to be provided as input, and as a consequence, approximate inference algorithms for these models usually require Ω(N) time per iteration, precluding their application to larger real-world networks. In contrast, triangular modeling requires less computation, while providing equivalent or better inference quality. A triangular motif is a vertex triple containing 2 or 3 edges, and the number of such motifs is Θ( ∑ iD 2 i ) (where Di is the degree of vertex i), which is much smaller than N 2 for low-maximum-degree networks. Using this representation, we develop a novel mixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree. For networks with high maximum degree, the triangular motifs can be naturally subsampled in a node-centric fashion, allowing for much faster inference at a small cost in accuracy. Empirically, we demonstrate that our approach, when compared to that of an edge-based model, has faster runtime and improved accuracy for mixed-membership community detection. We conclude with a large-scale demonstration on an N ≈ 280, 000-node network, which is infeasible for network models with Ω(N) inference cost.	approximation algorithm;computation;degree (graph theory);iteration;motif;network model;triangular matrix	Qirong Ho;Junming Yin;Eric P. Xing	2012			combinatorics;theoretical computer science;machine learning;mathematics	ML	-12.009711762927004	-40.40276713310113	165179
6f3a2cefaae6206d0869dfb3ebb0a6a51cef426f	bacterial biohybrid microswimmers		Over millions of years, Nature has optimized the motion of biological systems at the micro and nanoscales. Motor proteins to motile single cells have managed to overcome Brownian motion and solve several challenges that arise at low Reynolds numbers. In this review, we will briefly describe naturally motile systems and their strategies to move, starting with a general introduction that surveys a broad range of developments, followed by an overview about the physical laws and parameters that govern and limit motion at the microscale. We characterize some of the classes of biological microswimmers that have arisen in the course of evolution, as well as the hybrid structures that have been constructed based on these, ranging from Montemagno’s ATPase motor to the SpermBot. Thereafter, we maintain our focus on bacteria and their biohybrids. We introduce the inherent properties of bacteria as a natural microswimmer and explain the different principles bacteria use for their motion.We then elucidate different strategies that have been employed for the coupling of a variety of artificial microobjects to the bacterial surface, and evaluate the different effects the coupled objects have on the motion of the “biohybrid.” Concluding, we give a short overview and a realistic evaluation of proposed applications in the field.	biological system;brownian motion;reynolds-averaged navier–stokes equations;robotic sperm	Julio Bastos-Arrieta;Ainhoa Revilla-Guarinos;William E. Uspal;Juliane Simmchen	2018	Front. Robotics and AI	10.3389/frobt.2018.00097	artificial intelligence;computer science;active matter;computational physics;machine learning;brownian motion;ranging	AI	-7.010161728104507	-47.51670306760614	165219
a3854f499eafa883da197e7e1faa9c0182f21589	convexity in scientific collaboration networks		Abstract Convexity in a network (graph) has been recently defined as a property of each of its subgraphs to include all shortest paths between the nodes of that subgraph. It can be measured on the scale [0, 1] with 1 being assigned to fully convex networks. The largest convex component of a graph that emerges after the removal of the least number of edges is called a convex skeleton. It is basically a tree of cliques, which has been shown to have many interesting features. In this article the notions of convexity and convex skeletons in the context of scientific collaboration networks are discussed. More specifically, we analyze the co-authorship networks of Slovenian researchers in computer science, physics, sociology, mathematics, and economics and extract convex skeletons from them. We then compare these convex skeletons with the residual graphs (remainders) in terms of collaboration frequency distributions by various parameters such as the publication year and type, co-authors’ birth year, status, gender, discipline, etc. We also show the top-ranked scientists by four basic centrality measures as calculated on the original networks and their skeletons and conclude that convex skeletons may help detect influential scholars that are hardly identifiable in the original collaboration network. As their inherent feature, convex skeletons retain the properties of collaboration networks. These include high-level structural properties but also the fact that the same authors are highlighted by centrality measures. Moreover, the most important ties and thus the most important collaborations are retained in the skeletons.	apollonian network;centrality;clique (graph theory);computation;computer science;convex function;cycle (graph theory);embnet.journal;experiment;file spanning;graph (discrete mathematics);high- and low-level;internet backbone;locality of reference;random graph;scientometrics;shortest path problem;spanning tree;tree (data structure)	Lovro vSubelj;Dalibor Fiala;Tadej Ciglarivc;Luka Kronegger	2018	CoRR	10.1016/j.joi.2018.11.005	data mining;regular polygon;theoretical computer science;centrality;convexity;computer science;graph	ML	-17.04683338293508	-40.70318755269881	165418
c18b8a258e9ad5300ab0d22d03190dfa1d1d0cd9	a local approach for identifying clusters in networks		Graph clustering is a fundamental problem that has been extensively studied both in theory and practice. The problem has been defined in several ways in the literature and most of them have been proven to be NP-Hard. Due to their high practical relevancy, several heuristics for graph clustering have been introduced which constitute a central tool for coping with NP-completeness, and are used in applications of clustering ranging from computer vision, to data analysis, to learning. There exist many methodologies for this problem, however most of them are global in nature and are unlikely to scale well for very large networks. In this paper, we propose two scalable local approaches for identifying the clusters in any network. We further extend one of these approaches for discovering the overlapping clusters in these networks. Some experimentation results obtained for the proposed approaches are also presented.	cluster analysis;computer vision;directed graph;graph (discrete mathematics);heuristic (computer science);np-completeness;np-hardness;relevance;scalability	Sumit Singh	2012	CoRR		combinatorics;computer science;bioinformatics;machine learning;data mining	ML	-12.479089330553123	-41.074044612285306	165478
2eca0851b8ab7991739ffe7f84b930e771b14d1b	design, generation, and validation of extreme scale power-law graphs		Massive power-law graphs drive many fields: metagenomics, brain mapping, Internet-of-things, cybersecurity, and sparse machine learning. The development of novel algorithms and systems to process these data requires the design, generation, and validation of enormous graphs with exactly known properties. Such graphs accelerate the proper testing of new algorithms and systems and are a prerequisite for success on real applications. Many random graph generators currently exist that require realizing a graph in order to know its exact properties: number of vertices, number of edges, degree distribution, and number of triangles. Designing graphs using these random graph generators is a time-consuming trial-and-error process. This paper presents a novel approach that uses Kronecker products to allow the exact computation of graph properties prior to graph generation. In addition, when a real graph is desired, it can be generated quickly in memory on a parallel computer with no-interprocessor communication. To test this approach, graphs with 1012 edges are generated on a 40,000+ core supercomputer in 1 second and exactly agree with those predicted by the theory. In addition, to demonstrate the extensibility of this approach, decetta-scale graphs with up to 10^30 edges are simulated in a few minutes on a laptop.		Jeremy Kepner;Siddharth Samsi;William Arcand;David Bestor;Bill Bergeron;Timothy M E Davis;Vijay Gadepally;Michael Houle;Matthew Hubbell;Hayden Jananthan;Michael Quinn Jones;Anna Klein;Peter Michaleas;Roger Pearce;Lauren Milechin;Julie Mullen;Andrew Prout;Antonio Rosa;Geoffrey Sanders;Charles Yee	2018	2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2018.00055	distributed computing;computer science;graph property;theoretical computer science;computation;vertex (geometry);degree distribution;extensibility;random graph;bipartite graph;supercomputer	Theory	-12.149004339351954	-40.30933767405242	165634
16d6dce88b7def5b4c87593890ba58b63223847b	efficient visualization of large-scale data tables through reordering and entropy minimization	financial data processing;data visualisation;computational complexity;travelling salesman problems computational complexity data visualisation entropy financial data processing traffic engineering computing;large scale data data visualization heatmap data seriation data reordering traveling salesman problem;travelling salesman problems;traffic engineering computing;large scale data set visual exploration large scale data table visualization reordering entropy minimization heat map data distribution information theoretic approach predictive coding residual ordered data table em ordering rescaling columns traveling salesman problem tsp heuristic time complexity real world traffic analysis financial data set analysis;entropy;data visualization cities and towns entropy heating clustering algorithms principal component analysis minimization	Visualization of data tables with n examples and m columns using heat maps provides a holistic view of the original data. As there are n! ways to order rows and m! ways to order columns, and data tables are typically ordered without regard to visual inspection, heat maps of the original data tables often appear as noisy images. However, if rows and columns of a data table are ordered such that similar rows and similar columns are grouped together, a heat map may provide a deep insight into the underlying data distribution. We propose an information-theoretic approach to produce a well-ordered data table. In particular, we search for ordering that minimizes entropy of residuals of predictive coding applied on the ordered data table. This formalization leads to a novel ordering procedure, EM-ordering, that can be applied separately on rows and columns. For ordering of rows, EM-ordering repeats until convergence the steps of (1) rescaling columns and (2) solving a Traveling Salesman Problem (TSP) where rows are treated as cities. To allow fast ordering of large data tables, we propose an efficient TSP heuristic with modest O(n log(n)) time complexity. When compared to the existing state-of-the-art reordering approaches, we show that the method often provides heat maps of higher visual quality, while being significantly more scalable. Moreover, analysis of real-world traffic and financial data sets using the proposed method, which allowed us to readily gain deeper insights about the data, further confirmed that EM-ordering can be a valuable tool for visual exploration of large-scale data sets.	algorithm;column (database);dspace;data compression;data visualization;heat map;heuristic;holism;information theory;scalability;table (information);time complexity;travelling salesman problem;visual inspection	Nemanja Djuric;Slobodan Vucetic	2013	2013 IEEE 13th International Conference on Data Mining	10.1109/ICDM.2013.63	entropy;computer science;theoretical computer science;machine learning;data mining;mathematics;computational complexity theory;data visualization;statistics	DB	-5.741021719516265	-38.81698775535989	165642
261f27576a9b81ac194b3c03635753eec56dbff8	i/o-efficient techniques for computing pagerank	out of core;search engine;web pages;inverted index;search engines;large data sets;link based ranking;pagerank;graph algorithm;external memory algorithms	Over the last few years, most major search engines have integrated link-based ranking techniques in order to provide more accurate search results. One widely known approach is the Pagerank technique, which forms the basis of the Google ranking scheme, and which assigns a global importance measure to each page based on the importance of other pages pointing to it. The main advantage of the Pagerank measure is that it is independent of the query posed by a user; this means that it can be precomputed and then used to optimize the layout of the inverted index structure accordingly. However, computing the Pagerank measure requires implementing an iterative process on a massive graph corresponding to billions of web pages and hyperlinks.In this paper, we study I/O-efficient techniques to perform this iterative computation. We derive two algorithms for Pagerank based on techniques proposed for out-of-core graph algorithms, and compare them to two existing algorithms proposed by Haveliwala. We also consider the implementation of a recently proposed topic-sensitive version of Pagerank. Our experimental results show that for very large data sets, significant improvements over previous results can be achieved on machines with moderate amounts of memory. On the other hand, at most minor improvements are possible on data sets that are only moderately larger than memory, which is the case in many practical scenarios.	computation;computer memory;graph theory;hyperlink;input/output;inverted index;iteration;iterative method;out-of-core algorithm;pagerank;precomputation;web page;web search engine	Yen-Yu Chen;Qingqing Gan;Torsten Suel	2002		10.1145/584792.584882	google matrix;computer science;theoretical computer science;data mining;database;world wide web;information retrieval;link farm;search engine	DB	-9.646664074441858	-41.0044047081257	165677
c898238ab1de938ed8826e975f89e56342703e21	learning attribute-weighted voter model over social networks		We propose an opinion formation model, an extension of the voter model that incorporates the strength of each node, which is modeled as a function of the node attributes. Then, we address the problem of estimating parameter values for these attributes that appear in the function from the observed opinion formation data and solve this by maximizing the likelihood using an iterative parameter value updating algorithm, which is efficient and is guaranteed to converge. We show that the proposed algorithm can correctly learn the dependency in our experiments on four real world networks for which we used the assumed attribute dependency. We further show that the influence degree of each node based on the extended voter model is substantially different from that obtained assuming a uniform strength (a naive model for which the influence degree is known to be proportional to the node degree), and is more sensitive to the node strength than the node degree even for a moderate value of the node strength.	algorithm;attribute grammar;converge;estimation theory;experiment;functional dependency;heuristic;interaction;iterative method;social network;voter model;whole earth 'lectronic link	Yuki Yamagishi;Kazumi Saito;Kouzou Ohara;Masahiro Kimura;Hiroshi Motoda	2011			mathematical optimization;artificial intelligence;machine learning;mathematics	AI	-17.32742305438541	-44.13213207915846	165723
132f4ae2ed0ce7c74f70aebddab3f755fb9a982b	identifying susceptible agents in time varying opinion dynamics through compressive measurements		We provide a compressive-measurement based method to detect susceptible agents who may receive misinformation through their contact with ‘stubborn agents’ whose goal is to influence the opinions of agents in the network. We consider a DeGroot-type opinion dynamics model where regular agents revise their opinions by linearly combining their neighbors' opinions, but stubborn agents, while influencing others, do not change their opinions. Our proposed method hinges on estimating the temporal difference vector of network-wide opinions, computed at time instances when the stubborn agents interact. We show that this temporal difference vector has approximately the same support as the locations of the susceptible agents. Moreover, both the interaction instances and the temporal difference vector can be estimated from a small number of aggregated opinions. The performance of our method is studied both analytically and empirically. We show that the detection error decreases when the social network is better connected, or when the stubborn agents are ‘less talkative’.	social network;temporal difference learning	Hoi-To Wai;Asuman E. Ozdaglar;Anna Scaglione	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462377	compressed sensing;robustness (computer science);social network;artificial intelligence;temporal difference learning;small number;misinformation;pattern recognition;computer science	Robotics	-17.56947083483935	-43.63738284819841	165731
5a9a722ac113e9622096947e4087a9286dd22914	metrics for community analysis: a survey		Detecting and analyzing dense groups or communities from social and information networks has attracted immense attention over the last decade due to its enormous applicability in different domains. Community detection is an ill-defined problem, as the nature of the communities is not known in advance. The problem has turned even more complicated due to the fact that communities emerge in the network in various forms such as disjoint, overlapping, and hierarchical. Various heuristics have been proposed to address these challenges, depending on the application in hand. All these heuristics have been materialized in the form of new metrics, which in most cases are used as optimization functions for detecting the community structure, or provide an indication of the goodness of detected communities during evaluation. Over the last decade, a large number of such metrics have been proposed. Thus, there arises a need for an organized and detailed survey of the metrics proposed for community detection and evaluation. Here, we present a survey of the start-of-the-art metrics used for the detection and the evaluation of community structure. We also conduct experiments on synthetic and real networks to present a comparative analysis of these metrics in measuring the goodness of the underlying community structure.	acm computing surveys;algorithm;computer scientist;emergentism;experiment;ground truth;heuristic (computer science);mathematical optimization;modularity (networks);network topology;newman's lemma;qualitative comparative analysis;semantic network;sensor;synthetic intelligence;user-generated content	Tanmoy Chakraborty;Ayushi Dalmia;Animesh Mukherjee;Niloy Ganguly	2017	ACM Comput. Surv.	10.1145/3091106	data mining;management science;computer science;heuristics;disjoint sets;community structure	DB	-12.625679584607276	-41.24010992936142	165897
0ab663a2e3f6e5fc1121252eedc14fa15c68243e	time centrality in dynamic complex networks	time varying graphs;diffusion process;temporal networks	There is an ever-increasing interest in investigating dynamics in timevarying graphs (TVGs). Nevertheless, so far, the notion of centrality in TVG scenarios usually refers to metrics that assess the relative importance of nodes along the temporal evolution of the dynamic complex network. For some TVG scenarios, however, more important than identifying the central nodes under a given node centrality definition is identifying the key time instants for taking certain actions. In this paper, we thus introduce and investigate the notion of time centrality in TVGs. Analogously to node centrality, time centrality evaluates the relative importance of time instants in dynamic complex networks. In this context, we present two time centrality metrics related to diffusion processes. We evaluate the two defined metrics using a real-world dataset representing an in-person contact dynamic network. We validate the concept of time centrality showing that diffusion starting at the best classified time instants (i.e. the most central ones), according to our metrics, can perform a faster and more efficient diffusion process. 1 ar X iv :1 50 4. 00 24 1v 2 [ cs .S I] 1 5 A pr 2 01 5	centrality;complex network;time-varied gain	Eduardo Chinelate Costa;Alex Borges Vieira;Klaus Wehmuth;Artur Ziviani;Ana Paula Couto da Silva	2015	Advances in Complex Systems	10.1142/S021952591550023X	network theory;network science;random walk closeness centrality;simulation;katz centrality;artificial intelligence;alpha centrality;diffusion process;machine learning;mathematics;centrality;betweenness centrality;statistics	AI	-15.840225965674795	-41.12215451282511	166282
73c4e2f8c8e5830db1d6df1a8f37d97a0f63ecb6	theories for influencer identification in complex networks		In social and biological systems, the structural heterogeneity of interaction networks gives rise to the emergence of a small set of influential nodes, or influencers, in a series of dynamical processes. Although much smaller than the entire network, these influencers were observed to be able to shape the collective dynamics of large populations in different contexts. As such, the successful identification of influencers should have profound implications in various real-world spreading dynamics such as viral marketing, epidemic outbreaks and cascading failure. In this chapter, we first summarize the centrality-based approach in finding single influencers in complex networks, and then discuss the more complicated problem of locating multiple influencers from a collective point of view. Progress rooted in collective influence theory, belief-propagation and computer science will be presented. Finally, we present some applications of influencer identification in diverse real-world systems, including online social platforms, scientific publication, brain networks and socioeconomic systems.	belief propagation;biological system;cascading failure;centrality;complex network;computer science;emergence;interaction network;population;scientific literature;software propagation;world-system	Sen Pei;Flaviano Morone;Hernán A. Makse	2017	CoRR		cascading failure;complex network;management science;centrality;mathematics;viral marketing;small set;influencer marketing	ML	-18.04205381729247	-40.40596917481046	166506
0bf5190c8e69a5e172dcfc1e85eca0856afa5cc7	ten simple rules for creating biomolecular graphics		One need only compare the number of three-dimensional molecular illustrations in the first (1990) and third (2004) editions of Voet & Voet’s Biochemistry in order to appreciate this field’s profound communicative value in modern biological sciences – ranging from medicine, physiology, and cell biology, to pharmaceutical chemistry and drug design, to structural and computational biology. The cliché about a picture being worth a thousand words is quite poignant here: The information ‘content’ of an effectively-constructed piece of molecular graphics can be immense. Because biological function arises from structure, it is difficult to overemphasize the utility of visualization and graphics in molding our current understanding of the molecular nature of biological systems. Nevertheless, creating effective molecular graphics is not easy – neither conceptually, nor in terms of effort required. The present collection of Rules is meant as a guide for those embarking upon their first molecular illustrations; it most closely parallels the previous collections devoted to publishing papers [1], making oral presentations [2], and creating good posters [3].	biological system;computational biology;function (biology);medicinal chemistry;molecular graphics;parallels desktop for mac	Cameron Mura	2013	CoRR			Comp.	-7.5980116025637345	-48.66165743591273	166821
4be5c6bc96676492e2887172dea86c2bad0445ab	coupled sparse matrix factorization for response time prediction in logistics services		Nowadays, there is an emerging way of connecting logistics orders and van drivers, where it is crucial to predict the order response time. Accurate prediction of order response time would not only facilitate decision making on order dispatching, but also pave ways for applications such as supply-demand analysis and driver scheduling, leading to high system efficiency. In this work, we forecast order response time on current day by fusing data from order history and driver historical locations. Specifically, we propose Coupled Sparse Matrix Factorization (CSMF) to deal with the heterogeneous fusion and data sparsity challenges raised in this problem. CSMF jointly learns from multiple heterogeneous sparse data through the proposed weight setting mechanism therein. Experiments on real-world datasets demonstrate the effectiveness of our approach, compared to various baseline methods. The performances of many variants of the proposed method are also presented to show the effectiveness of each component.	baseline (configuration management);experiment;ibm notes;logistics;performance;response time (technology);responsiveness;scheduling (computing);sparse matrix	Yuqi Wang;Jiannong Cao;Lifang He;Wengen Li;Lichao Sun;Philip S. Yu	2017		10.1145/3132847.3132948	sparse matrix;incomplete lu factorization;scheduling (computing);theoretical computer science;response time;factorization;incomplete cholesky factorization;computer science;non-negative matrix factorization	AI	-13.933634131686594	-48.40638658253955	167216
816b9ca08f4ec0f41e61c6ac5ea34af573760c2a	inferring individual influence in social network	social network services;graph theory;social networking online graph theory;factor graph;social network;inferring model;computational modeling;internet;social influence rules influence factor graph edge features uniform inferring model personalized influence ability value zarchary coediting social networks wikipedia coediting social networks;social networking online;inferring model factor graph social network;inference algorithms;electronic publishing;encyclopedias;social network services computational modeling encyclopedias electronic publishing internet inference algorithms	We study the integration of individuals attributes to infer their influence ability in social network in this paper. The influence between individuals is usually asymmetric and can propagate via edges gradually. We suggest an Influence Factor Graph(IFG) which can integrate different node and edge features into a uniform inferring model. And for each node the model can compute personalized influence ability value. Experiment results in Zarchary and Wikipedia co-editing social networks show that, the model can depict influence reasonably and reveal some interesting social influence rules.	algorithm;blog network;casio loopy;experiment;factor graph;marginal model;message passing;personalization;social network;software propagation;wikipedia	Haisu Zhang;Wenyan Gan;Feng Xu	2012	2012 Ninth Web Information Systems and Applications Conference	10.1109/WISA.2012.53	network science;computer science;machine learning;data mining;world wide web	AI	-18.75751897740127	-43.18037174602827	167244
7291766e1c94e2dd35e6524c708358cc73f9dcd3	formulating membrane dynamics with the reaction of surface objects	clathrin-coated vesicular transport;new formulation;surface object;important structure;artificial chemistry;natural membrane system;membrane dynamic	Membranes form important structures in living systems. In this paper, we propose a new formulation of membrane dynamics as an extension to our artificial chemistry. It does not explicitly specify membranes to react; instead, the surface objects of membranes decide which membranes transform and how. We model the clathrin-coated vesicular transport by the formalism, and thereby show the compatibility of our approach with natural membrane systems.		Kazuto Tominaga;Tooru Watanabe;Maki Suzuki	2007		10.1007/978-3-540-74913-4_2	computational physics;living systems;vesicular transport protein;machine learning;artificial intelligence;formalism (philosophy);artificial chemistry;computer science;membrane	Vision	-5.800874135014964	-49.462034482989516	167816
4fa811c9623e493ae9345e20ba30979d1046cc8e	fast parallel path concatenation for graph extraction	graph extraction;heterogenegous graph;parallelisim;path concatenation	"""Heterogeneous graph is a popular data model to represent the real-world relations with abundant semantics. To analyze heterogeneous graphs, an important step is extracting homogeneous graphs from the heterogeneous graphs, called homogeneous graph extraction. In an extracted homogeneous graph, the relation is defined by a line pattern on the heterogeneous graph and the new attribute values of the relation are calculated by user-defined aggregate functions. The key challenges of the extraction problem are how to efficiently enumerate paths matched by the line pattern and aggregate values for each pair of vertices from the matched paths. To address above two challenges, we propose a parallel graph extraction framework, where we use vertex-centric model to enumerate paths and compute aggregate functions in parallel. The framework compiles the line pattern into a path concatenation plan, which determines the order of concatenating paths and generates the final paths in a divide-and-conquer manner. We introduce a cost model to estimate the cost of a plan and discuss three plan selection strategies, among which the best plan can enumerate paths in <inline-formula><tex-math notation=""""LaTeX"""">$\mathcal {O}(log(l))$</tex-math><alternatives> <inline-graphic xlink:href=""""huang-ieq1-2716939.gif""""/></alternatives></inline-formula> iterations, where <inline-formula> <tex-math notation=""""LaTeX"""">$l$</tex-math><alternatives><inline-graphic xlink:href=""""huang-ieq2-2716939.gif""""/> </alternatives></inline-formula> is the length of a pattern. Furthermore, to improve the performance of evaluating aggregate functions, we classify the aggregate functions into three categories, i.e., distributive aggregation, algebraic aggregation, and holistic aggregation. Since the distributive and algebraic aggregations can be computed from the partial paths, we speed up the aggregation by computing partial aggregate values during the path enumeration."""	aggregate data;aggregate function;analysis of algorithms;concatenation;data model;enumerated type;holism;iteration;linear algebra;xlink	Yingxia Shao;Kai Lei;Lei Chen;Zi Xuan Huang;Bin Cui;Zhongyi Liu;Yunhai Tong;Jin Xu	2017	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2017.2716939	discrete mathematics;data mining;vertex (geometry);concatenation;homogeneous;computer science;graph	DB	-7.535119724392014	-39.095341116084384	168026
d9c174f58dfc526d81002d1b957e0b3d446f8720	non-random mating involving inheritance of social status	non random mating;population evolution;social gene;social status;computer simulation	We propose a unified theoretical scheme to calculate genotype frequencies of polymorphic populations with biological and social inheritance. This scheme is used to simulate the effects of social non-random mating on the population's evolution in the general situation of multiple biological gene loci and allele inheritance with natural selection and mutation. We study the case of hemophilia in several diverse situations when there exists non-random mating due to inheritance of social status. This study clearly shows that interaction between the social and the biological gene has significant impact on the population's evolution.		Myung-Hoon Chung;Chul Koo Kim	2010	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2008.0152	computer simulation;biology;social status;genetics	Comp.	-4.716548931137171	-47.52501277090829	168166
3a057d66d426a46e1ea7b93009cb293e85e42c1b	structural changes in an email-based social network	structural change;social network;clustering coefficient	Different ways of detecting structural changes in email-based social networks are presented in the paper. A social network chosen for experiments was created on the basis of the Wroclaw University of Technology email server logs covering the period of 20 months. Structural parameters like degree centrality and prestige, clustering coefficients as well as betweenness and closeness centrality were computed for each of the consecutive months and their changes were analyzed. Our aim was to make an insight into dynamics of Internet-based social networks based on email service. It was found that the major changes in the structure of the network concern its local topology. Global indices like betweenness and closeness centrality remain relatively stable which also concerns the distribution of the local parameters such as degree centrality and prestige. However, the network size and local topology changes significantly which may be detected with motif analysis and visible changes in node clustering coefficients.	email;social network	Krzysztof Juszczyszyn;Katarzyna Musial	2009		10.1007/978-3-642-01665-3_5	network theory;random walk closeness centrality;computer science;katz centrality;artificial intelligence;alpha centrality;machine learning;structural change;data mining;clustering coefficient;centrality;betweenness centrality;social network	ECom	-16.805576049086866	-40.41819732043024	168342
8f8ca95c8685cbd576df7ce11eeedd903d1959b1	stdsort: efficient pre-processing for faster vector similarity join using standard deviation	similarity join pre processing;vector pre processing;all pair similarity search;length filtering;prefix filtering;vector similarity join	Vector Similarity Join is an important operation that is used in duplication detection, entity resolution and other data analysis. It is an essential operation used in many fields, therefore researched extensively. In this paper we propose an efficient data pre-processing technique called StdSort. It utilizes the fact that the dimensions of vectors have different standard deviation values. Applied to the prefix and length filtering technique, StdSort method can expedite the vector similarity join process. It requires O(n) of pre-processing time which is equal to the existing pre-processing method. Through experiments, we showed that StdSort reduces the overall time taken for similarity join operation and the number of candidates for similar pairs than existing pre-processing method.	algorithm;data mining;data pre-processing;experiment;grey goo;mapreduce;preprocessor;sigkdd;scalability;similarity search;vldb;wang tile;yang	Hyun Joon Kim;Sang-goo Lee	2015		10.1145/2701126.2701135	data mining;database;information retrieval	DB	-5.492323203976728	-38.500999656458475	168979
1c342c6704ce9639c016691049df483038a117a0	on the value of simple stoichiometry to alife simulations using ecosim	diversity;adaptability;ecosystems biological system modeling organisms evolution biology chemicals material storage earth chemistry biological materials solar heating;food web;biological system modeling;construction industry;materials;ecology;genetics;stability;simulation software;adaptation model;intelligent agents;ecosim;ecosystems;stoichiometry;intelligent agent;stoichiometry ecology stability;alife simulations;robustness;survivability;biochemistry;simple stoichiometry;intelligent agents simple stoichiometry alife simulations ecosim ecosystems food web adaptability survivability robustness diversity	Simulation software is vital to ALife research. It is known that stoichiometry is important for modeling real world ecosystems. However, most currently available ALife simulators ignore stoichiometry mechanics. Of course, not all ALife research is trying to model life as it is found on earth. In this paper, we demonstrate the value of including simple stoichiometry in ALife simulation software. Through extensive simulation results, we show that it is worthwhile even in simulators far removed from modeling real organisms on earth. Including simple stoichiometry allows agents to fill niches in a complex food web instead of all directly competing for the same resources. This increases the adaptability, survivability, robustness, and diversity of the simulated agents. Consequently the simulator is more powerful and will ultimately yield more intelligent agents.	artificial life;ecosim;ecosystem;intelligent agent;simulation software;xfig	Matthew Conforth;Yan Meng	2009	2009 IEEE Symposium on Artificial Life	10.1109/ALIFE.2009.4937705	biology;adaptability;ecosystem;simulation;stability;simulation software;computer science;artificial intelligence;stoichiometry;ecology;intelligent agent;robustness;food web	Embedded	-5.245048109789235	-47.73638453732044	168980
536aeeb8a88cb11361765b8d66f3865f331ac1bd	accelerating topic detection on web for a large-scale data set via stochastic poisson deconvolution				Jinzhong Lin;Junbiao Pang;Li Su;Yugui Liu;Qingming Huang	2019		10.1007/978-3-030-05710-7_49		ML	-10.839136354066184	-49.650559602289604	169435
cc244dbb4a3a19a3f70b8e5506a2d0f3f7bf8b3c	web prediction using online support vector machine	multi class web prediction support vector machine online learning;support vector machines;web visit sequentiality;learning model;web visit sequentiality web prediction online support vector machine svm based online learning algorithm online ls svm multiclass learning model;support vector machines internet learning artificial intelligence;online ls svm multiclass learning model;online learning;support vector machines world wide web history support vector machine classification educational institutions web server computer networks information science telecommunication traffic network servers;learning systems;internet;online support vector machine;web prediction;support vector machine;learning artificial intelligence;multi class;svm based online learning algorithm	In this paper, a SVM-based online learning algorithm is proposed and applied to the problem of Web prediction. A method to construct an online LS-SVM multi-class learning model has been presented. This method is able to capture the inherent sequentiality of Web visits and successfully predict the future accesses. The experimental results show the effective performance of our method	algorithm;black box;data mining;hit (internet);kerrison predictor;least squares;personalization;sigkdd;support vector machine;web page	Zhili Zhang;Changgeng Guo;Shu Yu;Deyu Qi;Songqian Long	2005	17th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'05)	10.1109/ICTAI.2005.128	support vector machine;computer science;online machine learning;machine learning;pattern recognition;data mining;active learning	Robotics	-17.94283390252709	-50.39368159223154	169566
468f330287828193e5b1bcd77d4b31990d126c4f	graph indexing for large networks: a neighborhood tree-based approach	graph matching;social network;graph querying;indexing;neighborhood tree	Graphs are used to model complex data objects and their relationships in the real world. Finding occurrences of graph patterns in large graphs is one of the fundamental graph analysis tools used to discover underlying characteristics from these complex networks. In this paper, we propose a new tree-based approach for improving subgraph-matching performance. First, we introduce a new graph indexing mechanism known as  N eighborhood  Tree s (NTree), which records the neighborhood relationships of each vertex in the large graph to filter negative vertices. Second, we decompose a query graph into a set of neighborhood trees and only a subset of candidate trees, which can properly recover the original query graph. In this way, the tree-at-a-time method is used to obtain the matched graphs. Third, we employ a graph query optimizer to determine the neighborhood tree selection order on the basis of the cost evaluation of tree join operations. Experiments on both real and synthetic databases demonstrate that our approach is more efficient than other state-of-the-art indexing methods.		Zhen Lin;Yijun Bei	2014	Knowl.-Based Syst.	10.1016/j.knosys.2014.08.025	spqr tree;block graph;graph power;search engine indexing;factor-critical graph;geometric graph theory;graph bandwidth;null graph;graph property;computer science;clique-width;simplex graph;machine learning;pattern recognition;voltage graph;distance-hereditary graph;graph;moral graph;butterfly graph;complement graph;tree;line graph;matching;tree decomposition;social network	DB	-8.192226310891504	-39.07533441487477	169727
77320455914c6278ece1bc60a8dff7610650f327	fringe: a new approach to the detection of overlapping communities in graphs	community detection;overlapping communities;unweighted networks;social networks;graph algorithms	Currently, there is a growing interest in identifying communities in social networks. Although there are many algorithms that suitably resolve this problem, they do not properly find overlaps among communities. This paper describes a new approach to the detection of overlapping communities based on the ideas of friendship and leadership, using a new centrality measure, called extended degree. We describe the algorithm in detail and discuss its results in comparison to CFinder, a well-known algorithm for finding overlapping communities. These results show that our proposal behaves well in networks with a clear leadership relationship, in addition it not only returns the overlapping communities detected but specifies their leaders as well.	algorithm;centrality;complex network;sensor;social network;sparse matrix;weighted network	Camilo Palazuelos;Marta E. Zorrilla	2011		10.1007/978-3-642-21931-3_49	machine learning;data mining;mathematics;distributed computing;social network	AI	-16.54972075996816	-41.27191734754867	170088
e6fdacbc76e8f85368104bf3395fb2e563fd9543	compressing big graph data: a relative node importance approach		Given a complex graph of large network, people always want to capture its desirable information of interest. In order to dig the underlying structure of the extremely large graph dataset, compression is necessary, which also helps in further communication and computation. In this paper, by invoking a newly proposed node centrality metric named relative node importance (RNI), an effective compressing scheme is presented for complex graph datasets. Besides measuring the distance and connectivity distribution of the graph structure, we firstly take k-core distribution into consideration. Compared with the existing schemes, the proposed one has lower computational complexity and fits different kinds of networks, e.g., social network, the World Wide Web (WWW) and autonomous systems (AS) network. Numerical results show that our RNI-based method outperforms other schemes and well preserving the basic features of a graph.	autonomous robot;autonomous system (internet);centrality;computation;computational complexity theory;degeneracy (graph theory);fits;numerical method;social network;www;world wide web	Jiamei Yan;Zhaoyang Zhang	2017	2017 9th International Conference on Wireless Communications and Signal Processing (WCSP)	10.1109/WCSP.2017.8171109	theoretical computer science;dig;real-time computing;computer science;centrality;harmonic analysis;computation;social network;autonomous system (internet);computational complexity theory;graph	DB	-11.591445930148259	-41.69499693087898	170136
f979c397c27a054b8da8b427edb86eef321f0208	community detection based on differential evolution using modularity density		Currently, many community detection methods are proposed in the network science field. However, most contemporary methods only employ modularity to detect communities, which may not be adequate to represent the real community structure of networks for its resolution limit problem. In order to resolve this problem, we put forward a new community detection approach based on a differential evolution algorithm (CDDEA), taking into account modularity density as an optimized function. In the CDDEA, a new tuning parameter is used to recognize different communities. The experimental results on synthetic and real-world networks show that the proposed algorithm provides an effective method in discovering community structure in complex networks.	differential evolution	Caihong Liu;Qiang Liu	2018	Information	10.3390/info9090218	machine learning;artificial intelligence;differential evolution;network science;computer science;complex network;modularity;effective method;community structure	Vision	-14.087682478227523	-42.52274693954732	170195
abeaa794b4ab7982150a5383f98cec18c4d56e3a	the social bow tie		Understanding tie strength in social networks, and the factors that influence it, have received much attention in a myriad of disciplines for decades. Several models incorporating indicators of tie strength have been proposed and used to quantify relationships in social networks, and a standard set of structural network metrics have been applied to predominantly online social media sites to predict tie strength. Here, we introduce the concept of the “social bow tie” framework, a small subgraph of the network that consists of a collection of nodes and ties that surround a tie of interest, forming a topological structure that resembles a bow tie. We also define several intuitive and interpretable metrics that quantify properties of the bow tie. We use random forests and regression models to predict categorical and continuous measures of tie strength from different properties of the bow tie, including nodal attributes. We also investigate what aspects of the bow tie are most predictive of tie strength in two distinct social networks: a collection of 75 rural villages in India and a nationwide call network of European mobile phone users. Our results indicate several of the bow tie 1 ar X iv :1 71 0. 04 17 7v 2 [ cs .S I] 1 2 O ct 2 01 7 metrics are highly predictive of tie strength, and we find the more the social circles of two individuals overlap, the stronger their tie, consistent with previous findings. However, we also find that the more tightly-knit their non-overlapping social circles, the weaker the tie. This new finding complements our current understanding of what drives the strength of ties in social networks.	complement (complexity);induced subgraph;mobile phone;random forest;social media;social network	Heather Mattie;Kenth Engø-Monsen;Rich Ling;Jukka-Pekka Onnela	2017	CoRR		computer science;artificial intelligence;machine learning;regression analysis;data mining;categorical variable;random forest;social media;social network;metrics;bow tie	Web+IR	-16.661603523764352	-40.16071345899494	170201
8148b77cb45ed6d0b4b4c116133efbaeb14337f9	a two-list framework for accurate detection of frequent items in data streams		The problem of detecting the most frequent items in large data sets and providing accurate frequency estimates for those items is becoming more and more important in a variety of domains. We propose a new two-list framework for addressing this problem, which extends the state-of-the-art Filtered Space-Saving (FSS) algorithm. An algorithm called FSSA giving an efficient array-based implementation of this framework is presented. An adaptive version of this algorithm is also presented, which adjusts the relative sizes of the two lists based on the estimated number of distinct keys in the data set. Analytical comparison with the FSS algorithm showed that FSSA has smaller expected frequency estimation errors, and experiments on both artificial and real workloads confirm this result. A theoretical analysis of space and time complexity for FSSA and its benchmark algorithms was performed. Finally, we showed that FSS2L framework can be naturally parallelized, leading to a linear decrease in the maximum frequency estimation error.		David Vengerov	2018		10.1007/978-3-319-96136-1_19	pattern recognition;computer science;artificial intelligence;machine learning;data stream mining;data set	ML	-4.575918111554719	-39.92048064629392	170473
0193162db4fcf4fc3fe59cf7bca3912e757de79e	(missing) concept discovery in heterogeneous information networks	incollection;bisoziation;inproceedings	This article proposes a new approach to extract existing (or detect missing) concepts from a loosely integrated collection of information units by means of concept graph detection. Thereby a concept graph defines a concept by a quasi bipartite sub-graph of a bigger network with the members of the concept as the first vertex partition and their shared aspects as the second vertex partition. Once the concepts have been extracted they can be used to create higher level representations of the data. Concept graphs further allow the discovery of missing concepts, which could lead to new insights by connecting seemingly unrelated information units.	algorithm;bridging (networking);complex systems;formal concept analysis;jaccard index;sensor	Tobias Kötter;Michael R. Berthold	2011		10.1007/978-3-642-31830-6_16	artificial intelligence;machine learning;data mining;mathematics	ML	-12.87623682304524	-39.66299034493849	170513
f03ace8651f73c2406845957d2ecdb2d9fe981af	a minimal model of coevolution between learning and niche construction	ecological inheritance;learning;coevolution;niche construction;scale free;artificial life	Recently, roles of ecological processes such as learning and niche construction in evolution are attracting much attention in evolutionary studies. However, various interactions among ecological processes, including these two, are still poorly understood. The purpose of this study is to clarify self-organizing properties of coevolution between learning and niche construction. We construct a minimal evolutionary model in which individuals can both perform learning and niche construction. We found that there was a clear scale-free property in the distribution of the size of environmental changes. It was also shown that these changes were caused by the increased generic variation of niche-constructing genes, followed by the increase in the phenotypic plasticity. The analyses of effects of ecological inheritance showed that it decreased the speed and size of environmental changes, while maintaining the scale-free property of the whole dynamics.	niche blogging	Naoki Hayashi;Reiji Suzuki;Takaya Arita	2014		10.4108/icst.bict.2014.257902	biology;niche construction;ecology;evolutionary biology	Theory	-4.774334783703274	-47.18765450802657	170590
fb3ef793173bb1cea3dca72772f277855a455c07	cross-media user profiling with joint textual and social user embedding		In realistic scenarios, a user profiling model (e.g., gender classification or age regression) learned from one social media might perform rather poorly when tested on another social media due to the different data distributions in the two media. In this paper, we address cross-media user profiling by bridging the knowledge between the source and target media with a uniform user embedding learning approach. In our approach, we first construct a cross-media user-word network to capture the relationship among users through the textual information and a modified cross-media user-user network to capture the relationship among users through the social information. Then, we learn user embedding by jointly learning the heterogeneous network composed of above two networks. Finally, we train a classification (or regression) model with the obtained user embeddings as input to perform user profiling. Empirical studies demonstrate the effectiveness of the proposed approach to two cross-media user profiling tasks, i.e., cross-media gender classification and cross-media age regression.	baseline (configuration management);bridging (networking);social media	Jingjing Wang;Shoushan Li;MingQi Jiang;Hanqian Wu;Guodong Zhou	2018			computer science;artificial intelligence;natural language processing;profiling (computer programming);embedding	AI	-17.500033855578057	-48.034364053253746	170593
2f8c13f1a8399af5337898bddc753d9b40e9f536	influence maximization with novelty decay in social networks	influence maximization;meetings and proceedings;novelty decay;social networks	Influence maximization problem is to find a set of seed nodes in a social network such that their influence spread is maximized under certain propagation models. A few algorithms have been proposed for solving this problem. However, they have not considered the impact of novelty decay on influence propagation, i.e., repeated exposures will have diminishing influence on users. In this paper, we consider the problem of influence maximization with novelty decay (IMND). We investigate the effect of novelty decay on influence propagation in real-life datasets and formulate the IMND problem. We further analyze the problem properties and propose an influence estimation technique. We demonstrate the performance of our algorithms on four social networks.	artificial intelligence;entropy maximization;expectation–maximization algorithm;experiment;heap feng shui;jason cong;kempe chain;moe;real life;social network;software propagation;tier 2 network;ver (command)	Shanshan Feng;Xuefeng Chen;Gao Cong;Yifeng Zeng;Yeow Meng Chee;Yanping Xiang	2014			artificial intelligence;machine learning;social network	AI	-17.014716977162266	-43.632783491136266	171116
187f3972f8e4982c7272d10ef0b5e8a82947fad2	discovering shakers from evolving entities via cascading graph inference	cascading graph;dynamic entity;shakers	In an interconnected and dynamic world, the evolution of one entity may cause a series of significant value changes for some others. For example, the currency inflation of Thailand caused the currency slump of other Asian countries, which eventually led to the financial crisis of 1997. We call such high impact entities shakers. To discover shakers, we first introduce the concept of a cascading graph to capture the causality relationships among evolving entities over some period of time, and then infer shakers from the graph. In a cascading graph, nodes represent entities and weighted links represent the causality effects. In order to find hidden shakers in such a graph, two scoring functions are proposed, each of which estimates how much the target entity can affect the values of some others. The idea is to artificially inject a significant change on the target entity, and estimate its direct and indirect influence on the others, by following an inference rule under the Markovian assumption. Both scoring functions are proven to be only dependent on the structure of a cascading graph and can be calculated in polynomial time. Experiments included three datasets in social sciences. Without directly applicable previous methods, we modified three graphical models as baselines. The two proposed scoring functions can effectively capture those high impact entities. For example, in the experiment to discover stock market shakers, the proposed models outperform the three baselines by as much as 50% in accuracy with the ground truth obtained from Yahoo!~Finance.	baseline (configuration management);causality;entity;graphical model;ground truth;polynomial;scoring functions for docking;time complexity	Xiaoxiao Shi;Wei Fan;Jianping Zhang;Philip S. Yu	2011		10.1145/2020408.2020570	artificial intelligence;machine learning;data mining	ML	-17.88056219982832	-42.82525188745425	171493
16aa7bf7659b826046d6e52f60686ce064c5e2aa	robustness, stability and efficiency of phage lambda genetic switch: dynamical structure analysis	developmental landscape;genetics;robust stability;stochastic effect;phage λ;genetic switch;robustness;structure analysis	Based on the dynamical structure theory for complex networks recently developed by one of us and on the physical-chemical models for gene regulation, developed by Shea and Ackers in the 1980's, we formulate a direct and concise mathematical framework for the genetic switch controlling phage lambda life cycles, which naturally includes the stochastic effect. The dynamical structure theory states that the dynamics of a complex network is determined by its four elementary components: The dissipation (analogous to degradation), the stochastic force, the driving force determined by a potential, and the transverse force. The potential may be interpreted as a landscape for the phage development in terms of attractive basins, saddle points, peaks and valleys. The dissipation gives rise to the adaptivity of the phage in the landscape defined by the potential: The phage always has the tendency to approach the bottom of the nearby attractive basin. The transverse force tends to keep the network on the equal-potential contour of the landscape. The stochastic fluctuation gives the phage the ability to search around the potential landscape by passing through saddle points. With molecular parameters in our model fixed primarily by the experimental data on wild-type phage and supplemented by data on one mutant, our calculated results on mutants agree quantitatively with the available experimental observations on other mutants for protein number, lysogenization frequency, and a lysis frequency in lysogen culture. The calculation reproduces the observed robustness of the phage lambda genetic switch. This is the first mathematical description that successfully represents such a wide variety of major experimental phenomena. Specifically, we find: (1) The explanation for both the stability and the efficiency of phage lambda genetic switch is the exponential dependence of saddle point crossing rate on potential barrier height, a result of the stochastic motion in a landscape; and (2) The positive feedback of cI repressor gene transcription, enhanced by the CI dimer cooperative binding, is the key to the robustness of the phage lambda genetic switch against mutations and fluctuations in kinetic parameter values.		X.-M. Zhu;L. Yin;L. Hood;Ping Ao	2004	Journal of bioinformatics and computational biology	10.1142/S0219720004000946	biology;computer science;bioinformatics;mathematics;structural analysis;stochastic;ecology;genetics;robustness	Comp.	-5.10290227436323	-47.43563260394014	171702
dd75717d33dda619a394240dfe41e5becb4f79c3	social trust prediction using rank-k matrix recovery	exact rank;trust prediction;trace norm minimization;new objective function;trace norm approximation;completion method;social trust prediction;trace norm;trust relationship;trust matrix;rank-k matrix recovery;new robust rank-k matrix	Trust prediction, which explores the unobserved relationships between online community users, is an emerging and important research topic in social network analysis and many web applications. Similar to other social-based recommender systems, trust relationships between users can be also modeled in the form of matrices. Recent study shows users generally establish friendship due to a few latent factors, it is therefore reasonable to assume the trust matrices are of low-rank. As a result, many recommendation system strategies can be applied here. In particular, trace norm minimization, which uses matrix’s trace norm to approximate its rank, is especially appealing. However, recent articles cast doubts on the validity of trace norm approximation. In this paper, instead of using trace norm minimization, we propose a new robust rank-k matrix completion method, which explicitly seeks a matrix with exact rank. Moreover, our method is robust to noise or corrupted observations. We optimize the new objective function in an alternative manner, based on a combination of ancillary variables and Augmented Lagrangian Multiplier (ALM) Method. We perform the experiments on three real-world data sets and all empirical results demonstrate the effectiveness of our method.	approximation algorithm;augmented lagrangian method;distrust;experiment;integer programming;lagrange multiplier;latent variable;loss function;low-rank approximation;online community;optimization problem;privacy;recommender system;social graph;social network analysis;sparse matrix;trix (operating system);trust metric;web application	Jin Huang;Feiping Nie;Heng Huang;Yu Lei;Chris H. Q. Ding	2013			mathematical optimization;artificial intelligence;machine learning;data mining;mathematics;statistics;low-rank approximation	AI	-18.952939316392218	-46.500958810516174	171780
2051fe5f1d4aceb8d04119ec48f0edf7954151b8	an algorithm and hardware design for very fast similarity search in high dimensional space	databases;nearest neighbor searches;random access memory;statistical distributions approximation theory query processing random access storage;artificial neural networks random access memory databases accuracy algorithm design and analysis nearest neighbor searches hardware;ram;high dimensionality;query processing;real time;fast high dimensional space nearest neighbor search hardware;high dimensional space;fast;approximation theory;accuracy;artificial neural networks;approximate similarity search;statistical distributions;ram p stable distribution random access memory query results approximate similarity search vote count based algorithm;vote count based algorithm;random access storage;hardware design;p stable distribution;nearest neighbor search;query results;stable distribution;high dimension;high performance;scientific research;hardware implementation;algorithm design and analysis;similarity search;hardware	Similarity search in very high dimensions is vital for many scientific research activities as well as real applications. A high performance, scalable, and optimal quality solution to the problem still remains challenging. We\footnote{Both authors have contributed equally to the work.} propose a vote count based algorithm using $p$-stable distribution for approximate similarity search. Approximate similarity search effectively serves purpose for many real applications. Our algorithm is efficient and scalable with both dimension and database size. We also propose a novel hardware implementation of the algorithm using simple modification to Random Access Memory (RAM). The hardware design gives real time search for millions of points at practical cost. We empirically achieve high accuracy for query results using our algorithm on 128 dimensional synthetic and real datasets.	approximation algorithm;dhrystone;display resolution;fast fourier transform;random access;random-access memory;scalability;similarity search;synthetic data	Vishwakarma Singh;Wenyu Jiang	2010	2010 IEEE International Conference on Granular Computing	10.1109/GrC.2010.114	computer science;theoretical computer science;machine learning;data mining;best-first search;nearest neighbor search;artificial neural network;statistics	DB	-5.106102937424067	-40.90313470084988	171808
5bc16be3a0c76773b85ed08169f0fb97ccca9115	an “estimate & score algorithm” for simultaneous parameter estimation and reconstruction of incomplete data on social networks	data mining and knowledge discovery;information systems and communication service	Dynamic activity involving social networks often has distinctive temporal patterns that can be exploited in situations involving incomplete information. Gang rivalry networks, in particular, display a high degree of temporal clustering of activity associated with retaliatory behavior. A recent study of a Los Angeles gang network shows that known gang activity between rivals can be modeled as a self-exciting point process on an edge of the rivalry network. In real-life situations, data is incomplete and law-enforcement agencies may not know which gang is involved. However, even when gang activity is highly stochastic, localized excitations in parts of the known dataset can help identify gangs responsible for unsolved crimes. Previous work successfully incorporated the observed clustering in time of the data to identify gangs responsible for unsolved crimes. However, the authors assumed that the parameters of the model are known, when in reality they have to be estimated from the data itself. We propose an iterative method that simultaneously estimates the parameters in the underlying point process and assigns weights to the unknown events with a directly calculable score function. The results of the estimation, weights, error propagation, convergence and runtime are presented.	algorithm;cluster analysis;computability;estimation theory;iterative method;point process;propagation of uncertainty;real life;social network;software propagation;stochastic process	Rachel A. Hegemann;Erik A. Lewis;Andrea L. Bertozzi	2012	Security Informatics	10.1186/2190-8532-2-1	simulation;computer science;artificial intelligence;machine learning;data mining;world wide web;computer security;statistics	ML	-17.777600641023188	-43.65571880729542	172154
eef3806ff07f5061ca7bfaa8dcaf85ed9ba27107	on algebraic connectivity of directed scale-free networks		Abstract In this paper, we study the algebraic connectivity of directed complex networks with scale-free property. Algebraic connectivity of a directed graph is the eigenvalue of its Laplacian matrix whose real part is the second smallest. This is known as an important measure for the diffusion speed of many diffusion processes over networks (e.g. consensus, information spreading, epidemics). We propose an algorithm, extending that of Barabasi and Albert, to generate directed scale-free networks, and show by simulations the relations between algebraic connectivity and network size, exponents of in/out-degree distributions, and minimum in/out degrees. The results are moreover compared to directed small-world networks, and demonstrated on a specific diffusion process, reaching consensus.	algebraic connectivity	O Ghafari;Kai Cai	2018	J. Franklin Institute	10.1016/j.jfranklin.2018.07.038	algebraic connectivity;mathematical optimization;discrete mathematics;eigenvalues and eigenvectors;laplacian matrix;directed graph;complex network;diffusion process;mathematics;scale-free network	Theory	-15.811810515753601	-39.97484165056538	172576
7eda07dca19883c142d0146581bd0ff0ad06eb08	modeling the dynamics of online learning activity	cs si;learning;cs lg;stat ml;machine learning;statistics;computer science	People are increasingly relying on the Web and social media to find solutions to their problems in a wide range of domains. In this online setting, closely related problems often lead to the same characteristic learning pattern, in which people sharing these problems visit related pieces of information, perform almost identical queries or, more generally, take a series of similar actions. In this paper, we introduce a novel modeling framework for clustering continuous-time grouped streaming data, the hierarchical Dirichlet Hawkes process (HDHP), which allows us to automatically uncover a wide variety of learning patterns from detailed traces of learning activity. Our model allows for efficient inference, scaling to millions of actions taken by thousands of users. Experiments on real data gathered from Stack Overflow reveal that our framework can recover meaningful learning patterns in terms of both content and temporal dynamics, as well as accurately track users’ interests and goals over time.	cluster analysis;image scaling;social media;stack overflow;streaming media;tracing (software);world wide web	Charalampos Mavroforakis;Isabel Valera;Manuel Gomez-Rodriguez	2016	CoRR		semi-supervised learning;computer science;artificial intelligence;online machine learning;machine learning;data mining;mathematics;active learning;statistics	ML	-18.354419850956816	-48.42583999198604	172622
ac8073962b525c6707c2030376abf84f1dd4c028	locust collective motion and its modeling	statistical mechanics;agent based modeling;locusts;evolutionary developmental biology;nymphs;collective animal behavior;population dynamics;deserts	Over the past decade, technological advances in experimental and animal tracking techniques have motivated a renewed theoretical interest in animal collective motion and, in particular, locust swarming. This review offers a comprehensive biological background followed by comparative analysis of recent models of locust collective motion, in particular locust marching, their settings, and underlying assumptions. We describe a wide range of recent modeling and simulation approaches, from discrete agent-based models of self-propelled particles to continuous models of integro-differential equations, aimed at describing and analyzing the fascinating phenomenon of locust collective motion. These modeling efforts have a dual role: The first views locusts as a quintessential example of animal collective motion. As such, they aim at abstraction and coarse-graining, often utilizing the tools of statistical physics. The second, which originates from a more biological perspective, views locust swarming as a scientific problem of its own exceptional merit. The main goal should, thus, be the analysis and prediction of natural swarm dynamics. We discuss the properties of swarm dynamics using the tools of statistical physics, as well as the implications for laboratory experiments and natural swarms. Finally, we stress the importance of a combined-interdisciplinary, biological-theoretical effort in successfully confronting the challenges that locusts pose at both the theoretical and practical levels.	agent-based model;collective motion;dual;experiment;qualitative comparative analysis;self-propelled particles;simulation;swarm	Gil Ariel;Amir Ayali	2015		10.1371/journal.pcbi.1004522	collective animal behavior;biology;simulation;statistical mechanics;population dynamics;nymph;ecology;genetics;evolutionary developmental biology	ML	-7.017067617436037	-47.41191402724203	172671
7a23d5a436535d2c0bba61941daf4760daf3198c	coexistence in a one-predator, two-prey system with indirect effects		The importance of indirect effects is well established in Biology (see [1–5]), for example, in the case of predation (see [6]), the predator can alter the morphology (see [7]) or the behavior of the preys. The preys, in order to reduce the possibility of contacts with the predators, could modify their normal conduct by reducing their activity or by hiding themselves for long time. There are many types of indirect effects (see [3] for a detailed discussion); another interesting case is the refuge indirect effect (see [8], e.g.); anyway, it is of great interest trying to describe the indirect interactions in population dynamics. Recently in [9] a model including indirect effects was proposed, modeling the effects of predator Daphnia over two groups of Phytoplankton of different morphology (see [10]), having Phosphorous as a resource (see [11] or [12]). A general model describing indirect effects of predation can be written in the following form:	coexist (image);galaxy morphological classification;interaction;population dynamics;prey	Renato Colucci	2013	J. Applied Mathematics	10.1155/2013/625391	mathematical optimization;extinction;indirect effect;predator;ecology;predation;mathematics	HCI	-5.488117868176838	-46.02539447804388	172978
62fd55e2d68b98759beeccf7df628aa9318e82f9	network performance rank: an approach for comparison of complex networks		Researchers have typically concentrated on analyzing what happens internally in a complex network and using this to distinguish between nodes. However, there has been less effort towards comparing between different networks. In this paper, we proposed a novel approach to rank alternative complex networks based on their performances. We consider this as a ranking problem in decision analysis based on occurring positive/negative frequent events as criteria, and using the TOPSIS method to rank alternatives. In order to assign a score to the networks for each criterion, a statistical method that estimates the expected value of positive/negative frequent events on a random node is presented. The proposed technique is efficient in terms of algorithm complexity and is capable of discriminating events occurring between important nodes over those between less significant nodes. The experiments, conducted on several synthetic networks, demonstrate the feasibility and applicability of the ranking methodology.	algorithm;complex network;decision analysis;experiment;network performance;social network;synthetic intelligence;telecommunications network;whole earth 'lectronic link	Zeynab Bahrami Bidoni;Roy George	2014	CoRR		machine learning;data mining;mathematics;statistics	Web+IR	-14.74452908063105	-40.90104275764311	173170
d3e62d84f0fef549b6ad10b77ad980f071205444	social network rebuilder: a tool to estimate a social network of financial crisis propagation	financial crisis;social simulation;data collection;degree distribution;questionnaire survey;social network;estimation;genetic algorithm	The objective of this paper is to find a new method to estimate real social networks based on observed data collected by questionnaire surveys. Studies on social networks have been increasing in order to analyze social phenomena from a micro viewpoint. Most social phenomena can be explained by micro-level interactions among people. Spread of rumor and pandemics are typical example of micro interaction? However, there has not been much work on an analysis of real social networks based on observed data. This study tries to establish a methodology that exploits a genetic algorithm to rebuild a social network based on the data observed indirectly from real social networks. This paper introduces our proposed method, which allows us to rebuild a social network to some extent from degree distributions of a target real social network.	betweenness;clustering coefficient;degree distribution;estimation theory;genetic algorithm;heuristic (computer science);interaction;mqtt;mathematical optimization;np-hardness;observed information;optimization problem;rev;social network;software propagation;software release life cycle;web application	Kohei Ichikawa;Toshihiko Takemura;Masatoshi Murakami;Kazunori Minetaki;Taiyo Maeda	2011	The Review of Socionetwork Strategies	10.1007/s12626-010-0016-8	organizational network analysis;network science;questionnaire;estimation;genetic algorithm;degree distribution;dynamic network analysis;marketing;socioeconomics;social simulation;management science;social psychology;statistics;social network;data collection	AI	-18.663640296416087	-42.74760248454736	173207
e8e994f42265eadf8a32e45ca3baaa8f0ee3db7e	harvesting on a stage-structured single population model with mature individuals in a polluted environment and pulse input of environmental toxin	population model;stage structure;critical threshold;single population;resource manager;global attractivity;life history;stage structured;permanence;point of view	In the natural world, there are many species whose individual members have a life history that they take them with two distinct stages: immaturity and maturity. In particular, we have in mind mammalian populations and some amphibious animals. We improve the assumption of a single population as a whole. It is assumed that the immature individuals and mature individuals are divided by a fixed period. This paper concentrates on the study of a stage-structured single population model with mature individuals in a polluted environment and pulse input of environmental toxin at fixed moments. Furthermore, the mature individuals are harvested continuously. We show that the population goes extinct if the harvesting rate is beyond a critical threshold. Conditions for the extended permanence of the population are also examined. From the biological point of view, it is easy to protect species by controlling the harvesting amount, impulsive period of the exogenous input of toxin and toxin impulsive input amount, etc. Our results provide reasonable tactics for biological resource management.	assumed;capability maturity model;mammals;mind;population model	Jianjun Jiao;Xiaosong Yang;Lansun Chen	2009	Bio Systems	10.1016/j.biosystems.2009.05.004	biology;population model;resource management;object permanence;ecology;life history theory	ECom	-5.2622298671545416	-46.25171592036417	173397
fd109bac25e8a43c3a0c0730927c5442eba991dd	the anatomy of the facebook social graph		We study the structure of the social graph of active Facebook users, the largest social network ever analyzed. We compute numerous features of the graph including the number of users and friendships, the degree distribution, path lengths, clustering, and mixing patterns. Our results center around three main observations. First, we characterize the global structure of the graph, determining that the social network is nearly fully connected, with 99.91% of individuals belonging to a single large connected component, and we confirm the ‘six degrees of separation’ phenomenon on a global scale. Second, by studying the average local clustering coefficient and degeneracy of graph neighborhoods, we show that while the Facebook graph as a whole is clearly sparse, the graph neighborhoods of users contain surprisingly dense structure. Third, we characterize the assortativity patterns present in the graph by studying the basic demographic and network properties of users. We observe clear degree assortativity and characterize the extent to which ‘your friends have more friends than you’. Furthermore, we observe a strong effect of age on friendship preferences as well as a globally modular community structure driven by nationality, but we do not find any strong gender homophily. We compare our results with those from smaller social networks and find mostly, but not entirely, agreement on common structural network characteristics.	assortativity;cluster analysis;clustering coefficient;connected component (graph theory);degeneracy (graph theory);degree distribution;mixing patterns;six degrees of separation;social graph;social network;sparse matrix	Johan Ugander;Brian Karrer;Lars Backstrom;Cameron Marlow	2011	CoRR		combinatorics;theoretical computer science;machine learning;clustering coefficient;mathematics;world wide web	Web+IR	-16.447061955999256	-40.30290534807249	173873
11927f158acdad85251504c158340012ef649b40	robust influence maximization	influence maximization;submodular optimization;uncertainty;robust optimization;noise	Uncertainty about models and data is ubiquitous in the computational social sciences, and it creates a need for robust social network algorithms, which can simultaneously provide guarantees across a spectrum of models and parameter settings. We begin an investigation into this broad domain by studying robust algorithms for the Influence Maximization problem, in which the goal is to identify a set of k nodes in a social network whose joint influence on the network is maximized. We define a Robust Influence Maximization framework wherein an algorithm is presented with a set of influence functions, typically derived from different influence models or different parameter settings for the same model. The different parameter settings could be derived from observed cascades on different topics, under different conditions, or at different times. The algorithm's goal is to identify a set of k nodes who are simultaneously influential for all influence functions, compared to the (function-specific) optimum solutions.  We show strong approximation hardness results for this problem unless the algorithm gets to select at least a logarithmic factor more seeds than the optimum solution. However, when enough extra seeds may be selected, we show that techniques of Krause et al. can be used to approximate the optimum robust influence to within a factor of 1-1/e. We evaluate this bicriteria approximation algorithm against natural heuristics on several real-world data sets. Our experiments indicate that the worst-case hardness does not necessarily translate into bad performance on real-world data sets; all algorithms perform fairly well.	approximation algorithm;best, worst and average case;computation;computational sociology;expectation–maximization algorithm;experiment;heuristic (computer science);social network	Xinran He;David Kempe	2016		10.1145/2939672.2939760	econometrics;mathematical optimization;robust optimization;uncertainty;noise;machine learning;mathematics;statistics	ML	-16.95002030873289	-44.27903333357864	173899
785020ed346fb5e68fcdafa7d98dd396d93173c6	large-scale matrix factorization using mapreduce	machine learning algorithms;matrix factorization;information services;data mining;scaling up;map reduce nonnegative matrix factorization;large scale;internet;clustering algorithms machine learning algorithms internet data mining information services web sites sparse matrices;machine learning;large scale matrix factorization;matrix decomposition;nonnegative matrix factorization;distributed programming;web sites;map reduce;clustering algorithms;machine learning large scale matrix factorization mapreduce nonnegative matrix factorization matrix multiplication real world datasets data mining;matrix multiplication;mapreduce;real world datasets;sparse matrices;matrix multiplication distributed programming matrix decomposition;massive data sets	Due to the popularity of nonnegative matrix factorization and the increasing availability of massive data sets, researchers are facing the problem of factorizing large-scale matrices of dimensions in the orders of millions. Recent research [11] has shown that it is feasible to factorize a million-by-million matrix with billions of nonzero elements on a MapReduce cluster. In this work, we present three different matrix multiplication implementations and scale up three types of nonnegative matrix factorizations on MapReduce. Experiments on both synthetic and real-world datasets show the excellent scalability of our proposed algorithms.	algorithm;gradient descent;image scaling;mapreduce;matrix multiplication;non-negative least squares;non-negative matrix factorization;scalability;synthetic intelligence	Zhengguo Sun;Tao Li;Naphtali Rishe	2010	2010 IEEE International Conference on Data Mining Workshops	10.1109/ICDMW.2010.155	computer science;theoretical computer science;machine learning;data mining;mathematics;matrix decomposition	DB	-9.738475063819932	-41.963325911840016	174174
39e6cc309afa74e6b7eeb108153f942ae3113bc1	ranking node importance in large-scale complex network: from a perspective of local abnormal links		with the advent of big data era, the design of ranking algorithms to enhance network survivability and structural stability on large-scale networks has gained increasing attention in the recent years. In this paper, we emphasize the role of abnormal links in evaluating the importance of nodes. The abnormal links refer to those are deduced by link prediction method that the existence probability is very low yet really exist in the network, and the similarity between nodes can be used to characterize the credibility or importance of the edge that connecting the two nodes. For a pair of nodes where no link is observed in the network, the similarity between the two can be used to predict the possibility of generating link. While for node pairs that exist links, similarity index can be used to evaluate the credibility or importance of the link in the network. By taking into account the degree value and the irrationality of the existence of nodesu0027 adjacent links, we propose a simple yet effective metric for ranking node structural importance. We evaluate the effectiveness of the proposed method on six realworld networks by static attacks and dynamic attacks, by comparing the decline rate of the relative size of the giant component of all networks, we show that the proposed method is better than other local metrics.	complex network	Yi-Run Ruan;Song-Yang Lao;Jun-De Wang;Liang Bai	2017		10.1109/BIGCOM.2017.19	survivability;computer science;artificial intelligence;data mining;machine learning;learning to rank;complex network;big data;credibility;ranking;giant component	ML	-15.583849452946513	-41.605716820325455	174332
175478a488d47e44f86e6647cf083a41ea6e9c06	structural and functional growth in self-reproducing cellular automata	self reproducers;genetics;structure and function;cellular automata	In this article, we analyze the dynamics of change in two-dimensional self-reproducers, identifying the processes that drive their evolution. We show that changes in self-reproducers structure and behavior depend on their genetic memory. This consists of distinct yet interlinked components determining their form and function. In some cases these components degrade gracefully, changing only slightly; in others the changes destroy the original structure and function of the self-reproducer. We sketch these processes at the genotype and the phenotype level—showing that they follow distinct trajectories within mutation space and quantifying the degree of change produced by different trajectories. We show that changes in structure and behavior depend on the interplay between the genotype and the phenotype. This determines universal structures, from which it is possible to construct a great number of self-reproducing systems, as we observe in biology. Creative processes of change produce divergent and/or convergent methods for the generation of self-reproducers. Divergence involves the creation of completely new information convergence involves local change and specialization of the structures concerned. © 2006 Wiley Periodicals, Inc. Complexity 11: 12–29, 2006	automata theory;cellular automaton;complex systems;fault tolerance;genetic algorithm;genetic memory (computer science);john d. wiley;mutation (genetic algorithm);partial template specialization;self-replication	Eleonora Bilotta;Pietro S. Pantano	2006	Complexity	10.1002/cplx.20131	cellular automaton;biology;computer science;artificial intelligence;mathematics;ecology;genetics	SE	-4.813269299507273	-47.362240326211946	174631
050740c91c07dc7dd58b2e3e96acce00b42dcbe0	mchits: monte carlo based method for hyperlink induced topic search on networks	influence ranking;social network;random walk;monte carlo method	Hyperlink Induced Topic Search (HITS) is the most authoritative and most widely used personalized ranking algorithm on networks. The HITS algorithm ranks nodes on networks according to power iteration, and has high complexity of computation. This paper models the HITS algorithm with the Monte Carlo method, and proposes Monte Carlo based algorithms for the HITS computation. Theoretical analysis and experiments show that the Monte Carlo based approximate computing of the HITS ranking reduces computing resources a lot while keeping higher accuracy, and is significantly better than related works.	algorithm;approximate computing;approximation algorithm;computation;context of computational complexity;experiment;hyperlink;information retrieval;monte carlo method;parallel algorithm;personalization;power iteration;web page;world wide web	Zhaoyan Jin;Dian-xi Shi;Quanyuan Wu;Hua Fan	2013	JNW	10.4304/jnw.8.10.2376-2382	quasi-monte carlo method;hybrid monte carlo;computer science;theoretical computer science;machine learning;data mining;rejection sampling;random walk;monte carlo algorithm;statistics;monte carlo method;social network	AI	-10.773571778413459	-42.570256247874816	174687
23f6e2f68dd452f6f173dfa6aa487f8263515c59	artificial chemistry and molecular darwinian evolution of dna/rna-like systems ii - programmable folding	chemical reactors;computer program;secondary structure;artificial life	Third simplified model of Darwinian evolution at the molecular level, following the two studied in our previous approach,  is studied by applying the methods of artificial chemistry. An artificial-life application is designed as a modification of  the metaphor of chemical reactor (chemostat), where the secondary structure of binary strings (RNA-like molecules) specifies  instructions for replication of binary strings. It means that a molecular phenotype is interpreted as a computer program for  an artificial-life system. Properties of such a system unambiguously demonstrate its capability to model quick evolutionary  emergence of replicators — programs that are able of quick and complete replication.  	artificial chemistry;models of dna evolution	Marian Bobrik;Vladimir Kvasnicka;Jiri Pospichal	2008		10.1007/978-3-540-75767-2_15	biology;zoology;evolutionary biology	EDA	-5.575332916577608	-48.908159736161416	174732
9b194ee4c71eb526078627bf7da9b9275b0e421b	recommendation as link prediction: a graph kernel-based machine learning approach	information retrieval;digital library;kernel methods;link prediction;interaction network;recommender system;machine learning;collaborative filtering;graph representation;kernel method;indirect interaction;heuristic algorithm	Recommender systems have demonstrated commercial success in multiple industries. In digital libraries they have the potential to be used as a support tool for traditional information retrieval functions. Among the major recommendation algorithms, the successful collaborative filtering (CF) methods explore the use of user-item interactions to infer user interests. Based on the finding that transitive user-item associations can alleviate the data sparsity problem in CF, multiple heuristic algorithms were designed to take advantage of the user-item interaction networks with both direct and indirect interactions. However, the use of such graph representation was still limited in learning-based algorithms. In this paper, we propose a graph kernel-based recommendation framework. For each user-item pair, we inspect its associative interaction graph (AIG) that contains the users, items, and interactions n steps away from the pair. We design a novel graph kernel to capture the AIG structures and use them to predict possible user-item interactions. The framework demonstrates improved performance on an online bookstore dataset, especially when a large number of suggestions are needed.	algorithm;collaborative filtering;digital library;graph (abstract data type);graph kernel;heuristic;information retrieval;interaction;library (computing);machine learning;online shopping;recommender system;sparse matrix	Xin Li;Hsinchun Chen	2009		10.1145/1555400.1555433	kernel method;digital library;computer science;machine learning;pattern recognition;data mining;graph kernel;world wide web;recommender system	AI	-17.655178060670856	-47.15204536151346	174973
536bfb016bf10f9eef66d8a9bbfba4ac8825cde3	forking the commons: developmental tensions and evolutionary patterns in open source software	divergence;specialization;software evolution;forking	Open source software (OSS) presents opportunities and challenges for developers to exploit its commons based licensing regime by creating specializations of a software technology to address plurality of goals and priorities. By ‘forking’ a new branch of development separate from the main project, development diverges into a path in order to relieve tensions related to specialization, which later encounters new tensions. In this study, we first classify forces and patterns within this divergence process. Such tensions may stem from a variety of sources including internal power conflicts, emergence of new environmental niches such as demand for specialized uses of same software, or differences along stability vs. development speed trade-off. We then present an evolutionary model which combines divergence options available to resolve tensions, and how further tensions emerge. In developing this model we attempt to define open software evolution at the level of systems of software, rather than at individual software project level.	development speed;emergence;models of dna evolution;open-source software;partial template specialization;software evolution;software project management	Mehmet Gençer;Bülent Özel	2012		10.1007/978-3-642-33442-9_27	simulation;computer science;systems engineering;software evolution;operations management;software engineering;fork;divergence	SE	-5.6673807257811815	-46.44109315667209	175129
74c0a00d16f4af6b0b4a090db29891b931fbe648	efficient algorithms for supergraph query processing on graph databases	query processing;efficient algorithm;graph database;exact algorithm;indexation;supergraph query;graph indexing	We study the problem of processing supergraph queries on graph databases. A graph database D is a large set of graphs. A supergraph query q on D is to retrieve all the graphs in D such that q is a supergraph of them. The large number of graphs in databases and the NP-completeness of subgraph isomorphism testing make it challenging to efficiently processing supergraph queries. In this paper, a new approach to processing supergraph queries is proposed. Specifically, a method for compactly organizing graph databases is first presented. Common subgraphs of the graphs in a database are stored only once in the compact organization of the database, in order to reduce the overall cost of subgraph isomorphism testings from the stored graphs to queries during query processing. Then, an exact algorithm and an approximate algorithm for generating the significant feature set with optimal order are proposed, followed by the algorithms for indices construction on graph databases. The optimal order on the feature set is to reduce the number of subgraph isomorphism testings during query processing. Based on the compact organization of graph databases, a novel algorithm for testing subgraph isomorphisms from multiple graphs to one graph is presented. Finally, based on all the above techniques, a query processing method is proposed. Analytical and experimental results show that the proposed algorithms outperform the existing similar algorithms by one to two orders of mag-	approximation algorithm;exact algorithm;graph (discrete mathematics);graph database;ibm notes;np-completeness;online and offline;organizing (structure);subgraph isomorphism problem	Shuo Zhang;Xiaofeng Gao;Weili Wu;Jianzhong Li;Hong Gao	2011	J. Comb. Optim.	10.1007/s10878-009-9221-1	block graph;split graph;factor-critical graph;cograph;universal graph;graph product;graph property;computer science;clique-width;theoretical computer science;comparability graph;data mining;subgraph isomorphism problem;graph factorization;database;mathematics;voltage graph;distance-hereditary graph;tree-depth;graph isomorphism;induced subgraph isomorphism problem;graph homomorphism;graph database;line graph	DB	-7.386421475283056	-38.98316708125929	175451
40e9b4a9fbffc0d0732137c40d255b2e0565bd4a	a unified approach to building hybrid recommender systems	content based filtering;cold start;recommender system;collaborative filtering;boltzmann machine;recommender systems;boltzmann machines	"""Content-based recommendation systems can provide recommendations for """"cold-start"""" items for which little or no training data is available, but typically have lower accuracy than collaborative filtering systems. Conversely, collaborative filtering techniques often provide accurate recommendations, but fail on cold start items. Hybrid schemes attempt to combine these different kinds of information to yield better recommendations across the board.  We describe unified Boltzmann machines, which are probabilistic models that combine collaborative and content information in a coherent manner. They encode collaborative and content information as features, and then learn weights that reflect how well each feature predicts user actions. In doing so, information of different types is automatically weighted, without the need for careful engineering of features or for post-hoc hybridization of distinct recommender systems.  We present empirical results in the movie and shopping domains showing that unified Boltzmann machines can be used to combine content and collaborative information to yield results that are competitive with collaborative techniques in recommending items that have been seen before, and also effective at recommending cold-start items."""	coherence (physics);cold start;collaborative filtering;encode;hoc (programming language);recommender system	Asela Gunawardana;Christopher Meek	2009		10.1145/1639714.1639735	boltzmann machine;cold start;computer science;collaborative filtering;machine learning;data mining;world wide web;recommender system	Web+IR	-18.75473228253406	-48.291030001139305	175836
80ca63885c239b32a48b9d53b82ae878454b7801	distributed compression of graphical data		In contrast to time series, graphical data is data indexed by the nodes and edges of a graph. Modern applications such as the internet, social networks, genomics and proteomics generate graphical data, often at large scale. The large scale argues for the need to compress such data for storage and subsequent processing. Since this data might have several components available in different locations, it is also important to study distributed compression of graphical data. In this paper, we derive a rate region for this problem which is a counterpart of the Slepian-Wolf Theorem. We characterize the rate region when the statistical description of the distributed graphical data is one of two types - a marked sparse Erdos-Renyi ensemble or a marked configuration model. Our results are in terms of a generalization of the notion of entropy introduced by Bordenave and Caputo in the study of local weak limits of sparse graphs.		Payam Delgosha;Venkat Anantharam	2018	2018 IEEE International Symposium on Information Theory (ISIT)	10.1109/ISIT.2018.8437614	fold (higher-order function);discrete mathematics;theoretical computer science;the internet;compression (physics);social network;computer science;graph	Metrics	-14.679656444913556	-38.57117900581133	175894
145f3b32138495ff8fb9bdbae9b8c820478f9692	automatic detection of feature interactions in temporal logic	temporal logic	 this paper; we give some hints on this topic in the final section. In the same section,we discuss how our current view of the system should be refined in order to arrive at thedistributed telecommunication networks that exist in practice.2 The Framework 	temporal logic	Johan Blom;Roland N. Bol;Lars Kempe	1995			feature interaction problem;feature (computer vision);temporal logic;computer vision;pattern recognition;computer science;artificial intelligence	Vision	-11.876664901173159	-52.080213862732855	176278
0f96dfc44946797f8c0a5b83af87bdde3d4a5d28	a model of scale-free proportion based on mutual anticipation	mutual anticipation;component;scale free proportion;collective behavior;swarm model	Recently, new empirical research of flocking behavior has been accumulated. Scale-free proportion has revealed how a flock can appear to behave as if it has one mind and body. The notion of scale-free proportion implies that the correlated domain within a flock is not constant size, but is proportional to flock size. Scale-free proportion can be explained by previous models, such as BOIDS based on the fixed radius neighborhood where an agent interacts with others if the critical valued parameter and a huge neighborhood are given. However, it is hard to explain under the normal neighborhood condition. The authors propose a new computational model that, although also based on BOIDS, incorporates mutual anticipation, which is implemented by modeling the resonance between the potential transitions available to each agent, allowing overlap between them. Via mutual anticipation, this model implements interactions not only among individuals but also between individuals and the field. The authors show that this model reveals the dynamic and robust structure of a flock or swarm, as well as scale-free proportion over a wide range of the flock sizes, comparing previous models, and that its predictions correlate well with empirical field data. DOI: 10.4018/jalr.2012010104 International Journal of Artificial Life Research, 3(1), 34-44, January-March 2012 35 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. & Vicsek, 2009; Vicsek, Czirok, Ben-Jacob, & Shochet, 1995). In this model, each agent interacts with their neighbors in a neighborhood with a fixed radius by averaging the directions of motion of its neighbors (i.e., the velocity matching), coupled with external random noise. The SPP model shows a phase transition with respect to the average of the entire alignment plotted against the external noise. Based on experimental studies of actual animals, it has been proposed that this transition depends on density. Couzin and others showed that, in aggregating locusts, a one-dimensional phase transition from disordered to ordered is dependent on this density and that this behavior can be mimicked by SPP (Buhl, Sumpter, Couzin, Hale, Despland, Miller, & Simpson, 2006). The model based on a neighborhood with metric distance has been regarded as a powerful tool to explain animal collective behavior in general, although discrepancies between the metric distance and empirical data have also been reported (Gregoire, Chate, & Tu, 2003; Gregoire & Chate, 2004). Use of a neighborhood with an indefinite boundary described by fuzzy logic or with a variable boundary has also been introduced as a variation on BOIDS or SPP (Bajec, Zimic, & Mraz, 2005). Ballerini and others, on the basis of their empirical studies, have shown that the interaction between individuals in a flock does not rely on metric distance, but rather on topological distance (Ballerini et al., 2008a, 2008b). Although the notion of metric distance means that a bird interacts with neighbors within a fixed distance, the notion of topological distance means that a bird interacts with a fixed number of neighbors (e.g., six or seven). In their simulation with predator’s attack, once individuals leave their metric distance neighborhood, they can no longer interact with other members and cannot return to the flock. Agents in a topological flock, however, can rejoin their group if they stray, because their interactions are based on topological distance. Although the notion of topological distance models some aspects of flocking behavior well, like metric distance, it has some problems. First, if each agent interacts with only seven neighbors, information propagation through an entire flock is very slow when size of the flock is very large. Second, it is not clear how a single bird would be able to monitor its distant flockmates. Another important discovery about animal collective behavior is scale-free proportion in a bird flock (Cavagnaa et al., 2010). The distribution of fluctuation vectors in a flock indicates that it contains a conspicuous sub-domain in which fluctuation vectors are aligned and synchronized. Here, fluctuation vector means that the difference of the velocity of the individual from the mean in a flock. It has also been shown that the scale of this sub-domain is proportional to the size of the flock. Here, we present a new theoretical model that allows for ambiguity in the definition of neighborhood in a two-dimensional lattice space. In our model, each individual has its own principal vector, and a number of potential transitions are derived from the principal vector with the maximal angle. Updating of our model is asynchronously implemented with three steps. First, Individuals move to the popular site at which potential transitions among individuals is concentrated. These transitions implement using mutual anticipation. Second, if the mutual anticipation does not provide a transition, individuals follow their antecessors. Third, If Individuals could not move at above two steps, they move freely by choosing a direction from a list of potential transitions. We show that our model can demonstrate scale-free proportion, comparing previous models. (Recently, we also showed that scale-free proportion could be explained in our flock model by a switch, dependent on a particular condition, from a topological neighborhood to metric one) (Niizato & Gunji, 2010).	artificial life;boids;computation;computational model;decibel;flock;flocking (behavior);fuzzy logic;interaction;maximal set;mind;mind–body problem;noise (electronics);quantum fluctuation;resonance;self-propelled particles;simpson's rule;simulation;software propagation;swarm;theory;velocity (software development)	Hisashi Murakami;Takayuki Niizato;Yukio-Pegio Gunji	2012	IJALR	10.4018/jalr.2012010104	artificial intelligence;collective behavior;component	AI	-15.840802044869296	-38.87445702593386	176411
c68b8bb55ca75dd23ab83f112e8f16c404eb789f	effective and efficient boundary-based clustering for three-dimensional geoinformation studies	consistent high-quality cluster;different proximity graph;high-quality volumetric cluster;input argument;inherent volumetric nature;popular proximity graph;undirected k-nearest neighbor graph;three-dimensional geoinformation studies;different size;different density;argument-free two-dimensional boundary-based clustering;k nearest neighbor;data acquisition;gis;acoustic noise;heuristic;three dimensions;data mining;graph theory;two dimensions;three dimensional;clustering algorithms;geographic information systems;encoding;merging;astronomy	Due to their inherent volumetric nature, underground andmarinegeoinformationstudiesandevenastronomydemandclusteringtechniquescapableof dealingwith threedimensionaldata. However, mostrobust and exploratory spatialclusteringapproachesfor GISonlyconsidertwodimensions. Weextendrobustargument-fr eetwo-dimensional boundary-basedclustering [8] to three dimensions. The progressionto 3D demandsmanipulationof oneargument from users and the encodingof proximity and densityinformation in different proximity graphs. Fortunately, the input argumentallows exploration of weaknesses in clusters, and detectionof regionsfor potentialmerge or split. We alsoprovidean effectiveheuristicto obtaingoodinitial valuesfor theinput argument.Thismaximizesuserfriendlinessandminimizesexplorationtime. Experimental results demonstr ate that for two popular proximity graphs (DelaunayTetrahedrizationandundirectedk-nearestneighbor graph)our approachis robustto thepresenceofnoiseandis able to detecthigh-qualityvolumetricclusters for complex situationssuch asnon-convex clusters, clusters of different densitiesand clusters of different sizes. Our experiments also showthat, undirectedk-nearestneighborgraphsproduceconsistenthigh-quality clusters with little sensitivity to valuesof k. Moreover, this variant of thealgorithmonly requiressubquadr atic time.	computer cluster;experiment;geographic information system	Ickjai Lee;Vladimir Estivill-Castro	2001			three-dimensional space;heuristic;computer science;graph theory;theoretical computer science;machine learning;data mining;database	HPC	-13.251202899698322	-38.160265538564886	176432
6c001febd55e2baf6466a3a9d661d7c72603caf8	learning theory toward genome informatics	pac learning;amino acid sequence;molecular biology;learning theory	This paper discusses some problems in Molecular Biology to which learning paradigms may be applicable. As a case, we present our recent study on knowledge discovery from amino acid sequences by PAC-learning paradigm.		Satoru Miyano	1993	IEICE Transactions	10.1007/3-540-57370-4_34	computational biology;computer science;bioinformatics;machine learning;learning theory;peptide sequence;probably approximately correct learning	Visualization	-5.980287233823392	-51.851592699400435	176613
4bbc081c5681617ea53a390a2279b014d23391c3	the heterogeneous feature selection with structural sparsity for multimedia annotation and hashing: a survey		There is a rapid growth of the amount of multimedia data from real-world multimedia sharing web sites, such as Flickr and Youtube. These data are usually of high dimensionality, high order, and large scale. Moreover, different types of media data are interrelated everywhere in a complicated and extensive way by context prior. It is well known that we can obtain lots of features from multimedia such as images and videos; those high-dimensional features often describe various aspects of characteristics in multimedia. However, the obtained features are often over-complete to describe certain semantics. Therefore, the selection of limited discriminative features for certain semantics is hence crucial to make the understanding of multimedia more interpretable. Furthermore, the effective utilization of intrinsic embedding structures in various features can boost the performance of multimedia retrieval. As a result, the appropriate representation of the latent information hidden in the related features is hence crucial during multimedia understanding. This paper introduces many of the recent efforts in sparsity-based heterogenous feature selection, the representation of the intrinsic latent structure embedded in multimedia, and the related hashing index techniques.	cryptographic hash function;embedded system;feature selection;flickr;hash function;image;sparse matrix	Fei Wu;Yahong Han;Xiang Liu;Jian Shao;Yueting Zhuang;Zhongfei Zhang	2012	International Journal of Multimedia Information Retrieval	10.1007/s13735-012-0001-9	computer science;machine learning;pattern recognition;data mining;database;multimedia;world wide web;information retrieval	DB	-15.391966402115797	-50.26027758641217	176826
a7bed2a276e43691e43cc0f0cdfbd84bc4f31ddd	generalized tag-induced cross-domain collaborative filtering		One of the most challenging problems in recommender systems based on the collaborative filtering (CF) concept is data sparseness, i.e., limited user preference data is available for making recommendations. Cross-domain collaborative filtering (CDCF) has been studied as an effective mechanism to alleviate data sparseness of one domain by transferring knowledge about user preferences from other domains. However, there are two key issues that need to be addressed to make a CDCF approach successful: (a) what common characteristics can be used to establish a link between different domains and (b) how to get each domain effectively and efficiently benefit from such a link. In this paper, we propose a novel algorithm, Generalized Tag-induced Cross-domain Collaborative Filtering (GTagCDCF), that exploits user-contributed tags as common characteristics to link different recommender domains together. Formulated from the probabilistic point of view, GTagCDCF takes into account all the user-item relations, the user-tag relations and the item-tag relations from different domains, resulting in a substantially extended collective matrix factorization framework, in which the common tags take the role of effectively transferring the knowledge between different domains. GTagCDCF is also found to be efficient, since its complexity is linear in the number of observed relations among users, items and tags from all domains. Using publicly available CF datasets to represent three cross-domain cases, i.e., two two-domain cases and one three-domain case, we experimentally demonstrate that GTagCDCF substantially outperforms several state-of-the-art single domain and cross-domain CF-based recommendation approaches. GTagCDCF is also shown to be effective for heterogeneous cross-domain cases, in which different domains are characterized by different types of user preferences. In addition, our investigation of the impact of user tagging behavior on GTagCDCF led to the conclusion that users can already benefit from GTagCDCF if they only share a few common tags. Finally, we validate the robustness of GTagCDCF with respect to the scale of datasets and the number of domains, based on a three-domain experiment.	algorithm;collaborative filtering;emoticon;experiment;neural coding;recommender system;scalability;user (computing)	Yue Shi;Martha Larson;Alan Hanjalic	2013	CoRR		collaborative filtering;machine learning;artificial intelligence;computer science	AI	-18.062644244927743	-47.59706843825423	176989
7c4005c6216eafa2b43497c15cdb0a0a1867ccb2	user interaction based community detection in online social networks		Phase 4: Hierarchical Clustering on Probability Graph  Similarity measure: puv values from 3 rd phase  Linkage Criterion: UPGMA Phase 3: Computing the Probabilities  Given an interaction graph, GI(V, E, W), and a group interaction graph, GGI(V, Eˊ, Wˊ), we construct a probability graph GP(V, EUEˊ, P), where each weight puv ε P represents probability between vertices u and v to belong in the same community.	hierarchical clustering;linkage (software);similarity measure;upgma	Himel Dev;Mohammed Eunus Ali;Tanzima Hashem	2014		10.1007/978-3-319-05813-9_20	data mining;world wide web	ML	-15.374175662632393	-44.71699247514622	177158
757ff9f08a2029005faafee660bab2c21287b095	distributed nonnegative matrix factorization for web-scale dyadic data analysis on mapreduce	matrix factorization;dyadic data;data locality;distributed computing;scaling up;data analysis;nonnegative matrix factorization;mapreduce	The Web abounds with dyadic data that keeps increasing by every single second. Previous work has repeatedly shown the usefulness of extracting the interaction structure inside dyadic data [21, 9, 8]. A commonly used tool in extracting the underlying structure is the matrix factorization, whose fame was further boosted in the Netflix challenge [26]. When we were trying to replicate the same success on real-world Web dyadic data, we were seriously challenged by the scalability of available tools. We therefore in this paper report our efforts on scaling up the nonnegative matrix factorization (NMF) technique. We show that by carefully partitioning the data and arranging the computations to maximize data locality and parallelism, factorizing a tens of millions by hundreds of millions matrix with billions of nonzero cells can be accomplished within tens of hours. This result effectively assures practitioners of the scalability of NMF on Web-scale dyadic data.	computation;dyadic transformation;image scaling;locality of reference;mapreduce;non-negative matrix factorization;parallel computing;scalability;self-replication;the matrix;world wide web	Chao Liu;Hung-chih Yang;Jinliang Fan;Li-Wei He;Yi-Min Wang	2010		10.1145/1772690.1772760	computer science;theoretical computer science;machine learning;data mining;data analysis;matrix decomposition;non-negative matrix factorization	ML	-8.267292288777153	-43.076268218408345	177315
478a4c5ea5034eedb8f2eb2ce6e62dc9d2cb9ade	how might group selection explain the major evolutionary transitions?		The central idea of group selection is that an individual will reduce its fitness so that the mean fitness of a group (of possibly non-related individuals) may increase. This is relevant to the major evolutionary transitions where an individual will cooperate by stopping reproduction on its own and reproduce instead as part of a group. Explaining the major evolutionary transitions should simply be a case of applying models of group selection. However, group selection may only work when there is a small difference between the fitness of a cooperator and a defector (Traulsen et al., 2006, Proc. Nat. Acad. Sci. U.S.A., 103, 10952). A new approach (Bryden, 2008, PhD thesis, University of Leeds) sheds fresh light on this topic however. We must carefully look at what we mean by fitness and what we mean by group. Recent perspectives on fitness (Metz et al., 1992, Trends Ecol. Evol., 7, p.198) argue that fitness should be calculated over a range of environments. This contrasts with the Hamiltonian perspective of fitness (Hamilton, 1964, J. Theor. Biol., 7, p.1) which is the number of adult offspring. By calculating over a range of environments, some traits which prosper in some environments decay in other environments. Tools are available (e.g., Tuljapurkar, 1990, Proc. Nat. Acad. Sci. U.S.A., 87, p.1139; Bryden 2008) for modelling long-run growth rates over varied environments. To apply this long-run perspective on fitness to the major evolutionary transitions, resource allocation strategy modelling has been done by Bryden (2005, ECAL, p.551; 2007, ECAL, p.645;2008). The problem of the major evolutionary transitions is reformulated as a question as to whether an individual will invest resources in a higher reproductive process: a process of generating new offspring with two or more individuals having some genetic stake in, and contributing resources to, the new offspring. When an individual reproduces clonally, it will grow faster in favourable environments than those that contribute toward a higher reproductive process. However, reproduction can be risky and leave the fast reproducing lineage dangerously low on resources during unfavourable environments. Analytic methods and computer simulations have shown how a strategy of collective reproduction (by sharing resources between several individuals and one offspring equally) can dominate a strategy of producing clonal offspring (Bryden 2007). When individuals are selfish and only contribute minimal resources, increases in the amplitude of environmental resource fluctuations becomes increasingly significant (Bryden 2008). The reason the shared strategy is successful is because the clonal strategy is very weak in harsh environments. These results demonstrate that it is plausible that an individual may lower its Hamiltonian fitness (i.e., its reproductive output) to increase its long-run fitness by contributing to a higher reproductive process (such as those in the major evolutionary transitions). If I suggest a definition of a group as a lineage that is temporally spread out across several environmental eras, rather than all being present at the same time, we may then compare the long-run fitnesses of lineages to determine the most successful. In other words, the group that has the greatest long-run fitness is selected for. This theory calls for verification through scientific experiments and expansion through further modelling of the major evolutionary transitions.	computer simulation;experiment;fitness function;hamiltonian (quantum mechanics);lineage (evolution);network address translation;temporal logic;the major transitions in evolution	John Bryden	2008				AI	-4.939279814364753	-46.48198308312659	177807
9b91f275fb5aad65a69e6a4fd105b1827e2bca0c	towards neighborhood window analytics over large-scale graphs	neighborhood aggregation;graph window;graph analytics	Information networks are often modeled as graphs, where the vertices are associated with attributes. In this paper, we study neighborhood window analytics, namely k-hop window query, that aims to capture the properties of a local community involving the k-hop neighbors (defined on the graph structures) of each vertex. We develop a novel index, Dense Block Index (DBIndex), to facilitate efficient processing of k-hop window queries. Extensive experimental studies conducted over both real and synthetic datasets with hundreds of millions of vertices and edges show that our proposed solutions are four orders of magnitude faster in query performance than the non-index algorithm, and are superior over the state-of-the-art solution in terms of both scalability	algorithm;data recovery;experiment;graph (discrete mathematics);hop-by-hop transport;scalability;synthetic intelligence;vertex (geometry);vertex (graph theory);window function	Qi Fan;Zhengkui Wang;Chee Yong Chan;Kian-Lee Tan	2016		10.1007/978-3-319-32049-6_13	computer science;theoretical computer science;machine learning;data mining;database	DB	-11.533183085921456	-41.00172500563334	177935
e5af9352d9708470161f6e6adf31d8974a9edfea	experimental studied on growing chemical organisms		The difference between biological organisms and the complex chemical systems made by humans cannot be underestimated. The former contain such a large number of physical and chemical processes, each marked by incredible spatial and temporal organization and preciseness, that as of yet, their artificial reproduction is unachievable. There remains, in addition, a gulf between biological growth and human controlled technology. These methods are not compatible. Human built complex chemical systems are assembled, whereas the biological systems are grown. Even the simplest biological cell cannot be disassembled and later reconstructed as if it were an AK-47. Few known phenomena show promise of bridging this gulf; one of these is the ‘Chemical Garden’. In such systems, chemical reactions between a few elements drive fluid flow to spontaneously form precipitation structures. These structures can be grown from a ‘seed`, and the specific structure produced closely correlates to the composition of the seed and the environment. Chemical gardens have growth trajectories that span a vast morphological space, which includes hierarchical structures and also structures that move (chemical motors). Among the first works devoted to these systems was published by Leduc in 1911 under the title “The Mechanism of Life”. Leduc recognized the similarity of chemical gardens with biological systems and believed that this similarity could teach us something about the origin of life. In his book, he wrote that “The study of synthetic biology is therefore the study of physical forces and conditions which can produce cavities surrounded by osmotic membranes... and specialized their functions of living beings.” Examples of the structures that can be grown in this manner are presented below.	biological system;bridging (networking);gulf of evaluation;humans;synthetic biology	Jerzy Maselko;James Pantaleone;Vitaliy Kaminker	2013		10.7551/978-0-262-31709-2-ch119	computational biology;machine learning;artificial intelligence;computer science	Comp.	-6.741770225440988	-47.81121133216129	178399
532a43484caef8455b81722fddb4e65b7aa6afe0	efficient processing node proximity via random walk with restart	conference paper	Graph is a useful tool to model complicated data structures. One important task in graph analysis is assessing node proximity based on graph topology. Recently, RandomWalk with Restart (RWR) tends to pop up as a promising measure of node proximity, due to its proliferative applications in e.g. recommender systems, and image segmentation. However, the best-known algorithm for computing RWR resorts to a large LU matrix factorization on an entire graph, which is cost-inhibitive. In this paper, we propose hybrid techniques to efficiently compute RWR. First, a novel divide-and-conquer paradigm is designed, aiming to convert the large LU decomposition into small triangular matrix operations recursively on several partitioned subgraphs. Then, on every subgraph, a “sparse accelerator” is devised to further reduce the time of RWR without any sacrifice in accuracy. Our experimental results on real and synthetic datasets show that our approach outperforms the baseline algorithms by at least one constant factor without loss of exactness.	algorithm;baseline (configuration management);computation;data structure;image segmentation;lu decomposition;matrix multiplication;programming paradigm;recommender system;recursion;running with rifles;sparse matrix;synthetic intelligence;time complexity;topological graph theory;triangular matrix	Bingqing Lv;Weiren Yu;Liping Wang;Julie A. McCann	2014		10.1007/978-3-319-11116-2_50	computer science;artificial intelligence;theoretical computer science;operating system;machine learning;data mining;database;distributed computing;world wide web;algorithm	DB	-10.006060059889133	-41.02632122813777	178470
1ac2d9938c60e2fea56a1b6aff4d46bcad50ee79	a hybrid agent-based and differential equations model for simulating antibiotic resistance in a hospital ward	antibacterial activity;differential equations;digital simulation;hospitals;medical computing;multi-agent systems;patient treatment;probability;agent-based simulation;antibiotic dynamics;antibiotic resistance;antibiotic resistant infections;antibiotic-resistant bacteria;differential equations model;health-care workers;hospital units;hospital ward;hybrid agent-based model;ill-chosen antibiotic treatment strategies;inconsistent disinfection procedures;interhost interactions;intrahost bacterial dynamics;nosocomial infections;patients;probabilistic model;resistant bacterial strains	Serious infections due to antibiotic-resistant bacteria are pervasive, and of particular concern within hospital units due to frequent interaction among health-care workers and patients. Such nosocomial infections are difficult to eliminate because of inconsistent disinfection procedures and frequent interactions among infected persons, and because ill-chosen antibiotic treatment strategies can lead to a growth of resistant bacterial strains. Clinical studies to address these concerns have several issues, but chief among them are the effects on the patients involved. Realistic simulation models offer an attractive alternative. This paper presents a hybrid simulation model of antibiotic resistant infections in a hospital ward, combining agent-based simulation to model the inter-host interactions of patients and health-care workers with a detailed differential equations and probabilistic model of intra-host bacterial and antibiotic dynamics. Initial results to benchmark the model demonstrate realistic behavior and suggest promising extensions to achieve a highly-complex yet accurate mechanism for testing antibiotic strategies.	agent-based model;agent-based social simulation;benchmark (computing);interaction;pervasive informatics;statistical model	Lester Caudill;Barry Lawson	2013	2013 Winter Simulations Conference (WSC)		computer science;multi-agent system;probability;mathematics;biological engineering;differential equation;statistics	AI	-7.8330264856646625	-50.24487231456519	178514
42bd99eae11d8595f95a02a94cc571bdde574469	selfish network creation with non-uniform edge cost		Network creation games investigate complex networks from a game-theoretic point of view. Based on the original model by Fabrikant et al. [PODC’03] many variants have been introduced. However, almost all versions have the drawback that edges are treated uniformly, i.e. every edge has the same cost and that this common parameter heavily influences the outcomes and the analysis of these games. We propose and analyze simple and natural parameter-free network creation games with non-uniform edge cost. Our models are inspired by social networks where the cost of forming a link is proportional to the popularity of the targeted node. Besides results on the complexity of computing a best response and on various properties of the sequential versions, we show that the most general version of our model has constant Price of Anarchy. To the best of our knowledge, this is the first proof of a constant Price of Anarchy for any network creation game.	anarchy;bilateral filter;communication endpoint;complex network;converge;game theory;linear function;locality of reference;np-hardness;network of cancer genes;social network;sparse matrix;whole earth 'lectronic link	Ankit Chauhan;Pascal Lenzner;Anna Melnichenko;Louise Molitor	2017		10.1007/978-3-319-66700-3_13	complex network;mathematical optimization;drawback;computer science	Theory	-12.536549731917018	-42.41809561568519	178891
cbbdca01aa163b54b8bf7602275c4d6447617970	urdar - an artificial ecology platform		Cross-feeding interactions are a common feature of many microbial systems, such as colonies of E. coli grown on a single limiting resource. We have studied this phenomenon in Gerlee and Lundh (2010) from an abstract point of view by considering artificial organisms which metabolise binary strings from a shared environment. The organisms are represented as simple cellular automaton rules and the analog of energy in the system is an approximation of the Shannon entropy of the binary strings. Only organisms which increase the entropy of the transformed strings are allowed to replicate. This system exhibits a large degree of species diversity, which increases when the flow of binary strings into the system is reduced. Introduction The origin of biodiversity has been a long standing problem in ecology and the evolution and maintenance of diversity was long difficult to account for, especially in the light of the proposed competitive exclusion principle which states that several species competing for the same resources cannot co-exist. Related to these issues is the question of how species diversity influences ecosystem productivity (Waide et al., 1999). Several experiments and theoretical models have been devised to resolve this issue, but many of the results have been inconclusive and even contradictory. One of the simplest ecological system where diversity emerges, and is stably maintained, is in populations of E. coli growing in a homogeneous environment limited by a single resource, usually glucose. The diversity is facilitated by cross-feeding (syntrophy), where one strain partially degrades the limiting resource into a secondary metabolite which is then utilised by a second strain. This phenomenon was first observed by Helling et al. (1987). In Gerlee and Lundh (2010), we present a more general model of the evolution of cross-feeding, which is not aimed at modelling a specific biological system, but rather extracts and models the general principles governing systems where cross-feeding might emerge. In order do this, we have devised a novel Artificial Life system, named Urdar1 in which Urdarbrunnr is one of the three wells that lie beneath the world aj ri r'i ΔEj ak r''i ΔEk aj ak P(ΔEj)	approximation;artificial life;biological system;cellular automaton;ecology;ecosystem;entropy (information theory);experiment;interaction;population;self-replicating machine;shannon (unit)	Philip Gerlee;Torbjörn Lundh	2011			artificial intelligence;computer science	AI	-5.273741092156328	-46.85207556069279	179052
7129520cead99b7d7ae540bcd9282d69ddc51aab	efficient complex social event-participant planning based on heuristic dynamic programming		To manage the Event Based Social Networks (EBSNs), an important task is to solve the Global Event Planning with Constraints (GEPC) problem, which arranges suitable social events to target users. Existing studies are not efficient enough because of the two-step framework. In this paper, we propose a more efficient method, called Heuristic-DP, which asynchronously considers all the constraints together. Using this method, we improve the computational complexity from (O(|E|^2 + |U||E|^2)) to O(|U||E|), where |U| is the number of users and |E| is the number of events in an EBSN platform. We also propose an improved heuristic strategy in one function of the heuristic-DP algorithm, which slightly increases the time cost, but can obtain a more accurate result. Finally, we verify the effectiveness and efficiency of our proposed algorithms through extensive experiments over real and synthetic datasets.	dynamic programming;heuristic	Junchang Xin;Mo Li;Wangzihao Xu;Yizhu Cai;Minhua Lu;Zhiqiong Wang	2018		10.1007/978-3-319-91458-9_16	data mining;computer science;machine learning;computational complexity theory;dynamic programming;social network;heuristic;artificial intelligence	AI	-16.507470562888386	-44.98107138460998	179256
027bac258f2306d8a6035117653c6ccd0f0cd5f2	quality and efficiency in high dimensional nearest neighbor search	locality sensitive hashing;high dimensionality;relational database;indexation;nearest neighbor;nearest neighbor search;quality control;access method;linear space	Nearest neighbor (NN) search in high dimensional space is an important problem in many applications. Ideally, a practical solution (i) should be implementable in a relational database, and (ii) its query cost should grow sub-linearly with the dataset size, regardless of the data and query distributions. Despite the bulk of NN literature, no solution fulfills both requirements, except locality sensitive hashing (LSH). The existing LSH implementations are either rigorous or adhoc. Rigorous-LSH ensures good quality of query results, but requires expensive space and query cost. Although adhoc-LSH is more efficient, it abandons quality control, i.e., the neighbor it outputs can be arbitrarily bad. As a result, currently no method is able to ensure both quality and efficiency simultaneously in practice.  Motivated by this, we propose a new access method called the locality sensitive B-tree (LSB-tree) that enables fast high-dimensional NN search with excellent quality. The combination of several LSB-trees leads to a structure called the LSB-forest that ensures the same result quality as rigorous-LSH, but reduces its space and query cost dramatically. The LSB-forest also outperforms adhoc-LSH, even though the latter has no quality guarantee. Besides its appealing theoretical properties, the LSB-tree itself also serves as an effective index that consumes linear space, and supports efficient updates. Our extensive experiments confirm that the LSB-tree is faster than (i) the state of the art of exact NN search by two orders of magnitude, and (ii) the best (linear-space) method of approximate retrieval by an order of magnitude, and at the same time, returns neighbors with much better quality.	approximation algorithm;asymptote;b-tree;content-based image retrieval;experiment;least significant bit;locality of reference;locality-sensitive hashing;nearest neighbor search;nearest-neighbor interpolation;overhead (computing);relational database;requirement;lsh	Yufei Tao;Ke Yi;Cheng Sheng;Panos Kalnis	2009		10.1145/1559845.1559905	quality control;best bin first;relational database;computer science;machine learning;data mining;database;nearest neighbor search;fixed-radius near neighbors;access method;k-nearest neighbors algorithm;locality-sensitive hashing;linear space	DB	-5.903107925367628	-41.181781588050036	179507
3ee3bc2ec4b453060ef1151fd9ebc6a6f8a19819	a cellular automata model of population infected by periodic plague	modelizacion;duracion;old age;self control;adaptability;adaptabilite;environnement hostile;autocontrol;securite;environmental conditions;genetic code;duration;adaptabilidad;autocontrole;medio ambiente hostil;modelisation;enrejado;maturite;maturity;treillis;modelo 2 dimensiones;automate cellulaire;safety;modele 2 dimensions;hostile environment;cellular automata;code genetique;seguridad;modeling;cellular automaton;codigo genetico;two dimensional model;madurez;duree;lattice;automata celular	Evolution of a population consisting of individuals, each holding a unique genetic code, is modeled on the 2D cellular automata lattice. The genetic code represents three episodes of life: the youth, the maturity and the old age. Only the mature individuals can procreate. Durations of the life-episodes are variable and are modified due to evolution. We show that the genetic codes of individuals self-adapt to environmental conditions in such a way that the entire ensemble has the greatest chance to survive. For a stable environment, the youth and the mature periods extend extremely during evolution, while the old age remains short and insignificant. The unstable environment is modeled by periodic plagues, which attacks the colony. For strong plaques the young individuals vanishes while the length of the old age period extends. We concluded that while the maturity period decides about the reproductive power of the population, the idle life-episodes set up the control mechanisms allowing for self-adaptation of the population to hostile environment. The youth accumulates reproductive resources while the old age accumulates the space required for reproduction.	capacitor plague;cellular automaton	Witold Dzwinel	2004		10.1007/978-3-540-30479-1_48	cellular automaton;adaptability;self-control;simulation;systems modeling;computer science;lattice;duration;mathematics;genetic code;maturity;algorithm	Theory	-5.221231837583309	-46.03050014690971	179532
1359ed2a1c37cee5691266a3021e2f9038da4400	epidemic spreading on complex networks with community structures		Many real-world networks display a community structure. We study two random graph models that create a network with similar community structure as a given network. One model preserves the exact community structure of the original network, while the other model only preserves the set of communities and the vertex degrees. These models show that community structure is an important determinant of the behavior of percolation processes on networks, such as information diffusion or virus spreading: the community structure can both enforce as well as inhibit diffusion processes. Our models further show that it is the mesoscopic set of communities that matters. The exact internal structures of communities barely influence the behavior of percolation processes across networks. This insensitivity is likely due to the relative denseness of the communities.	community;complex network;graph - visual representation;mesoscopic physics;percolation;random graph;vertex (graph theory)	Clara Stegehuis;Remco van der Hofstad;Johan van Leeuwaarden	2016		10.1038/srep29748	clique percolation method;ecology;community structure	ML	-16.113520709479097	-40.091243385788474	179576
2a091c4f6bd86ac1c4b8d905d67f47a2316c7b97	efficient service discovery in decentralized online social networks	silicon;swarm intelligence;manganese;big data;decentralized online social networks;service discovery;peer to peer	Online social networks (OSNs) have attracted millions of users worldwide over the last decade. In response to a series of urgent issues faced by existing OSNs, such as information overload, single-point failure, and the privacy issue, this paper introduces a self-organized decentralized OSN (SDOSN) over a social overlay resembling real-life social graph. The social overlay considers social relationship and semantic content of users and focuses on the key OSNs functionality of efficient information dissemination and service discovery. Then a swarm intelligence search method is proposed to facilitate adaptive learning and effective service discovery in decentralized environments. Our evaluation, performed in simulation over a real-world dataset, shows that the proposed approach achieves better performance comparing with the state-of-the-art methods on different network structures.	information overload;real life;reliability engineering;self-organization;service discovery;simulation;social graph;social network;swarm intelligence	Bo Yuan;Lu Liu;Nick Antonopoulos	2016	2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT)	10.1145/3006299.3006338	engineering;knowledge management;data mining;world wide web	HPC	-18.98725952620732	-44.784194001970015	179603
4aa8b87ecf684860c18dc8a528125016b2394eb3	a multi-relational association mining algorithm for screening suspected adverse drug reactions	drugs;drugs indexes algorithm design and analysis association rules safety;association mining;association rules multirelational association mining algorithm adverse drug reactions multirelational data table mining sql based algorithm database index food and drug administration fda spontaneous reporting system adr detection;association rules;indexes;sql data mining database indexing drugs food technology relational databases;safety;adverse drug reaction association mining multi relational data;adverse drug reaction;algorithm design and analysis;multi relational data	Existing association mining algorithms generally assume that the data is in a single table (relation). One approach to mining multi-relational data tables is to convert the data into a single table and then apply the existing algorithms. However, the converted table may be too large to fit into memory. Moreover, these algorithms often need structures to store large intermediate data, which further restricts them by available memory. In this study, we developed an efficient SQL-based algorithm that directly dealt with multi-relational data tables that need less allocated memory. We also investigated how database indexes and the number of connections affect the performance of such an algorithm. The proposed algorithm was tested using data from the FDA's (Food and Drug Administration) spontaneous reporting system. The data collected was used for detecting potential adverse drug reactions (ADRs) which represent a serious worldwide problem. Our experiment results indicate that the algorithm performs well and is scalable in terms of the number of association rules that are evaluated and the size of the data.	algorithm;association rule learning;database theory;lookup table;sql;scalability;sensor;spontaneous order;table (database)	Yanqing Ji;Fangyang Shen;John Tran	2014	2014 11th International Conference on Information Technology: New Generations	10.1109/ITNG.2014.96	database index;algorithm design;association rule learning;computer science;data science;data mining;database	ML	-6.6647857742280445	-44.27834558601828	179784
e7f140c4118ba155a8c8083ffa789fc1c7ed9944	large-scale robust online matching and its application in e-commerce	assignment problem;online optimization;robust optimization;two sided matching	"""This talk will be focused on large-scale matching problem that aims to find the optimal assignment of tasks to different agents under linear constraints. Large-scale matching has found numerous applications in e-commerce. An well known example is budget aware online advertisement. A common practice in online advertisement is to find, for each opportunity or user, the advertisements that fit best with his/her interests. The main shortcoming with this greedy approach is that it did not take into account the budget limits set by advertisers. Our studies, as well as others, have shown that by carefully taking into budget limits of individual advertisers, we could significantly improve the performance of the advertisement system. Despite of rich literature, two important issues are often overlooked in the previous studies of matching/assignment problem. The first issues arises from the fact that most quantities used by optimization are estimated based on historical data and therefore are likely to be inaccurate and unreliable. The second challenge is how to perform online matching as in many e-commerce problems, tasks are created in an online fashion and algorithm has to make assignment decision immediately when every task emerges. We refer to these two issues as challenges of """"robust matching"""" and """"online matching"""".  To address the first challenge, I will introduce two different techniques for robust matching. The first approach is based on the theory of robust optimization that takes into account the uncertainties of estimated quantities when performing optimization. The second approach is based on the theory of two-sided matching whose result only depends on the partial preference of estimated quantities. To deal with the challenge of online matching, I will discuss two online optimization techniques, one based on theory of primal-dual online optimization and one based on minimizing dynamic regret under long term constraints. We verify the effectiveness of all these approaches by applying them to real-world projects developed in Alibaba."""	assignment problem;e-commerce payment system;greedy algorithm;mathematical optimization;online advertising;online optimization;regret (decision theory);robust optimization	Rong Jin	2016		10.1145/2983323.2983370	mathematical optimization;robust optimization;computer science;artificial intelligence;machine learning;data mining;assignment problem	Web+IR	-17.236324654659867	-45.70490470075071	179812
865709d126235a5fd3911f541e64cf64cab1da8c	gpop: scalable group-level popularity prediction for online content in social networks	graph clustering;tensor decomposition;content prediction	Predicting the popularity of online content in social networks is important in many applications, ranging from ad campaign design, web content caching and prefetching, to websearch result ranking. Earlier studies target this problem by learning models that either generalize behaviors of the entire network population or capture behaviors of each individual user. In this paper, we claim that a novel approach based on group-level popularity is necessary and more practical, given that users naturally organize themselves into clusters and that users within a cluster react to online content in a uniform manner. We develop a novel framework by first grouping users into cohesive clusters, and then adopt tensor decomposition to make predictions. In order to minimize the impact of noisy data and be more flexible in capturing changes in users’ interests, our framework exploits both the network topology and interaction among users in learning a robust user clustering. The PARAFAC tensor decomposition is adapted to work with hierarchical constraint over user groups, and we show that optimizing this constrained function via gradient descent achieves faster convergence and leads to more stable solutions. Extensive experimental results over two social networks demonstrate that our framework is scalable, finds meaningful user groups, and significantly outperforms eight baseline methods in terms of prediction accuracy.	baseline (configuration management);cpu cache;cluster analysis;computer cluster;gradient descent;network topology;scalability;signal-to-noise ratio;social network;web cache;web content;web search engine	Minh X. Hoang;Xuan-Hong Dang;Xiang Wu;Zhenyu Yan;Ambuj K. Singh	2017		10.1145/3038912.3052626	computer science;data science;machine learning;pattern recognition;clustering coefficient;world wide web	Web+IR	-18.57803612656755	-48.04296513440334	179941
89802a0e5222b031cd5798801f75e9c0d3b72e13	signal-regulated systems and networks	robust solution;signal regulatory network;wiley periodicals;gene regulatory network;self-organizing it system;inc. complexity;exemplar srs;signal-regulated system;complex adaptive systems	The paper presents the use of signal regulatory networks, a biologically-inspired model based on gene regulatory networks. Signal regulatory networks are a way of understanding a class of self-organising IT systems, signal regulated systems. The paper builds on the theory of signal regulated systems and introduces some formalisms to clarify the discussion. An exemplar signal regulated system that can be evaluated using signal regulatory networks is presented. Finally an implementation of an adaptive and robust solution, built on a theory of signal regulated systems and analysed as a signal regulatory network, is shown to be plausible.	action potential;complex systems;computer science;emergence;gene regulatory network;mathematical optimization;multi-core processor;parallel computing;scalability;self-organization;software engineering;software system;theory	Terence L. van Zyl;Elizabeth Marie Ehlers	2010	Complexity	10.1002/cplx.20314	complex adaptive system;gene regulatory network;computer science;bioinformatics;artificial intelligence;genetics	Embedded	-5.995218200602056	-50.144668834185055	180031
3ac92446f28e202938ba8f364f959217af648290	adaptive and parallel data acquisition from online big graphs		Acquisition of contents from online big graphs (OBGs) like linked Web pages, social networks and knowledge graphs, is critical as data infrastructure for Web applications and massive data analysis. However, effective data acquisition is challenging due to the massive, heterogeneous, dynamically evolving properties of OBGs with unknown global topological structures. In this paper, we give an adaptive and parallel approach for effective data acquisition from OBGs. We adopt the ideas of Quasi Monte Carlo (QMC) and branch u0026 bound methods to propose an adaptive Web-scale sampling algorithm for parallel data collection implemented upon Spark. Experimental results show the effectiveness and efficiency of our method.	data acquisition	Zidu Yin;Kun Yue;Hao Wu;Yingjie Su	2018		10.1007/978-3-319-91452-7_21	web page;data mining;data collection;computer science;web application;sampling (statistics);spark (mathematics);theoretical computer science;quasi-monte carlo method;social network;data acquisition	ML	-11.570425217196922	-38.79905791209375	180194
bfdeff85f164d0bf4b7e78dd2247d25984062220	a o(log n) signature-based string matching algorithm	complexity theory;text processing;probability density function;construction industry;string matching problem;data mining;suffix array;arrays;indexes;number searching problem;space complexity signature based string matching algorithm string matching problem number searching problem suffix array technique;signature based string matching algorithm;string searching;computational complexity;network address translation information technology text processing;space complexity;algorithms;suffix array technique;search problems;string matching;text processing algorithms string matching string searching;algorithm design and analysis;string matching computational complexity search problems	This paper presents a new algorithm for the string matching problem. The new technique is based on converting the string matching problem into a number searching problem. The algorithm is investigated and compared to the suffix array technique. Experiments show faster search and preprocessing time with comparable space complexity.	dspace;experiment;preprocessor;string searching algorithm;suffix array	Samer Nofal	2009	2009 Sixth International Conference on Information Technology: New Generations	10.1109/ITNG.2009.59	database index;algorithm design;probability density function;approximate string matching;commentz-walter algorithm;computer science;3-dimensional matching;theoretical computer science;machine learning;boyer–moore string search algorithm;compressed suffix array;dspace;computational complexity theory;string metric;bitap algorithm;rabin–karp algorithm;string searching algorithm	Robotics	-6.724960364599883	-40.02684517601845	180282
9877a45d9c81a24f7173f40ea62c58815f783b8f	characterizing and mining the citation graph of the computer science literature	graph theory;strongly connected component;citation graph;loi puissance;teoria grafo;small worlds;descomposicion grafo;red www;ley poder;digital library;reseau web;information space;data mining;small world;theorie graphe;degree distribution;graph connectivity;biblioteca electronica;internet;fouille donnee;world wide web;electronic library;information system;networked information spaces;power law;article;busca dato;systeme information;bibliotheque electronique;graph decomposition;minimum cut;decomposition graphe;sistema informacion	Citation graphs representing a body of scientific literature convey measures of scholarly activity and productivity. In this work we present a study of the structure of the citation graph of the computer science literature. Using a web robot we built several topic-specific citation graphs and their union graph from the digital library ResearchIndex. After verifying that the degree distributions follow a power law, we applied a series of graph theoretical algorithms to elicit an aggregate picture of the citation graph in terms of its connectivity. We discovered the existence of a single large weakly-connected and a single large biconnected component, and confirmed the expected lack of a large strongly-connected component. The large components remained even after removing the strongest authority nodes or the strongest hub nodes, indicating that such tight connectivity is widespread and does not depend on a small subset of important nodes. Finally, minimum cuts between authority papers of different areas did not result in a balanced partitioning of the graph into areas, pointing to the need for more sophisticated algorithms for clustering the graph.	aggregate data;algorithm;biconnected component;citation graph;citeseerx;cluster analysis;computer science;connected component (graph theory);digital library;scientific literature;strongly connected component;usb hub;verification and validation	Yuan An;Jeannette C. M. Janssen;Evangelos E. Milios	2003	Knowledge and Information Systems	10.1007/s10115-003-0128-3	spqr tree;power law;digital library;the internet;degree distribution;minimum cut;null graph;computer science;artificial intelligence;connectivity;graph theory;theoretical computer science;machine learning;data mining;database;mathematics;graph;critical graph;complement graph;strongly connected component;information system	AI	-9.26068285932045	-44.250612953307154	180505
d832fb2b7a72640844e1eef439c2092b35e40f60	k-core decomposition of large networks on a single pc		Studying the topology of a network is critical to inferring underlying dynamics such as tolerance to failure, group behavior and spreading patterns. k-core decomposition is a well-established metric which partitions a graph into layers from external to more central vertices. In this paper we aim to explore whether k-core decomposition of large networks can be computed using a consumer-grade PC. We feature implementations of the “vertex-centric” distributed protocol introduced by Montresor, De Pellegrini and Miorandi on GraphChi and Webgraph. Also, we present an accurate implementation of the Batagelj and Zaversnik algorithm for k-core decomposition in Webgraph. With our implementations, we show that we can efficiently handle networks of billions of edges using a single consumer-level machine within reasonable time and can produce excellent approximations in only a fraction of the execution time. To the best of our knowledge, our biggest graphs are considerably larger than the graphs considered in the literature. Next, we present an optimized implementation of an external-memory algorithm (EMcore) by Cheng, Ke, Chu, and Özsu. We show that this algorithm also performs well for large datasets, however, it cannot predict whether a given memory budget is sufficient for a new dataset. We present a thorough analysis of all algorithms concluding that it is viable to compute k-core decomposition for large networks in a consumer-grade PC.	approximation;degeneracy (graph theory);out-of-core algorithm;run time (program lifecycle phase);solar cell;webgraph	Wissam Khaouid;Marina Barsky;Srinivas Karthik Venkatesh;Alex Thomo	2015	PVLDB	10.14778/2850469.2850471	computer science;theoretical computer science;data mining;database;mathematics;algorithm;statistics	DB	-12.13698185147738	-40.57739421863714	180931
226c8d6b72cdf901cf6f613196f74ed1b0b36cf0	diffusion-aware sampling and estimation in information diffusion networks	estimation theory;information diffusion networks;diffusion aware sampling;sampling;internet;social networks;social networking online;data handling	Partially-observed data collected by sampling methods is often being studied to obtain the characteristics of information diffusion networks. However, these methods usually do not consider the behavior of diffusion process. In this paper, we propose a novel two-step (sampling/estimation) measurement framework by utilizing the diffusion process characteristics. To this end, we propose a link-tracing based sampling design which uses the infection times as local information without any knowledge about the latent structure of diffusion network. To correct the bias of sampled data, we introduce three estimators for different categories, link-based, node-based, and cascade-based. To the best of our knowledge, this is the first attempt to introduce a complete measurement framework for diffusion networks. We also show that the estimator plays an important role in correcting the bias of sampling from diffusion networks. Our comprehensive empirical analysis over large synthetic and real datasets demonstrates that in average, the proposed framework outperforms the common BFS and RW sampling methods in terms of link-based characteristics by about 37% and 35%, respectively.	algorithm;experiment;read-write memory;routh–hurwitz stability criterion;sampling (signal processing);sociological theory of diffusion;synthetic intelligence;vii;whole earth 'lectronic link	Motahareh Eslami Mehdiabadi;Hamid R. Rabiee;Mostafa Salehi	2012	2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing	10.1109/SocialCom-PASSAT.2012.98	sampling;econometrics;the internet;computer science;group method of data handling;data mining;mathematics;sociology;estimation theory;statistics;social network	ML	-18.27539876747818	-43.55826464657695	181009
f84ef5f58e1ab37056714924f06ee1bc183dc4da	using triangle inequality to efficiently process continuous queries on high-dimensional streaming time series	databases;nearest neighbor searches;histograms;high dimensional query point;high dimensionality;query processing;application software;high dimensional streaming time series;continuous query;triangle inequality;filters;query processing tree data structures tree searching;hyperspectral sensors;time series;tree data structures;image sensors;consecutive query point;nearest neighbor;kd tree;nearest neighbor searches databases sensor arrays hyperspectral sensors partitioning algorithms application software filters hyperspectral imaging histograms image sensors;pattern filtering;tree searching;hyperspectral imaging;sensor arrays;pattern filtering triangle inequality continuous query nearest neighbor high dimensional query point consecutive query point query processing kd tree high dimensional streaming time series;partitioning algorithms	In many applications, it is important to quickly find, from a database of patterns, the nearest neighbors of highdimensional query points that come into the system in a streaming form. Treating each query point as a separate one is inefficient. Consecutive query points are often neigh bors in the high-dimensional space, and intermediate results in the processing of one query should help the processing of the next. This paper extends the KD tree with triangle inequality to deal with high-dimensional streaming time se ries. More specifically, the distances calculated for earli er query points (to patterns) are used to filter out patterns tha t are not possible to be the nearest neighbor of the current one. Experiments show that this extension works well.	experiment;social inequality;time series	Zhengrong Yao;Like Gao;Xiaoyang Sean Wang	2003		10.1109/SSDM.2003.1214985	online aggregation;sargable;query optimization;boolean conjunctive query;computer science;hyperspectral imaging;machine learning;pattern recognition;data mining;database;range query;statistics;spatial query	DB	-5.1481505808454555	-40.35958667674152	181103
2f3edcd93771db8acf74fc2498b86e194a8ba535	the hopfield model and its role in the development of synthetic biology.	hopfield model;biology computing;neurobiology;biological system modeling synthetic biology physics biological systems mathematical model context modeling biological neural networks genetics neural networks hopfield neural networks;synthetic biology;hopfield neural nets;biological systems hopfield model synthetic biology neural network models neurobiology;mathematical model;biological systems;scientific communication;neural network models;hopfield neural nets biology computing;neural network model	Neural network models make extensive use of concepts coming from physics and engineering. How do scientists justify the use of these concepts in the representation of biological systems? How is evidence for or against the use of these concepts produced in the application and manipulation of the models? It will be shown in this article that neural network models are evaluated differently depending on the scientific context and its modeling practice. In the case of the Hopfield model, the different modeling practices related to theoretical physics and neurobiology played a central role for how the model was received and used in the different scientific communities. In theoretical physics, where the Hopfield model has its roots, mathematical modeling is much more common and established than in neurobiology which is strongly experiment driven. These differences in modeling practice contributed to the development of the new field of synthetic biology which introduced a third type of model which combines mathematical modeling and experimenting on biological systems and by doing so mediates between the different modeling practices.	autonomous robot;biological system;experiment;functional derivative;gene regulatory network;hopfield network;materiality (digital text);mathematical model;synthetic biology;synthetic intelligence	Andrea Loettgers	2007	2007 International Joint Conference on Neural Networks	10.1109/IJCNN.2007.4371175	computer science;artificial intelligence;machine learning;mathematical model;synthetic biology;artificial neural network	AI	-6.371446691905747	-47.343456079989096	181322
43883bacf6b890b2037a0e25352559c71081b735	mining frequent trajectory pattern based on vague space partition	prefixspan algorithm;gsp algorithm;data mining;journal;algorithms;frequent trajectory pattern;vague space partition	Frequent trajectory pattern mining is an important spatiotemporal data mining problem with broad applications. However, it is also a difficult problem due to the approximate nature of spatial trajectory locations. Most of the previously developed frequent trajectory pattern mining methods explore a crisp space partition approach [8,10] to alleviate the spatial approximation concern. However, this approach may cause the sharp boundary problem that spatially close trajectory locations may fall into different partitioned regions, and eventually result in failure of finding meaningful trajectory patterns. In this paper, we propose a flexible vague space partition approach to solve the sharp boundary problem. In this approach, the spatial plane is divided into a set of vague grid cells, and trajectory locations are transformed into neighboring vague grid cells by a distance-based membership function. Based on two classical sequential mining algorithms, the PrefixSpan and GSP algorithms, we propose two efficient trajectory pattern mining algorithms, called VTPM-PrefixSpan and VTPM-GSP, to mine the transformed trajectory sequences with time interval constraints. A comprehensive performance study on both synthetic and real datasets shows that the VTPM-PrefixSpan algorithm outperforms the VTPM-GSP algorithm in both effectiveness and scalability.	vagueness	Liang Wang;Kunyuan Hu;Tao Ku;Xiaohui Yan	2013	Knowl.-Based Syst.	10.1016/j.knosys.2013.06.002	mathematical optimization;gsp algorithm;computer science;machine learning;data mining	DB	-8.923311036662119	-38.44903526541331	181403
ade9fb62ec404a159421722cbb448d4fe552fece	pqtable: fast exact asymmetric distance neighbor search for product quantization using hash tables	artificial neural networks indexing tuning training quantization signal data structures;pqtable query vector inverted indexing based approaches linear pq search product quantization based hash table hash tables fast exact asymmetric distance neighbor search;search problems database indexing quantisation signal query processing	We propose the product quantization table (PQTable), a product quantization-based hash table that is fast and requires neither parameter tuning nor training steps. The PQTable produces exactly the same results as a linear PQ search, and is 102 to 105 times faster when tested on the SIFT1B data. In addition, although state-of-the-art performance can be achieved by previous inverted-indexing-based approaches, such methods do require manually designed parameter setting and much training, whereas our method is free from them. Therefore, PQTable offers a practical and useful solution for real-world problems.	code generation (compiler);hash table;javaserver pages;overhead (computing)	Yusuke Matsui;Toshihiko Yamasaki;Kiyoharu Aizawa	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.225	double hashing;hash function;perfect hash function;primary clustering;computer science;theoretical computer science;machine learning;database;rolling hash;hash tree	Vision	-6.886071993440398	-42.60222906403419	181553
a68428981cbd8eaf49ae17401d19e78293e2836e	skyline-sensitive joins with lr-pruning	skyline aware join processing;point to point	Efficient processing of skyline queries has been an area of growing interest. Most existing techniques assume that the skyline query is applied to a single data table. Unfortunately, this is not true in many applications where, due to the complexity of the schema, the skyline query may involve attributes belonging to multiple tables. Recently, various hybrid skyline-join algorithms have been proposed. However, the current proposals suffer from several drawbacks: they often need to scan the input tables exhaustively in order to obtain the set of skyline-join results; moreover, the pruning techniques employed to eliminate the tuples are largely based on expensive pairwise tuple-to-tuple comparisons. In this paper, we aim to address these shortcomings by proposing two novel skyline-join algorithms, namely skyline-sensitive join (S 2J) and symmetric skyline-sensitive join (S 3J), to process skyline queries over multiple tables. Our approaches compute the results using a novel layer/region pruning technique (LR-pruning) that prunes the join space in blocks as opposed to individual data points, thereby avoiding excessive pairwise point-to-point dominance checks. Furthermore, the S 3J algorithm utilizes an early stopping condition in order to successfully compute the skyline results by accessing only a subset of the input tables. We report extensive experimental results that confirm the advantages of the proposed algorithms over the state-of-the-art skyline-join techniques.	algorithm;alpha–beta pruning;cardinality (data modeling);dvb-s2;data point;dominance drawing;early stopping;experiment;iterative method;join (sql);lr parser;linear scale;pareto efficiency;point-to-point protocol;rapid refresh;skyline operator;table (information);trie;z-order	Mithila Nagendra;K. Selçuk Candan	2012		10.1145/2247596.2247627	point-to-point;computer science;theoretical computer science;data mining;database	DB	-5.6886893360631285	-38.50568466233661	182434
70d5ceb59118334e1a6eed33a149234413147b92	deep interest evolution network for click-through rate prediction		Click-through rate (CTR) prediction, whose goal is to estimate the probability of a user clicking on the item, has become one of the core tasks in the advertising system. For CTR prediction model, it is necessary to capture the latent user interest behind the user behavior data. Besides, considering the changing of the external environment and the internal cognition, user interest evolves over time dynamically. There are several CTR prediction methods for interest modeling, while most of them regard the representation of behavior as the interest directly, and lack specially modeling for latent interest behind the concrete behavior. Moreover, little work considers the changing trend of the interest. In this paper, we propose a novel model, named Deep Interest Evolution Network (DIEN), for CTR prediction. Specifically, we design interest extractor layer to capture temporal interests from history behavior sequence. At this layer, we introduce an auxiliary loss to supervise interest extracting at each step. As user interests are diverse, especially in the e-commerce system, we propose interest evolving layer to capture interest evolving process that is relative to the target item. At interest evolving layer, attention mechanism is embedded into the sequential structure novelly, and the effects of relative interests are strengthened during interest evolution. In the experiments on both public and industrial datasets, DIEN significantly outperforms the state-of-the-art solutions. Notably, DIEN has been deployed in the display advertisement system of Taobao, and obtained 20.7% improvement on CTR.	cognition;display advertising;e-commerce;embedded system;evolution;experiment;online advertising;personalization;randomness extractor;taobao marketplace	Guorui Zhou;Na Mou;Ying Fan;Qi Pi;Weijie Bian;Chang Zhou;Xiaoqiang Zhu;Kun Gai	2019	CoRR		artificial intelligence;machine learning;click-through rate;extractor;computer science;cognition	ML	-18.390468693426282	-46.37593293985007	182457
48446f9f7c3022f1521eeb9d4ff2897d5bc67a45	semantic image query based on bimodal hash		With the popularity of social network, there has been an explosive growth of web images data, which requires a highperformance image query engine. Many researchers have paid significant efforts in this area and the key points are: semantic-based query and hash-based query. Semantic-based query techniques can make the search engine more intelligent and accurate. While, hash-based techniques can deal with large-scaled image data. In order to tackcle the huge amount image data, a novel bimodal hash approach for semantic images query is proposed in this paper. According to the distribution characteristics of web images, our hash function is synthesized based on a semi-supervised learning methods and considers the image modality as well as text modality. Since our hashing method can preserve semantic similarity among images, our image query technique is likely to be efficient and accurate. The results of extensive experiments on bimodal datasets prove the validity of our approach.	algorithm;code;experiment;hash function;machine learning;modal logic;modality (human–computer interaction);semantic similarity;semi-supervised learning;semiconductor industry;social network;supervised learning;usability;web search engine	Jiale Wang;Deng Chen;Guohui Li	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393105	semantic similarity;hamming distance;hash function;search engine;artificial intelligence;machine learning;semantics;linear programming;computer science;social network;matrix decomposition	DB	-15.918578276398735	-50.4963546595247	182523
749c2e35d1c1f132889f7fa9b58aabdc7fd60c38	local item-item models for top-n recommendation	top n recommendation;local models;collaborative filtering;slim	Item-based approaches based on SLIM (Sparse LInear Methods) have demonstrated very good performance for top-N recommendation; however they only estimate a single model for all the users. This work is based on the intuition that not all users behave in the same way -- instead there exist subsets of like-minded users. By using different item-item models for these user subsets, we can capture differences in their preferences and this can lead to improved performance for top-N recommendations. In this work, we extend SLIM by combining global and local SLIM models. We present a method that computes the prediction scores as a user-specific combination of the predictions derived by a global and local item-item models. We present an approach in which the global model, the local models, their user-specific combination, and the assignment of users to the local models are jointly optimized to improve the top-N recommendation performance. Our experiments show that the proposed method improves upon the standard SLIM model and outperforms competing top-N recommendation approaches.	existential quantification;experiment;sparse	Evangelia Christakopoulou;George Karypis	2016		10.1145/2959100.2959185	simulation;computer science;collaborative filtering;machine learning;data mining;world wide web	AI	-18.65761107276561	-47.82600500793082	182653
027464ccfd1e87efb042d50ebcd3aa3e5fa30a59	effects of habitat destruction in model ecosystems: parity law depending on species richness	mean field theory;habitat destruction;percolation network;parity law;cumulant;lattice ecosystem;species richness;mass extinction	Habitat destruction is one of the primary causes of recent mass extinction of biospecies. Even if the destruction is limited to a local and small area, the cumulative destruction increases the risk of extinction. In this paper, we explore the effect of habitat destruction in lattice ecosystems composed of multiple species. Simulations reveal a parity law: the response of the system shows different behaviors by whether the species richness of system is even or odd. The mean-field theory partially predicts such a parity law.	ecosystem	Nariyuki Nakagiri;Yukio Sakisaka;Tatsuya Togashi;Satoru Morita;Kei-ichi Tainaka	2010	Ecological Informatics	10.1016/j.ecoinf.2010.05.003	biology;habitat destruction;mean field theory;species richness;paleontology;ecology;cumulant;extinction event	NLP	-5.331930267865664	-45.98954384657624	182732
5b5cc6e4beb644b1d283dd6256994d5b1b1c071d	designing adaptive feedback for improving data entry accuracy	time varying;intelligent user interface;repetitive task;data entry;resource allocation;adaptive interface;probabilistic model;data quality;information gain;cognitive model;form design	Data quality is critical for many information-intensive applications. One of the best opportunities to improve data quality is during entry. Usher provides a theoretical, data-driven foundation for improving data quality during entry. Based on prior data, Usher learns a probabilistic model of the dependencies between form questions and values. Using this information, Usher maximizes information gain. By asking the most unpredictable questions first, Usher is better able to predict answers for the remaining questions. In this paper, we use Usher's predictive ability to design a number of intelligent user interface adaptations that improve data entry accuracy and efficiency. Based on an underlying cognitive model of data entry, we apply these modifications before, during and after committing an answer. We evaluated these mechanisms with professional data entry clerks working with real patient data from six clinics in rural Uganda. The results show that our adaptations have the potential to reduce error (by up to 78%), with limited effect on entry time (varying between -14% and +6%). We believe this approach has wide applicability for improving the quality and availability of data, which is increasingly important for decision-making and resource allocation.	cognitive model;data quality;information gain in decision trees;intelligent user interface;kullback–leibler divergence;statistical model	Kuang Chen;Joseph M. Hellerstein;Tapan S. Parikh	2010		10.1145/1866029.1866068	statistical model;cognitive model;simulation;data quality;human–computer interaction;resource allocation;computer science;artificial intelligence;data mining;kullback–leibler divergence	HCI	-10.132846309110342	-46.15207678088929	182797
57529470616cdf1726c39a5f6c9c8adfd8ab9642	sensitive analysis of timeframe type and size impact on community evolution prediction		One of the most interesting issues in the field of social network analysis is community evolution prediction in dynamic social networks. To start with, the dynamic network is split into a series of timeframes, each one containing interactions aggregated over a time period such as a month, a day or an hour. Splitting the network into timeframes is of crucial importance to capture the right communities’ temporal evolution before predicting their future. Our paper investigates the problem of choosing the appropriate scale for network splitting which would improve the prediction. The experiments we conducted on Facebook and Higgs Twitter datasets offer strong empirical evidence of the usefulness of considering the appropriate network splitting as a first step in predicting community evolution in dynamic social networks.	bsd;evolution;experiment;fuzzy control system;interaction;social network analysis	Hao-Han Ho;Fatima Benbouzid-Si Tayeb;Yahya Slimani;Karima Benatchba	2018	2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2018.8491668	task analysis;social network analysis;machine learning;dynamic network analysis;artificial intelligence;social network;empirical evidence;computer science;fuzzy control system	Web+IR	-17.295021957083538	-41.79134938727585	182912
7ad914492f10621f4917e62d229aa749ab84f2d4	which tumblr post should i read next?		Microblogging sites have emerged as major platforms for bloggers to create and consume posts as well as to follow other bloggers and get informed of their updates. Due to the large number of users, and the huge amount of posts they create, it becomes extremely difficult to identify relevant and interesting blog posts. In this paper, we propose a novel convex collective matrix completion (CCMC) method that effectively utilizes user-item matrix and incorporates additional user activity and topic-based signals to recommend relevant content. The key advantage of CCMC over existing methods is that it can obtain a globally optimal solution and can easily scale to large-scale matrices using Hazan’s algorithm. To the best of our knowledge, this is the first work which applies and studies CCMC as a recommendation method in social media. We conduct a large scale study and show significant improvement over existing state-ofthe-art approaches.	algorithm;blog;experiment;maxima and minima;social media	Zornitsa Kozareva;Makoto Yamada	2016				NLP	-18.687526849470295	-47.84467317599906	183134
dd44c3e306455182223faddf824b0da9787cc929	analysis of distance functions in graphs		Many machine learning algorithms use graphs to model relations between data points. One of the main objects of interest for such algorithms is the distance between vertices. There are several distance functions defined between graph vertices, each reflecting different properties of the underlying graph. The first part of this thesis characterizes properties of global distance functions in graphs: • Shortest path distance: The behavior of the shortest path distance depends on how we construct our graph from the data points. I show that in unweighted k-nearest neighbor graphs, the shortest path distance converges to an unpleasant distance function whose properties are detrimental to some machine learning problems. • p-resistance distance: The p-resistance distance is a generalization of the resistance distance and contains several other distances as its special cases. I study the convergence of the presistance distance in large geometric graphs and show that an interesting phase transition takes place. There exist two critical thresholds p∗ and p∗∗ such that if p < p∗, then the p-resistance depends on meaningful global properties of the graph, whereas if p > p∗∗, it only depends on trivial local quantities and does not convey any useful information. The second part of this thesis deals with local distances in graphs. Local clustering and friend recommendation are the topics covered in this part. • Local clustering is the task of finding a highly connected cluster around a vertex of interest. I propose a new random walk model for local clustering, consisting of several “agents” connected by ropes. All agents move independently but their distances are constrained by the ropes between them. The main insight is that for several agents it is harder to simultaneously travel over the bottleneck of a graph than for just one agent. • Local distances are used for recommending new friends in social networks. Here, I propose a new distance between members of the network that exploits the temporal data in the friendship network to recommend new friends. The third part of my thesis is devoted to the problem of downsampling massive graphs. The goal of downsampling is to produce a smaller “version” of a given graph, which would be easier to process and visualize. Here, a new method is proposed and followed by its thorough statistical analysis. The output of this method is a downsampled graph that provably inherits many properties of the original graph.		Morteza Alamgir	2014			computer science;artificial intelligence;calculus	Theory	-12.91078412941308	-40.63896737007502	183230
16c2ccb89d082574f768870f16c73349b3d637b5	evaluating the interestingness of characteristic rules	information retrieval;artificial intelligence;algorithms;data base management;knowledge base;mathematics computers information science management law miscellaneous	Knowledge Discovery Systems can be used to generate classification rules describing data from databases. Typically, only a small fraction of the rules generated may actually be of interest. Measures of rule intemstingness allow us to filter out less interesting rules. Classification rules may be discriminant (e + h) or characteristic (h + e), where e is evidence, and h is an hypothesis. For discriminant rules, e distinguishes h from -&. For characteristic rules, e summarizes one or more properties common to all instances of h. Both rule types can contribute insight into the data under analysis. In this paper, we first expand on the rule interestingness measure principles proposed by Piatetsky-Shapiro (1991) and by Major and Mangano (1993) by adding a principle which, unlike the others, considers the difference between discriminant and characteristic rules. We establish experimentally that the three popular interestingness measures for discriminant rules found in the literature do not fully serve their purpose when applied to characteristic rules. To our knowledge, no interestingness measures for characteristic rules have been published. We propose IC++, an interestingness measure for characteristic rules based on necessity and sticiency (Duda, Gaschnig, & Hart 1981). ICtt obeys each of the rule interestingness principles, unlike the other measures studied. If a given characteristic rule is found to be uninteresting by I@+, three additional measures, which we present, can be used to derive other useful information regarding h and e.	database;discriminant;experiment;flickr	Micheline Kamber;Rajjan Shinghal	1996			knowledge base;computer science;artificial intelligence;machine learning;data mining;mathematics	ML	-7.6143555868389585	-44.151411154645416	183729
1d55ff9b24f987035c24fad9356aa0ce27b1de2a	metric learning combining with boosting for user distance measure in multiple social networks		How to model user distance from multiple social networks is an important challenge. People often simultaneously appear in multiple social networks that can provide complementary services. Thus, knowledge from different social networks can help overcome the data sparseness problem. However, the knowledge cannot be directly obtained due to that they are from different social networks. To solve this problem, we construct an adaptive model to learn user distance in multiple social networks via combining distance metric learning and boosting technologies. The basic idea of our model is to embed related social networks into a potential feature space, while retaining the topologies of social networks. To get the solution to our model, we formulate it as a convex optimization problem. Moreover, we propose an adaptive user distance measure algorithm whose time complexity is linear with the number of the links. We verify the feasibility and effectiveness of our model on the link prediction problem. Experiments on two real large-scale data sets demonstrate that our method outperforms the compared methods. To the best of our knowledge, the joint learning of metric learning with boosting is first studied in multiple social networks.	algorithm;convex optimization;feature vector;mathematical optimization;neural coding;optimization problem;social network;time complexity	Yufei Liu;Dechang Pi;Lin Cui	2017	IEEE Access	10.1109/ACCESS.2017.2756102	boosting (machine learning);computer science;time complexity;metric (mathematics);feature vector;convex optimization;network topology;machine learning;algorithm design;artificial intelligence;social network	ML	-17.016548858169507	-45.882585544662476	183803
3b0906ee7ab381cabd7dda6690c6579f23f6df03	contextual genetic algorithms: evolving developmental rules	fuzzy set;rna editing;evidence theory;genetics;gene expression;biological systems;genetic algorithm	A genetic algorithm scheme with a stochastic genotype/phenotype relation is proposed. The mechanisms responsible for this intermediate level of uncertainty, are inspired by the biological system of RNA editing found in a variety of organisms. In biological systems, RNA editing represents a significant and potentially regulatory step in gene expression. The artificial algorithm here presented, will propose the evolution of such regulatory steps as an aid to the modeling of differentiated development of artificial organisms according to environmental, contextual, constraints. This mechanism of genetic string editing will then be utilized in the definition of a genetic algorithm scheme, with good scaling and evolutionary properties, in which phenotypes are represented by mathematical structures based on fuzzy set and evidence theories.	biological system;fuzzy set;genetic algorithm;image scaling;mathematical structure;theory	Luis Mateus Rocha	1995		10.1007/3-540-59496-5_312	biology;gene expression;genetic algorithm;computer science;bioinformatics;rna editing;fuzzy set;genetics	AI	-4.924868292381504	-48.43051917464095	183809
7c71bc273ec0cf54c0386e6a23aa7d7b4bb90e9d	building virtual ecosystems from artificial chemistry	virtual organization;trophic level;artificial life	This paper adopts an interdisciplinary view of the significant elements of ecosystems and the methods by which these might be simulated to explore theoretical issues of relevance to Artificial Life and Ecology. Artificial Life has largely been concerned with evolutionary ecosystems of agents in trivial environments. Ecology commonly produces models of specific habitats and organism populations unsuited to general exploration of theoretical issues. We propose that limitations of the simulations in these disciplines can be overcome by simulating ecosystems from the level of artificial chemistry. We demonstrate the approach’s feasibility by describing several virtual organisms represented at this level. The organisms naturally fall into trophic levels, generate energy from chemical bonds and transform material elements in the process. Virtual organisms may interact with one another and their abiotic environment using the same chemistry. Biosynthesis and decay may also be simulated through this mechanism.	artificial chemistry;artificial life;computer simulation;ecology;ecosystem;evolutionary algorithm;habitat;population;relevance;trophic function	Alan Dorin;Kevin B. Korb	2007		10.1007/978-3-540-74913-4_11	biology;computer science;bioinformatics;artificial intelligence;trophic level;ecology;artificial life	ML	-5.600546235671738	-47.927280201183166	183821
08f69806d3d02d3bba34320807ddff97bcf5e919	probabilistic analysis of the human transcriptome with side information	genetic program;theoretical framework;high dimensionality;statistical machine learning;computational techniques;gene regulation;genetics;activity pattern;scaling up;protein synthesis;probabilistic model;data analysis;statistical learning;human genome;molecular biology;human body;functional genomics;g5 artikkelivaitoskirja;high throughput;side information;quantitative method;biological network;genome sequence;open source	Lahti, L. (2010): Probabilistic analysis of the human transcriptome with side information Doctoral thesis, Aalto University School of Science and Technology, Dissertations in Information and Computer Science, TKK-ICS-D19, Espoo, Finland.	information and computer science;probabilistic analysis of algorithms	Leo Lahti	2010	CoRR		functional genomics;high-throughput screening;statistical model;biological network;human genome;whole genome sequencing;human body;regulation of gene expression;quantitative research;bioinformatics;data science;machine learning;data mining;data analysis;protein biosynthesis	AI	-5.7988315360823455	-51.887131586016	183861
60a1bab96aeb9fa3cb3b5828185a84bf021d2e4c	a community structure-based approach for network immunization	centrality;community structure;network immunization;modularity	We propose a community structure-based approach that does not require community labels of nodes, for network immunization. Social networks have been widely used as daily communication infrastructures these days. However, fast spreading of information over networks may have downsides such as computer viruses or epidemics of diseases. Because contamination is propagated among subgraphs communities along links in a network, use of community structure of the network would be effective for network immunization. However, despite various research efforts, it is still difficult to identify ground-truth community labels of nodes in a network. Because communities are often interwoven through intermediate nodes, we propose to identify such nodes based on the community structure of a network without requiring community labels. By regarding the community structure in terms of nodes, we construct a vector representation of nodes based on a quality measure of communities. The distribution of the constructed vectors is used for immunizing intermediate nodes among communities, through the hybrid use of the norm and the relation in the vector representation. Experiments are conducted over both synthetic and real-world networks, and our approach is compared with other network centrality-based approaches. The results are encouraging and indicate that it is worth pursuing this path.		Tetsuya Yoshida;Yuu Yamada	2017	Computational Intelligence	10.1111/coin.12082	computer science;data science;machine learning;data mining;modularity;centrality;community structure;statistics	AI	-15.101994407978125	-42.470439197147954	184020
7f100be1136ce5a9f6cd254a94a7d9e6684007bf	amorphous computing in the presence of stochastic disturbances	amorphous computers;q science general;qa 75 electronic computers computer science;developmental biology;bio inspired computing	Amorphous computing is a non-standard computing paradigm that relies on massively parallel execution of computer code by a large number of small, spatially distributed, weakly interacting processing units. Over the last decade or so, amorphous computing has attracted a great deal of interest both as an alternative model of computing and as an inspiration to understand developmental biology. A number of algorithms have been developed that can take advantage of the massive parallelism of this computing paradigm to solve specific problems. One of the interesting properties of amorphous computers is that they are robust with respect to the loss of individual processing units, in the sense that a removal of some of them should not impact on the computation as a whole. However, much less understood is to what extent amorphous computers are robust with respect to minor disturbances to the individual processing units, such as random motion or occasional faulty computation short of total component failure. In this article we address this question. As an example problem we choose an algorithm to calculate a straight line between two points. Using this example, we find that amorphous computers are not in general robust with respect to Brownian motion and noise, but we find strategies that restore reliable computation even in their presence. We will argue that these strategies are generally applicable and not specific to the particular AC we consider, or even specific to electronic computers.	algorithm;amorphous computing;brownian motion;computation (action);computer;computers;developmental biology;failure cause;interaction;parallel computing;programming paradigm	Dominique Chu;David J. Barnes;Samuel Perkins	2014	Bio Systems	10.1016/j.biosystems.2014.09.010	biology;bio-inspired computing;simulation;computer science;bioinformatics;artificial intelligence;theoretical computer science;machine learning;developmental biology;mathematics;genetics	Metrics	-6.765884947114878	-47.96290622273922	184288
a8318fd5e7d2d5b9a708ee146bfa525881bae6a8	discovering frequent geometric subgraphs	graph theory;embedding;errors;frequent pattern;spatial databases transaction databases data mining computer science itemsets us department of energy high performance computing contracts military computing chemical compounds;geometric graph;perforation;information retrieval;geometric forms;geometry;pattern recognition very large databases data mining graph theory;data processing;data bases;data mining;graphs;invariance;collection;chemical compounds;frequent itemset;experimental results frequent geometric subgraph discovery transactions very large databases data mining frequent itemsets data sets database objects frequent pattern discovery computationally efficient algorithm;heuristic methods;translations;scaling factor;pattern recognition;algorithms;optimization;rotation scaling and translation;chemical structure;very large databases;methodology	As data mining techniques are being increasingly applied to non-traditional domains, existing approaches for finding frequent itemsets cannot be used as they cannot model the requirement of these domains. An alternate way of modeling the objects in these data sets, is to use a graph to model the database objects. Within that model, the problem of finding frequent patterns becomes that of discovering subgraphs that occur frequently over the entire set of graphs. In this paper we present a computationally efficient algorithm for finding frequent geometric subgraphs in a large collection of geometric graphs. Our algorithm is able to discover geometric subgraphs that can be rotation, scaling and translation invariant, and it can accommodate inherent errors on the coordinates of the vertices. We evaluated the performance of the algorithm using a large database of over 20,000 real two dimensional chemical structures, and our experimental results show that our algorithms requires relatively little time, can accommodate low support values, and scales linearly on the number of transactions.	algorithm;algorithmic efficiency;cluster analysis;consensus sequence;data mining;graph (discrete mathematics);graph database;image scaling;iteration;iterative method;k-means clustering;mathematical optimization;vertex (geometry)	Michihiro Kuramochi;George Karypis	2002		10.1109/ICDM.2002.1183911	scale factor;collection;data processing;computer science;graph theory;theoretical computer science;invariant;embedding;methodology;data mining;database;mathematics;chemical structure;graph	ML	-6.012602497694831	-39.259410677403466	184527
70ef43fa164c0d0afa02da678596c07cf1434d7a	quasi-metrics, similarities and searches: aspects of geometry of protein datasets	distance function;metric space;high dimensionality;nucleotides;information retrieval;protein sequence;triangle inequality;satisfiability;indexation;protein motif;quantitative method;similarity measure;probability measure;similarity search	A quasi-metric is a distance function which satisfies the triangle inequality but is not symmetric: it can be thought of as an asymmetric metric. The central result of this thesis, developed in Chapter 3, is that a natural correspondence exists between similarity measures between biological (nucleotide or protein) sequences and quasi-metrics. Chapter 2 presents basic concepts of the theory of quasi-metric spaces and introduces a new examples of them: the universal countable rational quasi-metric space and its bicompletion, the universal bicomplete separable quasi-metric space. Chapter 4 is dedicated to development of a notion of the quasi-metric space with Borel probability measure, or pq-space. The main result of this chapter indicates that `a high dimensional quasi-metric space is close to being a metric space'. Chapter 5 investigates the geometric aspects of the theory of database similarity search in the context of quasi-metrics. The results about $pq$-spaces are used to produce novel theoretical bounds on performance of indexing schemes. Finally, the thesis presents some biological applications. Chapter 6 introduces FSIndex, an indexing scheme that significantly accelerates similarity searches of short protein fragment datasets. Chapter 7 presents the prototype of the system for discovery of short functional protein motifs called PFMFind, which relies on FSIndex for similarity searches.	prototype;similarity search;social inequality	Aleksandar Stojmirovic	2005	CoRR		nucleotide;probability measure;metric;quantitative research;metric space;machine learning;protein sequencing;triangle inequality;structural motif;information retrieval;statistics;satisfiability	ML	-5.578361940628815	-43.10782110102493	184776
34420022200cd6f837ebb41286137945c37d063c	bioscape: a modeling and simulation language for bacteria-materials interactions	stochastic simulation;stochastic pi calculus;bacteria materials interaction	We design BioScape, a concurrent language for the stochastic simulation of biological and bio-materials processes in a reactive environment in 3D space. BioScape is based on the Stochastic Pi-Calculus, and it is motivated by the need of individual-based, continuous motion, and continuous space simulation in modeling complex bacteria-materials interactions. Our driving example is a bio-triggered drug delivery system for infection-resistant medical implants. Our models in BioScape will help in identifying biological targets and materials strategies to treat biomaterialsassociated bacterial infections. The novel aspects of BioScape include syntactic primitives to declare the scope in space where species can move, diffusion rate, shape, and reaction distance, and an operational semantics that deals with the specifics of 3D locations, verifying reaction distance, and random movement.	british informatics olympiad;interaction;operational semantics;parallel computing;simulation language;verification and validation;π-calculus	Adriana B. Compagnoni;Vishakha Sharma;Yifei Bao;Matthew Libera;Svetlana A Sukhishvili;Philippe Bidinger;Livio Bioglio;Eduardo Bonelli	2013	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2013.02.017	simulation;computer science;theoretical computer science;stochastic simulation;mathematics	AI	-7.499166515081291	-50.17609718586597	185857
fe7b64cb10b7a1b23a60d055cf16fa72e2903349	node importance ranking of complex networks with entropy variation		The heterogeneous nature of a complex network determines the roles of each node in the network that are quite different. Mechanisms of complex networks such as spreading dynamics, cascading reactions, and network synchronization are highly affected by a tiny fraction of so-called important nodes. Node importance ranking is thus of great theoretical and practical significance. Network entropy is usually utilized to characterize the amount of information encoded in the network structure and to measure the structural complexity at the graph level. We find that entropy can also serve as a local level metric to quantify node importance. We propose an entropic metric, Entropy Variation, defining the node importance as the variation of network entropy before and after its removal, according to the assumption that the removal of a more important node is likely to cause more structural variation. Like other state-of-the-art methods for ranking node importance, the proposed entropic metric is also used to utilize structural information, but at the systematical level, not the local level. Empirical investigations on real life networks, the Snake Idioms Network, and several other well-known networks, demonstrate the superiority of the proposed entropic metric, notably outperforming other centrality metrics in identifying the top-k most important nodes.	centrality;complex network;measure-preserving dynamical system;real life	Xinbo Ai	2017	Entropy	10.3390/e19070303	complex network;structural variation;mathematical optimization;synchronization networks;mathematics;centrality;machine learning;structural complexity;artificial intelligence;graph;ranking	ML	-15.422959604626259	-41.10031676386773	185900
dd7c5f0696e500251925c12bcc105e073ec635d5	community based influence maximization in the independent cascade model		Community detection is a widely discussed topic in network science which allows us to discover detailed information about the connections between members of a given group. Communities play a critical role in the spreading of viruses or the diffusion of information. In [1], [8] Kempe et al. proposed the Independent Cascade Model, defining a simple set of rules that describe how information spreads in an arbitrary network. In the same paper the influence maximization problem is defined. In this problem we are looking for the initial vertex set which maximizes the expected number of the infected vertices. The main objective of this paper is to further improve the efficiency of influence maximization by incorporating information on the community structure of the network into the optimization process. We present different community-based improvements for the infection maximization problem, and compare the results by running the greedy maximization method.	computer virus;entropy maximization;expectation–maximization algorithm;greedy algorithm;kempe chain;mathematical optimization;network science;real life;simple set;time complexity;usb hub;vertex (graph theory)	László Hajdu;András Bóta;Miklós Krész	2018	2018 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2018F201	task analysis;machine learning;network science;artificial intelligence;expected value;cascade;simple set;vertex (geometry);computer science;maximization;community structure	DB	-16.67698828828495	-43.177369434910766	186056
4bcd40d0896c5684ef947499c68622d0aa560c63	revealing the densest communities of social networks efficiently through intelligent data space reduction		The inherent structure and connectivity of a group are important features of social networks. Finding the densest subgraphs of a graph directly maps to revealing the densest communities of a social network. Various techniques, e.g., edge density, k-core, near-cliques and k-cliques, have been developed to characterize graphs and extract the densest subgraphs of the graphs. However, as extraction of subgraphs with constraints is NP-hard, these techniques face a major difficulty of processing big and/or streaming data sets from social networks. This demands new methods from the expert and intelligent systems perspective for computation of the densest subgraph problem (DSP) with big and/or streaming data. The most recent method for this purpose is the Sampling method. It samples the big data sets, thus reducing the data space and consequently speeding up the DSP computation. But the sampled data inevitably miss out many useful data items. A new approach is presented in this paper for accelerated DSP computation with big and/or steaming data through data space reduction without loss of useful information. It uses a sliding window of small graphs with a fixed number of edges. Then, it filters out the least connected edges for each small graph. While the small graphs are processed, subgraphs are incrementally put together to reveal the densest subgraphs. Finally, the data space previously filtered out is checked for recovery of globally important edges. The approach is incorporated with existing subgraph extraction techniques for scalable and efficient DSP computation with improved accuracy. It is demonstrated for four subgraph extraction techniques over four Twitter data sets, and is shown to outperform the sampling method.	dataspaces;social network	Tao Han;Yu-Chu Tian;Yuqing Lan;Fenglian Li;Limin Xiao	2018	Expert Syst. Appl.	10.1016/j.eswa.2017.10.047	sliding window protocol;theoretical computer science;big data;scalability;digital signal processing;computation;sampling (statistics);data set;intelligent decision support system;computer science	DB	-11.222820970029412	-40.28458186326166	186100
2057c7b95198eb5001c60c811d3176760b0bbb2f	generative modeling for maximizing precision and recall in information visualization	generic model;information visualization;nonlinear dimensionality reduction	Information visualization has recently been formulated as an information retrieval problem, where the goal is to find similar data points based on the visualized nonlinear projection, and the visualization is optimized to maximize a compromise between (smoothed) precision and recall. We turn the visualization into a generative modeling task where a simple user model parameterized by the data coordinates is optimized, neighborhood relations are the observed data, and straightforward maximum likelihood estimation corresponds to Stochastic Neighbor Embedding (SNE). While SNE maximizes pure recall, adding a mixture component that “explains away” misses allows our generative model to focus on maximizing precision as well. The resulting model is a generative solution to maximizing tradeoffs between precision and recall. The model outperforms earlier models in terms of precision and recall and in external validation by unsupervised classification.	data point;generative modelling language;generative model;information retrieval;information visualization;nonlinear system;precision and recall;smoothing;unsupervised learning	Jaakko Peltonen;Samuel Kaski	2011			computer science;machine learning;pattern recognition;data mining	Web+IR	-12.542693723273176	-47.92222544129428	186145
4c96c8d3953d08c255c5e44a181a05ab930d6317	online learning-based clustering approach for news recommendation systems	indexes;computational modeling;heuristic algorithms;clustering algorithms;recommender systems;context;algorithm design and analysis	Recommender agents are widely used in online markets, social networks and search engines. The recent online news recommendation systems such as Google News and Yahoo! News produce real-time decisions for ranking and displaying highlighted stories from massive news and users access per day. The more relevant highlighted items are suggested to users, the more interesting and better feedback from users achieve. Therefore, the distributed online learning can be a promising approach that provides learning ability for recommender agents based on side information under dynamic environment in large scale scenarios. In this work, we propose a distributed algorithm that is integrated online K-Means user contexts clustering with online learning mechanisms for selecting a highlighted news. Our proposed algorithm for online clustering with lower bound confident clustering approximates closer to offline K-Means clusters than greedy clustering and gives better performance in learning process. The algorithm provides a scalability, cheap storage and computation cost approach for large scale news recommendation systems.	cluster analysis;coat of arms;computation;context-free language;distributed algorithm;feedback;google news;greedy algorithm;iteration;k-means clustering;kinetic monte carlo;online and offline;real-time transcription;recommender system;scalability;social network;web search engine	Minh N. H. Nguyen;Chuan Pham;Jae Hyeok Son;Choong Seon Hong	2016	2016 18th Asia-Pacific Network Operations and Management Symposium (APNOMS)	10.1109/APNOMS.2016.7737269	database index;algorithm design;computer science;data mining;cluster analysis;computational model;world wide web;information retrieval;recommender system	AI	-18.616301782300823	-49.02486542744734	186364
286eba18ba65544d46b857f120788ba484407608	the lovasz-bregman divergence and connections to rank aggregation, clustering, and web ranking		We extend the recently introduced theory of Lovász Bregman (LB) divergences [20] in several ways. We show that they represent a distortion between a “score” and an “ordering”, thus providing a new view of rank aggregation and order based clustering with interesting connections to web ranking. We show how the LB divergences have a number of properties akin to many permutation based metrics, and in fact have as special cases forms very similar to the Kendall-τ metric. We also show how the LB divergences subsume a number of commonly used ranking measures in information retrieval, like NDCG [23] and AUC [36]. Unlike the traditional permutation based metrics, however, the LB divergence naturally captures a notion of “confidence” in the orderings, thus providing a new representation to applications involving aggregating scores as opposed to just orderings. We show how a number of recently used web ranking models are forms of Lovász Bregman rank aggregation and also observe that a natural form of Mallow’s model using the LB divergence has been used as conditional ranking models for the “Learning to Rank” problem.	bregman divergence;cluster analysis;distortion;information retrieval;lattice boltzmann methods;learning to rank;lovász number;mallows's cp;pagerank;ranking (information retrieval);review aggregator	Rishabh K. Iyer;Jeff A. Bilmes	2013	CoRR			Web+IR	-10.871070690822995	-45.109499850698505	186789
1293a481d49e651800274deee7a7edfd6f04dbc7	collaborative translational metric learning		Recently, matrix factorization–based recommendation methods have been criticized for the problem raised by the triangle inequality violation. Although several metric learning–based approaches have been proposed to overcome this issue, existing approaches typically project each user to a single point in the metric space, and thus do not suffice for properly modeling the intensity and the heterogeneity of user-item relationships in implicit feedback. In this paper, we propose TransCF to discover such latent user-item relationships embodied in implicit user-item interactions. Inspired by the translation mechanism popularized by knowledge graph embedding, we construct user-item specific translation vectors by employing the neighborhood information of users and items, and translate each user toward items according to the user's relationships with the items. Our proposed method outperforms several state-of-the-art methods for top-N recommendation on seven real-world data by up to 17% in terms of hit ratio. We also conduct extensive qualitative evaluations on the translation vectors learned by our proposed method to ascertain the benefit of adopting the translation mechanism for implicit feedback-based recommendations.		Chanyoung Park;Donghyun Kim;Xing Xie;Hwanjo Yu	2018	2018 IEEE International Conference on Data Mining (ICDM)	10.1109/ICDM.2018.00052	machine learning;artificial intelligence;graph embedding;computer science;euclidean distance;metric space;qualitative evaluations;matrix decomposition;embodied cognition;triangle inequality	DB	-18.222295197214542	-46.84963128979202	186827
421d8b62f25b95956d165250d2763335e6c02499	collective spatial keyword queries: a distance owner-driven approach	spatial keyword querying;distance owner driven approach	Recently, spatial keyword queries become a hot topic in the literature. One example of these queries is the collective spatial keyword query (CoSKQ) which is to find a set of objects in the database such that it covers a set of given keywords collectively and has the smallest cost. Unfortunately, existing exact algorithms have severe scalability problems and existing approximate algorithms, though scalable, cannot guarantee near-to-optimal solutions. In this paper, we study the CoSKQ problem and address the above issues.  Firstly, we consider the CoSKQ problem using an existing cost measurement called the maximum sum cost. This problem is called MaxSum-CoSKQ and is known to be NP-hard. We observe that the maximum sum cost of a set of objects is dominated by at most three objects which we call the distance owners of the set. Motivated by this, we propose a distance owner-driven approach which involves two algorithms: one is an exact algorithm which runs faster than the best-known existing algorithm by several orders of magnitude and the other is an approximate algorithm which improves the best-known constant approximation factor from 2 to 1.375.  Secondly, we propose a new cost measurement called diameter cost and CoSKQ with this measurement is called Dia-CoSKQ. We prove that Dia-CoSKQ is NP-hard. With the same distance owner-driven approach, we design two algorithms for Dia-CoSKQ: one is an exact algorithm which is efficient and scalable and the other is an approximate algorithm which gives a √3-factor approximation.  We conducted extensive experiments on real datasets which verified that the proposed exact algorithms are scalable and the proposed approximate algorithms return near-to-optimal solutions.	approximation algorithm;chi;dia;diameter (protocol);exact algorithm;experiment;feasible region;loss function;np-hardness;query language;scalability;traverse	Cheng Long;Raymond Chi-Wing Wong;Ke Wang;Ada Wai-Chee Fu	2013		10.1145/2463676.2465275	computer science;theoretical computer science;data mining;database;approximation algorithm	DB	-9.802704208465718	-40.09624021657511	186855
d0db9b3377c2c531c4947b99b21012baf2c22b28	an anamnestic semantic tree-based relevance feedback method in cbir system	pattern clustering;low level visual features;dynamic updating process;clustering techniques;query formulation;query refinement;mean shift;tree data structures;visual databases content based retrieval image retrieval pattern clustering query formulation relevance feedback tree data structures;query refining;information retrieval image retrieval image databases content based retrieval indexes information science learning systems mars negative feedback educational institutions;visual features;content based image retrieval;relevance feedback;content based retrieval;cbir system;low level visual features anamnestic semantic tree based relevance feedback method cbir system content based image retrieval long term learning mechanism dynamic updating process query refining clustering techniques;long term learning mechanism;visual databases;image retrieval;anamnestic semantic tree based relevance feedback method	Relevance feedback is a usually used technique to narrow the gap between high-level concepts and low-level visual features in the content-based image retrieval. In this paper, a novel long-term learning mechanism is proposed to grasp the retrieval intention as much as possible. With more retrieval sessions going on, an anamnesis semantic tree is constructed to record the semantic relationship between the query and the retrieved back images on the high level concepts. In the dynamic updating process of the anamnesis semantic tree, both the mean shift based query refining and clustering techniques are adopted. The final experimental results show that the proposed approach greatly improves the retrieval performance	cluster analysis;content-based image retrieval;high- and low-level;high-level programming language;mean shift;relevance feedback	Xiaoxia Xie;Yao Zhao;Zhenfeng Zhu	2006	First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)	10.1109/ICICIC.2006.409	visual word;mean-shift;image retrieval;computer science;concept search;pattern recognition;data mining;tree;information retrieval	Vision	-11.01973239852443	-50.96950675660026	186966
d42a68eb1d45b23b42eef93ae9fad634287ec2e1	performance evaluation of respite care services through multi-agent based simulation		Caregivers of patients with chronic diseases are undergoing a daily burnout in their lives. Although respite care seems a promising solution, no quantitative analysis has yet been provided to demonstrate its positive impact. In this article, we propose (i) a new model of caregivers' burnout evolution based on Markov chain and machine learning to model health state evolution, and (ii) a multi-agent based simulation approach to describe the burnout evolution of caregivers and the impact of respite structures on the system. Optimal capacity of respite structures is obtained through a design of experiment. Several management strategies are also tested (collaboration between structures, reservation of beds for emergent cases). Key performance indicators considered are quality of service and costs. Results show a positive impact of respite services on both quality of service and costs. The model also show a trade-off between quality of service and costs when bed reservation policies are used.	agent-based model;design of experiments;emergence;machine learning;markov chain;multi-agent system;performance evaluation;quality of service;simulation	Oussama Batata;Vincent Augusto;Setareh Ebrahimi;Xiaolan Xie	2017	2017 Winter Simulation Conference (WSC)	10.1109/WSC.2017.8248013	quality of service;burnout;systems engineering;computer science;management science;respite care;performance indicator;markov chain	AI	-8.931710932000604	-50.16066204840763	187322
492a5ee5c759ab701aa33c160981875972aee14e	temporal subgraph isomorphism	graph theory;security of data computational complexity financial data processing graph theory;financial data processing;conference publication;peer to peer lending;trend prediction;peer to peer computing conferences educational institutions topology semantics social network services electronic mail;temporal information;np complete problem temporal subgraph isomorphism network data sets link activation sequences ideas propagation scientific collaboration network disease spread infected individuals susceptible individuals funds flow online financial transaction network suspicious behaviour;computational complexity;subgraph isomorphism problem;information diffusion;twitter;security of data	Temporal information is increasingly available with network data sets. This information can expose underlying processes in the data via sequences of link activations. Examples range from the propagation of ideas through a scientific collaboration network, to the spread of disease via contacts between infected and susceptible individuals. We focus on the flow of funds through an online financial transaction network, in which given patterns might signify suspicious behaviour. The search for these patterns may be formulated as a temporally constrained subgraph isomorphism problem. We compare two algorithms which use temporal data at different stages during the search, and empirically demonstrate one to be significantly more efficient.	algorithm;embnet.journal;graph isomorphism problem;scientific collaboration network;software propagation;subgraph isomorphism problem;temporal logic	Ursula Redmond;Padraig Cunningham	2013	2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)	10.1145/2492517.2492586	combinatorics;computer science;artificial intelligence;graph theory;theoretical computer science;machine learning;data mining;subgraph isomorphism problem;mathematics;distributed computing;computational complexity theory;world wide web	ML	-16.90127743857334	-41.87353138324116	187425
44ce8b17d513bc96c1cfa6a26dafb6e0b4bc3f48	edge balance ratio: power law from vertices to edges in directed complex network	complex networks twitter image edge detection numerical simulation materials communities;power law balance profile complex network directed graph edge balance ratio microblogging network online social network positivity	"""Power law distribution is common in real-world networks including online social networks. Many studies on complex networks focus on the characteristics of vertices, which are always proved to follow the power law. However, few researches have been done on edges in directed networks. In this paper, edge balance ratio is firstly proposed to measure the balance property of edges in directed networks. Based on edge balance ratio, balance profile and positivity are put forward to describe the balance level of the whole network. Then the distribution of edge balance ratio is theoretically analyzed. In a directed network whose vertex in-degree follows the power law with scaling exponent <formula formulatype=""""inline""""><tex Notation=""""TeX"""">$\gamma$</tex></formula>, it is proved that the edge balance ratio follows a piecewise power law, with the scaling exponent of each section linearly dependent on <formula formulatype=""""inline""""> <tex Notation=""""TeX"""">$\gamma$</tex></formula>. The theoretical analysis is verified by numerical simulations. Moreover, the theoretical analysis is confirmed by statistics of real-world online social networks, including Twitter network with 35 million users and Sina Weibo network with 110 million users."""	approximation algorithm;complex network;computer simulation;computer-aided software engineering;degree distribution;directed graph;hoare logic;image scaling;numerical analysis;social inequality;social network;vertex (geometry);vertex (graph theory);world online	Xiaohan Wang;Zhaoqun Chen;Pengfei Liu;Yuantao Gu	2013	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2013.2245299	combinatorics;simulation;computer science;machine learning;mathematics	Metrics	-16.986611261195677	-40.52352151622806	187988
386b2e25ab1a138cdee39286734572cefd0c90eb	a user-item relevance model for log-based collaborative filtering	busqueda informacion;modelizacion;relevance model;text;formal specification;formal model;information retrieval;musica;metodo formal;methode formelle;pertinencia;recommandation;user preferences;texte;probabilistic approach;formal method;specification formelle;modelisation;especificacion formal;musique;hierarchical classification;fichier log;fichero actividad;col;collaborative filtering;recherche information;linear interpolation;smoothing;enfoque probabilista;approche probabiliste;pertinence;comportement utilisateur;text retrieval;alisamiento;classification hierarchique;representacion parsimoniosa;recomendacion;interpolation lineaire;recommendation;modele donnee;user behavior;relevance;information system;texto;sparse representation;modeling;clasificacion jerarquizada;music;lissage;systeme information;comportamiento usuario;log file;interpolacion lineal;data models;representation parcimonieuse;sistema informacion	Implicit acquisition of user preferences makes log-based collaborative filtering favorable in practice to accomplish recommendations. In this paper, we follow a formal approach in text retrieval to re-formulate the problem. Based on the classic probability ranking principle, we propose a probabilistic user-item relevance model. Under this formal model, we show that user-based and item-based approaches are only two different factorizations with different independence assumptions. Moreover, we show that smoothing is an important aspect to estimate the parameters of the models due to data sparsity. By adding linear interpolation smoothing, the proposed model gives a probabilistic justification of using TF×IDF-like item ranking in collaborative filtering. Besides giving the insight understanding of the problem of collaborative filtering, we also show experiments in which the proposed method provides a better recommendation performance on a music play-list data set.	collaborative filtering;document retrieval;experiment;linear interpolation;mathematical model;relevance;smoothing;sparse matrix;tf–idf;user (computing)	Jun Wang;Arjen P. de Vries;Marcel J. T. Reinders	2006		10.1007/11735106_5	formal methods;relevance;computer science;artificial intelligence;collaborative filtering;machine learning;music;data mining;formal specification;linear interpolation;programming language;information retrieval;information system;recommender system;smoothing	Web+IR	-15.526478538817788	-49.52374313935541	188370
5cfee0bb4509303b6fc49829293c88d1bb680596	organizing patterns and evolution of indian movie industry	complex networks;movie actor networks;weighted networks	In this study we focus on basic structural and temporal characteristics of a network formed by linking movie actors in accordance with their co-occurrences with other fellow movie actors. In an actor network, the actors are represented as nodes and a link is formed between a pair of actor if they appear in the same movie. Each link is assigned a weight, representing the number of co-occurrences of the pair in different movies. A statistical analysis of the structure of networks reveals the organizing pattern of movie actors and temporal analysis of networks shows the evolution of the movie actors and industry on a whole. In this study we compare the national and regional movie industry in India along selected weighted network measures. The data set contains movies (from 1980 to 2013) encompassing three movie industries, namely Bollywood (national), Tamil and Malayalam (regional). We observe strong agreement between the three movie industries for degree distribution, strength distribution, average weight as a function of degree, and weighted clustering coefficient as a function of degree. The aforementioned observance does not deviate in temporal scale. However we observe a key difference in affinity measure between national and regional movie industries suggesting more acceptance of new actors in case of regional movie industry.	organizing (structure)	Srinivasan Radhakrishnan;Rohit Jiji Jacob;Arjun Duvvuru;Sagar Kamarthi	2014		10.1016/j.procs.2014.09.070	simulation;computer science;data mining;complex network	Vision	-17.03825499705024	-40.14971423343427	188694
28d2d96af45f42062dc4678232c55ff60d1db2e3	"""social search in """"small-world"""" experiments"""	attrition;small world;social network;small world experiment;social search	"""The """"algorithmic small-world hypothesis"""" states that not only are pairs of individuals in a large social network connected by short paths, but that ordinary individuals can find these paths. Although theoretically plausible, empirical evidence for the hypothesis is limited, as most chains in """"small-world"""" experiments fail to complete, thereby biasing estimates of """"true"""" chain lengths. Using data from two recent small-world experiments, comprising a total of 162,328 message chains, and directed at one of 30 """"targets"""" spread across 19 countries, we model heterogeneity in chain attrition rates as a function of individual attributes. We then introduce a rigorous way of estimating true chain lengths that is provably unbiased, and can account for empirically-observed variation in attrition rates. Our findings provide mixed support for the algorithmic hypothesis. On the one hand, it appears that roughly half of all chains can be completed in 6-7 steps--thus supporting the """"six degrees of separation"""" assertion--but on the other hand, estimates of the mean are much longer, suggesting that for at least some of the population, the world is not """"small"""" in the algorithmic sense. We conclude that search distances in social networks are fundamentally different from topological distances, for which the mean and median of the shortest path lengths between nodes tend to be similar."""	assertion (software development);attrition (website);biasing;shortest path problem;six degrees of separation;small-world experiment;social network;social search	Sharad Goel;Roby Muhamad;Duncan J. Watts	2009		10.1145/1526709.1526804	machine learning;social network	Web+IR	-17.032759780082735	-38.34352816008322	189126
1aeaf760805bf08e3e70b013f73e5b1559bbd71b	common design structure discovery from cad models	graph theory;computer aided design;design automation;knowledge discovery 3d cad model face adjacency graph fag two dimensional plane frequent subgraph discovery algorithm frequent candidate subgraph generation frequent subgraph merging subgraph matching graph descriptive code computer aided design cdsd common design structure discovery mechanical design engineering structure;engineering graphics;face adjacency graph;cad;mechanical design;two dimensional plane;computational geometry;frequent subgraph discovery algorithm;data mining;cdsd;engineering structure;structural engineering computing cad computational geometry data mining engineering graphics graph theory merging pattern matching solid modelling;graph descriptive code;computational modeling;shape;structural engineering computing;pattern matching;solid modeling;subgraph matching;fag;merging;face;design automation shape design engineering merging libraries data compression information retrieval information analysis frequency mathematical model;frequent candidate subgraph generation;frequent subgraph merging;3d cad model;common design structure discovery;algorithm design and analysis;solid modelling;knowledge discovery	This paper presents a method to solve the problem of common design structure discovery from a large number of CAD models. First, a CAD model is transformed into a Face Adjacency Graph (FAG) and each node of FAG is mapped to a point in two-dimensional plane after representing face shape information with two coordinates. So the shapes of models are directly compared through the point coordinates of FAGs' nodes. Thus, the common design structures are just the frequent appearing subgraphs of FAGs. Second, we develop an algorithm to discovery frequent subgraphs of FAGs. The main steps of the algorithm include: (1)frequent candidate subgraph generation based on merging of last discovered frequent subgraphs; (2)subgraph matching with graph descriptive code. The experiment shows a reasonable result of the discovered common design structures with our approach.	approximation algorithm;computer-aided design;principle of good enough	Lujie Ma;Zhengdong Huang;Yanwei Wang	2009	2009 11th IEEE International Conference on Computer-Aided Design and Computer Graphics	10.1109/CADCG.2009.5246877	face;algorithm design;electronic design automation;computational geometry;shape;computer science;graph theory;theoretical computer science;machine learning;pattern matching;data mining;cad;mathematics;solid modeling;computational model	EDA	-6.48919433274612	-39.34488358819182	189145
d504eb703390e1482916482e2e14b2fd11ad1a18	evolutionary clustering by hierarchical dirichlet process with hidden markov state	social network services;infinite hidden markov model;hdp htm;time sharing computer systems;history;evolutionary clustering;application software;hidden markov model;probability density function;matrix algebra data mining hidden markov models;usa councils;matrix algebra;information services;data mining;statistical model;transition matrix;evolution biology;hidden markov models;internet;web sites;social network analysis;inference algorithms;hierarchical dirichlet process;computer science;hidden markov models data mining computer science usa councils social network services application software information services web sites internet time sharing computer systems;hdp htm evolutionary clustering hierarchical dirichlet process infinite hidden markov model;data mining evolutionary clustering hierarchical dirichlet process social network analysis hierarchical transition matrix infinite hierarchical hidden markov state model;data models	This paper studies evolutionary clustering, which is a recently hot topic with many important applications, noticeably in social network analysis. In this paper, based on the recent literature on Hierarchical Dirichlet Process (HDP) and Hidden Markov Model (HMM), we have developed a statistical model HDP-HTM that combines HDP with a Hierarchical Transition Matrix (HTM) based on the proposed Infinite Hierarchical Hidden Markov State model (iH2MS) as an effective solution to this problem. The HDP-HTM model substantially advances the literature on evolutionary clustering in the sense that not only it performs better than the existing literature, but more importantly it is capable of automatically learning the cluster numbers and structures and at the same time explicitly addresses the correspondence issue during the evolution. Extensive evaluations have demonstrated the effectiveness and promise of this solution against the state-of-the-art literature.	cluster analysis;evolutionary algorithm;html;hidden markov model;hierarchical temporal memory;ibm notes;markov chain;social network analysis;statistical model;stochastic matrix	Tianbing Xu;Zhongfei Zhang;Philip S. Yu;Bo Long	2008	2008 Eighth IEEE International Conference on Data Mining	10.1109/ICDM.2008.24	statistical model;data modeling;probability density function;application software;social network analysis;the internet;computer science;data science;machine learning;hidden semi-markov model;data mining;stochastic matrix;markov model;brown clustering;information system;hidden markov model;statistics;hierarchical dirichlet process	DB	-12.810929928584905	-44.7425520512188	189150
fe5a9b08b94c64cb0fe9c5abcf3a7c4bce0faef0	visual search reranking via adaptive particle swarm optimization	adaptive particle swarm optimization;journal article;particle swarm optimizer;visual search;list distance;visual search reranking;video search	Visual search reranking involves an optimization process that uses visual content to recover the ''genuine'' ranking list from the helpful but noisy one generated by textual search. This paper presents an evolutionary approach, called Adaptive Particle Swarm Optimization (APSO), for unsupervised visual search reranking. The proposed approach incorporates the visual consistency regularization and the ranking list distance. In addition, to address the problem that existing list distance fails to capture the genuine disagreement between two ranking lists, we propose a numerical ranking list distance. Furthermore, the parameters in APSO are self-tuned adaptively according to the fitness values of the particles to avoid being trapped in local optima. We conduct extensive experiments on automatic search task over TRECVID 2006-2007 benchmarks and show significant and consistent improvements over state-of-the-art works.	mathematical optimization;particle swarm optimization	Lu Zhang;Tao Mei;Yuan Liu;Dacheng Tao;He-Qin Zhou	2011	Pattern Recognition	10.1016/j.patcog.2011.01.016	multi-swarm optimization;visual search;machine learning;pattern recognition;mathematics;information retrieval	Vision	-15.845217810875099	-50.95869441311867	189181
1fad4c1d0843c96c905524401255b2cd854d0b12	implicit user modeling in group chat		"""In recent years, enterprise group chat collaboration tools such as Slack, IBM's Watson Workspace and Microsoft Teams, have presented unprecedented growth. With all the potential benefits of these tools """" productivity increase and improved group communication """" come significant challenges. Specifically, users find it hard to focus their attention on content that is relevant to them due to the load of conversational content. This load can be handled by personalized content presentation and summarization mitigated by user profiling. We present an unsupervised approach for implicitly modeling group chat users through a combination of a probabilistic topic model and social analysis. We evaluate our approach by testing it on a task of conversation participation prediction, serving as a proxy for anticipating user interests, and show that by utilizing our approach, a system successfully predicts users participation in conversations. We further analyze the contribution of the various user model components and show them to be significant."""	chat room;personalization;proxy server;slack variable;topic model;unsupervised learning;user modeling;workspace	Anat Hashavit;Naama Tepper;Inbal Ronen;Lior Leiba;Amir D. N. Cohen	2018		10.1145/3213586.3225236	data mining;workspace;user modeling;conversation;automatic summarization;topic model;profiling (computer programming);probabilistic logic;unsupervised learning;computer science	Web+IR	-18.61522596258646	-49.81908741293643	189286
2a16cbcf0e3a26669a81722211fc6ac44c736911	fault tolerant direct nat structure extraction from pairwise causal interaction patterns		Non-impeding noisy-And Trees (NATs) provide a general, expressive, and efficient causal model for conditional probability tables (CPTs) in discrete Bayesian networks (BNs). A CPT may be directly expressed as a NAT model or compressed into a NAT model. Once CPTs are NAT-modeled, efficiency of BN inference (both space and time) can be significantly improved. The most important operation in NAT modeling CPTs is extracting NAT structures from interaction patterns between causes. Early method does so through a search tree coupled with a NAT database. A recent advance allows extraction of NAT structures from full, valid causal interaction patterns based on bipartition of causes, without requiring the search tree and the NAT database. In this work, we extend the method to direct NAT structure extraction from partial and invalid causal interaction patterns. This contribution enables direct NAT extraction from all conceivable application scenarios.	algorithm;bayesian network;cpt (file format);causal filter;causal model;data compression;experiment;network address translation;search tree	Yang Xiang	2017		10.1007/978-3-319-67582-4_10	data mining;causal model;computer science;artificial intelligence;machine learning;bayesian network;conditional probability;inference;search tree;pairwise comparison;nat;graphical model	AI	-8.055818211701103	-39.26122297002636	189343
2f1e20f9f5d6e879a8ee1c92395082ed6371df45	summarizing big graphs by means of pseudo-boolean constraints	heart;niobium;data mining;big data;image edge detection;mathematical model;bipartite graph	How to succinctly represent the truly relevant information in big data graphs? The approach presented in this paper aims to discover hidden graph structures and exploit them to compactly summarize large graphs. First, we show that some special graph classes such as cliques and bicliques can be represented efficiently as Pseudo-Boolean (PB) constraints. Then, we propose three new graph classes representable as PB constraints, called nested, sequence and clique-nested bi-partite graphs. Finally, we derive a general approach for partial or complete summarization of an arbitrary graph as a disjunction of PB constraints. Our representation can be seen as an original way to represent the edges of the graph, as they correspond to particular solutions of the PB constraints. An extensive experimental evaluation on several real-world networks shows that our framework is competitive with the state-of-the-art compression technique.	algorithm;big data;clique (graph theory);cluster analysis	Saïd Jabbour;Nizar Mhadhbi;Abdesattar Mhadhbi;Badran Raddaoui;Lakhdar Sais	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840683	1-planar graph;outerplanar graph;pathwidth;topological graph theory;combinatorics;discrete mathematics;cograph;universal graph;graph product;null graph;graph property;clique-width;forbidden graph characterization;machine learning;comparability graph;mathematics;voltage graph;distance-hereditary graph;graph;complement graph;graph operations;intersection graph;line graph	ML	-10.498217461129535	-39.04774827289957	189392
2b1de93e7354b9c1e997504ac7abbf8e74af0d90	estimating the clustering coefficient of a social network by a non-backtracking random walk		The clustering coefficient of social networks can be estimated via an unbiased sampling technique such as random walk sampling. To this end, we propose an algorithm that assumes no prior knowledge of the network and that accesses the network only through a publicly available interface. The clustering coefficient of a network is estimated by counting triangles in a non-backtracking random walk (NBRW). A social network is viewed as an undirected graph and the NBRW is retrieved through a public interface, which theoretically guarantees its retrieval. In a simulation study on real social networks, the proposed method achieved higher efficiency and accuracy than the prior state-of-the-art method based on a simple random walk.	algorithm;backtracking;clustering coefficient;graph (discrete mathematics);public interface;sampling (signal processing);simulation;social network	Kenta Iwasaki;Kazuyuki Shudo	2018	2018 IEEE International Conference on Big Data and Smart Computing (BigComp)	10.1109/BigComp.2018.00025	backtracking;random walk;big data;cluster analysis;simple random sample;sampling (statistics);clustering coefficient;social network;mathematics;artificial intelligence;pattern recognition	DB	-11.010519926138116	-43.199264734711996	189632
5d5ad004bb48c6b36198b8a6154d61650929779c	a comparative study of different approaches for tracking communities in evolving social networks		In real-world social networks, there is an increasing interest in tracking the evolution of groups of users and detecting the various changes they are liable to undergo. Several approaches have been proposed for this. In studying these approaches, we observed that most of them use a two-stage process. In the first stage, they run an algorithm to identify groups of users at each timestamp. In the second stage, a pairwise comparison based on a similarity measure is employed to track groups of users and detect changes they may undergo. While the majority of existing approaches use a two-stage process, they all run different algorithms to identify communities and rely on different similarity measures to track groups of users over time. Noting that the different approaches may perform differently depending on the dynamic social network under investigation, we decided to make a high level survey of some existing tracking approaches and then do a comparative analysis of some of them. In our analysis, we compared the algorithms in two main situations: (1) when groups of users do not overlap and (2) when the groups are overlapping. The study was done on three different testbeds extracted from the DBLP, Autonomous System (AS) and Yelp datasets.	algorithm;autonomous system (internet);dbl-browser;experiment;high-level programming language;overlap–add method;qualitative comparative analysis;sensor;similarity measure;social network;testbed	Ziwei He;Etienne Gael Tajeuna;Shengrui Wang;Mohamed Bouguessa	2017	2017 IEEE International Conference on Data Science and Advanced Analytics (DSAA)	10.1109/DSAA.2017.62	machine learning;computer science;timestamp;artificial intelligence;data mining;similarity measure;social network;markov process;cluster analysis;pairwise comparison;autonomous system (mathematics)	DB	-14.930877398408299	-43.04982194855119	189955
2834254de8b8623fb8c389de3b7da0d6316e5cca	clsh: cluster-based locality-sensitive hashing	approximate nearest neighbor search;locality sensitive hashing;会议论文;clustering;localitysensitive hashing;distributed cluster;high dimensional indexing	Locality-sensitive hashing (LSH) usually consumes large memory in similarity search, which limits its scalability for large scale applications. In this paper, we propose a novel cluster-based locality-sensitive hashing (CLSH) approach, which extends the conventional LSH framework and aims at indexing and searching large scale high-dimensional datasets. We first utilize a clustering algorithm to partition the raw feature dataset into clusters, and map these clusters to a distributed cluster. Then, LSH method is applied to construct the index for each cluster, and we present two criteria to choose the cluster(s) for further detailed search in order to improve the search quality. This proposed framework comes with following properties. Firstly, CLSH can cope with large scale feature dataset. Secondly, the generated clusters can guide the feature dataset automatical mappings to a distributed cluster. After that, the search time can be reduced a lot by searching on multiple computing nodes. Experiments show that the proposed approach outperforms the existing approaches in terms of efficiency and scalability.	algorithm;cluster analysis;computer cluster;locality of reference;locality-sensitive hashing;minimal mappings;scalability;similarity search;lsh	Xiangyang Xu;Tongwei Ren;Gangshan Wu	2014		10.1145/2632856.2632868	computer science;machine learning;data mining;database;locality preserving hashing;2-choice hashing;locality-sensitive hashing	DB	-5.059181882070374	-39.977250399459365	189974
4311a913926fa5171d6447457a5460fe238b70cc	molecular computation and evolutionary wetware: a cutting-edge technology for artificial life and nanobiotechnologies	molecular computation;focusing;modelizacion;design model;biology computing;biocomputing;semiconducteur;evolutionary computation;procesamiento informacion;focalizacion;nanobioscience;nanobioict molecular computation evolutionary wetware cutting edge technology artificial life nanobiotechnologies molecular information processing mechanism problem solving algorithms nanobiomachines;focalisation;etude experimentale;information technology;branching;biological system modeling;nanobioict;biologia molecular;technologie information;nanotechnology;nanobiomachines;sistema complejo;computational method;tube;nanobiotechnologies;semiconductor material;evolutionary wetware;problem solving algorithms;modelisation;evolution biology;nanobiotechnology artificial life biocomputing biology computing evolutionary computation;semiconductor materials;nanotecnologia;vie artificielle;calculo molecular;information and communication technology;systeme complexe;molecular evolution;complex system;biomimetique;structure moleculaire;ramificacion;molecular biology;information processing;ramification;communications technology;algorithme evolutionniste;molecular information processing mechanism;molecular computing;algoritmo evolucionista;informatics;biotecnologia;tubo;evolutionary algorithm;nanotechnologie;traitement information;nanobioscience artificial life biological computing molecular computation;tecnologia informacion;biotechnology;estructura molecular;modeling;information analysis;estudio experimental;biological computing;biotechnologie;calcul moleculaire;artificial life;buildings;nanobioscience biology computing communications technology information processing evolution biology biological system modeling buildings informatics laboratories information analysis;problem solving;cutting edge technology;biomimetics;biologie moleculaire;nanobiotechnology;molecular structure	Focusing on the new frontiers opened by the integration of artificial life and nanobiotechnologies, this paper reviews mainstream biomolecular computation from the viewpoint of an information processing mechanism, computing methods, and problem-solving algorithms. We also discuss evolutionary wetware as a tool for unconventional computing, inspired by biomolecular systems in nature. Biomolecular computation uses a different paradigm of computing than that of the semiconductor computer. It includes several branches based on different molecular materials or molecular structures. Wetware can be used to demonstrate molecular evolution by engineered operations in test tubes. This makes evolutionary wetware capable of bridging the two domains of molecular computation and artificial life so that molecular information processing methods can be extended from carrying out computational tasks to modeling scalable complex systems. From a systematic study of nanobiomachines, we expect to designate models of artificial life, and to search for a novel methodology of nanobioICT (Information and Communication Technology) in the near future	artificial life;autonomic computing;autonomous robot;autopoiesis;bayesian inference in phylogeny;bridging (networking);cell signaling;cognitive science;complex systems;crosstalk;cyberinfrastructure;cybernetics;emergence;enlightenment;evolutionary algorithm;gene regulatory network;hoc (programming language);information processing;interaction;natural computing;parallel computing;performance;pervasive informatics;phylogenetics;problem solving;programming paradigm;scalability;self-consciousness;semiconductor;synthetic intelligence;system of systems;systems design;theory of computation;transduction (machine learning);unconventional computing;wetware (brain)	Jian-Qin Liu;Katsunori Shimohara	2007	IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)	10.1109/TSMCC.2006.887011	information and communications technology;information processing;computer science;artificial intelligence;evolutionary algorithm;mathematics;information technology;nanobiotechnology;algorithm;evolutionary computation	HPC	-6.710263883558039	-49.76584016467911	189975
125e782a8946891dee9ccff38e74253eaa0ca3f9	enterprise community detection	social network services;employment;companies;matrix decomposition;mathematical model;user generated content	"""Employees in companies can be divided into different social communities, and those who frequently socialize with each other are treated as close friends and will be grouped in the same community. In the enterprise context, a large amount of information about the employees is available in both (1) offline company internal sources and (2) online enterprise social networks (ESNs). What's more, each of the information sources can also contain multiple categories of employees' socialization activity information at the same time. In this paper, we propose to detect the social communities of the employees in companies based on these different information sources simultaneously, and the problem is formally called the """"Enterprise Community Detection"""" (ECD) problem. To address the problem, a novel community detection framework named """"HeterogeneoUs MultisOurce ClusteRing"""" (HUMOR) is introduced in this paper. Based on the various enterprise social intimacy measures introduced in this paper, HUMOR detects a set of micro community structures of the employees based on these different categories of information available in the online and offline sources respectively. (A full version of this paper is available in [5])."""	enterprise social networking;experiment;explanatory combinatorial dictionary;graphics processing unit;ibm notes;online and offline;social network;socialization;titan (supercomputer)	Jiawei Zhang;Philip S. Yu;Yuanhua Lv	2017	2017 IEEE 33rd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2017.79	computer science;knowledge management;mathematical model;data mining;database;matrix decomposition;user-generated content;statistics	DB	-17.951471527009087	-45.93384020535283	190068
d3bb40b0ce9979e5e7a80cd14cc43f1c27f3623e	processamento eficiente de junções espaçotemporais	spatial index	It's a well-known fact that the new GIS applications need to keep track of temporal information. Among other operations, a spatiotemporal DBMS should efficiently answer the spatiotemporal join. The best-known spatial index structure, the R-Tree (and its variants), does not preserve the MBRs’ evolution. New indexing structures were proposed in the literature that allows the retrieving of present and past states of data, and most of them are R-Tree based. This paper presents a first study of spatiotemporal join processing using these new structures, particularly a partially persistent R-Tree called Temporal R-Tree and the 2+3D R-Tree. Starting from spatial join algorithms, we present algorithms for processing spatiotemporal joins over time instants and intervals on both spatiotemporal structures. Then, we propose some improvements that lead to a better performance and try to show the correctness of our algorithms. Finally, we implement and test these new algorithms with some spatiotemporal data sets. Our experiments shows that our algorithms performance is good even on extreme cases, like datasets with many changes, showing its good scalability – especially for the TR-Tree. In addition, with minor adaptations, the main ideas of our algorithm can be used for evaluating joins using other partially persistent structures, like the MVB-Tree.	algorithm;correctness (computer science);experiment;gis applications;geographic information system;join (sql);r-tree;scalability;spatial database;train communication network	Geraldo Zimbrão;Jano Moreira de Souza;Victor Teixeira de Almeida	2001			data mining;search engine indexing;correctness;spatial database;scalability;gis applications;theoretical computer science;joins;data set;computer science	DB	-8.128283851097958	-38.04264312718939	190239
cbbf566672757dc8237f9ddd019b330afbc789ac	a visual and textual recurrent neural network for sequential prediction		Sequential prediction is a fundamental task for Web applications. Due to the insufficiency of user feedbacks, sequential prediction usually suffers from the cold start problem. There are two kinds of popular approaches based on matrix factorization (MF) and Markov chains (MC) for item prediction. MF methods factorize the user-item matrix to learn general tastes of users. MC methods predict the next behavior based on recent behaviors. However, they have limitations. MF methods can merge additional information to address cold start but could not capture dynamic properties of user’s interest, and MC based sequential methods have difficulty in addressing cold start and has a strong Markov assumption that the next state only depends on the last state. In this work, to deal with the cold start problem of sequential prediction, we propose a RNN model adopting visual and textual content of items, which is named as Visual and Textual Recurrent Neural Network (VTRNN). We can simultaneously learn the sequential latent vectors that dynamically capture the user’s interest, as well as content-based representations that contribute to address the cold start. Experiments on two real-world datasets show that our proposed VT-RNN model can effectively generate the personalized ranking list and significantly alleviate the cold start problem.	artificial neural network;backpropagation through time;cold start;convergence insufficiency;experiment;markov chain;personalization;random neural network;recurrent neural network;web application	Qiang Cui;Shu Wu;Qiang Liu;Liang Wang	2016	CoRR		speech recognition;machine learning;data mining;statistics	AI	-19.049719524859782	-48.23771339847506	190325
4a3c2062ab7271e302ec31f288ec33ec373c49aa	music data processing and mining in large databases for active media		The aim of this paper was to investigate the problem of music data processing and mining in large databases. Tests were performed on a large database that included approximately 30000 audio files divided into 11 classes corresponding to music genres with different cardinalities. Every audio file was described by a 173-element feature vector. To reduce the dimensionality of data the Principal Component Analysis (PCA) with variable value of factors was employed. The tests were conducted in the WEKA application with the use of k-Nearest Neighbors (kNN), Bayesian Network (Net) and Sequential Minimal Optimization (SMO) algorithms. All results were analyzed in terms of the recognition rate and computation time efficiency.	database	Piotr Hoffmann;Bozena Kostek	2014		10.1007/978-3-319-09912-5_8	speech recognition;computer science;data science;machine learning;data mining;database;world wide web;statistics	ML	-4.6439397151056685	-39.30473569438501	190468
0bc22755e96b3f377e70fadf31013067590d072a	scalable and parallel processing of influence maximization for large-scale social networks	hadoop;influence maximization;parallel algorithm;social network	Influence maximization is a problem of finding a small subset of nodes as seeds in a social network such that the total influence of this subset of nodes for disseminating a message in the social network can be maximized. The problem has been extensively investigated in recent years and many influence maximization algorithms have been proposed. However, all of the existing algorithms are sequentially executed algorithms. It would take long time if they run on large-scale social networks. In this paper, we study parallel algorithms for two influence maximization problems in large-scale social networks: influence maximization without budget limitation and influence maximization with limited budget. We propose two parallel algorithms, Community-based Max Degree (CMD) algorithm and Max Degree Cost Ratio (MDCR) algorithm, respectively for the two problems. Both algorithms can run in parallel on Hadoop platform. Experiments are conducted for various sizes of social networks. The results show that our algorithms are scalable and outperform the common heuristic algorithms.	apache hadoop;creative micro designs;entropy maximization;expectation–maximization algorithm;heuristic;parallel algorithm;regular expression;scalability;social network;time complexity	Yafei Chang;Hejiao Huang;Xiaohua Jia	2017	2017 3rd International Conference on Big Data Computing and Communications (BIGCOM)	10.1109/BIGCOM.2017.28	artificial intelligence;machine learning;computer science;parallel algorithm;scalability;heuristic;social network;maximization;parallel processing	DB	-10.336567195824538	-41.943558378503056	190607
4287fb86fdaaba5d576df93f6d725b90aed2d21e	tracking the evolution of community structures in time-evolving social networks	social network services;analytical models;social networking online matrix algebra;social network services computer science image edge detection heuristic algorithms correlation data mining analytical models;data mining;image edge detection;heuristic algorithms;yelp community structures time evolving social networks time sequential way modified jaccard measure matrix dblp autonomous system;computer science;correlation;community evolution dynamic social network critical events threshold similarity	In real-world social networks, there is increasing interest in tracking the evolution of groups of users. Existing approaches track evolving communities, in a time-sequential way, by comparing communities in terms of nodes using a similarity measure such as the Jaccard or a modified Jaccard measure. The measure allows the use of a one-to-one comparison in order to match communities. However, tracking a given community based on this measure alone may, at the end of its lifespan yield a community that does not share any node with the community initially observed. In this paper we present a novel approach for modeling and detecting the evolution of communities. In our model, we first build a matrix that counts the number of nodes shared between two communities. The individual rows of the obtained matrix are then used to represent nodes shared by a community with all other communities over time. This effectively captures the trace of the communities that should be compared over the period of observation. We then propose a new similarity measure, named mutual transition, for tracking the communities and rules for capturing significant transition events a community can undergo. The proposed approach is general in the sense that it can be applied to different social networks. To demonstrate the suitability of the proposed method, we conducted experiments on real data extracted from the DBLP, Autonomous System and YELP.	autonomous system (internet);evolution;experiment;jaccard index;node (computer science);one-to-one (data model);rendering (computer graphics);sensor;similarity measure;social network;trix (operating system)	Etienne Gael Tajeuna;Mohamed Bouguessa;Shengrui Wang	2015	2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)	10.1109/DSAA.2015.7344876	simulation;computer science;machine learning;data mining;community structure	DB	-14.977053608079949	-42.821863819040296	190680
72072d0afab5191568ca84ad570fabfa97206956	centrality-based connected dominating sets for complex network graphs	betweeness;connected dominating set;network graphs;centrality;correlation	The author proposes the use of centrality-metrics to determine connected dominating sets (CDS) for complex network graphs. The author hypothesizes that nodes that are highly ranked by any of these four well-known centrality metrics (such as the degree centrality, eigenvector centrality, betweeness centrality and closeness centrality) are likely to be located in the core of the network and could be good candidates to be part of the CDS of the network. Moreover, the author aims for a minimum-sized CDS (fewer number of nodes forming the CDS and the core edges connecting the CDS nodes) while using these centrality metrics. The author discusses our approach/algorithm to determine each of these four centrality metrics and run them on six real-world network graphs (ranging from 34 to 332 nodes) representing various domains. The author observes the betweeness centrality-based CDS to be of the smallest size in five of the six networks and the closeness centrality-based CDS to be of the smallest size in the smallest of the six networks and incur the largest size for the remaining networks. Centrality-Based Connected Dominating Sets for Complex Network Graphs		Natarajan Meghanathan	2014	IJITN	10.4018/ijitn.2014040101	network theory;network science;random walk closeness centrality;katz centrality;alpha centrality;machine learning;connected dominating set;centrality;betweenness centrality;correlation	AI	-14.669134048170932	-40.89479957987333	190822
68151343db72713c87f646546d12c84306109643	joint community and structural hole spanner detection via harmonic modularity	community detection;structural hole;social network;modularity;harmonic function	Detecting communities (or modular structures) and structural hole spanners, the nodes bridging different communities in a network, are two essential tasks in the realm of network analytics. Due to the topological nature of communities and structural hole spanners, these two tasks are naturally tangled with each other, while there has been little synergy between them. In this paper, we propose a novel harmonic modularity method to tackle both tasks simultaneously. Specifically, we apply a harmonic function to measure the smoothness of community structure and to obtain the community indicator. We then investigate the sparsity level of the interactions between communities, with particular emphasis on the nodes connecting to multiple communities, to discriminate the indicator of SH spanners and assist the community guidance. Extensive experiments on real-world networks demonstrate that our proposed method outperforms several state-of-the-art methods in the community detection task and also in the SH spanner identification task (even the methods that require the supervised community information). Furthermore, by removing the SH spanners spotted by our method, we show that the quality of other community detection methods can be further improved.	bourne shell;bridging (networking);enigma machine;experiment;interaction;network science;sparse matrix;synergy	Lifang He;Chun-Ta Lu;Jiaqi Ma;Jianping Cao;Linlin Shen;Philip S. Yu	2016		10.1145/2939672.2939807	harmonic function;simulation;modularity;machine learning;modularity;social network	ML	-14.267214307989864	-44.71615989499551	191010
f9f4a53a0d352aff6158a62f2b2b1b0aecf4569f	an edge-based clustering algorithm to detect social circles in ego networks	graph clustering;ego networks;link community;social circles	Organizing users’ friends in personal social networks, i.e., ego networks, into circles is an important task for online social networks. Social networking sites allow users to manually categorize their friends into social circles. However, it is time consuming and does not update automatically as a user adds more friends. In this paper, we propose an edge-based clustering algorithm to detect social circles in ego networks automatically. Firstly, we reconstruct ego networks by predicting the missing links. Then, we define the similarity of adjacent edges and cluster edges by single-linkage hierarchical clustering algorithm. Finally, we label each circle by abstracting its common properties to explain why this circle forms. The experimental results demonstrate it is a better way to characterize social circles from the respect of edges. Our algorithm outperforms the link community algorithm and low-rank embedding algorithm in terms of accuracy, and is more efficient than the probabilistic model algorithm. Our proposed method is validated as an effective algorithm in identifying social circles.	algorithm;categorization;cluster analysis;contact list;emergence;hierarchical clustering;linkage (software);social network;statistical model;time complexity	Yu Wang;Lin Gao	2013	JCP	10.4304/jcp.8.10.2575-2582	computer science;artificial intelligence;machine learning;data mining;clustering coefficient;mathematics;world wide web	ML	-15.42960470909034	-43.706960105041446	191148
c450c82dfd33f8729c2be724b34d8ef82c8a3bd4	modeling musical taste evolution with recurrent neural networks		Finding the music of the moment can often be a challenging problem, even for well-versed music listeners. Musical tastes are constantly in flux, and the problem of developing computational models for musical taste dynamics presents a rich and nebulous problem space. A variety of factors all play some role in determining preferences (e.g., popularity, musicological, social, geographical, generational), and these factors vary across different listeners and contexts. In this paper, we leverage a massive dataset on internet radio station creation from a large music streaming company in order to develop computational models of listener taste evolution. We delve deep into the complexities of this domain, identifying some of the unique challenges that it presents, and develop a model utilizing recurrent neural networks. We apply our model to the problem of next station prediction and show that it not only outperforms several baselines, but excels at long tail music personalization, particularly by learning the long-term dependency structure of listener music preference evolution.	artificial neural network;baseline (configuration management);computation;computational model;dependency grammar;evolution;information gain in decision trees;kullback–leibler divergence;learning to rank;long tail;neural networks;personalization;problem domain;radio broadcasting;random neural network;recurrent neural network;streaming media	Massimo Quadrana;Marta Reznáková;Tao Ye;Erik Schmidt;Hossein Vahabi	2018	CoRR		personalization;musical;computer science;leverage (finance);data mining;computational model;internet radio;machine learning;recurrent neural network;popularity;artificial intelligence	AI	-17.986504479443283	-51.82973956829668	191238
6a5ae0e083ab69153ce395874c8dddcd830dfcfd	influence at scale: distributed computation of complex contagion in networks	influence maximization;distributed computation;social networks;mapreduce;submodular	We consider the task of evaluating the spread of influence in large networks in the well-studied independent cascade model. We describe a novel sampling approach that can be used to design scalable algorithms with provable performance guarantees. These algorithms can be implemented in distributed computation frameworks such as MapReduce. We complement these results with a lower bound on the query complexity of influence estimation in this model. We validate the performance of these algorithms through experiments that demonstrate the efficacy of our methods and related heuristics.	algorithm;complex contagion;computation;decision tree model;distributed computing;experiment;heuristic (computer science);mapreduce;provable security;sampling (signal processing);scalability;whole earth 'lectronic link	Brendan Lucier;Joel Oren;Yaron Singer	2015		10.1145/2783258.2783334	computer science;theoretical computer science;submodular set function;machine learning;distributed computing;social network	ML	-15.685573788934466	-43.34737099238986	191443
93735ada8353b1fa0faa9e122efcba8337582c8b	the weighting analysis of influence factor in clinical skin physiology assessment via rough set method		The main purpose of this paper is to study the weighting analysis of clinical skin physiology assessment. First of all, we analyze skin's physiological factors, which include four factors: Cutometer, PH value, Sebumeter and Mx16. A detailed description of each influence factors is offered and based on the analysis of 61 experimental objects’ testing numerical values. Then, using rough set method in soft computing theory and the weighting model, we derived the factors’ weighting and relational of skin physical system from the skin characteristic analysis of diverse-aged research objects. This paper also applies Matlab to develop a complete human-machine interface type of toolbox in order to support the calculation and verification the huge data. Finally, some further suggestions are indicated for the research in the future.	matlab;numerical analysis;ph (complexity);rough set;soft computing;user interface	Hui-Yi Liang;Ya-Ting Lee;Mei-Li You;Kun-Li Wen	2009		10.1007/978-3-642-10616-3_1	computer science;artificial intelligence;data mining;operations research	HCI	-10.549966955070063	-47.94811862025854	191578
7862807baa2cc756dfc4a4cdcbdcb55e8ce3c86e	substructure similarity search in graph databases	database system;frequencies;quantiles;sorting;complex structure;sliding windows;data streams;indexation;graphics processors;feature selection;memory bandwidth;similarity search	Advanced database systems face a great challenge raised by the emergence of massive, complex structural data in bioinformatics, chem-informatics, and many other applications. The most fundamental support needed in these applications is the efficient search of complex structured data. Since exact matching is often too restrictive, similarity search of complex structures becomes a vital operation that must be supported efficiently.In this paper, we investigate the issues of substructure similarity search using indexed features in graph databases. By transforming the edge relaxation ratio of a query graph into the maximum allowed missing features, our structural filtering algorithm, called Grafil, can filter many graphs without performing pairwise similarity computations. It is further shown that using either too few or too many features can result in poor filtering performance. Thus the challenge is to design an effective feature set selection strategy for filtering. By examining the effect of different feature selection mechanisms, we develop a multi-filter composition strategy, where each filter uses a distinct and complementary subset of the features. We identify the criteria to form effective feature sets for filtering, and demonstrate that combining features with similar size and selectivity can improve the filtering and search performance significantly. Moreover, the concept presented in Grafil can be applied to searching approximate non-consecutive sequences, trees, and other complicated structures as well.	approximation algorithm;bioinformatics;cluster analysis;community informatics;computation;emergence;feature selection;filter (signal processing);graph database;linear programming relaxation;rendering (computer graphics);selectivity (electronic);similarity search	Xifeng Yan;Philip S. Yu;Jiawei Han	2005		10.1145/1066157.1066244	quantile;computer science;sorting;machine learning;frequency;pattern recognition;data mining;generalized complex structure;database;feature selection;memory bandwidth	DB	-7.077767001856218	-39.954321297622265	191631
1a8cf53ecb23db9aa0e809f8dc6cd908203bdb79	counting triangles in large graphs by random sampling	sampling methods computational complexity graph theory;approximation algorithms;random sampling;data engineering;approximation algorithms knowledge engineering data engineering classification algorithms sampling methods monitoring heuristic algorithms;monitoring;triangle counting;heuristic algorithms;classification algorithms;sublinear time algorithms large graphs random sampling algorithm exact algorithm approximate algorithm triangle counting algorithms;sampling methods;random sampling triangle counting;knowledge engineering	The problem of counting triangles in graphs has been well studied in the literature. However, all existing algorithms, exact or approximate, spend at least linear time in the size of the graph (except a recent theoretical result), which can be prohibitive on today's large graphs. Nevertheless, we observe that the ideas in many existing triangle counting algorithms can be coupled with random sampling to yield potentially sublinear-time algorithms that return an approximation of the triangle count without looking at the whole graph. This paper makes these random sampling algorithms more explicit, and presents an experimental and analytical comparison of different approaches, identifying the best performers among a number of candidates.	approximation algorithm;monte carlo method;sampling (signal processing);time complexity	Bin Wu;Ke Yi;Zhenguo Li	2016	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2016.2556663	random graph;sampling;mathematical optimization;combinatorics;probabilistic analysis of algorithms;computer science;artificial intelligence;theoretical computer science;knowledge engineering;mathematics;approximation algorithm	DB	-10.378627156397847	-39.620999448585536	191632
10cf332d4f4bca1ffdddb6fedc9d445d663439fe	on the optimal dissemination of information in social networks	optimisation;alternating direction method of multipliers admm;social networks;misinformation;information dissemination;social networking online;optimization;leader selection;combinatorial mathematics;stochastic matrices	We consider social networks which contain agents that spread misinformation and refuse to change their opinion. For a fixed number of information disseminating agents, we formulate an optimization problem to find their optimal location within the network such that the spread of misinformation is countered and public awareness is maximally raised. Once the location of the information disseminators is identified, we examine how to maximize their social influence either by creating new social links or by strengthening their existing links. Our formulation leads to a combinatorial optimization problem that is solved using the alternating direction method of multipliers. Illustrative examples are provided to demonstrate our theoretical developments.	augmented lagrangian method;combinatorial optimization;mathematical optimization;optimization problem;regular expression;social network;steady state	Makan Fardad;Xi Zhang;Fu Lin;Mihailo R. Jovanovi&#x0107;	2012	2012 IEEE 51st IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2012.6426070	computer science;artificial intelligence;mathematics;management science;social network	Vision	-16.822620902774695	-43.32394709756041	191827
e5a89bb83f53112a3ecb503b0b529c6b2e8c6d62	statistical reliability and path diversity based pagerank algorithm improvements		In this paper we present new improvement ideas of the original PageRank algorithm. The first idea is to introduce an evaluation of the statistical reliability of the ranking score of each node based on the local graph property and the second one is to introduce the notion of the path diversity. The path diversity can be exploited to dynamically modify the increment value of each node in the random surfer model or to dynamically adapt the damping factor. We illustrate the impact of such modifications through examples and simple simulations.	algorithm;damping factor;graph property;pagerank;simulation	Dohy Hong	2011	CoRR		computer science;theoretical computer science;machine learning;world wide web;algorithm	ML	-11.246155000566393	-42.79146381158296	191934
7bb4ab3f4437dd30889751880f944a68bec00dce	correlation coefficient analysis of centrality metrics for complex network graphs		The high-level contribution of this paper is a correlation coefficient analysis of the well-known centrality metrics (degree centrality, eigenvector centrality, betweenness centrality, closeness centrality, farness centrality and eccentricity) for network analysis studies on real-world network graphs representing diverse domains (ranging from 34 nodes to 332 nodes). We observe the two degree-based centrality metrics (degree and eigenvector centrality) to be highly correlated across all the networks studied. There is predominantly a moderate level of correlation between any two of the shortest paths-based centrality metrics (betweenness, closeness, farness and eccentricity) and such a correlation is consistently observed across all the networks. Though we observe a poor correlation between a degree-based centrality metric and a shortest-path based centrality metric for regular random networks, as the variation in the degree distribution of the vertices increases (i.e., as the network gets increasingly scale-free), the correlation coefficient between the two classes of centrality metrics increases.	centrality;coefficient;complex network	Natarajan Meghanathan	2015		10.1007/978-3-319-18503-3_2	random walk closeness centrality;katz centrality;alpha centrality;centrality	NLP	-15.513268456313785	-40.64476933545494	192065
0fdb2a00b270f89af7719534c1d33e845ccaf591	approximate spreading activation for efficient knowledge retrieval from large datasets	large dataset;spreading activation	This paper describes a new approximate implementation of Spreading Activation (SA) for knowledge selection in very large datasets. SA is used to prime relevant knowledge domains and reduce considerably the graph queried and therefore the query time. The method is based on the representation of the dataset as a sparse matrix of integers and the application on the corresponding graph of fast path searching algorithm which counts the number of times a node is reached following independent paths. The algorithm is implemented and tested on a CUDA enabled GPU on a dataset containing about 100 million of nodes and 850 million of statements. The numerical evaluation indicates that the approximate SA mechanism proposed is quite promising for real time applications achieving the activation of about 64 million nodes and 374 million of statements in about 5.5 seconds.	spreading activation	Maurice Grinberg;Vladimir Haltakov;Hristo Stefanov	2010		10.3233/978-1-60750-692-8-326	computer science;theoretical computer science;machine learning;data mining;spreading activation;algorithm	ML	-8.701354503296088	-40.576231962098525	192110
7435307dfa34bbf683d35d61d3e5482ccbbbec2b	empirical analysis of the coauthorship network based on dblp	empirical analysis digital bibliography and library project computer science discipline small world network approximate exponential distribution power law distribution author productivity complex network dblp library article information extraction collaboration pattern coauthorship network;empirical analysis dblp complex network coauthorship network;network theory graphs bibliographies digital libraries exponential distribution;abstracts correlation	To identify and analyze the collaboration pattern between scientists in computer science discipline, we constructed a new coauthorship network by extracting the article information of the whole year of 2012 from DBLP library. The nodes are scientists and two scientists are connected if they have coauthored a paper. We study the network in depth by the theory of complex network. The empirical analysis shows that the productivity of authors follows a power-law distribution and the size of collaboration follows an approximate exponential distribution. Moreover, we find that the network is a small-world network and have no apparent scale-free property. Finally, four different indicators are employed to study the impact of authors.	approximation algorithm;complex network;computer science;dbl-browser;node (computer science);time complexity	Yanxia Liu;Bin Lu;Qin Zhang	2013	2013 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2013.6890752	computer science;data science	ML	-17.593267798292512	-40.63582176022206	192236
ab20b24788cbbeab7d32e6d47aacf1afffe63396	incremental graph matching for situation awareness	graph theory;truncated search tree approach incremental graph matching problem situation awareness subgraph isomorphism problem situation assessment data fusion bounded incremental algorithm data graph trust approach;complexity theory;situation assessment graph matching;data fusion;data mining;graph matching;tree searching graph theory pattern matching sensor fusion;incremental graph matching problem;truncated search tree approach;pattern matching;heuristic algorithms;subgraph isomorphism problem;bounded incremental algorithm;data graph;situation awareness;trust approach;humans;search problems;incremental algorithm;sensor fusion;tree searching;humans runtime pattern matching data engineering systems engineering and theory performance evaluation fusion power generation decision making pattern analysis;situation assessment;algorithm design and analysis	In this paper, an incremental subgraph matching problem is introduced as an enhancement to a batched inexact subgraph isomorphism for situation assessment in higher levels of data fusion. The procedure is shown to be a bounded incremental algorithm, meaning that its runtime is a function of the size of the change in the data graph. Solution quality results are shown to be equal to that of TruST [5] with large improvements in runtime for graphs even in the size range of thousands of nodes. This new enhancement allows subgraph isomorphism procedures to be applied to new types of fusion problems.	algorithm;computation;discontinuous galerkin method;graph isomorphism;incremental search;matching (graph theory);state space search;subgraph isomorphism problem;vocabulary	Adam Stotz;Rakesh Nagi;Moises Sudit	2009	2009 12th International Conference on Information Fusion		factor-critical graph;combinatorics;machine learning;pattern recognition;subgraph isomorphism problem;mathematics;induced subgraph isomorphism problem;maximum common subgraph isomorphism problem	Robotics	-7.749438063655707	-38.862214075649305	192339
606b64926c10fc1ceff0da40f46eaedce97ddc1f	a link prediction algorithm based on ant colony optimization	ant colony optimization;complex networks;link prediction	The problem of link prediction has attracted considerable recent attention from various domains such as sociology, anthropology, information science, and computer sciences. In this paper, we propose a link prediction algorithm based on ant colony optimization. By exploiting the swarm intelligence, the algorithm employs artificial ants to travel on a logical graph. Pheromone and heuristic information are assigned in the edges of the logical graph. Each ant chooses its path according to the value of the pheromone and heuristic information on the edges. The paths the ants traveled are evaluated, and the pheromone information on each edge is updated according to the quality of the path it located. The pheromone on each edge is used as the final score of the similarity between the nodes. Experimental results on a number of real networks show that the algorithm improves the prediction accuracy while maintaining low time complexity. We also extend the method to solve the link prediction problem in networks with node attributes, and the extended method also can detect the missing or incomplete attributes of data. Our experimental results show that it can obtain higher quality results on the networks with node attributes than other algorithms.	algorithm;ant colony optimization algorithms;artificial ants;computation;computer science;data mining;display resolution;heuristic;information science;mathematical optimization;randomness;swarm intelligence;time complexity	Bolun Chen;Ling Chen	2014	Applied Intelligence	10.1007/s10489-014-0558-5	ant colony optimization algorithms;computer science;artificial intelligence;machine learning;data mining	AI	-14.478201354120259	-43.40423599474488	192687
53ac0b5ae27b8b6b10107c384a6cde229eb10cb3	molecular tiling and dna self-assembly	molecular kinetics;dna self-assembly;tiling;dna nanotechnology;graph covering.;kinetics;self assembly	We examine hypotheses coming from the physical world and address new mathematical issues on tiling. We hope to bring to the attention of mathematicians the way that chemists use tiling in nanotechnology, where the aim is to propose building blocks and experimental protocols suitable for the construction of 1D, 2D and 3D macromolecular assembly. We shall especially concentrate on DNA nanotechnology, which has been demonstrated in recent years to be the most effective programmable self-assembly system. Here, the controlled construction of supramolecular assemblies containing components of fixed sizes and shapes is the principal objective. We shall spell out the algorithmic properties and combinatorial constraints of ”physical protocols”, to bring the working hypotheses of chemists closer to a mathematical formulation.	self-assembly;supramolecular assembly;tiling window manager	Alessandra Carbone;Nadrian C. Seeman	2004		10.1007/978-3-540-24635-0_5	engineering;nanotechnology;algorithm	Robotics	-7.764226948555339	-48.2816005602818	192788
6ea937da7ecfcfb4191074955e91c7e42b86276c	reinforced co-training		Co-training is a popular semi-supervised learning framework to utilize a large amount of unlabeled data in addition to a small labeled set. Co-training methods exploit predicted labels on the unlabeled data and select samples based on prediction confidence to augment the training. However, the selection of samples in existing co-training methods is based on a predetermined policy, which ignores the sampling bias between the unlabeled and the labeled subsets, and fails to explore the data space. In this paper, we propose a novel method, Reinforced Co-Training, to select high-quality unlabeled samples to better co-train on. More specifically, our approach uses Q-learning to learn a data selection policy with a small labeled dataset, and then exploits this policy to train the co-training classifiers automatically. Experimental results on clickbait detection and generic text classification tasks demonstrate that our proposed method can obtain more accurate text classification results.	agent-based model;clickbait;co-training;dataspaces;document classification;futures studies;q-learning;sampling (signal processing);semi-supervised learning;semiconductor industry;statistical classification;supervised learning	Jiawei Wu;Lei Li;William Yang Wang	2018			machine learning;artificial intelligence;co-training;computer science;exploit;sampling bias	NLP	-16.588323125654735	-49.148189203684026	193044
326f14110c33f0d5a813bbb0b24a2b8356be30df	click prediction for web image reranking using multimodal sparse coding	graph theory;optimisation;image coding;large scale database web image reranking algorithm multimodal sparse coding text based image search textual metadata visual content visual feature extraction semantic similarities click information search queries click based method click data multimodal hypergraph learning based sparse coding method image click prediction local smoothness optimization procedure voting strategy binary event;text analysis feature extraction graph theory image coding image retrieval internet learning artificial intelligence meta data optimisation;text analysis;internet;feature extraction;image coding image reconstruction vectors encoding visualization optimization laplace equations;meta data;learning artificial intelligence;image retrieval	Image reranking is effective for improving the performance of a text-based image search. However, existing reranking algorithms are limited for two main reasons: 1) the textual meta-data associated with images is often mismatched with their actual visual content and 2) the extracted visual features do not accurately describe the semantic similarities between images. Recently, user click information has been used in image reranking, because clicks have been shown to more accurately describe the relevance of retrieved images to search queries. However, a critical problem for click-based methods is the lack of click data, since only a small number of web images have actually been clicked on by users. Therefore, we aim to solve this problem by predicting image clicks. We propose a multimodal hypergraph learning-based sparse coding method for image click prediction, and apply the obtained click data to the reranking of images. We adopt a hypergraph to build a group of manifolds, which explore the complementarity of different features through a group of weights. Unlike a graph that has an edge between two vertices, a hyperedge in a hypergraph connects a set of vertices, and helps preserve the local smoothness of the constructed sparse codes. An alternating optimization procedure is then performed, and the weights of different modalities and the sparse codes are simultaneously obtained. Finally, a voting strategy is used to describe the predicted click as a binary event (click or no click), from the images' corresponding sparse codes. Thorough empirical studies on a large-scale database including nearly 330 K images demonstrate the effectiveness of our approach for click prediction when compared with several other methods. Additional image reranking experiments on real-world data show the use of click prediction is beneficial to improving the performance of prominent graph-based image reranking algorithms.	algorithm;clickstream;code;complementarity theory;experiment;extraction;graph - visual representation;image retrieval;mathematical optimization;multimodal interaction;neural coding;relevance;sparse matrix;text-based (computing);vertex (geometry);web search engine;web search query;weight	Jun Yu;Yong Rui;Dacheng Tao	2014	IEEE Transactions on Image Processing	10.1109/TIP.2014.2311377	the internet;feature extraction;image retrieval;computer science;graph theory;machine learning;pattern recognition;metadata;information retrieval	Web+IR	-16.7738391722094	-48.13926358867865	193190
4abda2f4b33f89ef9d38e83a3b12e1a792983c90	network science: a new paradigm shift	sociology network science paradigm shift mathematics physics biology electrical engineering computer engineering;social network services;computer engineering;mathematics;biological system modeling;biology;electrical and computer engineering;physics;internet;paradigm shift;internet social network services telecommunication network management biological system modeling;network science;electrical engineering;network theory graphs;sociology;telecommunication network management	We argue that we are witnessing a paradigm shift in science, which could be referred to as network science. Some of the fundamental findings and open problems in network science are reviewed. Since most questions are still largely open, it is expected that the network science will still attract researches with different background: mathematics, physics, biology, electrical and computer engineering, and sociology, to mention only a few.	complex systems;computer engineering;network science;programming paradigm;reverse engineering	Ljupco Kocarev;Visarath In	2010	IEEE Network	10.1109/MNET.2010.5634436	network science;paradigm shift;the internet;computer science;artificial intelligence	Theory	-18.330998937265832	-39.69608905021051	193245
a4f0e2618e1a576fe75e8ad06ae0b50f631ca7fc	a novel algorithm for community detection and influence ranking in social networks	social network services;social networking online network theory graphs;communities vectors algorithm design and analysis social network services conferences benchmark testing clustering algorithms;vectors;community detection undirected networks influence vector influence cascade model network topology influence based connectivity social networks influence ranking;clustering algorithms;communities;algorithm design and analysis;benchmark testing;conferences	Community detection and influence analysis are significant notions in social networks. We exploit the implicit knowledge of influence-based connectivity and proximity encoded in the network topology, and propose a novel algorithm for both community detection and influence ranking. Using a new influence cascade model, the algorithm generates an influence vector for each node, which captures in detail how the node's influence is distributed through the network. Similarity in this influence space defines a new, meaningful and refined connectivity measure for the closeness of any pair of nodes. Our approach not only differentiates the influence ranking but also effectively finds communities in both undirected and directed networks, and incorporates these two important tasks into one integrated framework. We demonstrate its superior performance with extensive tests on a set of real-world networks and synthetic benchmarks.	algorithm;benchmark (computing);centrality;cluster analysis;experiment;graph (discrete mathematics);network topology;simulation;social network;synthetic intelligence;topological graph theory;weighted network	Wenjun Wang;William Nick Street	2014	2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)	10.1109/ASONAM.2014.6921641	algorithm design;benchmark;computer science;theoretical computer science;machine learning;data mining;cluster analysis;world wide web;algorithm	DB	-14.91641482984413	-42.54340666509889	193268
4c7f27125ef51451dc6e0de7ace829308dcffc1c	frequent conceptual links and link-based clustering: a comparative analysis of two clustering techniques	pattern clustering;frequent conceptual links;frequent patterns;data mining;social network;social network mining;clustering;knowledge acquisition;social networking online	Numerous social network mining methods have been proposed until now for addressing social mining tasks and specially searching for communities or frequent social patterns. However, the degree of complementarity of these methods has been very little studied. In this paper, we focus on two knowledge extraction processes in social networks: a link-based clustering that may extract social communities and a recent approach that searches for frequent conceptual link involving both clustering and search for frequent social patterns. We explore how the models extracted by each method may match and which potential useful knowledge they may provide. Our objective is to evaluate the potential relationships between communities and frequent conceptual links. For this purpose, we propose a set of measures for evaluating the degree to which these patterns extracted from the same dataset are matching. Our approach is applied on two datasets and demonstrates the importance of considering simultaneously various kinds of knowledge and their complementarity.	cluster analysis;complementarity theory;qualitative comparative analysis;social network	Erick Stattner;Martine Collard	2013	2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)	10.1145/2492517.2492548	social science;computer science;data science;machine learning;data mining;cluster analysis;social network;conceptual clustering	ML	-14.798169079915459	-43.65048549893744	193301
1b2084f46e1133624438f87d3c3edba0bfd053fc	joint implicit and explicit neural networks for question recommendation in cqa services		Community question answering (CQA) services have emerged as a type of popular social platforms. In the social network, experts provide knowledgeable answers to the questions in their domain of expertise, while celebrities publish influential opinions toward the topics led by some questions. Given the large amount of knowledge organized in the format of question–answers, an interesting research problem is to recommend questions to users so as to maximize their engagements with the platform. However, recommending questions in CQA services is a non-trivial task. Data sources in the CQA services are of different types. It is challenging to incorporate heterogeneous information for the recommendation task. Furthermore, data sparsity is an inherent problem in such platforms. In this paper, we propose a model that is able to jointly model both implicit and explicit information for question recommendation. The model integrates multiple data sources and addresses the problem of data heterogeneity. In the proposed model, we dynamically discover latent user groups and incorporate those hierarchical information to bridge the semantic gaps among users in the shared latent space. We evaluate the proposed model on two real-world datasets, and demonstrate that our model outperforms the state-of-the-art alternatives by a large margin. We also investigate different structures of the proposed model to study the effects of different data sources.		Hongkui Tu;Jiahui Wen;Aixin Sun;Xiaodong Wang	2018	IEEE Access	10.1109/ACCESS.2018.2881119	task analysis;artificial neural network;data mining;feature extraction;semantics;distributed computing;publication;computer science;social network;question answering;data modeling	Web+IR	-17.921200071189027	-47.59826635169969	193345
978e11fdc361ba2ec33019c3ba3b8a80d8874a72	interpreting network communicability with stochastic models and data		The recently introduced concept of dynamic communicability is a valuable tool for ranking the importance of nodes in a temporal network. Two metrics, broadcast score and receive score, were introduced to measure the centrality of a node with respect to a model of contagion based on time-respecting walks. This article examines the temporal and structural factors influencing these metrics by considering a versatile stochastic temporal network model. We analytically derive formulae to accurately predict the expectation of the broadcast and receive scores when one or more columns in a temporal edge-list are shuffled. These methods are then applied to two publicly available data-sets and we quantify how much the centrality of each individual depends on structural or temporal influences. From our analysis we highlight two practical contributions: a way to control for temporal variation when computing dynamic communicability, and the conclusion that the broadcast and receive scores can, under a range of circumstances, be replaced by the row and column sums of the matrix exponential of a weighted adjacency matrix given by the data.	adjacency matrix;centrality;column (database);network model;stochastic process;the matrix;time complexity	Ewan Colman;Nathaniel Charlton	2016	CoRR		adjacency matrix;data mining;matrix exponential;centrality;network model;ranking;broadcasting;mathematics;data set	Vision	-15.606200859995155	-40.950820248505416	193356
5046eb56512618aa04538dc351c2617bf808fad3	leveraging history for faster sampling of online social networks	cs si;physics soc ph	With vast amount of data available on online social networks, how to enable efficient analytics over such data has been an increasingly important research problem. Given the sheer size of such social networks, many existing studies resort to sampling techniques that draw random nodes from an online social network through its restrictive web/API interface. While these studies differ widely in analytics tasks supported and algorithmic design, almost all of them use the exact same underlying technique of random walk a Markov Chain Monte Carlo based method which iteratively transits from one node to its random neighbor. Random walk fits naturally with this problem because, for most online social networks, the only query we can issue through the interface is to retrieve the neighbors of a given node (i.e., no access to the full graph topology). A problem with random walks, however, is the “burn-in” period which requires a large number of transitions/queries before the sampling distribution converges to a stationary value that enables the drawing of samples in a statistically valid manner. In this paper, we consider a novel problem of speeding up the fundamental design of random walks (i.e., reducing the number of queries it requires) without changing the stationary distribution it achieves thereby enabling a more efficient “drop-in” replacement for existing sampling-based analytics techniques over online social networks. Technically, our main idea is to leverage the history of random walks to construct a higher-ordered Markov chain. We develop two algorithms, Circulated Neighbors and Groupby Neighbors Random Walk (CNRW and GNRW) and rigidly prove that, no matter what the social network topology is, CNRW and GNRW offer better efficiency than baseline random walks while achieving the same stationary distribution. We demonstrate through extensive experiments on real-world social networks and synthetic graphs the superiority of our techniques over the existing ones.	algorithm;application programming interface;baseline (configuration management);burn-in;directed graph;experiment;fits;markov chain monte carlo;monte carlo method;network topology;sampling (signal processing);social network;stationary process;synthetic data;topological graph theory;web api;world online	Zhuojie Zhou;Nan Zhang;Gautam Das	2015	PVLDB	10.14778/2794367.2794373	random graph;computer science;theoretical computer science;machine learning;data mining;database;mathematics;statistics	DB	-10.934291213967443	-43.23740865504721	193418
6b726e8fa9188967b44b3d32bad851a9729f1654	spti: efficient answering the shortest path query on large graphs	community;social network services;graph theory;shortest path;query processing;information science;trunk;indexing;distance query;communities;labeling	The shortest path distance computing between any two vertices in large scale graphs is an essential problem, e.g., social network analysis, route planning in road map, and has been studied over the past few decades. To answer the query efficiently, the index is widely used. However, when it comes to large scale graphs composed of millions of vertices and edges, they suffer from drawbacks of scalability. To solve these problems, we put forward SPTI, an indexing and query processing framework for the shortest path distance computing. We only select a small part of vertices from the original graph to construct index, instead of all of them. It not only can reduce the construction time and index size dramatically, but also can help speed up the-state-of-the-art approaches significantly. Our experimental results demonstrate that the SPTI can perform on graphs with millions of vertices/edges and offers apparent performance improvement over existing approaches in term of index construction time, index size and query time.	algorithm;breadth-first search;database;network theory;pathfinding;scsi pass through interface;scalability;shortest path problem;social network analysis;vertex (geometry);vertex (graph theory)	Yifei Zhang;Guoren Wang	2013	2013 IEEE International Congress on Big Data	10.1109/BigData.Congress.2013.34	sargable;query optimization;longest path problem;theoretical computer science;machine learning;data mining;mathematics;shortest path problem;distance	DB	-9.575391373966555	-39.98312793119315	193870
fd514250167fcfd6d3ba6408942bc315a4cebf4c	a multilevel approach for overlapping community detection	community detection;complex networks;multilevel algorithms;social networking online computational complexity graph theory large scale systems network theory graphs;multilevel algorithms complex networks community detection overlapping community;overlapping community;undirected graph multilevel approach real world complex networks overlapping community structure network vertex crisp overlapping community detection problem computational costs large scale networks time impact analysis solution quality analysis;communities partitioning algorithms accuracy radio frequency facebook proposals	Many real world complex networks have an a overlapping community structure, in which a vertex belongs to one or more communities. Numerous approaches for crisp overlapping community detection were proposed in the literature, most of them have a good accuracy but their computational costs are considerably high and infeasible for large-scale networks. Since the multilevel approach has not been previously applied to deal with overlapping communities detection problem, in this paper we propose an adaptation of this approach to tackle the detection problem to overlapping communities case. The goal is to analyze the time impact and the quality of solution of our multilevel strategy regarding to traditional algorithms. Our experiments show that our proposal consistently produces good performance compared to single-level algorithms and in less time.	analysis of algorithms;complex network;computation;experiment;greedy algorithm;multi-level cell;refinement (computing);vertex (graph theory)	Alan Valejo;Jorge Carlos Valverde-Rebaza;Alneu de Andrade Lopes	2014	2014 Brazilian Conference on Intelligent Systems	10.1109/BRACIS.2014.76	computer science;theoretical computer science;machine learning;data mining	AI	-13.582748377241222	-42.707498485861755	194173
2f455e70875a6a0f4309eebb866fffe15951c7d0	towards the analysis of co-authorship networks by iterative spectral partitioning	graph theory;pattern clustering data mining graph theory iterative methods matrix algebra;pattern clustering;social network services communities partitioning algorithms clustering algorithms cost accounting vectors joints;spectral decomposition;dblp;algebraic connectivity;matrix analysis;matrix algebra;dblp spectral partitioning fiedler vector co authorship;data mining;spectral clustering;characteristic valuation coauthorship network analysis iterative spectral partitioning graph analysis matrix analysis spectral clustering data mining data structures algebraic connectivity;iterative methods;spectral partitioning;fiedler vector;co authorship	Spectral partitioning is a well known method in the area of graph and matrix analysis. Several approaches based on spectral partitioning and spectral clustering were used to detect structures and mine data from real world networks. In this paper, we use a simple spectral decomposition to analyze a co-authorship network. We use a straightforward approach based on algebraic connectivity and characteristic valuation and show that even this simple form of spectral partitioning is useful for the analysis of relations and communities in a co-authorship networks.	algebraic connectivity;binary space partitioning;cluster analysis;iterative method;matrix analysis;spectral clustering;value (ethics)	Václav Snásel;Pavel Krömer;Jan Platos	2011	2011 11th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2011.6121635	algebraic connectivity;combinatorics;discrete mathematics;graph theory;theoretical computer science;mathematics	Robotics	-11.467689506871887	-42.25057973244676	194237
be636066f7b8ede07915d1113743881818256333	symmetry breaking in cyclic competition by niche construction	competitive hierarchy;lyapunov exponent;cellular automata;spatial heterogeneity;trade off	Niche construction theory, which portrays organisms as active agents that modify their environment rather than mere passive entities selected by their environment, has received increasing attention in ecology and evolutionary biology. Here, we investigate the ecological consequences of niche construction in the system of three cyclically competing metapopulations, engaging a rock-scissors-paper game. Using cellular automata, we detected a variety of dynamic behaviors, including damped oscillation, periodical fluctuation and stage equilibrium, and the system transformed from disorder to order with gradually increasing niche-constructing intensity. Increasing niche-constructing intensity of a species, counterintuitively, reduced its own occupancy, but increased that of its inferior competitor. These species displayed interesting ripples in the two-dimension lattice space, with the pattern sensitive to the symmetry of competition intensity and other vital rates. Spatial heterogeneity induced by niche construction, together with the competition hierarchy, formed a stable and fixed range for each species with clear boundaries. Our results highlighted the necessity of investigating the adaptive dynamics of niche constructing traits to better understand the eco-evolutionary consequence of niche construction.	niche blogging;symmetry breaking	Xiaozhuo Han;Baoying Chen;Cang Hui	2016	Applied Mathematics and Computation	10.1016/j.amc.2016.02.056	cellular automaton;trade-off;mathematics;lyapunov exponent;spatial heterogeneity	Theory	-4.602233935331209	-46.40795815336828	194266
a9706bb650568d58c182fbb024bc39907b26bc42	modeling social strength in social media community via kernel-based learning	social relationship;heterogeneous data;social ties;social network;photo sharing;kernel based learning;social behavior;social networks;interest groups;social media;learning to rank	Modeling continuous social strength rather than conventional binary social ties in the social network can lead to a more precise and informative description of social relationship among people. In this paper, we study the problem of social strength modeling (SSM) for the users in a social media community, who are typically associated with diverse form of data. In particular, we take Flickr---the most popular online photo sharing community---as an example, in which users are sharing their experiences through substantial amounts of multimodal contents (e.g., photos, tags, geo-locations, friend lists) and social behaviors (e.g., commenting and joining interest groups). Such heterogeneous data in Flickr bring opportunities yet challenges to the research community for SSM. One of the key issues in SSM is how to effectively explore the heterogeneous data and how to optimally combine them to measure the social strength. In this paper, we present a kernel-based learning to rank framework for inferring the social strength of Flickr users, which involves two learning stages. The first stage employs a kernel target alignment algorithm to integrate the heterogeneous data into a holistic similarity space. With the learned kernel, the second stage rectifies the pair-wise learning to rank approach to estimating the social strength. By learning the social strength graph, we are able to conduct collaborative recommendation and collective classification. The promising results show that the learning-based approach is effective for SSM. Despite being focused on Flickr, our technique can be applied to model social strength of users in any other social media community.	algorithm;flickr;holism;information;kernel (operating system);learning to rank;multimodal interaction;social media;social network	Jinfeng Zhuang;Tao Mei;Steven C. H. Hoi;Xian-Sheng Hua;Shipeng Li	2011		10.1145/2072298.2072315	social learning;computer science;social heuristics;machine learning;data mining;world wide web;social computing;learning to rank;social network	ML	-18.82853676192253	-46.62699840023527	194962
f95547a84d10fb60a723038f2b513af45ae98801	label propagation algorithm based on adaptive h index		Label propagation algorithm is a part of semi-supervised learning method, which is widely applied in the field of community partition. The algorithm is simple and fast, especially in the large complex community network. The algorithm shows nearly linear time complexity, but it has great instability and randomness. Many scholars make their improvements on the original label propagation, but most of them are not suitable for large community network discovery, which usually have higher time complexity. Therefore, we propose a label propagation algorithm based on adaptive H index, which improves the stability and accuracy of LPA by using the refined H index as a measure of node importance. Finally, the algorithm is tested by public standard dataset and synthetic benchmark network dataset, and the test result shows that the proposed algorithm has better stability and accuracy than some existing classic algorithms.	label propagation algorithm;software propagation	Xiaoxiang Zhu;Zhengyou Xia	2018		10.1007/978-3-319-93803-5_6	time complexity;randomness;machine learning;computer science;partition (number theory);artificial intelligence;instability;algorithm	Vision	-13.322678223224457	-42.20074833332121	195012
3855ebabb09b711b84361e357006897ca303be2e	bsn: an automatic generation algorithm of social network data	pattern search;automatic generation;social network;biclustering;social network analysis;optimization	In recent years, there has been considerable interest in the analysis of social network data. In this paper, we propose a novel automatic generation algorithm of social network data - the Biclustering Algorithm for Social Network Data algorithm. The algorithm introduces biclustering to social network analysis for automatic identification of associations among a group of actors and entities. The algorithm is different from existing ones in that it employs a combination of min-max and pattern searching procedures to construct hierarchical biclusters and discover the relationships among these actors, in order to easily interpret social network data. The algorithm is not subject to convexity limitations, and does not need to use derivatives information.	algorithm;social network	Lixin Han;Hong Yan	2011	Journal of Systems and Software	10.1016/j.jss.2011.04.025	organizational network analysis;pattern search;social network analysis;computer science;dynamic network analysis;data science;machine learning;data mining;biclustering;social network	Metrics	-13.902203843269803	-43.6951798309297	195412
0854013eec73f06111b8bb26ee2287f21298e5a8	pomelo: accurate and decentralized shortest-path distance estimation in social graphs	graph size;shortest-path distance;graph structure;social graph;decentralized shortest-path distance estimation;different representative social graph;decentralized manner;breadth-first search;novel partial bfs;computational overhead;graph coordinate system;breadth first search;shortest path;coordinate system	Computing the shortest-path distances between nodes is a key problem in analyzing social graphs. Traditional methods like breadth-first search (BFS) do not scale well with graph size. Recently, a Graph Coordinate System, called Orion, has been proposed to estimate shortest-path distances in a scalable way. Orion uses a landmark-based approach, which does not take account of the shortest-path distances between non-landmark nodes in coordinate calculation. Such biased input for the coordinate system cannot characterize the graph structure well. In this paper, we propose Pomelo, which calculates the graph coordinates in a decentralized manner. Every node in Pomelo computes its shortest-path distances to both nearby neighbors and some random distant neighbors. By introducing the novel partial BFS, the computational overhead of Pomelo is tunable. Our experimental results from different representative social graphs show that Pomelo greatly outperforms Orion in estimation accuracy while maintaining the same computational overhead.	breadth-first search;overhead (computing);scalability;shortest path problem;social graph	Zhuo Chen;Yang Chen;Cong Ding;Beixing Deng;Xing Li	2011		10.1145/2018436.2018491	mathematical optimization;breadth-first search;theoretical computer science;coordinate system;machine learning;distance-hereditary graph;shortest path problem	AI	-11.624026459569334	-41.31185873941681	196166
4ea4b53e5a72e418438203fc16ef2c05b0474440	towards the extraction of hierarchical building descriptions from 3d indoor scans	miscellaneous;h 3 3 information storage and retrieval;i 5 m pattern recognition;i 3 m computer graphics;h 3 1 information storage and retrieval;information search and retrieval;content analysis and indexing	We present a new method for the hierarchical decomposition of 3D indoor scans and the subsequent generation of an according hierarchical graph-based building descriptor. The hierarchy consists of four basic levels with according entities, building storey room object. All entities are represented as attributed nodes in a graph and are linked to the upper level entity they are located in. Additionally, nodes of the same level are linked depending on their spatial and topological relationship. The hierarchical description enables easy navigation in the formerly unstructured data, measurement takings, as well as carrying out retrieval tasks that incorporate geometric, topological, and also functional building properties describing e.g. the designated use of single rooms according to the objects it contains. In contrast to previous methods which either focus on the segmentation into rooms or on the recognition of indoor objects, our holistic approach incorporates a rather large spectrum of entities on different semantic levels that are inherent to 3D building representations. In our evaluation we show the feasibility of our method for extraction of hierarchical building descriptions for various tasks using synthetic as well as real world	entity;graph (discrete mathematics);graph edit distance;holism;point cloud;shortest path problem;synthetic intelligence	Sebastian Ochmann;Richard Vock;Raoul Wessel;Reinhard Klein	2014		10.2312/3dor.20141054	computer vision;computer science;data mining;communication	AI	-11.784985935189042	-50.27389552240819	196294
1eca07a013199ee12dda22be08096434cf75a48f	ssde-cluster: fast overlapping clustering of networks using sampled spectral distance embedding and gmms	social network services;graph theory;cluster algorithm;pattern clustering;approximate algorithm;measurement;social networking services;gaussian processes;approximation algorithms;efficient algorithm;social networking online gaussian processes graph theory pattern clustering;online interaction;cpu machine sampled spectral distance embedding cluster overlapping clustering social network clustering online interaction graph metric clustering gaussian mixture model dblp paper paper network;social network;gaussian mixture model;communities clustering algorithms measurement social network services approximation algorithms computer science algorithm design and analysis;social networking online;clustering algorithms;computer science;communities;algorithm design;algorithm design and analysis	Clustering social networks is vital to understanding online interactions and influence. This task becomes more difficult when communities overlap, and when the social networks become extremely large. We present an efficient algorithm for constructing overlapping clusters, (approximately linear). The algorithm first embeds the graph and then performs a metric clustering using a Gaussian Mixture Model (GMM). We evaluate the algorithm on the DBLP paper-paper network which consists of about 1 million nodes and over 30 million edges, we can cluster this network in under 20 minutes on a modest single CPU machine.	algorithm;central processing unit;cluster analysis;google map maker;interaction;mixture model;social network	Malik Magdon-Ismail;Jonathan T. Purnell	2011	2011 IEEE Third Int'l Conference on Privacy, Security, Risk and Trust and 2011 IEEE Third Int'l Conference on Social Computing	10.1109/PASSAT/SocialCom.2011.237	correlation clustering;algorithm design;k-medians clustering;computer science;graph theory;theoretical computer science;canopy clustering algorithm;machine learning;data mining;cluster analysis;statistics	Vision	-10.445229644825318	-41.88204786851213	196321
3408507ad7a57b42fe9c178edc66929194dd04d8	leveraging homomorphisms and bitmaps to enable the mining of embedded patterns from large data trees		Finding interesting tree patterns hidden in large datasets is an important research area that has many practical applications. Along the years, research has evolved from mining induced patterns to mining embedded patterns. Embedded patterns allow for discovering useful relationships which cannot be captured by induced patterns. Unfortunately, previous contributions have focused almost exclusively on mining patterns from a set of small trees. The problem of mining embedded patterns from large data trees has been neglected. This is mainly due to the complexity of this task related to the problem of unordered tree embedding test being NP-Complete. However, mining embedded patterns from large trees is important for many modern applications that arise naturally and in particular with the explosion of big data.	bitmap;graph embedding;tree (data structure)	Xiaoying Wu;Dimitri Theodoratos	2015		10.1007/978-3-319-18120-2_1	computer science;theoretical computer science;data mining;database	HCI	-10.450114575820095	-38.71429045606639	196520
a46b5dca62fa90fe4176adf9b6f490a6bffbaefc	fast incremental simrank on link-evolving graphs	matrix decomposition equations vectors approximation methods accuracy matrix converters heuristic algorithms;conference paper;accuracy;vectors;matrix decomposition;heuristic algorithms;link update algorithm fast incremental simrank node pair similarity measure link evolving graphs hyperlinks graph factorization singular value decomposition svd link updates node pair similarities o r 4 n 2 time estimation low rank approximation fast incremental paradigm simrank update matrix rank one sylvester matrix equation fast incremental algorithm o kn 2 time pruning technique;matrix converters;singular value decomposition approximation theory computational complexity graph theory information retrieval learning artificial intelligence;approximation methods	SimRank is an arresting measure of node-pair similarity based on hyperlinks. It iteratively follows the concept that 2 nodes are similar if they are referenced by similar nodes. Real graphs are often large, and links constantly evolve with small changes over time. This paper considers fast incremental computations of SimRank on link-evolving graphs. The prior approach [12] to this issue factorizes the graph via a singular value decomposition (SVD) first, and then incrementally maintains this factorization for link updates at the expense of exactness. Consequently, all node-pair similarities are estimated in O(r4n2) time on a graph of n nodes, where r is the target rank of the low-rank approximation, which is not negligibly small in practice. In this paper, we propose a novel fast incremental paradigm. (1) We characterize the SimRank update matrix ΔS, in response to every link update, via a rank-one Sylvester matrix equation. By virtue of this, we devise a fast incremental algorithm computing similarities of n2 node-pairs in O(Kn2) time for K iterations. (2) We also propose an effective pruning technique capturing the “affected areas” of ΔS to skip unnecessary computations, without loss of exactness. This can further accelerate the incremental SimRank computation to O(K(nd+|AFF|)) time, where d is the average in-degree of the old graph, and |AFF| (≤ n2) is the size of “affected areas” in ΔS, and in practice, |AFF| ≪ n2. Our empirical evaluations verify that our algorithm (a) outperforms the best known link-update algorithm [12], and (b) runs much faster than its batch counterpart when link updates are small.	algorithm;computation;directed graph;emoticon;hyperlink;iteration;low-rank approximation;programming paradigm;simrank;singular value decomposition;sylvester matrix	Weiren Yu;Xuemin Lin;Wenjie Zhang	2014	2014 IEEE 30th International Conference on Data Engineering	10.1109/ICDE.2014.6816660	mathematical optimization;accuracy and precision;matrix decomposition;statistics	DB	-10.325656775775975	-41.380731394487086	196559
2eeb993b383f9748606825548063e694aeffc6e6	social influence based clustering and optimization over heterogeneous information networks	graph clustering;heterogeneous information network;social influence	Social influence analysis has shown great potential for strategic marketing decision. It is well known that people influence one another based on both their social connections and the social activities that they have engaged in the past. In this article, we develop an innovative and high-performance social influence based graph clustering framework with four unique features. First, we explicitly distinguish social connection based influence (self-influence) and social activity based influence (co-influence). We compute the self-influence similarity between two members based on their social connections within a single collaboration network, and compute the co-influence similarity by taking into account not only the set of activities that people participate but also the semantic association between these activities. Second, we define the concept of influence-based similarity by introducing a unified influence-based similarity matrix that employs an iterative weight update method to integrate self-influence and co-influence similarities. Third, we design a dynamic learning algorithm, called SI-Cluster, for social influence based graph clustering. It iteratively partitions a large social collaboration network into K clusters based on both the social network itself and the multiple associated activity information networks, each representing a category of activities that people have engaged. To make the SI-Cluster algorithm converge fast, we transform sophisticated nonlinear fractional programming problem with respect to multiple weights into a straightforward nonlinear parametric programming problem of single variable. Finally, we develop an optimization technique of diagonalizable-matrix approximation to speed up the computation of self-influence similarity and co-influence similarities. Our SI-Cluster-Opt significantly improves the efficiency of SI-Cluster on large graphs while maintaining high quality of clustering results. Extensive experimental evaluation on three real-world graphs shows that, compared to existing representative graph clustering algorithms, our SI-Cluster-Opt approach not only achieves a very good balance between self-influence and co-influence similarities but also scales extremely well for clustering large graphs in terms of time complexity while meeting the guarantee of high density, low entropy and low Davies--Bouldin Index.	algorithm;approximation;baseline (configuration management);cluster analysis;collaboration graph;computation;computer cluster;converge;display resolution;embnet.journal;fractional programming;iterative method;line graph;mathematical optimization;nonlinear system;parametric programming;self-similarity;semantic similarity;similarity measure;singular value decomposition;social collaboration;social network;time complexity;weight function	Yang Zhou;Ling Liu	2015	TKDD	10.1145/2717314	correlation clustering;fuzzy clustering;social influence;computer science;theoretical computer science;machine learning;data mining;clustering coefficient;mathematics	ML	-14.414065844924009	-43.42435127334578	197569
180b730145943ad061bfc9dbe04426fb9674bd8b	lrbm: a restricted boltzmann machine based approach for representation learning on linked data	social network services;semantic web boltzmann machines data mining knowledge representation learning artificial intelligence;linked data;tensile stress;latent feature representation lrbm restricted boltzmann machine model representation learning linked data mining knowledge discovery deep learning method;data mining;receivers;data models tensile stress receivers data mining feature extraction probability distribution social network services;deep learning;restricted boltzmann machine;feature extraction;probability distribution;representation learning deep learning linked data restricted boltzmann machine;representation learning;data models	Linked data consist of both node attributes, e.g., Preferences, posts and degrees, and links which describe the connections between nodes. They have been widely used to represent various network systems, such as social networks, biological networks and etc. Knowledge discovery on linked data is of great importance to many real applications. One of the major challenges of learning linked data is how to effectively and efficiently extract useful information from both node attributes and links in linked data. Current studies on this topic either use selected topological statistics to represent network structures, or linearly map node attributes and network structures to a shared latent feature space. However, while approaches based on statistics may miss critical patterns in network structure, approaches based on linear mappings may not be sufficient to capture the non-linear characteristics of nodes and links. To handle the challenge, we propose, to our knowledge, the first deep learning method to learn from linked data. A restricted Boltzmann machine model named LRBM is developed for representation learning on linked data. In LRBM, we aim to extract the latent feature representation of each node from both node attributes and network structures, non-linearly map each pair of nodes to the links, and use hidden units to control the mapping. The details of how to adapt LRBM for link prediction and node classification on linked data have also been presented. In the experiments, we test the performance of LRBM as well as other baselines on link prediction and node classification. Overall, the extensive experimental evaluations confirm the effectiveness of the proposed LRBM model in mining linked data.	biological network;critical section;deep learning;experiment;feature learning;feature vector;gaussian (software);graph factorization;hidden variable theory;linked data;machine learning;nonlinear system;restricted boltzmann machine;social network	Kang Li;Jing Gao;Suxin Guo;Nan Du;Xiaoyi Li;Aidong Zhang	2014	2014 IEEE International Conference on Data Mining	10.1109/ICDM.2014.22	probability distribution;linked data structure;data modeling;feature learning;feature extraction;computer science;machine learning;pattern recognition;linked data;data mining;deep learning;data stream mining;stress;restricted boltzmann machine;deep belief network;statistics	ML	-15.104312207215896	-46.69414991126225	197834
e1061cae54eb203c496483c168431a88d2b56816	an integrated computing environment for bio-molecular networks	molecular network	With the development of systems biology, more and more researchers focus on the study of biomolecular networks. In recent years, researchers in different fields have accumulated a large number of biological experimental data and algorithms for analysis and calculation of bio-molecular networks, but these data and methods are relatively independent, difficult to be utilized by biologists. Based on PSE-Bio, a problem solving environment for bioinformatics, this paper describes an integrated computing environment for bio-molecular networks in order to achieve molecular homology analysis, bio-molecular network building, querying, statistics and visualization.	algorithm;bioinformatics;british informatics olympiad;homology (biology);problem solving environment;systems biology	Jiang Xie;Guoyong Mao;Shilin Zhang;Wu Zhang	2010	JCIT		end-user computing;utility computing;autonomic computing	HPC	-5.510639050955325	-51.313065692186726	198144
7c720ab3a223306097de769d63f4f6d87ca7f402	attentive recurrent social recommendation		Collaborative filtering(CF) is one of the most popular techniques for building recommender systems. To alleviate the data sparsity issue in CF, social recommendation has emerged by leveraging social influence among users for better recommendation performance. In these systems, usersu0027 preferences over time are determined by their temporal dynamic interests as well as the general static interests. In the meantime, the complex interplay between usersu0027 internal interests and the social influence from the social network drives the evolution of usersu0027 preferences over time. Nevertheless, traditional approaches either neglected the social network structure for temporal recommendation or assumed a static social influence strength for static social recommendation. Thus, the problem of how to leverage social influence to enhance temporal social recommendation performance remains pretty much open. To this end, in this paper, we present an attentive recurrent network based approach for temporal social recommendation. In the proposed approach, we model usersu0027 complex dynamic and general static preferences over time by fusing social influence among users with two attention networks. Specifically, in the dynamic preference modeling process, we design a dynamic social aware recurrent neural network to capture usersu0027 complex latent interests over time, where a temporal attention network is proposed to learn the temporal social influence over time. In the general static preference modeling process, we characterize each useru0027s static interest by introducing a static social attention network to model the stationary social influence among users. The output of the dynamic preferences and the static preferences are combined together in a unified end-to-end framework for the temporal social recommendation task. Finally, experimental results on two real-world datasets clearly show the superiority of our proposed model compared to the baselines.	artificial neural network;baseline (configuration management);collaborative filtering;end-to-end principle;recommender system;recurrent neural network;social network;sparse matrix;stationary process	Peijie Sun;Le Wu;Meng Wang	2018		10.1145/3209978.3210023	leverage (finance);recommender system;computer science;collaborative filtering;data mining;recurrent neural network;social network;social influence	AI	-18.70789945788563	-46.72740800518102	198329
bc69a8de6cebaae84e6dcd0b5852f2438b9063f9	detecting overlapping temporal community structure in time-evolving networks	cs si;cs lg;stat ml;physics soc ph	We present a principled approach for detecting overlapping temporal community structure in dynamic networks. Our method is based on the following framework: find the overlapping temporal community structure that maximizes a quality function associated with each snapshot of the network subject to a temporal smoothness constraint. A novel quality function and a smoothness constraint are proposed to handle overlaps, and a new convex relaxation is used to solve the resulting combinatorial optimization problem. We provide theoretical guarantees as well as experimental results that reveal community structure in real and synthetic networks. Our main insight is that certain structures can be identified only when temporal correlation is considered and when communities are allowed to overlap. In general, discovering such overlapping temporal community structure can enhance our understanding of realworld complex networks by revealing the underlying stability behind their seemingly chaotic evolution.	combinatorial optimization;complex network;convex optimization;evolving networks;linear programming relaxation;mathematical optimization;optimization problem;sensor;snapshot (computer storage);synthetic intelligence	Yudong Chen;Vikas Kawadia;Rahul Urgaonkar	2013	CoRR		mathematical optimization;machine learning;data mining;mathematics	ML	-13.327676669988449	-40.79290345201544	198398
5da040b0ab36a34fc1b28622c25c54dfd60c77d4	probabilistic near-duplicate detection using simhash	duplicate detection;web pages;large scale;hamming distance;clustering;similarity;simhash;dimensional reduction	This paper offers a novel look at using a dimensionality-reduction technique called simhash to detect similar document pairs in large-scale collections. We show that this algorithm produces interesting intermediate data, which is normally discarded, that can be used to predict which of the bits in the final hash are more susceptible to being flipped in similar documents. This paves the way for a probabilistic search technique in the Hamming space of simhashes that can be significantly faster and more space-efficient than the existing simhash approaches. We show that with 95% recall compared to deterministic search of prior work, our method exhibits 4-14 times faster lookup and requires 2-10 times less RAM on our collection of 70M web pages.	andrei broder;bentley–ottmann algorithm;cp/m;cpm-goms;cluster analysis;computation;cryptographic hash function;curse of dimensionality;cyclomatic complexity;datar;dave grossman (game developer);dimensionality reduction;hamming space;integrated services digital network;international joint conference on artificial intelligence;lookup table;murmurhash;p (complexity);perceptrons: an introduction to computational geometry;r* tree;random-access memory;rounding;scalability;sethi–ullman algorithm;similarity search;symposium on computational geometry;symposium on theory of computing;vldb;www;web crawler;web page;world wide web;yang;lsh	Sadhan Sood;Dmitri Loguinov	2011		10.1145/2063576.2063737	hamming distance;similarity;computer science;artificial intelligence;machine learning;web page;data mining;database;cluster analysis;world wide web;information retrieval	Web+IR	-6.974870543361468	-42.24649176322563	198758
fb205311a5b16d45b7f4af8adee465453aaa56b5	efficient analysis of node influence based on sir model over huge complex networks	expanding knowledge in the information and computing sciences;standards;distributed computing;social networking online complex networks optimisation;integrated circuit modeling;networking and communications;expanding knowledge;approximation methods;twitter;social networks node influence sir model huge complex networks information diffusion epidemic threshold topology based centralities bond percolation process maximization problem greedy search strategy pruning techniques computational efficiency diffusion probability shortest path maximum influence path;twitter standards integrated circuit modeling approximation methods educational institutions;information and computing sciences	Node influence is yet another useful concept to quantify how important each node is over a network and can share the same role that other centrality measures have. It can provide new insight into the information diffusion phenomena such as existence of epidemic threshold which the other topology-based centralities cannot do. We focus on information diffusion process based on the SIR model, and address the problem of efficiently estimating the influence degree for all the nodes in the network. The proposed approach is a further improvement over the existing work of the bond percolation process [1], [2] which was demonstrated to be very effective, i.e., three orders of magnitude faster than direct Monte Carlo simulation, in approximately solving the influence maximization problem under a greedy search strategy. We introduce two pruning techniques which improve computational efficiency by an order of magnitude. This is a generic approach for the SIR model setting and can be instantiated to any specific diffusion model. It does not require any approximations or assumptions to the model, e.g., small diffusion probability, shortest path, maximum influence path, etc., that were needed in the existing approaches. We demonstrate its effectiveness by extensive experiments on two large real social networks. Main finding includes that different network structures have different epidemic thresholds and the node influence can identify influential nodes that the other centrality measures cannot.	approximation;centrality;complex network;computation;entropy maximization;experiment;greedy algorithm;monte carlo method;percolation theory;shortest path problem;simulation;social network;yet another	Masahiro Kimura;Kazumi Saito;Kouzou Ohara;Hiroshi Motoda	2014	2014 International Conference on Data Science and Advanced Analytics (DSAA)	10.1109/DSAA.2014.7058076	computer science;artificial intelligence;theoretical computer science;machine learning	DB	-16.324719224852487	-43.365407830929314	199368
a07d04a802e4ab2a7200fab6b7205a04bc560fe6	q-measures for binary divided networks: an investigation within the field of informetrics	binary divided networks;informetrics;iz none of these;social networking;bridging function;q measures;co author network;proceedings paper;social network analysis;joint authorship;stars;but in this section	Social network theory can be described as a strategy for investigating social structures. Its methods, however, can be applied in many fields, including the information sciences. Here scientists study publication and citation networks, co-citation networks, bibliographic coupling, collaboration structures, web relations and many other forms of social interaction networks (Adamic & Adar, 2003; Newman, 2001; van Raan, 2005). The so-called ‘small world phenomenon’ has attracted the attention of many scientists (Björneborn & Ingwersen, 2001; Braun, 2004; Kochen, 1989; Milgram, 1967; Newman & Watts, 1999; Rousseau, 2005). Such a small-world network is characterized as a graph or network exhibiting a high degree of clustering and having at the same time a small average distance between nodes.		Ronald Rousseau	2005		10.1002/meet.1450420118	social network analysis;social science;telecommunications;stars;computer science;artificial intelligence;sociology;operations research;law;social network	AI	-17.659325846544576	-40.55255456484843	199430
815f5692c2b37ad06266655c0db42edcd091ac57	graph topic scan statistic for spatial event detection	conference_paper;large graph;spatial event detection;scan statistic;topic model	Spatial event detection is an important and challenging problem. Unlike traditional event detection that focuses on the timing of global urgent event, the task of spatial event detection is to detect the spatial regions (e.g. clusters of neighboring cities) where urgent events occur. In this paper, we focus on the problem of spatial event detection using textual information in social media. We observe that, when a spatial event occurs, the topics relevant to the event are often discussed more coherently in cities near the event location than those far away. In order to capture this pattern, we propose a new method called Graph Topic Scan Statistic (Graph-TSS) that corresponds to a generalized log-likelihood ratio test based on topic modeling. We first demonstrate that the detection of spatial event regions under Graph-TSS is NP-hard due to a reduction from classical node-weighted prize-collecting Steiner tree problem (NW-PCST). We then design an efficient algorithm that approximately maximizes the graph topic scan statistic over spatial regions of arbitrary form. As a case study, we consider three applications using Twitter data, including Argentina civil unrest event detection, Chile earthquake detection, and United States influenza disease outbreak detection. Empirical evidence demonstrates that the proposed Graph-TSS performs superior over state-of-the-art methods on both running time and accuracy.	algorithm;netware;off topic;social media;steiner tree problem;time complexity;topic model;unrest	Yu Liu;Baojian Zhou;Feng Chen;David Wai-Lok Cheung	2016		10.1145/2983323.2983744	computer science;machine learning;pattern recognition;data mining;topic model;statistics	AI	-12.511432134266173	-46.638320726741114	199522
ca68ac8223ffa49d2a01b192eb6653ecb823deab	discovering key nodes in a temporal social network		[Background]Discovering key nodes plays a significant role in Social Network Analysis(SNA). Effective and accurate mining of key nodes promotes more successful applications in fields like advertisement and recommendation. [Methods] With focus on the temporal and categorical property of users’ actions when did they re-tweet or reply a message, as well as their social intimacy measured by structural embeddings, we designed a more sensitive PageRank-like algorithm to accommodate the growing and changing social network in the pursue of mining key nodes. [Results] Compared with our baseline PageRank algorithm, key nodes selected by our ranking algorithm noticeably perform better in the SIR disease simulations with SNAP Higgs dataset. [Conclusion] These results contributed to a better understanding of disseminations of social events over the network.	algorithm;baseline (configuration management);pagerank;scalability;simulation;social network	Jinshuo Liu;Chenghao Mou;Dong-Hong Ji	2018	CoRR		snap;data mining;artificial intelligence;machine learning;computer science;social network analysis;categorical variable;pagerank;social network;ranking	ML	-18.978761246278946	-43.672934814850564	199840
02420f58a6379a518993165f4166668a137e69e4	number of connected components in a graph: estimation via counting patterns		Due to the limited resources and the scale of the graphs in modern datasets, we often get to observe a sampled subgraph of a larger original graph of interest, whether it is the worldwide web that has been crawled or social connections that have been surveyed. Inferring a global property of the original graph from such a sampled subgraph is of a fundamental interest. In this work, we focus on estimating the number of connected components. It is a challenging problem and, for general graphs, little is known about the connection between the observed subgraph and the number of connected components of the original graph. In order to make this connection, we propose a highly redundant and large-dimensional representation of the subgraph, which at first glance seems counter-intuitive. A subgraph is represented by the counts of patterns, known as network motifs. This representation is crucial in introducing a novel estimator for the number of connected components for general graphs, under the knowledge of the spectral gap of the original graph. The connection is made precise via the Schatten k-norms of the graph Laplacian and the spectral representation of the number of connected components. We provide a guarantee on the resulting mean squared error that characterizes the bias variance tradeoff. Experiments on synthetic and real-world graphs suggest that we improve upon competing algorithms for graphs with spectral gaps bounded away from zero.		Ashish Khetan;Harshay Shah;Sewoong Oh	2018	CoRR			AI	-13.181413677130147	-40.352234579169156	199928
