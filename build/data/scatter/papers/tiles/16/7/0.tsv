id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
d274ff5c236052dbfef1dbd1f8e655d95e097fdb	text dependent speaker verification using a small development set		Voice biometrics for user authentication is a task in which the object is to perform convenient, robust and secure authentication of speakers. Recently we have investigated the use of state-of-the-art text-independent and text-dependent speaker verification technology for user authentication and obtained satisfactory results within a framework of a proof of technology. However, the use we have made of a quite large development set limits the practical potential of our system. In this work we investigate the ability to build an accurate user authentication system with the limitation of having a small development set.	authentication;biometrics;speaker recognition	Hagai Aronowitz	2012			speaker diarisation;computer science;pattern recognition;artificial intelligence	Security	-16.016110572071845	-96.73381954974586	85164
506d9b1b187d87557e425949cf4a235872df0d2d	speaker recognition using adaptively boosted decision tree classifier	decision tree classifier;speaker identification;decision tree;adaptive boosting;measurement uncertainty;speaker verification;speaker recognition;conference paper;roads;equations radio access networks entropy measurement uncertainty roads gain measurement;entropy;gain measurement;radio access networks	In this paper, a novel approach for speaker recognition is proposed. The approach makes use of adaptive boosting (AdaBoost) and C4.5 decision trees for closed set, text-dependent speaker recognition. A subset of 20 speakers, 10 male and 10 female, drawn from the YOHO speaker verification corpus is used to assess the performance of the system. Results reveal that an accuracy of 99.5% of speaker identification may be achieved.	adaboost;c4.5 algorithm;decision tree;speaker recognition	Say Wei Foo;Eng Guan Lim	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5743678	speaker recognition;entropy;speaker diarisation;speech recognition;decision tree learning;computer science;machine learning;decision tree;pattern recognition;statistics;measurement uncertainty	Robotics	-15.685062887816516	-94.56461111435556	85745
5cb67dd4d389334d9e04b2d9938cb4de0262aa9b	an online system for synchronized processing of video and audio signals	synchronized audio and video signals;time stamping technique;image recognition;audio signal processing;video information;video signal processing;application software;clocks;time stamping video processing audio processing signals integration synchronized audio and video signals;time stamping;video signal processing audio signal processing synchronisation;video processing;signal integrity;synchronisation;audio processing;streaming media;synchronization;signal processing;online system;audio visual applications;active objects;video frames;speech recognition;audio visual;signal processing streaming media synchronization speech recognition timing clocks encoding image recognition text recognition application software;text recognition;encoding;signals integration;synchronized video signal processing;object detection;video information online system synchronized video signal processing audio signal processing audio visual applications time stamping technique speech recognition video frames;timing	For many audio-visual applications, the integration and synchronization of audio and video signals is essential. The objective of this paper is to develop a system that displays the active objects in the captured video signal, integrated with their respective audio signals in the form of text. The video and audio signals are captured and processed separately. The signals are buffered and integrated and synchronized using a time-stamping technique. Time-stamps provide the timing information for each of the audio and video processes, the speech recognition and the object detection, respectively. This information is necessary to correlate the audio packets to the video frames. Hence, integration is achieved without the use of video information, such as lip movements. The results obtained are based on a specific implementation of the speech recognition module, which is determined to be the bottleneck process in the proposed system	audio signal processing;finite-state machine;object detection;real-time clock;real-time computing;speech recognition;synchronization (computer science);on-line system	Mary Mikhail;Giovanni Palumbo;Jinane Mohammad;Mohamed El-Helaly;Aishy Amer	2006	2006 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2006.277564	video compression picture types;audio electronics;synchronization;computer vision;audio mining;speech recognition;digital audio;aes11;telecommunications;audio signal processing;computer science;video capture;audio signal;speech coding;signal processing;video tracking;video processing;smacker video;audio signal flow;s-video;matrix mixer	HCI	-8.081182762258218	-96.63873012577518	86445
7d35ced26f107252a0f5d0d2567968350082a948	product-code vector quantization of cepstral parameters for speech recognition over the www	wireless network;speech recognition;client server;product code	We follow the paradigm that we have previously introduced for the encoding of the recognizer parameters in a client-server model used for recognition over wireless networks and the WWW, trying to maximize recognition performance instead of perceptual reproduction. We present a new encoding scheme for the mel frequency-warped cepstral parameters (MFCCs) that uses product-code vector quantization, and we find that the required bit rate to achieve the recognition performance of highquality unquantized speech is just 2000 bits per second. We also investigate the effect of additive noise on the recognition performance when quantized features are used, and we find that a small increase in the bit rate can provide the necessary robustness.	additive white gaussian noise;cepstrum;client–server model;data rate units;finite-state machine;line code;programming paradigm;server (computing);speech recognition;utility functions on indivisible goods;vector quantization;www	Vassilios Digalakis;Leonardo Neumeyer;Manolis Perakakis	1998			robustness (computer science);wireless network;speech recognition;encoding (memory);client–server model;voice activity detection;artificial intelligence;pattern recognition;vector quantization;cepstrum;quantization (physics);computer science	Vision	-15.202929276085355	-95.78173352074606	88316
bd520573c85b116e664d29d8441301f4b0d1ae67	towards an automatic on-line signature verifier using only one reference per signer	hidden markov models protocols engines silicon atmospheric modeling integrated circuit modeling handwriting recognition	What can be done with only one enrolled real hand-written signature in Automatic Signature Verification (ASV)? Using 5 or 10 signatures for training is the most common case to evaluate ASV. In the scarcely addressed case of only one available signature for training, we propose to use modified duplicates. Our novel technique relies on a fully neuromuscular representation of the signatures based on the Kinematic Theory of rapid human movements and its Sigma-Lognormal model. This way, a real on-line signature is converted into the Sigma-Lognormal model domain. The model parameters are then varied to generate new duplicated signatures.	antivirus software;electronic signature;online and offline;type signature	Moisés Díaz Cabrera;Andreas Fischer;Réjean Plamondon;Miguel Angel Ferrer-Ballester	2015	2015 13th International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2015.7333838	speech recognition;computer science;data mining;signature recognition	Vision	-16.169774793132294	-96.95600562806408	89642
080bfa78c5006ad5fb8372f9107189f3bae68749	on the equalization of keystroke timing histograms	building block;keystroke dynamics;histogram equalization;biometry	The effect of parametric equalization of time interval histograms (key down–down intervals) on the performance of keystroke-based user verification algorithms is analyzed. Three algorithms are used throughout this analysis: a classic one for static (structured) texts, a second one, also proposed in literature, for both static and arbitrary (free) text, and a new one for arbitrary text based verification. Their performances are reported before and after time interval histogram equalization, and the results corroborate with the hypothesis that the nonlinear memoryless time interval transform proposed here, despite its simplicity, can be a useful and almost costless building block in keystroke-based biometric systems. 2006 Elsevier B.V. All rights reserved.	algorithm;biometrics;event (computing);histogram equalization;nonlinear system;performance;text-based (computing)	Jugurta R. Montalvão Filho;Eduardo O. Freire	2006	Pattern Recognition Letters	10.1016/j.patrec.2006.01.010	computer vision;real-time computing;speech recognition;keystroke dynamics;computer science;theoretical computer science;biostatistics;histogram equalization	Vision	-9.645174953560925	-95.79624624202528	90919
2b52be01dcdc49db4e0a72e831f338cba6ff6e2c	about handling boundary uncertainty in a speaking rate dependent modeling approach		Variability dependent modeling provides a way of handling the impact of some variability sources in the modeling. In many cases, the variability factor is estimated in a deterministic way, leading to a mere selection of the most adequate model. However, there are always some uncertainty in the estimation of the variability sources which may induce a sub optimal model selection. This paper considers the context of a speaking rate dependent modeling approach, and shows that the uncertainty on the speech segment boundaries, which translates in an uncertainty on the speaking rate estimation, can be handled in the training process and/or in the decoding process. Preliminary results reported here are promising for dealing with variability estimation uncertainty.	heart rate variability;model selection;spatial variability	Denis Jouvet;Dominique Fohr;Irina Illina	2011			simulation;uncertainty analysis;computer science;sensitivity analysis;statistics	AI	-13.70543573997735	-94.33348522404825	91360
5d6611162884cf1a035e3a09013d58bd42aaf469	automatic speech recognition over error-prone wireless networks	distributed system;detection erreur;vocabulaire;telephonie internet;deteccion error;degradation;base donnee;streaming;systeme reparti;transmission error;telecommunication sans fil;signal audio;taux erreur;man machine dialogue;information transmission;out of vocabulary;correction erreur;degradacion;speech processing;vocabulary;wireless network;audio signal;database;tratamiento palabra;traitement parole;base dato;telecommunication network;vocabulario;correction directe erreur;error transmision;interlacing;internet telephony;interface utilisateur vocale;speech based user interfaces;automatic speech recognition;transmission en continu;automatic recognition;sistema repartido;forward error correction;reconocimiento voz;community networks;red telecomunicacion;telecomunicacion sin hilo;error correction;robustesse;entrelacement;distributed speech recognition;reseau telecommunication;out of vocabulary detection;signal acoustique;channel error robustness;error rate;speech recognition;dialogo hombre maquina;robustness;acoustic signal;transmision informacion;reconnaissance parole;correccion error;error detection;transmission information;indice error;discriminacion;senal acustica;senal audio;discrimination;erreur transmission;reconocimiento automatico;reconnaissance automatique;robustez;dialogue homme machine;wireless telecommunication	The past decade has witnessed a growing interest in deploying automatic speech recognition (ASR) in communication networks. The networks such as wireless networks present a number of challenges due to e.g. bandwidth constraints and transmission errors. The introduction of distributed speech recognition (DSR) largely eliminates the bandwidth limitations and the presence of transmission errors becomes the key robustness issue. This paper reviews the techniques that have been developed for ASR robustness against transmission errors. In the paper, a model of network degradations and robustness techniques is presented. These techniques are classified into three categories: error detection, error recovery and error concealment (EC). A one-frame error detection scheme is described and compared with a frame-pair scheme. As opposed to vector level techniques a technique for error detection and EC at the sub-vector level is presented. A number of error recovery techniques such as forward error correction and interleaving are discussed in addition to a review of both feature-reconstruction and ASR-decoder based EC techniques. To enable the comparison of some of these techniques, evaluation has been conduced on the basis of the same speech database and channel. Special attention is given to the unique characteristics of DSR as compared to streaming audio e.g. voice-over-IP. Additionally, a technique for adapting ASR to the varying quality of networks is presented. The frame-error-rate is here used to adjust the discrimination threshold with the goal of optimising out-of-vocabulary detection. This paper concludes with a discussion of applicability of different techniques based on the channel characteristics and the system requirements. 2005 Elsevier B.V. All rights reserved.	ansi escape code;automated system recovery;cognitive dimensions of notations;error concealment;error detection and correction;forward error correction;internet access;requirement;speech recognition;streaming media;system requirements;telecommunications network;vocabulary	Zheng-Hua Tan;Paul Dalsgaard;Børge Lindberg	2005	Speech Communication	10.1016/j.specom.2005.05.007	human error assessment and reduction technique;error detection and correction;speech recognition;telecommunications;computer science;speech processing	Mobile	-13.25267884951576	-95.88593782847457	102113
f1c0a11aef64d05c07bc6627b0987aac089d40ed	rule-based trajectory segmentation for modeling hand motion trajectory	hidden markov model;rule based;hmm initialization;hand gesture recognition;trajectory segmentation	In this paper, we propose a simple but effective method of modeling hand gestures based on the angles and angular change rates of the hand trajectories. Each hand motion trajectory is composed of a unique series of straight and curved segments. In our Hidden Markov Model (HMM) implementation, these trajectories are modeled as a connected series of states analogous to the series of phonemes in speech recognition. The novelty of the work presented herein is that it provides an automated process of segmenting gesture trajectories based on a simple set of threshold values in the angular change measure. In order to represent the angular distribution of each separated state, the von Mises distribution is used. A likelihood based state segmentation was implemented in addition to the threshold based method to ensure that the gesture sets are segmented consistently. The proposed method can separate each angular state of the training data at the initialization step, thus providing a solution to mitigate the ambiguities on initializing the HMM. The effectiveness of the proposed method was demonstrated by the higher recognition rates in the experiments compared to the conventional methods. & 2013 Elsevier Ltd. All rights reserved.	algorithm;angularjs;effective method;elegant degradation;experiment;gesture recognition;hidden markov model;iteration;markov chain;norm (social);overfitting;portable document format;simple set;speech recognition	Jounghoon Beh;David K. Han;Hanseok Ko	2014	Pattern Recognition	10.1016/j.patcog.2013.11.010	computer vision;speech recognition;computer science;machine learning;pattern recognition;hidden markov model	AI	-18.14411811548209	-95.0690783068043	103461
ffb41c119790bad6a99b9202d9ca3aa98ba02bd0	light weight mobile device targeted speaker clustering algorithm	unsupervised learning;clustering algorithms speech algorithm design and analysis distance measurement mobile handsets gaussian distribution robustness;cluster algorithm;pattern clustering;unsupervised learning bayes methods mobile handsets pattern clustering signal detection speaker recognition;mobile device;bayes methods;signal detection;speech;speaker segmentation light weight mobile device targeted unsupervised speaker clustering algorithm bayesian information criterion false alarm compensation speaker change detector;speaker recognition;speaker change detector;distance measurement;mobile handsets;clustering algorithms;speaker segmentation;robustness;light weight mobile device targeted unsupervised speaker clustering algorithm;bayesian information criterion;algorithm design;algorithm design and analysis;false alarm compensation;gaussian distribution	In this paper we present a novel light weight speaker clustering algorithm based on the Bayesian information criterion (BIC). Algorithm utilises BIC profiles, which were earlier used for false alarm compensation (FAC) in our speaker change detector (SCD). Proposed speaker segmentation followed by a light weight clustering is targeted to segment and label mobile device recordings directly in the device itself. Thus the main criterion in algorithm design was to maintain high detection accuracy while keeping computational costs in low level. Clustering algorithm gave F-score performance of 0.90 for speaker segmentation, which is 29% relative improvement compared to baseline results. Speaker segment labelling performance was 88%, when the number of speakers was undetermined. The experimental results indicate that our unsupervised speaker clustering algorithm is sufficiently effective and efficient for speaker segmentation applications in mobile devices.	algorithm design;baseline (configuration management);bayesian information criterion;cluster analysis;computation;computer cluster;fly-by-wire;google map maker;hidden markov model;mobile device;pc speaker;requirement;test set	Olli Vuorinen;Tommi Lahti;Satu-Marja Mäkelä;Johannes Peltola	2008	2008 IEEE 10th Workshop on Multimedia Signal Processing	10.1109/MMSP.2008.4665062	unsupervised learning;speaker recognition;algorithm design;speech recognition;computer science;machine learning;pattern recognition;statistics	Vision	-15.347240843490338	-94.43505655063075	104120
5d767fac768f0a165b3f340622bddb0b84b505ab	music similarity evaluation using the variogram for mfcc modelling	mirex ams task;music similarity;variogram;mfccs	This chapter describes two different approaches using the variogram in the context of Mel Frequency Cepstral Coefficients MFCCs and the evaluation of music similarity. The first approach is referred to as the full variogram approach; in this case, all the lags of the variogram of the second coefficient of the MFCC are employed. The second choice is referred to as the reduced variogram approach; in this case, a subset of the lags of the variogram of the MFCC matrix is considered. Thus, the usage of the variogram is proposed as a tool to synthesize the timbre information contained in the MFCCs.#R##N##R##N#Also, four different weighting functions are tested for the calculation of the distance measure between songs. The performance of the methods proposed is evaluated by applying the pseudo-objective evaluation scheme of the MIREX AMS task. The results are compared against the scores obtained by other methods submitted to the MIREX AMS 2011.		Lorenzo J. Tardón;Isabel Barbancho	2012		10.1007/978-3-642-41248-6_17	econometrics;speech recognition;variogram;mathematics;statistics	NLP	-11.672143382212404	-95.02019105307757	106854
5e706784dfa4c8b60de52ab3f0d54727f49f404d	heart sound segmentation using switching linear dynamical models		Localization of exact positions of the fundamental heart sounds (FHS) is an essential step towards automatic analysis of heart sound phonocardiogram (PCG) recordings, the automatic segmentation allows for data-driven classification of heart pathological events. Current approach using probabilistic models such as hidden Markov models (HMMs) has improved accuracy of heart sound segmentation. In this paper, we propose a switching linear dynamic system (SLDS) of piece-wise stationary autoregressive (AR) processes for segmenting the heart sounds into four fundamental components with distinct second order structure (auto-correlation). The SLDS is able to capture simultaneously both the continuous state-space in the hidden dynamics in PCG, and the regime switching in the dynamics using a discrete Markov chain. This overcomes limitation of HMMs which is based on a single-layer of discrete states. Compared to AR processes, the Gaussian mixture densities in HMM do not account for the temporal autorrelation structure in PCG which has one-to-one correspondence to frequency content a distinctive feature of HS components. We introduce three schemes for model estimation: (1) switching Kalman filter (SKF) model. (2) refinement by switching Kalman filter (SKS), and (3) fusion of SKF and the duration-dependent Viterbi algorithm (SKF-Viterbi). Results on a large PCG dateset of Physionet/Challenge 2016 shows SKF-Viterbi significantly outperforms SKF by improvement of segmentation accuracy from 71% to 84.2%.	autocorrelation;autoregressive model;dynamical system;hidden markov model;kalman filter;markov chain;one-to-one (data model);refinement (computing);state space;stationary process;viterbi algorithm	Jos&#xE9; Gonz&#xE1;lez Enr&#xED;quez;Sheikh Hussain Shaikh Salleh;Chee-Ming Ting;Hadri Hussain	2017	2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2017.8309111	viterbi algorithm;kalman filter;phonocardiogram;autoregressive model;mathematics;hidden markov model;probabilistic logic;heart sounds;pattern recognition;artificial intelligence;markov chain	Vision	-6.623026049086466	-97.95228810134745	107796
391658cf03272c77834d6ed96f3e8ea9d82023db	using computer vision to generate customized spatial audio	databases;audio user interfaces;audio signal processing;personal computers;video signal processing;personal computer;transfer functions;low frequency;real time;torso;head tracking;customized spatial audio;anthropometric measurements;frequency measurement;audio user interfaces computer vision transfer functions video signal processing audio signal processing;computer vision;head related transfer functions;indexes;computer vision headphones transfer functions cameras microcomputers torso ear frequency measurement indexes databases;head related transfer function;ear;real time head tracking;customized room response models;indexation;head and torso model;model development;head and torso model computer vision customized spatial audio headphones real time head tracking head related transfer functions customized room response models cameras personal computers anthropometric measurements;headphones;high frequency;microcomputers;spatial audio;cameras	Creating high quality virtual spatial audio over headphones requires real-time head tracking, personalized head-related transfer functions (HRTFs) and customized room response models. While there are expensive solutions to address these issues based on costly head trackers, measured personalized HRTFs and room responses, these are not suitable for widespread or easy deployment and use. We report on the development of a system that uses computer vision to produce customizable models for both the HRTF and the room response, and to achieve head-tracking. The system uses relatively inexpensive cameras and widely available personal computers. Computer-vision based anthropometric measurements of the head, torso, and the external ears are used for HRTF customization. For low-frequency HRTF customization we employ a simple head-and-torso model developed recently [1]. For high frequency customization we employ measured pinna characteristics as an index into a database of HRTFs [2]. For head tracking we employ an online implementation of the POSIT algorithm [3] along with active markers to compute head pose in real-time. The system provides an enhanced virtual listening experience at low cost. Partial support of NSF award #0205271 and ONR grant N000140110571 is gratefully acknowledged.	algorithm;anthropometry;computer vision;display resolution;head-related transfer function;headphones;ibm notes;motion capture;personal computer;personalization;real-time clock;real-time transcription;software deployment;surround sound	Ankur Mohan;Ramani Duraiswami;Dmitry N. Zotkin;Daniel DeMenthon;Larry S. Davis	2003		10.1109/ICME.2003.1221247	database index;computer vision;speech recognition;audio signal processing;torso;computer science;head-related transfer function;high frequency;microcomputer;anthropometry;transfer function;low frequency;computer graphics (images)	Vision	-8.86140940059691	-94.51573762187977	109511
074b445e1ceaffba5f853f2263f7ba07f41cefb1	an embedded face verification system against image degradation	image degradation;tms320c67ii dsp chip;degradation face recognition working environment noise parameter estimation digital signal processing chips vectors hardware appropriate technology smart cards biometrics;noise parameter estimation;vectors face recognition;face verification;chip;face recognition;vectors;image vector;image vector embedded face verification system image degradation tms320c67ii dsp chip noise parameter estimation;parameter estimation;embedded face verification system	In this paper, we propose an embedded face verification system against image degradation using a TMS320C6711 DSP chip. Our proposed system has several advantages over general systems in terms of price, size and applicability. As well as physical merits, it has an efficient approach for degraded facial images based on noise parameter estimation under real-life environments. The proposed method linearly combines image vector and noise parameters into one vector for training. When estimating noise parameters, we calculate the optimal coefficients of linear decomposition of an input image vector only. The noise parameters can be obtained from the linear composition step using these optimal coefficients. In contrast to conventional methods, we add the estimated noises to original images instead of removing them. Finally we perform a verification step with the input and synthesized image. The experimental results of this system show that the proposed method can estimate noise parameters accurately while improving the performance of photo image verification.	captcha;coefficient;digital signal processor;elegant degradation;embedded system;estimation theory;quantum decoherence;real life	Sang-Woong Lee;Seong-Whan Lee	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413904	chip;facial recognition system;computer vision;speech recognition;computer science;estimation theory	EDA	-15.110680743464812	-96.72286955892862	120787
a5c8dd7592b90f47c3548fe658c96d735a330a8a	subspace restricted boltzmann machine		The subspace Restricted Boltzmann Machine (subspaceRBM) is a third-order Boltzmann machine where multiplicative interactions are between one visible and two hidden units. There are two kinds of hidden units, namely, gate units and subspace units. The subspace units reflect variations of a pattern in data and the gate unit is responsible for activating the subspace units. Additionally, the gate unit can be seen as a pooling feature. We evaluate the behavior of subspaceRBM through experiments with MNIST digit recognition task, measuring reconstruction error and classification error.	experiment;interaction;mnist database;restricted boltzmann machine	Jakub M. Tomczak;Adam Gonczarek	2014	CoRR		speech recognition;computer science;machine learning;pattern recognition	ML	-17.243973725566484	-95.41583033924177	121958
24c2f5a84eebd984238b25a1be51f0a972303e46	an information theoretic approach toward assessing perceptual audio quality using eeg	nonlinear time varying communication channel;music sequences;audio signal processing;information theoretic approach;time varying channels approximation theory audio signal processing electroencephalography gaussian processes information theory mixture models;human subject;sensors;gaussian processes;electroencephalograph;electroencephalography distortion brain modeling entropy sensors testing;music distortion information theoretic approach perceptual audio quality electroencephalograph eeg measurements transmission chain stimulus generation brain processing human subject response measurements nonlinear time varying communication channel mutual information brainwave responses time varying distortions multidimensional gaussian mixture model gmm low complexity approximation subjective audio quality perception music sequences;subjective audio quality perception;perceptual audio quality;gmm;electroencephalography eeg;testing;low complexity approximation;brain processing;gaussian mixture model gmm mutual information perception audio quality electroencephalography eeg;multidimensional gaussian mixture model;approximation theory;distortion;brain modeling;audio quality;response measurements;stimulus generation;eeg measurements;mutual information;gaussian mixture model gmm;transmission chain;entropy;music distortion;brainwave responses;perception;electroencephalography;mixture models;time varying distortions;information theory;time varying channels	In this paper, we propose a novel information theoretic model to interpret the entire “transmission chain” comprising stimulus generation, brain processing by the human subject, and the electroencephalograph (EEG) response measurements as a nonlinear, time-varying communication channel with memory. We use mutual information (MI) as a measure to assess audio quality perception by directly measuring the brainwave responses of the human subjects using a high resolution EEG. Our focus here is on audio where the quality is impaired by time varying distortions. In particular, we conduct experiments where subjects are presented with audio whose quality varies with time between different possible quality levels. The recorded EEG measurements can be modeled as a multidimensional Gaussian mixture model (GMM). In order to make the computation of the MI feasible, we present a novel low-complexity approximation technique for the differential entropy of the multidimensional GMM. We find the proposed information theoretic approach to be successful in quantifying subjective audio quality perception, with the results being consistent across different music sequences and distortion types.	algorithm;approximation;automatic vectorization;channel (communications);column (database);computation;differential entropy;distortion;erp;electroencephalography;end-to-end principle;experiment;focus stacking;google map maker;gradient descent;hessian;image resolution;information theory;magnetoencephalography;matrix multiplication;mixture model;mutual information;neural oscillation;nonlinear system;perturbation theory;series expansion;sound quality;the matrix	Ketan Mehta;Joerg Kliewer	2015	IEEE Transactions on Molecular, Biological and Multi-Scale Communications	10.1109/TMBMC.2015.2501744	entropy;speech recognition;distortion;electroencephalography;audio signal processing;information theory;computer science;generalized method of moments;sensor;machine learning;mixture model;sound quality;gaussian process;mathematics;software testing;mutual information;perception;statistics;approximation theory	ML	-10.294078487868132	-94.62136304259408	124649
57a2fffccec4731cce07de9692d7be43749dc3cc	exploiting temporal feature integration for generalized sound recognition	transformation ondelette;signal image and speech processing;analisis imagen;sound recognition;traitement signal;evaluation performance;statistical moment;analisis escena;analyse scene;audio signal processing;performance evaluation;filter bank;banc filtre;modelo autorregresivo;signal audio;implementation;evaluacion prestacion;moment statistique;audio signal;modele markov variable cachee;compresion senal;experimental device;acoustic signal processing;probabilistic approach;reconocimiento sonido;statistical model;compression signal;autoregressive model;dispositif experimental;momento estadistico;automatic recognition;hidden markov models;quantum information technology spintronics;traitement signal audio;enfoque probabilista;approche probabiliste;signal processing;banco filtro;feature integration;signal compression;reconnaissance son;modele statistique;signal acoustique;image analysis;modelo estadistico;acoustic signal;traitement signal acoustique;transformacion ondita;implementacion;modele autoregressif;procesamiento senal;analyse image;wavelet transformation;senal acustica;senal audio;reconocimiento automatico;reconnaissance automatique;scene analysis;dispositivo experimental	This paper presents a methodology that incorporates temporal feature integration for automated generalized sound recognition. Such a system can be of great use to scene analysis and understanding based on the acoustic modality. The performance of three feature sets based on Mel filterbank, MPEG-7 audio protocol, and wavelet decomposition is assessed. Furthermore we explore the application of temporal integration using the following three different strategies: (a) short-term statistics, (b) spectral moments, and (c) autoregressive models. The experimental setup is thoroughly explained and based on the concurrent usage of professional sound effects collections. In this way we try to form a representative picture of the characteristics of ten sound classes. During the first phase of our implementation, the process of audio classification is achieved through statistical models (HMMs) while a fusion scheme that exploits the models constructed by various feature sets provided the highest average recognition rate. The proposed system not only uses diverse groups of sound parameters but also employs the advantages of temporal feature integration.	acoustic cryptanalysis;autonomous system (internet);autoregressive model;complexity class;computation;display resolution;emoticon;feature extraction;feature integration theory;filter bank;mpeg-7;mel-frequency cepstrum;microsoft windows;modality (human–computer interaction);multiresolution analysis;prometheus;sensor;statistical model;wavelet	Stavros Ntalampiras;Ilyas Potamitis;Nikos Fakotakis	2009	EURASIP J. Adv. Sig. Proc.	10.1155/2009/807162	statistical model;computer vision;image analysis;speech recognition;audio signal processing;computer science;audio signal;signal processing;filter bank;autoregressive model;implementation;statistics	Vision	-11.756232461824219	-96.14427503095006	130037
99d53b0ce6b5f37ab116ec3fd127f2d0a9f80264	applied mel-frequency discrete wavelet coefficients and parallel model compensation for noise-robust speech recognition	noise robust asr;nivel ruido;traitement signal;evaluation performance;base donnee;error reduction;performance evaluation;taux erreur;correction erreur;ondelette;evaluacion prestacion;signal analysis;speech processing;weighting;modele markov variable cachee;database;tratamiento palabra;niveau bruit;traitement parole;base dato;analisis de senal;inmunidad ruido;probabilistic approach;ponderacion;noise robustness;reduccion ruido;feature vector;mel frequency cepstral coefficient;parallel models;performance improvement;cepstral analysis;hidden markov models;noise immunity;noise level;reconocimiento voz;methode domaine frequence;analyse cepstrale;local features;frequency domain method;enfoque probabilista;approche probabiliste;error correction;signal processing;noise reduction;feature weighting;reduction bruit;error rate;speech recognition;rapport signal bruit;relacion senal ruido;ponderation;reconnaissance parole;correccion error;metodo dominio frecuencia;immunite bruit;signal to noise ratio;frequency domain;indice error;procesamiento senal;wavelets;wavelet;analyse signal;local feature	Interfering noise severely degrades the performance of a speech recognition system. The Parallel Model Compensation (PMC) technique is one of the most efficient techniques for dealing with such noise. Another approach is to use features local in the frequency domain, such as Mel-Frequency Discrete Wavelet Coefficients (MFDWCs). In this paper, we investigate the use of PMC and MFDWC features to take advantage of both noise compensation and local features (MFDWCs) to decrease the effect of noise on recognition performance. We also introduce a practical weighting technique based on the noise level of each coefficient. We evaluate the performance of several wavelet-schemes using the NOISEX-92 database for various noise types and noise levels. Finally, we compare the performance of these versus Mel-Frequency Cepstral Coefficients (MFCCs), both using PMC. Experimental results show significant performance improvements for MFDWCs versus MFCCs, particularly after compensating the HMMs using the PMC technique. The best feature vector among the six MFDWCs we tried gave 13.72 and 5.29 points performance improvement, on the average, over MFCCs for 6 and 0 dB SNR, respectively. This corresponds to 39.9% and 62.8% error reductions, respectively. Weighting the partial score of each coefficient based on the noise level further improves the performance. The average error rates for the best MFDWCs dropped from 19.57% to 16.71% and from 3.14% to 2.14% for 6 dB and 0 dB noise levels, respectively, using the weighting scheme. These improvements correspond to 14.6% and 31.8% error reductions for 6 dB and 0 dB noise levels, respectively. 2006 Elsevier B.V. All rights reserved.	coefficient;discrete fourier transform;feature vector;hidden markov model;mel-frequency cepstrum;noise (electronics);signal-to-noise ratio;speech recognition;wavelet	Zekeriya Tufekci;John N. Gowdy;Sabri Gurbuz;Eric K. Patterson	2006	Speech Communication	10.1016/j.specom.2006.06.006	wavelet;speech recognition;telecommunications;computer science;signal processing;statistics	AI	-13.673330731652166	-95.6831271478542	134367
526ec7641df5a08d66c985b9485ad792f82f7e14	combining the glottal mixture model (glomm) with ubm for speaker recognition	signal processing;europe;conferences	We present an iterative algorithm to extract the voice source waveform from recordings of speech for speaker identification. The method detects glottal closings, then constructs a speaker-dependent library of glottal pulse waveforms by clustering data windows centered on the linear prediction error time-series at the glottal closures. With the voice source modeled as scaled and shifted glottal pulses, the algorithm iteratively determines the vocal tract parameters in each frame. In experiments, we combine the extracted voice source information with a universal background model (UBM). Using the TIMIT data corpus and a 200-speaker population size, we demonstrate a factor of three speaker recognition error reduction.	algorithm;cluster analysis;experiment;iterative method;microsoft windows;mixture model;speaker recognition;timit;time series;tract (literature);waveform	Paul M. Baggenstoss	2016	2016 24th European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2016.7760630	speech recognition;acoustics;computer science;communication	ML	-12.799781193015802	-94.31896347612319	139591
2629e312aa8db1b8d2698e63454caaa1062ddb36	prediction of synchrostate transitions in eeg signals using markov chain models	data partitioning based cross validation scheme stochastic model interstate transitions millisecond order quasi stable phase synchronized patterns multichannel electroencephalogram signals eeg signals first order transition probability matrices second order transition probability matrices face perception tasks finite markov chain models synchrostate transition;synchrostate eeg markov chain prediction;qa75 electronic computers computer science;markov processes brain models electroencephalography face switches hidden markov models;synchronisation electroencephalography markov processes matrix algebra medical signal processing probability	This letter proposes a stochastic model using the concept of Markov chains for the inter-state transitions of the millisecond order quasi-stable phase synchronized patterns or synchrostates, found in multi-channel Electroencephalogram (EEG) signals. First and second order transition probability matrices are estimated for Markov chain modelling from 100 trials of 128-channel EEG signals during two different face perception tasks. Prediction accuracies with such finite Markov chain models for synchrostate transition are also compared, under a data-partitioning based cross-validation scheme.	cognition;cross-validation (statistics);data validation;electroencephalography;face perception;markov chain;markov model;semiconductor industry;statistical model	Wasifa Jamal;Saptarshi Das;Ioana-Anastasia Oprescu;Koushik Maharatna	2015	IEEE Signal Processing Letters	10.1109/LSP.2014.2352251	time reversibility;markov chain;maximum-entropy markov model;markov kernel;speech recognition;markov property;computer science;machine learning;markov model;hidden markov model;statistics;variable-order markov model	ML	-6.516486397895716	-97.9784308525578	139853
209b6faa40df68dcdd3beab6cf055f149edb3577	a regression approach to speech source localization exploiting deep neural network		This paper presents a data-driven framework to speech source localization (SSL) using deep neural network (DNN), which directly construct the nonlinear regressive transform between the extracted feature and the direction-of-arrival (DOA) of indoor speech source. The proposed method incorporates a feature extractor front-end and a regression network back-end. First, since the DOA information contained in the steering vector of speech source can be represented by the eigenvector associated with the signal subspace, it is extracted as the input feature by eigenanalysis. Second, a regression DNN is adopted to model the nonlinear relationship between the eigenvector and source direction, where time delay neural network (TDNN) is chosen as the basic network architecture. Several experiments are conducted under the simulated and real environments using an eight-channel circular array, which reveal the superiority and potential of the proposed method for SSL.	algorithm;artificial neural network;broadcast delay;deep learning;direction of arrival;experiment;mobile data terminal;network architecture;nonlinear system;randomness extractor;signal subspace;time delay neural network	Zhaoqiong Huang;Ji Xu;Jielin Pan	2018	2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)	10.1109/BigMM.2018.8499477	network architecture;artificial neural network;eigenvalues and eigenvectors;time delay neural network;azimuth;feature extraction;nonlinear system;pattern recognition;signal subspace;artificial intelligence;computer science	Robotics	-15.341534766246344	-95.1393925986251	140772
2036e071e6bfd1e4b8cb9f3167eb89bb7c2fb8cd	confidence and reliability measures in speaker verification	bayes estimation;mismatching;background noise;modelizacion;confidence estimation;bayesian network;fiabilidad;reliability;base donnee;reliability estimation;measurement;etude experimentale;database;base dato;probabilistic approach;error modelling;speaker verification;modelisation;systeme incertain;estimacion bayes;desadaptacion;reconocimiento voz;verification locuteur;medida;enfoque probabilista;approche probabiliste;fiabilite;error rate;speech recognition;ruido fondo;mesure;desadaptation;reconnaissance parole;estimation error;verificacion locutor;sistema incierto;bruit fond;modeling;estudio experimental;uncertain system;estimation bayes;bayesian networks	Speaker verification is a biometric identity verification technique whose performance can be severely degraded by the presence of noise. Using a coherent notation, we reformulate and review several methods which have been proposed to quantify the uncertainty in verification results, some with a view to coping with the effects of mismatched training-testing environments. We also include a recently proposed method, which is firmly rooted in a probabilistic approach and interpretation, and explicitly measures signal quality before assigning a reliability value to the speaker verification classifier’s decision. We evaluate the performance of the confidence and reliability measures over a noisy 251-users database, showing that taking into account signal-domain quality can lead to better accuracy in prediction of classifier errors. We discuss possible strategies for using the measures in a speaker verification system, balancing acquisition duration and verification error rate. r 2006 The Franklin Institute. Published by Elsevier Ltd. All rights reserved.	bayesian network;biometrics;coherence (physics);franklin electronic publishers;identity verification service;modal logic;multimodal interaction;speaker recognition;synthetic data;timit;verification and validation	Jonas Richiardi;Andrzej Drygajlo;Plamen J. Prodanov	2006	J. Franklin Institute	10.1016/j.jfranklin.2006.07.002	speech recognition;computer science;pattern recognition;bayesian network;runtime verification;statistics	AI	-13.697593661684596	-96.03544652943538	142578
31428bf68274fcd3838c8e5135ce66a8502f88cc	introduction to the special issue on neural networks for speech processing	speech understanding special issue neural networks speech processing research papers speech analysis speech synthesis speech recognition;speech synthesis;neural nets;speech processing;speech analysis;neural nets speech analysis and processing speech synthesis speech recognition;research paper;special issues and sections neural networks speech processing biological neural networks speech analysis artificial neural networks speech synthesis speech recognition hidden markov models network synthesis;speech analysis and processing;speech recognition;neural network	The goal of this special issue is to present a representative set of current research papers that address the topic of neural networks in speech processing. The application areas addressed include speech analysis, synthesis, recognition and understanding. Due to the large volume of current research in these areas, the authors make the usual disclaimer and apologize for any work that may not be represented in the limited space allocated here. However, the included papers provide a fairly broad overview of each area, as well as citations to the related literature. >	artificial neural network;speech processing	Allen L. Gorin;Richard J. Mammone	1994	IEEE Trans. Speech and Audio Processing	10.1109/89.260355	voice activity detection;natural language processing;speech technology;speech recognition;speech corpus;computer science;speech processing;time delay neural network;acoustic model;artificial neural network;speech analytics	ML	-16.42394825763556	-97.88769772178323	143122
048c59f6dc51d2dc7cd9975ea639a3b8ade81a81	a new hierarchical decision structure using wavelet packet and svm for brazilian phonemes recognition	transformation ondelette;phoneme;hierarchical system;phonetique;analisis estadistico;lip;subband decomposition;systeme hierarchise;database;base dato;fonema;wavelet packet;classification a vaste marge;sistema jerarquizado;reconocimiento voz;levre;statistical analysis;descomposicion subbanda;wavelet packet transform;analyse statistique;success rate;base de donnees;fonetica;portugues;speech recognition;labio;phonetics;transformacion ondita;reconnaissance parole;support vector machine;maquina ejemplo soporte;decomposition sous bande;vector support machine;reseau neuronal;portuguese;red neuronal;wavelet transformation;portugais;neural network	In this work, a new phonemes recognition system is proposed. The base of decision of the proposed system is the tongue position and roundedness of the lips. The features of the speech are the coefficients of Wavelet Packet Transform with sub-bands selected through the Mel scale. The SVM (Support Vector Machine) is used as classifier in the structure of a Hierarchical Committee Machine. The database used for the recognition was a set of oral vocalic phonemes of the Portuguese language. The experimental results show success rates of 97.50% for the user-dependent case and 91.01% for the user-independent case. This new proposal increased 3.5% the success rate in relation to the “one vs. all” decision strategy.	wavelet	Adriano de Andrade Bresolin;Adrião Duarte Dória Neto;Pablo Javier Alsina	2006		10.1007/11893257_18	phonetics;support vector machine;speech recognition;computer science;artificial intelligence;machine learning;artificial neural network;portuguese	Robotics	-13.150899505622409	-96.86165248202536	143263
27a94c5598d53bcb7611902da08b1df6c6f94252	dynamic speech models - theory, algorithms, and applications	stochastic resonance;speech synthesis;speech processing;telecommunication computing;books;computational modeling;hidden markov models;heuristic algorithms;mathematical model;humans	This book deals with the theory, algorithms, and applications of dynamic speech models. It contains a survey done in a holistic manner of the relevant research and the current best practices in this area spanning over the last two decades. This monograph is intended to be advanced materials of speech and signal processing for graduate level teaching, for professionals and engineering practitioners, as well as for researchers and engineers specializing in speech processing and researchers in applied neural networks.	algorithm	L. Deng	2009	IEEE Trans. Neural Networks	10.1109/TNN.2009.2016023	natural language processing;speech recognition;computer science;artificial intelligence;machine learning;mathematical model;speech processing;computational model;speech synthesis;stochastic resonance	Arch	-16.449649034898023	-97.94404794787097	144771
30bafee09fc2ce38a2152417a143b5390071d926	glottal information based spectral recuperation in multi-channel speaker recognition	utilisation information;uso informacion;text;mobile radiocommunication;information use;biometrie;biometrics;biometria;texte;long terme;communication service mobile;radiocommunication service mobile;long term;speaker recognition;automatic recognition;reconocimiento voz;largo plazo;automatic speaker recognition;mobile communication;court terme;reconnaissance locuteur;speech recognition;reconnaissance parole;radiocomunicacion servicio movil;texto;corto plazo;short term;reconocimiento automatico;reconnaissance automatique	Recently, expansion of mobile communication arise lots of research interests in robust speaker recognition under multi-channel environments. Thus, building robust automatic speaker recognition (ASR) system becomes an urgent and necessary problem. Though glottal information was successfully used in many speaker recognition systems, the spectral variations caused by it were not taken into account under multi-channel environment. In this paper, a method that can utilize this influence, using both long-term and short-term glottal information, is proposed. Through this recuperation, spectral features will behave more robust in text-independent ASR system under channel influences. Our method was applied to the large multi-channel SRMC corpus. The experimental works show promising results.	automated system recovery;speaker recognition	Pu Yang;Yingchun Yang;Zhaohui Wu	2004		10.1007/978-3-540-30548-4_69	speaker recognition;speech recognition;mobile telephony;telecommunications;computer science;short-term memory;biometrics	Robotics	-13.304278363625068	-95.85628813096113	146447
1503725c733c380b0e782e39615694cded2c3f18	hrtf magnitude synthesis via sparse representation of anthropometric features	hrtf personalization;acoustics training measurement ear conferences vectors transfer functions;vectors audio signal processing data handling pattern classification regression analysis speech synthesis transfer functions;anthropometric features head related transfer function hrtf personalization hrtf synthesis sparse representation;hrtf synthesis;head related transfer function;anthropometric features;hrtf classifier hrtf magnitude synthesis anthropometric feature sparse representation head related transfer function magnitude synthesis sparse combination anthropometric data sparse vector anthropometric feature linear superposition hrtf tensor data ridge regression manikin data log spectral distortion;sparse representation	We propose a method for the synthesis of the magnitudes of Head-related Transfer Functions (HRTFs) using a sparse representation of anthropometric features. Our approach treats the HRTF synthesis problem as finding a sparse representation of the subject's anthropometric features w.r.t. the anthropometric features in the training set. The fundamental assumption is that the magnitudes of a given HRTF set can be described by the same sparse combination as the anthropometric data. Thus, we learn a sparse vector that represents the subject's anthropometric features as a linear superposition of the anthropometric features of a small subset of subjects from the training data. Then, we apply the same sparse vector directly on the HRTF tensor data. For evaluation purpose we use a new dataset, containing both anthropometric features and HRTFs. We compare the proposed sparse representation based approach with ridge regression and with the data of a manikin (which was designed based on average anthropometric data), and we simulate the best and the worst possible classifiers to select one of the HRTFs from the dataset. For instrumental evaluation we use log-spectral distortion. Experiments show that our sparse representation outperforms all other evaluated techniques, and that the synthesized HRTFs are almost as good as the best possible HRTF classifier.	anthropometry;distortion;experiment;head-related transfer function;simulation;sparse approximation;sparse matrix;superposition principle;test set	Piotr Tadeusz Bilinski;Jens Ahrens;Mark R. P. Thomas;Ivan Tashev;John C. Platt	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854447	speech recognition;computer science;machine learning;head-related transfer function;pattern recognition;sparse approximation	Vision	-9.164538194615528	-94.45788724059567	147609
d566044aeb6a5fc4d02068ad799e20e2b931c24a	noise subspace fuzzy c-means clustering for robust speech recognition	modelizacion;robust speech recognition;fuzzy c mean;analyse amas;automovil;decision function;real time;subband decomposition;logique floue;vector space;database;base dato;logica difusa;fonction decision;classification;voice;voz;fuzzy logic;modelisation;570 biowissenschaften;cluster analysis;reconocimiento voz;subbanda;descomposicion subbanda;automobile;funcion decision;subband;motor car;temps reel;base de donnees;speech recognition;tiempo real;analisis cluster;espace vectoriel;reconnaissance parole;decomposition sous bande;voice activity detection;fuzzy c means clustering;discriminacion;modeling;espacio vectorial;real time application;sous bande;clasificacion;discrimination;biologie;voix	In this paper a fuzzy C-means (FCM) based approach for speech/non-speech discrimination is developed to build an effective voice activity detection (VAD) algorithm. The proposed VAD method is based on a soft-decision clustering approach built over a ratio of subband energies that improves recognition performance in noisy environments. The accuracy of the FCM-VAD algorithm lies in the use of a decision function defined over a multiple-observation (MO) window of averaged subband energy ratio and the modeling of noise subspace into fuzzy prototypes. In addition, time efficiency is also reached due to the clustering approach which is fundamental in VAD real time applications, i.e. speech recognition. An exhaustive analysis on the Spanish SpeechDat-Car databases is conducted in order to assess the performance of the proposed method and to compare it to existing standard VAD methods. The results show improvements in detection accuracy over standard VADs and a representative set of recently reported VAD algorithms.	algorithm;cluster analysis;database;fuzzy cognitive map;speech recognition;voice activity detection	Juan Manuel Górriz;Javier Ramírez;José C. Segura;Carlos García Puntonet;J. J. González	2006		10.1007/11751649_85	voice activity detection;fuzzy logic;discrimination;speech recognition;systems modeling;telecommunications;vector space;biological classification;computer science;artificial intelligence;cluster analysis;voice	AI	-13.260884960034904	-96.46074164600796	148680
4248b64b1b99690fcbe2bc24a182320f88df52ec	segmenting multiple concurrent speakers using microphone arrays	speech;microphone array;lathoud;mccowan;moore	Speaker turn detection is an important task for many speech p rocessing applications. However, accurate segmentation can be hard to achieve if there are multiple concurrent speakers (o verlap), as is typically the case in multi-party conversations . In such cases, the location of the speaker, as measured using a microphone array, may provide greater discrimination than tr ditional spectral features. This was verified in previous wo rk which obtained a global segmentation in terms of single spea ker classes, as well as possible overlap combinations. However , such a global strategy suffers from an explosion of the numbe r of overlap classes, as each possible combination of concurr ent speakers must be modeled explicitly. In this paper, we propo se two alternative schemes that produce an individual segment ation decision for each speaker, implicitly handling all ove rlapping speaker combinations. The proposed approaches also al low straightforward online implementations. Experiments are presented comparing the segmentation with that obtained us ing the previous system.	microphone;powered speakers	Guillaume Lathoud;Iain McCowan;Darren Moore	2003			speaker recognition;speaker diarisation;speech recognition;computer science;speech;linguistics	AI	-17.29320717588764	-94.33558897860775	155621
e8036050a1e63372c050085b09b1a04207a7e7b0	a family of discriminative manifold learning algorithms and their application to speech recognition	graph theory;computational geometry;graph embedding discriminative manifold learning algorithms noise robust automatic speech recognition feature space dimensionality reduction local manifold structure preservation separability maximization feature vectors manifold space nonlinear kernels distance measures euclidean distance cosine correlation based distance task domains noise corrupted utterances connected digits read newspaper text word error rate wer acoustic noise conditions;vectors;speech recognition cosine distances dimensionality reduction discriminative manifold learning feature extraction graph embedding;feature extraction;manifolds vectors speech algorithm design and analysis covariance matrices kernel noise;speech recognition;vectors computational geometry feature extraction graph theory learning artificial intelligence speech recognition;learning artificial intelligence	This paper presents a family of discriminative manifold learning approaches to feature space dimensionality reduction in noise robust automatic speech recognition (ASR). The specific goal of these techniques is to preserve local manifold structure in feature space while at the same time maximizing the separability between classes of feature vectors. In the manifold space, the relationships among the feature vectors are defined using nonlinear kernels. Two separate distance measures are used to characterize the kernels, namely the conventional Euclidean distance and a cosine-correlation based distance. The performance of the proposed techniques is evaluated on two task domains involving noise corrupted utterances of connected digits and read newspaper text. Performance is compared to existing approaches used for feature space transformations, including linear discriminant analysis (LDA) and locality preserving linear projections (LPP). The proposed approaches are found to provide a significant reduction in word error rate (WER) with respect to the more well-known techniques for a variety of noise conditions. Another contribution of the paper is to quantify the interaction between acoustic noise conditions and the shape and size of local neighborhoods which are used in manifold learning to define local relationships among feature vectors. Based on this analysis, a procedure for reducing the impact of varying acoustic conditions on manifold learning is proposed .	acoustic cryptanalysis;algorithm;euclidean distance;feature vector;linear discriminant analysis;linear separability;locality of reference;nonlinear dimensionality reduction;nonlinear system;speech recognition;word error rate	Vikrant Singh Tomar;Richard C. Rose	2014	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2013.2286906	feature learning;speech recognition;feature vector;feature extraction;computational geometry;computer science;graph theory;machine learning;pattern recognition;mathematics;nonlinear dimensionality reduction;k-nearest neighbors algorithm;dimensionality reduction;manifold alignment	ML	-16.931790813517132	-94.60722573439978	157585
253f7ed4100c702dbee05a005a5f1745fb08bf52	biosec multimodal biometric database in text-dependent speaker recognition	speaker recognition	In this paper we briefly describe the BioSec multimodal biometric database and analyze its use in automatic text-dependent speaker recognition research. The paper is structured into four parts: a short introduction to the problem of text-dependent speaker recognition; a brief review of other existing databases, including monomodal text-dependent speaker recognition databases and multimodal biometric recognition databases; a description of the BioSec database; and, finally, an experimental section in which speaker recognition results on BioSec and other database widely used in speaker recognition are presented and compared, using the same underlying speaker recognition technique in all cases.	biometrics;database;multimodal interaction;speaker recognition	Doroteo Torre Toledano;Daniel Hernández López;Cristina Esteve-Elizalde;Julian Fiérrez;Javier Ortega-Garcia;Daniel Ramos-Castro;Joaquín González-Rodríguez	2008			speech recognition;speaker recognition;speaker diarisation;computer science;biometrics;pattern recognition;artificial intelligence	AI	-16.06795555571261	-96.87955493733908	162297
ebfb7411cb6e493efc4225649cdc04b1b1b838c7	particle swarm optimization based nearest neighbor algorithm on chinese text categorization	training particle swarm optimization text categorization optimization vectors error analysis equations;text analysis;parameter optimization particle swarm optimization nearest neighbor k weighted nearest neighbor text categorization;text analysis natural language processing particle swarm optimisation pattern classification;pattern classification;particle swarm optimisation;natural language processing;minimum categorization error rate particle swarm optimization based nearest neighbor algorithm chinese text categorization nearest neighbor classifier single objective optimization problem solution vector	In this paper, the nearest neighbor method on Chinese text categorization is formulated as an optimization problem. The particle swarm optimization is utilized to optimize a nearest neighbor classifier to solve the Chinese text categorization problem. The parameter k was first optimized to obtain the minimum error, then the categorization problem is formulated as a discrete, constrained, and single objective optimization problem. Each dimension of solution vector is dependent on each other in the solution space. The parameter k and the number of labeled examples for each class are optimized together to reach the minimum categorization error. In the experiment, with the utilization of particle swarm optimization, the performance of a nearest neighbor algorithm can be improved, and the algorithm can obtain the minimum categorization error rate.	categorization;document classification;feasible region;mathematical optimization;multi-objective optimization;nearest neighbor search;nearest neighbour algorithm;nearest-neighbor interpolation;optimization problem;particle swarm optimization;statistical classification;support vector machine;swarm intelligence;text corpus	Shi Cheng;Yuhui Shi;Quande Qin;Tiew On Ting	2013	2013 IEEE Symposium on Swarm Intelligence (SIS)	10.1109/SIS.2013.6615174	nearest-neighbor chain algorithm;large margin nearest neighbor;multi-swarm optimization;best bin first;speech recognition;machine learning;pattern recognition;mathematics;nearest neighbor search;metaheuristic	AI	-18.989124859446317	-94.55226275329544	162874
fe014e04e099362d4c122995844f2addea4fec5e	probabilistic kernel principal component analysis through time	modelizacion;modelo markov oculto;dato observacion;densite probabilite;kernel principal component analysis;analisis componente principal;analisis estadistico;probability density;modele markov cache;hidden markov model;database;voice disorders;base dato;time series;probabilistic approach;classification;voice;voz;densidad probabilidad;modelisation;statistical analysis;mixture model;enfoque probabilista;approche probabiliste;principal component analysis;analyse statistique;serie temporelle;base de donnees;analyse composante principale;serie temporal;teoria mezcla;donnee observation;reseau neuronal;classification accuracy;high order statistics;mixture theory;modeling;theorie melange;clasificacion;red neuronal;observation data;neural network;voix	Masters in Electrical Engineering, Universidad Tecnologica de Pereira, Colombia 2006 In my masters thesis, Dimensionality Reduction of Dynamic Features using Markov processes for automatic identification of pathologies in biosignals, I proposed a combination between Hidden Markov Models and different versions of Principal Component Analysis to reduce the dimensionality of features varying through time. I applied the method to the detection of abnormalities in ECG signals and voice signals. Thesis Grade: Outstanding. Average Grade: 4.9/5.	automatic identification and data capture;dimensionality reduction;electrical engineering;hidden markov model;kernel principal component analysis;markov chain	Mauricio A. Álvarez;Ricardo Henao	2006		10.1007/11893028_83	principal component regression;econometrics;probability density function;speech recognition;systems modeling;biological classification;kernel principal component analysis;computer science;machine learning;time series;mixture model;mathematics;voice;artificial neural network;hidden markov model;statistics;principal component analysis	AI	-12.972632691030052	-97.28300181912063	164466
6329213271f3a3189802a67d8420f018e267ee15	hidden markov model-based gesture recognition with fmcw radar	chirp;training;hidden markov models;feature extraction;radar imaging;gesture recognition	In this paper we present experimental results for the development of a gesture recognition system using a 77 GHz FMCW radar system. We measure the micro-Doppler signature of a gesturing hand to construct an energy distribution in velocity space over time. A gesturing hand is fundamentally a dynamical system with unobservable “state” (i.e. the name of the gesture) which determines the sequence of associated observable velocity-energy distributions, so a Hidden Markov Model is used to for gesture recognition, a more tailored approach than the SVM classifiers used in previous work. We also describe a method for reducing the length of our feature vectors by a factor of 12 without hurting the recognition performance, by reparameterizing them in terms of a sum of Gaussians.	dynamical system;feature vector;gesture recognition;hidden markov model;markov chain;observable;radar;velocity (software development)	Greg Malysa;Dan Wang;Lorin Netsch;Murtaza Najabat Ali	2016	2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2016.7905995	computer vision;speech recognition;computer science;pattern recognition	Robotics	-10.157014974389353	-94.90802539411769	164920
c79c54e585ec763509ac2d8c051cde37ff3e2e59	an endpoint relaxation method for dynamic time warping algorithms	disruption tolerant networking;relaxation methods heuristic algorithms vocabulary testing error correction pattern recognition error analysis speech physics disruption tolerant networking;relaxation methods;vocabulary;speech;testing;physics;error analysis;heuristic algorithms;error correction;pattern recognition;error rate;dynamic time warping	Inaccurate detection of the endpoints of the test and reference patterns is a major source of errors in discrete utterance recognition by dynamic time warping. If the vocabulary contains similar sounding words whose differences are at their beginnings or ends as in the alphadigit vocabulary, the error rate may greatly increase due to endpoint detection errors. Several methods to improve the recognition accuracy by relaxing or adjusting the endpoints have been suggested. They, however, do not work well in all cases and actually the error rate may increase. We propose a new method to compensate for endpoint detection errors and compare our improved method with two existing methods on the alphadigit vocabulary.	algorithm;communication endpoint;dynamic time warping;linear programming relaxation;relaxation (approximation);relaxation (iterative method)	Seppo Haltsonen	1984		10.1109/ICASSP.1984.1172349	error detection and correction;speech recognition;word error rate;computer science;speech;theoretical computer science;machine learning;dynamic time warping;delay-tolerant networking;software testing;statistics	ML	-15.854837205301322	-95.76081457519328	167274
94672b50eaf6be110d8ce3c908ffc72a265e3e80	histogram equalization utilizing window-based smoothed cdf estimation for feature compensation	mismatching;cumulative distribution function;appareillage essai;robust speech recognition;tecnologia electronica telecomunicaciones;test statistique;order statistic;egalisation;feature compensation;fonction repartition;correction erreur;error relativo;speech processing;test estadistico;statistique ordre;additive noise;statistical test;tratamiento palabra;ruido aditivo;traitement parole;equalization;bruit additif;window function;funcion distribucion;histogram;distribution function;window based cdf estimation;relative error;cepstral analysis;desadaptacion;reconocimiento voz;histogramme;analyse cepstrale;igualacion;error correction;aparato ensayo;estadistica orden;erreur relative;fonction de fenetre;testing equipment;speech recognition;desadaptation;reconnaissance parole;correccion error;tecnologias;grupo a;histograma;funcion de ventana;histogram equalization	In this letter, we propose a new histogram equalization method to compensate for acoustic mismatches mainly caused by corruption of additive noise and channel distortion in speech recognition. The proposed method employs an improved test cumulative distribution function (CDF) by more accurately smoothing the conventional order statisticsbased test CDF with the use of window functions for robust feature compensation. Experiments on the AURORA 2 framework confirmed that the proposed method is effective in compensating speech recognition features by reducing the averaged relative error by 13.12% over the order statisticsbased conventional histogram equalization method and by 58.02% over the mel-cepstral-based features for the three test sets. key words: feature compensation, histogram equalization, robust speech recognition, window-based CDF estimation	acoustic cryptanalysis;additive white gaussian noise;approximation error;cepstrum;distortion;histogram equalization;smoothing;speech recognition;utility functions on indivisible goods;window function	Youngjoo Suh;Hoirin Kim;Munchurl Kim	2008	IEICE Transactions	10.1093/ietisy/e91-d.8.2199	statistical hypothesis testing;approximation error;order statistic;error detection and correction;speech recognition;equalization;telecommunications;cumulative distribution function;computer science;histogram matching;distribution function;speech processing;histogram;window function;adaptive histogram equalization;histogram equalization;statistics	ML	-13.666927576101982	-95.70245700672092	171769
708a47cd0072cd48b31e01f644f4063febcada43	automatic prototype extracion for adaptive ocr	additional text recognition automatic prototype extraction adaptive ocr bayesian method character bitmap isolation paragraph length samples heavily degraded text images text transcript error tolerance multifont commercial ocr software labeled character images;prototypes optical character recognition software character recognition bayesian methods robustness image recognition text recognition design engineering systems engineering and theory degradation;bayes methods;optical character recognition;adaptive signal processing;document image processing;adaptive signal processing optical character recognition bayes methods document image processing	A Bayesian method of isolating character bitmaps from paragraph-length samples of heavily degraded text images as demonstrated. The method requires a transcript of the text, but it is suficiently robust t o tolerate errors in transcripts obtained from multifont commercial OCR software. The resulting prototypes (labeled character images) are used to recognize additional text in the same document.	bitmap;comparison of optical character recognition software;prototype	George Nagy;Yihong Xu	1997		10.1109/ICDAR.1997.619856	adaptive filter;computer vision;speech recognition;intelligent character recognition;computer science;intelligent word recognition;pattern recognition;optical character recognition	Graphics	-11.161081458017335	-97.3750200519869	176502
80bdf7022d05ffd131e40b277ab0119ceb7a9ecb	texture representation and retrieval using the causal autoregressive model	texture representation;ar model;cbir;estimated parameters;texture retrieval;autoregressive model;precision recall;perceptual meaning;parameter estimation;content based image retrieval;causality	In this paper we propose to revisit the well-known autoregressive model (AR) as a texture representation model. We consider the AR model with causal neighborhoods. First, we will define the AR model and discuss briefly the parameters estimation process. Then, we will present the synthesis algorithm and we will show some experimental results. A perceptual interpretation of the AR estimated parameters will be then proposed and discussed. In particular, a computational measure to estimate the degree of randomness/regularity of textures is proposed. The set of the estimated parameters will be then applied in content-based image retrieval (CBIR) to model texture content and experimental results are shown. Benchmarking, using the precision/recall measures conducted on the well-known Brodatz database, shows interesting results. 2010 Elsevier Inc. All rights reserved.	algorithm;autoregressive model;causal filter;computation;content-based image retrieval;estimation theory;microsoft query;precision and recall;randomness;relevance;user interface;whole earth 'lectronic link	Noureddine Abbadeni	2010	J. Visual Communication and Image Representation	10.1016/j.jvcir.2010.04.004	computer vision;machine learning;pattern recognition;mathematics;autoregressive model;statistics	Vision	-11.332981912193192	-95.27785243993371	177353
b7a8a2c98cfba4415d91eb483b7840a8209d56e9	hmm/mlp speech recognition system using a novel data clustering approach		We present a novel approach for large speech databases quantization. It uses an unsupervised iterative process to regulate a similarity measure to set the number of clusters and their boundaries, thus overcoming the shortcomings of conventional clustering algorithms such as k-Means and Fuzzy C-Means, which require a priori knowledge of the number of clusters and a similarity measure that follows the data distribution, and which are sensitive to the choice of the initial configuration. The integration of the proposed clustering algorithm into an automatic hybrid HMM/MLP speech recognition system showed improved recognition accuracy in comparison with that obtained by FCM clustering whose outcome was optimized by a genetic algorithm.	acoustic cryptanalysis;automated system recovery;cluster analysis;data structure;database;deep belief network;deep learning;experiment;fuzzy cognitive map;genetic algorithm;hidden markov model;iteration;k-means clustering;memory-level parallelism;similarity measure;software release life cycle;speech recognition;text corpus;unicode collation algorithm;unsupervised learning	Lilia Lazli;Mounir Boukadoum;Otmane Aït Mohamed	2017	2017 IEEE 30th Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2017.7946644	flame clustering;fuzzy clustering;correlation clustering;computer science;cluster analysis;machine learning;determining the number of clusters in a data set;canopy clustering algorithm;speech recognition;cure data clustering algorithm;pattern recognition;brown clustering;artificial intelligence	DB	-17.719781928320895	-95.97714502290185	180920
60da84ea376cf4a68f52632756f91e8049f85620	parametric subspace modeling of speech transitions	extraction information;modelizacion;gtm;analisis componente principal;base donnee;methode parametrique;information extraction;caracteristica dinamica;reconocimiento palabra;metodo parametrico;speech processing;parametric method;database;tratamiento palabra;traitement parole;base dato;diphones;classification;isolet;experimental result;modelisation;principal component analysis;principal curves;resultado experimental;analyse composante principale;caracteristique dynamique;speech recognition;reconnaissance parole;recurrent neural network;dynamic characteristic;time constraint pca;resultat experimental;timit;modeling;speech dynamics;clasificacion;subspace trajectory;extraction informacion;time constraint	This report describes an attempt at capturing segmental transition information for speech recognition tasks. The slowly varying dynamics of spectral trajectories carries much discriminant information that is very crudely modelled by traditional approaches such as HMMs. In approaches such as recurrent neural networks there is the hope, but not the convincing demonstration, that such transitional information could be captured. The method presented here starts from the very di erent position of explicitly capturing the trajectory of short time spectral parameter vectors on a subspace in which the temporal sequence information is preserved. We approach this by introducing a temporal constraint into the well known technique of Principal Component Analysis. On this subspace, we attempt a parametric modelling of the trajectory, and compute a distance metric to perform classi cation of diphones. We use the principal curves method of Hastie and Stuetzle and the Generative Topographic map (GTM) technique of Bishop, Svenson and Williams to describe the temporal evolution in terms of latent variables. On the di cult problem of /bee/, /dee/, /gee/ we are able to retain discriminatory information with a small number of parameters. Experimental illustrations present results on ISOLET and TIMIT database. Zusammenfassung Dieser Bericht beschreibt den Versuch Informationen  uber dynamische Transitionen in phonetischen Sprachsegmenten zu erfassen, um sie f ur die Spracherkennung nutzbar zu machen. Gerade die dynamischen Prozesse der spektralen Trajektoren reprasentieren charakteristische Unterscheidungsmerkmale, welche durch die traditionellen statistischen Mustererkenner, wie z.B. Hidden Markov Model, ungen ugend ber ucksichtigt werden. Man ho te, durch die Anwendung von rekursiven neuronalen Netzen (RNNs) diese dynamischen Informationen besser in Systeme integrieren zu konnen, welches aber nicht  uberzeugend belegt werden konnte. In diesem Bericht wird von einem unterschiedlichen Blickwinkel aus gezeigt, wie Trajektoren, die aus spektralen Parametervektoren gebildet werden, explizit modelliert werden konnen. Diese Modellierung erfolgt in einem Unterraum, der zeitlich-sequenzielle Informationen erh alt. Dies wird durch die Integrierung einer zeitbezogenen Nebenbedingung in die Standardmethode Principal Component Analysis realisiert. In diesem Unterraum erfolgt eine parametrische Modellierung der Trajektoren. Mittels einer Abstandsmetrik wird eine Klassi zierung von Diphonen vorgenommen. Mit den Methoden Principal Curves von Hastie/Stuetzle und der Generative Topographic Map (GTM) von Bishop, Svenson und Williams wird die zeitliche Entwicklung der Vektoren mit Hilfe von latenten Variablen beschrieben. An der Problematik zur Unterscheidung der Diphone /bee/, /dee/ und /gee/ mit Hilfe von charakteristischen Trajektoren zeigen wir, da eine hohe Klassi zierungsrate erreichbar ist, wobei eine sehr geringe Anzahl von Parametern ben otigt wird. Unsere Ergebnisse werden mit Hilfe der Datenbanken ISOLET und TIMIT experimentell illustriert, die in den Bericht integriert sind. 1	artificial neural network;discriminant;eine and zwei;generative topographic map;hidden markov model;internet explorer;latent variable;markov chain;parametric oscillator;parity (physics);principal component analysis;recurrent neural network;sie (file format);solid modeling;speech recognition;timit;topography;unified model;whole earth 'lectronic link	Klaus Reinhard;Mahesan Niranjan	1999	Speech Communication	10.1016/S0167-6393(98)00067-3	speech recognition;systems modeling;biological classification;computer science;artificial intelligence;recurrent neural network;machine learning;speech processing;information extraction;statistics;principal component analysis	ML	-13.125267903163078	-97.69731643298452	182224
7f30456262691daaa31d26cc3a55de0fc922f259	efficient speaker change detection using adapted gaussian mixture models	cable television;analyse parole;broadcast news;phonetic heterogeneity;algorithme rapide;analytical models;processus gauss;evaluation performance;phonetique;news;change detection;document audiovisuel;high resolution;algorithm performance;performance evaluation;gaussian processes;analisis palabra;implementation;acoustics;evaluacion prestacion;speech analysis;probabilistic approach bilateral scoring phonetic heterogeneity;speaker recognition gaussian processes;probabilistic approach;indexing terms;streaming media loudspeakers computational efficiency change detection algorithms testing acoustic signal detection broadcasting performance analysis speech recognition indexing;speaker recognition;accuracy;haute resolution;gaussian mixture model;computational modeling;adaptation model;precision;efficient implementation;deteccion cambio;heterogeneidad;resultado algoritmo;enfoque probabilista;approche probabiliste;documento audiovisual;fast algorithm;alta resolucion;performance algorithme;phonetic variation speaker change detection gaussian mixture models computational efficiency;audiovisual document;teledistribution;mathematical model;fonetica;noticias;teoria mezcla;phonetics;gaussian process;detection changement;implementacion;proceso gauss;computational efficiency;mixture theory;actualites;bilateral scoring;theorie melange;article;algoritmo rapido;teledistribucion;heterogeneity;heterogeneite;data models	A new approach to speaker change detection is proposed and investigated. The method, which is based on a probabilistic framework, provides an effective means for tackling the problem posed by phonetic variation in high-resolution speaker change detection. Additionally, the approach incorporates the capability for dealing with undesired effects of variations in speech characteristics. Using the experimental investigations conduced with clean and broadcast news audio, it is shown that the proposed method is significantly more effective than the currently popular techniques for speaker change detection. To enhance the computational efficiency of the proposed method, modified implementation algorithms are introduced which are based on the exploitation of the redundant operations and a fast scoring procedure. It is shown that, through the use of the proposed fast algorithm, the computational efficiency of the approach can be increased by over 77% without significant reduction in its accuracy. The paper discusses the principles and characteristics of the proposed speaker change detection method, and provides a detailed description of its efficient implementation. The experiments, investigating the performance of the proposed method and its effectiveness in relation to other approaches, are described and an analysis of the results is presented.	algorithm;algorithmic efficiency;bayesian information criterion;computation;database normalization;experiment;http 404;image resolution;mixture model;pc speaker;pattern matching;real-time clock	Amit S. Malegaonkar;Aladdin M. Ariyaeeinia;Perasiriyan Sivakumaran	2007	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2007.896665	phonetics;speaker recognition;speech recognition;computer science;artificial intelligence;machine learning;gaussian process;mathematics;accuracy and precision;statistics	SE	-14.77774072790112	-95.5999589581829	183214
0619b8450217b4fc519809b554d72315df996c4f	influence of gsm speech coding on the performance of text-independent speaker recognition	databases;desciframiento;codage parole;evaluation performance;circuit codeur;degradation;base donnee;mobile radiocommunication;coding circuit;speech gsm feature extraction speech coding speaker recognition speech recognition databases;performance evaluation;decodage;decoding;telecommunication sans fil;reconocimiento palabra;evaluacion prestacion;degradacion;sistema gsm;speech processing;database;tratamiento palabra;traitement parole;base dato;speech;speech coding;websearch;radiocommunication service mobile;speaker recognition;gsm system;gaussian mixture model;reconnaissance caractere;telecomunicacion sin hilo;feature extraction;circuito codificacion;reconnaissance locuteur;speech recognition;teoria mezcla;reconnaissance parole;systeme gsm;gsm;radiocomunicacion servicio movil;mixture theory;theorie melange;bibliotheque numerique rero doc;character recognition;reconocimiento caracter;wireless telecommunication	We have investigated the influence of GSM speech coding in the performance of a text-independent speaker recognition system based on Gaussian Mixture Models (GMM). The performance degradation due to the utilization of the three GSM speech coders was assessed, using three trans-coded databases, obtained by passing the TIMIT through each GSM coder / decoder. The recognition performance was also assessed using the original TIMIT and its 8 kHz downsampled version. Then, different experiments were carried out in order to explore feature calculation directly from the GSM EFR encoded parameters and to measure the degradation introduced by different aspects of the coder.	database;decimation (signal processing);elegant degradation;enhanced full rate;experiment;google map maker;mixture model;source-to-source compiler;speaker recognition;speech coding;timit	Sara Grassi;Laurent Besacier;Alain Dufaux;Michael Ansorge;Fausto Pellandini	2000	2000 10th European Signal Processing Conference		speech recognition;telecommunications;engineering;communication	ML	-13.656477486860531	-95.80433229180609	183824
f3b714f4491179a9b7867c7c4e5927eb16cba451	multiphase learning for an interval-based hybrid dynamical system	hybrid dynamical system;tecnologia electronica telecomunicaciones;interval transition system identification;interval transition;clustering of dynamical systems;dynamic system;system identification;expectation maximization algorithm;tecnologias;grupo a	This paper addresses the parameter estimation problem of an interval-based hybrid dynamical system (interval system). The interval system has a two-layer architecture that comprises a finite state automaton and multiple linear dynamical systems. The automaton controls the activation timing of the dynamical systems based on a stochastic transition model between intervals. Thus, the interval system can generate and analyze complex multivariate sequences that consist of temporal regimes of dynamic primitives. Although the interval system is a powerful model to represent human behaviors such as gestures and facial expressions, the learning process has a paradoxical nature: temporal segmentation of primitives and identification of constituent dynamical systems need to be solved simultaneously. To overcome this problem, we propose a multiphase parameter estimation method that consists of a bottom-up clustering phase of linear dynamical systems and a refinement phase of all the system parameters. Experimental results show the method can organize hidden dynamical systems behind the training data and refine the system parameters successfully. key words: hybrid dynamical system, interval transition, system identification, clustering of dynamical systems, expectation-maximization algorithm	automaton;bottom-up proteomics;cluster analysis;dynamical system;estimation theory;expectation–maximization algorithm;finite-state machine;hybrid system;newton's method;refinement (computing);system identification	Hiroaki Kawashima;Takashi Matsuyama	2005	IEICE Transactions	10.1093/ietfec/e88-a.11.3022	recurrence quantification analysis;linear dynamical system;dynamical systems theory;random dynamical system;system identification;expectation–maximization algorithm;computer science;artificial intelligence;dynamical system;machine learning;control theory;mathematics	AI	-6.5968128902020196	-97.98262631687732	184192
5a62616a072923bcd8fa4ad9d2bdd597ba05d1d5	flowavenet : a generative flow for raw audio		Most of modern text-to-speech architectures use a WaveNet vocoder for synthesizing a high-fidelity waveform audio, but there has been a limitation for practical applications due to its slow autoregressive sampling scheme. A recently suggested Parallel WaveNet has achieved a real-time audio synthesis by incorporating Inverse Autogressive Flow (IAF) for parallel sampling. However, the Parallel WaveNet requires a two-stage training pipeline with a well-trained teacher network and is prone to mode collapsing if using a probability distillation training only. We propose FloWaveNet, a flow-based generative model for raw audio synthesis. FloWaveNet requires only a single maximum likelihood loss without any additional auxiliary terms and is inherently parallel due to the flow-based transformation. The model can efficiently sample the raw audio in real-time with a clarity comparable to the original WaveNet and ClariNet. Codes and samples for all models including our FloWaveNet is available via GitHub: https://github.com/ksw0306/FloWaveNet	autoregressive model;generative model;integrated architecture framework;loss function;real-time cmix;real-time clock;sampling (signal processing);speech synthesis;vocoder;waveform	Sungwon Kim;Sang-gil Lee;Jongyoon Song;Sungroh Yoon	2018	CoRR		speech recognition;generative grammar;artificial intelligence;sampling (statistics);inverse;generative model;flow (psychology);autoregressive model;raw audio format;pattern recognition;waveform;computer science	ML	-18.105276247678084	-97.09358281705988	186394
89daff445afca411a2f20a8983e8d37a9a0e0340	a possibly misleading conclusion as to the inferiority of one method for pattern recognition to a second method to which it is guaranteed to be superior	instruments;handwriting recognition;logic;testing;pattern recognition logic character recognition inspection testing instruments correlation telephony lead handwriting recognition;inspection;telephony;lead;pattern recognition;correlation;character recognition		pattern recognition	Leonard Uhr	1961	IRE Trans. Electronic Computers	10.1109/TEC.1961.5219163	speaker recognition;computer vision;lead;speech recognition;inspection;feature;intelligent character recognition;computer science;engineering;pattern recognition;handwriting recognition;telephony;logic;correlation;signature recognition	Vision	-11.038366901968873	-97.39125096827716	187893
54a6d20c31bf2e17ad84a9d4951a8d8200062d9f	a fundamental frequency estimator for the real-time processing of musical sounds for cochlear implants	analyse parole;evaluation performance;performance evaluation;pitch acoustics;signal audio;taux erreur;analisis palabra;real time;evaluacion prestacion;speech processing;speech analysis;audio signal;tratamiento palabra;traitement parole;real time processing;tonie;frecuencia fundamental;speech perception;musical instruments;journal article;vocal;sung vowels;verbal perception;musical sound;tratamiento tiempo real;codificacion;traitement temps reel;percepcion verbal;musical instrument;reconocimiento voz;instrumento musical;methode domaine frequence;frequency domain method;altura sonida;coding;instrument musique;estimacion parametro;signal acoustique;error rate;speech recognition;ruido blanco;voyelle;pitch;acoustic signal;reconnaissance parole;parameter estimation;metodo dominio frecuencia;estimation parametre;son musical;frequency domain;bruit blanc;indice error;vowel;cochlear implant;frequence fondamentale;artificial intelligence and image processing;white noise;fundamental frequency;senal acustica;senal audio;sonido musical;codage;perception verbale	A real-time fundamental frequency (F0) estimator that operates in the frequency domain was developed for the processing of musical sounds in cochlear-implant (CI) sound processors. Its performance was evaluated with male and female sung-vowel stimuli in quiet, and in white noise and babble noise. The error rates of the developed F0 estimator were much lower than those of a temporal F0 estimator that was previously used in CI sound processors, and were comparable to the published error rates of F0 estimators that were designed for other applications and evaluated with speech or musical instrument stimuli. It is envisaged that the experimental F0 estimator will be used in advanced CI coding strategies to improve the perception of pitch by CI users, which may result in improved perception of musical sounds, as well as improved speech perception for tonal languages. 2007 Elsevier B.V. All rights reserved.	central processing unit;cochlear implant;pitch (music);real-time clock;real-time transcription;white noise	Justin A. Zakis;Hugh J. McDermott;Andrew E. Vandali	2007	Speech Communication	10.1016/j.specom.2006.12.001	pitch (music);speech recognition;speech perception;word error rate;computer science;audio signal;speech processing;pitch;linguistics;fundamental frequency;white noise;coding;estimation theory;frequency domain;statistics	Robotics	-11.821509014788209	-95.98627854372297	190546
3336b953c45760d2c1cceeebfe084c11c17df1a7	speech enhancement from noise: a regenerative approach	filtering;filtrage;analisis componente principal;eigen filters;analisis datos;speech processing;filtrado;ruido;tratamiento palabra;traitement parole;speech enhancement;data analysis;signal regeneration;principal component analysis;bruit;analyse composante principale;analyse donnee;noise	Abstract   In this paper a speech enhancement technique is proposed based on principal component analysis and a new criterion for the selection of the parsimonious number of components for noise-free signal regeneration. Both isolated phonemes and continuous speech experiments are presented. The results have been evaluated by informal listening and SNR computations, which show that the methodology has an improved performance compared to existing techniques.	speech enhancement	Markos Dendrinos;Stelios Bakamidis;George Carayannis	1991	Speech Communication	10.1016/0167-6393(91)90027-Q	filter;speech recognition;computer science;noise;speech processing;data analysis;statistics;principal component analysis	NLP	-13.350513606293143	-94.84220011850418	192528
8e9ab5528ce259f24fae63acbff7d1ed1232ad9c	time-based clustering for phonetic segmentation	linear predictive coding;speech;testing;clustering algorithms;time measurement;glass;speech segmentation;acceleration;maximum likelihood estimation;metrics;speech recognition	This paper describes a approach to speech segmentation. Unlike approaches based on spectral measurements, our algorithm iteratively clusters on an LPC representation of time waveform blocks. The algorithm uses a generalized maximum likelihood criterion for deciding when two neighboring pieces of the signal should be joined. This paper describes the algorithm and shows that it yields superior results when compared to metrics based on spectral or cepstral measurements.	algorithm;cepstrum;cluster analysis;lpc;speech segmentation;waveform	Brian Eberman;William Goldenthal	1996			speech recognition;machine learning;segmentation-based object categorization;pattern recognition;mathematics;maximum likelihood sequence estimation;scale-space segmentation	AI	-12.818697416393839	-94.35632505537967	192565
4bfb1a64269563c5d4bd4998613f3c6332248498	hard c-means clustering for voice activity detection	background noise;traitement signal;evaluation performance;570 biowissenschaften biologie;base donnee;subband energy;mobile radiocommunication;performance evaluation;detection signal;diminution cout;complexite calcul;telecommunication sans fil;decision function;prototypes;evaluacion prestacion;sistema gsm;speech processing;signal detection;subband decomposition;c means;database;base dato;inmunidad ruido;fonction decision;metodo secuencial;metodo subespacio;sequential method;noise robustness;radiocommunication service mobile;senal vocal;methode sous espace;algorithme;prototipo;algorithm;570 biowissenschaften;signal vocal;minimizacion costo;accuracy;complejidad computacion;gsm system;deteccion senal;precision;noise immunity;minimisation cout;reconocimiento voz;cost minimization;descomposicion subbanda;computational complexity;clustering;telecomunicacion sin hilo;funcion decision;signal processing;spanish;signal classification;subspace method;classification signal;signal acoustique;methode sequentielle;speech recognition;ruido fondo;acoustic signal;reconnaissance parole;decomposition sous bande;classification automatique;voice activity detection;immunite bruit;systeme gsm;ddc 570;espagnol;reduccion costes;radiocomunicacion servicio movil;automatic classification;vocal signal;bruit fond;discriminacion;procesamiento senal;clasificacion automatica;real time application;prototype;cost lowering;senal acustica;discrimination;biologie;algoritmo;espanol;wireless telecommunication	An effective voice activity detection (VAD) algorithm is proposed for improving speech recognition performance in noisy environments. The proposed speech/pause discrimination method is based on a hard-decision clustering approach built on a set of subband log-energies and noise prototypes that define a cluster. Detecting the presence of speech (a new cluster) is achieved using a basic sequential algorithm scheme (BSAS) according to a given ‘‘distance’’ (in this case, geometrical distance) and a suitable threshold. The accuracy of the Cluster VAD (ClVAD) algorithm lies in the use of a decision function defined over a multiple-observation (MO) window of averaged subband log-energies and a suitable noise subspace model defined in terms of prototypes. In addition, the reduced computational cost of the clustering approach makes it adequate for real-time applications, i.e. speech recognition. An exhaustive analysis is conducted on the Spanish SpeechDat-Car databases in order to assess the performance of the proposed method and to compare it to existing standard VAD methods. The results show improvements in detection accuracy over standard VADs such as ITU-T G.729, ETSI GSM AMR and ETSI AFE and a representative set of recently reported VAD algorithms for noise robust speech processing. 2006 Elsevier B.V. All rights reserved.	adaptive multi-rate audio codec;algorithmic efficiency;analog front-end;aurora;canny edge detector;cluster analysis;computation;database;edge detection;experiment;g.729;li-chen wang;real-time clock;real-time computing;sequential algorithm;speech processing;speech recognition;voice activity detection;word error rate	Juan Manuel Górriz;Javier Ramírez;Elmar Wolfgang Lang;Carlos García Puntonet	2006	Speech Communication	10.1016/j.specom.2006.07.006	speech recognition;telecommunications;computer science;signal processing;speech processing;accuracy and precision;prototype	AI	-13.325437670048426	-96.36170299973078	193535
849f0afc3f98ff8b42dea6a37d70b58f4820719b	a hybrid svm/ddbhmm decision fusion modeling for robust continuous digital speech recognition	background noise;modelizacion;processus gauss;evaluation performance;word error rate;base donnee;procesamiento informacion;performance evaluation;taux erreur;hidden markov model;evaluacion prestacion;speech processing;modele markov variable cachee;maquina vector soporte;database;tratamiento palabra;traitement parole;base dato;decision markov;data fusion;chino;probabilistic approach;similitude;signal noise ratio;modelisation;machine vecteur support;gaussian mixture model;hidden markov models;reconocimiento voz;enfoque probabilista;approche probabiliste;fusion donnee;information processing;signal classification;similarity;decision fusion;pattern recognition;classification signal;error rate;speech recognition;ruido blanco;ruido fondo;markov decision;teoria mezcla;rapport signal bruit;relacion senal ruido;reconnaissance forme;gaussian process;reconnaissance parole;support vector machine;similitud;duration distribution based hidden markov model ddbhmm;reconocimiento patron;signal to noise ratio;bruit blanc;traitement information;fusion datos;proceso gauss;bruit fond;indice error;chinois;mixture theory;chinese;modeling;theorie melange;white noise	This paper proposes an improved hybrid support vector machine and duration distribution based hidden Markov (SVM/DDBHMM) decision fusion model for robust continuous digital speech recognition. We investigate the probability outputs combination of support vector machine and Gaussian mixture model in pattern recognition (called FSVM),and embed the fusion probability as similarity into the phone state level decision space of our duration distribution based hidden Markov model (DDBHMM) speech recognition system (named FSVM/DDBHMM). The performances of FSVM and FSVM/DDBHMM are demonstrated in Iris database and continuous mandarin digital speech corpus in 4 noise environments (white, volvo, babble and destroyerengine) from NOISEX-92. The experimental results show the effectiveness of FSVM in Iris data, and the improvement of average word error rate reduction of FSVM/DDBHMM from 6% to 20% compared with the DDBHMM baseline at various signal noise ratios (SNRs) from -5dB to 30dB by step of 5dB.	speech recognition	Jingwei Liu;Zuoying Wang;Xi Xiao	2007	Pattern Recognition Letters	10.1016/j.patrec.2006.12.007	speech recognition;word error rate;computer science;artificial intelligence;machine learning;pattern recognition;signal-to-noise ratio;hidden markov model	Vision	-13.804540838886258	-95.77026506507562	194554
6627a93cd900e7149b156110912dab7c049b0560	manifold learning-based automatic signal identification in cognitive radio networks	adaptive signal identification;automatic signal identification;intelligent technique;manifold learning;siemap;journal;cognitive radio network;signal classification;signal to noise ratio;siemap manifold learning automatic signal identification cognitive radio network adaptive signal identification signal to noise ratio intelligent technique signal classification;telecommunication computing cognitive radio learning artificial intelligence signal classification	Adaptive signal identification has been an important issue in cognitive radio networks (CRNs). Most existing techniques require high-level signal-to-noise ratio (SNR) for signal identification. This study presents an intelligent technique that focuses on a theoretical and experimental study of the signal identification by using manifold learning algorithm in CRNs. The authors pose the problem of signal identification in CRNs as signal classification by using manifold learning on high dimensions, and a novel manifold learning algorithm named as SIEMAP is proposed, which is able to identify signals in a low-dimensional space. Simulation results indicate that SIEMAP outperforms classical methods in low dimensions and is capable of identifying signal types from the received signals.	algorithm;cognitive radio;experiment;high- and low-level;nonlinear dimensionality reduction;signal-to-noise ratio;simulation	Shancang Li;Xinheng Wang;Jue Wang	2012	IET Communications	10.1049/iet-com.2010.0590	cognitive radio;speech recognition;computer science;machine learning;pattern recognition;nonlinear dimensionality reduction;signal-to-noise ratio;computer network;signal	ML	-4.74103593134179	-96.04472712911033	196984
3d5cc9c1a821d3a7d9065d48030cd83648384d45	speaker turn tracking with mobile microphones: combining location and pitch information	microphones;azimuth;in ear binaural microphones speaker turn tracking mobile microphones location information pitch information particle filtering interaural time difference itd noisy sensor motion data moving listener version speaker diarization task spectro temporal regions scenario recordings;speaker recognition acoustic signal processing microphones particle filtering numerical methods;magnetic heads;noise measurement;noise measurement microphones magnetic heads tracking noise azimuth density estimation robust algorithm;density estimation robust algorithm;active listening speaker change tracking binaural hearing pitch extraction particle filtering;tracking;noise	This paper considers the problem of using binaural microphones to track speakers in a situation where the microphones are themselves in motion (i.e. due to listener head movement). We present a general framework that applies particle filtering to combine sequential interaural time difference (ITD) cues with noisy sensor motion data. The framework is demonstrated in a meeting scenario applied to a moving-listener version of a speaker-diarization task. The paper extends previous work by investigating two potentially complementary ways of exploiting pitch track estimates in this framework, either, i) informing the time points at which speaker turn changes may occur, or ii) improving the ITD estimates by allowing integration over spectro-temporal regions grouped by pitch. Experiments using real meeting scenario recordings, made with in-ear binaural microphones, show that the latter approach leads to large and significant reductions in diarization error rate.	binaural beats;experiment;microphone;particle filter;pitch (music);speaker diarisation;speaker recognition	Heidi Christensen;Jon Barker	2010	2010 18th European Signal Processing Conference		speech recognition;acoustics;engineering;communication	Vision	-11.342513858367159	-94.34499861094615	198020
975c95e873b8776d4ef231be94ce74cb71ee6cc2	discrimination of true defect and indefinite defect with visual inspection using svm	incorrect recognition;electronic board;difficult discrimination;true defect;visual distinction;visual inspection;incorrect recognition ratio;new approach;classification approach;indefinite defect	This paper proposes a new approach to discriminate the true defect and the indefinite defect with visual distinction of the electronic board. Some classification approaches have been proposed for the limited kinds of defects and there may be some incorrect recognitions for the defect which is difficult with the visual distinction. This paper proposes the approach to reduce the incorrect recognition ratio for the defects with difficult discrimination using the margin of SVM. Real electronic board image data are tested and evaluated with the proposed approach.	fault (technology);visual inspection	Yuji Iwahori;Kazuya Futamura;Yoshinori Adachi	2011		10.1007/978-3-642-23866-6_13	computer vision;speech recognition;engineering;engineering drawing	SE	-10.928074206820849	-97.4833964486958	198206
