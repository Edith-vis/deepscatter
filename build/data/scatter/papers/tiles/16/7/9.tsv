id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
339bfaeeb9d78270ccda7ad0b2359095bad3592e	read-write causality	distributed system;formal specification;causal relation	We introduce a new kind of causality between events of a distributed system that takes the nature of the events into account. More precisely, we distinguish between read (receive) and write (send) operations, yielding a relation called . We clarify the intuition of our causality relation through examples, and we compare it with classical models of causality. Also, we show that it is better suited than the classical relations for debugging of formal speci cations.	causality;debugging;distributed computing;relation (database)	Corrado Priami;Daniel Yankelevich	1994		10.1007/3-540-58338-6_103	computer science;formal specification;causal model	Security	-17.84697884508851	29.79195326835183	40778
79ff49c72bf1bbbc35b540d62663fb3756df6021	toward formal development of programs from algebraic specifications: implementations revisited (extended abstract)	algebraic specifications;formal development;extended abstract;implementations revisited;satisfiability	The program development process is viewed as a sequence of implementation steps leading from a specification to a program. Based on an elementary notion of refinement, two notions of implementation are studied: constructor implementations which involve a construction “on top of” the implementing specification, and abstractor implementations which additionally provide for abstraction from some details of the implemented specification. These subsume most formal notions of implementation in the literature. Both kinds of implementations satisfy a vertical composition and a (modified) horizontal composition property. All the definitions and results are shown to generalise to the framework of an arbitrary institution, and a way of changing institutions during the implementation process is introduced. All this is illustrated by means of simple concrete examples.		Donald Sannella;Andrzej Tarlecki	1987		10.1007/3-540-17660-8_50	reference implementation;computer science;theoretical computer science;formal specification;algorithm	Theory	-13.878290691269461	19.532435275545964	40785
5234ac940b15193b7f9be8fc972b8dbb7cb434f5	refinement sensitive formal semantics of state machines with persistent choice	state machine;formal semantics;nondeterminism;journal article;modeling language;3 valued satisfaction;μ calculus	Modeling languages usually support two kinds of nondeterminism, an external one for interactions of a system with its environment, and one that stems from under-specification as familiar in models of behavioral requirements. Both forms of nondeterminism are resolvable by composing a system with an environment model and by refining under-specified behavior (respectively). Modeling languages usually don’t support nondeterminism that is persistent in that neither the composition with an environment nor refinements of under-specification will resolve it. Persistent nondeterminism is used, e.g., for modeling faulty systems. We present a formal semantics for UML state machines enriched with an operator “persistent choice” that models persistent nondeterminism. This semantics is based on abstract models – μ-automata with a novel refinement relation – and a sound three-valued satisfaction relation for properties expressed in the μ-calculus.	interaction;machine code;nondeterministic algorithm;persistence (computer science);persistent data structure;programming language;refinement (computing);requirement;semantics (computer science);simulation;transition system;unified modeling language	Harald Fecher;Michael Huth;Heiko Schmidt;Jens Schönborn	2009	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2009.08.006	discrete mathematics;computer science;theoretical computer science;formal semantics;unbounded nondeterminism;mathematics;finite-state machine;modeling language;programming language;algorithm	PL	-12.365235670208515	23.621628896972116	40835
fc52df5cf4f7d8c783a2c865d257509e945a2ea8	rewriting and symbolic transformations for multiscale methods	symbolic computation;partial differential equation;approximation method;equational reasoning;computer algebra system;multiscale modeling;rule based programming;multiscale method;term rewriting	This work is motivated by a challenging problem of computer-aided derivation of multiscale models of arrays of microand nanosystems. In this domain a model is a partial differential equation. Multiscale methods approximate it by another partial differential equation. The challenge is to formalize these approximating methods within a computer algebra system, e.g. Maple. Since most of the transformation steps correspond to equational reasoning (i.e. symbolic transformations based on equalities) we address the question of extending Maple with rewriting and strategies. Our contribution consists in transferring most of the term rewriting concepts and techniques to the symbolic computation community. We provide a Maple package for rule-based programming and its combination with standard Maple code. We illustrate its practical interest by applying the package functions to provide a formal proof of a convergence property of a two-scale operator.	approximation algorithm;computer algebra system;formal proof;logic programming;maple;multiscale modeling;rewriting;symbolic computation	Walid Belkhir;Alain Giorgetti;Michel Lenczner	2011	CoRR		discrete mathematics;theoretical computer science;mathematics;confluence;algorithm	Logic	-17.80585504284276	18.508259932597923	40937
1e36354a30e60d791265326e17ce7b322d39bb5a	local consistency and junction tree for diagnosis of discrete-event systems	conference paper;discrete event system;tree structure	We extend the decentralised/distributed approach of diagnosis of discrete-event systems modeled using automata. The goal is to avoid computing a global diagnosis, which is expensive, and to perform local diagnoses instead. To still ensure global consistency, we transform the topology of the system into a junction tree where each vertex represents a subsystem. Local consistency between the diagnoses of these subsystems ensures global consistency due to the tree structure. This technique will work best for systems whose natural structure is close to a tree structure, as the generated automata will be of reasonable size.	automata theory;automaton;local consistency;tree decomposition;tree structure	Priscilla Kan John;Alban Grastien	2008		10.3233/978-1-58603-891-5-209	segment tree;computer science;theoretical computer science;consistency model;machine learning;incremental decision tree;fractal tree index;tree structure;algorithm	AI	-7.497532080962577	28.152242488127836	41219
2e65869a7caeffe4a730f37fb3a15ebae5fda112	towards certifying network calculus	established theory;faulty network design;industrial tool set;dimensioning buffer;message delay;implementation error;airbus a380 afdx backbone;network calculus;subtle reasoning;embedded system	Network Calculus (NC) [5] is an established theory for determining bounds on message delays and for dimensioning buffers in the design of networks for embedded systems. It is supported by academic and industrial tool sets and has been widely used, including for the design and certification of the Airbus A380 AFDX backbone [1,3,4]. However, while the theory of NC is generally well understood, results produced by existing tools have to be trusted: some algorithms require subtle reasoning in order to ensure their applicability, and implementation errors could result in faulty network design, with unpredictable consequences. Tools used in design processes for application domains with strict regulatory requirements are subject to a qualification process in order to gain confidence in the soundness of their results. Nevertheless, given the safety-critical nature of network designs, we believe that more formal evidence for their correctness should be given. We report here on work in progress towards using the interactive proof assistant Isabelle/HOL [6] for certifying the results of NC computations. In a nutshell (cf. Figure 1), the NC tool outputs a trace of the calculations it performs, as well as their results. The validity of the trace (w.r.t. the applicability of the computation steps and the numerical correctness of the result) is then established offline by a trusted checker. The approach of result certification is useful in general for computations performed at design time, as is the case with the use of NC tools, and the idea of using interactive theorem provers for result certification is certainly not new. In particular, it is usually easier to instrument an existing tool in order to produce a checkable trace than to attempt a full-fledged correctness proof. Also, the NC tool can be implemented by a tool provider using any software development process, programming language, and hardware, and it can be updated without having to be requalified, as long as it still produces certifiable traces. In the remainder, we give a brief introduction to NC, outline our ongoing work on formalizing NC in Isabelle/HOL, and finally illustrate its use for the certification of bounds on the message delay in a toy network.	algorithm;avionics full-duplex switched ethernet;embedded system;internet backbone;network calculus;network planning and design	Etienne Mabille;Marc Boyer;Loïc Fejoz;Stephan Merz	2013		10.1007/978-3-642-39634-2_37	real-time computing;computer science;distributed computing;algorithm	Logic	-13.94671377191416	27.595683757151413	41224
142643af02cca8c4a26287a8b9e653330b35f744	operational versus weakest precondition semantics for the probabilistic guarded command language	macquarie university institutional repository;programming language semantics;probability;standards;researchonline;digital repository;macquarie university;semantics probabilistic logic markov processes cost accounting command languages cognition standards;semantics;cost accounting;programming language semantics markov processes probability;cognition;command languages;markov processes;probabilistic logic;liberal expected cumulative rewards operational precondition semantics weakest precondition semantics probabilistic guarded command language simple operational semantics dijkstra s guarded command language probabilistic choice pgcl wp semantics parameterised markov decision processes state rewards post expectation operational model pgcl program parameterised mdp liberal pre expectations	This paper proposes a simple operational semanticsof pGCL, Dijkstra's guarded command language extended withprobabilistic choice, and relates this to pGCL's wp-semantics byMcIver and Morgan. Parameterised Markov decision processeswhose state rewards depend on the post-expectation at handare used as operational model. We show that the weakest pre-expectationof a pGCL-program w.r.t. a post-expectation correspondsto the expected cumulative reward to reach a terminalstate in the parameterised MDP associated to the program. In asimilar way, we show a correspondence between weakest liberalpre-expectations and liberal expected cumulative rewards.	computation;denotational semantics;guarded command language;markov chain;morgan;operational semantics;precondition;randomized algorithm	Friedrich Gretz;Joost-Pieter Katoen;Annabelle McIver	2012	2012 Ninth International Conference on Quantitative Evaluation of Systems	10.1109/QEST.2012.21	cognition;computer science;theoretical computer science;probability;mathematics;semantics;probabilistic logic;markov process;programming language;algorithm;statistics;cost accounting	Robotics	-12.47916436543758	21.767611932371654	41282
8a4e3e35bb3c9a7ef1ddee38193b08234228c578	a finite representation of the narrowing space	comunicacion en congreso;capitulo de libro	Narrowing basically extends rewriting by allowing free variables in terms and by replacing matching with unification. As a consequence, the search space of narrowing becomes usually infinite, as in logic programming. In this paper, we introduce the use of some operators that allow one to always produce a finite graph that still represents all the narrowing derivations. Furthermore, we introduce a novel, compact equational representation of the (possibly infinite) answers computed by narrowing for a given initial term. Both the finite graphs and the equational representation of the computed answers might be useful in a number of areas, like program comprehension, static analysis, program transformation, etc.	control flow;correctness (computer science);free variables and bound variables;graph (discrete mathematics);list comprehension;logic programming;partial evaluation;program comprehension;program transformation;programming language;rewriting;sakai project;static program analysis;tree (data structure);unification (computer science)	Naoki Nishida;Germán Vidal	2013		10.1007/978-3-319-14125-1_4	computer science	AI	-18.234111230681005	24.272704703098956	41370
171ccdaa931e04c9a1cd60bde4632f9bcebb7738	a logic for non-terminating golog programs	verification;temporal logic;temporal logics;fixed point;golog;es;robot control;symbolic model checking;situation calculus	Typical Golog programs for robot control are nonterminating. Analyzing such programs so far requires metatheoretic arguments involving complex fix-point constructions. In this paper we propose a logic based on the situation calculus variant ES, which includes elements from branching time, dynamic and process logics and where the meaning of programs is modelled as possibly infinite sequences of actions. We show how properties of non-terminating programs can be formulated in the logic and, for a subset of it, how existing ideas from symbolic model checking in temporal logic can be applied to automatically verify program properties.	divergence (computer science);model checking;rewriting;robot control;situation calculus;temporal logic	Jens Claßen;Gerhard Lakemeyer	2008			linear temporal logic;verification;temporal logic;interval temporal logic;computation tree logic;computer science;artificial intelligence;theoretical computer science;fixed point;robot control;situation calculus;substructural logic;algorithm;temporal logic of actions	Logic	-16.990155837568906	21.96993011589794	41385
9e191885b57e114a33975728c4984cbdd67540be	lawful functions and program verification in miranda	langage fonctionnel;lenguaje programacion;programmation;programming language;lenguaje funcional;program verification;programacion;verificacion programa;miranda;langage programmation;term rewriting;verification programme;functional language;free algebra;programming	Laws in the Miranda programming language provide a means of implementing non-free algebraic types, by means of term rewriting. In this paper we investigate program veri cation in such a context. Speci cally, we look at how to deduce properties of functions over these `lawful' types. After examining the general problem, we look at a particular class of functions, the faithful functions. For such functions we are able, in a direct manner, to transfer properties of functions from free types to non-free types. We introduce su cient model theory to explain these transfer results, and then nd characterisations of various classes of faithful functions. Then we investigate an application of this technique to general, unfaithful, situations. In conclusion we survey Wadler's work on views and assess the utility of laws and views.	algebraic data type;formal verification;miranda;programming language;rewriting;test template framework	Simon J. Thompson	1989	Sci. Comput. Program.	10.1016/0167-6423(90)90070-T	free algebra;programming;computer science;theoretical computer science;programming language;functional programming;algorithm	Logic	-15.461623621428082	18.821267151218912	41591
1770908995932c7ace9f9d9e2fa673153b86c119	reconstructing solutions after blocked clause elimination	satisfiability;conjunctive normal form	Preprocessing has proven important in enabling efficient Boolean satisfiability (SAT) solving. For many real application scenarios of SAT it is important to be able to extract a full satisfying assignment for original SAT instances from a satisfying assignment for the instances after preprocessing. We show how such full solutions can be efficiently reconstructed from solutions to the conjunctive normal form (CNF) formulas resulting from applying a combination of various CNF preprocessing techniques implemented in the PrecoSAT solver—especially, blocked clause elimination combined with SatElite-style variable elimination and equivalence reasoning.	boolean satisfiability problem;conjunctive normal form;preprocessor;solver;turing completeness;variable elimination	Matti Järvisalo;Armin Biere	2010		10.1007/978-3-642-14186-7_30	conjunctive normal form;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;linguistics;algorithm;satisfiability	AI	-14.081342225344711	21.145792311522367	41609
4d616052d507d7a8c15582a3c4a775f28ada2a05	environmental bisimulations for delimited-control operators with dynamic prompt generation		We present sound and complete environmental bisimilarities for a variant of Dybvig et al.’s calculus of multi-prompted delimited-control operators with dynamic prompt generation. The reasoning principles that we obtain generalize and advance the existing techniques for establishing program equivalence in calculi with single-prompted delimited control. The basic theory that we develop is presented using Madiot et al.’s framework that allows for smooth integration and composition of up-to techniques facilitating bisimulation proofs. We also generalize the framework in order to express environmental bisimulations that support equivalence proofs of evaluation contexts representing continuations. This change leads to a novel and powerful up-to technique enhancing bisimulation proofs in the presence of control operators.	bisimulation;command-line interface;continuation;delimiter;r. kent dybvig;smoothing;soundness (interactive proof);turing completeness	P. A. Aristizábal AndrésA.Aristizábal;Dariusz Biernacki;Sergueï Lenglet;Piotr Polesiuk	2017	Logical Methods in Computer Science	10.23638/LMCS-13(3:27)2017	computer science;algorithm	PL	-12.014686703915139	22.17523512538612	41615
56dd32a8deb352d1b16475b03b91d986997189f1	solving games without controllable predecessor		Two-player games are a useful formalism for the synthesis of reactive systems. The traditional approach to solving such games iteratively computes the set of winning states for one of the players. This requires keeping track of all discovered winning states and can lead to space explosion even when using efficient symbolic representations. We propose a new method for solving reachability games. Our method works by exploring a subset of the possible concrete runs of the game and proving that these runs can be generalised into a winning strategy on behalf of one of the players. We use counterexample-guided backtracking search to identify a subset of runs that are sufficient to consider to solve the game. We evaluate our algorithm on several families of benchmarks derived from real-world device driver synthesis problems.	algorithm;backtracking;benchmark (computing);chaitin's constant;conjunctive normal form;device driver;heuristic (computer science);memoization;reachability;semantics (computer science)	Nina Narodytska;Alexander Legg;Fahiem Bacchus;Leonid Ryzhyk;Adam Walker	2014		10.1007/978-3-319-08867-9_35	combinatorial game theory;simulation;artificial intelligence;algorithm	Logic	-13.87421716975559	26.439091390162634	41756
54d490202bef605573a83a8cb19f5538230acbbc	empirically efficient verification for a class of infinite-state systems	developpement logiciel;sistema infinito;sistema transicion;optimisation;verificacion modelo;reachability;shared memory;classe etat;optimizacion;red petri;data path;maquina estado finito;memoria compartida;canal con perdida;verification modele;program verification;camino datos;transition system;grammaire cf;verificacion programa;systeme transition;model checking;context free grammar;desarrollo logicial;asequibilidad;state class;canal transmission avec perte;software development;gramatica independiente;transition systems;clase estado;memory systems;atteignabilite;chemin donnees;optimization;decidibilidad;decidabilite;machine etat fini;verification programme;petri net;lossy channel;systeme infini;finite state machine;reseau petri;memoire partagee;decidability;infinite system	Well-structured transition systems(WSTS) are a broad and well-studied class of infinite-state systems, for which the problem of verifying the reachability of an upward-closed set of error states is d ecidable (subject to some technicalities). Recently, Bingham proposed a new a lgorithm for this problem, but applicable only to the special cases of broadca st protocols and petri nets. The algorithm exploits finite-state symbolic mo del checking and was shown to outperform the classical WSTS verification algorit hm on a contrived example family of petri nets. In this work, we generalize the earlier results to handle a la rger class of WSTS, which we dubnicely sliceable, that includes broadcast protocols, petri nets, context-free grammars, and lossy channel systems. We also a dd an optimization to the algorithm that accelerates convergence. In addition , we introduce a new reduction that soundly converts the verification of para meterized systems with unbounded conjunctive guards into a verification probl em on nicely sliceable WSTS. The reduction is complete if a certain decid able side condition holds. This allows us to access industrially relevant chall enge problems from parameterized memory system verification. Our empirical re sults show that, although our new method performs worse than the classical ap pro ch on small petri net examples, it performs substantially better on the larger examples based on real, parameterized protocols (e.g., German’s cache coh rence protocol, with data paths).	algorithm;coh-metrix;context-free grammar;context-free language;dd (unix);linear algebra;lossy compression;mathematical optimization;petri net;reachability;verification and validation;well-structured transition system	Jesse D. Bingham;Alan J. Hu	2005		10.1007/978-3-540-31980-1_6	decidability;model checking;shared memory;computer science;artificial intelligence;theoretical computer science;software development;finite-state machine;context-free grammar;reachability;petri net;algorithm	Logic	-12.461401991770416	26.48572110574635	41932
8d69a0c0676c048e48a16aee9e4a16064f284eef	approximating the behaviour of graph transformation systems	distributed system;gramatica grafo;systeme reparti;red petri;simultaneidad informatica;graph transformation;transformation graphe;concurrency;sistema repartido;grammaire graphe;rewriting systems;graph grammar;abstract interpretation;petri net;simultaneite informatique;systeme reecriture;reseau petri	We propose a technique for the analysis of graph transformation systems based on the construction of finite structures approximating the behaviour of such systems with arbitrary accuracy. Following a classical approach, one can construct a chain of finite under-approximations (k-truncations) of the Winskel’s style unfolding of a graph grammar. More interestingly, also a chain of finite overapproximations (k-coverings) of the unfolding can be constructed and both chains converge (in a categorical sense) to the full unfolding. The finite overand under-approximations can be used to check properties of a graph transformation system, like safety and liveness properties, expressed in (meaningful fragments of) the modal μ-calculus. This is done by embedding our approach in the general framework of abstract interpretation.	abstract interpretation;algorithm;approximation;converge;graph coloring;graph rewriting;heuristic (computer science);liveness;modal logic;modal μ-calculus;model checking;petri net;point of view (computer hardware company);terminate (software);type system;unfolding (dsp implementation);verification and validation	Paolo Baldan;Barbara König	2002		10.1007/3-540-45832-8_4	combinatorics;discrete mathematics;graph embedding;concurrency;null graph;graph property;computer science;clique-width;mathematics;voltage graph;distributed computing;programming language;petri net;quartic graph;algorithm;strength of a graph	Logic	-8.622750063065677	22.842572303019086	41937
b4aff946b209434eff248e8bd28d3b761a004298	an interval-based algebra for restricted event detection	calculateur embarque;real time;event detection;program verification;sistema reactivo;verificacion programa;temps reel;boarded computer;reactive system;systeme reactif;tiempo real;verification programme;real time application;calculador embarque	In this article, we propose an interval based algebra for detection of complex events. The algebra includes a strong restriction policy in order to comply with the resource requirements of embedded or realtime applications. We prove a set of algebraic properties to justify the novel restriction policy and to establish the relation between the unrestricted algebra and the restricted version. Finally, we present an efficient algorithm that implements the proposed algebra.	algorithm;computation;embedded system;formal proof;linear algebra;real-time clock;real-time computing;requirement;time complexity	Jan Carlson;Björn Lisper	2003		10.1007/978-3-540-40903-8_10	embedded system;real-time computing;reactive system;computer science;theoretical computer science;operating system;mathematics;algorithm	Embedded	-9.897368708315593	25.83737828927011	41972
44cb6f43ebbd46e4656f217b867c129e7fd8068c	strong normalization in the π-calculus with intersection and union types	union types;intersection types	In this work we present an extension of the linear typing disc ipline for π-calculus, introduced by Yoshida,Honda and Berger, with Intersection and Un ion Types. We show that we are able to define a typing system for the π-calculus, which guarantees that every well-typed term is s trongly normalizing. This typing system is an extension of that pres ented in [30] since it is able to type more terms than that presented there. Introduction Types are a very powerful tool in order to study computational propertie s of programming languages based onλ-calculus. The use of types has been extended for the study of languag es designed to model concurrency and mobility of agents, like the π-calculus [27]. In the past many typing systems were designed for a concurrent setting in order to provide static analysis tools f r checking safety/security properties for the design of mobile systems and web applications. These pro p rties often range from simple error-free execution to secure access control (according to a s ecurity policy), control of information flow and many others. In this work we study the strong normalization prope rty, which essentially claims that computation in a process necessarily terminates, regardless of ev aluation strategy. The notion of normal form and consequently of strong normalization has be en studied extensively in λ-calculus. In the literature many typing systems for λ-calculus have been designed: simply typed λcalculus, System F, and many others. Mainly, these typing systems are built b y decorating withλ-terms Intuitionistic Logic proofs. Mainly, strong normalization proofs are semantical proofs. The proof sc hema is the following: a term model is defined where types are interpreted into set of strongly normalizing terms (calledsaturated sets ). Then avalidity theorem is proved: it states that if the typing systems are able to assign a type α to a λ-term, then this term will belong to the set interpreted by α. This argument was first used by Tait [28] to show the strong normalization property for simply typed λ-calculus and then revisited for System F by Girard [11]. 2 Mauro Piccolo / SN in theπ-calculus with Intersection and Union Types For this reason we say that these systems are soundwith respect to strong normalization. The converse property, calledcompletenessis never true neither for simply typed λ-calculus, nor for System F. Completeness claims that every strongly normalizing λ-term is typed by the system. An immediate consequence for a sound and complete typing system for strong normaliza tion is the undecidability of type inference. Intersection types were introduced by Coppo and Dezani [9], as an ex te sion of simply typedλcalculus. Using intersection types, it is possible to design a typing system whic h is sound and complete with respect to strong normalization. So an exact characterization of stron gly normalizingλ-terms is given by the intersection type system. The strong normalization proof has be en provided by Pottinger [24]. More recently, union types were added to the intersection type system in [2] as a dual connective of intersection. In this work we would like to study strong normalization property in a concurre nt s tting and to extend the typing discipline already known for λ-calculus toπ-calculus (in particular Intersection and Union types). Linearπ-calculus was introduced by Kobayashi, Pierce and Turner in [18]. Th ey give a typing system for π-calculus based on Linear Logic [12], to control in a fine way the usage o f names in processes. The linear typing discipline was greatly improved by Yoshida, Honda and Ber ger. In fact, using linearity they are able to encode fully abstractlya family of the functional calculi such as PCF [4], simply typedλ-calculus with sum and products [30], System F [5] and λμ-calculus [15]. The key idea is to limit concurrent and non-deterministic behavior of π-terms using types, in order to capture the confluent functional core of π-calculus. In this paper we use the calculus of Yoshida, Honda and Berger, which is an a ynchronous variant of theπ-calculus [27]. Computation in this calculus is generated by interaction betwee n processes. a(~x).P|a〈~t〉 −→ P{~t/~x} (1) where~x is a potentially empty vector of names, | denotes parallel composition, a(~x).P is an input and a〈~t〉 is an output. Operationally this reduction represents the consumption of an as ynchronous message by a receptor. The idea extends to a receptor with replication !a(~x).P|a〈~t〉 −→ P{~t/~x}|!a(~x).P (2) where the replicated process remains in the configuration after reduction. However, to control the usage of names carefully, it is important to control dynamic sharing of names. For this reason, by analogy with [4], we choose as language the internal fragment ofπ-calculus, where only bound name passing is allowed: this allows tight control of sharing and can control name usage in a more stringent way. So we use the asynchronous bound output a(~x) P (used also in [4]), which is a rewriting of (ν~x)(a〈~x〉|P) and we can replace (1) with a(~x).P|a(~x) Q −→ (ν~x)(P|Q) (3) In this work, we follow the approach of [30], where the main ingredient of the linear typing discipline for strong normalization are linear typesandaction types with causality Mauro Piccolo / SN in theπ-calculus with Intersection and Union Types 3 Linear Types They ensure that each channel is used exactly once in input and at most once in output, and, for replicated channels, an input occurs exactly once and outputs occur zero or more times. For instance, the process a.b|a is linear sincea andb are used exactly once in input and in output mode. But the process a.b|b.a|a is not linear since it uses a twice in output mode. However the process!a.b|!b.a|a respects linearity, since for the two outputs on a and for all outputs onb there is always a server listening on the same channel, which is able to deal with their request. The typing discipline is realized by using sortings augmented with special modalities to ma rk whether a channel is used in input/output and in a replicated/non-replicated way. Action Types with Causality Linear discipline is not sufficient to guarantee strong normalization. In fact the process(νa, b)(!a.b|!b.a|a) is still linear, even though from it an infinite reduction sequence starts. This is due to cyclic dependencies in the use of a andb. To fix this problem, we use action types with causality, where causality is represented by edges in a directed graph (where nodes are names) whose acyclicity ensures absence of circular depen dencies. The proof technique we use to prove the strong normalization property is es sentially an adaptation of Tait’s reducibility method toπ-calculus. In order to re-use the Tait’s argument in this context we need a good notion of saturated set. The idea is provided by the paper [1], in which a double negation closure of pr edicate of termination is used to prove strong normalization in Classical Logic. This idea was also develope d by Urban and Laird in Classical Logic [20, 29], by Okada in Phase Semantics [22] and by Girar d in Ludics [13]. The main notion is the notion of orthogonality; two processes P, Q are orthogonal if when composed, they always normalize i.e. they reduces to 0 for all possible sequences of reductions. This induces a closure oper at r · and a saturated set is just a set of processes P such thatP = P. After having defined saturated sets of processes, we build up the usual mode , where types are interpreted into saturated sets and we prove a validity theorem. The main contributions of this paper can be summarized as following. 1. We propose an upgrading of the linear typing discipline introduced by Yo shida, Honda and Berger in [30] with Intersection Types, Union Types and Subtyping. The upgrad ing can be briefly described as following. We replace the branching input construct in [30] w ith sums and its neutral element. We use Intersection Types for typing sums, while Union Types are n ot introduced directly, but via the Subtyping relation. In this way, we can show that we are a ble to type more terms than in [30], since here reduction is no more confluent. 2. We show, like in [30] how it is possible to obtain a type assignment system fo r theπ-calculus that guarantees that every well typed π-term is strongly normalizing. 3. We prove the strong normalization property for the π-calculus generalizing the proof in [30], by providing an opportune notion of saturated set. This generalization allows u s to simplify the formalization of results already pointed out in [30] concerning the axiomatiza tion of behavioral equivalences and the Separation property. Finally we discuss the significance of the Strong Normalization property in a co ncurrent setting, in particular its relationships with the Liveness property. Note that our typing system is not complete w.r.t. strong normalization. We hope th at, in future works, we will be able to characterize via a typing system, all strongly normaliz ing π-terms. 4 Mauro Piccolo / SN in theπ-calculus with Intersection and Union Types Related work The first work dealing with strong normalization in the π-calculus is surely [30], which is the base of our work. However there are previous works by Sangior gi [25, 26] and Kobayashi [19] dealing with similar properties in the π-calculus. Subtyping, Intersections and Union Types were already studied in π-calculus in previous works [16, 27, 8, 17]. However, the work by Castagna, Dezani-Ciancaglini and Varacca [8] was the first work in which a typing system with intersection and union type is introduced and the su b-typing relation is preserved in the semantics. Also in our work we have this property: it would be interesting to study the connections between the two works. The structure of the proof of strong normalization we present use some co nstructions (like the biorthogonality closure) which are very common in the Ludics setting [13]. A re lationship between li	normalization property (abstract rewriting);π-calculus	Mauro Piccolo	2012	Fundam. Inform.	10.3233/FI-2012-777	computer science;pure mathematics;mathematics;programming language;algorithm;union type	Logic	-9.25730134177571	19.953527095316844	42232
68cdfb6c7d676a707d0894803442cd803950c59b	identification of incompatible states in mode switching	control systems;clocks;incompatible states;switches process control control systems clocks automata maintenance engineering trajectory;control design;supervisory control theory;maintenance engineering;automata;discrete event system;discrete event systems control system synthesis;trajectory;discrete events systems control design;control system synthesis;discrete event systems;process control;supervisory control theory incompatible states mode switching mode management discrete events systems control design;process model;mode management;mode switching;switches	Mode management is one of the problems in discrete events systems control design. Even based on a simple specification, it is very difficult to prove that models of each mode and mutual interaction are correct. This paper demonstrates that supervisory control theory is an effective tool for detecting specification incompatibilities because it clearly separates process, models and specifications. We use simple cases to present a method that introduces flexibility into mode specification. This method can be used to adjust or to modify incompatibilities between specifications and thereby promotes correct mode switching.	control theory;reachability;sensor;software incompatibility	Gregory Faraut;Laurent Piétrac;Éric Niel	2008	2008 IEEE International Conference on Emerging Technologies and Factory Automation	10.1109/ETFA.2008.4638382	maintenance engineering;control engineering;real-time computing;network switch;engineering;control system;artificial intelligence;trajectory;process control;process modeling;control theory;automaton	Robotics	-5.48927436072213	28.82822504966717	42473
fd9a61681dfea05a976375577da44d5431cb787e	bisimulation on speed: lower time bounds	timed process algebra;lower time bounds;asynchrone;bisimulacion;relation semantique;asynchronous systems;relacion semantica;semantics;composicionalidad;bisimulation;discrete time;moller tofts preorder;semantica;semantique;algebra proceso;compositionnalite;informatique theorique;68q85 asynchronous systems;preordre moller tofts;compositionality;borne inferieure;algebre processus;semantic relation;systeme asynchrone;algebre processus temporise;tiempo discreto;temps discret;process algebra;faster than relation;lower bound;asincrono;asynchronous;cota inferior;computer theory;informatica teorica	More than a decade ago, Moller and Tofts published their seminal work on relating processes that are annotated with lower time bounds, with respect to speed. Their paper has left open many questions concerning the semantic theory for their suggested bisimulation–based faster–than preorder, the MT–preorder, which have not been addressed since. The encountered difficulties concern a general compositionality result, a complete axiom system for finite processes, and a convincing intuitive justification of the MT–preorder. This paper solves these difficulties by developing and employing novel tools for reasoning in discrete–time process algebra, in particular a general commutation lemma relating the sequencing of action and clock transitions. Most importantly, it is proved that the MT–preorder is fully– abstract with respect to a natural amortized preorder that uses a simple bookkeeping mechanism for deciding whether one process is faster than another. Together these results reveal the intuitive roots of the MT– preorder as a faster–than relation, while testifying to its semantic elegance. This lifts some of the barriers that have so far hampered progress in semantic theories for comparing the speed of processes.	amortized analysis;asynchronous i/o;axiomatic system;bisimulation;denotational semantics;formal verification;linear algebra;møller–plesset perturbation theory;norm (social);parametricity;process calculus;smalltalk mt;sparse matrix;theory;workbench	Gerald Lüttgen;Walter Vogler	2005	ITA	10.1051/ita:2005030	discrete time and continuous time;combinatorics;process calculus;discrete mathematics;computer science;bisimulation;asynchronous communication;mathematics;semantics;upper and lower bounds;algorithm;principle of compositionality;statistics;algebra	PL	-9.227959532870594	22.165927527687682	42531
1b5717b1d640a6790b0b175ec11337de5b913dba	interval diagrams: increasing efficiency of symbolic real-time verification	oscillations;decision diagrams;automata timing formal verification real time systems clocks reachability analysis computer networks aerospace electronics medical simulation world wide web;medical simulation;system configuration;boolean functions;clocks;real time symbolic model checking;automata theory formal verification decision diagrams boolean functions reachability analysis;real time;decision diagram;timed reachability analysis;traffic control;boolean function;discrete time;satisfiability;interval maps;embedded system;computer networks;symbolic real time verification;formal method;automata;timed automaton;formal verification;binary decision diagrams;interval decision diagrams;finite state automata;interval diagram;aerospace electronics;on the fly;automata theory;world wide web;timed automata;symbolic model checking;state transition graph;petri net;data structure;real time application;reachability analysis;symbolic model checking symbolic real time verification interval diagram timed automata formal verification boolean functions interval decision diagrams timed reachability analysis;discrete system;real time systems;binary decision diagram;timing;time constraint	In this paper, we suggest interval diagram techniquesfor formal verification of real-time systems modeled by means of timed automata. Interval diagram techniques are based on interval decision diagrams(IDDs)—representing sets of system configurations of, e.g., timed automata—and interval mapping diagrams (IMDs)— modeling their transition behavior. IDDs are canonical rep resentations of Boolean functions and allow for their efficient mani pulation. Our approach is used for performing both timed reachability analysis and real-time symbolic model checking. We present the me thods necessary for our approach and compare its results to anothe r, similar verification technique—achieving a speedup of 7 and more . 1 Introdu tion	automata theory;diagram;formal verification;interval arithmetic;model checking;reachability;real-time clock;real-time computing;real-time transcription;speedup;timed automaton	Karsten Strehl	1999		10.1109/RTCSA.1999.811303	medical simulation;data structure;computer science;theoretical computer science;boolean function;algorithm	Logic	-11.527853280700567	27.128271065719133	42577
8fff42b53444c9989eb9acbc9634b696da1f72e2	new up-to techniques for weak bisimulation	prueba;bisimilarite;technology;bisimulacion;bisimulation;abstract machine;weak bisimilarity;preuve;borne electrique;informatique theorique;technologie;borne electrico;termination;up to techniques;terminaison;proof;commutation;computer theory;tecnologia;informatica teorica	Up-to techniques have been introduced to enhance the bisimulation proof method for establishing bisimilarity results. While up-to techniques for strong bisimilarity are well understood, the irregularities that appear in the weak case make it difficult to give a unified presentation. We propose a uniform and modular theory of up-to techniques for weak bisimulation that captures most of the existing proof technology and introduces new techniques. Some proofs rely on nontrivial – and new – commutation results based on termination guarantees. All results presented in this paper have been formally proved using the Coq proof assistant.	bisimulation;coq (software);proof assistant;weak ai	Damien Pous	2007	Theor. Comput. Sci.	10.1016/j.tcs.2007.02.060	discrete mathematics;computer science;bisimulation;proof;mathematics;abstract machine;programming language;algorithm;technology	Logic	-10.407678019419832	19.333273994208476	42630
bf44b6d88593701347977dbe3bb07d9bc8cd59db	two-thirds simulation indexes and modal logic characterization	simulation;metric;two thirds simulation;process calculus;yanfang ma min zhang yixiang chen liang chen 模态逻辑 模拟 表征 抽象描述 执行情况 二元关系 近似程度 逻辑特性 two thirds simulation indexes and modal logic characterization;simulation metric two thirds simulation process calculus	Two-thirds simulation provides a kind of abstract description of an implementation with respect to a specification. In order to characterize the approximate two-thirds simulation, we propose the definition of a two-thirds simulation index which expresses the degree to which a binary relation between processes is two-thirds simulation. λ-two-thirds simulation and its substitutivity laws are given in this paper. And, based on λ-two-thirds simulation, we present a measure model for describing the degree of approximation between processes. In particular, we give the modal logical characterization of λ-two-thirds simulation.	approximation algorithm;audio feedback;correctness (computer science);descriptive complexity theory;modal logic;order of approximation;simulation;software bug;trustworthy computing	Yanfang Ma;Min Zhang;Yixiang Chen;Liang Chen	2011	Frontiers of Computer Science in China	10.1007/s11704-011-0140-9	process calculus;metric;computer science;artificial intelligence;programming language;algorithm	Logic	-11.984368346727214	22.613762173245924	42673
74e365b566601fa1f542061ba65c7e607709afd1	on bulk data type constructors and manipulation primitives: a framework for analyzing power and complexity			type constructor	Richard Hull;Jianwen Su	1989				Theory	-17.127385604882356	18.862278954329746	42741
ed6b1db22b72909484ec66f7dc4fb3e0de1048de	on the refinement and simulation of data types and processes	data types	This paper presents a behavioural semantics for abstract data types, and thus a correspondence between data types and processes. The value of this correspondence lies in the fact that simulation of the abstract data types is easily verified, and is equivalent to failures refinement of the corresponding processes. The method of constructing a semantics, and the method of proving equivalence between notions of refinement, are independent of the chosen interpretation. The same methods can be used to establish other correspondences between state-based and behavioural descriptions.	abstract data type;refinement (computing);simulation;turing completeness	Christie Marr;Jim Davies;Jim Woodcock	1999		10.1007/978-1-4471-0851-1_15	theoretical computer science;data type;abstract data type;computer science;semantics	Logic	-13.105772164798143	19.50648960812831	42976
eea1984076171804a47e38cc0da03126e17056b2	labelled calculi of resources	reduction;geometry of interaction;lambda calculus;labels;linear logic;interaction nets;explicit substitutions	Lévy’s labelled λ-calculus has played an important role in the understanding of the Geometry of Interaction and its applications to the implementation of λ-evaluators: labels relate to the multiplicative information of paths. In this paper, we generalise the structure of labels, and the underlying term structure, in order to keep track of exponential information too. We first define two labelled calculi with explicit substitutions and resource management, where labels are in close correspondence with paths in call-by-value and call-by-name translations of the λ-calculus into linear logic proof nets, respectively. We observe a tight relationship between labels and the dynamics of substitutions; this will then guide us through the design of a third calculus that combines the advantages of the previous two, where labels fully reflect the dynamics of substitutions.	bureaucracy;compiler;computation;confluence (abstract rewriting);critical pair (logic);explicit substitution;free variables and bound variables;geometry of interaction;intermediate representation;lambda calculus;lazy evaluation;linear logic;modulo operation;normalization property (abstract rewriting);on the fly;requirement;software propagation;substitution (logic);time complexity;turing completeness;universal instantiation	Maribel Fernández;Nikolaos Siafakas	2014	J. Log. Comput.	10.1093/logcom/exs021	linear logic;combinatorics;typed lambda calculus;discrete mathematics;reduction;geometry of interaction;computer science;lambda calculus;mathematics;programming language;algorithm	PL	-12.443329295112518	22.07015479463285	43027
ae1cf08a6102a29b48211b711dad48d96b09be4c	rewriting, decision procedures and lemma speculation for automated hardware verification	verification;circuito aritmetico;rewrite rule;hardware verification;essai automatique;prueba automatica;theorem prover;decision procedure;reecriture;computer hardware;verificacion;rewriting;materiel informatique;automatic test;material informatica;circuit arithmetique;reescritura;arithmetic circuit	 IntroductionThe use of a rewrite-based, induction theorem prover, Rewrite Rule Laboratory(RRL) [13] is discussed for verifying arithmetic circuits at the gate level. Itis shown that the induction scheme generation heuristic in RRL based on thecover set method [18], the integration of decision procedures [7] with contextualrewriting [17], and the intermediate lemma speculation heuristics can help, withminimal user guidance, in finding verification proofs of arithmetic circuitsRRL has... 	electronic design automation;rewriting	Deepak Kapur	1997		10.1007/BFb0028393	verification;rewriting;computer science;theoretical computer science;automated theorem proving;programming language;intelligent verification;algorithm	EDA	-15.572940113618603	28.240642739703812	43039
60306341cc2669c2048ad7cda7b109023134959b	learning automata-based adaptive petri net and its application to priority assignment in queuing systems with unknown parameters	neural networks;learning automata vectors neural networks firing adaptation models adaptive systems automata;learning automata;firing;automata;vectors;adaptive systems;stochastic pn learning automata adaptive petri net priority assignment queuing systems apn la model conflicting transitions;petri nets pns adaptive petri net apn conflict resolution learning automata;adaptation models;queueing theory learning automata petri nets	In this paper, an adaptive Petri net (PN), capable of adaptation to environmental changes, is introduced by the fusion of learning automata and PN. In this new model, called learning automata-based adaptive PN (APN-LA), learning automata are used to resolve the conflicts among the transitions. In the proposed APN-LA model, transitions are portioned into several sets of conflicting transitions and each set of conflicting transitions is equipped with a learning automaton which is responsible for controlling the conflicts among transitions in the corresponding transition set. We also generalize the proposed APN-LA to ASPN-LA which is a fusion between LA and stochastic PN (SPN). An application of the proposed ASPN-LA to priority assignment in queuing systems with unknown parameters is also presented.	algebraic petri net;automata theory;automaton;british undergraduate degree classification;computation;emoticon;first-class function;generator matrix;gramática de la lengua castellana;irreducibility;item unique identification;job stream;learning automata;level of detail;markov chain;pa-risc;pp (complexity);queueing theory;run time (program lifecycle phase);simulation;state space;steady state;substitution-permutation network;time complexity;xfig	Seyed Mehdi Vahidipour;Mohammad Reza Meybodi;Mehdi Esnaashari	2015	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2015.2406764	stochastic petri net;computer science;artificial intelligence;adaptive system;machine learning;automaton;mobile automaton;petri net;artificial neural network;algorithm	Robotics	-5.617708194661762	28.050537751004317	43064
e2c0dd9288a01f6f278a124487a4cd3c8d12fb0a	translation of a ddl digital system specification to boolean equations	design language translation;design automation;logic design automation;boolean equations;level of detail;digital systems;flip flop input equations;computer design;boolean equations computer design design language translation flip flop input equations logic design automation;flip flop;equational logic	A digital system design language, DDL, has been described and shown to provide a concise yet precise means of specifying the organization and operation of digital systems, regardless of timing mode or hardware types, at various levels of detail [2]. This paper defines a series of tasks that transform any DDL document to Boolean and next-state equations from which a system may be implemented. Each task of the transformation produces another DDL description of a system which uses fewer features of the language.	data definition language;digital electronics;systems design	James R. Duley;Donald L. Dietmeyer	1969	IEEE Transactions on Computers	10.1109/T-C.1969.222657	boolean algebra;embedded system;boolean circuit;computer architecture;parallel computing;equational logic;electronic design automation;computer science;theoretical computer science;level of detail;programming language;algorithm	EDA	-14.27921335503369	30.79741261624697	43077
5f0babb9d4d90f3c66ec52d02253f052d9dde2a4	markovian testing equivalence and exponentially timed internal actions	complete axiomatization;process calculus;modal logic;polynomial time;process development;logic in computer science	In the theory of testing for Markovian processes developed s o far, exponentially timed internal actions are not admitted within processes. When present, these acti ons cannot be abstracted away, because their execution takes a nonzero amount of time and hence can b e observed. On the other hand, they must be carefully taken into account, in order not to equ ate processes that are distinguishable from a timing viewpoint. In this paper, we recast the definiti on of Markovian testing equivalence in the framework of a Markovian process calculus including e xponentially timed internal actions. Then, we show that the resulting behavioral equivalence is a congruence, has a sound and complete axiomatization, has a modal logic characterization, and ca be decided in polynomial time.	axiomatic system;congruence of squares;modal logic;polynomial;process calculus;time complexity;turing completeness	Marco Bernardo	2009		10.4204/EPTCS.13.2	modal logic;time complexity;combinatorics;process calculus;discrete mathematics;process development execution system;computer science;mathematics;programming language;algorithm	Theory	-9.603247847173286	22.517985941691602	43097
9269df2f574022b8c65e824f39b270d66ac552d2	higher-order functional languages and intensional logic	proposed algorithm systematically;higher-order functional language;source program;intensional logic;intensional code;order zero;m-dimensional intensional program;semantically equivalent multidimensional intensional;initial functional program;m step;higher-order functional program;higher order functions;functional language;first order;functional programming;logic programming	The purpose of this dissertation is to demonstrate tl1at higher-order functional programs can be transformed into zero-order intensional ones in a semantics preserv~ng wa.y. As there exists a straightforward execution model for the resulting iu t:ensional programs, the practical outcome of our research is a promising, well-defined implcmenta.tion technique for functional languages. Ou the foundational side, the goal of our study is to bring new -. insights and a better understanding of the nature of functional programming. The starting point of our research is the work of A. Yaghi [Yag84] a.nd W. Wa.dgt' [Wad91], who were the first to define transformation algorithms from functional to intensional languages. More specifically, Yaghi studied the first-order subset of fund;ional languages, while Wadge extended Yaghi's technique to apply to a. significant class ol' highcrorder functional programs. The main shortcoming of both these works is that the trnnsformations they provide are semi-formal and consequently they lack a. conectness proof. In particular, although the algorithm in [Yag84] is relatively easy to understand intuitively, the one in [Wad91] is much more complex, making in this way imperative tlte need for a. precise formulation. We start by revising, formalizing and giving a correctness proof of Yaghi 's transformation algorithm for first-order functional programs. The formal definition we give is hased on the idea that if two expressions in the source program arc idcntica.J, then they <1.re assigned identical intensional expressions during the translation. The correctn<1fls proof of the algorithm is established by showing that a functio.t call in the extensional program has thu same meaning as the intensional expression that results from its translation. We then consider the translation of higher-ordt1r functional progra.ms into zero-order intensional ones. We demonstrate that although Wadge's algorithm is in Uw right, direction, it does not always preserve the semantics of the source programs. 'l'o overcome this deficiency, we define a richer target intensional language and an extended a)goritbm which.	algorithm;angular defect;care-of address;correctness (computer science);dataflow;emoticon;first-order predicate;functional programming;higher-order function;imperative programming;intensional logic;logic programming;parameter (computer programming);programming language;semiconductor industry	Panos Rondogiannis;William W. Wadge	1999	J. Funct. Program.		montague grammar;ontology language;genus–differentia definition;intensional logic	PL	-17.85436311410863	21.576256510979327	43195
e0247ce2670c2ea2fdc47f6afbc2a0977aea8318	induction variable analysis without idiom recognition: beyond monotonicity	cambio variable;algorithm complexity;programme commande;complexite calcul;efficient algorithm;complejidad algoritmo;exact solution;dependence;dependance;solucion exacta;algoritmo genetico;monotonie;complejidad computacion;induccion;complexite algorithme;induction;commande ecoulement;computational complexity;control program;monotonicity;changement variable;control flow;algorithme genetique;programa mando;genetic algorithm;aritmetica intervalo;monotonia;solution exacte;interval arithmetic;arithmetique intervalle;flow control;variable transformation;dependencia	Traditional induction variables (IV) analyses focus on computing the closed form expressions of variables. This paper presents a new IV analysis based on an IV property called distance interval . This property captures the value changes of a variable along a given controlflow path of a program. Based on distance intervals, an efficient algorithm detects dependences for array accesses that involve induction variables. This paper describes how to compute distance intervals and how to compute closed form expressions and test dependences based on distance intervals. This work is an extension of the previous induction variable analyses based on monotonic evolution [11]. With the same computational complexity, the new algorithm improves the monotonic evolution-based analysis in two aspects: more accurate dependence testing and the ability to compute closed form expressions. The experimental results demonstrate that when dealing with induction variables, dependence tests based on distance intervals are both efficient and effective compared to closed-form based dependence tests.	algorithm;benchmark (computing);computation;computational complexity theory;data structure;induction variable;iterator;mathematical induction;maximal set;non-monotonic logic;pointer (computer programming);pointer analysis;polaris (poker bot);recursion	Peng Wu;Albert Cohen;David A. Padua	2001		10.1007/3-540-35767-X_28	genetic algorithm;monotonic function;computer science;calculus;flow control;mathematics;interval arithmetic;programming language;computational complexity theory;control flow;algorithm	AI	-17.64226145747666	25.08060142177148	43253
15b26005bd1e499f2e26237a8f184a6abe69e4b3	describing and verifying synchronous circuits with the boyer-moore theorem prover	functional form;sequential circuits;theorem prover;hardware description language	In this paper, we address the problem of finding a simple and efficient functional form for describing synchronous sequential circuits in the Boyer-Moore logic. By simple, we mean that it must be both user-readable and easily obtained by translation from a Hardware Description Language like VHDL. By efficient, we mean that it must be well-adapted to the proof mechanisms of the tool, Nqthm. We propose two different recursive models, which are inspired from former results. We explain how they can be expressed in the Boyer-Moore logic, and we compare them on simple but illustrative examples. We also give the Nqthm proof of their equivalence. Finally, we conclude about their respective advantages and drawbacks.	automated theorem proving;hardware description language;higher-order function;nqthm;recursion;turing completeness;vhdl;verification and validation	Laurence Pierre	1995		10.1007/3-540-60385-9_3	discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-15.119236059562596	28.31932218423215	43273
742633c862dbb29e10cf127abba8ff8994947a39	property-driven benchmark generation: synthesizing programs of realistic structure	property oriented expansion;sat solving;theorem proving;model checking;path condition extraction;code motion;ltl synthesis;benchmark generation	We present a systematic approach to the automatic generation of platform-independent benchmarks of realistic structure and tailored complexity for evaluating verification tools for reactive systems. The idea is to mimic a systematic constraint-driven software development process by automatically transforming randomly generated temporal-logic-based requirement specifications on the basis of a sequence of property-preserving, randomly generated structural design decisions into executable source code of a chosen target language or platform. Our automated transformation process steps through dedicated representations in terms of Büchi automata, Mealy machines, decision diagram models, and code models. It comprises LTL synthesis, model checking, property-oriented expansion, path condition extraction, theorem proving, SAT solving, and code motion. This setup allows us to address different communities via a growing set of programming languages, tailored sets of programming constructs, different notions of observation, and the full variety of LTL properties—ranging from mere reachability over general safety properties to arbitrary liveness properties. The paper illustrates the corresponding tool chain along accompanying examples, emphasizes the current state of development, and sketches the envisioned potential and impact of our approach.	array data structure;automata theory;automated theorem proving;benchmark (computing);boolean satisfiability problem;büchi automaton;code generation (compiler);compiler;concurrency (computer science);correctness (computer science);executable;hardware description language;influence diagram;java;liveness;loop-invariant code motion;mealy machine;modal logic;modal μ-calculus;model checking;open-source software;procedural generation;process modeling;programming language;promela;randomness;reachability;refinement (computing);service-oriented device architecture;software development process;temporal logic;time complexity;toolchain;vhdl;verilog	Bernhard Steffen;Malte Isberner;Stefan Naujokat;Tiziana Margaria;Maren Geske	2014	International Journal on Software Tools for Technology Transfer	10.1007/s10009-014-0336-z	model checking;computer science;theoretical computer science;automated theorem proving;programming language;algorithm	Logic	-17.371360959608456	27.506152243573986	43274
2d977e8e5dde97390a6ed15785aa53455865229b	behavioral equivalences for higher-order languages with probabilities		Higher-order languages, whose paradigmatic example is the λ-calculus, are languages with powerful operators that are capable of manipulating and exchanging programs themselves. This thesis studies behavioral equivalences for programs with higher-order and probabilistic features. Behavioral equivalence is formalized as a contextual, or testing, equivalence, and two main lines of research are pursued in the thesis. The first part of the thesis focuses on contextual equivalence as a way of investigating the expressiveness of different languages. The discriminating powers offered by higherorder concurrent languages (Higher-Order π-calculi) are compared with those offered by higher-order sequential languages (à la λ-calculus) and by first-order concurrent languages (à la CCS). The comparison is carried out by examining the contextual equivalences induced by the languages on two classes of first-order processes, namely nondeterministic and probabilistic processes. As a result, the spectrum of the discriminating powers of several varieties of higher-order and first-order languages is obtained, both in a nondeterministic and in a probabilistic setting. The second part of the thesis is devoted to proof techniques for contextual equivalence in probabilistic λ-calculi. Bisimulation-based proof techniques are studied, with particular focus on deriving bisimulations that are fully abstract for contextual equivalence (i.e., coincide with it). As a first result, full abstraction of applicative bisimilarity and similarity are proved for a call-by-value probabilistic λ-calculus with a parallel disjunction operator. Applicative bisimulations are however known not to scale to richer languages. Hence, more robust notions of bisimulations for probabilistic calculi are considered, in the form of environmental bisimulations. Environmental bisimulations are defined for pure callby-name and call-by-value probabilistic λ-calculi, and for a (call-by-value) probabilistic λ-calculus extended with references (i.e., a store). In each case, full abstraction results are derived.		Valeria Vignudelli	2017			nondeterministic algorithm;equivalence (measure theory);discrete mathematics;operator (computer programming);probabilistic logic;expressivity;bisimulation;process calculus;mathematics;abstraction	PL	-11.317735663416324	19.09178121093713	43301
03d116ed97eb988ef8949a52929b871d13d85f6d	a lightweight regular model checking approach for parameterized systems	abstraction;regular model checking;datavetenskap datalogi;computer science	In recent years, we have designed a lightweight approach to regular model checking specifically designed for parameterized systems with global conditions. Our approach combines the strength of regular languages, used for representing infinite sets of configurations, with symbolic model checking and approximations. In this paper, we give a uniform presentation of several variations of a symbolic backward reachability scheme in which different classes of regular expressions are used in place of BDDs. The classification of the proposed methods is based on the precision of the resulting approximated analysis.	approximation algorithm;model checking;reachability;regular expression;regular language	Giorgio Delzanno;Ahmed Rezine	2011	International Journal on Software Tools for Technology Transfer	10.1007/s10009-011-0213-y	model checking;computer science;theoretical computer science;abstraction;abstraction model checking;symbolic trajectory evaluation;algorithm	SE	-13.157543108958086	25.948929763484827	43329
327545d0a379fdacfbddf2021cd7dc02b529c643	loop checking and the wll-founded semantics	well founded semantics;query evaluation;proof search	Using a calculus of goals, we define the success and failure of a goal for propositional programs in the presence of loop checking. The calculus is sound with respect to the well-founded semantics; for finite programs, it is also complete. A Prolog-style proof search strategy for a modification of this calculus provides a query evaluation algorithm for finite propositional programs under the well-founded semantics. This algorithm is implemented as a meta-interpreter.	algorithm;prolog;well-founded semantics;whole earth 'lectronic link	Vladimir Lifschitz;Norman McCain;Teodor C. Przymusinski;Robert F. Stärk	1995		10.1007/3-540-59487-6_10	action semantics;proof-theoretic semantics;formal semantics;programming language;well-founded semantics;operational semantics;denotational semantics;algorithm;computational semantics	PL	-16.505126688059637	21.302125509642305	43578
02ed59abb1562e6115a4344adf32336df4886617	abstract diagnosis of functional programs	debugging;puesta a punto programa;bottom up method;functional models;bottom up;implementation;specification;diagnostico;forma normal;semantics;logical programming;exactitude programme;semantica;semantique;functional programming;methode ascendante;term rewrite system;debogage;exactitud programa;programa puesta a punto;especificacion;programmation logique;rewriting systems;normal form;forme normale;interpretation abstraite;implementacion;abstract interpretation;diagnosis;programacion logica;systeme reecriture;programme debogage;debugging program;modele fonctionnel;diagnostic;program correctness	Diagnosis of Functional Programs M. Alpuente, M. Comini, S. Escobar, M. Falaschi, and S. Lucas 1 Departamento de Sistemas Informáticos y Computación-DSIC Technical University of Valencia, Camino de Vera s/n, 46022 Valencia, Spain. {alpuente,sescobar,slucas}@dsic.upv.es 2 Dipartimento di Matematica e Informatica University of Udine, Via delle Scienze 206, 33100 Udine, Italy. {comini,falaschi}@dimi.uniud.it Abstract. We present a generic scheme for the declarative debugging We present a generic scheme for the declarative debugging of functional programs modeled as term rewriting systems. We associate to our programs a semantics based on a (continuous) immediate consequence operator, TR, which models the (values/normal forms) semantics of R. Then, we develop an effective debugging methodology which is based on abstract interpretation: by approximating the intended specification of the semantics of R we derive a finitely terminating bottom-up diagnosis method, which can be used statically. Our debugging framework does not require the user to either provide error symptoms in advance or answer questions concerning program correctness. We have made available a prototypical implementation in Haskell and have tested it on some non trivial examples.	abstract interpretation;abstract rewriting system;algorithmic program debugging;bottom-up parsing;camino;constructor (object-oriented programming);correctness (computer science);database normalization;dataflow;fixed point (mathematics);functional programming;haskell;newman's lemma;obj (programming language);prototype;quickcheck;rewriting;software bug;top-down and bottom-up design	María Alpuente;Marco Comini;Santiago Escobar;Moreno Falaschi;Salvador Lucas	2002		10.1007/3-540-45013-0_1	computer science;theoretical computer science;top-down and bottom-up design;semantics;algorithmic program debugging;programming language;functional programming;implementation;debugging;specification;algorithm	PL	-19.094052398749444	20.965425733770292	43720
df0dabef5f88a2b96315d5a8272998cc8afdc04b	enforcing periodic transition deadlines in time petri nets with net unfoldings	plant execution;net unfolding;supervisory control;job shop scheduling;processor scheduling;petri net transition latency;industrial plants;supervisory controller;indexing terms;automatic generation;production control scheduling;supervisor generation;time petri net;production control;task duration;target transition;supervisory control petri nets production control scheduling;schedules;time petri nets;petri nets;production control scheduling periodic transition deadline enforcement time petri nets net unfolding supervisory controller delays supervisor generation plant execution task duration petri net transition latency target transition maintenance system;petri net;maintenance system;schedules job shop scheduling delay processor scheduling petri nets real time systems;periodic transition deadline enforcement;delays;production control delays industrial plants petri nets;real time systems	We define a method for the automatic generation of supervisory controllers that force a plant to perform a given operation by a given deadline. The operation must be executed by a prespecified delay λ with respect to the previous execution of the operation. Although our supervisor generation occurs offline with respect to plant execution, the resulting controllers automatically take into account variable task durations in an effort to increase the flexibility of operation schedules in the controlled plant. We model both the controlled plant and control supervisors as time Petri nets. In this setting, our control supervisors must force a target transition td to fire within λ time units since the previous firing of td. Our supervisor generation is based on the concept of a transition latency. The latency of a Petri net transition t is the time interval during which t must be disabled in order for target transition td to fire by its deadline. If a transition t that may delay the firing of td has latency l(t), then t must be disabled at least l(t) time units before the expiration of the deadline on td. In this paper, we discuss in detail two algorithms for generating transition latencies, and we show an application to a maintenance system.	algorithm;liveness;mutual exclusion;online and offline;petri net;physical plant;scalability;schedule (computer science);state space;task manager;unfolding (dsp implementation)	Haisheng Wang;Liviu Grigore;Ugo Buy;Mihai Lehene;Houshang Darabi	2011	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2010.2076397	job shop scheduling;mathematical optimization;real-time computing;computer science;petri net	Embedded	-6.060585434857394	28.603483997476722	43737
2bb74c388ffe529ca62f496375f233fe9d3f9d90	probabilistic coherence spaces are fully abstract for probabilistic pcf	probabilistic pcf;full abstraction	Probabilistic coherence spaces (PCoh) yield a semantics of higher-order probabilistic computation, interpreting types as convex sets and programs as power series. We prove that the equality of interpretations in Pcoh characterizes the operational indistinguishability of programs in PCF with a random primitive.  This is the first result of full abstraction for a semantics of probabilistic PCF. The key ingredient relies on the regularity of power series.  Along the way to the theorem, we design a weighted intersection type assignment system giving a logical presentation of PCoh.	computation;concurrency (computer science);denotational semantics;functional programming;linear logic;probabilistic turing machine;programming computable functions;randomized algorithm;time complexity;x-machine	Thomas Ehrhard;Christine Tasson;Michele Pagani	2014		10.1145/2535838.2535865	probabilistic ctl;computer science;theoretical computer science;probabilistic logic;algorithm	Logic	-11.439863504714381	18.965775202305785	43758
fd45ab27dd42f2336e20c9b7ab9b862871eaf15a	when the decreasing sequence fails	program anslysis;fixpoint approximation;abstract interpretation	The classical method for program analysis by abstract interpretation consists in computing a increasing sequence with widening, which converges towards a correct solution, then computing a decreasing sequence of correct solutions without widening. It is generally admitted that, when the decreasing sequence reaches a fixpoint, it cannot be improved further. As a consequence, all efforts for improving the precision of an analysis have been devoted to improving the limit of the increasing sequence. In this paper, we propose a method to improve a fixpoint after its computation. The method consists in projecting the solution onto well-chosen components and to start again increasing and decreasing sequences from the result of the projection.	abstract interpretation;computation;fixed point (mathematics);iteration;program analysis	Nicolas Halbwachs;Julien Henry	2012		10.1007/978-3-642-33125-1_15	algorithm	Logic	-12.26685636043868	30.49883467298751	43974
be2082b2438eab8fd67480881e3f349ee64a0d52	the box algebra = petri nets + process expressions	structured operational semantics;process algebra;petri net	The paper describes a Petri net as well as a structural operational semantics for an algebra of process expressions. It speciically addresses this problem for the box algebra, a model of concurrent computation which combines Petri nets and standard process algebras. The main result is that it is possible to obtain a framework where process expressions can be given two, entirely consistent, kinds of semantics: one based on Petri nets, the other on SOS rules. This consistency can also be extended to a partial order semantics.	computation;concurrency (computer science);concurrent computing;operational semantics;petri net;process calculus;thinking outside the box	Eike Best;Raymond R. Devillers;Maciej Koutny	2002	Inf. Comput.	10.1006/inco.2002.3117	process calculus;discrete mathematics;stochastic petri net;computer science;theoretical computer science;distributed computing;process architecture;petri net	Logic	-11.67440214912885	21.391174111431148	44077
e9a8eb7c9b5d506d9b57430bfddc496c916755aa	on reducing test length for fsms with extra states	finite state machines;test generation;fault domain	A long-standing problem when testing from a deterministic finite state machine is to guarantee full fault coverage even if the faults introduce extra states in the implementations. It is well known that such tests should include the sequences in a traversal set which contains all input sequences of length defined by the number of extra states. This paper suggests the SPY method, which helps reduce the length of tests by distributing sequences of the traversal set and reducing test branching. It is also demonstrated that an additional assumption about the implementation under test relaxes the requirement of the complete traversal set. The results of the experimental comparison of the proposed method with an existing method indicate that the resulting reduction can reach 40%. Experimental results suggest that the additional assumption about the implementation can help in further reducing the test suite length. Copyright q 2011 John Wiley & Sons, Ltd.	algorithm;deterministic finite automaton;emoticon;experiment;fault coverage;finite-state machine;horizontal situation indicator;identifier;john d. wiley;many-one reduction;test suite;tree traversal	Adenilso da Silva Simão;Alexandre Petrenko;Nina Yevtushenko	2012	Softw. Test., Verif. Reliab.	10.1002/stvr.452	real-time computing;computer science;engineering;finite-state machine;engineering drawing;algorithm	SE	-12.741999700867522	28.419505057290717	44092
75c3ff6e56f213ca5699b963b339cb88f41ad4fe	place invariant simplification in optimal supervisor synthesis for fms	manufacturing systems;cybernetics;frequency modulation;law;system recovery;petri nets;conferences	The theory of regions is an important method to derive an optimal and liveness-enforcing supervisor for a flexible manufacturing systems based on Petri nets. It first partitions the reachability graph into a live zone (LZ) and a deadlock zone (DZ). Then, activity places are used to construct place invariants (PIs) to prevent the system from entering DZ and permit all markings in LZ. This work studies the reduction of the number of places to be considered in the optimal PI designs. First, the concepts of critical transitions and critical activity places are defined, and an algorithm is provided to compute the sets of critical and uncritical activity places. Then, the proof of that only critical activity places need to be considered in such optimal PI designs is established.	algorithm;deadlock;item unique identification;lz77 and lz78;level of detail;liveness;petri net;reachability	Bo Huang;Mengchu Zhou;Yi-Sheng Huang	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844620	frequency modulation;simulation;cybernetics;computer science;artificial intelligence;control theory;mathematics;petri net;algorithm	EDA	-6.18611439030366	27.646361064658542	44263
e4ef56d1ef4afefaa277cab1b5719d2a8cee07a6	automatic generation of hints for symbolic traversal	modele comportement;modelizacion;behavior model;sistema transicion;diseno circuito;diagrama binaria decision;verificacion modelo;verification symbolique;hardware verification;diagramme binaire decision;tranchage;verificacion hardware;verification materiel;generacion automatica;behavior modeling;verificacion simbolica;graph flow;symbolic verification;modelo comportamiento;heuristic method;circuit design;verification modele;metodo heuristico;flux donnee;flujo datos;interpretacion abstracta;program verification;control flow graph;flow graphs;automatic generation;transition system;flujo grafo;modelisation;verificacion programa;slicing;generation automatique;systeme transition;model checking;flot graphe;state space;transition systems;graphe flux;chapeado;conception circuit;graphe fluence;methode heuristique;state explosion;interpretation abstraite;program slicing;data flow;abstract interpretation;verification programme;modeling;grafo fluencia;reachability analysis;fluence graph;analyse atteignabilite;binary decision diagram	Recent work in avoiding the state explosion problem in hardware verification during breath-first symbolic traversal (BFST) based on Binary Decision Diagrams (BDDs) applies hints to constrain the transition relation of the circuit being verified [14]. Hints are expressed as constraints on the primary inputs and states of a circuit modeled as a finite transition system and can often be found with the help of simple heuristics by someone who understands the circuit well enough to devise simulation stimuli or verification properties for it. However, finding good hints requires one to constrain the transition system so that small intermediate BDDs arise during image computations that produce large numbers of reachable states. Thus, the ease of finding good hints is limited by the user’s ability to predict their usefulness. In this paper we present a method to statically and automatically determine good hints. Working on the control flow graph(s) of a behavioral model of the circuit being analyzed, our algorithm extracts sets of related execution paths. Each set has a corresponding enabling predicate which is a candidate hint. Program slicing is employed to identify execution paths. Abstract interpretation and model checking are used to ascertain properties along these paths. Hints generated automatically using our technique result in ordersof-magnitude reductions in time and space requirements during state space exploration compared to BFST and are usually as good as those produced by someone who understands the circuit.	abstract interpretation;algorithm;behavioral modeling;binary decision diagram;computation;control flow graph;disk partitioning;heuristic (computer science);model checking;program slicing;prototype;reachability;requirement;simulation;state space;the times;transition system;tree traversal;verilog	David Ward;Fabio Somenzi	2005		10.1007/11560548_17	behavioral modeling;real-time computing;computer science;artificial intelligence;theoretical computer science;algorithm	Logic	-15.645290532575183	28.174497632357987	44402
5ed2465d50940c63032c04f2c4602067c0a06fcc	principles of distributed test synthesis based on true-concurrency models	distributed system;conformance testing;model checking;sequential test	Automatic synthesis of test cases for conformance testing has been principally developed with the objective of generating sequential test cases. In the distributed system context, it is worth extending the synthesis techniques to the generation of multiple testers. We base our work on our experience in using model-checking techniques, as successfully implemented in the TGV tool. Continuing the works of A. Ulrich and H. König, we propose to use a trueconcurrency model based on graph unfolding. The article presents the principles of a complete chain of synthesis, starting from the definition of test purposes and ending with a projection onto a set of testers.	concurrency (computer science);conformance testing;distributed computing;könig's lemma;model checking;test case;unfolding (dsp implementation)	Claude Jard	2002		10.1007/978-0-387-35497-2_22	reliability engineering;real-time computing;computer science;theoretical computer science;test management approach	Logic	-17.495365053191133	27.724935584605696	44462
c953946dc458979d69cd8975d416fd77b6cff88b	marking observer of labeled petri nets with uncertainty in the initial marking	linear algebra;state estimation petri nets discrete event systems;observers vectors equations petri nets automata uncertainty computers;set theory;observers;state estimation;labeled directed graph labeled petri nets initial marking uncertainty marking estimation convex set silent transitions indistinguishable transitions linear algebraic terms marking observer boundedness assumptions;discrete event systems;set theory linear algebra observers petri nets;petri nets	In this paper we consider marking estimation in labeled Petri nets whose initial marking is known to belong to a given convex set. We allow for silent transitions (i.e., transitions labeled with the empty word) and indistinguishable transitions (i.e., transitions sharing the same label with other transitions). We demonstrate that all sets of markings consistent with a given sequence of observations can be described in linear algebraic terms (as a union of convex sets) and a marking observer may be constructed offline under appropriate bounded ness assumptions. This reduces the problem of computing the set of markings consistent with a given observation sequence to the problem of moving along a path in a labeled directed graph.	convex set;directed graph;earthbound;empty string;item unique identification;linear algebra;online and offline;petri net	Maria Paola Cabasino;Carla Seatzu;Christoforos N. Hadjicostis	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.401	combinatorics;discrete mathematics;stochastic petri net;linear algebra;mathematics;petri net;set theory	Robotics	-6.634524813650486	26.567339443638403	44487
b8bdd64a89ef312b88b39f5ee785bf9eebcc17e8	comments on the algorithms of verhelst for the conversion of limited-entry decision tables to flowcharts	flow charting;preprocessor;search;sequential test;optimal programs;decision table	In his interesting contribution to the study of the conversion of limited-entry decision tables to flowcharts or sequential testing procedures, Verhelst [1] presents two algorithms, one of which is claimed to produce the optimal flowchart with respect to run-time. Unfortunately this minimum run-time algorithm does not always produce the result claimed, as we illustrate by example below.	algorithm;decision table;flowchart	Peter J. H. King;Roger G. Johnson	1974	Commun. ACM	10.1145/360767.360807	decision table;flowchart;computer science;data mining;programming language;preprocessor;algorithm;statistics	PL	-14.710900349642351	31.735581387049397	44725
0145fae0c221275854369f34288525bf0e295144	proof-based verification approaches for dynamic properties: application to the information system domain	b formal method;property patterns;proof;dynamic properties	This paper proposes a formal approach for generating necessary and sufficient proof obligations to demonstrate a set of dynamic properties using the B method. In particular, we consider reachability, non-interference and absence properties. Also, we show that these properties permit a wide range of property patterns introduced by Dwyer to be expressed. An overview of a tool supporting these approaches is also provided.	algorithm;automated theorem proving;b-method;condition number;conditional (computer programming);correctness (computer science);emoticon;explicit substitution;free variables and bound variables;information system;interference (communication);mathematical induction;model checking;network address translation;non-interference (security);path (graph theory);precondition;predicate transformer semantics;reachability;recursion;software testing;turing completeness;verification and validation	Amel Mammar;Marc Frappier	2014	Formal Aspects of Computing	10.1007/s00165-014-0323-x	discrete mathematics;theoretical computer science;proof;mathematics;algorithm	Logic	-15.828864446309284	26.999425826833132	44789
831d4b2c5151f7480d5b3718de6965775a413818	tool-based verification of a relational vertex coloring program		We present different approaches of using a special purpose computer algebra system and theorem provers in software verification. To this end, we first develop a purely algebraic while-program for computing a vertex coloring of an undirected (loop-free) graph. For showing its correctness, we then combine the well-known assertion-based verification method with relation-algebraic calculations. Based on this, we show how automatically to test loop-invariants by means of the RelView tool and also compare the usage of three different theorem provers in respect to the verification of the proof obligations: the automated theorem prover Prover9 and the two proof assistants Coq and Isabelle/HOL. As a result, we illustrate that algebraic abstraction yields verification tasks that can easily be verified with off-the-shelf theorem provers, but also reveal some shortcomings and difficulties with theorem provers that are nowadays available.	assertion (software development);automated theorem proving;computer algebra system;coq (software);correctness (computer science);embedded system;graph (discrete mathematics);graph coloring;hol (proof assistant);isabelle;linear algebra;proof assistant;software verification;whole earth 'lectronic link	Rudolf Berghammer;Peter Höfner;Insa Stucke	2015		10.1007/978-3-319-24704-5_17	combinatorics;theoretical computer science;engineering drawing	Logic	-16.531544495274648	25.234015189867655	45015
d1d662e258e1768c5451b68fe115c020909ecfe8	experiments with implementations of two theoretical constructions	programming language;fixed point;program generation;pattern matching;partial evaluation;linear time;pushdown automata	This paper reports two experiments with implementations of constructions from theoretical computer science. The first one deals with Kleene’s and Rogers’ second recursion theorems and the second is an implementation of Cook’s linear time simulation of two way deterministic pushdown automata (2DPDAs). Both experiments involve the treatment of programs as data objects and their execution by means of interpreters. For our implementations we have been using a small LISP-like language called Mixwell, originally devised for the partial evaluator MIX used in the second experiment. LISP-like languages are especially suitable since programs are data (S-expressions) so the tedious coding of programs as Gödel numbers so familiar from recursive function theory is completely avoided. We programmed the constructions in the standard proofs of Kleene’s and Rogers’ recursion theorems and found (as expected) the programs so constructed to be far too inefficient for practical use. We then designed and implemented a new programming language called Reflect in which Kleene and Rogers “fixed-point” programs can be expressed elegantly and much more efficiently. We have programmed some examples in Reflect in an as yet incomplete attempt to find out for which sort of problems the second recursion theorems are useful program generating tools. The second experiment concerns an automaton that can solve many non-trivial pattern matching problems. Cook [4] has shown that any 2DPDA can be simulated in linear time by a clever memoization technique. We wrote a simple interpreter to execute 2DPDA programs and an interpreter using Cook’s algorithm, and we observed that the latter was indeed much faster on certain language recognition problems. Both have, however, a high computational overhead, since they in effect work by interpretation rather than compilation. In order to alleviate this we applied the principle of partial evaluation, see [5], to specialize each of the two interpreters to fixed 2DPDAs. The result was a substantial speedup. 1 The Second Recursion Theorems This section deals with a practical and, as it turns out, reasonably efficient implementation of the so-called second recursion theorems of Kleene, [9], and Rogers, [11]. For a much more thorough but also more theoretical treatment, see these or any other good textbook on the subject, e.g. [10]. We give a brief review of the fundamentals, then discuss our implementation of the theorem(s) and report a few experiments devised to demonstrate the adequacy of the implementation and reveal some of the power hidden in the recursion theorems.	algorithm;automata theory;compiler;computability theory;deterministic pushdown automaton;experiment;fixed-point arithmetic;gödel numbering;interpretation (logic);interpreter (computing);mix;memoization;overhead (computing);pl/i;partial evaluation;pattern matching;programming language;recursion (computer science);s-expression;simulation;speedup;stack (abstract data type);theoretical computer science;time complexity;two-way deterministic finite automaton	Torben Amtoft;Thomas Nikolajsen;Jesper Larsson Träff;Neil D. Jones	1989		10.1007/3-540-51237-3_11	time complexity;deterministic pushdown automaton;discrete mathematics;computer science;nested word;theoretical computer science;pattern matching;mathematics;context-free language;fixed point;programming language;partial evaluation;pushdown automaton;embedded pushdown automaton;algorithm	PL	-18.190375399567444	19.739559776368164	45198
fc46188582ebd7d4c456367cf2181638a5b90208	multi-valued abstraction using lattice operations	distributive lattice;concurrent computing;lattices;system analysis and design;quasi boolean;abstraction;mixed simulation model checking abstraction multi valued distributive lattice quasi boolean de morgan execution steering;multivalued model multivalued abstraction lattice operations model checking multivalued logic;model checking;de morgan;lattices concrete reactive power model checking concurrent computing system analysis and design encoding;mixed simulation;encoding;multi valued;program diagnostics formal verification multivalued logic;concrete;execution steering;reactive power	In model checking, abstractions can cause spurious results, which need to be verified in the concrete system to gain conclusive results. Verification based on a multi-valued logic can distinguish between conclusive and inconclusive results, provides increased precision, and allows for encoding additional information into the model, which gives rise to new applications. To ensure a correct abstraction, one can use a mixed simulation [1] to relate a multi-valued model to its abstraction. In this paper we extend the notion of mixed simulation to include inconsistent values, thereby resolving an asymmetry in the definition and allowing for abstractions with increased precision when inconsistent values are available.	encode;extended precision;model checking;runtime verification;simulation;state space	Stefan Vijzelaar;Wan Fokkink	2015	2015 15th International Conference on Application of Concurrency to System Design	10.1109/ACSD.2015.18	distributive lattice;model checking;concrete;concurrent computing;computer science;theoretical computer science;lattice;abstraction;ac power;programming language;abstraction model checking;structured systems analysis and design method;algorithm;encoding	EDA	-16.320940314010773	21.303954528788545	45242
8d4ce517f5ee55c176ffd5543555805d145b3e02	first-order ltl model checking using mdgs	logique lineaire;machine abstraite;linear time temporal logic;model based reasoning;diagrama binaria decision;verificacion modelo;raisonnement base sur modele;logica temporal;diagramme binaire decision;langage ordre 1;logique arbre calcul;automate bschi;maquina estado finito;temporal logic;temps lineaire;maquina abstracta;verification modele;first order language;interpretacion abstracta;bschi automata;tiempo lineal;program verification;analisis automatico;logica lineal;abstract machine;analisis programa;logica ctl;verificacion programa;automatic analysis;analyse syntaxique;modelo logico;first order;metamodel;model checking;metamodele;analisis sintaxico;metamodelo;automata bschi;syntactic analysis;linear time;analyse automatique;program analysis;logic model;multiway decision graphs;interpretation abstraite;analyse programme;analisis semantico;abstract interpretation;analyse semantique;machine etat fini;verification programme;lenguaje orden 1;linear logic;logique temporelle;finite state machine;branching temporal logic ctl;semantic analysis;modele logique;binary decision diagram	In this paper, we describe a first-order linear time temporal logic (LTL) model checker based on multiway decision graphs (MDG). We developed a first-order temporal language, LMDG∗, which expresses a subset of many-sorted first-order LTL and extends an earlier language, LMDG , defined for an MDG based abstract CTL model checking. We derived a set of rules, enabling the transformation of LMDG∗ formulas into generalized Büchi automata (GBA). The product of this GBA and the abstract state machine (ASM) model is checked for language emptiness. We have lifted two instances of the generalized Strongly Connected Component(SCC)-hull (GSH) checking algorithm [17] to support abstract data and uninterpreted functions based on operators available in the MDG package. Experimental results have shown the superiority of our tool compared to the same instances of GSH implemented with BDDs in VIS.	abstract state machines;algorithm;automata theory;binary decision diagram;finite-state machine;first-order logic;first-order predicate;generalized büchi automaton;linear temporal logic;model checking;visual instruction set	Fang Wang;Sofiène Tahar;Otmane Aït Mohamed	2004		10.1007/978-3-540-30476-0_36	program analysis;metamodeling;time complexity;model checking;linear logic;temporal logic;computer science;artificial intelligence;model-based reasoning;parsing;first-order logic;mathematics;abstract machine;finite-state machine;programming language;binary decision diagram;algorithm	Logic	-16.42639248311707	27.064835283264994	45549
0601f3bfa8e59faf5c8845bc6462a42835c48b02	composing invariants	temporal logic;software components;reactive and concurrent systems;formal specification;compositional verification;invariants	We explore the question of the composition of invariance specifications in a context of concurrent and reactive systems. Depending on how compositionality is stated and how invariants are defined, invariance specifications may or may not be compositional. This article first examines two classic forms of invariants and their compositional properties. After pointing out what we see as deficiencies of these two kinds of invariants, two new forms are defined and shown to have useful compositional properties that the more classic forms do not enjoy. The last form, in particular, is shown to be well suited to situations where none of the other three is adapted. c © 2005 Elsevier B.V. All rights reserved.	concurrent computing;invariant (computer science)	Michel Charpentier	2003		10.1007/978-3-540-45236-2_23	discrete mathematics;mathematics;communication;algorithm	SE	-9.818913721040232	22.405028868379034	45558
5771a0ac4c97e0688c950cdb85eedbef60ab511e	is the interesting part of process logic uninteresting? a translation from pl to pdl	computers;digital computers;computational mechanics;time dependent;general and miscellaneous mathematics computing and information science;time complexity;temporal logic;data processing;testing;satisfiability;mathematical logic;time dependence;dynamics;mechanics;process logic;processing 990200 mathematics computers;calculation methods;computer codes;algorithms;validation;propositional dynamic logic;supercomputers	With the (necessary) condition that atomic programs in process logic (PL) be binary, the authors present an algorithm for the translation of a PL formula p into a program /zeta/(p) of propositional dynamic logic (PDL) such that a finite path satisfies p iff it belongs to /zeta/(p). This reduction has two immediate corollaries: 1) validity in this PL can be tested by testing validity of formulas in PDL; 2) all state properties expressible in this PL are expressible in PDL. The translation, however, is of nonelementary time complexity. The significance of the result to the search for natural and powerful logics of programs is discussed.	perl data language (pdl)	Rivi Sherman;Amir Pnueli;David Harel	1984	SIAM J. Comput.	10.1137/0213051	time complexity;dynamics;combinatorics;mathematical logic;discrete mathematics;data processing;temporal logic;computer science;computational mechanics;theoretical computer science;mathematics;software testing;programming language;algorithm;algebra;satisfiability	Theory	-9.363243161477723	19.34219899245631	45580
ccc7461ba5f5a3bd9100640ad55a0600d59ed025	formal verification of a pipelined processor with new memory	automatic verification;circuit analysis computing pipeline processing microprocessor chips hardware description languages formal verification;formal verification hardware design languages solid modeling computer architecture electronic mail circuits large scale systems memory architecture computational modeling engines;hardware description languages;out of order;solidify model checkers new processor verification complex out of order issue processors abstraction fully automatic verification invariant problems pipelined processor formal verification memory hierarchy;formal verification;memory hierarchy;circuit analysis computing;pipeline processing;microprocessor chips	Recently, model checkers have become commercially available. To investigate their ability, Solidify is selected as the representative of them and applied to a verification of a new processor. The processor adopts new memory hierarchy and new instructions. Its instruction issue is pipelined and in-order. Our experiment reveals that Solidify can verify the processor but drastic abstraction is indispensable for successful verification. The experimental results also suggest that it is quite hard to verify more complex outof-order issue processors without very drastic and efficient abstraction. Through the experience, we also recognize the benefit of fully automatic verification. However, we suffer from the invariant problems. Experience is still important for this problem.	central processing unit;experience;experiment;formal verification;invariant (computer science);memory hierarchy;microprocessor;model checking;out-of-order execution;pipeline (computing)	Hiroshi Nakamura;Takanori Arai;Masahiro Fujita	2002		10.1109/PRDC.2002.1185653	computer architecture;parallel computing;formal verification;software verification;computer science;out-of-order execution;high-level verification;runtime verification;hardware description language;programming language;intelligent verification;functional verification	Logic	-15.391199099962652	30.37369019426518	45664
1f5ee882c3700009ff3dd26988c70b9033a29e00	optimisation de la construction d'une approximation de l'espace d'état des systèmes préemptifs	modelizacion;distributed system;graphe lineaire;optimisation;preemptive system;verificacion modelo;systeme reparti;polyedre;optimizacion;redundancia;red petri;poliedro;analisis cuantitativo;real time;preempcion;forma normal;verification modele;polyhedron;grafo lineal;modelisation;polyhedral system;time petri net;sistema repartido;estimation erreur;redundancy;model checking;state space method;analyse quantitative;methode espace etat;error estimation;dbm;temps reel;estimacion error;state class graph;preemption;quantitative analysis;normal form;tiempo real;forme normale;optimization;petri net;modeling;reseau petri;redondance;metodo espacio estado;linear graph	We present in this paper an algorithm allowing an efficient computation of the tightest DBM over-approximation of the state class graph of preemptive systems modeled by using Time Petri Nets with inhibitor arcs. For this effect, we express each class of the approximated graph as a pair (M, D̃), where M is a marking and D̃ is the system of all DBM inequalities even the redundant ones. We thereby make it possible to compute the system D̃ straightforwardly in its normal form, without requiring to compute the intermediary polyhedra. Hence, we succeed to reduce appreciably the computation cost of a class, and to remove the implementation dysfunctions reported for other approaches. We also discuss how to determine from the graph linear and quantitative properties of the model. Finally, we give some experimental results that compare the implementations of the different approaches.		Abdelli Abdelkrim	2009	Technique et Science Informatiques	10.3166/tsi.28.1143-1170	model checking;systems modeling;computer science;quantitative analysis;artificial intelligence;operating system;linear equation;preemption;redundancy;petri net;algorithm;polyhedron;dbm	Logic	-8.505754831557434	25.842445876379564	45669
3acf4485ea99c48cf6f748cd40aa57da34e5325e	on presburger liveness of discrete timed automata	logique lineaire;inf;theorie automate;logica temporal;temporal logic;discrete time;correspondence problem;automaton;automata;model checking;linear temporal logic;automate;automata theory;teoria automata;decidibilidad;timed automata;tiempo discreto;temps discret;decidabilite;linear logic;logique temporelle;decidability	Using an automata-theoretic approach, we investigate the d ecidability of liveness properties (called Presburger liveness properties ) for timed automata when Presburger formulas on configurations are allowed. Whi le t e general problem of checking a temporal logic such as TPTL augmented with P resburger clock constraints is undecidable, we show that there are various c las es of Presburger liveness properties which are decidable for discretetimed automata. For instance, it is decidable, given a discrete timed automaton A and a Presburger property P , whether there exists an !-path ofAwhereP holds infinitely often. We also show that other classes of Presburger liveness properties are in deed undecidable for discrete timed automata, e.g., whether P holds infinitely oftenfor each!-path of A. These results might give insights into the corresponding p roblems for timed automata over dense domains, and help in the definition of a fr agment of linear temporal logic, augmented with Presburger conditions on co nfigurations, which is decidable for model checking timed automata.	linear temporal logic;liveness;model checking;presburger arithmetic;timed automaton;undecidable problem	Zhe Dang;Pierluigi San Pietro;Richard A. Kemmerer	2001		10.1007/3-540-44693-1_12	combinatorics;discrete mathematics;computer science;presburger arithmetic;mathematics;automaton;timed automaton;algorithm	Logic	-10.254819024565569	24.22103516511935	45691
53605bf073df05fe0c25fa56789a0829be4c2483	symbol manipulation by threaded lists	digital computers;computer programming;algebra;coding;human factors engineering	"""(:tefi,~(~ il i,~ col~junction with lhe functions-~[f/] and t[,~l, which give the /rausformations thai ~ under-2,1~{ I goes between the points S a,~d T respeclively and the exit. u~.l tO : eally, We h~ve most r[~] = [~rH[~I-' ~'[f,[~ll; T-~ s[fd~]]l Ill0r(, ~ti0ns~)8sell. in this } apu~a~ ;ider ~i, definil f that! s, fii~ii2 t ion ,eks g~ hat di) will i 4; Given a flowch'trt with a single, entrance and a single exit, it is easy ~(, write down the recursive function that gives the tr'msformation of tim state vector from entrance to exit in terms of the corresponding functions for the !4 ¸ computation blocks and the predicates of tile branch points. In general, we proceed as follows. In figure 6, let ~ be an n-way branch point, and let fi, ... , f, be the computations leading to branch points fl~, f12, ' """" , fl .... l~et 4) be the function that transforms between fl and the exit of the chart, and let (b~ , • """" • , 4,~ be the corresponding functions for f l ~ , ."""	assembly language;code;compiler;complexity;computation;computer;linear algebra;recursion (computer science);sion's minimax theorem;subroutine;symmetric multiprocessing;tass times in tonetown	Alan J. Perlis;Charles Thornton	1960	Commun. ACM	10.1145/367177.367202	computer science;human factors and ergonomics;theoretical computer science;computer programming;coding;programming language	Theory	-14.883633704384618	31.841029079744764	45888
a9271e60ac6bd6d823913a4bba1afc2701ced809	temporal logic verification of stochastic systems using barrier certificates		This paper presents a methodology for temporal logic verification of discrete-time stochastic systems. Our goal is to find a lower bound on the probability that a complex temporal property is satisfied by finite traces of the system. Desired temporal properties of the system are expressed using a fragment of linear temporal logic, called safe LTL over finite traces. We propose to use barrier certificates for computations of such lower bounds, which is computationally much more efficient than the existing discretization-based approaches. The new approach is discretization-free and does not suffer from the curse of dimensionality caused by discretizing state sets. The proposed approach relies on decomposing the negation of the specification into a union of sequential reachabilities and then using barrier certificates to compute upper bounds for these reachability probabilities. We demonstrate the effectiveness of the proposed approach on case studies with linear and polynomial dynamics.	computation;curse of dimensionality;discretization;formal verification;linear temporal logic;mathematical optimization;polynomial;reachability;stochastic process;sum-of-squares optimization;tracing (software)	Pushpak Jagtap;Sadegh Soudjani;Majid Zamani	2018		10.1007/978-3-030-01090-4_11	mathematical optimization;discrete mathematics;mathematics;linear temporal logic;polynomial;negation;curse of dimensionality;discretization;reachability;temporal logic;upper and lower bounds	Logic	-11.170482383204794	27.88667898950676	45913
bd7425291c21248d1944211eaf0c5c9c21e95dd7	declarative algorithms for generation, counting and random sampling of term algebras		From a declarative variant of Rémy's algorithm for uniform random generation of binary trees, we derive a generalization to term algebras of an arbitrary signature. With trees seen as sets of edges connecting vertices labeled with logic variables, we use Prolog's multiple-answer generation mechanism to derive a generic algorithm that counts terms of a given size, generates them all, or samples a random term given the signature of a term algebra.  As applications, we implement generators for term algebras defining Motzkin trees, use all-term and random-term generation for mutual cross-testing and describe an extension mechanism that transforms a random sampler for Motzkin trees into one for closed lambda terms.	algorithm;binary tree;c++;clutter;compiler;correctness (computer science);declarative programming;functional programming;generic programming;haskell;ibm notes;java;lambda lifting;library (computing);logic programming;monad (functional programming);monad transformer;name mangling;procedural generation;programming language;prolog;proof assistant;random graph;sampling (signal processing);scala;scalability;swift (programming language);term algebra;transformers	Paul Tarau	2018		10.1145/3167132.3167262	term algebra;genetic algorithm;sampling (statistics);binary tree;vertex (geometry);algorithm;mathematics;prolog	PL	-6.977532413002911	21.139695424534192	46016
edbc3b075068fa93c3eba06e27d4245022bba951	how the location of * influences complexity in kleene algebra with tests	sensibilidad contexto;context aware;algebra relacional;localization;algebre kleene;semantics;intelligence artificielle;localizacion;optimizacion compiladora;logical programming;exactitude programme;semantica;semantique;program optimization;exactitud programa;first order;localisation;programmation logique;kleene algebra with tests;logique ordre 1;compiler optimization;algebra kleene;artificial intelligence;optimisation programme;inteligencia artificial;kleene algebra;algebre relationnelle;sensibilite contexte;programacion logica;relational algebra;optimisation compilateur;first order logic;logica orden 1;optimizacion programa;program correctness	The universal Horn theory of relational Kleene algebra with tests is of practical interest, particularly for program semantics, where Horn formulas can be used to verify correctness of programs or compiler optimizations. Unfortunately, this theory is known to be Π 1  1 -complete. However, many formulas arising in practice fall into fragments of the theory that are of lower complexity. In this paper, we see that the location of occurrences of the Kleene asterate operator * within a formula has a great impact on complexity. Using syntactic criteria based on the location of *, we give a fragment of the theory that is Σ 0  1 -complete, and a slightly larger fragment that is Π 0  2 -complete. We show that the same results hold over *-continuous Kleene algebras with tests. The techniques exhibit a relationship between first-order logic and the Horn theories of relational and *-continuous Kleene algebra, even though the theories are not first-order axiomatizable.	kleene algebra	Christopher Hardin	2004		10.1007/978-3-540-32275-7_16	kleene star;discrete mathematics;kleene's recursion theorem;computer science;artificial intelligence;kleene algebra;first-order logic;mathematics;semantics;programming language;algorithm	Logic	-8.001614341601359	19.377570558342587	46034
52ee2cf68340d964c7183320f7bfe07e6c6bfae7	a tool for lazy verification of security protocols	verification;automated deduction;protocols;rewrite rule;deduction automatique;cryptographic protocols;cryptographic protocols electronic mail program processors logic societies fets testing;cryptographic protocol;automatic programming;formal verification;rewriting systems;cryptography;automatic programming protocols cryptography formal verification rewriting systems;reecriture;protocoles cryptographiques;intruder behavior lazy verification security protocols lazy strategy compiler cryptographic protocols casrul protocol verification rewrite rules automatic tools generated rules lazy model;modele paresseux;term rewriting;lazy model;security protocol	We present the lazy strategy implemented in a compiler of cryptographic protocols, Casrul. The purpose of this compiler is to verify protocols and to translate them into rewrite rules that can be used by several kinds of automatic or semi-automatic tools for finding flaws, or proving properties. It is entirely automatic, and the efficiency of the generated rules is guaranteed because of the use of a lazy model of an Intruder behavior. This efficiency is illustrated on several examples.	compiler;cryptographic protocol;cryptography;lazy evaluation;rewriting;semiconductor industry	Yannick Chevalier;Laurent Vigneron	2001		10.1109/ASE.2001.989832	lazy initialization;computer science;theoretical computer science;cryptographic protocol;programming language;algorithm	Logic	-18.23937865319834	26.12645258533011	46088
6b250d5561eb1e04293a402679e2deac01b5abba	realizability of schedules by stochastic time petri nets with blocking semantics		This paper considers realizability of schedules by stochastic concurrent timed systems. Schedules are high level views of desired exe- cutions represented as partial orders decorated with timing constraints, while systems are represented as elementary stochastic time Petri nets. We first consider logical realizability: a schedule is realizable by a net N if it embeds in a time process of N that satisfies all its constraints. How- ever, with continuous time domains, the probability of a time process that realizes a schedule is null. We hence consider probabilistic realiz- ability up to α time units, that holds if the probability that N logically realizes S with constraints enlarged by α is strictly positive. Upon a sensible restriction guaranteeing time progress, logical and probabilistic realizability of a schedule can be checked on the finite set of symbolic prefixes extracted from a bounded unfolding of the net. We give a con- struction technique for these prefixes and show that they represent all time processes of a net occurring up to a given maximal date. We then show how to verify existence of an embedding and compute the proba- bility of its realization.	petri net	Loïc Hélouët;Karim Kecir	2016		10.1007/978-3-319-39086-4_11	combinatorics;discrete mathematics;mathematics;algorithm	SE	-10.428863555641332	24.661092002243485	46244
8571d0c767582b91877e8aafc467adbd889cd890	linear-time register allocation for a fixed number of registers	register allocation;linear time algorithm;polynomial time algorithm;data dependence;linear time	We show that for any j&d number of registers there is a linear-time algorithm which given a structured (= goto-fiee) program finds, if possible, an allocation of variables to registers without using intermediate storage. Our algorithm allows for rescheduling, i.e. that straightline sequences of statements may be reordered to achieve a better register allocation as long as the data dependencies of the program are not violated. If we al80 allow for registers of different types, e.g. for integers and floats, we can give only a polynomial time algorithm. In fact we show that the problem then becomes hard for the W-hierarchy which is a strong indication that no O(n’) algorithm exists for it with c independent on the number of registers. However, if we do not allow for rescheduling then this non-uniform register case is also solved in linear time.	algorithm;arithmetical hierarchy;data dependency;goto;p (complexity);parameterized complexity;register allocation;time complexity	Hans L. Bodlaender;Jens Gustedt;Jan Arne Telle	1998			time complexity;combinatorics;parallel computing;real-time computing;berlekamp–massey algorithm;computer science;theoretical computer science;mathematics;linear feedback shift register;register allocation;algorithm	Theory	-15.688001520094996	31.94963816353093	46328
372c27387f87db1d91cc7acdc4c5e230a6b38220	survey of statistical verification of linear unbounded properties: model checking and distances	verification;statistical model checking;probabilistic systems	We survey statistical verification techniques aiming at linear#N#properties with unbounded or infinite horizon, as opposed to#N#properties of runs of fixed length. We discuss statistical#N#model checking of Markov chains and Markov decision processes#N#against reachability, unbounded-until, LTL and mean-payoff#N#properties. Moreover, the respective strategies can be#N#represented efficiently using statistical techniques. Further,#N#we also discuss when it is possible to statistically estimate#N#linear distances between Markov chains.	model checking	Jan Kretínský	2016		10.1007/978-3-319-47166-2_3	verification;theoretical computer science;machine learning;distributed computing	Logic	-10.349855868737778	27.356996786936296	46775
277bc74001e42a5d58495271532145b24ee0106a	remarks on the notion of concurrency relation in the case of systems	concurrent systems	This paper is an attempt to investigate certain properties of concurrent systems on the basis of concurrency-like relations. Some general properties of simmetric and irreflexive relations have been stated. However, the result obtained here are not satisfatory — only properties of so simple nets as those which were used in the examples could be investigated without difficulties.		Piotr W. Prószynski	1981		10.1007/3-540-10854-8_34	discrete mathematics;computer science;mathematics;distributed computing;algorithm	Logic	-9.727499922233905	20.831565489118482	46980
d10edf415f1cc5ec3fb0ca396135ed69145490c5	an explicit transition system construction approach to ltl satisfiability checking		We propose a novel algorithm for the satisfiability problem for linear temporal logic (LTL). Existing automata-based approaches first transform the LTL formula into a Büchi automaton and then perform an emptiness checking of the resulting automaton. Instead, our approach works on-the-fly by inspecting the formula directly, thus enabling to find a satisfying model quickly without constructing the full automaton. This makes our algorithm particularly fast for satisfiable formulas. We construct experiments on different pattern formulas, the experimental results show that our approach is superior to other solvers under automata-based framework.	algorithm;benchmark (computing);boolean satisfiability problem;büchi automaton;cns;experiment;ibm notes;internet of things;linear temporal logic;transition system;trustworthy computing;zhi-li zhang	Jianwen Li;Lijun Zhang;Shufang Zhu;Geguang Pu;Moshe Y. Vardi;Jifeng He	2017	Formal Aspects of Computing	10.1007/s00165-017-0442-2	discrete mathematics;linear temporal logic;combinatorics;satisfiability;transition system;büchi automaton;boolean satisfiability problem;mathematics	AI	-13.496038192533963	24.929558425676543	47101
785a79dd58eaa14e99d6ec0db3de7ba991c58b73	confluence of reduction rules for lexicographic ordering constraints	lexicographic ordering constraints;symmetry breaking;constraint programming;lexicographic order	The lex leader method for breaking symmetry in CSPs typically produces a large set of lexicographic ordering constraints. Several rules have been proposed to reduce such sets whilst preserving logical equivalence. These reduction rules are not generally confluent: they may reach more than one fixpoint, depending on the order of application. These fixpoints vary in size. Smaller sets of lex constraints are desirable so ensuring reduction to a global minimum is essential. We characterise the systems of constraints for which the reduction rules are confluent in terms of a simple feature of the input, and define an algorithm to determine whether a set of lex constraints reduce confluently.	academy;algorithm;confluence (abstract rewriting);expectation propagation;fixed point (mathematics);lex (software);lexicographical order;maxima and minima;microsoft research;reduction strategy (lambda calculus);turing completeness;while	Andrew Grayland;Ian Miguel;Colva M. Roney-Dougal	2009			symmetry breaking;constraint programming;combinatorics;discrete mathematics;computer science;lexicographical order;mathematics;algorithm	EDA	-14.756054567912543	21.478518518697985	47144
340c776275703aa558348bab3a2a48f82d2aafad	deciding semantic finiteness of pushdown processes and first-order grammars w.r.t. bisimulation equivalence	004;pushdown processes first order grammars bisimulation regularity	The problem if a given configuration of a pushdown automaton (PDA) is bisimilar with some (unspecified) finite-state process is shown to be decidable. The decidability is proven in the framework of first-order grammars, which are given by finite sets of labelled rules that rewrite roots of first-order terms. The framework is equivalent to PDA where also deterministic popping epsilon-steps are allowed, i.e. to the model for which Sénizergues showed an involved procedure deciding bisimilarity (FOCS 1998). Such a procedure is here used as a black-box part of the algorithm. For deterministic PDA the regularity problem was shown decidable by Valiant (JACM 1975) but the decidability question for nondeterministic PDA, answered positively here, had been open (as indicated, e.g., by Broadbent and Goeller, FSTTCS 2012). 1998 ACM Subject Classification F.4.2 Grammars and Other Rewriting Systems	algorithm;bisimulation;black box;first-order predicate;journal of the acm;personal digital assistant;pushdown automaton;rewrite (programming);rewriting;stack (abstract data type);symposium on foundations of computer science;turing completeness	Petr Jancar	2016		10.4230/LIPIcs.MFCS.2016.52	deterministic context-free language;deterministic pushdown automaton;combinatorics;discrete mathematics;deterministic context-free grammar;computer science;mathematics;embedded pushdown automaton;algorithm	Logic	-5.90213869433144	21.908005809226477	47195
9bd224b75b0f61a196d5100b52add472f149c6ee	permutation equivalence of dpo derivations with negative application conditions based on subobject transformation systems	operational semantics;graph transformation;modelling framework;process analysis;static analysis	Switch equivalence for transformation systems has been successfully used in many domains for the analysis of concurrent behaviour. When using graph transformation as modelling framework for these systems, the concept of negative application conditions (NACs) is widely used – in particular for the specification of operational semantics. In this paper we show that switch equivalence can be improved essentially for the analysis of systems with NACs by our new concept of permutation equivalence. Two derivations respecting all NACs are called permutation-equivalent, if they are switch-equivalent disregarding the NACs. In fact, there are permutation-equivalent derivations which are not switch-equivalent with NACs. As main result of the paper, we solve the following problem: Given a derivation with NACs, we can efficiently derive all permutation-equivalent derivations to the given one by static analysis. The results are based on extended techniques for subobject transformation systems, which have been introduced recently.	application domain;attributed graph grammar;critical pair (logic);graph rewriting;hidden line removal;high-level programming language;modeller;maximal set;operational semantics;parallel computing;pattern matching;petri net;process modeling;situated;static program analysis;time complexity;turing completeness	Frank Hermann	2008	ECEASST	10.14279/tuj.eceasst.16.249	computer science;programming language;operational semantics;static analysis;algorithm	Logic	-17.69527025378963	28.529919615127696	47279
d1623d3077e6251fd76d2d3b82b38d3189780670	temporal reasoning about real time reactive systems	real time;reactive system;temporal reasoning			Ji Wang;Huowang Chen	1992			real-time computing;computer science;reactive system	AI	-10.62294884950413	25.976849323093408	47422
c6e24686302ab5061cb975689e36ddabf8b60034	hierarchical reachability graph generation for petri nets	hierarchical structure;reachability set;invariant analysis;reachable set;petri nets;petri net;state space explosion;divide and conquer;reachability analysis;reachability graph	Reachability analysis is the most general approach to the analysis of Petri nets. Due to the wellknown problem of state-space explosion, generation of the reachability set and reachability graph with the known approaches often becomes intractable even for moderately sized nets. This paper presents a new method to generate and represent the reachability set and reachability graph of large Petri nets in a compositional and hierarchical way. The representation is related to previously known Kronecker-based representations, and contains the complete information about reachable markings and possible transitions. Consequently, all properties that it is possible for the reachability graph to decide can be decided using the Kronecker representation. The central idea of the new technique is a divide and conquer approach. Based on net-level results, nets are decomposed, and reachability graphs for parts are generated and combined. The whole approach can be realized in a completely automated way and has been integrated in a Petri net-based analysis tool.	algorithm;binary decision diagram;bisimulation;computation;heuristic;ibm notes;interaction;linear temporal logic;matrix representation;model checking;numerical analysis;petri net;reachability;residential gateway;state space;stochastic process;turing completeness;undefined behavior	Peter Buchholz;Peter Kemper	2002	Formal Methods in System Design	10.1023/A:1020321222420	stochastic petri net;computer science;theoretical computer science;programming language;petri net	Logic	-8.60206041649919	24.915211960859605	47644
c5ea1e3740fb4d5d245020668d0f8f6a63837bfb	a non-prenex, non-clausal qbf solver with game-state learning	non prenex;non clausal;quantified boolean formula;model checking;qbf;clause learning;dpll	We describe a DPLL-based solver for the problem of quantified boolean formulas (QBF) in non-prenex, non-CNF form. We make two contributions. First, we reformulate clause/cube learning, extending it to non-prenex instances. We call the resulting technique game-state learning. Second, we introduce a propagation technique using ghost literals that exploits the structure of a non-CNF instance in a manner that is symmetric between the universal and existential variables. Experimental results on the QBFLIB benchmarks indicate our approach outperforms other state-of-the-art solvers on certain benchmark families, including the tipfixpoint and tipdiam families of model checking problems.	benchmark (computing);conjunctive normal form;dpll algorithm;ghost;model checking;overhead (computing);prenex normal form;rollover (key);software propagation;solver;true quantified boolean formula	William Klieber;Samir Sapra;Sicun Gao;Edmund M. Clarke	2010		10.1007/978-3-642-14186-7_12	model checking;discrete mathematics;computer science;theoretical computer science;mathematics;dpll algorithm;algorithm	AI	-15.367151849288135	23.626960664318982	47682
1ad8fc1977b061082391c8ed18edf52a8ddc50a0	a general approach for the computation of a liveness enforcing supervisor for the petri net model of an fms.		In this paper, a general approach is proposed for the computation of a liveness enforcing supervisor for the Petri net model of a flexible manufacturing system (FMS) prone to deadlocks. The proposed method is applicable to a lot of PN classes. A global sink/source place (GP) is used temporarily in the design steps and is finally removed when the liveness of the system is achieved. The aim is to obtain an easy to design deadlock prevention policy for PN models of FMSs that ensures liveness with optimal or near optimal permissiveness while maintaining the necessary computations simple.	computation;deadlock;liveness;petri net	Murat Uzam;Zhiwu Li;Umar Suleiman Abubakar	2014			real-time computing;operations management;distributed computing	Robotics	-6.763906476726451	29.328441588193815	47881
40ecd03adc3c99414f7ecafad54706f371ce9100	invariant checking of nra transition systems via incremental reduction to lra with euf		Model checking invariant properties of designs, represented as transition systems, with non-linear real arithmetic (NRA), is an important though very hard problem. On the one hand NRA is a hard-to-solve theory; on the other hand most of the powerful model checking techniques lack support for NRA. In this paper, we present a counterexample-guided abstraction refinement (CEGAR) approach that leverages linearization techniques from differential calculus to enable the use of mature and efficient model checking algorithms for transition systems on linear real arithmetic (LRA) with uninterpreted functions (EUF). The results of an empirical evaluation confirm the validity and potential of this approach.		Alessandro Cimatti;Alberto Griggio;Ahmed Irfan;Marco Roveri;Roberto Sebastiani	2017		10.1007/978-3-662-54577-5_4	discrete mathematics;mathematics;algorithm	Logic	-14.650223465163425	25.986179217853376	48070
1513aa00bc3beb4f6adeb14760bc029463594eb0	higher-order constraint simplification in dependent type theory	logical frameworks;logical framework;higher order;unification;dependent type theory;dependent types	Higher-order unification is undecidable, but has fragments which admit practical algorithms, which are used extensively in logical frameworks. For example, it is decidable whether unification problems in the pattern fragment are solvable, and they enjoy unique most general unifiers when they are.  Often we wish to treat more general problems which are nonetheless solvable by incrementally reasoning about the parts of them that fall in the pattern fragment after more progress has been made --- to this end constraint simplification algorithms have been proposed, which work on the so-called dynamic pattern fragment. However, their theory turns out to be surprisingly subtle. The constraint simplification algorithm implemented in Twelf, for instance, is not terminating, despite the sketch of a proof of its termination in the literature. We describe and prove correct a new, terminating constraint simplification algorithm for a dynamic pattern fragment of higher-order unification in a dependent type system, and discuss its implementation.	algorithm;decision problem;dependent type;level of detail;newman's lemma;twelf;type system;type theory;undecidable problem;unification (computer science)	Jason Reed	2009		10.1145/1577824.1577832	combinatorics;discrete mathematics;mathematics;algorithm	PL	-12.657285940257971	19.69353815900106	48116
a7e8b085085bfd5d79e0362d62909f1e11b7f740	lira: handling constraints of linear arithmetics over the integers and the reals	theorem prover;efficient implementation;decision procedure;first order logic	The mechanization of many verification tasks relies on efficient implementations of decision procedures for fragments of first-order logic. Interactive theorem provers like pvs also make use of such decision procedures to increase the level of automation. Our tool lira3 implements decision procedures based on automata-theoretic techniques for first-order logics with linear arithmetic, namely, for FO(N,+), FO(Z,+, <), and FO(R, Z,+, <). The theoretical foundations for using automata to decide logics like Presburger arithmetic, i.e., FO(N,+) were laid in the 1960s [4]: For Presburger arithmetic, the elements of the domain are represented by finite words, and for a given formula, one constructs recursively over the formula structure an automaton that accepts precisely the words that represent the natural numbers that satisfy the formula. Automata constructions handle the logical connectives and quantifiers. A similar approach works for FO(Z,+, <) and FO(R, Z,+, <). To represent reals, one uses infinite words. In [2], it is shown that weak deterministic Büchi automata (wdbas) suffice to decide FO(R, Z,+, <). wdbas are a restricted class of Büchi automata, which can be handled algorithmically almost as efficiently as deterministic finite automata (dfas). lira also provides an automata library that efficiently represents and manipulates dfas and wdbas. lira’s automata library can be compared to a bdd library for representing and manipulating finite sets encoded by booleans. Instead of bdds, lira uses dfas to represent and manipulate sets that are definable in FO(N,+) and FO(Z,+, <), and uses wdbas for sets definable in FO(R, Z,+, <). Efficiently representing and manipulating such definable sets has applications beyond deciding these logics efficiently. For instance, in the safety verification of integer-counter systems and hybrid systems one has to cope with such sets. Furthermore,	algorithm;binary decision diagram;büchi automaton;decision problem;deterministic finite automaton;finite-state machine;first-order logic;first-order predicate;hybrid system;logical connective;presburger arithmetic;proof assistant;quantifier (logic);recursion	Bernd Becker;Christian Dax;Jochen Eisinger;Felix Klaedtke	2007		10.1007/978-3-540-73368-3_36	computer science;first-order logic;automated theorem proving;programming language;algorithm	Logic	-13.089709340272057	24.111014429474633	48143
7184d04a7be3fd9112befc49af9c5a97fa94df4a	tree regular model checking for lattice-based automata		Tree Regular Model Checking (TRMC) is the name of a family of techniques for analyzing infinite-state systems in which states are represented by terms, and sets of states by Tree Automata (TA). The central problem in TRMC is to decide whether a set of bad states is reachable. The problem of computing a TA representing (an overapproximation of) the set of reachable states is undecidable, but efficient solutions based on completion or iteration of tree transducers exist. Unfortunately, the TRMC framework is unable to efficiently capture both the complex structure of a system and of some of its features. As an example, for JAVA programs, the structure of a term is mainly exploited to capture the structure of a state of the system. On the counter part, integers of the java programs have to be encoded with Peano numbers, which means that any algebraic operation is potentially represented by thousands of applications of rewriting rules. In this paper, we propose Lattice Tree Automata (LTAs), an extended version of tree automata whose leaves are equipped with lattices. LTAs allow us to represent possibly infinite sets of interpreted terms. Such terms are capable to represent complex domains and related operations in an efficient manner. We also extend classical Boolean operations to LTAs. Finally, as a major contribution, we introduce a new completion-based algorithm for computing the possibly infinite set of reachable interpreted terms in a finite amount of time.	algorithm;automata theory;boolean operations on polygons;iteration;java;linear algebra;model checking;peano axioms;refinement (computing);rewriting;transducer;tree automaton;undecidable problem	Thomas Genet;Tristan Le Gall;Axel Legay;Valérie Murat	2012	CoRR		combinatorics;discrete mathematics;computer science;mathematics;programming language;algorithm	Logic	-11.263319023598871	23.65705983581479	48164
350c144a8a28910e4bce053a75dc19f89068b369	a modular modeling approach for cnc machines control using petri nets	computer numerical control machine control petri nets programmable control hardware risk analysis automatic control control systems manuals manufacturing systems;programmable controllers;risk analysis;risk management;risk management computerised numerical control programmable controllers petri nets;computerised numerical control;machine plc control model modular modeling approach cnc machine control petri nets computerized numeric control machine interlocking programmable logical controller axes positioning part cut off sequence operator interface emergency sequences environment integration machine plc control sequencing control functions modular machine plc control modeling approach risk analysis task hardware software interlocking;computerized numerical control;petri nets;petri net;programmable logic controller	Machine control can be executed in an integrated way using computerized numeric control (CNC) and the programmable logical controller (PLC). CNC deals with axes positioning and speed, the part cut off sequence and the operator interface, while PLC deals with machine interlocking, emergency sequences, start and stop sequences, among others, considering its environment integration. The machine PLC control can be divided into interlocking and sequencing control functions. The article proposes a modular machine PLC control modeling approach using Petri nets. This approach also considers a risk analysis task that defines and classifies the hardware and software interlocking to be implemented to avoid loss and damage. In this approach, the machine PLC control model can be constructed and the Petri net analysis technique can be used to verify and validate it.	petri net	Edilson R. R. Kato;Orides Morandin;Paulo R. Politano;Heloisa A. Camargo	2000		10.1109/ICSMC.2000.886478	embedded system;real-time computing;risk management;computer science;programmable logic controller;petri net	Robotics	-5.17286315298063	28.843424940349063	48446
0b3a134f6ca871bb3b02d004d3e847ce65dd8e98	typestate checking and regular graph constraints	linked data;programming language;shape analysis;correspondence problem;program verification;specification language;graph homomorphism;type checking;monadic second order logic;logic in computer science;data structure;regular graph	We introduce regular graph constraints and explore their decidability properties. The motivation for regular graph constraints is 1) type checking of changing types of objects in the presence of linked data structures, 2) shape analysis techniques, and 3) generalization of similar constraints over trees and grids. Typestate checking for recursive and potentially cyclic data structures requires verifying the validity of implication for regular graph constraints. The implication of regular graph constraints also arises in shape analysis algorithms such as role-analysis and some analyses based on three-valued logic. Over the class of lists regular graph constraints reduce to a nondeterministic finite state automaton as a special case. Over the class of trees the constraints reduce to a nonde-terministic top-down tree automaton, and over the class of grids our constraints reduce to domino system and tiling problems. We define a subclass of graphs called heaps as an abstraction of the data structures that a program constructs during its execution. We show that satisfiability of regular graph constraints over the class of heaps is decidable. However, determining the validity of implication for regular graph constraints over the class of heaps is undecidable. The undecidability of implication is the central result of the paper. The result is somewhat surprising because our simple constraints are strictly less expressive than existential monadic second-order logic over graphs. In the key step of our proof we introduce the class of corresponder graphs which mimic solutions of Post correspondence problem instances. We show undecidability by exhibiting a characterization of corresponder graphs in terms of presence and absence of homomorphisms to a finite number of fixed graphs. The undecidability of implication of regular graph constraints implies that there is no algorithm that will verify that procedure preconditions are met or that the invariants are maintained when these properties are expressed in any specification language at least as expressive as regular graph constraints. the Singapore-MIT Alliance.	algorithm;data structure;finite-state machine;heap (data structure);linked data;monadic predicate calculus;nondeterministic finite automaton;post correspondence problem;precondition;recursion;shape analysis (digital geometry);specification language;three-valued logic;tiling window manager;tree automaton;type system;undecidable problem;verification and validation	Viktor Kuncak;Martin C. Rinard	2002	CoRR		lattice graph;data structure;specification language;null graph;graph property;computer science;regular graph;clique-width;forbidden graph characterization;comparability graph;linked data;shape analysis;voltage graph;distance-hereditary graph;graph;correspondence problem;programming language;graph homomorphism;vertex-transitive graph;complement graph;algorithm	PL	-9.453079845055234	20.376121768042356	48573
e186676e3c412695210833de4590765ae518e5c6	efficient learning of real time one-counter automata	learning algorithm;real time;control structure;decomposition algorithm;research paper or report	We present an eecient learning algorithm for languages accepted by deterministic real time one-counter automata (ROCA). The learning algorithm works by rst learning an initial segment, Bn, of the innnite state machine that accepts the unknown language and then decomposing it into a complete control structure and a partial counter. A new, eecient ROCA decomposition algorithm, which will be presented in detail, allows this result. The decomposition algorithm works in time O(n 2 log(n)) where nc is the number of states of Bn for some language-dependent constant c. If Angluin's algorithm for learning regular languages is used to learn Bn and the complexity of this step is h(n; m), where m is the length of the longest counterexample necessary for Angluin's algorithm, the complexity of our algorithm is O(h(n; m) + n 2 log(n)).	algorithm;automata theory;automaton;control flow;extensibility;finite-state machine;machine learning;real-time clock;regular language;time complexity	Amr F. Fahmy;Robert S. Roos	1995		10.1007/3-540-60454-5_26	real-time computing;wake-sleep algorithm;computer science;theoretical computer science;machine learning;stability;programming language;control flow;active learning;generalization error	Theory	-4.930126800536808	26.103124399326852	48652
23668d25c714aaa788032a31d463736ebc7b3786	on the structure of the monadic logic of the binary tree	second order;automate arbre;logic;orden 2;boolean combinaison;monadic second order;upper bound;tree automaton;arbol binario;automata arbol;arbre binaire;monadic logic;ordre 2;borne superieure;logique;logica;cota superior;binary tree	Since the work of Rabin [9], it has been known that any monadic second order property of the (labeled) binary tree with successor functions (and not the prefix ordering) is a monadic ∆3 property. In this paper, we show this upper bound is optimal in the sense that there is a monadic Σ2 formula, stating the existence of a path where a given predicate holds infinitely often, which is not equivalent to any monadic Π2 formula. We even show that some monadic second order definable properties of the binary tree are not definable by any boolean combination of monadic Σ2 and Π2 formulas. These results rely in particular on applications of Ehrenfeucht-Fräıssé like game techniques to the case of monadic Σ2 formulas.	binary tree;monadic predicate calculus;newton–cotes formulas;prefix order	David Janin;Giacomo Lenzi	1999		10.1007/3-540-48340-3_28	combinatorics;discrete mathematics;binary tree;mathematics;monadic predicate calculus;logic;algorithm	Logic	-5.862361939121947	18.85696285708621	48741
5ff626bf4d1594c810c1e99a6b2e8963713bb509	extending separation logic with fixpoints and postponed substitution	fixpoint connective;separation logic;assertion language;recursive definition;sp;abstract interpretation;separation-logic-based static analysis;axiomatic semantics;mutable data structure;forward analysis;fixpoint;extending separation logic;xp oint;wlp;static analysis;data structure;rule based	  We are interested in static analysis of programs which use shared mutable data structures. We introduce a backward and a forward  analyses with a separation logic called BI    μν  . This logic is an extension of BI logic [7], to which we add fixpoint connectives and a postponed substitution. This allows  us to express recursive definitions within the logic as well as the axiomatic semantics of while statements. Unlike the existing rule-based approach to program proof using separation logic, our approach does not have syntactical  restrictions on the use of rules.    	separation logic	Élodie-Jane Sims	2004		10.1007/978-3-540-27815-3_36	program analysis;boolean algebra;knowledge base;separation logic;data structure;computer science;artificial intelligence;software development;mathematics;semantics;axiom;programming language;axiomatic semantics;expert system;static analysis;algorithm;philosophy of logic	Logic	-17.145545836288914	21.21070369279253	48799
727c9a475f25de7d99972ae711a5bb891b674d16	satisfiability modulo recursive programs	verification;functional programs;satisfiability modulo theories;counterexample	We present a semi-decision procedure for checking satisfiability of expressive correctness properties of recursive first-order functional programs. In our approach, both properties and programs are expressed in the same language, a subset of Scala. We implemented our procedure and integrated it with the Z3 SMT solver and the Scala compiler. Our procedure is sound for counterexamples and for proofs of terminating functions. It is terminating and thus complete for many important classes of specifications, including all satisfiable formulas and all formulas where recursive functions satisfy certain syntactic restrictions. Using our system, Leon, we verified detailed correctness properties for functional data structure implementations, as well as syntax tree manipulations. We have found our system to be fast for both finding counterexamples and finding correctness proofs, and to scale to larger programs than alternative techniques.	compiler;correctness (computer science);decision problem;first-order predicate;leon;modulo operation;parse tree;purely functional data structure;recursion (computer science);rewriting;scala;semiconductor industry;simultaneous multithreading;solver	Philippe Suter;Ali Sinan Köksal;Viktor Kuncak	2011		10.1007/978-3-642-23702-7_23	verification;counterexample;programming language;satisfiability modulo theories;algorithm	PL	-16.54743413810736	22.075847038207417	48988
4064168bd797d6fbd51911ab3b20155e80c2a52d	ccs expressions, finite state processes, and three problems of equivalence	observational equivalence;calculus of communicating systems;role;computational complexity;finite state automata;script;enrollment;finite state processes	We examine the computational complexity of testing finite state processes for equivalence, in the Calculus of Communicating Systems (CCS). This equivalence problem in CCS is presented as a refinement of the familiar problem of testing whether two nondeterministic finite state automata (n.f.s.a.) accept the same language. Three notions of equivalence, proposed for CCS, are investigated: (1) observation equivalence, (2) congruence, and (3) failure equivalence. We show that observation equivalence (@@@@) can be tested in cubic time and is the limit of a sequence of equivalence notions (@@@@<subscrpt><italic>k</italic></subscrpt>), where, @@@@<subscrpt>1</subscrpt> is the familiar n.f.s.a. equivalence and, for each fixed <italic>k</italic>, @@@@<subscrpt><italic>k</italic></subscrpt> is PSPACE-complete. We provide an <italic>O</italic>(<italic>nlogn</italic>) test for congruence for <italic>n</italic> state processes of bounded fanout, by extending the algorithm that minimizes the states of d.f.s.a.'s. Finally, we show that, even for a very restricted type of process, testing for failure equivalence is PSPACE-complete.	algorithm;automata theory;calculus of communicating systems;computational complexity theory;congruence of squares;cubic function;fan-out;finite-state machine;negation as failure;pspace-complete;refinement (computing);time complexity;turing completeness	Paris C. Kanellakis;Scott A. Smolka	1983		10.1145/800221.806724	combinatorics;discrete mathematics;computer science;role;mathematics;finite-state machine;programming language;computational complexity theory;calculus of communicating systems;algorithm	Logic	-6.107892553210555	23.036892392959196	49091
1ff08116d1e0c1374ce1eff95514bdbe7e386b5e	multi-amalgamation of rules with application conditions in -adhesive categories		Amalgamation is a well-known concept for graph transformations that is used to model synchronised parallelism of rules with shared subrules and corresponding transformations. This concept is especially important for an adequate formalisation of the operational semantics of statecharts and other visual modelling languages, where typed attributed graphs are used for multiple rules with nested application conditions. However, the theory of amalgamation for the double-pushout approach has so far only been developed on a set-theoretical basis for pairs of standard graph rules without any application conditions. For this reason, in the current paper we present the theory of amalgamation for M-adhesive categories, which form a slightly more general framework than (weak) adhesive HLR categories, for a bundle of rules with (nested) application conditions. The two main results are the Complement Rule Theorem, which shows how to construct a minimal complement rule for each subrule, and the Multi-Amalgamation Theorem, which generalises the well-known Parallelism and Amalgamation Theorems to the case of multiple synchronised parallelism. In order to apply the largest amalgamated rule, we use maximal matchings, which are computed according to the actual instance graph. The constructions are illustrated by a small but meaningful running example, while a more complex case study concerning the firing semantics of Petri nets is presented as an introductory example and to provide motivation.	double pushout graph rewriting;hidden line removal;matching (graph theory);maximal set;modeling language;operational semantics;parallel computing;petri net;whole earth 'lectronic link	Ulrike Golas;Annegret Habel;Hartmut Ehrig	2014	Mathematical Structures in Computer Science	10.1017/S0960129512000345	combinatorics;discrete mathematics;petri net;semantics;bundle;operational semantics;mathematics;graph	Logic	-8.83606447798077	20.812704094897775	49181
cc185372621b680ab02999c888a95164912f0538	a termination criterion for graph transformations with negative application conditions	graph transformation	Termination of graph transformations is in general undecidable, but it is possible to prove it for specific systems by checking for sufficient conditions. In the presence of rules with negative application conditions, the difficulties increase. In this paper we propose a different approach to the identification of a (sufficient) criterion for termination, based on the construction of a labelled transition system whose states represent overlaps between the negative application condition and the right hand side that can give rise to cycles.	graph rewriting;transition system;undecidable problem	Paolo Bottoni;Francesco Parisi-Presicce	2010	ECEASST	10.14279/tuj.eceasst.30.419	null graph;computer science;programming language;complement graph;algorithm;graph rewriting	Logic	-9.146098015289237	21.270491676451172	49216
4a5d9e0896ae3c51a2c4169d9a63a9c3d15ad5bf	propositional dynamic logic of context-free programs and fixpoint logic with chop	complexite;procesamiento informacion;algorithm analysis;dynamique;logique programme;complejidad;formal languages;simultaneidad informatica;context free;logique propositionnelle;complexity;specification programme;dinamica;concurrency;dynamics;propositional logic;informatique theorique;concurrence;information processing;datavetenskap datalogi;analyse algorithme;decidibilidad;computer science;logica proposicional;propositional dynamic logic;traitement information;decidabilite;program specification;simultaneite informatique;lenguaje formal;especificacion programa;analisis algoritmo;formal language;decidability;computer theory;informatica teorica;langage formel	This paper compares Propositional Dynamic Logic of Non-Regular Programs and Fixpoint Logic with Chop. It identifies a fragment of the latter which is equiexpressive to the former. This relationship transfers several decidability and complexity results between the two logics.	context-free language;decision problem;dynamic logic (modal logic);first-order logic;fixed point (mathematics)	Martin Lange;Rafal Somla	2006	Inf. Process. Lett.	10.1016/j.ipl.2006.04.019	dynamic logic;zeroth-order logic;t-norm fuzzy logics;modal μ-calculus;formal language;classical logic;resolution;linear temporal logic;description logic;higher-order logic;information processing;many-valued logic;interval temporal logic;intuitionistic logic;computer science;intermediate logic;artificial intelligence;łukasiewicz logic;mathematics;propositional variable;well-formed formula;substructural logic;algorithm;autoepistemic logic	Logic	-7.271247802109161	19.863032637402974	49280
7f468eeda39604872ff5cb37e94ab81c86aa13e1	on the verification of probabilistic i/o automata with unspecified rates	distributed system;probabilistic automata;model checking;exact computation;probabilistic i o automata;markov decision process;distributed systems	We consider the Probabilistic I/O Automata framework, for which we address the verification of reachability properties in case the rates (also called delay parameters) are unspecified. We show that the problem of finding (or even approximating) the supremum probability that a set of states is reached is undecidable. However, we give an algorithm to obtain a non-trivial over-estimation of this value. We explain why this over-estimation may result useful for many systems. Finally, in order to compare our approach against Markov Decision Processes, we study a simple protocol for anonymous fair service. In this case, the over-estimation computed over the PIOA gives a more realistic result than the exact computation over the MDP.	algorithm;automata theory;automaton;computation;input/output;markov chain;markov decision process;reachability;undecidable problem	Sergio Giro;Pedro R. D'Argenio	2009		10.1145/1529282.1529406	markov decision process;model checking;quantum finite automata;computer science;theoretical computer science;probabilistic automaton	Logic	-9.752654566361318	26.186835714462692	49435
d63dd22315619e4e23d826df0a4338007cceb956	continuization of timed petri nets: from performance evaluation to observation and control	modelizacion;evaluation performance;systeme evenement discret;performance evaluation;temps polynomial;algorithm analysis;red petri;evaluacion prestacion;state observer;analyse temporelle;analisis temporal;parametric design;time analysis;sistema acontecimiento discreto;modelisation;discrete event system;time petri net;discrete model;polynomial time;analyse algorithme;state explosion;petri net;modeling;analisis algoritmo;reseau petri;discrete system;tiempo polinomial	State explosion is a fundamental problem in the analysis and synthesis of discrete event systems. Continuous Petri nets can be seen as a relaxation of discrete models allowing more efficient (in some cases polynomial time) analysis and synthesis algorithms. Nevertheless computational costs can be reduced at the expense of the analyzability of some properties. Even more, some net systems do not allow any kind of continuization. The present work first considers these aspects and some of the alternative formalisms usable for continuous relaxations of discrete systems. Particular emphasis is done later on the presentation of some results concerning performance evaluation, parametric design and marking (i.e., state) observation and control. Even if a significant amount of results are available today for continuous net systems, many essential issues are still not solved. A list of some of these are given in the introduction as an invitation to work on them.	algorithm;computation;discrete system;item unique identification;linear programming relaxation;parametric design;performance evaluation;petri net;time complexity	Manuel Silva Suárez;Laura Recalde	2005		10.1007/11494744_4	time complexity;simulation;systems modeling;computer science;artificial intelligence;discrete system;state observer;petri net;algorithm	Logic	-9.48350384650044	26.78113444791004	49439
7e3573aa3fb835d62f127a82245af215dc1ceeaa	worst case examples for operations on obdds	boolean functions;efficient algorithm;software verification;boolean function;ordered binary decision diagram;model checking;computational complexity;ordered binary decision diagrams;variable ordering;equivalence checking;heuristic algorithm	Ordered BDDs (OBDDs) are representations of Boolean functions which have found a lot of applications in hardware and software verification, model checking, and CAD. The reason is that OBDDs allow efficient algorithms for important operations like synthesis and equivalence check. Moreover, good variable orderings are constructed by heuristic algorithms like sifting. Here worst case examples for these operations and algorithms are presented.  2000 Elsevier Science B.V. All rights reserved.	algorithm;best, worst and average case;binary decision diagram;computer-aided design;heuristic;model checking;ordered pair;software verification;turing completeness	Ingo Wegener	2000	Inf. Process. Lett.	10.1016/S0020-0190(00)00053-3	combinatorics;discrete mathematics;software verification;computer science;mathematics;boolean function;algorithm	EDA	-14.076994003898717	27.95822561849053	49622
39d8ad0550e5e6d305f8b83b035ebbdef63de624	deadlock control policy for a class of automated manufacturing systems with key resources	manufacturing systems;resource allocation computational complexity computer aided manufacturing concurrency control petri nets;complexity theory;会议论文;polynomials;firing;petri nets system recovery manufacturing systems firing fires polynomials complexity theory;petri net automated manufacturing systems deadlock control;system recovery;petri nets;fires;ams with center resources deadlock control policy automated manufacturing systems key resources one unit resources resource transition circuits optimal petri net based polynomial complexity deadlock avoidance policies petri net models deadlock control problem secondary deadlock maximal perfect control transition circuits mpct circuits	For automated manufacturing systems (AMSs) without center resources that are one-unit resources shared by two or more maximal perfect resource transition circuits, the optimal Petri net-based polynomial complexity deadlock avoidance policies are synthesized in our previous work. Based on Petri net models of AMSs, this work focuses on the deadlock control problem for AMSs with center resources. First, the concepts of key resources and key transitions are presented. It can prove that key transitions can bring AMSs with key resources into secondary-deadlock, i.e. the controlled Petri nets of AMSs with key resources are not live. Second, for a class of AMSs with key resources, secondary-deadlock can be characterized by maximal perfect control transition circuits (MPCT-circuits) that are saturated at a reachable marking of the controlled system. Then, by adding a control place and related arcs to each MPCT-circuit, secondary-deadlock can be prevented. Thereby, a deadlock control policy for a class of AMSs with center resources is synthesized. Finally, a few examples are provided to demonstrate the presented policy and used to compare them with the state-of-the-art methods.	automation;deadlock;item unique identification;maximal set;petri net;time complexity	Huixia Liu;Weimin Wu	2015	2015 IEEE 12th International Conference on Networking, Sensing and Control	10.1109/ICNSC.2015.7116085	real-time computing;computer science;distributed computing;petri net;deadlock prevention algorithms;polynomial	Robotics	-6.3556944942211535	29.03552381410604	49746
c9b11c19905d24072c0313bdd62d808fbe5c9469	a mechanized proof environment for the convenient computations proof method	distributed database;sequential consistency;pvs;computational method;satisfiability;theorem proving;theorem prover;convenient computations;formal verification;distributed shared memory;deductive theorem proving;proof environment	A mechanized verification environment made up of theories over the deductive mechanized theorem prover PVS is presented, which allows taking advantage of the “convenient computations” method. This method reduces the conceptual difficulty of proving a given property for all the possible computations of a system by separating two different concerns: (1) proving that special convenient computations satisfy the property, and (2) proving that every computation is related to a convenient one by a relation which preserves the property. The approach is especially appropriate for applications in which the first concern is trivial once the second has been shown, e.g., where the specification itself is that every computation reduces to a convenient one. Two examples are the serializability of transactions in distributed databases, and sequential consistency of distributed shared memories. To reduce the repetition of effort, a clear separation is made between “infrastructural” theories to be supplied as a proof environment PVS library to users, and the specification and proof of particular examples. The provided infrastructure formally defines the method in its most general way. It also defines a computation model and a reduction relation—the equivalence of computations that differ only in the order of finitely many independent operations. One way to prove that this relation holds between every computation and some convenient one involves the definition of a measure function from computations into a well-founded set. Two possible default measures, which can be applied in many cases, are also defined in the infrastructure, along with useful lemmas that assist in their usage. We show how the proof environment can be used, by a step-by-step explanation of an application example.		Marcelo Glusman;Shmuel Katz	2003	Formal Methods in System Design	10.1023/A:1024746015231	computer science;theoretical computer science;automated theorem proving;programming language;distributed database;proof complexity;algorithm	PL	-14.50879695567442	18.936674899886842	49799
c628e5f1d6ab39680c1c386d606b47f5c949099a	the use of symbol-state tables		This paper describes how a certain kind of table may be used to check the syntax of a string of symbols. In its simplest form, a symbol-state table closely resembles a finite state machine, and requires a large amount of space, often to store duplicated information. By means of the subroutine principle such duplication may be avoided, and recursive use of the table achieved, making possible the checking of recursive structures. When syntax checking is not the only aim, or when checking needs to be performed which is beyond the scope of the table, sections of program coding may be called from the table. Applications have included the writing in FORTRAN of a syntax checker for FORTRAN, and the automatic proof reading of dictionary entries.	dictionary;finite-state machine;fortran;grammar checker;recursion;state transition table;string (computer science);subroutine	A. C. Day	1970	Comput. J.	10.1093/comjnl/13.4.332	table	PL	-14.87884638196485	32.00157586285704	49820
f634bba2c4e56e1448431320b1ffd4bf593828cb	implicit computation of compatible sets for state minimization of isfsms	minimisation;minimization;data structures boolean functions logic automata laboratories minimization methods;synthese circuit;diagrama binaria decision;diagramme binaire decision;logic design;circuito secuencial;maquina estado finito;funcion logica;minimisation of switching nets;asynchronous synthesis implicit computation compatible sets state minimization isfsm incompletely specified finite state machine sequential synthesis bdd logic synthesis algorithm;sequential circuits;circuit sequentiel;minimizacion;function synthesis;set theory;methode calcul;logical function;fonction logique;algorithme;metodo calculo;algorithm;finite state machines;logic synthesis;sintesis funcion;characteristic function;sintesis circuito;asynchronous circuits;synthese fonction;set theory finite state machines sequential circuits asynchronous circuits logic design minimisation of switching nets;machine etat fini;finite state machine;computing method;circuit synthesis;algoritmo;binary decision diagram;sequential circuit	The computation of sets of compatibles of incompletely specified finite-state machines (ISFSMs) is a key step in sequential synthesis. This paper presents implicit computations to obtain sets of maximal compatibles, compatibles, prime compatibles, implied sets, and class sets. The computations are implemented by means of BDDs that realize the characteristic functions of these sets. We have demonstrated with experiments from a variety of benchmarks that implicit techniques allow us to handle examples exhibiting a number of compatibles up to 2/sup 1500/, an achievement outside the scope of programs based on explicit enumeration. We have shown, in practice, that ISFMSs with a very large number of compatibles may be produced as intermediate steps of logic synthesis algorithms, for instance, in the case of asynchronous synthesis. This shows that the proposed approach not only has a theoretical interest, but also practical relevance for current logic synthesis applications, as shown by its application to ISFSM state minimization.	computation	Timothy Kam;Tiziano Villa;Robert K. Brayton;Alberto L. Sangiovanni-Vincentelli	1997	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.644029	combinatorics;discrete mathematics;logic synthesis;computer science;mathematics;sequential logic;finite-state machine;algorithm	EDA	-13.816800209728834	23.360178130210954	49913
9eb9c5df85b90b8289db299c0535b314de8f4931	fault detection by backwards analysis in coloured workflow nets	coloured petri nets;workflow nets;fault detection;backwards analysis	The increasing complexity of the business processes requires automated methods for trouble-shooting and debugging of the process model in operation. This paper proposes mechanism of fault detection based on the concept of backwards reachability for the coloured workflow nets. The formal verification methods defined for coloured Petri nets such as state space method or place invariants declaration suffer from fast-growing computational complexity. The article offers the set of firing rules for backwards token-play on coloured workflow net. This method helps to detect resource-related failures of the two most common types: ”the missing arc” fault (the resource was not initialized) and ”wrong expression fault” (there is no proper resource).	business process;coloured petri net;computational complexity theory;debugging;declaration (computer programming);fault detection and isolation;formal verification;process modeling;reachability;state space;word lists by frequency	Vasilii Ganishev;Olga Fengler;Wolfgang Fengler	2015		10.5220/0005513000880094	real-time computing;computer science;distributed computing;fault detection and isolation;algorithm	Logic	-17.705801488790406	29.268168435134008	50071
257f575da7fceebce4989214d34f4e8212f733a9	bounded model checking of software using smt solvers instead of sat solvers	software analysis;satisfiability;bounded model checking;computer experiment;propositional logic;sat solver;satisfiability modulo theories	C bounded model checking (cbmc) has proved to be a successful approach to automatic software analysis. The key idea is to (i) build a propositional formula whose models correspond to program traces (of bounded length) that violate some given property and (ii) use state-of-the-art SAT solvers to check the resulting formulae for satisfiability. In this paper, we propose a generalisation of the cbmc approach on the basis of an encoding into richer (but still decidable) theories than propositional logic. We show that our approach may lead to considerably more compact formulae than those obtained with cbmc. We have built a prototype implementation of our technique that uses a satisfiability modulo theories (SMT) solver to solve the resulting formulae. Computer experiments indicate that our approach compares favourably with—and on some significant problems outperforms—cbmc.	boolean satisfiability problem;computer experiment;model checking;modulo operation;propositional calculus;prototype;satisfiability modulo theories;shattered world;solver;theory;tracing (software)	Alessandro Armando;Jacopo Mantovani;Lorenzo Platania	2008	International Journal on Software Tools for Technology Transfer	10.1007/s10009-008-0091-0	computer experiment;computer science;theoretical computer science;software analysis pattern;dpll algorithm;propositional calculus;boolean satisfiability problem;satisfiability modulo theories;algorithm;satisfiability	SE	-14.987812440502223	25.11817351562533	50307
b28fc98dc8e9f98f7efb30309d9b05337c01c641	complexity of bio-computation: symbolic dynamics in membrane systems	computational mechanics;robust bio computation;vector addition systems;membrane systems;dataflow computation;geometry of computation;automata decomposition;symbolic dynamics	We discuss aspects of biological relevance to the modelling of bio-computation in a multiset rewriting system context: turnover, robustness against perturbations, and the dataflow programming paradigm. The systems under consideration are maximally parallel and asynchronous parallel membrane systems, the latter corresponding to computation in which the notion of time is operationally meaningless. A natural geometrical setting which seems promising for the study of computational processes in general multiset rewriting systems is presented. Configuration space corresponds to a subset of the lattice , d ∈ N, and state transitions correspond to vector addition. The similarities and differences with Vector Addition Systems and Petri nets are discussed. Symbolic dynamics are introduced on special partitions of configuration space and we indicate different notions of complexity for membrane systems based on this and related concepts such as graph complexity and minimal automata. Some examples of synchronized, pipelined dataflow computations are given and decompositions into functional subunits are briefly commented on.	british informatics olympiad;computation	Michael Muskulus;Robert Brijder	2006	Int. J. Found. Comput. Sci.	10.1142/S0129054106003747	symbolic dynamics;combinatorics;discrete mathematics;computer science;computational mechanics;theoretical computer science;mathematics;algorithm	Logic	-10.155856385070255	20.6961025315759	50340
eb86300d0dd9662fc03862697dae40a24869adb5	the grammar hammer of 2012		Normal Form In order to fit any grammar into the conditions required by the previously described matching techniques, we demand the following normalisation: 1. lack of labels for production rules 2. lack of named subexpressions 3. lack of terminal symbols 4. maximal outward factoring of inner choices 5. lack of horizontal production rules 6. lack of separator lists 7. lack of trivially defined nonterminals (with α, ε or φ) 8. no mixing of chain and non-chain production rules 9. the nonterminal call graph is connected, and its top nonterminals are the starting symbols of the grammar It can be shown that transforming any grammar into its Abstract Normal Form is in fact a grammar mutation (see §3.8.1). In the prototype, I have implemented it to effectively generate bidirectional grammar transformation steps, so the normalisation preserves any information that it needs to abstract from. Grammar design mutation Some grammar design smells (terminology per [Sto12a]) like yaccification (per [SV99; BSV98]) or layered expressions (per [LZ09a]) have shown to be persistent enough to survive all normalisations and cause problems for establishing nominal and structural mappings. They can be identified and dealt with by automated analyses and mutations, but so far I have to proof that they are the only possible obstacles, and no guarantees about any other smells problematic for guided grammar convergence. 3.1.1 Generalisation of production signatures The method of establishing nonterminal mappings of different grammars of the same intended language, can be generalised as follows. Suppose that we have a metalanguage. Without loss of generality, let us assume that each grammar definition construct that is present in it, can be referred to by a single symbol: “,”, “?”, “*”, etc and uses prefix notation. This metasyntactic alphabet Λ will form the foundation of our footprints and signatures. Let us also assume that all metasymbols are unary or are encoded as unary, except for two composition constructs: a sequential “,” and an alternative “|”, which take a list of symbols. Then, a footprint of any nonterminal n in an expression x is a multiset of metasymbols that are used for occurrences of n within x: πn(x) =  {1} if x = n {μ} if x = μ(n), μ ∈ Λ ⋃ e∈L πn(e) if x = ,(L) ∅ otherwise, also if x = |(L) Our previously given definition of a production signature can still be used with this generally redefined footprints. It is well known that language equivalence is undecidable. Any formulation of the grammar equivalence problem, that is based on language equivalence, is thus also undecidable. Grammar convergence [LZ09a; LZ11] is a practically reformulated grammar equivalence problem that uses automated grammar transformation steps programmed by a human expert. By using these generalised metasyntactic signatures, we can infer converging transformation steps automatically, thus eliminating the weakest link of the present methodology. However, this is not the only application of the generalisation. The most trivial use of metasyntactic footprints and signatures would lie in grammarware metrics. Research on software metrics applied to context-free grammars has never been an extremely popular topic, but it did receive some attention in the 1970s [Gru71], 1980s [Kel81] and even recently [PM04; Čre+10]. Using quantitative aspects of metasyntactic footprints and signatures (numbers of different footprints within	antivirus software;call graph;context-free grammar;context-free language;definition;design smell;electronic signature;integer factorization;lambda calculus;maximal set;metasyntactic variable;polish notation;production (computer science);prototype;row hammer;skip list;software metric;terminal and nonterminal symbols;turing completeness;type signature;unary operation;undecidable problem	Vadim Zaytsev	2012	CoRR		computer science;artificial intelligence;algorithm	PL	-13.329125907779103	19.517774065527785	50455
0c5b71abbb2ccc86e0c26d2238765ac8854b96ce	encoding turing machines into the deterministic lambda-calculus		1. Weakly strategy independent : the image of the encoding is a very small fragment of the λ-calculus, that we call the deterministic λ-calculus Λdet. Essentially, it is the CPS (continuation-passing style) λ-calculus restricted to weak evaluation (i.e., not under abstractions). In Λdet every term has at most one redex, and so all weak strategies collapse into a single deterministic evaluation strategy, because there are no choices between redexes to be made. The important consequence of this property is that every weak evaluation strategy then allows to simulate Turing machines, as well as any strong strategy reducing weak head redexes (or even only weak head redexes) first.	continuation-passing style;lambda calculus;reduction strategy (code optimization);simulation;turing machine	Ugo Dal Lago;Beniamino Accattoli	2017	CoRR		2-exptime;description number;algorithm;nspace;discrete mathematics;probabilistic turing machine;turing machine examples;super-recursive algorithm;time hierarchy theorem;turing machine;computer science	Theory	-11.124864777819287	19.928394373734204	50507
65c04278f44ca76e050691190b084358d398ca16	automatic generation of simple lemmas from recursive definitions using decision procedures - preliminary report	canonical equation;circuito aritmetico;certification;search space;canonical form;generacion automatica;forme canonique;ecuacion canonica;teoria conjunto;distributed computing;theorie ensemble;equation canonique;set theory;automatic generation;induccion;generation automatique;induction;decision procedure;proof carrying code;algorithme reparti;certificacion;forma canonica;calculo repartido;algoritmo repartido;decidibilidad;information system;decidabilite;distributed algorithm;calcul reparti;systeme information;circuit arithmetique;decidability;arithmetic circuit;sistema informacion	Using recent results on integrating induction schemes into decidable theories, a method for generating lemmas useful for reasoning about T -based function definitions is proposed. The method relies on terms in a decidable theory admitting a (finite set of) canonical form scheme(s) and ability to solve parametric equations relating two canonical form schemes with parameters. Using nontrivial examples, it is shown how the method can be used to automatically generate many simple lemmas; these lemmas are likely to be found useful in automatically proving other nontrivial properties of T -based functions, thus unburdening the user of having to provide many simple intermediate lemmas. During the formalization of a problem, after a user inputs T -based definitions, the method can be employed in the background to explore a search space of possible conjectures which can be attempted, thus building a library of lemmas as well as false conjectures. This investigation was motivated by our attempts to automatically generate lemmas arising in proofs of generic, arbitrary data-width parameterized arithmetic circuits. The scope of applicability of the proposed method is broader, however, including generating proofs for proof-carrying codes, certification of proof-carrying code as well as in reasoning about distributed computation algorithms.	algorithm;computation;distributed computing;proof-carrying code;recursion (computer science);theory	Deepak Kapur;Mahadevan Subramaniam	2003		10.1007/978-3-540-40965-6_9	decidability;canonical form;distributed algorithm;discrete mathematics;computer science;artificial intelligence;machine learning;mathematics;distributed computing;certification;information system;algorithm;statistics;set theory	AI	-14.420342964019165	24.74406305998932	50608
85a5ed6f8dd5ad4fb4ed9182467811ccff5c40f0	convex hull abstractions in specialization of clp programs	top down method;methode descendante;programmation logique avec contrainte;constraint logic programs;bottom up;capsula convexa;desigualdad;inequality;top down;programacion logica con restriccion;inegalite;logical programming;expresion aritmetica;probleme terminaison;enveloppe convexe;programmation logique;arithmetic expression;expression arithmetique;termination problem;constraint logic programming;logic programs;interpretation abstraite;abstract interpretation;convex hull;programacion logica;problema terminacion	We introduce an abstract domain consisting of atomic formulas constrained by linear arithmetic constraints (or convex hulls). This domain is used in an algorithm for specialization of constraint logic programs. The algorithm incorporates in a single phase both top-down goal directed propagation and bottom-up answer propagation, and uses a widening on the convex hull domain to ensure termination. We give examples to show the precision gained by this approach over other methods in the literature for specializing constraint logic programs. The specialization method can also be used for ordinary logic programs containing arithmetic, as well as constraint logic programs. Assignments, inequalities and equalities with arithmetic expressions can be interpreted as constraints during specialization, thus increasing the amount of specialization that can be achieved.	algorithm;bottom-up parsing;constraint (mathematics);convex hull;logic programming;partial template specialization;software propagation;top-down and bottom-up design	Julio C. Peralta;John P. Gallagher	2002		10.1007/3-540-45013-0_8	discrete mathematics;computer science;artificial intelligence;top-down and bottom-up design;mathematics;programming language;algorithm	AI	-17.181909927160426	20.97373263604974	50902
5bc8b81d70efc6a52fa117cc528a230d9b8ed1e6	demand transformation analysis for concurrent constraint programs	concurrent constraint programming;concurrent constraint;dataflow analysis;cost estimation;abstract interpretation	This paper presents a demand transformation analysis that maps a predi-cate's output demands to its input demands. This backward dataaow analysis for concurrent constraint programs is constructed in the framework of abstract interpretation. In the context of stream parallelism, this analysis identiies an amount of input data for which predicate execution can safely wait without danger of introducing deadlock. We assume that programs are well-moded and prove that our analysis is safe. We have constructed an implementation of this analysis and tested it on some small, illustrative programs and have determined that it gives useful results in practice. We identify several applications of the analysis results to distributed implementations of concurrent constraint languages, including thread construction and communication granularity control. This analysis will enable existing computational cost estimation analyses to be applied to stream-parallel logic languages.	abstract interpretation;algorithmic efficiency;concurrent constraint logic programming;deadlock;map;parallel computing	Moreno Falaschi;Patrick Hicks;William H. Winsborough	1996	J. Log. Program.	10.1016/S0743-1066(99)00004-7	constraint logic programming;concurrent constraint logic programming;mathematical optimization;constraint programming;real-time computing;constraint satisfaction;programming language;hybrid algorithm;cost estimate	PL	-18.25718335447154	30.963137515987373	50930
d2220496f276af01976debed9f8fd66c107a3980	synthesis of petri net supervisors for fms via redundant constraint elimination	redundancy reduction;computacion informatica;grupo de excelencia;flexible manufacturing system;ciencias basicas y experimentales;期刊论文;linear programming;deadlock;petri nets	The Minimal number of Control Places Problem (MCPP), which is formulated to obtain optimal and structurally minimal supervisors, needs extensive computation. The current methods to reduce the computational burden have mainly focused on revision of the original formulation of MCPP. Instead, this paper presents methods to accelerate its solution by eliminating its redundant reachability constraints. The optimization problem scale required for supervisor synthesis is thus drastically reduced. First, a sufficient and necessary condition for a reachability constraint to be redundant is established in the form of an integer linear program (ILP), based on a newly proposed concept called feasible region of supervisors. Then, two kinds of redundancy elimination methods are proposed: an ILP one and a non-ILP one. Most of the redundant reachability constraints can be eliminated by our methods in a short time. The computational time to solve MCPP is greatly reduced after the elimination, especially for large-scale systems. The obtained supervisors are still optimal and structurally minimal. Finally, numerical tests are conducted to show the efficiency and effectiveness of the proposed methods.	petri net	Bo Huang;Mengchu Zhou;Gongxuan Zhang	2015	Automatica	10.1016/j.automatica.2015.08.011	real-time computing;linear programming;deadlock;control theory;mathematics;petri net;algorithm	Logic	-7.243780236208895	29.345522032101055	50943
f2c2fab68adb99a5ef41414c6771ceb6d3e5a149	on-line timed protocol trace analysis based on uncertain state descriptions	efsm model;on-line timed protocol trace;time constraints;uncertain state descriptions;passive monitoring;trace analysis;real time	This paper presents a new approach to the task of passive protocol tracing. The method called FollowSM for the first time meets all requirements of practical in-field use, including the checking of time constraints, the independence of the current state when starting the analysis, the admittance of nondeterminism, and on-line real time analysis capability. This is achieved by a suitable modeling of the implementation under test and the generalization of the tracing algorithm to operate on state information with any degree of uncertainty. FollowSM has been implemented as a prototype system and proved capable of minimizing the time required for troubleshooting.	algorithm;computability;correctness (computer science);experiment;golm metabolome database;online and offline;prototype;requirement;simple directmedia layer	Marek Musial	1997			real-time computing;computer science;distributed computing;algorithm	Embedded	-8.569619236338216	28.204663323617943	50981
414af7ff19b6febee0acdb13675acb2623f9a7c6	a measured collapse of the modal µ-calculus alternation hierarchy	reponse temporelle;verificacion modelo;game theory;algorithm complexity;hierarchized structure;complejidad algoritmo;decision diagram;espace etat;verification modele;simultaneidad informatica;teoria juego;structure hierarchisee;concurrent program;theorie jeu;logique point fixe;program verification;fonction caracteristique;verificacion programa;modal logic;concurrency;first order;complexite algorithme;model checking;logica punto fijo;state space method;time response;methode espace etat;informatique theorique;logique ordre 1;contexto;logique modale;state space;programa competidor;characteristic function;contexte;logica modal;concurrent programs;attraction;fixed point logic;state explosion;atraccion;espacio estado;respuesta temporal;verification programme;simultaneite informatique;funcion caracteristica;context;estructura jerarquizada;first order logic;programme concurrent;metodo espacio estado;computer theory;logica orden 1;informatica teorica	Theμ-calculus model-checking problem has been of great interest in the context of concurrent programs. Beyond the need to use symbolic methods in order to cope with the state-explosion problem, which is acute in concurrent settings, several concurrency related problems are naturally solved by evaluation of μ-calculus formulas. The complexity of a naive algorithm for model checking a μ-calculus formulaψ is exponential in the alternation depth d of ψ. Recent studies of theμ-calculus and the related area of parity games have led to algorithms exponential only in 2 . No symbolic version, however, is known for the improved algorithms, sacrificing the main practical attraction of the μ-calculus. Theμ-calculus can be viewed as a fragment of first-order fixpoint logic. One of the most fundamental theorems in the theory of fixpoint logic is the Collapse Theorem, which asserts that, unlike the case for the μ-calculus, the fixpoint alternation hierarchy over finite structures collapses at its first level. In this paper we show that the Collapse Theorem of fixpoint logic holds for a measured variant of theμ-calculus, which we call μ-calculus. Whileμ-calculus formulas represent characteristic functions, i.e., functions from the state space to {0, 1}, formulas of the μ-calculus represent measure functions, which are functions from the state space to some measure domain. We prove a M asured-Collapse Theorem : every formula in theμ-calculus is equivalent to a least-fixpoint formula in the μcalculus. We show that the Measured-Collapse Theorem provides a logical recasting of the improved algorithm for μ-calculus model-checking, and describe how it can be implemented symbolically using Algebraic Decision Diagrams. Thus, we describe, for the first time, a symbolic μ-calculus model-checking algorithm whose complexity matches the one of the best known enumerative algorithm.	algorithm;concurrency (computer science);diagram;first-order logic;first-order predicate;fixed point (mathematics);least fixed point;modal logic;modal μ-calculus;model checking;state space;time complexity	Doron Bustan;Orna Kupferman;Moshe Y. Vardi	2004		10.1007/978-3-540-24749-4_46	modal μ-calculus;game theory;computer science;calculus;first-order logic;mathematics;programming language;algorithm	Logic	-8.329145009951343	19.887366694503758	51051
82394a6f2cdf8fb91a6b81587ce07a5de100d6dc	string diagrams for free monads (functional pearl)	universal property;free monad;distributive law;string diagram;monad	We show how one can reason about free monads using their universal properties rather than any concrete implementation. We introduce a graphical, two-dimensional calculus tailor-made to accommodate these properties.	diagram;graphical user interface	Maciej Piróg;Nicolas Wu	2016		10.1145/2951913.2951947	distributive property;computer science;distributive law between monads;programming language;monad;algorithm;universal property	PL	-12.499887072163796	18.855138384198582	51088
c2d883dbad8a6746248ef0d98a72e7d8fb08a1d3	open to change: a theory for iterative test-driven modelling		We introduce open tests to support iterative test-driven process modelling. Open tests generalise the trace-based tests of Zugal et. al. to achieve modularity : whereas a trace-based test passes if a model exhibits a particular trace, an open test passes if a model exhibits a particular trace up to abstraction from additional activities not relevant for the test. This generalisation aligns open tests better with iterative test-driven development: open tests may survive the addition of activities and rules to the model in cases where trace-based tests do not. To reduce overhead in re-running tests, we establishing sufficient conditions for a model update to preserve test outcomes. We introduce open tests in an abstract setting that applies to any process notation with trace semantics, and give our main preservation result in this setting. Finally, we instantiate the general theory for the DCR Graph process notation, obtaining a method for iterative test-driven DCR process modelling.	complexity;constraint (mathematics);denotational semantics;iteration;iterative method;overhead (computing);process modeling;succession;test-driven development;tracing (software)	Tijs Slaats;Søren Debois;Thomas T. Hildebrandt	2018		10.1007/978-3-319-98648-7_3	systems engineering;notation;theoretical computer science;semantics;modularity;generalization;computer science;abstraction;graph;process modeling	AI	-17.088440475743408	23.99860147376556	51231
960e576f34c448274de90b4dd22c14e39553b438	using partial-order semantics to avoid the state explosion problem in asynchronous systems	partial order semantics;system modeling;branch point;delay insensitive;asynchronous system;model checking;state explosion;partial order	We avoid state explosion in model checking of delay-insensitive VLSI systems by not using states. Systems are networks of communicating finite-state nonsequential processes with well-behaved nondeterministic choice. A specification strategy based on partial orders allows precise description of the branching and recurrence structure of processes. Process behaviors are modelled by pomsete, but (discrete) sets of pomeets with implicit branching structure are replaced by pomtreee, which have finite presentations by (automaten-like) behavior machines. The latter distinguish both concurrency and branching points, and define a finite recurrence structure. Safety and liveness checking are integrated. In contrast to state methods, our methods do not require enumeration or recording of states. We avoid separate consideration of execution sequences that do not differ in their partial order, and ensure termination by recording only a small number of system loop cutpoints -in the form of system behavior states. In spite of the name, behavior states are not states.	asynchronous circuit;concurrency (computer science);delay insensitive circuit;liveness;model checking;very-large-scale integration	David K. Probst;Hon Fung Li	1990		10.1007/BFb0023728	partially ordered set;asynchronous system;model checking;branch point;real-time computing;systems modeling;computer science;theoretical computer science;algorithm	Logic	-11.990897413830472	27.215507809651264	51418
4a1138a1ea4704525118b99c5f165486c8851611	reachability analysis of time basic petri nets: a time coverage approach	settore inf 01 informatica;ignition;semantics;timed petri nets;symbolic reachability analysis;software engineering;symbolic reachability analysis timed petri nets linear constraints;semantics petri nets ignition fires reachability analysis buildings time factors;linear constraints;time factors;formal verification;model checking engine reachability analysis time basic petri nets time coverage approach realtime system time constraint transition firing time marking time description symbolic reachability graph time bounded inspection possibly infinite tree tree graph construction algorithm symbolic graph construction use case;model checking;petri nets;petri net;fires;use case;reachability analysis;buildings;real time systems formal verification petri nets reachability analysis;real time systems;time constraint	We introduce a technique for reach ability analysis of Time-Basic (TB) Petri nets, a powerful formalism for real time systems where time constraints are expressed as intervals, representing possible transition firing times, whose bounds are functions of marking's time description. The technique consists of building a symbolic reach ability graph relying on a sort of time coverage, and overcomes the limitations of the only available analyzer for TB nets, based in turn on a time-bounded inspection of a (possibly infinite) tree-tree. The graph construction algorithm has been automated by a tool-set, briefly described in the paper together with its main functionality and analysis capability. A running example is used throughout the paper to sketch the symbolic graph construction. A use case describing a small real system - that the running example is an excerpt from - has been employed to benchmark the technique and the tool-set. The main outcome of this test are also presented in the paper. Ongoing work, in the perspective of integrating with a model-checking engine, is shortly discussed.	algorithm;benchmark (computing);item unique identification;model checking;petri net;reachability;semantics (computer science);terabyte	Carlo Bellettini;Lorenzo Capra	2011	2011 13th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2011.16	real-time computing;computer science;software engineering;semantics;programming language;petri net;algorithm	SE	-12.99500096022155	26.083250384966618	51434
c5452703b5c1da1f0c35c4ea6c36a15a031cadeb	infinitely running concurrent processes with loops from a geometric viewpoint	concurrent systems;concurrent process	This report gives a formal topological semantics to inductively defined concurrent systems and investigates the properties of such systems. We allow loops and infinitely running computations, which is new in the topological investigations of concurrency. In this more general setting, we prove the equivalent to the result from [2] that deadlocks and unsafe points can be found using a finite number of deloopings.	computation;concurrency (computer science);deadlock;viewpoint	Lisbeth Fajstrup;Stefan Sokolowski	2001	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)01147-3	discrete mathematics;real-time computing;computer science;mathematics;distributed computing	Logic	-9.667357318100905	23.186825089772288	51534
603f1cbea708a99e3f41ed0cebff72a9c99799e5	transformation of combined data type and process specifications using projection algebras	algebraic specification;algebraic approach;metric space;data type	The concept of projection specifications was recently introduced as a purely algebraic approach to the specification of continuous algebras in the framework of metric spaces. It allows to combine data typeand process specifications within one formalism. Parameterized projection specifications, corresponding to usual algebraic parameterized specifications, carry over compositionality to combined data type and process specifications. The parameter part may contain data types as well as process types. Transformation concepts for algebraic specifications are shown to apply also to projection specifications; i.e. extension and refinement, and different notions of implementation can be generalized to projection specifications.		Martin Große-Rhode;Hartmut Ehrig	1989		10.1007/3-540-52559-9_69	mathematical analysis;discrete mathematics;singular point of an algebraic variety;recursive data type;algebraic data type;dimension of an algebraic variety;mathematics;real algebraic geometry;algebraic surface;algebraic extension;function field of an algebraic variety;algebraic function;differential algebraic geometry;generalized algebraic data type;algebraic cycle;algebra	DB	-12.927868685828381	18.927633034801364	51708
12fd8cf1e8b311033e076aadf24ed2ca65ef0680	lazy abstraction-based control for reachability		We present lazy abstraction-based controller synthesis (ABCS) for continuous-time nonlinear dynamical systems against reach-avoid specifications. State-of-the-art multi-layered ABCS pre-computes multiple finite-state abstractions of different coarseness and applies reactive synthesis to the coarsest abstraction whenever feasible, but adaptively considers finer abstractions when necessary. Our new algorithm improves this technique by constructing abstractions lazily on demand. Our insight is that the abstract transition relation only needs to be locally computed for a small set of frontier states of the courseness currently required by the synthesis algorithm. We show that lazy ABCS can significantly outperform previous multi-layered ABCS algorithms: on a standard benchmark, lazy ABCS was more than 4 times faster.	algorithm;benchmark (computing);dynamical system;lazy evaluation;nonlinear system;reachability	Kyle Hsu;Rupak Majumdar;Kaushik Mallik;Anne-Kathrin Schmuck	2018	CoRR		mathematics;dynamical systems theory;mathematical optimization;control theory;small set;nonlinear system;reachability;abstraction	Logic	-9.420723227468297	29.090418194059723	51897
ed1b4f90c4c3fef086c84d196dd68ec1ef318f66	regular model checking using widening techniques	regular language;safety properties;reachable set;model checking;transitive closure;symbolic model checking	Abstract   In this paper, we consider symbolic model checking of safety properties of linear parametrized systems. Sets of configurations are represented by regular languages and actions by regular relations. Since the verification problem amounts to the computation of the reachability set, we focus on the computation of  R  ∗ (φ) for a regular relation  R  and a regular language φ. We present a technique called  regular widening  that allows, when it terminates, the computation of either the reachability set  R  ∗ (φ) of a system or the transitive closure  R  ∗  of a regular relation. We show that our method can be uniformly applied to several parametrized systems. Furthermore, we show that it is powerful enough to simulate some existing methods that compute either  R  ∗  or  R  ∗ (φ) for each  R  (resp. φ) belonging to a subclass of regular relations (resp. belonging to a subclass of regular languages).	model checking	Tayssir Touili	2001	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)00187-2	model checking;combinatorics;discrete mathematics;regular language;computer science;mathematics;programming language;generalized star height problem;transitive closure;algorithm	Logic	-9.695174752132756	23.694685327297694	51910
353a8e3b736983fc0640b5022d55694ef7451ae7	removing useless variables in cost analysis of java bytecode	java bytecode;cost equations systems;complexity;cost analysis;computer algebra system;upper bound;information flow;mobile code;program slicing	Automatic cost analysis has interesting applications in the context of verification and certification of mobile code. For instance, the code receiver can use cost information in order to decide whether to reject mobile code which has too large cost requirements in terms of computing resources (in time and/or space) or billable events (SMSs sent, bandwidth required). Existing cost analyses for a variety of languages describe the resource consumption of programs by means of Cost Equation Systems (CESs), which are similar to, but more general than recurrence equations. CESs express the cost of a program in terms of the size of its input data. In a further step, a closed form (i.e., non-recursive) solution or upper bound can sometimes be found by using existing Computer Algebra Systems (CASs), such as Maple and Mathematica. In this work, we focus on cost analysis of Java bytecode, a language which is widely used in the context of mobile code and we study the problem of identifying variables which are useless in the sense that they do not affect the execution cost and therefore can be ignored by cost analysis. We identify two classes of useless variables and propose automatic analysis techniques to detect them. The first class corresponds to stack variables that can be replaced by program variables or constant values. The second class corresponds to variables whose value is cost-irrelevant, i.e., does not affect the cost of the program. We propose an algorithm, inspired in static slicing which safely identifies cost-irrelevant variables. The benefits of eliminating useless variables are two-fold: (1) cost analysis without useless variables can be more efficient and (2) resulting CESs are more likely to be solvable by existing CASs.	algorithm;british undergraduate degree classification;call stack;code mobility;decision problem;first-class function;java bytecode;maple;program slicing;recurrence relation;recursion;relevance;requirement;wolfram mathematica	Elvira Albert;Puri Arenas;Samir Genaim;Germán Puebla;Damiano Zanardini	2008		10.1145/1363686.1363779	program slicing;complexity;real-time computing;information flow;computer science;cost–benefit analysis;artificial intelligence;theoretical computer science;operating system;software engineering;machine learning;database;upper and lower bounds;programming language;world wide web;computer security;algorithm	SE	-18.736978129836658	28.975510941550418	51914
33b2ae4d09004a88776030b1e21294c3b0a11407	measurable cones and stable, measurable functions: a model for probabilistic higher-order programming		We define a notion of stable and measurable map between cones endowed with measurability tests and show that it forms a cpo-enriched cartesian closed category. This category gives a denotational model of an extension of PCF supporting the main primitives of probabilistic functional programming, like continuous and discrete probabilistic distributions, sampling, conditioning and full recursion. We prove the soundness and adequacy of this model with respect to a call-by-name operational semantics and give some examples of its denotations.	cartesian closed category;functional programming;higher-order programming;operational semantics;programming computable functions;recursion;sampling (signal processing)	Thomas Ehrhard;Michele Pagani;Christine Tasson	2017	PACMPL	10.1145/3158147	discrete mathematics;cartesian closed category;mathematics;higher-order programming;measurable function;operational semantics;probabilistic logic;functional programming;lambda calculus;soundness	PL	-11.32933056111053	18.563045946954567	51967
b120d7cf2cbefc0efbf68d3a6de5debc9d92fd22	high-level modelling and efficient analysis of randomized protocols		Model checking is a fully automatic verification technique that allows for proving that a given model satisfies under all circumstances a given property. In this thesis we develop and evaluate techniques that concern quantitative model checking of probabilistic nondeterministic systems against linear temporal properties, i.e., calculating the (minimum or maximum) probability that a linear temporal property is satisfied by the model. In this context we discuss the design and implementation of a probabilistic modelling language for probabilistic nondeterministic systems. This language named PROBMela features parallel interleaved processes, shared and local variables, nondeterministic and probabilistic choice, loops, asynchronous and synchronous communication via channels, lossy channels, randomized assignments and atomic regions. For this language we give operational semantics in terms of Markov decision processes (MDPs). The main obstacle in model checking is the state explosion problem which means that the size of the system under consideration (i.e., the model) usually grows exponentially with the number and size of their compositional parts like processes, variables and level of reactive behaviour. In this thesis we present work concerning reduction techniques to cope with the state explosion problem. The quantitative analysis of MDPs relies on the graphtheoretic analysis of particular sub-structures, the so-called end components. We discuss some improvements of the numerical solving stage of the quantitative analysis, which exploit particular properties of end components to construct a reduced version of the MDP that is considerable easier to solve, thus weakening the impact of the state explosion. We also discuss how the concept of fairness, a concept that is very important in the context of high-level modelling of parallel systems, can seamlessly be integrated into the quantitative analysis procedure. Furthermore we turn our attention to practical aspects of partial order reduction reduction method for probabilistic systems. The ample-set method involves five conditions, the socalled ample set conditions, that fix precise circumstances for a property-preserving reduction of the outgoing edges of the MDP’s states, thus yielding an (property-wise) equivalent reduced MDP. Without further changes these conditions are, because of their partially global nature, not ready-to-use for a practical implementation. We discuss sufficient, weaker local criteria for these five conditions and report on their potential in a practical implementation. All the aspects, the language, the numerical reduction, the partial order reduction and the fairness approach mentioned above, have been implemented in the explicit, virtual-machine based quantitative linear-time model checker LiQuor for experiments.	common criteria;control flow;experiment;fairness measure;high- and low-level;local variable;lossy compression;markov chain;markov decision process;model checking;modeling language;nondeterministic algorithm;numerical analysis;operational semantics;partial order reduction;probabilistic automaton;randomized algorithm;statistical model;time complexity;virtual machine	Frank Ciesinski	2011				Logic	-11.875553436702049	26.65248162919575	52193
17047a639180e384cea1f183cf9082e7e13f021a	pragmatic quotient types in coq	coq;quotient types;formalization of mathematics	In intensional type theory, it is not always possible to form the quotient of a type by an equivalence relation. However, quotients are extremely useful when formalizing mathematics, especially in algebra. We provide a Coq library with a pragmatic approach in two complementary components. First, we provide a framework to work with quotient types in an axiomatic manner. Second, we program construction mechanisms for some specific cases where it is possible to build a quotient type. This library was helpful in implementing the types of rational fractions, multivariate polynomials, field extensions and real algebraic numbers.	algebraic equation;coq (software);intensional logic;intuitionistic type theory;polynomial;quotient type;turing completeness	Cyril Cohen	2013		10.1007/978-3-642-39634-2_17	quotient algebra;discrete mathematics;pure mathematics;mathematics;algebra	PL	-14.178704558866428	18.88370293040836	52221
39c1beaad03d20cf40c673b667acbbaecfbc0b72	certified rule labeling	term rewriting confluence decreasing diagrams certification;004	The rule labeling heuristic aims to establish confluence of (left-)linear term rewrite systems via decreasing diagrams. We present a formalization of a confluence criterion based on the interplay of relative termination and the rule labeling in the theorem prover Isabelle. Moreover, we report on the integration of this result into the certifier CeTA, facilitating the checking of confluence certificates based on decreasing diagrams for the first time. The power of the method is illustrated by an experimental evaluation on a (standard) collection of confluence problems. 1998 ACM Subject Classification F.2 Analysis of Algorithms and Problem Complexity, F.4 Mathematical Logic and Formal Languages	algorithm;analysis of algorithms;automated theorem proving;confluence;diagram;heuristic;isabelle;rewrite (programming)	Julian Nagele;Harald Zankl	2015		10.4230/LIPIcs.RTA.2015.269	discrete mathematics;computer science;theoretical computer science;mathematics;algorithm	Logic	-15.026136570681754	22.410804262923584	52269
091df44065e455a0181804464d21841f24f3bb13	on the construction of pullbacks for safe petri nets	distributed system;synchronous transmission;systeme reparti;egalisation;transmission synchrone;red petri;simultaneidad informatica;equalization;automaton;automata;concurrency;igualador;sistema repartido;concurrent systems;igualacion;automate;equalizer;transmision sincronica;information system;petri net;simultaneite informatique;systeme information;reseau petri;egaliseur;sistema informacion	The product of safe Petri nets is a well known operation : it generalizes to concurrent systems the usual synchronous product of automata. In this paper, we consider a more general way of combining nets, called a pullback. The pullback operation generalizes the product to nets which interact both by synchronized transitions and/or by a shared sub-net (i.e. shared places and transitions). To obtain all pullbacks, we actually show that all equalizers can be defined in the category of safe nets. Combined to the known existence of products in this category, this gives more than what we need : we actually obtain that all small limits exist, i.e. that safe nets form a complete category.	automata theory;concurrency (computer science);formal language;petri net;trellis quantization;unfolding (dsp implementation)	Eric Fabre	2006		10.1007/11767589_10	real-time computing;computer science;artificial intelligence;automaton;algorithm	Logic	-8.007801039216488	22.949974329422247	52292
a33324c15879031cbcabbd9a57cd55c871449eae	model checking for symbolic-heap separation logic with inductive predicates	runtime verification;ucl;research outputs;inductive definitions;logics;separation logic;discovery;theses;conference proceedings;complexity;research publications;digital web resources;model checking;program testing;ucl discovery;open access;ucl library;book chapters;open access repository;qa 9 formal systems;ucl research	We investigate the *model checking* problem for symbolic-heap separation logic with user-defined inductive predicates, i.e., the problem of checking that a given stack-heap memory state satisfies a given formula in this language, as arises e.g. in software testing or runtime verification. First, we show that the problem is *decidable*; specifically, we present a bottom-up fixed point algorithm that decides the problem and runs in exponential time in the size of the problem instance. Second, we show that, while model checking for the full language is EXPTIME-complete, the problem becomes NP-complete or PTIME-solvable when we impose natural syntactic restrictions on the schemata defining the inductive predicates. We additionally present NP and PTIME algorithms for these restricted fragments. Finally, we report on the experimental performance of our procedures on a variety of specifications extracted from programs, exercising multiple combinations of syntactic restrictions.	algorithm;boolean satisfiability problem;bottom-up parsing;decision problem;exptime;expectation propagation;fixed point (mathematics);fixed-point iteration;inductive logic programming;inductive reasoning;memory management;model checking;np-completeness;online and offline;p (complexity);polynomial;principle of good enough;programmer;runtime verification;scalability;separation logic;software testing;time complexity;unit testing	James Brotherston;Nikos Gorogiannis;Max I. Kanovich;Reuben N. S. Rowe	2016		10.1145/2837614.2837621	model checking;complexity;separation logic;computer science;theoretical computer science;runtime verification;programming language;algorithm	PL	-15.656054870170294	22.909635906433756	52459
0d0691ac0f8b6632ca43d06817a90b356ebfd2a4	on probabilistic automata in continuous time	continuous time;stochastic process;concurrent computing;stochastic petri net;weak bisimulation semantics markov processes nondeterminism discrete time continuous time process algebra;bisimulation;discrete time;interactive markov chains;spectrum;semantic networks;compositional behavioural model;nondeterminism;probabilistic automata;automata;generalised stochastic petri net semantics;markov process;weak bisimulation semantics;bisimulation probabilistic automata continuous time compositional behavioural model interactive markov chains generalised stochastic petri net semantics;markov processes;probabilistic logic;petri nets;markov processes probabilistic logic automata concurrent computing delay petri nets;semantic networks bisimulation equivalence markov processes petri nets probabilistic automata;process algebra;bisimulation equivalence;markov chain	We develop a compositional behavioural model that integrates a variation of probabilistic automata into a conservative extension of interactive Markov chains. The model is rich enough to embody the semantics of generalised stochastic Petri nets. We define strong and weak bisimulations and discuss their compositionality properties. Weak bisimulation is partly oblivious to the probabilistic branching structure, in order to reflect some natural equalities in this spectrum of models. As a result, the standard way to associate a stochastic process to a generalised stochastic Petri net can be proven sound with respect to weak bisimulation.	automata theory;bisimulation;markov chain;probabilistic automaton;stochastic petri net;stochastic process	Christian Eisentraut;Holger Hermanns;Lijun Zhang	2010	2010 25th Annual IEEE Symposium on Logic in Computer Science	10.1109/LICS.2010.41	stochastic process;combinatorics;discrete mathematics;stochastic petri net;concurrent computing;computer science;mathematics;markov process;algorithm	Logic	-9.961224311261942	21.862627770735337	52468
aa52fc8d7df97fb44dc2a7d90859a7873a942e07	formal verification of an ssa-based middle-end for compcert	single static assignment;computacion informatica;mechanized proof;ciencias basicas y experimentales;compiler verification;grupo a	CompCert is a formally verified compiler that generates compact and efficient code for a large subset of the C language. However, CompCert foregoes using SSA, an intermediate representation employed by many compilers that enables writing simpler, faster optimizers. In fact, it has remained an open problem to verify formally an SSA-based compiler. We report on a formally verified, SSA-based middle-end for CompCert. In addition to providing a formally verified SSA-based middle-end, we address two problems raised by Leroy in [2009]: giving an intuitive formal semantics to SSA, and leveraging its global properties to reason locally about program optimizations.	compcert;compiler;formal verification;intermediate representation;program optimization;regular expression;semantics (computer science);static single assignment form	Gilles Barthe;Delphine Demange;David Pichardie	2014	ACM Trans. Program. Lang. Syst.	10.1145/2579080	parallel computing;computer science;programming language;algorithm	PL	-19.035665610652877	26.95882828999495	52498
d1e4180bb00b66d38b523df5b0a8ae0ed77f995e	a recursive shortcut for cegar: application to the modal logic k satisfiability problem		Counter-Example-Guided Abstraction Refinement (CEGAR) has been very successful in model checking. Since then, it has been applied to many different problems. It is especially proved to be a highly successful practical approach for solving the PSPACE complete QBF problem. In this paper, we propose a new CEGAR-like approach for tackling PSPACE complete problems that we call RECAR (Recursive Explore and Check Abstraction Refinement). We show that this generic approach is sound and complete. Then we propose a specific implementation of the RECAR approach to solve the modal logic K satisfiability problem. We implemented both CEGAR and RECAR approaches for the modal logic K satisfiability problem within the solver MoSaiC. We compared experimentally those approaches to the state-of-the-art solvers for that problem. The RECAR approach outperforms the CEGAR one for that problem and also compares favorably against the state-of-the-art on the benchmarks considered.	approximation;boolean satisfiability problem;classical modal logic;conjunctive normal form;decision problem;experiment;keyboard shortcut;maximum satisfiability problem;model checking;pspace-complete;polynomial hierarchy;recursion (computer science);refinement (computing);satisfiability modulo theories;solver;true quantified boolean formula	Jean-Marie Lagniez;Daniel Le Berre;Tiago de Lima;Valentin Montmirail	2017		10.24963/ijcai.2017/94	discrete mathematics;modal logic;recursion;algorithm;boolean satisfiability problem;mathematics	AI	-15.18858219629927	24.042821340266265	52570
23c0f8eeb947f451c1061637058bc822a207e9a7	slicing of timed automata with discrete data	timed systems;discrete data;systemy czasowe;plastrowanie programow;timed automata;automaty czasowe;program slicing;static analysis;analiza statyczna	The paper proposes how to use static analysis to extract an abstract model of a system. The method uses techniques of program slicing to examine syntax of a system modeled as a set of timed automata with discrete data, a common input formalism of model checkers dealing with time. The method is property driven. The abstraction is exact with respect to all properties expressed in the temporal logic CTL X*.	automata theory;ctl*;discrete mathematics;model checking;program slicing;semantics (computer science);static program analysis;temporal logic;timed automaton	Agata Janowska;Pawel Janowski	2006	Fundam. Inform.		program slicing;real-time computing;computer science;theoretical computer science;timed automaton;static analysis;algorithm	Logic	-17.880830502424523	27.68054513919393	52623
4d3e79e8836083330d59d90abff9bb6163c0f0d1	behavior composition as fully observable non-deterministic planning	model checking techniques;behavior module;current technology;behavior composition;composition problem;non trivial;non deterministic planning;automatic synthesis;planning problem	The behavior composition problem involves the automatic synthesis of a controller able to “realize” (i.e., implement) a target behavior module by suitably coordinating a collection of partially controllable available behaviors. In this paper, we show that the existence of a composition solution amounts to finding a strong cyclic plan for a special nondeterministic planning problem, thus establishing the formal link between the two synthesis tasks. Importantly, our results support the use of non-deterministic planing systems for solving composition problems in an off-the-shelf manner. We then empirically evaluate three state-of-the-art synthesis systems (a domain-independent automated planner and two game solvers based on model checking techniques) on various non-trivial composition instances. Our experiments show that while behavior composition is EXPTIME-complete, the current technology is already able to handle instances of significant complexity. Our work is, as far as we know, the first serious experimental work on behavior composition.	automated planning and scheduling;automatic control;exptime;experiment;model checking;non-deterministic turing machine;observable	Miquel Ramírez;Nitin Yadav;Sebastian Sardiña	2013			mathematical optimization;simulation;artificial intelligence;machine learning;mathematics;algorithm	AI	-14.015977031975524	26.6451534468005	52625
8f0257c3a5578d4c4938ae8f853ec7e73c42844a	mixed delay and threshold voters in critical real-time systems	systeme temps reel;distributed system;topology;theorie echantillonnage;teoria muestreo;systeme reparti;signal continu;theoretical framework;sistema hibrido;senal continua;systeme critique;logiciel a securite critique;topologie;sistema complejo;sistema reactivo;topologia;control system;fault tolerant system;sistema repartido;critical system;systeme complexe;complex system;safety critical software;retard;sistema tolerando faltas;hybrid system;reactive system;systeme reactif;systeme tolerant les pannes;real time system;sistema tiempo real;continuous signal;retraso;systeme hybride;real time systems;sampling theory	This paper addresses the question of extending the usual approximation and sampling theory of continuous signals and systems to those encompassing discontinuities, such as found in modern complex control systems (mode switches for instance). We provide a topological framework derived from the Skorokhod distance to deal with those cases in a uniform manner. We show how this theoretical framework can be used for voting on hybrid signals in critical real-time systems.	real-time transcription	Chiheb Kossentini;Paul Caspi	2004		10.1007/978-3-540-30206-3_4	fault tolerance;real-time operating system;reactive system;computer science;control system;artificial intelligence;continuous signal;algorithm;hybrid system	Embedded	-8.659575677213047	27.171887414991986	52749
22ea8227bd9fed2b51da78292ec3ab288a874b6a	accelerating parametric probabilistic verification		We present a novel method for computing reachability probabilities of parametric discrete-time Markov chains whose transition probabilities are fractions of polynomials over a set of parameters. Our algorithm is based on two key ingredients: a graph decomposition into strongly connected subgraphs combined with a novel factorization strategy for polynomials. Experimental evaluations show that these approaches can lead to a speed-up of up to several orders of magnitude in comparison to existing approaches.	algorithm;bisimulation;factorization of polynomials;interval arithmetic;local consistency;markov chain;parameter (computer programming);polynomial;reachability;scalability;software propagation;strongly connected component	Nils Jansen;Florian Corzilius;Matthias Volk;Ralf Wimmer;Erika Ábrahám;Joost-Pieter Katoen;Bernd Becker	2014		10.1007/978-3-319-10696-0_31	mathematical optimization	Logic	-11.21297684821168	28.023514933975484	52843
7d3f0b9484ddee9553667b581fe8a499e6a65a41	verification of message passing concurrent systems		This dissertation is concerned with the development of fully-automatic methods of veri cation, for message-passing based concurrent systems. In the rst part of the thesis we focus on Erlang, a dynamically typed, higher-order functional language with pattern-matching algebraic data types extended with asynchronous message-passing. We de ne a sound parametric controlow analysis for Erlang, which we use to bootstrap the construction of an abstract model that we call Actor Communicating System (ACS). ACS are given semantics by means of Vector Addition Systems (VAS), which have rich decidable properties. We exploit VAS model checking algorithms to prove properties of Erlang programs such as unreachability of error states, mutual exclusion, or bounds on mailboxes. To assess the approach empirically, we constructed Soter, a prototype implementation of the veri cation method, thereby obtaining the rst fully-automatic, in nite-state model checker for a core concurrent fragment of Erlang. The second part of the thesis addresses one of the major sources of imprecision in the ACS abstraction: process identities. To study the problem of algorithmically verifying models where process identities are accurately represented we turn to the π-calculus, a process algebra based around the notion of name and mobility. The full π-calculus is Turing-powerful so we focus on the depth-bounded fragment introduced by Roland Meyer, which enjoys decidability of some veri cation problems. The main obstacle in using depth-bounded terms as a target abstract model, is that depth-boundedness of arbitrary π-terms is undecidable. We therefore consider the problem of identifying a fragment of depth-bounded π-calculus for which membership is decidable. We de ne the rst such fragment by means of a novel type system for the π-calculus. Typable terms are ensured to be depth-bounded. Both type-checking and type inference are shown to be decidable. The constructions are based on the novel notion of T-compatibility, which imposes a hierarchy between names. The type system’s main goal is proving that this hierarchy is preserved under reduction, even in the presence of unbounded name creation and mobility.		Emanuele D'Osualdo	2015			model checking;algebraic data type;erlang (programming language);discrete mathematics;type inference;concurrency;decidability;functional programming;process calculus;mathematics	Logic	-17.547009061529764	26.270633191829777	52927
0a1d78ace0b1d0bcfcd50f0a6602743cba300619	two simulations about dpll(t)	polarities and focusing;sequent calculus;smt	In this paper we relate different formulations of the DPLL(T ) procedure. The first formulation is that of [NOT06] based on a system of rewrite rules, which we denote DPLL(T ). The second formulation is an inference system of [Tin02], which we denote LKDPLL(T ). The third formulation is the application of a standard proof-search mechanism in a sequent calculus LK(T ) introduced here. We formalise an encoding from DPLL(T ) to LKDPLL(T ) that was, to our knowledge, never explicitly given and, in the case where DPLL(T ) is extended with backjumping and Lemma learning, never even implicitly given. We also formalise an encoding from LKDPLL(T ) to LK (T ), building on Ivan Gazeau’s previous work: we extend his work in that we handle the “-modulo-Theory” aspect of SATmodulo-theory, by extending the sequent calculus to allow calls to a theory solver (seen as a blackbox). We also extend his work in that we handle advanced features of DPLL such as backjumping and Lemma learning, etc. Finally, we refine the approach by starting to formalise quantitative aspects of the simulations: the complexity is preserved (nunber of steps to build complete proofs). Other aspects remain to be formalised (non-determinism of the search / width of search space).	backjumping;dpll algorithm;inference engine;nondeterministic algorithm;rewriting;sequent calculus;simulation;solver	Mahfuza Farooque;Stéphane Lengrand;Assia Mahboubi	2012	CoRR		discrete mathematics;computer science;artificial intelligence;mathematics;programming language;sequent calculus;algorithm	Logic	-15.171450047150541	23.034178947769746	52994
1d9eee44415145c53d8a0b91f0918001fe820ec4	representable disjoint np-pairs	teoria demonstracion;theorie preuve;proof theory;teoria sistema;program verification;arithmetique;analisis programa;verificacion programa;aritmetica;systems theory;arithmetics;informatique theorique;theorie systeme;program analysis;analyse programme;verification programme;computer theory;informatica teorica	We investigate the class of disjoint NP-pairs under different reductions. The structure of this class is intimately linked to the simulation order of propositional proof systems, and we make use of the relationship between propositional proof systems and theories of bounded arithmetic as the main tool of our analysis. Specifically we exhibit a pair which is complete under strong reductions for all disjoint NP-pairs representable in a theory. We use these pairs to explain the simulation order of NP-pairs under these reductions. As corollaries we also get simplified proofs of results obtained earlier in [3] and [5].		Olaf Beyersdorff	2004	Electronic Colloquium on Computational Complexity (ECCC)	10.1007/978-3-540-30538-5_11	program analysis;computer science;pure mathematics;proof theory;mathematics;programming language;systems theory;algorithm	Theory	-9.155506501129484	20.142039944017096	53047
6d18dedd9dcff7490825c4f96fff6f05521f4d94	towards a maude tool for model checking temporal graph properties	model checking	We present our prototypical tool for the verification of graph transfor mation systems. The major novelty of our tool is that it provides a model checker f or temporal graph properties based on counterpart semantics for quantifie d μ-calculi. Our tool can be considered as an instantiation of our approach to counter part semantics which allows for a neat handling of creation, deletion and merging in syste ms with dynamic structure. Our implementation is based on the object-based machiner y of Maude, which provides the basics to deal with attributed graphs. Graph t nsformation systems are specified with term rewrite rules. The model checker evalu ates logical formulae of second-order modal μ-calculus in the automatically generated Counterpart Model (a sort of unfolded graph transition system) of the g raph transformation system under study. The result of evaluating a formula is a set of ass ignments for each state, associating node variables to actual nodes.	confluence;critical pair (logic);description logic;fixed point (mathematics);graph (discrete mathematics);graph property;graph rewriting;mathematical optimization;maude system;modal logic;modal μ-calculus;model checking;mutual exclusion;object-based language;process calculus;reachability;rewrite (programming);rewriting;temporal logic;transition system;universal instantiation	Andrea Vandin;Alberto Lluch-Lafuente	2011	ECEASST	10.14279/tuj.eceasst.41.640	model checking;computer science;theoretical computer science;programming language;algorithm	Logic	-12.192434571446435	23.286250881711357	53104
7817334557bb35466698ca31e63709d80f4d4420	sequential versus concurrent languages of labeled conflict-free petri nets	observability;petri nets supervisory control controllability observability polynomials concurrent computing;concurrent language;supervisory control;concurrent computing;normality labeled conflict free petri nets sequential languages concur rent languages polynomial time supervisory control controllability observability;controllability;normality;supervisory control theory;polynomials;labeled conflict free petri nets;computational complexity;polynomial time;discrete event systems;computational complexity discrete event systems petri nets observability controllability;concur rent languages;petri nets;petri net;sequential languages	For structurally deterministic labeled conflict-freePetri nets (PNs), we show that two PNs have identical sequential languages if and only if their concurrent languagesare identical as well, and whether a given labeled conflict-free PN is structurally deterministic or not can be checked in polynomial time. We also investigate a number of language-related problems in supervisory control theory for this class of PNs. As it turns out, the properties of controllability, observability, and normality in the sequential framework coincide with that in the concurrent framework.	control theory;petri net;time complexity	Hsu-Chun Yen	2002	IEEE Trans. Automat. Contr.	10.1109/TAC.2002.800664	discrete mathematics;concurrent computing;control theory;mathematics;distributed computing;petri net	Theory	-6.339515102658064	26.403154143950932	53432
f540ca7c8c31a093c1e7f87e0c80be8be57c3880	an algorithm based on structural analysis for model-based fault diagnosis	redundancy;complete matching;model based fault diagnosis;structural analysis;fault isolation;structure analysis;fault diagnosis	This paper presents a diagnosis system in which, an algorithm automatically finds all minimal structurally overdetermined (MSO) sets in a structural model of a system. It combines a first set of MSO sets to get the additional MSO sets, which were obtained after determining a complete matching between equations and unknown variables. Finding the complete set is useful for the diagnosis task increasing the fault isolability due to the fact that it can provide different signatures to each fault. The efficiency of the algorithm for finding all MSO sets is illustrated using the DAMADICS benchmark, which is a pneumatically-actuated control valve.	algorithm;structural analysis	Esteban R. Gelso;Sandra M. Castillo;Joaquim Armengol	2008		10.3233/978-1-58603-925-7-138	real-time computing;machine learning;mathematics;algorithm	Robotics	-7.159957731162622	28.286920672632586	53469
0d481aa3d4bfee05f37dda2565c22e20620c6d72	on the completeness of the euations for the kleene star in bisimulation	term rewriting	A classical result from Redko says that there does not exist a complete nite equational axiomatization for the Kleene star modulo trace equivalence Fokkink and Zantema showed that there does exist a complete nite equational axiom atization for the Kleene star up to strong bisimulation equivalence Their proof is based on a sophisticated term rewriting analysis In this paper we present a much simpler and shorter completeness proof Fur thermore we show that the three equations for the Kleene star are all essential for this completeness result	axiomatic system;bisimulation;kleene star;modulo operation;rewriting;turing completeness	Wan Fokkink	1996		10.1007/BFb0014315	kleene star;computer science;algorithm	Logic	-10.80390195443944	18.731310353823865	53620
bf18e9a1fec7941968fee18bb81b2da2cba3458c	automatic discovery and exploitation of promising subproblems for tabulation		The performance of a constraint model can often be improved by converting a subproblem into a single table constraint. In this paper we study heuristics for identifying promising subproblems. We propose a small set of heuristics to identify common cases such as expressions that will propagate weakly. The process of discovering promising subproblems and tabulating them is entirely automated in the tool Savile Row. A cache is implemented to avoid tabulating equivalent subproblems many times. We give a simple algorithm to generate table constraints directly from a constraint expression in Savile Row. We demonstrate good performance on the benchmark problems used in earlier work on tabulation, and also for several new problem classes.	algorithm;benchmark (computing);constraint programming;expectation propagation;heuristic (computer science);table (information)	Ozgur Akgun;Ian P. Gent;Christopher Jefferson;Ian Miguel;Peter Nightingale;András Z. Salamon	2018		10.1007/978-3-319-98334-9_1	mathematical optimization;small set;cache;table (information);simple algorithm;heuristics;expression (mathematics);computer science	AI	-19.027961955647353	30.201220056187765	53759
7349b993c1e51a10408e8afafaee8efc90b0e4aa	using nested logic programs for answer set programming	graph transformation;directed graph;answer set programming	We present a general method to improve computation of answer sets by analyzing structural properties of normal logic programs. Therefore we use labeled directed graphs associated to normal programs, which can be utilized to compute answer sets. The basic idea is to detect special subgraphs of those graphs corresponding to structural properties of normal programs and transform them into simpler but equivalent subgraphs by applying graph transformations. It turns out that there is no characterization for these graph transformations in terms of normal logic programs. Surprisingly, nested logic programs provide a semantics for the investigated transformations. In order to demonstrate its practical usefulness, we have implemented our approach in the noMoRe system.	answer set programming;computation;directed graph;graph rewriting;logic programming;stable model semantics	Thomas Linke	2003			inductive programming;discrete mathematics;graph (abstract data type);functional logic programming;answer set programming;directed graph;graph rewriting;algorithm;stable model semantics;logic programming;mathematics	AI	-15.569454942428479	22.123566447985898	53776
5e1f408bd72a955a892225718fac3856d3b07785	ensuring consistency of conditional graph rewriting - a constructive approach	satisfiability;graph rewriting;graph grammar;safety critical system	Consistency conditions describe basic properties of graphs as e g the existence or uniqueness of certain elements A graph grammar is consistent if the start graph satis es the consistency condition and the rules preserve this property We propose a general construction that transforms global consistency conditions into precondi tions for individual rules A so constructed rule is applicable to a consistent graph if and only if the derived graph is consistent too The relevance of this result is motivated by an example speci cation of a safety critical system that is a round about	critical system;graph rewriting;line graph;relevance	Reiko Heckel;Annika Wagner	1995	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80188-4	combinatorics;discrete mathematics;null graph;graph property;computer science;regular graph;clique-width;forbidden graph characterization;mathematics;voltage graph;distance-hereditary graph;graph;critical graph;vertex-transitive graph;complement graph;algorithm;strength of a graph;graph rewriting;satisfiability	ML	-9.07411718564256	20.933923521400956	53821
ceca7629b243873cadbb1349aec7a87530a17334	order-sorted equality enrichments modulo axioms	algebraic specifications;text;abstract data types;maude;equality predicate;rewriting logic;initial algebra semantics;order sorted equational logic modulo axioms;equality enrichment	We make the addition of equationally defined equality predicates effective and automatic by means of a transformation.The transformation is constructive and valid for a wide class of equational specifications.All the expected good properties of the input theory are preserved by the transformation.The transformation is implemented in Maude and integrated into Maude formal tools. Built-in equality and inequality predicates based on comparison of canonical forms in algebraic specifications are frequently used because they are handy and efficient. However, their use places algebraic specifications with initial algebra semantics beyond the pale of theorem proving tools based, for example, on explicit or inductionless induction techniques, and of other formal tools for checking key properties such as confluence, termination, and sufficient completeness. Such specifications would instead be amenable to formal analysis if an equationally-defined equality predicate enriching the algebraic data types were to be added to them. Furthermore, having an equationally-defined equality predicate is very useful in its own right, particularly in inductive theorem proving. Is it possible to effectively define a theory transformation E ? E ~ that extends an algebraic specification E to a specification E ~ having an equationally-defined equality predicate? This paper answers this question in the affirmative for a broad class of order-sorted conditional specifications E that are sort-decreasing, ground confluent, and operationally terminating modulo axioms B and have a subsignature of constructors. The axioms B can consist of associativity, or commutativity, or associativity-commutativity axioms, so that the constructors are free modulo B. We prove that the transformation E ? E ~ preserves all the just-mentioned properties of E . The transformation has been automated in Maude using reflection and is used as a component in many Maude formal tools.	modulo operation	Raúl Gutiérrez;José Meseguer;Camilo Rocha	2015	Sci. Comput. Program.	10.1016/j.scico.2014.07.003	rewriting;computer science;programming language;abstract data type;algorithm	Logic	-15.060980022354867	18.684078472541298	53906
2d9abfa10deb6f8e29af0581757655f2f86b109c	a survey of markovian behavioral equivalences	complete axiomatization	Markovian behavioral equivalences are a means to relate and manipulate the formal descriptions of systems with an underlying CTMC semantics. There are three fundamental approaches to their definition: bisimilarity, testing, and trace. In this paper we survey the major results appeared in the literature about Markovian bisimilarity, Markovian testing equivalence, and Markovian trace equivalence. The objective is to compare these equivalences with respect to a number of criteria such as their discriminating power, the exactness of the CTMC-level aggregations they induce, the achievement of the congruence property, the existence of sound and complete axiomatizations, the existence of logical characterizations, and the existence of efficient verification algorithms.	algorithm;bisimulation;congruence of squares;markov chain;turing completeness	Marco Bernardo	2007		10.1007/978-3-540-72522-0_5	computer science	Logic	-10.2845904401191	21.464499097319234	53949
487521b937a2fca242661f631f186fe36e979f90	visual multiset rewriting: applications to diagram parsing and reasoning	formal reasoning;finite state automata;petri net;linear logic	Diagrammatic notations, such as Venn diagrams, Petri-Nets and finite state automata, are in common use in mathematics and computer science. While the semantic domain of such systems is usually well formalized, the visual notation itself seldom is, so that they cannot be used as valid devices of formal reasoning. A complete formalization of such notations requires the construction of diagram systems with rigorously defined syntax and semantics. We discuss how diagram specification can be interpreted as multiset rewriting and, based on this, how it can be formalized in linear logic. We discuss the power of our approach through an illustration of its possible extension with reflective capabilities to manage negative conditions, and through the identification of a class of diagrammatic transformations which can be directly expressed in our framework.	diagram;parsing;rewriting	Paolo Bottoni;Bernd Meyer;Francesco Parisi-Presicce	2000		10.1007/3-540-45523-X_3	discrete mathematics;theoretical computer science;mathematics;algorithm	AI	-13.424693763543283	19.469534740602068	54039
3ce3fcae470ec465dee05aa793b5affa29bea3f5	supervisor design to enforce production ratio and absence of deadlock in automated manufacturing systems	system recovery petri nets manufacturing processes mathematical model synchronization;manufacturing systems;automated manufacturing systems;finite shared resources;supervisory control;marked graph;petri net class;supervisor design;production ratio;production control manufacturing systems petri nets;manufacturing processes;discrete event system;system recovery;production control;design method;synchronization;ratio control;automated manufacturing systems amss;discrete event systems;mathematical model;supervisory control automated manufacturing systems amss discrete event systems marked graph petri nets;petri nets;production planners;petri net;ratio enforced weighted augmented marked graphs;manufacturing system;production ratio supervisor design automated manufacturing systems petri net class ratio enforced weighted augmented marked graphs ratio control production planners finite shared resources	This paper proposes a new Petri net class, namely, Ratio-enforced weighted Augmented Marked Graphs (RAMGs), and solves ratio control and liveness-enforcing supervision problems for automated manufacturing systems. RAMGs can ensure the required product ratios as demanded by production planners. Since the deadlock of such a system can be attributed to improper acquisition of finite shared resources, a supervisor is introduced such that they are properly allocated. This paper proves that ratio and supervisory controllers for an RAMG can be separately designed. Their design methods are presented. Examples are given to illustrate them.	amplitude modulation signalling system;automation;concurrent computing;deadlock;job stream;liveness;marked graph;petri net;rework (electronics);routing;throughput;vhdl-ams;verilog-ams	Hesuan Hu;Mengchu Zhou;Zhiwu Li	2011	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2010.2058101	real-time computing;computer science;petri net	Robotics	-6.101972444509999	29.496937312501124	54650
cb0d6c443312ceb70ce2f80fe3315f9ff54ea03b	a verification system for concurrent programs based on the boyer-moore prover	modelizacion;regle inference;logica temporal;semantica formal;temporal logic;concurrent program;formal semantics;program verification;theorem proving;semantique formelle;inference rule;modelisation;demonstration theoreme;theorem prover;verificacion programa;computational logic;programa competidor;concurrent programs;binomial coefficient;demostracion teorema;verification programme;modeling;correctness proof;logique temporelle;programme concurrent;regla inferencia	We describe a mechanical proof system for concurrent programs, based on a formalization of the temporal framework of Manna and Pnueli as an extension of the computational logic of Boyer and Moore. The system provides a natural representation of specifications of concurrent programs as temporal logic formulas, which are automatically translated into terms that are subject to verification by the Boyer-Moore prover. Several specialized derived rules of inference are introduced to the prover in order to facilitate the verification of invariance (safety) and eventuality (liveness) properties. The utility of the system is illustrated by a correctness proof for a two-process program that computes binomial coefficients.	coefficient;computational logic;concurrent computing;correctness (computer science);liveness;proof calculus;temporal logic	David M. Russinoff	1992	Formal Aspects of Computing	10.1007/BF03180564	discrete mathematics;computer science;theoretical computer science;mathematics;automated theorem proving;algorithm	Logic	-14.185937351676563	22.04317012739604	54697
6c3c8643012b941f2f34d5c51ef97f2596939d6e	control of a class of hybrid systems application to a water supply system	water supply automata theory control system synthesis reachability analysis state space methods;state space methods;reachable state space determination hybrid system control water supply system control synthesis approach rectangular hybrid automata energy cost control specification;forward analysis;water supply;controllable and uncontrollable events control synthesis rectangular hybrid automaton forward analysis reacahble state space water supply system;control system synthesis;controllable and uncontrollable events;hybrid system control;control specification;reachable state space determination;reacahble state space;automata theory;energy cost;control synthesis approach;water supply system;rectangular hybrid automaton;read only memory;reachability analysis;control synthesis;rectangular hybrid automata	A control synthesis approach is proposed for a class of hybrid systems modeled with rectangular hybrid automata. This control is applied in a water supply system intended for consumption, in order to guarantee a certain level of water and a low energy cost. The basic steps of the approach consist in the addition of the desired control specifications to the system's model, then the determination of the reachable state spaces of the constrained automaton and finally the computation of the clock intervals that respect the specifications in a maximal permissive way.	automata theory;computation;hybrid automaton;hybrid system;maximal set	Sonia Batis;Hassane Alla	2013	2013 10th IEEE INTERNATIONAL CONFERENCE ON NETWORKING, SENSING AND CONTROL (ICNSC)	10.1109/ICNSC.2013.6548712	control engineering;real-time computing;computer science;engineering;operating system;automata theory;control theory;water supply;read-only memory;hybrid system	Robotics	-6.352481453960442	28.524797032861475	54739
1cb29c695f947e6a2fa520a83d08bc1751a31590	verification rules for exception handling in eiffel	method specification;total correctness;analogous manner;loop structure;present verification rule;stated postcondition;correctness reasoning;exception handling;verification rule;normal loop;eiffel exception mechanism	The Eiffel exception mechanism supports two methodological aspects. First, a method specification by a pre- and postcondition also determines when the method exits exceptionally, namely when the stated postcondition cannot be satisfied. Secondly, the rescue and retry statements combine catching an exception with a loop structure, thus requiring a dedicated form of correctness reasoning. We present verification rules for total correctness that take these two aspects into account. The rules handle normal loops and retry loop structures in an analogous manner. They also allow the Eiffel's mechanism to be slightly generalized. The verification rules are derived from a definition of statements by higher-order predicate transformers and have been checked with a theorem prover.	eiffel;exception handling	Emil Sekerinski;Tian Zhang	2012		10.1007/978-3-642-33296-8_14	postcondition;computer science;database;programming language;algorithm	HPC	-17.161378369796005	22.05182265677575	54807
3193e0f93ce417012aa4a40036d12d03ff7e62b2	automated rare event simulation for stochastic petri nets	standard simulation;rare event simulation;required information;automated rare event simulation;human insight;full state space;state space;stochastic petri net;formal algorithm;case study;high-level spn-description;automated approach	We introduce an automated approach for applying rare event simulation to stochastic Petri net (SPN) models of highly reliable systems. Rare event simulation can be much faster than standard simulation because it is able to exploit information about the typical behaviour of the system. Previously, such information came from heuristics, human insight, or analysis on the full state space. We present a formal algorithm that obtains the required information from the high-level SPNdescription, without generating the full state space. Essentially, our algorithm reduces the state space of the model into a (much smaller) graph in which each node represents a set of states for which the most likely path to failure has the same form. We empirically demonstrate the efficiency of the method with two case studies.	algorithm;extreme value theory;high- and low-level;simulation;state space;stochastic petri net;substitution-permutation network	Daniël Reijsbergen;Pieter-Tjerk de Boer;Werner R. W. Scheinhardt;Boudewijn R. Haverkort	2013		10.1007/978-3-642-40196-1_31	simulation;computer science;theoretical computer science;distributed computing	AI	-10.904200736341311	28.559139942956467	54926
429c172a984449bd767b026f643cc703d61e450e	pushdown automata on infinite trees and omega-kleene closure of context-free tree sets	context free;pushdown automata	This theory has been extended to infinite trees ([II], [16], [17], [19], [22], [23], [25], [26], [27], [34]). In [23], M. O. Rabin proves that a set of infinite trees is weakly definable in $2S iff the set and its complement are recognizable by Bl~chi tree automata. D. E. Muller et al. [16] introduce weak alternating tree automata and characterize the weak monadic second order theory of the tree. In [20], M. Nivat and A. Saoudi characterize rational logic programs and prove that the equivalence of two rational logic programs is decidable, where two programs are equivalent if they compute the same set of tree~(i.e, copmutations). In another example M. Y. Vardi and P. Wolper [33] construct decision procedure for temporal logic of fair concurrent programs using automata on infinite trees.	automata theory;deterministic pushdown automaton;kleene star;omega;stack (abstract data type);tree (set theory)	Ahmed Saoudi	1989		10.1007/3-540-51486-4_91	deterministic pushdown automaton;combinatorics;discrete mathematics;deterministic context-free grammar;computer science;nested word;mathematics;context-free language;pushdown automaton;embedded pushdown automaton;algorithm	Logic	-6.573843515998178	21.455686555982687	55128
ef1469039a259bb356eeedeb532de2b09acd6378	achieving distributed control through model checking	prioritized systems;disjunctive control;knowledge;model checking;distributed control	We apply model checking of knowledge properties to the design of distributed controllers that enforce global constraints on concurrent systems. We calculate when processes can decide, autonomously, to take or block an action so that the global constraint will not be violated. When the separate processes cannot make this decision alone, it may be possible to temporarily coordinate several processes in order to achieve sufficient knowledge jointly and make combined decisions. Since the overhead induced by such coordinations is important, we strive to minimize their number, again using model checking. We show how this framework is applied to the design of controllers that guarantee a priority policy	autonomous robot;blocking (computing);concurrency (computer science);content-control software;disjunctive normal form;distributed computing;distributed control system;electronic component;game theory;global serializability;interaction;linear programming;model checking;overhead (computing);regular language;scheduling (computing);simplex algorithm;undecidable problem	Susanne Graf;Doron A. Peled;Sophie Quinton	2012	Formal Methods in System Design	10.1007/s10703-011-0138-9	model checking;real-time computing;computer science;theoretical computer science;distributed computing;knowledge;programming language;algorithm	Logic	-6.726651036840578	29.621428196459274	55193
16a18e30456bcec4003ba761c6c7082a9a36f009	synthesis of optimal switching logic for hybrid systems	optimisation formal specification logic programming;optimisation;formal specification;measurement;dynamic system;numerical optimization;heating;satisfiability;algorithmic learning hybrid systems switching logic synthesis numerical optimization;trajectory;logic synthesis;logic programming;fuels;heuristic algorithms;switches trajectory optimization measurement heuristic algorithms heating fuels;algorithmic learning;hybrid system;numerical optimization hybrid systems multimodal dynamical system optimal switching logic synthesis quantitative specification;optimization;switching logic synthesis;switches;off the shelf;hybrid systems;penalty function	Given a multi-modal dynamical system, optimal switching logic synthesis involves generating conditions for switching between the system modes such that the resulting hybrid system satisfies a quantitative specification. We formalize and solve the problem of optimal switching logic synthesis for quantitative specifications over long run behavior. Our paper generalizes earlier work on synthesis for safety. We present an approach for specifying quantitative measures using reward and penalty functions, and illustrate its effectiveness using several examples. Each trajectory of the system, and each state of the system, is associated with a cost. Our goal is to synthesize a system that minimizes this cost from each initial state. Our algorithm works in two steps. For a single initial state, we reduce the synthesis problem to an unconstrained numerical optimization problem which can be solved by any off-the-shelf numerical optimization engines. In the next step, optimal switching condition is learnt as a generalization of the optimal switching states discovered for each initial state. We prove the correctness of our technique and demonstrate the effectiveness of this approach with experimental results.	algorithm;correctness (computer science);dynamical system;hybrid system;logic synthesis;longrun;mathematical optimization;modal logic;numerical analysis;optimization problem;with high probability	Susmit Jha;Sanjit A. Seshia;Ashish Tiwari	2011	2011 Proceedings of the Ninth ACM International Conference on Embedded Software (EMSOFT)	10.1145/2038642.2038660	mathematical optimization;real-time computing;computer science;programming language;algorithm;hybrid system	Embedded	-7.984102140324593	29.362314655912463	55386
5b55056021de55a3b70ad12c65688fd713aa9a20	kahn's fixed-point characterization for linear dynamic networks	proceso secuencial comunicante;semantica operacional;semantica denotacional;communicating sequential process;operational semantics;systeme non deterministe;kahn principle;fixed point;non deterministic system;semantique operationnelle;processus sequentiel communicant;informatique theorique;denotational semantics;xed point;sistema no determinista;semantique denotationnelle;dynamic networks;computer theory;informatica teorica	We consider dynamic Kahn-like dataflow networks defined by a simple language L containing the fork-statement. The first part of the Kahn principle states that such networks are deterministic on the I/O level: for each network, different executions provided with the same input deliver the same output. The second part of the principle states that the function from input streams to output streams (which is now defined because of the first part) can be obtained as a fixed point of a suitable operator derived from the network specification. The first part has been proven by us in [BN96, BN97]. To prove the second part, we will use the metric framework. We introduce a nondeterministic transition system NT from which we derive an operational semantics O n . We also define a deterministic transition system DT and prove that the operational semantics O d  derived from DT is the same as O n . Finally, we define a denotational semantics D and prove D = O d . This implies O n  = D. Thus the second part of the Kahn principle is established.	kahn process networks	Shan-Hwei Nienhuys-Cheng;Arie de Bruin	1997		10.1007/3-540-63774-5_133	computer science;artificial intelligence;mathematics;fixed point;programming language;operational semantics;denotational semantics;algorithm	EDA	-9.414231949005643	23.33255645237424	55440
06717f627c480bf9df6fef5c964bcbc006b8ec9f	an efficient algorithm for a sharp approximation of universally quantified inequalities	efficient algorithm;continuous domains;quantified constraints;interval arithmetic;quantified constraint satisfaction problem;branch and prune	This paper introduces a new algorithm for solving a subclass of quantified constraint satisfaction problems (QCSP) where existential quantifiers precede universally quantified inequalities on continuous domains. This class of QCSPs has numerous applications in engineering and design. We propose here a new generic branch and prune algorithm for solving such continuous QCSPs. Standard pruning operators and solution identification operators are specialized for universally quantified inequalities. Special rules are also proposed for handling the parameters of the constraints. First experimentation show that our algorithm outperforms the state of the art methods.	algorithm;approximation;constraint satisfaction problem;universal quantification	Alexandre Goldsztejn;Claude Michel;Michel Rueher	2008		10.1145/1363686.1363724	mathematical optimization;mathematical analysis;discrete mathematics;mathematics;interval arithmetic;algorithm	DB	-9.369737242214589	18.472631107729676	55584
4fbd9d6751b5dfe00cb6bc8e63aa40fc0fe9aba6	automatically proving equivalence by type-safe reflection		One difficulty with reasoning and programming with dependent types is that proof obligations arise naturally once programs become even moderately sized. For example, implementing an adder for binary numbers indexed over their natural number equivalents naturally leads to proof obligations for equalities of expressions over natural numbers. The need for these equality proofs comes, in intensional type theories, from the fact that the propositional equality enables us to prove as equal terms that are not judgementally equal, which means that the typechecker can’t always obtain equalities by reduction. As far as possible, we would like to solve such proof obligations automatically. In this paper, we show one way to automate these proofs by reflection in the dependently typed programming language Idris. We show how defining reflected terms indexed by the original Idris expression allows us to construct and manipulate proofs. We build a hierarchy of tactics for proving equivalences in semigroups, monoids, commutative monoids, groups, commutative groups, semi-rings and rings. We also show how each tactic reuses those from simpler structures, thus avoiding duplication of code and proofs.	adder (electronics);binary number;dependent type;idris;intensional logic;programming language;semiconductor industry;theory;turing completeness;type system	Franck Slama;Edwin Brady	2017		10.1007/978-3-319-62075-6_4	equivalence (measure theory);idris;discrete mathematics;natural number;mathematical proof;expression (mathematics);monoid;binary number;algorithm;commutative property;computer science	PL	-14.518154891835179	18.885907735399574	55613
a14d25ffddd2e5696aa5978bbb0eaceb5278c6bc	inference of k-testable directed acyclic graph languages		In this paper, we tackle the task of graph language learning. We first extend the well-known classes of k-testability and k-testability in the strict sense languages to directed graph languages. Second, we propose a graph automata model for directed acyclic graph languages. This graph automata model is used to propose a grammatical inference algorithm to learn the class of directed acyclic k-testable in the strict sense graph languages. The algorithm runs in polynomial time and identifies this class of languages from positive data.	algorithm;automata theory;automaton;directed acyclic graph;directed graph;grammar induction;graph rewriting;p (complexity);polynomial;time complexity;whole earth 'lectronic link	Damián López;Jorge Calera-Rubio;Antonio Javier Gallego Sánchez	2012			transpose graph;dependency graph;combinatorics;discrete mathematics;wait-for graph;feedback arc set;directed graph;null graph;graph property;graph labeling;simplex graph;machine learning;aperiodic graph;mathematics;voltage graph;graph;graph algebra;moral graph;directed acyclic word graph;complement graph;directed acyclic graph;line graph;graph rewriting	NLP	-4.591107734582271	21.7975180445821	55635
afe8849bf1e1bae00037dd74b4a84aad20da503e	efficient algorithms for interface timing verification	efficient algorithm;branch and bound algorithm;timing verification;upper bound;microprocessor bus;linear and max constraints;branch and bound;lower bound	We present algorithms for computing separations between events that are constrained to obey relationships specified by an acyclic event graph. The algorii,hms are useful for interface-timing verification, where separations are checked against timing requirments. The first algorithm computes separations when only linear and max constraints ezist. The algorithm is conjectured to run in O(Va 1ogV + VE) time. The second algorithm uses a branch-and-bound approach to compute separations when min constrainds also e&t. Ezperiments indicate the algorithms are efficient in practice. our efficient max-linear verification algorithm as the basis for a practical branch-and-bound algorithm. for the general min-max-linear problem. Our verification methods operate on a graph-based form of timing diagram; our present algorithm is restricted to acyclic timing diagrams. A timing diagram consists of a set of signals on which events occur, where an event is a change in the value of a signal. The interface specification includes timing characteristics and timing requirements which represent relationships between the times of events. Timing characteristics are represented by a set of delay constraints. A delay constraint cij, with a lower bound lmer[cij] and an upper bound upper[cij], belongs to one of the three types: 0 linear	algorithm;branch and bound;directed acyclic graph;linear programming;maxima and minima;multistage interconnection networks;requirement;static timing analysis;the times;timing diagram (unified modeling language);transponder timing	Ti-Yen Yen;Wayne H. Wolf;Albert E. Casavant;Alex Ishii	1994	Formal Methods in System Design	10.1023/A:1008680300467	mathematical optimization;computer science;theoretical computer science;upper and lower bounds;branch and bound;algorithm	EDA	-8.289429474748447	30.265010281077277	55846
5a162ec180a33ba7c1ca191542e3836f17dace6a	first-order functional languages and intensional logic	foundational side;formal basis;first-order functional language;intensional logic;nullary variable;better understanding;functional language;first-order functional language;dataflow machine;first-order functional program;new insight;intensional program	The purpose of this paper is to demonstrate that first-order functional programs can be transformed into intensional programs of nullary variables, in a semantics preserving way. On the foundational side, the goal of our study is to bring new insights and a better understanding of the nature of functional languages. From a practical point of view, our investigation provides a formal basis for the tagging mechanism that is used in the implementation of first-order functional languages on dataflow machines. Capsule Review Intensional programming languages such as Lucid treat certain parameters—such as time— implicitly. An intensional interpretation called eduction is then given to programs which introduces the implicit parameters more explicitly. Although intensional languages are clearly declarative, their precise connection to functional languages is worth investigating. This paper describes a translation from first-order functional programs into intensional programs in which the formal parameters to functions are eliminated, leaving them with ‘nullary variables’. This approach is interesting as an implementation method for first-order functional languages, and also demonstrates some useful connections to dataflow architectures.	dataflow architecture;first-order predicate;functional programming;intensional logic;lucid;model of computation;nullary constructor;programming language;reactive programming;victoria (3d figure)	Panos Rondogiannis;William W. Wadge	1997	J. Funct. Program.			PL	-18.018666784692673	21.649460055786385	56133
914a9a3a299edbb62c7ffb1e4bf466ac033e64d4	deductive synthesis of sorting programs	developpement logiciel;deductibility;programme tri;programa ordenacion;ingenieria logiciel;software engineering;deductibilite;sort routine;deducibilidad;desarrollo logicial;software development;genie logiciel;programme recursif;programa recursivo;recursive program	"""Using the deductive synthesis framework developed by Manna and Waldinger we have derived a wide variety of recursive sorting programs. These derivations represent the first application of the deductive framework to the derivation of nontfivial algorithms. While the programs given were derived manually, we ultimately hope that a computer implementation of the system (of which none currently exists) will find similar programs automatically. Our derivations are intended to suggest this possibility; the proofs are short in relation to program complexity (on the order of 40 steps per program) and individual derivation steps are uncontrived. We also present a new rule for the generation of auxiliary procedures, a common """"eureka"""" step in program construction."""	algorithm;deductive database;eureka (opac);recursion;sorting	Jonathan Traugott	1989	J. Symb. Comput.	10.1016/S0747-7171(89)80040-9	theoretical computer science;software development;algorithm	PL	-18.92781602876546	22.223652484867376	56260
9aab0816b233927e5712c9da446cd53d89c641d8	distributed graphs and graph transformation	distributed system;algebraic approach;formal specification;distributed computing;graph transformation;local system;category theory;graph grammar;localized state;dynamic networks	The new approach of distributed graphs and graph transformation as developed in this article allows to use structured graph transformation on two abstraction levels, the network and the local level. The network level contains the description of the topological structures of a system. The local level covers the description of states and their transitions in local systems. Local state transitions may depend on others using suitable synchronization mechanisms. The main distribution concepts of categorical graph grammars presented by Schneider are combined with the algebraic approach to distributed graph transformation introduced by Ehrig et.al. Modeling of distributed systems by this new kind of distributed graph transformation ooers a clear and elegant description of dynamic networks, distributed actions as well as communication and synchronization using a graphical notation. Moreover, distributed graph transformation ooers the possibility to describe splitting and joining of local graphs as well as parallel transformations in local systems. The formalization of distributed graph transformation is done by means of category theory. A distributed graph is formalized by a diagram in the category GRAPH of graphs and total graph morphisms. A distributed transformation step is characterized by a double-pushout in the category DISTR(GRAPH) of distributed graphs and distributed graph morphisms. A pushout over distributed graph morphisms cannot always be constructed componentwise in each local part. But especially the componentwise construction of a distributed transformation reeects distributed computations best. Thus, we present the necessary conditions for componentwise construction of pushouts in DISTR(GRAPH) and show that they are also suucient. These conditions are summarized in the distributed gluing condition. Moreover, these conditions are needed for componentwise construction of pushout complements which are used to characterize the rst step of a distributed graph transformation.	category theory;computation;concurrency (computer science);diagram;distributed computing;double pushout graph rewriting;high- and low-level;synchronization (computer science);total coloring	Gabriele Taentzer	1999	Applied Categorical Structures	10.1023/A:1008683005045	outerplanar graph;lattice graph;combinatorics;discrete mathematics;universal graph;directed graph;topology;null graph;graph property;clique-width;simplex graph;comparability graph;cubic graph;formal specification;symmetric graph;mathematics;voltage graph;distance-hereditary graph;graph;butterfly graph;vertex-transitive graph;complement graph;local system;line graph;coxeter graph;category theory;algebra;graph rewriting	Graphics	-8.246605039196902	22.956170929497123	56512
b0de7984654b8848005f30b6d9e172cb48bc5fb7	modeling and verification of a network player system with dcvalid	model checking tool;formal specification;real time interval temporal logic;real time systems calculus logic multimedia systems video on demand educational institutions computer science videoconference games internet telephony;generic model;videoconference;temporal logic;real time;logic;formal modelling;automated tool;model checking tool formal modelling formal verification network player system dcvalid duration calculus real time interval temporal logic automated tool;formal verification temporal logic multimedia systems formal specification;interval temporal logic;satisfiability;internet telephony;multimedia systems;formal verification;duration calculus;model checking;specification and verification;network player system;calculus;games;video on demand;dcvalid;computer science;real time systems	In this paper we study the formal modelling and verification of a network player system using Duration Calculus, a real time interval temporal logic. The system is modelled by the conjunction of a number of Duration Calculus formulae each capturing a basic property of the system. That the system satisfies the requirement is expressed by the entailment of the requirement formula from the system formula. We use an automated tool DCValid for verification. DCValid is a model checking tool and it cannot verify our system in the general form, and therefore a special instance is derived from the general model and subsequently checked using DCValid.		Jianzhong Wang;Qiwen Xu;Huadong Ma	2000		10.1109/APAQ.2000.883777	real-time computing;computer science;theoretical computer science;algorithm	AI	-11.322582469113788	25.824042980407636	56556
219dcb427ea5417b1c07b46e6b6961422e0e9b20	a mechanization of unity in pc-nqthm-92	operational semantics;unity;natural extension;theorem proving;theorem prover;parallelism;concurrency;pc nqthm;concurrent programs	This paper presents in detail how the Unity logic for reasoning about concurrent programs was formalized within the mechanized theorem prover PC-NQTHM-92. Most of Unity′s proof rules were formalized in the unquantified logic of NQTHM, and the proof system has been used to mechanically verify several concurrent programs. The mechanized proof system is sound by construction, since Unity′s proof rules were proved about an operational semantics of concurrency, also presented here. Skolem functions are used instead of quantifiers, and the paper describes how proof rules containing Skolem function are used instead of Unity′s quantified proof rules when verifying concurrent programs. This formalization includes several natural extensions to Unity, including nondeterministic statements. The paper concludes with a discussion of the cost and value of mechanization.	automated proof checking;automated theorem proving;concurrency (computer science);nqthm;operational semantics;pc dos (ibm dos);proof calculus;skolem normal form;unity;verification and validation	David M. Goldschlag	1999	Journal of Automated Reasoning	10.1023/A:1006262609127	discrete mathematics;computer science;mathematics;automated theorem proving;programming language;algorithm	PL	-15.701783918360874	20.515683638607086	56589
ae619d6558663e21b59adf1802c639885edef22f	conversion of decision tables by rule mask method without rule mask	rule mask technique;program generation;program generator;rule mask method;rule mask;decision tables;decision table	Two algorithms for generating computer programs from decision tables are described. The algorithms allow handling limited entry, extended emry, arid mixed entry tables. The algorithms are based on the rub mask method but need trot have the masks at execution time, They perform the logical operations immediately rather than at the end ~f the inte,r~reting process. Execution time can be considerably reduce4 by irtstantly marking rules which are not aNalicable {Algorithms 1 and 2) or conditrions wMeh are already tested (Algorithm 2). The new algorithms comMne to a certain degree the advantages of mask mettmds with those of tree mounds.	algorithm;computer program;decision table;item unique identification;logical connective;run time (program lifecycle phase)	Gert Dathe	1972	Commun. ACM	10.1145/355604.361596	decision table;computer science;programming language	DB	-14.669597503143503	31.758819367025055	56627
9a7315b1f2e7253c87be4637b946b86684e344d6	modifying esterel concepts to model hybrid systems	real time;discrete time;control flow;hybrid system	Abstract   A hallmark of the Esterel language is the combination of perfect synchrony with total orthogonality and powerful constructs for preemption, suspension and trap handling. It is desirable to make this kind of expressiveness available for the description of hybrid systems, that is, systems whose evolution is understood in terms of segment-wise continuous functions over the real time axis. Our approach consists of modifying Esterel concepts, most notably by replacing the discrete time frame by a continuously advancing one. We are then able to state a semantics made up of transitions with closed execution intervals of non-zero length. By an instant we understand an execution interval within this framework. Hybrid signals may change their value during such an instant, non-hybrid ones, that is, classical signals immediately settle to a specific state and keep it the whole time. Time consumption still has to be specified explicitly, namely in that instants reflect jumps among control flow locations defined by  pause  statements; all other statements take no time in the sense that arbitrarily many of them may be sequentially executed regardless of the instant's duration. A transfer of perfect synchrony from the discrete to the continuous is in this way accomplished. We also consider an example, which is from the automotive domain, traces, bisimilarity and compositionality.		Michael Baldamus;Thomas Stauner	2002	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80439-6	discrete time and continuous time;real-time computing;computer science;distributed computing;programming language;control flow;algorithm;hybrid system	ECom	-19.029520761906614	30.853224319731947	56816
08d5b07aeb27af91768440e65109286fe0d4502b	modal algebra and petri nets		We use the by now well established setting of modal semirings to derive a modal algebra for Petri nets. It is based on a relation-algebraic calculus for separation logic that enables calculations of properties in a pointfree fashion and at an abstract level. Basically, we start from an earlier logical approach to Petri nets that in particular uses modal box and diamond operators for stating properties about the state space of such a net. We provide relational translations of the logical formulas which further allow the characterisation of general behaviour of transitions in an algebraic fashion. From the relational structure an algebra for frequently used properties of Petri nets is derived. In particular, we give connections to typical used assertion classes of separation logic. Moreover, we demonstrate applicability of the algebraic approach by calculations concerning a standard example of a mutex net.	assertion (software development);correctness (computer science);fairness measure;inductive reasoning;linear algebra;liveness;microsoft outlook for mac;modal algebra;modal logic;mutual exclusion;newton–cotes formulas;petri net;separation logic;signal transition;state space;vertex-transitive graph	Han-Hing Dang;Bernhard Möller	2015	Acta Informatica	10.1007/s00236-015-0216-3	combinatorics;discrete mathematics;mathematics;process architecture;petri net;algorithm	Logic	-10.416026540216214	21.30927425921238	56834
56ad0ec1377ed274a615b3103d4072453ec4598a	automatic steering of behavioral model inference	critical behavior;model inference;bepress selected works;behavior modeling;software systems;statistical significance;automatic generation;finite state automata;temporal properties;dynamic analysis mining automata temporal properties;mining automata;verification and validation;dynamic analysis	Many testing and analysis techniques use finite state models to validate and verify the quality of software systems. Since the specification of such models is complex and time-consuming, researchers defined several techniques to extract finite state models from code and traces. Automatically generating models requires much less effort than designing them, and thus eases the verification and validation of large software systems. However, when models are inferred automatically, the precision of the mining process is critical. Behavioral models mined with imprecise processes can include many spurious behaviors, and can thus compromise the results of testing and analysis techniques that use those models.  In this paper, we increase the precision of automata inferred from execution traces, by leveraging two learning techniques. We first mine execution traces to infer statistically significant temporal properties that capture relations between non consecutive and possibly distant events. We then incrementally refine a simple initial automaton by merging likely equivalent states. We identify equivalent states by analyzing set of consecutive events, and we use the inferred temporal properties to evaluate whether two equivalent states can be merged or not. We merge equivalent states only if the merging does violate any temporal property, since a merging that violates temporal properties is likely to introduce an imprecise generalization. Our generalization process that preserves temporal properties while merging states avoids breaking non-local relations, and thus solves one of the major cause of overgeneralized models. Thus, mined properties steer the learning of behavioral models. The technique is completely automated and generates an automaton that both accepts the input traces and satisfies the mined temporal properties.  We evaluated our solution by comparing models inferred with and without checking mined temporal properties. Results show that our steering process can significantly improve precision without noticeable loss of recall.	automaton;behavioral modeling;dfa minimization;mined;software system;tracing (software);verification and validation	David Lo;Leonardo Mariani;Mauro Pezzè	2009		10.1145/1595696.1595761	behavioral modeling;verification and validation;computer science;theoretical computer science;software engineering;machine learning;data mining;dynamic program analysis;statistical significance;finite-state machine;programming language;software system	SE	-14.35693124450123	27.92892864428534	56856
d4c1de45539064cb99d04eb73ed9cd7c03878ac0	modular modeling for the diagnostic of complex discrete-event systems	diagnosers;batch neutralization process discrete event systems modular modeling fault diagnosis maintenance cost reduction timed automata deterministic automaton failure occurrence;timed automaton diagnosers discrete event systems fault detection fault diagnosis modular modeling software implementation;modular modelling;failure analysis;timed automaton;fault detection;discrete event systems;automata theory;fault diagnosis automata theory discrete event systems failure analysis;automata fault detection algorithm design and analysis discrete event systems fault diagnosis;software implementation;fault diagnosis	For the complex systems, the development of a methodology of fault diagnosis is important. Indeed, for such systems, an efficient diagnosis contributes to the improvement of the availability, the growth of production, and, of course, the reduction of maintenance costs. It is a key action in the improvement of performance of industrial feature. This paper proposes a new approach to diagnose complex systems modeled by communicating timed automata. Each component has been modeled separately by a timed automaton integrating various operating modes while the communication between the various components is carried out by the control module. Starting from each module of the complex system, a single deterministic automaton, called a diagnoser, is constructed that uses observable events to detect the occurrence of a failure. This modeling formalism provides means for formal verification of the complex system model and its diagnoser. The model-checking methods are used to check correctness properties. The steps of the method are described by an algorithm and illustrated through a batch neutralization process. The implementation of the algorithm is also discussed.	algorithm;automata theory;complex system;complex systems;control unit;correctness (computer science);dspace;deterministic automaton;fault model;formal proof;formal system;formal verification;model checking;numerical analysis;observable;sensor;timed automaton	Eric Gascard;Zineb Simeu-Abazi	2013	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2012.2229707	failure analysis;real-time computing;computer science;theoretical computer science;automata theory;distributed computing;timed automaton;fault detection and isolation;algorithm	Embedded	-7.266055570420228	27.915601380388022	56903
6b70570384b54c4cf5c47b3214a70272760e222a	automated generation of strictly conforming tests based on formal specification of dynamic semantics of the programming language	formal specification;programming language;automated test generation;satisfiability;conformance testing	A technique for an automated test generation for compilers, which is based on formal specifications of the programming language, is suggested. The technique makes it possible to generate tests correct from the dynamic semantics standpoint that do not depend on specific features (undefined or implementation-specific) of the semantics (the so-called strictly conforming tests). The application of the suggested technique to generating tests for C compilers is discussed in detail. For this purpose, a subset of C is defined the semantics of which, first, does not depend on the above-mentioned specific features and, second, possesses properties of type soundness and determinism, which guarantee the correct test execution for any implementation satisfying the C standard.	ansi c;compiler;computer;conformity;control flow;formal specification;futures studies;gnu debugger;programming language;requirement;soundness (interactive proof);test automation;type safety;undefined behavior	Alexander S. Kossatchev;P. Kutter;Mikhail Posypkin	2004	Programming and Computer Software	10.1023/B:PACS.0000036420.25147.da	computer science;theoretical computer science;conformance testing;formal semantics;formal specification;programming language;operational semantics;programming language specification;algorithm;semantics;satisfiability	PL	-17.54291050221013	22.635551978899468	56969
299a0b8e283235e73e35f7f2386a2a23d190e59d	an interactive solution to the n by n mutilated checkerboard problem	mutilated checkerboard problem;finite state machines;artificial intelligence;automated theorem proving	We present a method of modelling parameterized classes of finite state machines in the Boyer-Moore logic so that invariant properties can be verified interactively using the Boyer-Moore theorem prover. We illustrate our approach using an interactive proof of the impossibility of covering an n × n ‘mutilated’ checkerboard completely with dominoes. We model the problem by defining a simulator and formalize the well-known parity argument as a proof by mathematical induction on the length of action sequences executed starting with an empty board. The suitability of our approach for verifying properties depends strongly on the actual Lisp data structures and programs used in the simulation model. We explain some of the choices we made in simplifying the checkerboard problem representation and describe in detail the heuristic guidance given to the theorem prover.		Sakthi Subramanian	1996	J. Log. Comput.	10.1093/logcom/6.4.573	combinatorics;discrete mathematics;computer science;artificial intelligence;mathematics;automated theorem proving;finite-state machine;algorithm	Logic	-15.365754456279687	21.112732889707303	57079
b015165e6cc4f83f76f51560a5bbb12419a56d4a	code parallelization for the lgdg large-grain dataflow computation	optimizing compiler;data dependence;data flow analysis	This paper addresses the software aspect of an integrated hardware-software scheme called LGDG for computation with large-grain dataflow graph. It will show that data flow analysis can be used to obtain parallel LGDG code at a cost comparable to that of an optimizing compiler. Unintentional data dependencies have been recognized to be a major obstacle for parallelizing imperative languages. In order to reduce unintentional data dependencies and ease the data flow analysis, a restricted imperative language is specified. An efficient representation of assertion sets is proposed, and rules for calculating synthesized attributes are presented. A theorem is provided to ensure function-preserving parallelization.		Kechang Dai	1990		10.1007/3-540-53065-7_104	manifest expression;computer architecture;parallel computing;compiler correctness;computer science;optimizing compiler;programming language;automatic parallelization	EDA	-17.002773457627477	31.97455298255777	57376
c29cfad4c4987a7c507ef9d78c30b6da40b040de	expressive completeness failure in branching time structures	table verite;tense logic;programme correctness;plenitud;relacion orden;calcul propositionnel;ordering;concurrent program;truth table;time;exactitude programme;relation ordre;modal logic;exactitud programa;temps;informatique theorique;logique modale;programa competidor;propositional calculus;logica modal;calculo proposicional;completeness;completude;programme concurrent;computer theory;tiempo;informatica teorica	A propositional logic is expressively complete if there is a finite set of connectives which define all truth tables. Kamp, Stavi, and Gabbay proved that all tense logics over linear time are expressively complete. Amir and Gabbay brought examples of expressively complete non-linear time structures. Gabbay showed that the general time structure is not expressively complete. Here we narrow the gap and prove that for branching time, if the model is an infinite tree with an unbounded branching factor then there is no expressive completeness.		Amihood Amir	1987	J. Comput. Syst. Sci.	10.1016/0022-0000(87)90003-1	modal logic;discrete mathematics;completeness;order theory;truth table;mathematics;propositional calculus;algorithm	Theory	-7.172819751183782	20.122806202765318	57508
ca2640dd1f84380edfdc045543a99331e1dd0196	building strategies into qbf proofs		Strategy extraction is of paramount importance for quantified Boolean formulas (QBF), both in solving and proof complexity. It extracts (counter)models for a QBF from a run of the solver resp. the proof of the QBF, thereby allowing to certify the solver’s answer resp. establish soundness of the system. So far in the QBF literature, strategy extraction has been algorithmically performed from proofs. Here we devise the first QBF system where (partial) strategies are built into the proof and are piecewise constructed by simple operations along with the derivation. This has several advantages: (1) lines of our calculus have a clear semantic meaning as they are accompanied by semantic objects; (2) partial strategies are represented succinctly (in contrast to some previous approaches); (3) our calculus has strategy extraction by design; and (4) the partial strategies allow new sound inference steps which are disallowed in previous central QBF calculi such as Q-Resolution and long-distance Q-Resolution. The last item (4) allows us to show an exponential separation between our new system and the previously studied reductionless long-distance resolution calculus, introduced to model QCDCL solving. Our approach also naturally lifts to dependency QBFs (DQBF), where it yields the first sound and complete CDCL-type calculus for DQBF, thus opening future avenues into DQBF CDCL solving. 2012 ACM Subject Classification Theory of computation → Proof complexity	algorithm;conflict-driven clause learning;proof complexity;solver;soundness (interactive proof);theory of computation;time complexity;true quantified boolean formula	Olaf Beyersdorff;Joshua Blinkhorn;Meena Mahajan	2018	Electronic Colloquium on Computational Complexity (ECCC)			Logic	-13.30557889017753	21.042922875191994	57641
274ab3f5a10de9700c7b2cf5375bec7bc475b40a	reduction of distributions: definitions, properties, and applications	complexity theory;supervisory control;automata;computer architecture;indexes;discrete event systems;decentralized control;article post print	In this work, a notion of reduction of distributions is proposed as a technical tool for improving the complexity of decomposability verification and supporting parallel verification of decomposability, by exploiting the rich structures of distributions. We provide some results that reduce the search space of candidate reductions, as a first step toward efficiently computing optimal reductions. It is then shown that a distribution has a reduction if and only if a particular candidate reduction is indeed a reduction. We then provide a sound substitution-based proof technique that can be used for (automatic) reduction verification. Techniques for refuting candidate reductions are also provided. We then explain an application of the decomposability verification problem in the lower bound proofs for the problem of supervisor decomposition and the problem of existence of a decentralized supervisor. Finally, some other applications of the notion of reduction of distributions are also shown.	graph coloring	Liyong Lin;Simon Ware;Rong Su;W. M. Wonham	2017	IEEE Transactions on Automatic Control	10.1109/TAC.2017.2692561	control engineering;database index;decentralised system;computer science;theoretical computer science;distributed computing;automaton;supervisory control	PL	-6.8581667510357525	25.998322532679474	57876
53f51fb9326091ca81a5b71bc642eef22f9cf8e6	the acpath metric: precise estimation of the number of acyclic paths in c-like languages		NPATH is a metric introduced by Brian A. Nejmeh in [13] that is aimed at overcoming some important limitations of McCabe’s cyclomatic complexity. Despite the fact that the declared NPATH objective is to count the number of acyclic execution paths through a function, the definition given for the C language in [13] fails to do so even for very simple programs. We show that counting the number of acyclic paths in CFG is unfeasible in general. Then we define a new metric for C-like languages, called ACPATH, that allows to quickly compute a very good estimation of the number of acyclic execution paths through the given function. We show that, if the function body does not contain backward gotos and does not contain jumps into a loop from outside the loop, then such estimation is actually exact. Mathematics Subject Classification (2000) 68N30 CR Subject Classification D.2.8 · D.2.5	brian;context-free grammar;cyclomatic complexity;directed acyclic graph;mathematics subject classification	Roberto Bagnara;Abramo Bagnara;Alessandro Benedetti;Patricia M. Hill	2016	CoRR		algorithm	Theory	-16.100990822529	31.212604800164826	58032
9d9ca640d6c36ea73ca019accbb4bff2a88870d2	decidable subclassing-bounded quantification	bounded quantification;class and object encoding;upper bound;typed intermediate language;type checking;lower bound;decidability	Bounded Quantification allows Quantified types to specify subtyping bounds for the type variables they introduce. It has undecidable subtyping and type checking. This paper shows that subclassing-bounded Quantification---type variables have subclassing bounds---has decidable type checking. The main difficulty is that, type variables can have either upper bounds or lower bounds, which complicates the minimal type property.	type system;type variable;undecidable problem	Juan Chen	2005		10.1145/1040294.1040298	type system;subtyping;bounded quantification;computer science;upper and lower bounds;programming language;algorithm	PL	-12.748734986026847	18.265501493593227	58143
f62a938dc3f50be55947cd041b096f101a18e4c7	a predicative approach to the classification problem	functional program;predicative approach;automated complexity analysis prototype;nuprl proof development system;classification problem;simple abstract cost model;automated complexity analysis	This paper describes the Automated Complexity Analysis Prototype (ACAp) system for automated complexity analysis of functional programs synthesized with the Nuprl proof development system. We define a simple abstract cost model for NUPRL's ...	impredicativity	Salvatore Caporaso;Emanuele Covino;Giovanni Pani	2001	J. Funct. Program.		discrete mathematics;time complexity;theoretical computer science;recursion;transfinite number;slow-growing hierarchy;successor cardinal;computer science;hierarchy;elementary function;predicative expression	PL	-17.823610146658954	19.747820918349035	58296
0c3ee53cc6224b468c52f9d7cbc0d0342f5dd2da	an interactive protocol synthesis algorithm using a global state transition graph	interactive protocol synthesis algorithm;graph theory;protocols;first in first out;deadlock avoidance;global state transition graph;production rules;complete reception;automata;finite state machines;protocols system recovery algorithm design and analysis buffer overflow automata;system recovery;buffer overflow;finite automata;protocols finite automata graph theory interactive programming;deadlock freeness;interaction protocol;deadlock avoidance rules;buffer overflow interactive protocol synthesis algorithm global state transition graph finite state machines global state transition graph production rules deadlock avoidance rules complete reception deadlock freeness;state transition graph;communicating finite state machine;algorithm design and analysis;interactive programming;production rule	An interactive synthesis algorithm, to construct two communicating finite-state machines (protocols), is presented. The machines exchange messages over two unidirectional FIFI (first-in first-out) channels when the function of the protocol has been given. The synthesis algorithm first constructs the global state transition graph (GSTG) of a protocol to be synthesized and then produces the protocol. It is based on a set of production rules and a set of deadlock avoidance rules, which guarantee that complete reception and deadlock freeness capabilities are provided in the interacting process. This synthesis algorithm prevents a designer from creating unspecified reception and nonexecutable transition, avoids the occurrence of deadlocks, and monitors for the presence of buffer overflow. >	algorithm;state diagram	Yaoxue Zhang;Kaoru Takahashi;Norio Shiratori;Shoichi Noguchi	1988	IEEE Trans. Software Eng.	10.1109/32.4659	communications protocol;algorithm design;buffer overflow;fifo and lifo accounting;computer science;theoretical computer science;distributed computing;automaton;finite-state machine;programming language	EDA	-10.595286938639505	30.684382801376	58377
433b834fd808bf850bad30b7e5c5939acb6d827d	parameterized verification through view abstraction	view abstraction;datorsystem;computer systems;safety;small model properties;parameterized systems	We present a simple and efficient framework for automatic verification of systems with a parametric number of communicating processes. The processes may be organized in various topologies such as words, multisets, rings, or trees. Our method needs to inspect only a small number of processes in order to show correctness of the whole system. It relies on an abstraction function that views the system from the perspective of a fixed number of processes. The abstraction is used during the verification procedure in order to dynamically detect cut-off points beyond which the search of the state space need not continue. We show that the method is complete for a large class of well quasi-ordered systems including Petri nets. Our experimentation on a variety of benchmarks demonstrate that the method is highly efficient and that it works well even for classes of systems with undecidable verification problems. In particular, the method handles the fine-grained and full version of Szymanski’s mutual exclusion protocol, whose correctness, to the best of our knowledge, has not been proven automatically by any other existing methods.	benchmark (computing);correctness (computer science);mutual exclusion;network topology;petri net;state space;tree (data structure);undecidable problem;verification and validation	Parosh Aziz Abdulla;Frédéric Haziza;Lukás Holík	2015	International Journal on Software Tools for Technology Transfer	10.1007/s10009-015-0406-x	computer science;theoretical computer science;distributed computing;programming language;algorithm	Logic	-13.604973456429398	28.110275943613463	58510
1a67aa06ae8cff5f6aa6e0c13371e4383fbed5d8	runtime verification of ltl-based declarative process models	runtime verification;declarative business processes;operational support;process mining;monitoring;linear temporal logic;finite state automata;process model	Linear Temporal Logic (LTL) on finite traces has proven to be a good basis for the analysis and enactment of flexible constraintbased business processes. The Declare language and system benefit from this basis. Moreover, LTL-based languages like Declare can also be used for runtime verification. As there are often many interacting constraints, it is important to keep track of individual constraints and combinations of potentially conflicting constraints. In this paper, we operationalize the notion of conflicting constraints and demonstrate how innovative automatabased techniques can be applied to monitor running process instances. Conflicting constraints are detected immediately and our toolset (realized using Declare and ProM) provides meaningful diagnostics.	algorithm;automata theory;beam propagation method;business process;coloured petri net;continuation;declarative programming;deterministic finite automaton;event calculus;finite-state machine;interaction;interference (communication);internet backbone;lecture notes in computer science;linear temporal logic to büchi automaton;operating system;programmable read-only memory;runtime verification;springer (tank);timed automaton;tracing (software)	Fabrizio Maria Maggi;Michael Westergaard;Marco Montali;Wil M. P. van der Aalst	2011		10.1007/978-3-642-29860-8_11	linear temporal logic;real-time computing;computer science;theoretical computer science;process modeling;runtime verification;finite-state machine;process mining;programming language;algorithm	SE	-17.34069490528055	29.51159509345777	58620
5d8616f28f475f56f82aa3a7ca29c3d5fb190d16	automation of algorithmic tasks for virtual laboratories based on automata theory		In the work a description of an automata model of standard algorithm for constructing a correct solution of algorithmic tests is given. The described model allows a formal determination of the variant complexity of algorithmic test and serves as a basis for determining the complexity functions, including the collision concept – the situation of uncertainty, when a choice must be made upon fulfilling the task between the alternatives with various priorities. The influence of collisions on the automata model and its inner structure is described. The model and complexity functions are applied for virtual laboratories upon designing the algorithms of constructing variant with a predetermined complexity in real time and algorithms of the estimation procedures of students’ solution with respect to collisions. The results of the work are applied to the development of virtual laboratories, which are used in the practical part of massive online course on graph theory.	algorithm;automata theory;automation;complexity function;enterprise application integration;graph theory;real-time computing	Eugene A. Efimchik;Mikhail S. Chezhin;Andrey V. Lyamin	2016	ICST Trans. e-Education e-Learning	10.4108/eai.11-4-2016.151149	computer science;theoretical computer science;algorithm	Graphics	-6.908318185250378	24.225738642670343	58710
01c36566613ff4950c3b10687d7b19a7de9471ef	formal specification in vhdl for hardware verification	hardware verification;logic cad;certain formalism;standard vhdl;hardware description languages;relevant property;partial correctness;specification constructs;total correctness property;new specification;vhdl;temporal logic;total correctness properties;assert statement;certain hardware verification approach;formal specification;formal verification;hardware description language;formal specifications;read only memory;testing;digital circuits;object oriented	In this paper, we enrich VHDL with new specification constructs intended for hardware verification. Using our extensions, total correctness properties may now be stated whereas only partial correctness can be expressed using the standard VHDL assert statement. All relevant properties can now be specified in such a way that the designer does not need to use formalisms like temporal logics. As the specifications are independent from a certain formalism, there is no restriction to a certain hardware verification approach.	assertion (software development);correctness (computer science);electronic design automation;formal specification;semantics (computer science);temporal logic;vhdl	Ralf Reetz;Klaus Schneider;Thomas Kropf	1998			correctness;computer architecture;verification;formal methods;formal verification;vhdl;computer science;theoretical computer science;logic simulation;formal specification;formal equivalence checking;high-level verification;runtime verification;hardware description language;programming language;intelligent verification;register-transfer level;functional verification	Logic	-15.481592326675234	29.06716378391375	58802
fb36ba807be92f2a3725942ca1cce4f6cefa889c	a modular drinking philosophers algorithm	distributed algorithms;resource allocation;waiting time;drinking philosophers;modularity;dining philosophers	A variant of the drinking philosophers algorithm of Chandy and Misra is described and proved correct in a modular way. The algorithm of Chandy and Misra is based on a particular dining philosophers algorithm and relies on certain properties of its implementation. The drinking philosophers algorithm presented in this paper is able to use an arbitrary dining philosophers algorithm as a subroutine; nothing about the implementation needs to be known, only that it solves the dining philosophers problem. An important advantage of this modularity is that by substituting a more time-efficient dining philosophers algorithm than the one used by Chandy and Misra, a drinking philosophers algorithm withO(1) worst-case waiting time is obtained, whereas the drinking philosophers algorithm of Chandy and Misra hasO(n) worst-case waiting time (forn philosophers). Careful definitions are given to distinguish the drinking and dining philosophers problems and to specify varying degrees of concurrency.	best, worst and average case;concurrency (computer science);correctness (computer science);dijkstra's algorithm;dining philosophers problem;formal proof;graph coloring;input/output automaton;liveness;misra c;mutual exclusion;subroutine;temporal logic;time complexity;whole earth 'lectronic link	Jennifer L. Welch;Nancy A. Lynch	1993	Distributed Computing	10.1007/BF02242711	distributed algorithm;resource allocation;computer science;artificial intelligence;modularity;distributed computing;dining philosophers problem;algorithm	PL	-9.084132439989476	25.633100346289048	58874
ea9e8e4dd1531b0a94f5b471bbfdc2f5791b55fe	certifying proofs for ltl model checking		In the context of formal verification, certifying proofs are proofs of the correctness of a model in a deduction system produced automatically as outcome of the verification. They are quite appealing for high-assurance systems because they can be verified independently by proof checkers, which are usually simpler to certify than the proof-generating tools.Model checking is one of the most prominent approaches to formal verification of temporal properties and is based on an algorithmic search of the system state space. Although modern algorithms integrate deductive methods, the generation of proofs is typically restricted to invariant properties only.In this paper, we solve this issue in the context of Linear-time Temporal Logic. By exploiting the k-liveness algorithm, we show how to extend proof generation capabilities for invariant checking to cover full LTL properties, in a simple and efficient manner, with essentially no overhead for the model checker. We implemented the technique on top of an IC3 engine, and show the feasibility of the approach on a variety of benchmarks.		Alberto Griggio;Marco Roveri;Stefano Tonetta	2018	2018 Formal Methods in Computer Aided Design (FMCAD)	10.23919/FMCAD.2018.8603022	theoretical computer science;model checking;mathematical proof;formal verification;temporal logic;correctness;state space;automaton;invariant (mathematics);computer science	Logic	-14.703671344759519	26.5258543906462	58904
cfaac9a51c7ec41f9342be61cef7c201c1322a95	a framework for relating, implementing and verifying argumentation models and their translations	qa 75 electronic computers computer science	Computational argumentation theory deals with the formalisation of argument structure, conflict between arguments and domain-specific constructs, such as proof standards, epistemic probabilities or argument schemes. However, despite these practical components, there is a lack of implementations and implementation methods available for most structured models of argumentation and translations between them. This thesis addresses this problem, by constructing a general framework for relating, implementing and formally verifying argumentation models and translations between them, drawing from dependent type theory and the Curry-Howard correspondence. The framework provides mathematical tools and programming methodologies to implement argumentation models, allowing programmers and argumentation theorists to construct implementations that are closely related to the mathematical definitions. It furthermore provides tools that, without much effort on the programmer’s side, can automatically construct counter-examples to desired properties, while finally providing methodologies that can prove formal correctness of the implementation in a theorem prover. The thesis consists of various use cases that demonstrate the general approach of the framework. The Carneades argumentation model, Dung’s abstract argumentation frameworks and a translation between them, are implemented in the functional programming language Haskell. Implementations of formal properties of the translation are provided together with a formalisation of AFs in the theorem prover, Agda. The result is a verified pipeline, from the structured model Carneades into existing efficient SAT-based implementations of Dung’s AFs. Finally, the ASPIC model for argumentation is generalised to incorporate content orderings, weight propagation and argument accrual. The framework is applied to provide a translation from this new model into Dung’s AFs, together with a complete implementation.	agda;argumentation framework;automated theorem proving;computation;correctness (computer science);curry;curry–howard correspondence;dependent type;functional programming;haskell;programmer;programming language;software propagation;type theory;verification and validation	Bas van Gijzel	2016			computer science;theoretical computer science;algorithm	PL	-17.97085343205567	23.17454544874195	58951
cd558a5fdbbf2ac2bd8903550522fc9f525a7e0f	explaining relaxed memory models with program transformations		Weak memory models determine the behavior of concurrent programs. While they are often understood in terms of reorderings that the hardware or the compiler may perform, their formal definitions are typically given in a very different style—either axiomatic or operational. In this paper, we investigate to what extent weak behaviors of existing memory models can be fully explained in terms of reorderings and other program transformations. We prove that TSO is equivalent to a set of two local transformations over sequential consistency, but that nonmulti-copy-atomic models (such as C11, Power and ARM) cannot be explained in terms of local transformations over sequential consistency. We then show that transformations over a basic non-multi-copy-atomic model account for the relaxed behaviors of (a large fragment of) Power, but that ARM’s relaxed behaviors cannot be explained in a similar way. Our positive results may be used to simplify correctness of compilation proofs from a high-level language to TSO or Power.	c11 (c standard revision);compiler;coq (software);correctness (computer science);high- and low-level;high-level programming language;memory address;memory model (programming);program transformation;sequential consistency;strongarm;time sharing option;traffic collision avoidance system	Ori Lahav;Viktor Vafeiadis	2016		10.1007/978-3-319-48989-6_29	c11;theoretical computer science;compiler;memory model;mathematical proof;sequential consistency;axiom;correctness;program transformation;computer science	PL	-17.40723253659216	20.788956677353468	59043
55a68b24c54a6f971bbc7c8bbe63495573e3714b	the use of rippling to automate event-b invariant preservation proofs	event b;automated reasoning;lemma conjecture;rippling	The use of formal method techniques can contribute to the production of more reliable and dependable systems. However, a common bottleneck for the industrial adoption of formal methods techniques is proof automation. To address the challenge of proof automation, we aim to improve it by using automated theorem proving techniques. We set one popular formal method technique as our working domain in this thesis, that is, Event-B. More specifically, we target a family of proofs, i.e. invariant preservation (INV) proofs, which can account for a significant part of the proofs that needs human interactions. We apply one of the most successful automated theorem proving techniques, which is rippling, to improve the proof automation of Event-B INV POs. Rippling uses metalevel guidance to automate proofs, and it is in particular useful to develop proof patches to recover proofs based on the meta-level guidance when proof attempts fail. We are especially interested in the case when lemmas are required to recover failed proofs. We combine a scheme-based theory-exploration system, i.e. IsaScheme, with rippling to develop a proof patch for lemma discovery, i.e. ahLemma. In addition to ahLemma, we also develop two proof patches to suggest case-splits and to simplify the goal and hypotheses by rewriting. The combining use of rippling and these proof patches can improve the proof automation of INV POs.	automated theorem proving;b-method;dependability;formal methods;interaction;inverter (logic gate);rewriting;rippling	Yuhui Lin;Alan Bundy;Gudmund Grov	2012		10.1007/978-3-642-28891-3_23	discrete mathematics;computer science;mathematics;automated reasoning;algorithm;rippling	Logic	-14.916545917743269	27.402839840360617	59143
fe3a6c538f47ffab592fbe20d3fa89717394d635	specifying properties for modular pi-calculus	temporal logic pi calculus program verification;temporal logic;model checking algorithm;pi calculus;process equivalence;correctness proving;program verification;software engineering;model checking process algebra modal logics;modal logic;model checking;correctness proving modular pi calculus modal logic temporal properties spatial properties process equivalence model checking algorithm;spatial properties;temporal properties;process algebra;modal logics;modular pi calculus	We propose a modal logic for modular pi calculus: a logic to specify both temporal and spatial properties for processes in modular pi calculus. Characterization of process equivalence the logic induce is investigated, and it is shown that the distinguishing power of the logic falls between bisimilarity and structural congruence. Then a model checking algorithm for the logic over the finite-control subset of modular pi calculus is presented, and its correctness proved.	algorithm;bisimulation;congruence of squares;correctness (computer science);modal logic;model checking;sequent calculus;turing completeness;π-calculus	Takashi Kitamura;Huimin Lin	2008	2008 2nd IFIP/IEEE International Symposium on Theoretical Aspects of Software Engineering	10.1109/TASE.2008.36	modal logic;dynamic logic;zeroth-order logic;model checking;modal μ-calculus;process calculus;linear temporal logic;description logic;logic optimization;higher-order logic;π-calculus;temporal logic;computer science;intermediate logic;proof calculus;minimal logic;noncommutative logic;situation calculus;programming language;natural deduction;substructural logic;multimodal logic;algorithm	Logic	-11.886658739735449	22.885523809502377	59146
87f401ae5fa9c07d6d2e13cc68a83e513a95fa69	abstraction refinement for emptiness checking of alternating data automata		ion Refinement for Emptiness Checking of Alternating Data Automata Radu Iosif and Xiao Xu CNRS, Verimag, Université de Grenoble Alpes {Radu.Iosif,Xiao.Xu}@univ-grenoble-alpes.fr Abstract. Alternating automata have been widely used to model and verify systems that handle data from finite domains, such as communication protocols or hardware. The main advantage of the alternating model of computation is that complementation is possible in linear time, thus allowing to concisely encode trace inclusion problems that occur often in verification. In this paper we consider alternating automata over infinite alphabets, whose transition rules are formulae in a combined theory of booleans and some infinite data domain, that relate past and current values of the data variables. The data theory is not fixed, but rather it is a parameter of the class. We show that union, intersection and complementation are possible in linear time in this model and, though the emptiness problem is undecidable, we provide two efficient semi-algorithms, inspired by two state-ofthe-art abstraction refinement model checking methods: lazy predicate abstraction [8] and the Impact semi-algorithm [16]. We have implemented both methods and report the results of an experimental comparison. Alternating automata have been widely used to model and verify systems that handle data from finite domains, such as communication protocols or hardware. The main advantage of the alternating model of computation is that complementation is possible in linear time, thus allowing to concisely encode trace inclusion problems that occur often in verification. In this paper we consider alternating automata over infinite alphabets, whose transition rules are formulae in a combined theory of booleans and some infinite data domain, that relate past and current values of the data variables. The data theory is not fixed, but rather it is a parameter of the class. We show that union, intersection and complementation are possible in linear time in this model and, though the emptiness problem is undecidable, we provide two efficient semi-algorithms, inspired by two state-ofthe-art abstraction refinement model checking methods: lazy predicate abstraction [8] and the Impact semi-algorithm [16]. We have implemented both methods and report the results of an experimental comparison.	algorithm;alternating finite automaton;automata theory;data domain;encode;lazy evaluation;model checking;model of computation;predicate abstraction;production (computer science);re (complexity);refinement (computing);semiconductor industry;time complexity	Radu Iosif;Xiao Xu	2018		10.1007/978-3-319-89963-3_6	boolean data type;model checking;data domain;predicate abstraction;time complexity;discrete mathematics;undecidable problem;communications protocol;computer science;model of computation	Logic	-12.432643826111713	24.80268286504615	59189
797259fad89655e58f3031de0be6095ae66b3a6d	model checking propositional deontic temporal logic via a μ-calculus characterization	calculus characterization;model checking;calculus model checker;propositional deontic temporal logic;fault tolerant system;model checking purpose;deontic temporal logic;calculus model checking;model checking problem;corresponding decision procedure;case study	In this paper, we present a characterization of a propositional deontic temporal logic into μ-calculus. This logic has been proposed to specify and reason about fault tolerant systems, and even though is known to be decidable, no tool realizing its corresponding decision procedure has been developed. A main motivation for our work is enabling for the use of model checking, for analyzing specifications in this deontic temporal logic. We present the technical details involved in the characterization, and prove that the model checking problem on the deontic temporal logic is correctly reduced to μ-calculus model checking. We also show that counterexamples are preserved, which is crucial for our model checking purposes. Finally, we illustrate our approach via a case study, including the verification of some properties using a μ-calculus model checker.	decision problem;deontic logic;diode–transistor logic;fault tolerance;kripke structure (model checking);modal μ-calculus;model checking;pict;pspace;perl data language (pdl);physical address extension;relevance;temporal logic	Araceli Acosta;Cecilia Kilmurray;Pablo F. Castro;Nazareno Aguirre	2012		10.1007/978-3-642-33296-8_3	zeroth-order logic;model checking;linear temporal logic;interval temporal logic;computation tree logic;theoretical computer science;abstraction model checking;algorithm;temporal logic of actions	Logic	-11.781865454861165	23.558905346327627	59244
dafe8520c9e395997b1fceea3b1f618a2fe188d6	the minimal coverability graph for petri nets	reachable set;petri net	We present the unique minimal coverability graph for Petri nets. When the reachability graph of a Petri net is infinite, the minimal coverability graph allows as to decide the same problems as the well-known Karp-Miiler graph : the Finite Reaehability Tree Problem, the Finite Reachability Set Problem, the Boundedness Problem, the Quasi-Liveness Problem and the Regularity Problem. The algorithm given for computing the minimal coverabillty graph is based on a new optimization of the Karp and Miller procedure.	algorithm;liveness;mathematical optimization;petri net;reachability;whole earth 'lectronic link	Alain Finkel	1991		10.1007/3-540-56689-9_45	stochastic petri net;computer science;petri net	Logic	-4.549305234974294	22.71426827217745	59266
b1a9162b003b53a621d0b3b40de4e7c3b842f603	what topology tells us about diagnosability in partial order semantics	discrete event systems;diagnosis;petri nets;events;observability;partial order semantics;event structures	Abstract   From a partial observation of the behaviour of a labeled Discrete Event System,  fault Diagnosis  strives to determine whether or not a given “invisible” fault event has occurred. The  diagnosability problem  can be stated as follows: does the labeling allow for an outside observer to determine the occurrence of the fault, no later than a bounded number of events after that unobservable occurrence ? In concurrent systems, partial order semantics adds to the difficulty of the problem, but also provides a richer and more complex picture of observation and diagnosis. In particular, it is crucial to clarify the intuitive notion of  “time after fault occurrence “. To this end, we will use a unifying metric framework for event structures, providing a general topological description of diagnosability in both sequential and nonsequential semantics for Petri nets.		Stefan Haar	2010		10.3182/20100830-3-DE-4013.00036	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-7.855577797276328	23.735024544725945	59371
a7c833c7d5d4f6351c797c3b237bc30831c2838d	correctness of an incremental and worst-case optimal decision procedure for modal logic with eventualities		We present a simple theory explaining the construction and the correctness of an incremental and worst-case optimal decision procedure for modal logic with eventualities. The procedure gives an abstract account of important aspects of Goré and Widmann’s PDL prover. Starting from an input formula, the procedure grows a Pratt-style graph tableau until the tableau proves or disproves the satisfiability of the formula. The procedure provides a basis for practical provers since satisfiability and unsatisfiability of formulas can often be determined with small tableaux.	best, worst and average case;correctness (computer science);decision problem;knuth–morris–pratt algorithm;method of analytic tableaux;modal logic;perl data language (pdl)	Mark Kaminski;Gert Smolka	2011	CoRR		discrete mathematics;mathematics;algorithm	Logic	-9.201104104313906	18.487250388428738	59470
18e6836981bfd1c65c71a333bc8cf196851c9f4e	calculating correct compilers		In this article we present a new approach to the problem of calculating compilers. In particular, we develop a simple but general technique that allows us to derive correct compilers from high-level semantics by systematic calculation, with all the required compilation machinery falling naturally out of the calculation process. Our approach is based upon the use of standard equational reasoning techniques, and has been applied to calculate compilers for a wide range of language features and their combination, including arithmetic expressions, exceptions, local and global state, various forms of lambda calculi, bounded and unbounded loops, non-determinism, and interrupts. All the calculations have been mechanically verified using the Coq proof assistant.	arjan brussee;compiler;continuation;coq (software);correctness (computer science);exception handling;formal language;functional programming;high- and low-level;inductive reasoning;interrupt;lambda calculus;mathematical optimization;modularity (networks);nondeterministic algorithm;proof assistant;reasoning system;stepwise regression;structural induction;virtual machine	Patrick Bahr;Graham Hutton	2015	J. Funct. Program.	10.1017/S0956796815000180	a-normal form;computer science;theoretical computer science;programming language;algorithm	PL	-17.37185050928885	22.073467607892187	59622
4000da00472b46808912d121040b6a6b8fbbb16e	partial-order reduction techniques for real-time model checking	systeme temps reel;methode discretisation;relation ordre partiel;metodo reduccion;complexite calcul;real time;algorithme;algorithm;metodo discretizacion;complejidad computacion;model checking;computational complexity;partial ordering;partial order reduction;discretization method;real time system;methode reduction;sistema tiempo real;relacion orden parcial;reduction method;real time systems;algoritmo	A new notion, covering, generalising independence is introduced. It enables improved effects of partial-order reduction techniques when applied to real-time systems. Furthermore, we formulate a number of locally checkable conditions for covering that can be used as the basis for a practical algorithm. Correctness is proven with respect to a chosen discretisation method.	algorithm;automata theory;büchi automaton;correctness (computer science);depth-first search;discretization;lambda lifting;model checking;partial order reduction;real-time clock;real-time computing;real-time transcription;state space;timed text;triune continuum paradigm	Dennis Dams;Rob Gerth;Bart Knaack;Ruurd Kuiper	1998	Formal Aspects of Computing	10.1007/s001650050028	partially ordered set;model checking;partial order reduction;real-time operating system;computer science;calculus;mathematics;computational complexity theory;algorithm	Logic	-9.59255810664039	25.298228733743237	59644
7c906424595ed90c2494dc6f42e4ca6f0bae6402	the extended probabilistic powerdomain monad over stably compact spaces	modelizacion;metric space;espace metrique;relation ordre partiel;espacio metrico;logica monadica;semantics;programmation stochastique;probabilistic approach;compact space;espace compact;semantica;semantique;homomorphism;modelisation;espacio compacto;espace probabilite;theorie equationnelle;mesure probabilite;partial ordering;enfoque probabilista;approche probabiliste;logique monadique;homomorphisme;espacio probabilidad;monadic logic;relacion orden parcial;homomorfismo;stochastic programming;building model;modeling;probability space;probability measure;programacion estocastica;equational theory;medida probabilidad;teoria ecuacional;partial order	For the semantics of probabilistic features in programming mainly two approaches are used for building models. One is the Giry monad of Borel probability measures over metric spaces, and the other is Jones’ probabilistic powerdomain monad [6] over dcpos (directed complete partial orders). This paper places itself in the second domain theoretical tradition. The probabilistic powerdomain monad is well understood over continuous domains. In this case the algebras of the monad can be described by an equational theory [6, 9, 5]. It is the aim of this work to obtain similar results for the (extended) probabilistic powerdomain monad over stably compact spaces. We mainly want to determine the algebras of this powerdomain monad and the algebra homomorphisms.	monad (functional programming);power domains;spaces	Ben Cohen;Martín Hötzel Escardó;Klaus Keimel	2006		10.1007/11750321_54	partially ordered set;combinatorics;discrete mathematics;topology;mathematics;semantics;algorithm	Logic	-10.760742568193889	18.559167702138385	59702
ac847f49b4c1eed3cf99ac4bb3fd9ede1ba0867f	on the semantics of place/transition petri nets	petri net	Place/Transition (PT) Petri nets are one of the most widely u sed models of concurrency. However, they still lack, in our view, a satisfactory semantics: on th e one hand the ‘token game’ is too intensional, even in its more abstract interpretations in t erms of nonsequential processes and monoidal categories; on the other hand, Winskel’s basic unf olding construction, which provides a coreflection between nets and finitary prime algebraic domai ns, works only for safe nets. In this paper we extend Winskel’s result to PT nets. We start w ith a rather general category PTNets of PT nets, we introduce a category DecOcc of decorated (nondeterministic) occurrence nets and we define adjunctions between PTNets andDecOcc and betweenDecOcc andOcc, the category of occurrence nets. The role of DecOcc is to provide natural unfoldings for PT nets, i.e., acyclic s afe nets where a notion of family is used for relating multiple in stances of the same place. The unfolding functor fromPTNets toOcc reduces to Winskel’s when restricted to safe nets; moreover, the standard coreflection between Occ andDom, the category of finitary prime algebraic domains, when composed with the unfolding functor above, de termines a chain of adjunctions betweenPTNets andDom.	causal filter;computation;concurrency (computer science);diagram;dijkstra's algorithm;directed acyclic graph;intensional logic;interaction;linear algebra;map;norm (social);one-to-one (data model);panorama tools;petri net;relevance;sed;smoothing;trace monoid;turing completeness;unfolding (dsp implementation);unification (computer science)	José Meseguer;Ugo Montanari;Vladimiro Sassone	1997	Mathematical Structures in Computer Science	10.1017/S0960129597002314	combinatorics;discrete mathematics;computer science;mathematics;petri net;algorithm	Logic	-8.725352638786614	20.559623234746205	59840
a6fcb2430eb50715b51647a819728b4a3f77151a	semantic models for total correctness and fairness	semantic model	Assertional s-rings are introduced to provide an algebraic setting in which the finite and infinite behavior of nondeterministic  programs can be expressed and reasoned about. This includes expressing the fair infinite behavior of nondeterministic iterative  programs, and reasoning about termination under various fairness assumptions. We also address the question of when the reasoning  techniques are semantically complete.  	correctness (computer science);fairness measure	Michael G. Main;David L. Black	1989		10.1007/BFb0040261	natural language processing;theoretical computer science;database	Theory	-12.516886781255556	20.810405542679206	59907
17137aa000832854f3934bae0c688267b5f118ab	supervision based on place invariants: a survey	supervisory control;fault tolerant;linear constraint;satisfiability;mutual exclusion;petri nets;supervision based on place invariants;petri net;literature survey;reachability analysis	The supervision based on place invariants (SBPI) is an efficient technique for the supervisory control of Petri nets. This paper reveals the significance of the SBPI based on a literature survey, applications, and an analysis of problems and supervisory settings that can be addressed using SBPI. Special attention is given to the various settings within which the problem can be formulated. Such settings can be distinguished based on the concurrency type, the type of controllability and observability, and the centralized or decentralized type of supervision. As we show, it is possible to approach the most general settings in a purely structural way, without resorting to reachability analysis. We begin by describing the SBPI problem and the literature methods that address this problem or are related to it. Then, we proceed to show classes of problems that can be reduced to the SBPI problem. In the SBPI, the specification is described as a system of inequalities that the Petri net marking must satisfy at any time. However, as we show, problems involving more general specifications can be approached in the SBPI setting, sometimes under additional assumptions, by performing net and/or specification transformations. Four of the specifications we will consider are logic constraints, language specifications, disjunctions of linear constraints, and liveness. We conclude with a presentation of possible applications of the SBPI approach to programming with semaphores, fault tolerance, and synchronic-distance based designs.	centralized computing;concurrency (computer science);fault tolerance;ibm notes;item unique identification;linear inequality;liveness;matlab;petri net;problem solving;reachability	Marian V. Iordache;Panos J. Antsaklis	2006	Discrete Event Dynamic Systems	10.1007/s10626-006-0021-9	control engineering;discrete mathematics;computer science;control theory;distributed computing;programming language;petri net;algorithm	PL	-6.339195834126841	26.874506963557483	59971
4e05a826f54a272b6e05301493004d1977dce588	combinations of simplifying conditional term rewriting systems	rewrite rule;left hand side;right hand side;term rewrite system	A conditional term rewriting system (CTRS) is called simplifying if there exists a simpliication ordering > on terms such that the left-hand side of any rewrite rule is greater than the right-hand side and the terms occurring in the conditions of that rule. If a simplifying join CTRS consists of nitely many rules, it is terminating and the applicability of a rewrite rule is decidable by recursively reducing the terms in the conditions. Consider two nite CTRSs R1 and R2 which may share constructors (symbols that do not occur at the root position of the left-hand side of any rewrite rule) but no other function symbols. It will be shown that the combined CTRS R = R1R2 is simplifying if and only if R1 and R2 are simplifying. Moreover, connuence is a modular property of nite simplifying join CTRSs.	newman's lemma;recursion;rewrite (programming);rewriting;semiconductor industry	Enno Ohlebusch	1992		10.1007/3-540-56393-8_8	arithmetic;discrete mathematics;mathematics;algorithm	AI	-10.179440874596905	18.267936958287628	60170
1887440044ec6d00f954364c4c69bb75187b25ab	compositional semantics for open petri nets based on deterministic processe	algebraic specification;computacion informatica;data type;concurrent systems;ciencias basicas y experimentales;inf 01 informatica;matematicas;grupo a;petri net;composition operator	In order to model the behaviour of open concurrent systems by means of Petri nets, we introduceopen Petri nets , a generalization of the ordinary model where some places, designated as open, represent an interface of the system towards the environment. Besides generalizing the token game to reflect this extension, we define a truly concurrent semantics for open nets by extending the Goltz-Reisig process semantics of Petri nets. We introduce a composition operation over open nets, characterized as a pushout in the corresponding category, suitable to model both interaction through open places and synchronization of transitions. The deterministic process semantics is shown to be compositional with respect to such composition operation. If a net Z3 results as the composition of two netsZ1 andZ2, having a common subnet Z0, then any two deterministic processes of Z1 and Z2 which “agree” on the common part, can be “amalgamated” to produce a deterministic process ofZ3. Vice versa, any deterministic process of Z3 can be decomposed into processes of the component nets. The amalgamation and decomposition operations are shown to be inverse to each other, leading to a bijective correspondence between the deterministic processes of Z3 and pair of deterministic processes of Z1 andZ2 which agree on the common subnetZ0. Technically, our result is similar to the amalgamation theorem for data-types in the framework of algebraic specification. A possible application field of the proposed constructions and results is the modeling of interorganizational workflows, recently studied in the literature. This is illustrated by a running example.	concurrency (computer science);expect;facebook platform;graph rewriting;high-level programming language;interaction;julia set;linear algebra;petri net;process (computing);subnetwork;unfolding (dsp implementation);z1 (computer);z2 (computer);z3 (computer)	Paolo Baldan;Andrea Corradini;Hartmut Ehrig;Reiko Heckel	2005	Mathematical Structures in Computer Science	10.1017/S0960129504004311	discrete mathematics;data type;computer science;composition operator;theoretical computer science;mathematics;programming language;petri net;algorithm	Logic	-9.436016238543457	21.134612504770416	60520
9fe9e5fec3dcf749a913c1c8c1208a372861d582	beyond iteration vectors: instancewise relational abstract domains	fonction rationnelle;compilacion;programming language semantics;monoid;storage access;programme commande;analyse statique;temps polynomial;programacion automatica;structure arborescente;metodo arborescente;localization;nested loops;metodo formal;methode formelle;dependence;regional analysis;localizacion;dependance;interpretacion abstracta;automatic programming;program verification;linear constraint;analisis estatica;analisis programa;formal method;nested control;monoide;verificacion programa;induccion;formal reasoning;localisation;induction;estructura arborescente;control program;tree structure;estructura datos;control flow;acces memoire;control imbricado;polynomial time;paralelizacion automatica;acceso memoria;compilation;programme recursif;programa mando;tree structured method;commande rapprochee;structure donnee;methode arborescente;program analysis;parallelisation automatique;programa recursivo;recursive program;interpretation abstraite;analyse programme;funcion racional;static analysis;data flow;abstract interpretation;verification programme;machine model;rational function;data structure;automatic parallelization;programmation automatique;dependencia;constant propagation;tiempo polinomial	We introduce a formalism to reason about program properties at an infinite number of runtime control points, called instances. Infinite sets of instances are represented by rational languages. This framework gives a formal foundation to the well known concept of iteration vectors, extending it to recursive programs with any structured control flow (nested loops and recursive calls). We also extend the concept of induction variables to recursive programs. For a class of monoid-based data structures, including arrays and trees, induction variables capture the exact memory location accessed at every step of the execution. This compile-time characterization is computed in polynomial time as a rational function. Applications include dependence and region analysis for array and tree algorithms, array expansion, and automatic parallelization of recursive programs.	abstract interpretation;iteration	Pierre Amiranoff;Albert Cohen;Paul Feautrier	2006		10.1007/11823230_11	program analysis;time complexity;data flow diagram;rational function;internationalization and localization;nested loop join;data structure;computer science;theoretical computer science;monoid;tree structure;programming language;control flow;static analysis;constant folding;algorithm;recursive partitioning;automatic parallelization	Logic	-18.712410481910855	24.591376647679322	60902
33f5769852b314ccaf599f30273d720b3a464d7b	a note on the computational complexity of the pure classical implication calculus	automatic proving;programmation;complexite calcul;complejidad calculo;logique propositionnelle;computing complexity;demostracion automatica;programacion;theorem proving;demonstration automatique;demonstration theoreme;computational complexity;propositional logic;informatique theorique;logica proposicional;demostracion teorema;programming;computer theory;informatica teorica	In this paper we give an interpretakion of classical propositional logic in the pure implicational fragment of classical logic. We do this by giving a polynomial time bowadd reduction from arbitrary formulas built up from propositional variables and the set of co~ectives (-, &, V , -> } to formulas built up from only propositional variables and the --) -sign, such that .A is valid if and only if the reduction of A is valid. ~IWUL Each formula can be reduced (in polynomial time) to a logically equivalent form& of the shape A or the shape -A where A is negation-free, through the following transformations:	computational complexity theory;propositional calculus;propositional variable;reduction (complexity);time complexity	Gunnar Stålmarck	1989	Inf. Process. Lett.	10.1016/0020-0190(89)90086-0	dynamic logic;zeroth-order logic;complete theory;programming;discrete mathematics;classical logic;relevance logic;many-valued logic;intuitionistic logic;computer science;intermediate logic;mathematics;minimal logic;automated theorem proving;propositional variable;well-formed formula;propositional calculus;computational complexity theory;algorithm;autoepistemic logic;satisfiability	AI	-8.088340680812896	18.709552457660003	61002
43fe1203a2b39456179a6647f4c6c31268ce3a74	a theoretical framework for the declarative debugging of functional logic programs with lambda abstractions	declarative programming;rewrite rule;theoretical framework;higher order;first order;rewrite systems;rewriting logic;functional logic programming	In this paper, we extend the well-known Naish's declarative debugging scheme for diagnosing wrong computed answers in first-order lazy functional-logic programs to the higher-order setting of the simply typed λ-calculus, where programs are presented by conditional pattern rewrite systems. Our approach generalizes and combines declarative debugging techniques previously developed for less expressive declarative programming paradigms involving applicative rewrite rules instead of λ-abstractions and decidable higher-order unification. Debugging starts with the observation of a wrong computed answer which the user regards as incorrect w.r.t. an intended model that provides a declarative description of the program's semantics. Debugging proceeds by exploring an abridged proof tree built on a higher-order rewriting logic with λ-abstractions that provides a purely declarative view of the computation. Finally, debugging ends with the detection of a defined function rule in the program that is incorrect w.r.t. the intended model. We prove the logical correctness of the debugging method for any sound goal solving system whose computed answers are logical consequences of the program.	algorithmic program debugging	Rafael del Vado Vírseda;Ignacio Castiñeiras	2009		10.1007/978-3-642-11999-6_11	declarative programming;higher-order logic;rewriting;computer science;theoretical computer science;functional logic programming;first-order logic;algorithmic program debugging;programming language;algorithm	PL	-17.660877261070453	21.84169522079107	61012
dd066371f76d1e78117c7e452c7d51871b25b517	controllability and cooperativeness analysis for automatic abstraction refinement	controllability;abstraction refinement;cooperativeness;formal verification	We present a new abstraction refinement algorithm to better refine the abstract model for formal property verification. In previous work, refinements are selected either based on a set of counter examples of the current abstract model, as in [5, 6, 7, 8, 9, 20, 21], or independent of any counter examples, as in [18]. We (1) introduce a new controllability analysis that is independent of any particular counter examples, (2) apply a new cooperativeness analysis that extracts information from a particular set of counter examples and (3) combine both to better refine the abstract model. We implemented the algorithm and applied it to verify several real-world designs and properties. We compared the algorithm against the abstraction refinement algorithms in [20] and [21] and the interpolation-based reachability analysis in [15]. The experimental results indicate that the new algorithm outperforms the other three algorithms in terms of runtime, abstraction efficiency (as defined in [20]) and the number of proven properties.	refinement (computing)	Freddy Y. C. Mang;Pei-Hsin Ho	2006	Int. J. Found. Comput. Sci.	10.1142/S0129054106004091	discrete mathematics;controllability;formal verification;computer science;theoretical computer science;mathematics;programming language;algorithm	Logic	-17.389978635611843	27.8197913056674	61139
57f6830a95e37ea95ad5abfb6d44fbe263b04753	a model for timed-probabilistic behaviors	systeme temps reel;no determinismo;communication process;chaine markov;cadena markov;protocole transmission;teoria sistema;semantics;semantica;semantique;equivalence;proceso comunicacion;probabilistic model;processus communication;protocolo transmision;non determinism;systems theory;non determinisme;theorie systeme;modele probabiliste;real time system;sistema tiempo real;equational theory;equivalencia;modelo probabilista;markov chain;transmission protocol			Ming Fang;Hussein Zedan;Chris Ho-Stuart	1995	Journal of Systems and Software	10.1016/0164-1212(94)00059-V	equivalence;statistical model;markov chain;artificial intelligence;semantics;systems theory;algorithm;statistics	Logic	-9.384879782109556	26.548981440893026	61149
a926b6606c3a069f99af63ad7637ace6e4a3dc91	extended feature algebra	algebraic characterisation of fosd;feature orientation;feature algebra	Feature Algebra was introduced as an abstract framework for feature-oriented software development. One goal is to provide a common, clearly defined basis for the key ideas of feature-orientation. The algebra captures major aspects of feature-orientation, such as the hierarchical structure of features and feature composition. However, as we will show, it is not able to model aspects at the level of code, i.e., situations where code fragments of different features have to be merged. In other words, it does not reflect details of concrete implementations.#R##N##R##N#In this paper we first present concrete models for the original axioms of Feature Algebra which represent the main concepts of feature-oriented programs. This shows that the abstract Feature Algebra can be interpreted in different ways. We then use these models to show that the axioms of Feature Algebra do not properly reflect all aspects of feature-orientation from the level of directory structures down to the level of actual code. This gives motivation to extend the abstract algebra, which is the second main contribution of the paper. We modify the axioms and introduce the concept of an Extended Feature Algebra. As third contribution, we introduce more operators to cover concepts like overriding in the abstract setting.		Peter Höfner;Bernhard Möller	2016	J. Log. Algebr. Meth. Program.	10.1016/j.jlamp.2015.12.002	feature recognition;boolean algebra;term algebra;computer science;algebraic structure;pure mathematics;mathematics;algorithm	DB	-13.842984060407508	19.341971846906578	61464
7a068dfc3f5cb041690e3205ce3596bda41ec997	temporal preconditions of recursive procedures	predicate transformer;weakest precondition	"""The meaning of an imperative program is defined to be the precondition of the executions as a function of proposed behaviour. In the case of Dijkstra's weakest precondition, the proposed behaviour is termination in a state with a given postcondition. For the temporal predicate transformers of Lukkien, the proposed behaviour is specified in terms of predicates on the intermediate states. For example, for a command c and predicates p, q and r, the predicate wto.p.q.c.r is the precondition such that, for every execution sequence of c, a state in which p holds is eventually followed by a state in which q holds or by termination in a state in which r holds. We present these precondition functions for a language with operators for sequential composition, unbounded demonic choice and recursive procedures. Recursion is interpreted by means of extreme fixpoints. The treatment of """"eventually"""" is a straightforward generalization of the ordinary wp-calculus. For the treatment of """"leads-to"""", the new concept of accumulator turns out to be useful. The proofs of Lukkien's healthiness laws lead to insights in fixpoint induction. Some of the laws require the recursion to be guarded. It is shown that unfolding of the declaration preserves the semantics. Eeywords: weakest precondition, recursive procedure, leads-to, eventually, healthiness law, guarded recursion, unfolding."""	accumulator (computing);declaration (computer programming);fixed point (mathematics);imperative programming;inductive reasoning;postcondition;precondition;predicate transformer semantics;process calculus;recursion (computer science);transformers;unfolding (dsp implementation)	Wim H. Hesselink;Ronald Reinds	1992		10.1007/3-540-56596-5_36	precondition;natural language processing;predicate transformer semantics;programming language	PL	-16.952059499804726	21.39337667019197	61583
8e667884027410100352a7d3f5796405f25c5803	well quasi-orders in computer science (dagstuhl seminar 16031)	004;better quasi order well quasi order hierarchy infinite state machines logic noetherian space reducibility termination topological complexity	"""This report documents the program and the outcomes of Dagstuhl Seminar 16031 """"Well Quasi-Orders in Computer Science"""", the first seminar devoted to the multiple and deep interactions between the theory of Well quasi-orders (known as the Wqo-Theory) and several fields of Computer Science (Verification and Termination of Infinite-State Systems, Automata and Formal Languages, Term Rewriting and Proof Theory, topological complexity of computational problems on continuous functions). Wqo-Theory is a highly developed part of Combinatorics with ever-growing number of applications in Mathematics and Computer Science, and Well quasi-orders are going to become an important unifying concept of Theoretical Computer Science. In this seminar, we brought together several communities from Computer Science and Mathematics in order to facilitate the knowledge transfer between Mathematicians and Computer Scientists as well as between established and younger researchers and thus to push forward the interaction between Wqo-Theory and Computer Science."""	computer science	Jean Goubault-Larrecq;Monika Seisenberger;Victor L. Selivanov;Andreas Weiermann	2016	Dagstuhl Reports	10.4230/DagRep.6.1.69	computer science;pure mathematics;mathematics;algorithm	Theory	-4.553028539449143	24.602153365265675	61634
c8fa3e6b1c47fa6302e5b981ec2c35256df364ac	widening for control-flow		We present a parameterized widening operator that determines the control-flow sensitivity of an analysis, i.e., its flow-sensitivity, context-sensitivity, and path-sensitivity. By instantiating the operator’s parameter in different ways, the analysis can be tuned to arbitrary sensitivities without changing the abstract semantics of the analysis itself. Similarly, the analysis can be implemented so that its sensitivity can be tuned without changing the analysis implementation. Thus, the sensitivity is an independent concern, allowing the analysis designer to design and implement the analysis without worrying about its sensitivity and then easily experiment with different sensitivities after the fact. Additionally, we show that the space of control-flow sensitivities induced by this widening operator forms a lattice. The lattice meet and join operators are the product and sum of sensitivities, respectively. They can be used to automatically create new sensitivities from existing ones without manual effort. The sum operation in particular is a novel construction, which creates a new sensitivity less precise than either of its operands but containing elements of both.	combinatorial optimization;control flow;instance (computer science);mathematical optimization;operand;program analysis;sensitivity and specificity;separation of concerns;state space	Ben Hardekopf;Ben Wiedermann;Berkeley R. Churchill;Vineeth Kashyap	2014		10.1007/978-3-642-54013-4_26	algorithm	SE	-16.06399968039498	23.71118221791861	61669
bfc95be4c04e8f3b8c0efc05f9272ad832a4eafe	a generic framework for symbolic execution:theory and applications : theory and applications. (un cadre générique pour exécution symbolique / un cadre générique pour exécution symbolique : theorie et applications)		The modern world is shifting from the traditional workmanship to a more automated work environment, where software systems are increasingly used for automating, controlling and monitoring human activities. In many cases, software systems appear in critical places which may immediately affect our lives or the environment. Therefore, the software that runs on such systems has to be safe. This requirement has led to the development of various techniques to ensure software safety. In this dissertation we present a language-independent framework for symbolic execution, which is a particular technique for testing, debugging, and verifying programs. The main feature of this framework is that it is parametric in the formal definition of a programming language. We formally define programming languages and symbolic execution, and then we prove that the feasible symbolic executions of a program and the concrete executions of the same program mutually simulate each other. This relationship between concrete and symbolic executions allow us to perform analyses on symbolic programs, and to transfer the results of those analyses to concrete instances of the symbolic programs in question. We use our symbolic execution framework to perform program verification using Hoare Logic and Reachability Logic. For the latter, we propose an alternative proof system, and we show that under reasonable conditions, a certain strategy executing our proof system is sound and weakly complete. A prototype implementation of our symbolic execution framework has been developed in K. We illustrate it on the symbolic execution, model checking, and deductive verification of nontrivial programs.	apl;debugging;deductive database;formal verification;hoare logic;language-independent specification;model checking;programming language;proof calculus;prototype;reachability;simulation;software system;symbolic execution;verification and validation	Andrei Arusoaie	2014				SE	-18.69299278621527	22.733887951711772	61670
3511bf8e86a1728d03d6b5c1d0c4497c4eba7059	using simulated execution in verifying distributed algorithms	program behavior;distributed system;systeme reparti;analyse statique;distributed consensus;invariant detection;comportamiento programa;program verification;safety properties;theorem proving;demonstration theoreme;theorem prover;verificacion programa;detection invariant;formal verification;sistema repartido;algorithme reparti;comportement programme;verification formelle;algoritmo repartido;proof of correctness;demostracion teorema;static analysis;verification programme;distributed algorithm;dynamic analysis	This paper presents a methodology for using simulated execution to assist a theorem prover in verifying safety properties of distributed systems. Execution-based techniques such as testing can increase confidence in an implementation, provide intuition about behavior, and detect simple errors quickly. They cannot by themselves demonstrate correctness. However, they can aid theorem provers by suggesting necessary lemmas and providing tactics to structure proofs. This paper describes the use of these techniques in a machine-checked proof of correctness of the Paxos algorithm for distributed consensus .	automated theorem proving;consensus (computer science);correctness (computer science);distributed algorithm;distributed computing;invariant (computer science);paxos (computer science);run time (program lifecycle phase);sanity check;test suite;tracing (software);unipro;verification and validation	Toh Ne Win;Michael D. Ernst;Stephen J. Garland;Dilsun Kirli Kaynar;Nancy A. Lynch	2003	International Journal on Software Tools for Technology Transfer	10.1007/s10009-003-0126-5	distributed algorithm;computer science;theoretical computer science;distributed computing;automated theorem proving;programming language;algorithm	SE	-16.860801807881433	28.179218670225897	61722
ca754e25bec239ec0e7c67eb6bc5fb659e65f517	formal design of an abstract machine for constraint logic programming	logic programming;specifying and verifying about programs;abstract machine	By studying properties of CLP over an unspecified constraint domain X one obtains general results applicable to all instances of CLP(X ). The purpose of this paper is to study a general implementation scheme for CLP(X ) by designing a generic extension WAM(X ) of the WAM and a corresponding generic compilation scheme of CLP(X ) programs to WAM(X ) code which is based on Börger and Rosenzweig’s WAM specification and correctness proof. Thus, using the evolving algebra specification method, we obtain not only a formal description of our WAM(X ) scheme, but also a mathematical correctness proof for the design.∗ Keyword Codes: D.1.6, F.3.1	compiler;constraint logic programming;correctness (computer science);warren abstract machine	Christoph Beierle	1994			constraint logic programming;bunched logic;constraint programming;correctness;concurrent constraint logic programming;proof calculus;formal verification;signature (logic);algorithm;computer science	PL	-18.638578104464706	20.475701329159076	62036
fe638ebb016f0ebb0b8b7867b00d5116b54f8ca4	a noninterleaving model of concurrency based on transition systems with spatial structure	spatial structure;transition system;transition systems;coalgebra;petri nets;spatial logic;petri net	In an attempt to devise a general notion of model for spatial logic, we have been led to consider transition systems with an additional so-called spatial structure on the states, with both the transition and the spatial structures described in coalgebraic terms. In this paper we argue that such transition systems with spatial structure can be seen as a noninterleaving model of concurrency, by providing translations to and from a certain category of Petri nets.	concurrency (computer science);petri net	Luís Monteiro	2004	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.02.035	discrete mathematics;computer science;theoretical computer science;petri net;algorithm	DB	-10.203308219219922	21.6879625803444	62064
e87aa5dd5e63695c0b29b74525279761627cbf99	a formal representation for state diagrams in the omt methodology	abstract machine;state diagram;state transition	The paper proposes a formalization of the object and dynamic OMT models in terms of the Abstract Machine model and shows how certain relevant properties, such as whether a state is reachable, there are non-deterministic state transitions or cyclic state transitions, can be detected by using the proof mechanisms provide for Abstract Machine.	diagram;uml state machine	Elisa Bertino;Donatella Castelli;Federica Vitale	1996		10.1007/BFb0037414	state diagram;state diagram;interaction overview diagram;computer science;abstract machine;programming language;algorithm	HCI	-10.316027734456343	22.616296645837103	62144
469bc5316951eacadea7ea03417f14a527015c0c	complete testing from a stream x-machine specification	verification;testing;finite state machines;test generation;stream x machines	One of the strengths of using a stream X-machine to specify a system is that, under certain well defined conditions, it is possible to produce a test set that is guaranteed to determine the correctness of the implementation. This testing method assumes that the processing functions are correctly implemented, therefore it only tests the integration of the processing functions implementations into the system implementation. This paper uses a case study to illustrate how this method can be extended so that it will no longer require the implementations of the processing functions to be proved correct before the actual system testing can take place. Instead, the testing of the processing functions is performed along with the integration testing.	stream x-machine	Florentin Ipate;Mike Holcombe	2005	Fundam. Inform.		black-box testing;parallel computing;real-time computing;model-based testing;verification;orthogonal array testing;white-box testing;integration testing;computer science;functional testing;dynamic testing;software testing;system under test;finite-state machine;system testing;algorithm	SE	-16.527161454275788	30.916744390166535	62330
0061d50db46a06422fe3b7433918d9a39a79b74f	game-based abstraction for markov decision processes	mathematics computing;probability;zeroconf dynamic network configuration protocol;lower and upper bound;stochastic games formal verification markov processes mathematics computing probability reachability analysis;probabilistic behaviour;modelling systems;qa75 electronic computers computer science;formal verification;model checking;nondeterministic behaviour;game based abstraction;quantitative analysis;zeroconf dynamic network configuration protocol game based abstraction markov decision processes modelling systems model checking probabilistic behaviour nondeterministic behaviour;markov processes;markov decision process;state space explosion;markov decision processes;reachability analysis;stochastic games	In this paper we present a novel abstraction technique for Markov decision processes (MDPs), which are widely used for modelling systems that exhibit both probabilistic and nondeterministic behaviour. In the field of model checking, abstraction has proved an extremely successful tool to combat the state-space explosion problem. In the probabilistic setting, however, little practical progress has been made in this area. We propose an abstraction method for MDPs based on stochastic two-player games. The key idea behind this approach is to maintain a separation between nondeterminism present in the original MDP and nondeterminism introduced through abstraction, each type being represented by a different player in the game. Crucially, this allows us to obtain distinct lower and upper bounds for both the best and worst-case performance (minimum or maximum probabilities) of the MDP. We have implemented our techniques and illustrate their practical utility by applying them to a quantitative analysis of the Zeroconf dynamic network configuration protocol	best, worst and average case;markov chain;markov decision process;model checking;state space;zero-configuration networking	Marta Z. Kwiatkowska;Gethin Norman;David Parker	2006	Third International Conference on the Quantitative Evaluation of Systems - (QEST'06)	10.1109/QEST.2006.19	markov decision process;discrete mathematics;computer science;theoretical computer science;machine learning;abstraction model checking;statistics	Logic	-10.735280087975465	28.491843992760828	62426
af5ea2dc5c606ad6376c906aaab88e2509b1d0cd	extending sized type with collection analysis	termination analysis;polymorphism;program optimization;recursive data structure;data structure;fixed point	Many program optimizations and analyses, such as array-bounds checking, termination analysis, depend on knowing the size of a function's input and output. However, size information can be difficult to compute. Firstly, accurate size computation requires detecting a size relation between different inputs of a function. Secondly, size information may also be contained inside a collection (data structure with multiple elements). In this paper, we introduce some techniques to derive universal and existential size properties over collections of elements of recursive data structures. We shall show how a mixed constraint system could support the enhanced size type, and highlight examples where collection analysis are useful.	bounds checking;computation;constraint logic programming;constraint satisfaction problem;data structure;input/output;omega;program optimization;recursion;sensor;termination analysis;type inference;type system	Wei-Ngan Chin;Siau-Cheng Khoo;Dana N. Xu	2002		10.1145/777388.777397	polymorphism;mathematical optimization;data structure;computer science;recursive data type;theoretical computer science;program optimization;termination analysis;fixed point;programming language;algorithm	PL	-18.468877993458953	24.259203333968117	62538
ad0d615b72dd3d62db067c6709bad86eafd89e49	deadlock-free scheduling for timed petri net models combined with mpc and backtracking		This paper proposes algorithms that incrementally compute control sequences driving the marking of timed Petri nets from an initial value to a reference one, avoiding forbidden and dangerous states with a duration that approaches the minimal value. The proposed algorithms are applicable to a large class of discrete event systems in particular in the domain of flexible manufacturing, communication and computer science or transportation and traffic. To overcome the most burdensome part of the computations, only the sequences encoded in a small area of the reachability graph are worked out. The main contributions are to propose an estimation of the minimal duration of the remaining sequences to the reference based on the computation of the firing count vectors and a progressive search of the forbidden and dangerous states according to a backtracking phase. The approach is suitable for deadlock-free scheduling problems.	algorithm;approximation;backtracking;computation;computer science;deadlock;hybrid system;item unique identification;iterative method;petri net;reachability;refinement (computing);scheduling (computing)	Dimitri Lefebvre	2016	2016 13th International Workshop on Discrete Event Systems (WODES)	10.1109/WODES.2016.7497889	real-time computing;computer science;distributed computing;algorithm	AI	-7.363189774440163	29.010681518565992	62568
300e2353bf18a057d871d8d6c33ada1145531bd1	a hoare logic for the coinductive trace-based big-step semantics of while	hoare logic	In search for a foundational framework for reasoning about observable behavior of programs that may not terminate, we have previously devised a trace-based big-step semantics for While. In this semantics, both traces and evaluation (relating initial states of program runs to traces they produce) are defined coinductively. On terminating runs, it agrees with the standard inductive state-based semantics. Here we present a Hoare logic counterpart of our coinductive trace-based semantics and prove it sound and complete. Our logic subsumes both the partial correctness Hoare logic and the total correctness Hoare logic: they are embeddable. Since we work with a constructive underlying logic, the range of expressible program properties has a rich structure; in particular, we can distinguish between termination and nondivergence, e.g., unbounded total search fails to be terminating but is nonetheless nondivergent. Our metatheory is entirely constructive as well, and we have formalized it in Coq.	coinduction;hoare logic;operational semantics	Keiko Nakata;Tarmo Uustalu	2010		10.1007/978-3-642-11957-6_26	dynamic logic;linear logic;discrete mathematics;higher-order logic;separation logic;computer science;bunched logic;mathematics;predicate transformer semantics;hoare logic;programming language;axiomatic semantics;well-founded semantics;multimodal logic;algorithm	Logic	-13.093397736462096	21.244866382604066	62802
19a924f26581015a84eb3961c88bc5f35f99ad83	problems concerning fairness and temporal logic for conflict-free petri nets	resolucion conflicto;red petri;temporal logic;concurrent program;simultameidad;simultaneite;modelo;concurrency;resolution conflit;informatique theorique;modificacion;programa competidor;decidibilidad;modele;decidabilite;petri net;conflict resolution;models;logique temporelle;reseau petri;programme concurrent;decidability;computer theory;modification;lofica temporal;informatica teorica	In this paper, we examine the complexity of the fair nontermination problem for conflict-free Petri nets under several definitions of fairness. For each definition of fairness, we are able to show the problem to be complete for either NP, PTIME, or NLOGSPACE. We then address the question of whether these results extend to the more general model checking problem with respect to the temporal logic for Petri nets introduced by Suzuki. Since many of the model checking problems concerning finite state systems can be reduced to a version of the fair nontermina;ion problem, it would seem plausible that the model checking problem for conflict-free Petri nets would be decidable. However, it turns out that unless the logic is severely restricted, model checking is undecidable for conflict-free Petri nets. In particular, the problem is undecidable even when formulas are of the form Gf (“invariantly f **) where f contains no temporal logic operators. On the other hand, we show that model checking for conflict-free Petri nets is NP-complete for i(F, X)-the logic restricted to the operators F (eventually), X (next time), A, and v, with negations allowed only on the predicates.	divergence (computer science);fairness measure;model checking;np (complexity);np-completeness;p (complexity);petri net;temporal logic;undecidable problem	Rodney R. Howell;Louis E. Rosier	1989	Theor. Comput. Sci.	10.1016/0304-3975(89)90053-4	decidability;discrete mathematics;concurrency;temporal logic;computer science;artificial intelligence;conflict resolution;mathematics;petri net;algorithm	Logic	-8.07957196208009	20.7550356375232	62890
a9dbb373f1ffddb291671560f468d400e8da8f43	natural-semantics-based abstract interpretation (preliminary version)	programming language;formal semantics;data flow analysis;abstract interpretation	The original formulation of abstract interpretation (a.i.) [5] demonstrated clearly that a.i. is a formal-semantics-based methodology for deriving a provably correct, convergent, canonical iterative data flow analysis from a standard semantics of a programming language. But subsequent research in a.i. has obscured the methodology of the topic. For example, the recent slew of papers on closures analysis [2, 3, 17, 18, 21, 37, 39, 40, 41, 42, 43] mix implementation optimizations with specifications and leave unclear exactly what closures analysis is. In this paper, we reexamine the principles of a.i. and reformulate the topic on a foundation of coinductively defined natural semantics. We aim to demonstrate that the intensional and compositional aspects of natural semantics make it an ideal vehicle for formulating abstract interpretations of problems while preserving the essential characteristics of the subject.	abstract interpretation;operational semantics	David A. Schmidt	1995		10.1007/3-540-60360-3_28	natural language processing;formal system;formal semantics;formal verification;syntax;computer science;data-flow analysis;formal semantics;programming language;operational semantics;algorithm;semantics	Theory	-17.388749970289503	21.128762593990867	63278
0c9e8bb1bc4caa0dc593274dfab0abb9ccf4c804	model checking and abstraction	logica temporal;complexite calcul;temporal logic;abstraction;binary decision diagrams bdds;prise decision;abstraccion;complejidad computacion;modelo logico;binary decision diagrams;model checking;computational complexity;logic model;abstract interpretation;toma decision;logique temporelle;modele logique;binary decision diagram	We describe a method for using abstraction to reduce the complexity of temporal-logic model checking. Using techniques similar to those involved in abstract interpretation, we construct an abstract model of a program without ever examining the corresponding unabstracted model. We show how this abstract model can be used to verify properties of the original program. We have implemented a system based on these techniques, and we demonstrate their practicality using a number of examples, including a program representing a pipelined ALU circuit with over 101300 states.	abstract interpretation;arithmetic logic unit;model checking;pipeline (computing);temporal logic	Edmund M. Clarke;Orna Grumberg;David E. Long	1994	ACM Trans. Program. Lang. Syst.	10.1145/186025.186051	model checking;temporal logic;computer science;theoretical computer science;abstraction;computational complexity theory;binary decision diagram;abstraction model checking;algorithm	PL	-15.399684987709911	27.934291147767482	63382
b4c87a34f4cf9d82bcba24809ebe483bcd76670e	computing with features as formulae	minimal model;bottom up;computer science computer analysis computer science data processing data processing;satisfiability;trait distinctif;schonfinkel bernays;fixed point;algorithme;algorithm;informatique;computational linguistics;formule;theoretical foundation;linguistique informatique;structure	This paper extends the approach to feature structures developed in Johnson (1991a), which uses SchO'nfinkel-Bernays' formulae to express feature structure constraints. These are shown to be a disjunctive generalization of Datalog clauses, as used in database theory. This paper provides a fixed-point characterization of the minimal models of these formulae that serves as the theoretical foundation of a forward-chaining algorithm for determining their satisfiability. This algorithm, which generalizes the standard attribute-value unification algorithm, is also recognizable as a nondeterministic variant of the semi-naive bottom-up algorithm for evaluating Datalog programs, further strengthening the connection between the theory of feature structures and databases.	algorithm;bottom-up parsing;database theory;datalog;disjunctive normal form;forward chaining;semiconductor industry;unification (computer science)	Mark Johnson	1994	Computational Linguistics		natural language processing;structure;computer science;theoretical computer science;computational linguistics;top-down and bottom-up design;fixed point;linguistics;algorithm;satisfiability	AI	-7.24159185286065	19.224346590391367	63494
a38a651f726174f2aa74c9c8fe83143674eaaf00	a hierarchical approach to monadic second-order logic over graphs	theorie automate;descomposicion grafo;graphe fini;finite graph;logical programming;monadic second order;grafo finito;programmation logique;monadic second order logic;informatique theorique;automata theory;teoria automata;tree decomposition;programacion logica;graph decomposition;computer theory;decomposition graphe;informatica teorica	The expressiveness of existential monadic second-order logic is investigated over several classes of nite graphs among them the graphs of bounded tree-width. A hierarchical approach to the decomposition of graphs is introduced which is related to the notion of tree decomposition. Among other results we show that existential monadic second-order logic on graphs of bounded tree-width is not closed under complement.	automata theory;automaton;maximal set;monadic predicate calculus;tiling window manager;tree decomposition;treewidth	Ina Schiering	1997		10.1007/BFb0028029	combinatorics;discrete mathematics;computer science;automata theory;mathematics;modular decomposition;monadic predicate calculus;treewidth;chordal graph;indifference graph;algorithm;tree decomposition	Logic	-4.74002433061564	21.087220885565443	63668
be744d842b0317600c05da2ced2fe539775c51de	partial-order reduction for general state exploring algorithms	busqueda informacion;distributed system;verificacion modelo;systeme reparti;relation ordre partiel;algoritmo busqueda;esqueleto;componente logicial;information retrieval;algorithme recherche;search algorithm;verification modele;composant logiciel;program verification;skeleton;busca local;verificacion programa;sistema repartido;model checking;state space method;methode espace etat;recherche information;partial ordering;estructura datos;software component;partial order reduction;squelette;structure donnee;relacion orden parcial;verification programme;data structure;local search;recherche locale;metodo espacio estado;partial order	An important component of partial-order based reduction algorithms is the condition that prevents action ignoring, commonly known as the cycle proviso. In this paper we give a new version of this proviso that is applicable to a general search algorithm skeleton also known as the General State Expanding Algorithm (GSEA). GSEA maintains a set of open (visited but not expanded) states from which states are iteratively selected for exploration and moved to a closed set of states (visited and expanded). Depending on the open set data structure used, GSEA can be instantiated as depth-first, breadth-first, or a directed search algorithm. The proviso is characterized by reference to the open and closed set of states in GSEA. As a result the proviso can be computed in an efficient manner during the search based on local information. We implemented partial-order reduction for GSEA based on our proposed proviso in the tool HSF-SPIN, which is an extension of the model checker SPIN for directed model checking. We evaluate the state space reduction achieved by partial-order reduction according to the proviso that we propose by comparing it on a set of benchmark problems to other reduction approaches. We also compare the use of breadth-first search and A*, two algorithms ensuring that counterexamples of minimal length will be found, together with the proviso that we propose.	algorithm;partial order reduction	Dragan Bosnacki;Stefan Leue;Alberto Lluch-Lafuente	2006		10.1007/11691617_16	partially ordered set;data structure;computer science;calculus;mathematics;programming language;algorithm	EDA	-16.578375687799383	27.403193613744815	63732
c32d62ca6063133dd472c8c15349429637aa754d	infinite behaviour of petri nets	petri net	The infinite behaviour of a Petri net is the set of infinite, labelled or unlabelled firing sequences. For the well-known notion of i -acceptance for i ∈{1, 1',2,2',3} two hierarchies are established and their interrelation is studied.	petri net	Rüdiger Valk	1983	Theor. Comput. Sci.	10.1016/0304-3975(83)90115-9	stochastic petri net;computer science;mathematics;petri net	ECom	-9.22140476682611	21.953637799990002	64067
d79d2496f7d2240997c14b0c252950bf6a2b347c	strong sequentiality of left-linear overlapping rewrite systems	term rewrite system;rewrite systems;normal form	Connuent term rewriting systems can be seen as a model for functional computations, in which redexes corresponding to instances of left hand sides of rules are repeatedly replaced by their corresponding right hand side instance. Lazy sequential strategies reduce a given redex in a term if and only if this redex must be reduced by any other sequential strategy computing the normal form of the term. Lazy strategies always terminate when a normal form exist, and are indeed optimal. In a landmark paper, Huet and Levy showed that such a strategy always exists for left linear non-overlapping rewrite systems that are strongly sequential, a property that they proved decidable for such systems. This paper generalises the result to the case of left-linear, possibly overlapping rewrite systems.	a-normal form;computation;lazy evaluation;reduction strategy (code optimization);rewrite (programming);rewriting;terminate (software)	Jean-Pierre Jouannaud;Walid Sadfi	1994		10.1007/3-540-60381-6_14	discrete mathematics;normalization property;programming language;algorithm	AI	-14.82343027201589	21.419648073831347	64112
1db48ec3c19985262f5a3440ba2a6bbf705ff24f	corea: a synchronous calculus of parallel communicating reactive automata	reactive system	"""Reactive systems often require temporal and logical safety , concurrency and determinism. Several asynchronous or strong synchronous answers have been proposed to this problem. However, \asyn-chronous"""" languages such as CSP or CCS generally force the user to choose between determinism and concurrency. On the other hand, strong synchronous implementations are mainly sequential. The aim of this paper is to present a new paradigm for concurrent reactive programming, weak synchronism, responding to both concurrency and determinism."""	automaton;calculus of communicating systems;concurrency (computer science);indeterminacy in concurrent computation;programming paradigm;reactive programming	Frédéric Boniol	1994		10.1007/3-540-58184-7_135	discrete mathematics;theoretical computer science;algorithm	Logic	-11.697103053294098	21.413258659348568	64207
57a8409283c2065900e34108f8d23d4e38da6d41	fpsolve: a generic solver for fixpoint equations over semirings	newton s method	We introduce FPsolve, an implementation of generic algorithms for solving fixpoint equations over semirings. We first illustrate the interest of generic solvers by means of a scenario. We then succinctly describe some of the algorithms implemented in the tool, and provide some implementation details.	algorithm;fixed point (mathematics);generic-case complexity;solver	Javier Esparza;Michael Luttenberger;Maximilian Schlund	2014		10.1007/978-3-319-08846-4_1	discrete mathematics;theoretical computer science;mathematics;algorithm	PL	-17.712373626993365	18.642067053304938	64272
b7c7ffd55422693f4ca17679cc2516b9fe49083a	on proofs of programs for synchronization		Consider a program P and the program P that is obtained from P by reordering two adjacent statements x and y. Let x be the statement that comes before y in P , and after y in P. The statements x and y may be any two statements such that • reordering x and y doesn't eliminate any transitive happens-before edges in any valid execution (it will reverse the direct happens-before edge between x and y), • x and y are not conflicting accesses to the same variable, • x and y are not both synchronization actions, and • the intra-thread semantics of x and y allow reordering (e.g., x doesn't store into a register that is read by y). This means that common single-threaded compiler optimizations are legal here. Assume that we have a valid execution E of program P. To show that the transformation of P into P is legal, we need to show that there is a valid execution E of P that has the same observable behavior as E. Assume E = S, so, hb , co. We are going to show that E = S, so, hb, co is also a valid execution of P. Let a x and a y denote the actions corresponding to the statements x and y. Because of the reordering the happens-before ordering may be different but we know that hb − {a x → a y } ⊆ hb − {a y → a x }. Clearly, if E is consistent then E is consistent, so we only need to worry about showing a co that is valid as the justification order of E. • Assume that co = αa y βa x γ, and that a y is a read. Then co = αβa x a y γ is a valid justification order for E. Note that the happens-before ordering and the write seen by each read are identical. Since the happens before edges are the same and the write seen by a y in the execution E is in α, a y can see that same write in E (because it is happens-before consistent and the write comes before the read in the justification order). Additionally, we know that a y does not affect any of the actions in β, because it is a read that is not written out before β occurs in co. It doesn't …	observable;optimizing compiler;processor register;thread (computing)	Irene Greif	1976			discrete mathematics;computer science;mathematical proof;synchronization	Theory	-15.958106417535644	31.699978351309895	64454
0c27ad309494cbb1daa31c1f1e29aa1e82ab9bb9	toward formal development of ml programs: foundations and methodology (extended abstract)	ml programs;formal development;extended abstract;modular decomposition	"""A formal methodology is presented for the systematic evolution of modular Standard ML programs from speciications by means of veriied reenement steps, in the framework of the Extended ML speciication language. Program development proceeds via a sequence of design (modular decomposition), coding and reenement steps. For each of these three kinds of steps, conditions are given which ensure the correctness of the result. These conditions seem to be as weak as possible under the constraint of being expressible as \local"""" interface matching requirements. Interfaces are only required to match up to behavioural equivalence, which is seen as vital to the use of data abstraction in program development. y A later version will take into account the recent changes to ML described in HMT 88]. The relevant changes concern mainly functors with multiple arguments."""	abstraction (software engineering);correctness (computer science);extended ml;modular decomposition;requirement;standard ml;turing completeness;wiki	Donald Sannella;Andrzej Tarlecki	1989		10.1007/3-540-50940-2_48	computer science;algorithm	Theory	-17.096277584767577	23.822924353862454	64572
8db84d7974e80554930576e926b9d6de9727daed	path feasibility analysis for string-manipulating programs	feasibility analysis;finite domain;symbolic execution;test case generation;software development;satisfiability modulo theories	We discuss the problem of path feasibility for programs manipulating strings using a collection of standard string library functions. We prove results on the complexity of this problem, including its undecidability in the general case and decidability of some special cases. In the context of test-case generation, we are interested in an efficient finite model finding method for string constraints. To this end we develop a two-tier finite model finding procedure. First, an integer abstraction of string constraints are passed to an SMT (Satisfiability Modulo Theories) solver. The abstraction is either unsatisfiable, or the solver produces a model that fixes lengths of enough strings to reduce the entire problem to be finite domain. The resulting fixed-length string constraints are then solved in a second phase. We implemented the procedure in a symbolic execution framework, report on the encouraging results and discuss directions for improving the method further.	algorithm;approximation;automata theory;automaton;comparison of programming languages (string functions);encode;easychair;first-order predicate;first-order reduction;integer programming;java platform, standard edition;library (computing);mehryar mohri;microsoft word for mac;multitier architecture;null-terminated string;programmer;quantifier (logic);regular expression;sql injection;satisfiability modulo theories;shannon (unit);solver;static program analysis;string (computer science);symbolic execution;undecidable problem;unfolding (dsp implementation)	Nikolaj Bjørner;Nikolai Tillmann;Andrei Voronkov	2009		10.1007/978-3-642-00768-2_27	feasibility study;discrete mathematics;theoretical computer science;software development;mathematics;programming language;satisfiability modulo theories;algorithm	Logic	-16.77836538260203	24.34076877279344	64590
90a16ad4589aa9fdfcd42f3c52d3a8aa4b4f04f5	strand spaces with choice via a process algebra semantics	text;narrowing based reachability analysis;rewriting logic;cryptographic protocol analysis;process algebra;rewriting based model checking	Roles in cryptographic protocols do not always have a linear execution, but may include choice points causing the protocol to continue along different paths. In this paper we address the problem of representing choice in the strand space model of cryptographic protocols, particularly as it is used in the Maude-NPA cryptographic protocol analysis tool.  To achieve this goal, we develop and give formal semantics to a process algebra for cryptographic protocols that supports a rich taxonomy of choice primitives for composing strand spaces. In our taxonomy, deterministic and non-deterministic choices are broken down further. Non-deterministic choice can be either explicit, i.e., one of two paths is chosen, or implicit, i.e. the value of a variable is chosen non-deterministically. Likewise, deterministic choice can be either an (explicit) if-then-else choice, i.e. one path is chosen if a predicate is satisfied, while the other is chosen if it is not, or implicit deterministic choice, i.e. execution continues only if a certain pattern is matched. We have identified a class of choices which includes finite branching and some cases of infinite branching, which we address in this paper.  Our main theoretical results are two bisimulation results: one proving that the formal semantics of our process algebra is bisimilar to the forwards execution semantics of its associated strands, and another showing that it is also bisimilar with respect to the symbolic backwards semantics of the strands such as that supported by Maude-NPA. At the practical level, we present a prototype implementation of our process algebra in Maude-NPA, illustrate its expressive power and naturalness with various examples, and show how it can be effectively used in formal analysis.	bisimulation;conditional (computer programming);cryptographic protocol;cryptography;maude system;printer working group;process calculus;programming language;prototype;semantics (computer science);strand (programming language);taxonomy (general)	Fan Yang;Santiago Escobar;Catherine A. Meadows;José Meseguer;Sonia Santiago	2016		10.1145/2967973.2968609	process calculus;rewriting;computer science;theoretical computer science;programming language;algorithm	Logic	-16.827137987296826	23.68638240663252	64726
6e4ba57478ad072fcd84c8ad27df45b7c00993fc	on determining the minimum length, tree-like resolution refutation of 2sat, and extended 2sat formulas	teoria demonstracion;theorie preuve;formal specification;proof theory;efficient algorithm;linear approximation;problema np duro;constraint satisfaction;specification formelle;polynomial time algorithm;especificacion formal;satisfaction contrainte;np hard problem;probleme np difficile;satisfaccion restriccion	This paper is concerned with the design of polynomial time algorithms to determine the shortest length, tree-like resolution refutation proofs for 2SAT and Q2SAT (Quantified 2SAT) clausal systems. Determining the shortest length resolution refutation has been shown to be NP-complete, even for HornSAT systems (for both tree-like and dag-like proofs); in fact obtaining even a linear approximation for such systems is NP-Hard. In this paper we demonstrate the existence of simple and efficient algorithms for the problem of determining the exact number of steps in the minimum length tree-like resolution refutation proof of a 2SAT or Q2SAT clausal system. To the best of our knowledge, our results are the first of their kind.	2-satisfiability	K. Subramani	2002		10.1007/3-540-36184-7_7	constraint satisfaction;computer science;calculus;proof theory;np-hard;formal specification;mathematics;programming language;algorithm;linear approximation	Theory	-7.531805624596082	19.672887296312993	64820
256b3a6101c0f30718e8c076d6e404f795a1d99c	runtime verification using a temporal description logic	runtime verification;data description;book chapter;dynamic system;time points;satisfiability;transition system;finite prefix;model checki;model checking;linear temporal logic;transition systems;observed systems;dynamical systems;complete information;single state;description logic;propositional variables;run time verification;keywords actual system	Formulae of linear temporal logic (LTL) can be used to specify (wanted or unwanted) properties of a dynamical system. In model checking, the system’s behavior is described by a transition system, and one needs to check whether all possible traces of this transition system satisfy the formula. In runtime verification, one observes the actual system behavior, which at any time point yields a finite prefix of a trace. The task is then to check whether all continuations of this prefix to a trace satisfy (violate) the formula. In this paper, we extend the known approaches to LTL runtime verification in two directions. First, instead of propositional LTL we use ALC-LTL, which can use axioms of the description logic ALC instead of propositional variables to describe properties of single states of the system. Second, instead of assuming that the observed system behavior provides us with complete information about the states of the system, we consider the case where states may be described in an incomplete way by ALC-ABoxes.	abox;alphabet (formal languages);artificial intelligence;automata theory;best, worst and average case;boolean satisfiability problem;computer science;continuation;description logic;dynamical system;exptime;generalized büchi automaton;information and computation;jack lutz;jensen's inequality;klaus samelson;linear temporal logic;model checking;pierre wolper;propositional variable;rewriting;runtime verification;schmidt decomposition;software engineering;time complexity;tracing (software);transition system;worst-case complexity	Franz Baader;Andreas Bauer;Marcel Lippmann	2009		10.1007/978-3-642-04222-5_9	model checking;discrete mathematics;dynamical systems theory;linear temporal logic;description logic;computer science;artificial intelligence;theoretical computer science;dynamical system;mathematics;runtime verification;propositional variable;programming language;complete information;algorithm;satisfiability	AI	-10.879931960399336	24.060396579305543	64882
046690cb030822e57ef004221c8693f9215f7f58	real-time property preservation in approximations of timed systems	real-time property preservation;detectdesign error;execution trace;formal technique;formal link;designof real-time system;thereal-time property preservation;boththe model;real-time property;real-time property preservation givesinsight;neighbouringtimed state sequence;timed systems;logic;temporal logic;embedded system;real time system;real time systems;systems analysis;time measurement;formal specification;real time;satisfiability;system design	"""Formal techniques have been widely applied in the designof real-time systems and have significantly helped detectdesign errors by checking real-time properties of themodel. However, a model is only an approximation of itsrealization in terms of the issuing time of events. Therefore,a real-time property verified in the model can not always bedirectly transferred to the realization. In this paper, boththe model and the realization are viewed as sets of timedstate sequences. In this context, we first investigate thereal-time property preservation between two neighbouringtimed state sequences (execution traces of timed systems),and then extend the results to two """"neighbouring"""" timedsystems. The study of real-time property preservation givesinsight in building a formal link between real-time propertiessatisfied in the model and those in the realization."""	approximation;real-time clock;real-time computing;real-time transcription;tracing (software)	Jinfeng Huang;Jeroen Voeten;Marc Geilen	2003	First ACM and IEEE International Conference on Formal Methods and Models for Co-Design, 2003. MEMOCODE '03. Proceedings.		embedded system;systems analysis;real-time computing;temporal logic;formal verification;computer science;formal specification;algorithm;satisfiability	Embedded	-11.644266056944362	26.789569079758703	64959
bef2016a05fde75a2468aeb0e2bd40e31e2a87cf	an effective implementation of a symbolic-numeric cylindrical algebraic decomposition for quantifier elimination	quantifier elimination;computational techniques;dynamic evaluation;theoretical computer science;computer science all;certified numerical computation;numerical computation;cylindrical algebraic decomposition;symbolic numeric computation	Recently quantifier elimination (QE) has been of great interest in many fields of science and engineering. In this paper an effective symbolic-numeric cylindrical algebraic decomposition (SNCAD) algorithm and its variant specially designed for QE are proposed based on the authors' previous work and our implementation of those is reported. Based on analysing experimental performances, we are improving our design/synthesis of the SNCAD for its practical realization with existing efficient computational techniques and several newly introduced ones. The practicality of the SNCAD is now examined by a number of experimental results including practical engineering problems, which also reveals the quality of the implementation.	algorithm;performance;quadratic equation;quantifier (logic);query expansion	Hidenao Iwane;Hitoshi Yanami;Hirokazu Anai;Kazuhiro Yokoyama	2009		10.1145/1577190.1577203	discrete mathematics;computer science;theoretical computer science;algorithm	Logic	-9.071693363983368	30.742660260951336	64987
2336c600a15714eeee8fc9bb3481c5837a22648a	on reduction of asynchronous systems	asynchronous system	Abstract   In this paper, we give a formal treatment of the reduction idea in Lipton's paper, and investigate its application in proving correctness of asynchronous systems. In a complex system, it is both tempting and convenient to assume that certain sequences of actions behave like single indivisible or instantaneous actions. Such conceptual reduction of sequences of relatively small actions to single occurrences of relatively large actions is encountered very often in computer science. For asynchronous systems, reduction is particularly appealing because it helps to reduce the amount of interleaving of actions involved and also the complexity of the systems, thereby making correctness proofs more tractable. However, reduction is also very dangerous for it could easily lead to erroneous and disastrous conclusions about systems.  We establish, in this paper, simple and general sufficient conditions for reduction under which correctness is preserved, and any conclusions obtained about the correctness of the reduced systems are also valid for the original systems, as far as deadlock-freedom, homing, determinacy and the Church-Rosser property are concerned. We also show that the results in Lipton's paper are special cases of some of our results here.	asynchronous circuit	Y. S. Kwong	1977	Theor. Comput. Sci.	10.1016/0304-3975(77)90041-X	asynchronous system;combinatorics;discrete mathematics;computer science;mathematics;distributed computing;algorithm	ECom	-12.456105640222727	20.793535132455037	65096
1db80014cd21d2f4bdc4153ae95cfbbc49cccb06	probabilistic barbed congruence	observational equivalence;process calculus	This paper defines a probabilistic barbed congruence which turns out to coincide with observational equivalence in a probabilistic extension of CCS. Based on this coincidence result, we provide a sound and complete axiomatisation for the barbed congruence in a finite fragment of probabilistic CCS.	axiomatic system;congruence of squares;observational equivalence;turing completeness	Yuxin Deng;Wenjie Du	2007	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2007.07.011	combinatorics;process calculus;discrete mathematics;topology;computer science;mathematics;programming language	PL	-9.887376193741186	20.02584249720944	65113
13e0890d240555a862baeb29269667a11dc76150	simulation-based minimazation	minimization;time complexity;simulation;equivalence relation;space complexity	We present a minimization algorithm that receives a Kripke structure M and returns the smallest structure that is simulation equivalent to M. The simulation equivalence relation is weaker than bisimulation but stronger than the simulation preorder. It strongly preserves ACTL and LTL (as sublogics of ACTL*).We show that every structure M has a unique-up-to-isomorphism reduced structure that is simulation equivalent to M and smallest in size. Our Minimizing Algorithm constructs this reduced structure. It first constructs the quotient structure for M, then eliminates transitions to little brothers, and finally deletes unreachable states.Since the first step of the algorithm is based on the simulation preorder over M, it has maximal space requirements. To reduce them, we present the Partitioning Algorithm, which constructs the quotient structure for M without ever building the simulation preorder. The Partitioning Algorithm has improved space complexity, but its time complexity might have worse.	algorithm;bisimulation;dspace;kripke structure (model checking);linear temporal logic;maximal set;requirement;simulation preorder;time complexity;turing completeness;unreachable memory	Doron Bustan;Orna Grumberg	2003	ACM Trans. Comput. Log.	10.1145/635499.635502	time complexity;combinatorics;discrete mathematics;computer science;mathematics;dspace;equivalence relation;algorithm	Theory	-6.0649619324698865	23.795572759880642	65371
9d3b1e93c2dc7f4940904038e422bc3ce231c3f0	verifying relative error bounds using symbolic simulation		In this paper we consider the problem of formally verifying hardware that is specified to compute reciprocal, reciprocal square root, and power-of-two functions on floating point numbers to within a given relative error. Such specifications differ from the common case in which any given input is specified to have exactly one correct output. Our approach is based on symbolic simulation with binary decision diagrams, and involves two distinct steps. First, we prove a lemma that reduces the relative error specification to several inequalities that involve reasoning about natural numbers only. The most complex of these inequalities asserts that the product of several naturals is less-than/greaterthan another natural. Second, we invoke one of several customized algorithms that decides the inequality, without performing the expensive symbolic multiplications directly. We demonstrate the effectiveness of our approach on a next-generation Intel © processor design and report encouraging time and space metrics for these proofs.	algorithm;approximation error;binary decision diagram;decision problem;methods of computing square roots;power of two;processor design;social inequality;symbolic simulation;verification and validation	Jesse Bingham;Joe Leslie-Hurd	2014		10.1007/978-3-319-08867-9_18	theoretical computer science;algorithm	Logic	-14.300387679142032	25.36352391857581	65588
7f2f4329dd49b1262461b6e3105b6c7b0d4c7eae	stratified functional programs and computational complexity	functional programming;computational complexity	Synopsis. We develop two notions of stratified recur	computational complexity theory;video synopsis	Daniel Leivant	1993		10.1145/158511.158659	computational problem;decision tree model;computer science;computational resource;worst-case complexity;programming language;computational complexity theory;functional programming;computational learning theory;asymptotic computational complexity	PL	-6.324246679481327	18.70081147670243	65715
05c136cf6d06fc38b72861ddb6456edc8e5c8145	on-the-fly parallel model checking algorithm that is optimal for verification of weak ltl properties	explicit model checking;partial orderreduction;parallel;partial order reduction;on the fly	One of the most important open problems of parallel LTL model checking is to design an on-the-fly scalable parallel algorithm with linear time complexity. Such an algorithm would provide the same optimality we have in sequential LTL model checking. In this paper we give a partial solution to the problem: we propose an algorithm that has the required properties for a very rich subset of LTL properties, namely those expressible by weak Büchi automata. In addition to the previous version of the paper [1], we demonstrate how our new algorithm can be efficiently combined with a particular parallel technique for Partial Order Reduction and report on additional experiments.	automata theory;büchi automaton;cpu cache;computation;cycle detection;experiment;map;model checking;overhead (computing);parallel algorithm;partial order reduction;scalability;time complexity	Jiri Barnat;Lubos Brim;Petr Rockai	2012	Sci. Comput. Program.	10.1016/j.scico.2011.03.001	partial order reduction;computer science;parallel;algorithm	Logic	-13.527956883953347	26.232947701463726	65782
41ff2b39a23befcd393ac7d958209c59ab394bef	deterministic systems of sequential processes: theory and tools	petri net;expert system	In this paper, we present an analysis of an interesting class of Petri nets : Deterministic systems of sequential processes. We give an extension of this class and new theoretical results. We show how these theorems have been implemented by rules in an expert system for Petri nets analysis.		Younes Souissi;Nicolas Beldiceanu	1988		10.1007/3-540-50403-6_52	stochastic petri net;computer science;theoretical computer science;machine learning;petri net;algorithm	Logic	-5.9710340219974745	25.091541632482343	65851
88771249fe935dfefea07d1c96b998ce910e0cac	implementing real-time transactional security property using timed edit automata	security properties;real time properties;timed automata;edit automata;transactional properties	Timed edit automaton is action sequence transformer which takes a sequence of actions as input and produces another sequence of actions according to the policy it implements. If the input action sequence obeys the policy then timed edit automaton produces same actions sequence or an equivalent action sequence. If the input action sequence does not obey the policy then the timed edit automaton produces modified action sequence. Timed edit automaton is suitable for implementing real-time transactional security property.	automaton;real-time clock;real-time transcription;transformer	N. Rajamanickam;R. Nadarajan	2013		10.1145/2523514.2523578	real-time computing;computer science;theoretical computer science;timed automaton;algorithm	Logic	-11.304167690720195	25.648641065604707	66006
04cf99462b401ffe320698d5411bb84675627abc	mgtp: a parallel theorem prover based on lazy model generation	model generation;theorem prover	We have implemented a model-generation based parallel theorem prover in KL1 on a parallel inference machine, PIM. We have developed several techniques to improve the efficiency of forward reasoning theorem provers based on lazy model generation. The tasks of the model-generation based prover are the generation and testing of atoms to be the elements of a model for the given theorem. The problem with this method is the explosion in the number of generated atoms and in the computational cost in time and space, incurred by the generation processes. Lazy model generation is a new method that avoids the generation of unnecessary atoms that are irrelevant to obtaining proofs, and to provide flexible control for the efficient use of resources in a parallel environment. With this method we have achieved a more than one-hundred-fold speedup on a PIM consisting of 128 PEs.	automated theorem proving;lazy evaluation	Ryuzo Hasegawa;Miyuki Koshimura;Hiroshi Fujita	1992		10.1007/3-540-55602-8_223	discrete mathematics;computer science;artificial intelligence;mathematics;automated theorem proving;programming language;algorithm	Logic	-15.209850382495206	23.58932204928551	66040
de9db9efb052301a6f546e9329006b23cd769043	a unified rule format for bounded nondeterminism in sos with terms as labels	structural operational semantics;bounded nondeterminism;rule formats;labelled transition systems	We present a unified rule format for structural operational semantics with terms as labels that guarantees that the associated labelled transition system has some bounded-nondeterminism property. The properties we consider include finite branching, initials finiteness and image finiteness.	unbounded nondeterminism	Luca Aceto;Ignacio Fábregas;Álvaro García-Pérez;Anna Ingólfsdóttir	2017	J. Log. Algebr. Meth. Program.	10.1016/j.jlamp.2017.03.002	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-9.762744009336542	23.196337387457117	66209
7d2b07d0322f314f3c5d2935e1f9271a2c383f02	a syntax-directed logic simulator	logic element;satisfiability;input output;digital systems;normal form;fortran;system simulation	A syntax directed compiler to simulate digital systems is described. The compiler has been written in Fortran IV for the IBM 7044, and is capable of simulating digital systems containing up to approximately 2500 logic elements. It is a two pass compiler. During the first pass it scans the source statements by using the Backus Normal Form definition of allowed syntax. From this a coded description of the program is constructed. During the second pass this coded program description is used to generate machine code in an order suitable for system simulation. The system being simulated is then evaluated on the basis of its cost and ability to satisfy predetermined input-output requirements.	buffalo network-attached storage series;compiler;computer architecture;digital electronics;fortran;ibm 7040;information system;logic simulation;machine code;requirement	Charles M. Allen;Donald D. Givone;William M. Horner;Robert W. Snelsire	1968		10.1145/800167.805403	input/output;embedded system;electronic engineering;computer science;theoretical computer science;operating system;programming language;algorithm;satisfiability	PL	-13.981548275203199	31.536814018350473	66282
918612d77c895d567269be0c29fb9e82609ddb56	coalgebraic bisimulation-up-to	part of book or chapter of book	Bisimulation-up-to enhances the bisimulation proof method for process equivalence. We present its generalization from labelled transition systems to arbitrary coalgebras, and show that for a large class of systems, enhancements such as bisimulation up to bisimilarity, up to equivalence and up to context are sound proof techniques. This allows for simplified bisimulation proofs for many different types of state-based systems.	bisimulation;state space;turing completeness	Jurriaan Rot;Marcello M. Bonsangue;Jan J. M. M. Rutten	2013		10.1007/978-3-642-35843-2_32	combinatorics;discrete mathematics;computer science;bisimulation;mathematics;algorithm	Logic	-10.788839424894846	22.709789694088983	66356
e50000aa506847413bf87704f291e8327b291fe1	global value numbering: a precise and efficient algorithm		Global Value Numbering (GVN) is an important static analysis to detect equivalent expressions in a program. We present an iterative data-flow analysis GVN algorithm in SSA for the purpose of detecting total redundancies. The central challenge is defining a join operation to detect equivalences at a join point in polynomial time such that later occurrences of redundant expressions could be detected. For this purpose, we introduce the novel concept of value φ-function. We claim the algorithm is precise and takes only polynomial time.	algorithm;data-flow analysis;dataflow;global value numbering;iterative method;join point;polynomial;sensor;static program analysis;time complexity	Rekha R. Pai	2015	CoRR		global value numbering;theoretical computer science;algorithm	PL	-17.85535840743365	28.64253738178176	66482
2e393d042487e83a517c8750ebe5217a09bfb570	separation logic for multiple inheritance	verification;object oriented language;modular reasoning;separation logic;verificiation;hoare logic;multiple inheritance;data structure;article	As an extension to Floyd-Hoare logic, separation logic has been used to facilitate reasoning about imperative programs manipulating shared mutable data structures. Recently, it has also been extended to support modular reasoning in Java-like object-oriented languages where only single inheritance is allowed. In this paper we propose an extension of separation logic to support also the reasoning for multiple inheritance in C++ -like languages. To cater for multiple inheritance, we modified the standard storage model for separation logic in a way that the correct reference to a field or a method can be easily determined. On top of this storage model, a set of proof rules are proposed. Our verification system also provides basic support for behavioral subtyping.	multiple inheritance;separation logic	Chenguang Luo;Shengchao Qin	2008	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2008.04.051	dynamic logic;multiple inheritance;description logic;verification;higher-order logic;separation logic;data structure;computer science;theoretical computer science;bunched logic;non-monotonic logic;signature;ontology language;hoare logic;automated reasoning;programming language;object-oriented programming;substructural logic;multimodal logic;algorithm;autoepistemic logic	Logic	-16.99759429252634	19.648655401907604	66519
5a1c610477a499a638dda307d189309ee2d6cd6e	computing strong and weak bisimulations for psi-calculi	datavetenskap datalogi;datavetenskap;computer science	We present a symbolic transition system and strong and weak bisimulation equivalences for psi-calculi, and show that they are fully abstract with respect to bisimulation congruences in the non-symb ...	bisimulation;data structure;denotational semantics;level of measurement;scope (computer science);transition system;π-calculus	Magnus Johansson;Björn Victor;Joachim Parrow	2012	J. Log. Algebr. Program.	10.1016/j.jlap.2012.01.001	discrete mathematics;computer science;bisimulation;mathematics;algorithm	Logic	-10.967121764178756	21.32551357398715	66748
79de9d765cd211801c004e52352c2fc34954a9d7	pakota: a system for enforcement in abstract argumentation		In this paper we describe Pakota, a system implementation that allows for solving enforcement problems over argumentation frameworks. Via harnessing Boolean satisfiability (SAT) and Maximum satisfiability (MaxSAT) solvers, Pakota implements algorithms for extension and status enforcement under various central AF semantics, covering a range of NP-complete—via direct MaxSAT encodings—and Σ2 -complete—via MaxSAT-based counterexample-guided abstraction refinement—enforcement problems. We overview the algorithmic approaches implemented in Pakota, and describe in detail the system architecture, features, interfaces, and usage of the system. Furthermore, we present an empirical evaluation on the impact of the choice of MaxSAT solvers on the scalability of the system, and also provide benchmark generators for extension and status enforcement.	algorithm;anisotropic filtering;argumentation framework;benchmark (computing);boolean satisfiability problem;np-completeness;refinement (computing);scalability;systems architecture	Andreas Niskanen;Johannes Peter Wallner;Matti Järvisalo	2016		10.1007/978-3-319-48758-8_25	argumentation framework;implementation;maximum satisfiability problem;enforcement;scalability;theoretical computer science;systems architecture;satisfiability;boolean satisfiability problem;computer science	AI	-14.72706391244413	23.247250759551935	66802
b1fa8cb36b85294311dd40da9f70198001b855cb	graph generation to statically represent csp processes	specifications;complex concurrent systems;industrial projects;automatic generation;csp specifications;graph generation;concurrent systems;logic programming;specification and verification;data structures;animation;algorithms;deterministic execution;capitulo de libro;data structure	The CSP language allows the specification and verification of complex concurrent systems. Many analyses for CSP exist that have been successfully applied in different industrial projects. However, the cost of the analyses performed is usually very high, and sometimes prohibitive, due to the complexity imposed by the non-deterministic execution order of processes and to the restrictions imposed on this order by synchronizations. In this work, we define a data structure that allows us to statically simplify a specification before the analyses. This simplification can drastically reduce the time needed by many CSP analyses. We also introduce an algorithm able to automatically generate this data structure from a CSP specification. The algorithm has been proved correct and its implementation for the CSP’s animator ProB is publicly available.		Marisa Llorens;Javier Oliver;Josep Silva;Salvador Tamarit	2010		10.1007/978-3-642-20551-4_4	anime;real-time computing;data structure;computer science;programming language;logic programming;algorithm	SE	-14.673712916430171	26.73075591330114	66831
1ac216516ceae7fbbddecc3d5e748ea531df1bf3	computing a hierarchical static order for decision diagram-based representation from p/t nets	decision diagrams;hierarchy;state space generation	State space generation suffers from the typical combinatorial explosion problem when dealing with industrial specifications. In particular, memory consumption while storing the state space must be tackled to verify safety properties. Decision Diagrams are a way to tackle this problem. However, their performance strongly rely on the way variables encode a system. Another way to fight combinatorial explosion is to hierarchically encode the state space of a system. This paper presents how we mix the two techniques via the hierarchization of a precomputed variable order. This way we obtain a hierarchical static order for the variables encoding a system. This heuristic was implemented and exhibits good performance.	algorithm;benchmark (computing);central processing unit;complex systems;computation;deadlock;decision problem;encode;exception handling;heuristic;heuristic (computer science);influence diagram;model checking;polyorb;precomputation;state space;static program analysis;structural analysis	Silien Hong;Fabrice Kordon;Emmanuel Paviot-Adet;Sami Evangelista	2012	Trans. Petri Nets and Other Models of Concurrency	10.1007/978-3-642-29072-5_5	combinatorics;theoretical computer science;mathematics;algorithm	SE	-14.367785511179738	24.358795608797028	67048
98438eb9719cdeba3d8b887143fe7450cb5cff2e	deciding testing equivalence for real-time processes with dense time	real time processing;realtime system;time domain;real time systems	We present a decision algorithm for testing equivalence of realtime systems with a dense time domain. Real-time systems are modelled by timed graphs, while the decision algorithm uses mutually refined timer region graphs. The mutual refinement is important for the synchronization of the timers of different real-time systems. Key to our decision algorithm is the fact that — despite the dense time domain — testing can be reduced to -bisimulation in very much the same way as in the untimed case.		Bernhard Steffen;Carsten Weise	1993		10.1007/3-540-57182-5_61	real-time computing;time domain;computer science	Embedded	-11.02147446850317	25.43484815125706	67108
8e4927202c0a267d1caddd367c6fa095b1382dbc	induction variables in very high level languages	control flow;high level language	We explore the notion of an induction variable in the context of a set-theoretic programming langugage. An appropriate definition, we believe, involves both the necessity that changes in the variable around a loop be easily computable and that they be small. We attempt to justify these requirements and show why they are independent assumptions. Next the question of what operators on sets play the role of +, − and * for arithmetic languages is explored, and several theorems allowing us recursively to detect induction variables in a loop are given. It is shown that most of the usual set operations do fit nicely into the theory and help form induction variables. The reason most variables fail to be induction variables concerns the structure of control flow, more than it does the operators applied.	computable function;control flow;formal language;induction variable;mathematical induction;recursion;requirement;set theoretic programming;set theory	Amelia C. Fong;Jeffrey D. Ullman	1976		10.1145/800168.811544	computer science;programming language;control flow;high-level programming language;algorithm	PL	-15.81056439210826	19.821954051780462	67200
75acdad0ce95d26272ca8e1557276e71ad5d5be9	superoptimisation : provably optimal code generation using answer set programming		Code optimisation in modern compilers is an accepted misnomer for performance im­ provement some of the time. The code that compilers generate is often significantly improved, but it is unlikely to produce optimal sequences of instructions; and if it does, it will not be possible to determine that they are indeed optimal. None of the existing approaches, or techniques for creating new optimisations, is likely to change this state of play. Superoptimisation is a radical approach to generating provably optimal code, that per­ forms searches over the space of all possible instructions. Rather than starting with naively generated code and improving it, a superoptimiser starts with the specification of a function and performs a directed search for an optimal sequence of instructions that fulfils this specification. In this thesis, we present TOAST, the Total Optimisation using Answer Set Techno­ logy system, a provably optimal code generation system that applies superoptimising techniques to optimise acyclic integer-based code for modern microprocessor architec­ tures. TOAST utilises Answer Set Programming (ASP), a declarative logic program­ ming language, as an expressive modelling and efficient computational framework to solve the optimal code generation problem. We demonstrate the validity of the approach of superoptimisation using Answer Set Programming by optimising code sequences for two 32-bit RISC architectures, the MIPS R2000 and the SPARC V8. We also present an application of the TOAST sys­ tem as a peephole optimiser, by generating libraries of equivalence classes of all op­ timal instruction sequences of a given length for a specific microprocessor architecture. While this is a computationally expensive process, it only ever needs to be performed once per architecture. We also provide significant benchmarks for the performance of state of the art domain solver tools, further demonstrating the applicability of Answer Set Programming in modelling complex real-world problems.	32-bit;analysis of algorithms;answer set programming;code generation (compiler);compiler;directed acyclic graph;library (computing);logic programming;mathematical optimization;microprocessor;peephole optimization;r2000 (microprocessor);sparc;solver;stable model semantics;turing completeness	Thomas Crick	2009				AI	-15.441017360068892	24.82646540511222	67481
645fa9b773383240a0d1086daaa1af31938523bb	decidability problems in grammar systems	grammar;graph theory;distributed system;teoria grafo;systeme reparti;tree;red petri;language theory;arbol;formal languages;teoria lenguaje;theorie graphe;grammar systems;algorithme;decision problem;algorithm;sistema repartido;grammaire;converability tree;arbre;decidibilidad;decidabilite;petri net;gramatica;lenguaje formal;theorie langage;formal language;reseau petri;decidability;algoritmo;langage formel	Most of the basic decision problems concerning derivations in cooperating distributed grammar systems have so far been open, possibly because of the lack of unifying methods and techniques. In this paper such a unifying device is proposed. It is called a coverability tree because it bears some resemblance to the coverability graph of place/transition Petri nets and vector addition systems. The coverability tree is always finite, which leads to rather strong decidability properties concerning both arbitrary and terminal derivations. Our method is largely independent of the mode of the derivations and answers most of the direct decidability questions about the components of the system.	decision problem;grammar systems theory	Valeria Mihalache	1999	Theor. Comput. Sci.	10.1016/S0304-3975(97)00165-5	combinatorics;formal language;discrete mathematics;computer science;graph theory;mathematics;algorithm	ECom	-4.8938994344966895	21.264844140079838	67525
46ce9c84aedd09636d202eeda14da90c5513b5cc	editorial for the third international conference on energy-aware high performance computing		The June 2012 TOP500 list’s first rank named Sequoia, the IBM BlueGene/Q system installed at the Department of Energy’s Lawrence Livermore National Laboratory achieved an impressive 16.32 petaflop/s on the Linpack benchmark using 1,572,864 cores with a power input of about 8 MW. Its operation would produce an annual electricity bill of more than 8 million Euros based on German price levels. At the same time, the U.S. Department of Energy publishes roadmaps in its Exascale Computing Initiative and puts 20 MW as a “practical power limit” for a future Exaflops computer by the end of this decade. We are thus more than a factor of 60 away from our performance goal but today’s technology uses almost half of the acceptable power target. This situation calls for an intensified research in the field of energy efficiency technology, also called Green IT which has found its way into High Performance Computing. Only a few years ago, the community considered HPC as the Formula 1 of computing and ignored the fact of dramatically rising operational costs. However, as with these race cars, we conceived means to reduce power consumption and at the same time increase the speed. HPC also is beginning to learn from the field of embedded systems where batterypowered hardware always requires special mechanisms to reduce power consumption during phases of low performance demand. In 2010 we started this new conference series EnA-HPC in order to bring researchers, vendors, and HPC center ad-	benchmark (computing);blue gene;embedded system;flops;ibm sequoia;lunpack;microwave;race condition;supercomputer;top500	Timo Minartz;Thomas Ludwig	2012	Computer Science - Research and Development	10.1007/s00450-012-0231-3		HPC	-6.061555933126948	32.22043404178376	67551
25d9aa0086a27ea28544c3e47260d8d9f8e2a586	efficient techniques for parsing with tree automata		Parsing for a wide variety of grammar formalisms can be performed by intersecting finite tree automata. However, naive implementations of parsing by intersection are very inefficient. We present techniques that speed up tree-automata-based parsing, to the point that it becomes practically feasible on realistic data when applied to context-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature.	algorithm;automata theory;bitbucket;chart;context-free language;fastest;formal grammar;generic programming;java;parsing;parsing expression grammar;runtime system;semantics (computer science);tree automaton	Jonas Groschwitz;Alexander Koller;Mark Johnson	2016			bottom-up parsing;implementation;parser combinator;machine learning;computer science;artificial intelligence;speedup;parsing;top-down parsing;graph	NLP	-14.273897953789227	24.278974331663974	67721
2a2e0ad4fa773af586e462114a9eb22131e8c24c	timed vacuity		Vacuity is a leading sanity check in model-checking, applied when the system is found to satisfy the specification. The check detects situations where the specification passes in a trivial way, say when a specification that requires every request to be followed by a grant is satisfied in a system with no requests. Such situations typically reveal problems in the modelling of the system or the specification, and indeed vacuity detection is a part of most industrial model-checking tools. Existing research and tools for vacuity concern discrete-time systems and specification formalisms. We introduce real-time vacuity, which aims to detect problems with real-time modelling. Real-time logics are used for the specification and verification of systems with a continuous-time behavior. We study vacuity for the branching real-time logic TCTL, and focus on vacuity with respect to the time constraints in the specification. Specifically, the logic TCTL includes the temporal operator U , which specifies real-time eventualities in real-time systems: the parameter J ⊆ IR≥0 is an interval with integral boundaries that bounds the time in which the eventuality should hold. We define several tightenings for the U operator. These tightenings require the eventuality to hold within a strict subset of J . We prove that vacuity detection for TCTL is PSPACEcomplete, thus it does not increase the complexity of model-checking of TCTL. Our contribution involves an extension, termed TCTL, of TCTL, which allows the interval J not to be continuous, and for which model checking stays in PSPACE. Finally, we describe a method for ranking vacuity results according to their significance.	approximation algorithm;logic programming;model checking;pspace;pspace-complete;polynomial;real-time clock;real-time computing;real-time locating system;real-time transcription;sanity check;timed automaton	Hana Chockler;Shibashis Guha;Orna Kupferman	2018		10.1007/978-3-319-95582-7_26	theoretical computer science;computer science	Embedded	-11.319052800934674	24.95841898484518	67810
52cdef4950511436e3f9e17c4c826251fddb0dc8	higher order pushdown automata, the caucal hierarchy of graphs and parity games	parite;graphe infini;game theory;hierarchized structure;teoria juego;structure hierarchisee;theorie jeu;parity;program verification;higher order;jeu 2 personnes;automate a pile;verificacion programa;model checking;infinite graph;juego 2 personas;two person game;pushdown automata;paridad;grafo infinito;push down automaton;verification programme;game playing;estructura jerarquizada;automata a pila	"""We consider two-player parity games played on transition graphs of higher order pushdown automata. They are """"game-equivalent"""" to a kind of model-checking game played on graphs of the infinite hierarchy introduced recently by Caucal. Then in this hierarchy we show how to reduce a game to a graph of lower level. This leads to an effective solution and a construction of the winning strategies."""	automata theory;pushdown automaton;stack (abstract data type)	Thierry Cachat	2003		10.1007/3-540-45061-0_45	model checking;game theory;combinatorics;discrete mathematics;higher-order logic;parity;computer science;artificial intelligence;mathematics;distributed computing;programming language;pushdown automaton;algorithm	Logic	-6.634549472132382	21.604629677637693	68095
6e335a5b04a961339064b77cf7321f2b7a278c29	on the analysis of interacting pushdown systems	logique lineaire;verification;lenguaje programacion;analyse sequentielle;verificacion modelo;logica temporal;programming language;system modeling;programa control;temporal logic;branching;temps lineaire;specification;verification modele;simultaneidad informatica;sequential analysis;flux donnee;mu calculo;flujo datos;natural extension;tiempo lineal;program verification;logica lineal;analisis programa;synchronisation;automate a pile;verificacion programa;concurrency;dataflow analysis;model checking;especificacion;synchronization;ramificacion;theory;linear time;checking program;temporal properties;programme controle;langage programmation;ramification;decidibilidad;sincronizacion;program analysis;analyse programme;push down automaton;data flow;decidabilite;verification programme;linear logic;simultaneite informatique;analisis secuencial;pushdown systems;logique temporelle;mu calculus;automata a pila;decidability;mu calcul;ltl	Pushdown Systems (PDSs) have become an important paradigm for program analysis. Indeed, recent work has shown a deep connection between inter-procedural dataflow analysis for sequential programs and the model checking problem for PDSs. A natural extension of this framework to the concurrent domain hinges on the, somewhat less studied, problem of model checking Interacting Pushdown Systems. In this paper, we therefore focus on the model checking of Interacting Pushdown Systems synchronizing via the standard primitives - locks, rendezvous and broadcasts, for rich classes of temporal properties - both linear and branching time. We formulate new algorithms for model checking interacting PDSs for important fragments of LTL and the Mu-Calculus. Additionally, we also delineate precisely the decidability boundary for each of the standard synchronization primitives.	algorithm;data-flow analysis;dataflow;decision problem;interaction;linear temporal logic;lock (computer science);modal μ-calculus;model checking;program analysis;programming paradigm;stack (abstract data type)	Vineet Kahlon;Aarti Gupta	2007		10.1145/1190216.1190262	deterministic pushdown automaton;synchronization;computer science;theoretical computer science;programming language;algorithm	PL	-9.053931007426138	22.971544841279158	68299
42378bfbd503ab5f4f205f4fa2badf23f4f51aed	redundancy detection in logic programs is undecidable	run time checking logic programs decidability redundancy detection inference rules compile time analysis;inference mechanisms;indexing terms;inference rule;redundancy;logic programming;timing analysis;hilbert s tenth problem;logic programs;redundancy decidability inference mechanisms logic programming;decidability;logic programming automatic control application software costs artificial intelligence natural languages databases information retrieval contracts computer applications	The authors study the decidability of redundancy detection in logic programs which are composed of inference rules with functions in their arguments. They formally define redundancies based on the solution sets in logic programs, and represent solution sets in well-defined domains. They prove that redundancy detection in inference rules with functions is undecidable. The theoretical results obtained here complete the previously unknown properties of redundancy detection of logic programs. Except for nonrecursive inference rules, redundancy detection is undecidable in general. Although general results reveal the inherent difficulty of efficient execution of logic programs through redundancy detection, special redundant cases can still be detected and pruned. Tradeoffs between compile-time analysis of decidable cases and run-time checking of undecidable cases can still be made. >	logic programming;undecidable problem	Zheng Zhou;Benjamin W. Wah	1990		10.1109/CMPSAC.1990.139436	logic redundancy;dynamic logic;decidability;description logic;higher-order logic;index term;computer science;theoretical computer science;proof calculus;redundancy;programming language;logic programming;probabilistic logic network;static timing analysis;algorithm;hilbert's tenth problem;rule of inference	AI	-16.86670525991224	21.553153110520523	68445
1e1fc72ae51d672c5183cae1ce3809224be43a85	solving qbf with counterexample guided refinement	info eu repo semantics conferenceobject;software engineering;algorithms;cegar	We propose two novel approaches for using CounterexampleGuided Abstraction Refinement (CEGAR) in Quantified Boolean Formula (QBF) solvers. The first approach develops a recursive algorithm whose search is driven by CEGAR (rather than by DPLL). The second approach employs CEGAR as an additional learning technique in an existing DPLL-based QBF solver. Experimental evaluation of the implemented prototypes shows that the CEGAR-driven solver outperforms existing solvers on a number of families in the QBF-LIB and that the DPLL solver benefits from the additional type of learning. Thus this article opens two promising avenues in QBF: CEGAR-driven solvers as an alternative to existing approaches and a novel type of learning in DPLL.	dpll algorithm;database schema;overhead (computing);preprocessor;programming paradigm;recursion (computer science);refinement (computing);solver;true quantified boolean formula	Mikolás Janota;William Klieber;Joao Marques-Silva;Edmund M. Clarke	2012		10.1007/978-3-642-31612-8_10	mathematical optimization;discrete mathematics;computer science;theoretical computer science;mathematics;algorithm	AI	-15.3009323170412	23.945609980314995	68530
4971da93580d364f47e294ed8761f8dd30751917	real-time testing with timed automata testers and coverage criteria	systeme temps reel;reachability;automate deterministe;analyse fonctionnelle;real time;automate temporise;system under test;test conformite;automata contemporizado;conformance testing;deterministic automaton;functional analysis;asequibilidad;automata determinista;prueba conformidad;on the fly;atteignabilite;real time system;timed automata;sistema tiempo real;compliance test;reaction time;real time systems;analisis funcional	In previous work, we have proposed a framework for black-box conformance testing of realtime systems based on timed automata specifications and two types of tests: analog-clock or digital-clock. Our algorithm to generate analog-clock tests is based on an on-the-fly determinization of the specification automaton during the execution of the test, which in turn relies on reachability computations. The latter can sometimes be costly, thus problematic, since the tester must quickly react to the actions of the system under test. In this paper, we provide techniques which allow analog-clock testers to be represented as deterministic timed automata, thus minimizing the reaction time to a simple state jump. We also provide a method for (statically) generating a suite of digital-clock tests which covers the specification with respect to a number of criteria: location, edge or state coverage. This avoids having to generate too many tests, as can be evidenced on a small example.	algorithm;automata theory;black box;computation;conformance testing;powerset construction;reachability;real-time testing;system under test;test suite;timed automaton	Moez Krichen;Stavros Tripakis	2004		10.1007/978-3-540-30206-3_11	functional analysis;mental chronometry;real-time computing;simulation;real-time operating system;computer science;deterministic automaton;conformance testing;system under test;reachability;algorithm	Logic	-12.193235592678452	28.849678105540356	68590
39459426cce6daaf45a2dddba1acdbeaa350894f	execution time of lambda-terms via denotational semantics and intersection types	denotational semantics;intersec- tion types;additional key words and phrases: computational complexity;λ-calculus;linear logic;computational complexity;lambda calculus;relational model	This paper presents a work whose aim is to obtain information on execution time of λ-terms by semantic means. By execution time, we mean the number of steps in a computational model. As in [Ehrhard and Regnier 2006], the computational model considered in this paper will be Krivine’s machine, a more realistic model than β-reduction. Indeed, Krivine’s machine implements (weak) head linear reduction: in one step, we can do at most one substitution. In this paper, we consider two variants of this machine : the first one (Definition 2.4) computes the head-normal form of any λ-term (if it exists) and the second one (Definition 2.11) computes the normal form of any λ-term (if it exists). The fundamental idea of denotational semantics is that types should be interpreted as the objects of a category C and terms should be interpreted as arrows in C in such a way that if a term t reduces to a term t, then they are interpreted by the same arrow. By the Curry-Howard isomorphism, a simply typed λ-term is a proof in intuitionistic logic and the β-reduction of a λ-term corresponds to the cut-elimination of a proof. Now, the intuitionistic fragment of linear logic [Girard 1987] is a refinement of intuitionistic logic. This means that when we have a categorical structure (C, . . .) for interpreting intuitionistic linear logic, we can derive a category K that is a denotational semantics of intuitionistic logic, and thus a denotational semantics of λ-calculus. Linear logic has various denotational semantics; one of these is the multiset based relational model in the category Rel of sets and relations with the comonad associated to the finite multisets functor (see [Tortora de Falco 2000] for interpretations	beta normal form;charles m. falco;computation;computational model;curry;curry–howard correspondence;denotational semantics;intuitionistic logic;lambda calculus;linear logic;refinement (computing);rel;relational model;run time (program lifecycle phase)	Daniel de Carvalho	2008	CoRR		linear logic;discrete mathematics;relational model;normalisation by evaluation;action semantics;computer science;theoretical computer science;domain theory;lambda calculus;mathematics;noncommutative logic;programming language;denotational semantics of the actor model;computational complexity theory;well-founded semantics;operational semantics;denotational semantics;algorithm	Logic	-12.196724183481496	19.328429452545695	68717
9216c767bea92044a36d233208060fb2b7399cf0	towards an automatic parametric wcet analysis		Interpretation Abstract Interpretation Derives constraints on the values of the program variablesInterpretation Derives constraints on the values of the program variables Stefan Bygde and Björn Lisper Towards an Automatic Parametric WCET Analysis Introduction Method Implementation Conclusions Future Work Workflow	abstract interpretation;worst-case execution time	Stefan Bygde;Björn Lisper	2008			real-time computing;computer science;operating system;programming language;algorithm	PL	-18.64220599938465	27.258763295673152	68936
ff89b8166a497bf61e723fb70d5c7e20ee3f6ca4	a logic-based framework for reasoning about composite data structures	program verification;linear constraint;satisfiability;dynamic linking;compositional data;complex data;first order logic	We define a logic, called CSL, for the specification of complex data structures, and we show its use in program verification. Our framework allows to handle programs with dynamic linked structures and arrays carrying unbounded data, as well as the composition of these structures. The formulas in CSL allow a limited form of alternation between existential and universal quantifiers and they can express (1) constraints on reachability between positions in the heap following some pointer fields, (2) linear constraints on the lengths of the lists and the indexes of the arrays, and (3) constraints on the values of the data attached to these positions. For data constraints, the logic CSL is parameterized by a first-order logic over the considered data domain. We prove that the satisfiability problem of CSL is decidable whenever the underlying data logic is decidable and that CSL is closed under the computation of the strongest post-condition in the considered class of programs.	array data structure;automata theory;boolean satisfiability problem;citation style language;composite data type;computation;data domain;decision problem;first-order logic;first-order predicate;formal verification;global serializability;heuristic (computer science);linked data structure;logic programming;pointer (computer programming);postcondition;quantifier (logic);reachability problem;satisfiability modulo theories;universal quantification	Ahmed Bouajjani;Cezara Dragoi;Constantin Enea;Mihaela Sighireanu	2009		10.1007/978-3-642-04081-8_13	discrete mathematics;computer science;theoretical computer science;first-order logic;mathematics;programming language;algorithm;satisfiability;complex data type	Logic	-13.050354671737303	21.330827706297697	69000
3f8677565fafa2345cf06053a73ffbfb1a215a1b	logics with rigidly guarded data tests		The notion of orbit finite data monoid was recently introduced by Bojańczyk as an algebraic object for defining recognizable languages of data words. Following Büchi’s approach, we introduce a variant of monadic second-order logic with data equality tests that captures precisely the data languages recognizable by orbit finite data monoids. We also establish, following this time the approach of Schützenberger, McNaughton and Papert, that the first-order fragment of this logic defines exactly the data languages recognizable by aperiodic orbit finite data monoids. Finally, we consider another variant of the logic that can be interpreted over generic structures with data. The data languages defined in this variant are also recognized by unambiguous finite memory automata.	automata theory;congruence of squares;emoticon;existential quantification;fma instruction set;finite-state machine;first-order logic;first-order predicate;linear algebra;maximal set;regular language;turing completeness;undecidable problem	Thomas Colcombet;Clemens Ley;Gabriele Puppis	2014	Logical Methods in Computer Science	10.2168/LMCS-11(3:10)2015	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-5.7755174085202245	18.591592519656682	69322
879b86c161982a7f7c0257f07f9c20d12ff08bd7	predicate abstraction and cegar for higher-order model checking	automatic verification;higher order model checking;higher order;model checking;dependent types;higher order functions;predicate abstraction;cegar	Higher-order model checking (more precisely, the model checking of higher-order recursion schemes) has been extensively studied recently, which can automatically decide properties of programs written in the simply-typed λ-calculus with recursion and finite data domains. This paper formalizes predicate abstraction and counterexample-guided abstraction refinement (CEGAR) for higher-order model checking, enabling automatic verification of programs that use infinite data domains such as integers. A prototype verifier for higher-order functional programs based on the formalization has been implemented and tested for several programs.	algorithm;best, worst and average case;exptime;experiment;human brain project;model checking;polynomial;predicate abstraction;program optimization;prototype;recursion;refinement (computing);scalability;simply typed lambda calculus;worst-case complexity	Naoki Kobayashi;Ryosuke Sato;Hiroshi Unno	2011		10.1145/1993498.1993525	model checking;dependent type;higher-order logic;computer science;theoretical computer science;programming language;higher-order function;abstraction model checking;symbolic trajectory evaluation	PL	-16.398721598981457	24.475248026781326	69514
59b2dd63edca6e89883ceb8591ea53be42f89e39	efficient sat-based unbounded symbolic model checking using circuit cofactoring	fixed point;computability;logic circuits;quantifier elimination;boolean functions;industrial design;model checking	We describe an efficient approach for SAT-based quantifier elimination that significantly improves the performance of pre-image and fixed-point computation in SAT-based unbounded symbolic model checking (UMC). The proposed method captures a larger set of new states per SAT-based enumeration step during quantifier elimination, in comparison to previous approaches. The novelty of our approach is in the use of circuit-based cofactoring to capture a large set of states, and in the use of a functional hashing based simplified circuit graph to represent the captured states. We also propose a number of heuristics to further enlarge the state set represented per enumeration, thereby reducing the number of enumeration steps. We have implemented our techniques in a SAT-based UMC framework where we show the effectiveness of SAT-based existential quantification on public benchmarks, and on a number of large industry designs that were hard to model check using purely BDD-based techniques. We show several orders of improvement in time and space using our approach over previous CNF-based approaches. We also present controlled experiments to demonstrate the role of several heuristics proposed in the paper. Importantly, we were able to prove using our method the correctness of a safety property in an industry design that could not be proved using other known approaches.	boolean satisfiability problem;computation;conjunctive normal form;correctness (computer science);existential quantification;experiment;heuristic (computer science);model checking;quantifier (logic)	Malay K. Ganai;Aarti Gupta;Pranav Ashar	2004	IEEE/ACM International Conference on Computer Aided Design, 2004. ICCAD-2004.	10.1145/1112239.1112322	model checking;discrete mathematics;quantifier elimination;industrial design;logic gate;computer science;theoretical computer science;mathematics;fixed point;computability;boolean function;algorithm	EDA	-13.970102635173596	26.397820075250966	69587
b3f4e17d527abf4baf7f3b3cd1efe7c12dbf1b17	determination of variable dependence information through abstract interpretation	informatica;program generation;logic programs;abstract interpretation	Traditional schemes for abstract interpretation-based global analysis of logic programs generally focus on obtaining procedure argument mode and type information. Variable sharing information is often given only the attention needed to preserve the correctness of the analysis. However, such sharing information can be very useful. In particular, it can be used for predicting run-time goal independence, which can eliminate costly run-time checks in and-parallel execution. In this paper, a new algorithm for doing abstract interpretation in logic programs is described which infers the dependencies of the terms bound to program variables with increased precisión and at all points in the execution of the program, rather than just at a procedure level. Algorithms are presented for computing abstract entry and success substitutions which extensively keep track of variable aliasing and term dependence information. The algorithms are illustrated with examples.	abstract interpretation;algorithm;aliasing;compiler;correctness (computer science);logic programming;parallel computing;top-down and bottom-up design;unification (computer science)	Kalyan Muthukumar;Manuel V. Hermenegildo	1989			computer science;theoretical computer science;algorithm	PL	-17.797802940760736	24.605247573227405	69616
4764bf5f8d8ce14d75e6b4b2284c87d5a4697e48	making petri nets safe and free of internal transitions	petri net	This paper discusses the following results: that bounded Petri nets can be transformed into pomset-equivalent safe nets; that boundedmarked graphs can be transformed into step-language-equivalent safe marked graphs; that safe labelled marked graphs can be transformed into τ*-free safe labelled marked graphs; and that marked graphs can be separated. The paper also lists some open problems that have arisen in this context.	petri net	Eike Best;Philippe Darondeau;Harro Wimmel	2007	Fundam. Inform.		computer science;distributed computing;petri net;algorithm	EDA	-7.625040454283655	23.55982996588894	69853
2bfeb2405535f70bdd7f58e00c37db84c2200d3d	an optimal algorithm for purging regular schemes	degeneration;optimal algorithm	A joint application of four optimizing transformations for purging imperative programs—elimination of useless statements, unwinding of degenerate loops, removal from loops, and removal from branch statements—is considered. A model of regular schemes is introduced in terms of which the transformations and their context conditions are formulated. In the class of regular schemes, a subclass of irredundant schemes, which correspond to program fragments without redundant calculations, is separated. For the irredundant schemes, the context conditions of the transformations for removal from loops and branch statements are formulated, which are simpler than the standard context conditions. Algorithms for the elimination of useless statements, unwinding of degenerate loops, and removal from loops and hammocks are described. The correctness of the algorithms constructed is noted, and estimates of the time and memory required for their operation are given. The algorithms are shown to be optimal in terms of the number of the transformations used: the algorithms of elimination of useless statements and unwinding of degenerate loops are optimal in the whole class of regular schemes, and the algorithm of removal from loops and hammocks is optimal in the class of irredundant schemes without degenerate subloops. The practical implementation of the algorithms constructed is described.	algorithm;correctness (computer science);imperative programming;loop unrolling	Denis L. Uvarov	2000	Programming and Computer Software	10.1023/A:1026652406001	discrete mathematics;computer science;theoretical computer science;mathematics;algorithm	PL	-16.038777093724576	22.474617802522346	69895
b09c3c978a7229f716ecd150064a46af4e0c2463	concurrent semantics without the notions of state or state transitions	transition state;distributed system;domain theory;sistema transicion;systeme reparti;metric space;espace metrique;sistema temporizado;espacio metrico;metodo formal;timed system;methode formelle;semantics;simultaneidad informatica;concurrent program;punto fijo;semantica;semantique;transition system;formal method;fixed point;concurrency;sistema repartido;systeme transition;concurrent systems;point fixe;estado transitorio;programa competidor;transition systems;systeme temporise;concurrent programs;simultaneite informatique;fix point;etat transition;state transition;programme concurrent	This paper argues that basing the semantics of concurrent systems on the notions of state and state transitions is neither advisable nor necessary. The tendency to do this is deeply rooted in our notions of computation, but these roots have proved problematic in concurrent software in general, where they have led to such poor programming practice as threads. I review approaches (some of which have been around for some time) to the semantics of concurrent programs that rely on neither state nor state transitions. Specifically, these approaches rely on a broadened notion of computation consisting of interacting components. The semantics of a concurrent compositions of such components generally reduces to a fixed point problem. Two families of fixed point problems have emerged, one based on metric spaces and their generalizations, and the other based on domain theories. The purpose of this paper is to argue for these approaches over those based on transition systems, which require the notion of state.	computation;concurrency (computer science);fixed point (mathematics);interaction;software design pattern;theory;transition system	Edward A. Lee	2006		10.1007/11867340_2	discrete mathematics;formal methods;concurrency;metric space;computer science;domain theory;mathematics;semantics;fixed point;transition state;programming language;algorithm	Logic	-8.797462210343976	22.154030607167613	69924
75df4a622ef203d0a54e1f8d2582c8a44eaebd9a	rational spaces and set constraints	topological space;finite automaton;technical report;program analysis;computer science;type inference	Set constraints are inclusions between expressions denoting sets of ground terms. They have been used extensively in program analysis and type inference. In this paper we i n vestigate the topological structure of the spaces of solutions to systems of set constraints. We i d e n tify a family of topological spaces called rational spaces, which formalize the notion of a topological space with a regular or self-similar structure, such a s the Cantor discontinuum or the space of runs of a nite automaton. We develop the basic theory of rational spaces and derive generalizations and proofs from topological principles of some results in the literature on set constraints.	automaton;cantor;program analysis;rational set;self-similarity;spaces;type inference	Dexter Kozen	1996	Theor. Comput. Sci.	10.1016/0304-3975(96)00070-9	program analysis;closed set;topological dynamics;space;t1 space;topological vector space;category of topological spaces;combinatorics;discrete mathematics;function space;topological tensor product;connected space;isolated point;topological manifold;computer science;technical report;type inference;mathematics;topological space;finite-state machine;hausdorff space;separated sets;programming language;locally convex topological vector space;specialization (pre)order;zero-dimensional space;algorithm;finite topological space;homeomorphism;algebra	Logic	-10.318388409149216	18.94204557903923	69933
d8f57352a359dd8b0ff97a3193926288ec54f440	discrete timed automata and mona: description, specification and verification of a multimedia stream	second order;developpement logiciel;systeme temps reel;distributed system;time dependent;systeme reparti;multimedia;securite;multimedia streaming;specification;metodo formal;methode formelle;estructura automata;invarianza;automate temporise;qa 76 software;discrete time;fenetre coulissante;safety properties;formal method;automata contemporizado;invariance;computer programming;second order logic;logica orden 2;logique ordre 2;sistema repartido;dependance du temps;time dependence;specification and verification;decision procedure;especificacion;desarrollo logicial;structure automate;automaton structure;software development;safety;ventana deslizante;real time system;timed automata;sistema tiempo real;seguridad;dependencia del tiempo;sliding window	MONA implements an efficient decision procedure for the weak second-order logic WS1S, and has already been applied in many non-trivial problems. Among these, we follow on from the work of Smith and Klarlund on the verification of a sliding-window protocol. This paper extends the scope of MONA to the verification of time-dependent protocols. We present Discrete Timed Automata (DTA) as a suitable formalism to specify and verify such protocols. In this paper our case study will be the specification and verification of a multimedia stream. DTA are as much influenced by IO Automata (syntactically) as they are by Timed Automata (semantically). A composition strategy is given to combine a set of synchronising automata, resulting in a product automaton over which safety properties can be verified. Invariance proofs are then performed inductively on the automaton structure. MONA supports the mechanical verification of invariance proofs.	decision problem;digital television adapter;formal grammar;timed automaton	Rodolfo Gómez;Howard Bowman	2003		10.1007/978-3-540-39979-7_12	simulation;real-time operating system;formal methods;computer science;artificial intelligence;programming language;timed automaton;second-order logic;algorithm	Logic	-10.087597166193435	26.029309111029104	70124
7951f4c48b872c62f7a10db1945fd25a8a25b5f6	a causal semantics for time petri nets	distributed system;systeme reparti;partial order semantics;linear constraints net theory partial order semantics processes timing analysis;red petri;efficient algorithm;language theory;processes;semantics;analyse temporelle;teoria lenguaje;linear constraint;satisfiability;semantica;semantique;analisis temporal;time analysis;algorithme;optimization problem;algorithm;linear constraints;time petri net;sistema repartido;net theory;timing analysis;network theory;petri net;theorie reseau;theorie langage;reseau petri;algoritmo	The objective of this work is to give time Petri nets a partial order semantics, akin to the nonsequential processes of untimed net systems. To this end a time process of a time Petri net is defined as a traditionally constructed causal process with a valid timing. A timing is a labelling that attaches occurrence times to the events of the process that must satisfy specific validness criteria. The main result of the paper is the bijective correspondence between firing schedules (the classical interleaving semantics of time Petri nets) and linearizations of time processes. The result shows that time processes correctly represent the behavior of the system. Using the definition of validness, an efficient algorithm for checking validness of given timings is derived. Also a sufficient condition is given for when the invalidity of timings for a process can be inferred from its initial subprocess. To compute, e.g. the maximum time separation between two events in a time process an alternative characterization of validness is developed. This definition is used to derive an algorithm for constructing the set of all valid timings for a process. The set of valid timings is presented as sets of alternative linear constraints, which can be used in optimization problems. It is shown that the existence of a valid timing for a given process can be decided in NP time.	causal filter;petri net	Tuomas Aura;Johan Lilius	2000	Theor. Comput. Sci.	10.1016/S0304-3975(99)00114-0	network theory;optimization problem;combinatorics;discrete mathematics;computer science;artificial intelligence;philosophy of language;mathematics;semantics;petri net;static timing analysis;algorithm;satisfiability	Logic	-8.586143441032906	24.150598238602125	70173
389b3bf09fe129398f0b7480eb732f57933bd0b6	evc: a validity checker for the logic of equality with uninterpreted functions and memories, exploiting positive equality, and conservative transformations	verification;microprocessor;optimisation;automatic proving;memory function;optimizacion;fonction memoire;logique propositionnelle;demostracion automatica;parametrization;equality with uninterpreted functions and memories;satisfiability;parametrizacion;boolean satisfiability;demonstration automatique;formal verification;propositional logic;movimiento traslacion;verification formelle;mouvement translation;optimization;funcion memoria;microprocesseur;satisfaisabilite;logica proposicional;verificacion;translation motion;microprocesador;parametrisation	The property of Positive Equality [2] dramatically speedsup validity checkingof formulas in the logic of Equality with UninterpretedFunctionsand Memories(EUFM) [4]. The logic expressescorrectnessof high-level microprocessors. We presentEVC (Equality Validity Checker)—a tool that exploits Positive Equalityandotheroptimizationswhentranslatinga formula in EUFM to a propositional formula, which can then be evaluatedby any Booleansatisfiability (SAT) procedure.EVC hasbeenusedfor the automaticformal verification of pipelined, superscalar , and VLIW microprocessors. 1 Intr oduction Formal verification of microprocessorshas historically required extensi ve manual intervention.Burch andDill [4] raisedthe degreeof automationby usingflushing— feedingtheimplementationprocessorwith bubblesin orderto completepartially executedinstructions—tocomputeamappingfrom implementationto specificationstates. Thecorrectnesscriterionis thatonestepof theimplementationshouldbeequivalentto 0, or 1, or up to k (for animplementationthatcanfetchup to k instructionspercycle) stepsof a specificationsingle-cycle processorwhen startingfrom equivalent states, whereequivalency is determinedvia flushing.However, theverificationefficiency has still dependedon manuallyprovided case-splittingexpressions[4][5] whenusingthe specializeddecisionprocedureSVC [16]. In order to apply the methodto complex superscalarprocessors, Hosabettu[9] and Sawada [15] requiredmonthsof manual work, using the theoremproversPVS [13] andACL2 [10], respecti vely. We present EVC, a validity checker for the logic of EUFM, as an alternati ve highly ef ficient tool. 2 Hardware Description Language In orderto beverifiedwith EVC, ahigh-level implementationprocessorandits specificationmustbe definedin our HardwareDescriptionLanguage(HDL). That HDL is similar to a subsetof Verilog [17], exceptthat word-level valuesdo not have dimensionsbut arerepresentedwith a singleterm-level expression,accordingto thesyntax of EUFM [4]. Hence,netsare requiredto be declaredof type term or type bit. Additionally, anetcanbedeclaredasinput, e.g.,thephaseclocksthatdeterminethe updatingof stateor the signalsthat control the flushing.The HDL hasconstructsfor Mir oslav N. Velev* mvelev@ece.cmu.edu http://www.ece.cmu.edu/~mvelev Randal E. Bryant‡, * randy.bryant@cs.cmu.edu http://www.cs.cmu.edu/~bryant *Department of Electrical and Computer Engineering ‡School of Computer Science Carnegie Mellon Uni versity, Pittsburgh, PA 15213, U.S.A. 1. This research w as supported by the SRC under contract 00-DC-684. the definition of memories and latches (see Fig. 2 for the description of two stages of the processor in Fig. 1). Memories and latches can have multiple input and/or output ports—of type inport and outport, respectively. Latch ports have an enable signal and a list of data signals. Memory ports additionally have an address signal after the enable. Logic gates—and, or, not, = (term-level equality comparator), and mux (multiplexor, i.e., ITE operator)—are used for the description of the control path of a processor. Uninterpreted functions and uninterpreted predicates—such as ALU in Fig. 2—are used to abstract blocks of combinational logic—the ALU in Fig. 1—as black boxes. Uninterpreted functions and uninterpreted predicates with no arguments are considered as term variables and Boolean variables, respectively, and can be used to abstract constant values that have special semantic meaning, e.g., the data value 0. Fig. 1. Block diagram of a 3-stage pipelined processor. Flush_bar = (not Flush) IF_Valid = (and Valid Flush_bar)	arithmetic logic unit;black box;cpu cache;combinational logic;comparator;computer engineering;computer science;dfa minimization;diagram;formal verification;hardware description language;high- and low-level;instruction pipelining;microprocessor;multiplexer;pipeline (computing);semiconductor research corporation;superscalar processor;uninterpreted function;vesa enhanced video connector;verilog;xfig	Miroslav N. Velev;Randal E. Bryant	2001		10.1007/3-540-44585-4_20	parametrization;discrete mathematics;computer science;theoretical computer science;mathematics;programming language;algorithm	Logic	-14.690542000413185	30.685324777847818	70515
fc37646813c1a9f577e07afa2b78ce5300cccef4	pair-sharing over rational trees	language use;sharing analysis;a general works;logic programs;abstract interpretation	Abstract   Sharing information is useful in specialising, optimising and parallelising logic programs and thus sharing analysis is an important topic of both abstract interpretation and logic programming. Sharing analyses infer which pairs of program variables can never be bound to terms that contain a common variable. We generalise a classic pair-sharing analysis from Herbrand unification to trace sharing over rational tree constraints. This is useful for reasoning about programs written in SICStus and Prolog-III because these languages use rational tree unification as the default equation solver.		Andy King	2000	J. Log. Program.	10.1016/S0743-1066(00)00009-1	discrete mathematics;theoretical computer science;mathematics;programming language;algorithm	Logic	-16.1049925421869	20.533850828633042	70535
7469fe0784b098bde382a46471805493bcfa9465	satisfiability of general intruder constraints with and without a set constructor	multiple intruders;satisfiability;aci;decision problem;decision procedure;constraint solving;constraint system;dolev yao intruder;dolev yao deduction system;security;equational theory;associative commutative;security protocol;deducibility constraints	Many decision problems on security protocols can be reduced to solving socalled intruder constraints in the Dolev Yao model. Most constraint solving procedures for cryptographic protocol security rely on two properties of constraint systems called knowledge monotonicity and variable-origination. In this work we relax these restrictions by giving an NP decision procedure for solving general intruder constraints (that do not have these properties). Our result extends a rst work by L. Mazaré in several directions: we allow non-atomic keys, and an associative, commutative and idempotent symbol (for modeling sets). We also discuss several new applications of our result. Key-words: Security, Constraint solving, Dolev-Yao intruder, equational theory, ACI ∗ CASSIS, Loria-INRIA Grand Est, Tigran.Avanesov@loria.fr † IRIT-Université Paul Sabatier, ychevali@irit.fr ‡ CASSIS, Loria-INRIA Grand Est, Michael.Rusinowitch@loria.fr § CASSIS, Loria-INRIA Grand Est, Mathieu.Turuani@loria.fr in ria -0 04 80 63 2, v er si on 3 21 M ay 2 01 0 Satis abilité des systèmes de contraintes généraux avec et sans constructeur d'ensemble Résumé : De nombreux problèmes de décision relatifs à la sécurité des protocoles cryptographiques peuvent être réduits à la résolution de ce que l'on appelle contraintes d'intrus dans le modèle de Dolev Yao . La plupart des procédures de résolution de contraintes pour la sécurité des protocoles se basent sur deux propriétés de ces systèmes appelées monotonie des connaissances et ordonnancement des variables. Dans ce travail nous relâchons ces restrictions en présentant une procédure de décision dans NP pour la résolution de contraintes d'intrus générales, c'est à dire non soumises aux deux propriétés précédentes. Notre résultat prolonge un premier travail de L. Mazaré dans plusieurs directions: nous autorisons les clés non-atomique, ainsi qu'un symbole associatif, commutatif et idempotent (pour modéliser les ensembles). Nous considérons également de nouvelles applications de ce résultat. Mots-clés : Sécurité, résolution de contraintes, intrus de Dolev-Yao, théorie équationelle, ACI in ria -0 04 80 63 2, v er si on 3 21 M ay 2 01 0 Satis ability of General Intruder Constraints with and without a Set Constructor 3	ac (complexity);bibliothèque de l'école des chartes;constraint satisfaction problem;cryptographic protocol;cryptography;decision problem;dolev–yao model;idempotence;linear algebra;np (complexity);sans institute;yao graph	Tigran Avanesov;Yannick Chevalier;Michaël Rusinowitch;Mathieu Turuani	2017	J. Symb. Comput.	10.1016/j.jsc.2016.07.009	combinatorics;discrete mathematics;information security;decision problem;mathematics;algorithm;algebra;satisfiability	Crypto	-7.477193903105907	20.40561680802076	70548
033b9cc92391de9fe1f90f59bc30168ec309c54d	approximation metrics for discrete and continuous systems	lyapunov methods;sistema transicion;approximate bisimulation relations;system approximation;funcion lyapunov;fixed point theorem;langage simulation;approximation metrics;sistema hibrido;hierarchized structure;lyapunov function;bepress selected works;lenguaje simulacion;bisimulacion;pseudo metrics;branching;modelo hibrido;abstraction;metrics;continuous time systems robustness computational modeling costs safety concurrent computing formal verification engineering profession systems engineering and theory automata;bisimulation;metric;structure hierarchisee;transition systems abstraction approximation bisimulation metrics;punto fijo;approximate simulation;continuous system;modele physique;probabilistic approach;modele hybride;systeme deterministe;theoreme point fixe;teorema punto fijo;hybrid model;transition system;approximation;fixed point;abstraction approximation bisimulation metrics transition systems;approximation theory;continuous time systems;systeme transition;sistema determinista;fonction lyapunov;lyapunov like functions;point fixe;enfoque probabilista;approche probabiliste;ramificacion;exact algorithm;transition systems;hybrid system;modelo fisico;ramification;lyapunov like functions approximation metrics discrete systems continuous time systems system approximation approximate language inclusion approximate simulation approximate bisimulation relations pseudo metrics static game;metrico;simulation language;approximate language inclusion;discrete systems;physical model;static game;lyapunov methods approximation theory continuous time systems discrete systems;computer simulation;fix point;composition operator;estructura jerarquizada;metrique;discrete system;deterministic system;systeme hybride	Established system relationships for discrete systems, such as language inclusion, simulation, and bisimulation, require system observations to be identical. When interacting with the physical world, modeled by continuous or hybrid systems, exact relationships are restrictive and not robust. In this paper, we develop the first framework of system approximation that applies to both discrete and continuous systems by developing notions of approximate language inclusion, approximate simulation, and approximate bisimulation relations. We define a hierarchy of approximation pseudo-metrics between two systems that quantify the quality of the approximation, and capture the established exact relationships as zero sections. Our approximation framework is compositional for a synchronous composition operator. Algorithms are developed for computing the proposed pseudo-metrics, both exactly and approximately. The exact algorithms require the generalization of the fixed point algorithms for computing simulation and bisimulation relations, or dually, the solution of a static game whose cost is the so-called branching distance between the systems. Approximations for the pseudo-metrics can be obtained by considering Lyapunov-like functions called simulation and bisimulation functions. We illustrate our approximation framework in reducing the complexity of safety verification problems for both deterministic and nondeterministic continuous systems	approximation algorithm;bisimulation;computation;deterministic algorithm;directed graph;discrete system;fixed point (mathematics);hausdorff dimension;hybrid system;interaction;lyapunov fractal;nonlinear system;scott continuity;semi-continuity;simulation;software metric;temporal logic	Antoine Girard;George J. Pappas	2007	IEEE Transactions on Automatic Control	10.1109/TAC.2007.895849	computer simulation;combinatorics;discrete mathematics;metric;branching;physical model;lyapunov function;bisimulation;composition operator;approximation;discrete system;deterministic system;control theory;mathematics;fixed point;abstraction;ramification;fixed-point theorem;metrics;algorithm;hybrid system;simulation language;approximation theory	Logic	-8.548888358384541	25.52862004154867	70804
a6f7839a8c54ab62b39cebbebe1b46fa8fa1a118	cryptographic logical relations	logical relation;computadora;modelo dinamico;generation;observational equivalence;dynamique;category;generacion;ordinateur;logic;dynamic model;exploracion;cryptographic protocol;computer;equivalence;metalangage;dinamica;protocole cryptographique;categorie;formal verification;construccion;monad;metalanguage;dynamics;informatique theorique;modele dynamique;68xx;exploration;verification formelle;verification of cryptographic protocols;construction;equivalencia;contextual equivalence;logique;logica;68q60;computer theory;dynamic key generation;informatica teorica;metalenguaje	Using contextual equivalence (a.k.a. observational equivalence) to specify security properties is an important idea in the field of formal verification of cryptographic protocols. While contextual equivalence is difficult to prove directly, one is usually able to deduce it using the so-called logical relations in typed λ-calculi. We apply this technique to the cryptographic metalanguage—an extension of Moggi’s computational λ-calculus, where we use Stark’s model for name creation to explore the difficult aspect of dynamic key generation. The categorical construction of logical relations for monadic types (by Goubault-Larrecq et al.) then allows us to derive logical relations over the category SetI . Although SetI is a perfectly adequate model of dynamic key generation, it lacks in some aspects when we study relations between programs in the metalanguage. This leads us to an interesting exploration of what should be the proper category to consider. We show that, to define logical relations in the cryptographic metalanguage, a better choice of category is SetI→ that we proposed in [Y. Zhang, D. Nowak, Logical relations for dynamic name creation, in: Proceedings of the 17th International Workshop of Computer Science Logic and the 8th Kurt Gödel Colloqium, CSL & KGL, in: Lecture Notes in Computer Science, vol. 2803, Springer-Verlag, 2003, pp. 575–588]. However, this category is still lacking in some subtler aspects and we propose a refined category SetPI → to fix the flaws, but our final choice is SetI×I , which is equivalent to SetPI → . We define the contextual equivalence based on SetI×I and show that the cryptographic logical relation derived over SetI×I is sound and can be used to verify protocols in practice. c © 2007 Elsevier B.V. All rights reserved.	continuation;cryptographic protocol;cryptography;definition;fm broadcasting;formal verification;fuzzy set;gödel;jean;key generation;kripke semantics;lambda calculus;lecture notes in computer science;logical relations;monad (functional programming);needham–schroeder protocol;observational equivalence;pierce oscillator;programming computable functions;programming language theory;public-key cryptography;recursion;requirement;springer (tank);turing completeness;verification and validation;walter pitts	Yu Zhang	2008	Theor. Comput. Sci.	10.1016/j.tcs.2007.09.033	logical equivalence;equivalence;dynamics;combinatorics;discrete mathematics;generation;construction;category;exploration;formal verification;metalanguage;computer science;cryptographic protocol;mathematics;programming language;monad;logic;algorithm;algebra	Logic	-16.698328059868274	18.273885825402335	70957
4639eb207195500dc87e0dfa77526484e070a867	intel coretm i7 processor execution engine validation in a functional language based formal framework		  Formal verification of microprocessor components has been pursued in Intel processor development projects in various forms  for over a decade. Usually formal verification has been used to supplement more traditional coverage oriented testing activities.  For the Intel^{\mbox{\tiny\circledR}} CoreTM^{\mbox{\tiny TM}} i7 design we took a step further and used formal verification as the primary validation vehicle for the core execution cluster,  the component responsible for the functional behaviour of all microinstructions. We applied symbolic simulation based formal  verification techniques for full datapath, control and state validation for the cluster, and dropped coverage driven testing  entirely [2]. The project, involving some twenty person years of verification work, is one of the most ambitious formal verification  efforts in the hardware industry to date, and shows that under the right circumstances, full formal verification of a major  design component is a feasible, industrially viable and competitive validation approach.      Technically the verification work was carried out in the Forte verification framework, originally built on top of the Voss  system [1]. It is based on a strongly typed ML-like [4] lazy functional programming language reFLect. Most of the verification code is written in reFLect: specifications, whether they are functional specifications or relational  constraints, verification facilities, analysis routines etc. The execution of an individual verification task in the framework  amounts to the evaluation of a reFLect program, and the entire verification initiative involves significant software engineering  aspects [3]. In the reFLect language binary decision diagrams are first-class objects: the type Bool includes not just the  constants T and F, but arbitrary BDD’s. For verification purposes, a very important feature of the language is that it allows  symbolic evaluation of objects containing BDD’s and symbolic circuit simulation using BDD’s. Similar facilities exist for non-canonical graph  representations of Booleans, used for interfacing with satisfiability solvers.      	functional programming	Roope Kaivola	2011		10.1007/978-3-642-18378-2_1	computer architecture;parallel computing;computer science;programming language	PL	-15.473480210832378	30.476220089769356	71390
09cbe931ea0e6e9ad3ce3be9c95e501f3ba02c03	a formula-driven modular attack on state explosion	temporal logic;model checking;compositionality;state explosion;ccs	A common characteristic of the new distributed systems is the increasing complexity. Useful paradigms to cope with the complexity of systems are modularity and compositionality. In this paper we define a compositional method to attack the state explosion problem in model checking. The method, given a formula to be checked on a system composed of a set of parallel processes, allows syntactically reducing in a modular way the processes, in order to reduce the state space of their composition. The reduction is formula driven and is based on a notion of equivalence between processes, which is a congruence w.r.t. the parallel composition operator.		Nicoletta De Francesco;Antonella Santone	2002	Int. J. Found. Comput. Sci.	10.1142/S0129054102001412	model checking;discrete mathematics;temporal logic;computer science;theoretical computer science;mathematics;algorithm;principle of compositionality	Logic	-10.98992047918435	23.220038167809353	71591
566f3e0810253a33f1fac50dfd2a11656c17de5c	implementing sequential and parallel programs for the homing sequence problem	protocol verification;fault detection;parallel programs	Homing sequences play an important role in the testing of nite state systems and have been used in a number of applications such as hardware fault-detection 7], protocol veriication 4], and learning algorithms 11, 3, 1] etc. Here we present a parallel program implementation that nds a homing sequence for an input DFA. Our program can handle randomly generated instances with millions of states, and all DFA's with thousands of states. In addition to the design, analysis and implementation of the algorithm, we also discuss what constitute good test cases to test programs that deal with nite automata.	algorithm;automata theory;machine learning;missile guidance;procedural generation;test case	Bala Ravikumar;X. Xiong	1996		10.1007/3-540-63174-7_10	parallel computing;real-time computing;computer science;distributed computing	Logic	-13.02630158569562	27.538402931828013	71631
12a055170929db73e3a5a9763ef01c23ee8f6bc7	a novel test case generation method for prolog programs based on call patterns semantics	control flow graph;test case generation	A natural way to generate test cases for a Prolog program is to view the call patterns of the procedures in the program as an implicit representation of the control flow graph (CFG) of the program. This paper explores the idea by proposing a call patterns-based test case generation method, where a set of call patterns or computed answers is used to describe the paths in a CFG. With a constraint-based call patterns semantics, this method is formalized. Through the use of a proper constraints solver, we can generate test cases automatically from the sets of constraints. This method can be based on any approximation of the call patterns semantics. So compared with traditional CFG-based test case generation, the method is more flexible and can be easily adapted to meet the requirements of a tester expressed by the approximation of the call patterns semantics we use.	prolog;test case	Lingzhong Zhao;Tianlong Gu;Junyan Qian;Guoyong Cai	2007		10.1007/978-3-540-76637-7_8	computer science;theoretical computer science;programming language;algorithm;control flow graph	PL	-18.688767532117524	25.416857581339425	71685
02fafeae8822f755cff55011b97a0ebcb3cd8322	equilibrium and termination	correspondence problem;turing machine;continuous time markov chain;graph rewriting;logic in computer science	In this note we explore an aspect of the relationship between the notion of equilibrium of a continuous time Markov chain (CTMC) and that of the traditional concept of termination in rewriting systems. Unlike in deterministic dynamical systems, a Markov chain equilibrium is not a definite state, but rather a probability over the state space which is invariant under the Markov semigroup, and satisfies an additional property explained right below.	challenge-handshake authentication protocol;cobham's thesis;computable function;curve fitting;dynamical system;ising model;logic programming;markov chain;np (complexity);np-completeness;petri net;post correspondence problem;reachability;rewrite (programming);rewriting;state space;static program analysis;undecidable problem	Vincent Danos;Nicolas Oury	2010		10.4204/EPTCS.26.7	turing degree;combinatorics;discrete mathematics;function problem;turing reduction;alternating turing machine;computer science;turing machine;continuous-time markov chain;post correspondence problem;universal turing machine;description number;mathematics;word problem;probabilistic turing machine;correspondence problem;programming language;computational complexity theory;algorithm;undecidable problem;reduction;halting problem;graph rewriting	Logic	-6.588750398283042	23.6705914061236	71906
bf96ec7b88361ec17ef0915f660b2bfb000ae90a	distributed supervisor synthesis for automated manufacturing systems with flexible routes and assembly operations using petri nets	production management assembling control system synthesis distributed control manufacturing systems predictive control;system recovery manufacturing systems automation petri nets trajectory process control computers;model predictive control techniques distributed supervisor synthesis automated manufacturing systems flexible routes assembly operations petri nets ams aesm structure marked graph blocks deadlock resolution monolithic methodologies siphon based mechanism	Automated manufacturing systems (AMSs) are developing rapidly with increasingly sophisticated operations and complex topologies. In this paper, we propose a new kind of AMS structure, namely, AESMs, with processes expressed by flexible routes and embedded by marked graph blocks. Flexible routes and assembly operations are combined organically in this model, making it structurally complex and applicably adaptable. Deadlock resolution in AMSs is a fundamental and prerequisite issue. Conventional approach is based on monolithic methodologies by means of siphons, i.e., a special structural object closely related to deadlocks. Analysis shows that siphon-based mechanism is of limited applicability to control AESMs. On the contrary, we utilize a distributed approach in accordance with the philosophy of model predictive control techniques, which can create a trajectory dynamically leading the entire system to its desired destination. In our strategy, whether or not concerned tokens can reach critical places or critical place unities determines the feasibility of each step's execution. Control strategy is applied to processes locally such that they can concurrently proceed at the same pace. Global goals become attainable through the local observation, control, and execution upon local processes without knowing external and extra information.	admissible heuristic;assembly language;automation;complex systems;control theory;deadlock;embedded system;fault tolerance;liveness;marked graph;monolithic kernel;petri net;reachability;vhdl-ams	Chen Chen;Yan Yang;Hesuan Hu	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487552	control engineering;simulation;engineering	Robotics	-5.910748252976964	29.328974072907098	71946
cd96397a24bdafe8d9eae71188e7b1c0d7cd7f9a	a new mapping between combinatorial proofs and sequent calculus proofs read out from logical flow graphs	55qxx;prueba;tautologie;topological space;closure;combinatorial proof;fibration;graph flow;skew fibration;65jxx;logique propositionnelle;carta de datos;flujo grafo;homomorphism;sequent calculus;03b05;proposition;proposicion;preuve;propositional logic;calculo sequente;flot graphe;informatique theorique;mappage;homotopy group;68r10;homomorphisme;invariante;completitud;espace topologique;mapping;calcul sequent;cerradura;completeness;homomorfismo;logica proposicional;proof homomorphism;completude;proof;invariant;fermeture;espacio topologico;computer theory;informatica teorica	Combinatorial proofs are abstract invariants for sequent calculus proofs, similarly to homotopy groups which are abstract invariants for topological spaces. Sequent calculus fails to be surjective onto combinatorial proofs, and here we extract a syntactically motivated closure of sequent calculus from which there is a surjection onto a complete set of combinatorial proofs. We characterize a class of canonical sequent calculus proofs for the full set of propositional tautologies and derive a new completeness theorem for combinatorial propositions. For this, we define a new mapping between combinatorial proofs and sequent calculus proofs, different from the one originally proposed, which explicitly links the logical flow graph of a proof to a skew fibration between graphs of formulas. The categorical properties relating the original and the new mappings are explicitly discussed.	sequent calculus	Alessandra Carbone	2010	Inf. Comput.	10.1016/j.ic.2009.01.007	homomorphism;combinatorics;combinatorial proof;discrete mathematics;cut-elimination theorem;completeness;invariant;closure;homotopy group;proof;mathematics;topological space;propositional calculus;sequent;sequent calculus;natural deduction;fibration;algorithm;algebra	Logic	-8.172423777715293	18.580868618966242	71999
1c10295cdd9509ba7021a8d44959274b4934da77	towards an industry standard for benchmarking artificial intelligence systems		Over the past three decades, the Transaction Processing Performance Council (TPC) has developed many standards for performance benchmarking. These standards have been a significant driving force behind the development of faster, less expensive, and more energy efficient systems. Historically, we have seen benchmark standards for transaction processing, decision support systems and virtualization. In the recent years the TPC has developed benchmark standards for emerging areas such as big data analytics (BDA), the Internet of Things (IoT), database virtualization and Hyper-Convergence Infrastructure (HCI). This short paper discusses the TPC's plans for creating benchmark standards for Artificial Intelligence (AI) systems.	artificial intelligence;benchmark (computing);big data;broadcast driver architecture;decision support system;human–computer interaction;ibm tivoli storage productivity center;internet of things;the industry standard;transaction processing	Raghunath Othayoth Nambiar	2018	2018 IEEE 34th International Conference on Data Engineering (ICDE)	10.1109/ICDE.2018.00212	data mining;virtualization;efficient energy use;database virtualization;benchmarking;decision support system;big data;transaction processing;computer science;internet of things;artificial intelligence	DB	-6.063976718689805	32.1320122119625	72351
05c180ce0d8bd98862cd702828d6aade208ba6bd	bounded semantics of ctl and sat-based verification	satisfiability;bounded model checking;model checking;error detection;state explosion;symbolic model checking	Bounded model checking has been proposed as a complementary approach to BDD based symbolic model checking for combating the state explosion problem, esp. for efficient error detection [3]. This has led to a lot of successful work with respect to error detection in the checking of LTL, ACTL (the universal fragment of CTL) and ACTL* properties by satisfiability testing [3, 22, 25]. The use of bounded model checking for verification (in contrast to error detection) of LTL and ACTL properties has later also been studied [28, 30]. This paper studies the potentials and limitations of bounded model checking for the verification of CTL and CTL∗ formulas. On the theoretical side, we first provide a framework for discussion of bounded semantics, which serves as the basis for bounded model checking, then extend the bounded semantics of ACTL [30] to a bounded semantics of CTL, and discuss the limitation of developing such a bounded semantics for CTL∗. On the practical side, a deduction of a SAT-based bounded model checking approach for ACTL properties from the bounded semantics of CTL is demonstrated, and a comparison of such an approach with BDD-based model checking is presented based on experimental results.	error detection and correction;linear temporal logic;model checking;natural deduction	Wenhui Zhang	2009		10.1007/978-3-642-10373-5_15	model checking;error detection and correction;computer science;theoretical computer science;ctl*;abstraction model checking;algorithm;satisfiability	Logic	-14.202983984441552	25.9130422730888	72371
70dbc5f364b41b5f5fc80954f3f6ea0db66b4da0	unification problems with one-sided distributivity	distributivite;decomposition;programmation;programacion;unification;theorie equationelle;distributivity;decidibilidad;descomposicion;decidabilite;programming;equational theory;decidability	This work is a study of unification in some equational theories that have a one-sided distributivity axiom: x x (y + z) = x x y + x x z. First one-sided distributivity, the theory which has only this axiom, is studied. It is shown that, although one-sided distributivity is a simple theory in many ways, its unification problem is not trivial, and known universal unification procedures fail to provide a decision procedure for it. We give a unification procedure based on a process of decomposition combined with a generalized occurs check, which may be applied in any permutative theory, and another test. These tests together ensure termination. Next, we show that unification is undecidable if the laws of associativity x + (y + z) = (x + y) + z and a one-sided unit element x x 1 = x are added to one-sided distributivity. Unification under one-sided distributivity with (one-sided) unit element is shown to be as hard as Markov's problem (associative unification), whereas unification under two-sided distributivity, with or without unit element, is NP-hard. The study of these problems is motivated by possible applications in circuit synthesis and by the need for gaining insight in the problem of combining theories with overlapping sets of operator symbols.	han unification	Erik Tidén;Stefan Arnborg	1987	J. Symb. Comput.	10.1016/S0747-7171(87)80026-3	decidability;programming;discrete mathematics;distributivity;unification;mathematics;decomposition;algorithm;algebra	Theory	-8.095911204222201	18.622521988888533	72398
c9d8eacd1ab99e317f3f270a9a10dbd6aef8617c	repetitive quiescence in implementation and testing.		This paper studies implementation relations and testing based on labelled transition systems, using the assumption that implementations communicate with their environment via inputs and outputs. Such implementations are formalized by restricting the class of transition systems to those systems that can always accept input actions. Implementation relations, which formalize the notion of correctness of these implementations with respect to labelled transition system specifications, are defined analogous to the theories of testing equivalence and preorder, and refusal testing. A test generation algorithm is given, which is proved to produce a sound and exhaustive test suite from a specification, i.e., a test suite that fully characterizes the set of correct implementations.	algorithm;correctness (computer science);quiescence search;test suite;transition system;turing completeness	Jan Tretmans	1997			transition system;implementation;equivalence (measure theory);correctness;theoretical computer science;test suite;computer science;preorder	SE	-13.433538080727612	25.295686646538435	72427
25e85c336f033e8929f8c5820b75f7de86b2b390	staging static analyses for program generation	staging;code generation;run time code generation;program generation;conference paper;timing analysis;static analysis	Program generators are most naturally specified using a quote/antiquote facility; the programmer writes programs with holes which are filled in, at program generation time, by other program fragments. If the programs are generated at compile-time, analysis and compilation follow generation, and no changes in the compiler are needed. However, if program generation is done at run time, compilation and analysis need to be optimized so that they will not overwhelm overall execution time. In this paper, we give a compositional framework for defining program analyses which leads directly to a method of staging these analyses. The staging allows the analysis of incomplete programs to be started at compile time; the residual work to be done at run time may be much less costly than the full analysis. We give frameworks for forward and backward analyses, present several examples of specific analyses, and give timing results showing significant speed-ups for the run-time portion of the analysis relative to the full analysis.	acm transactions on programming languages and systems;backward compatibility;code generation (compiler);compile time;compiler;computer science;context-free grammar;data-flow analysis;dataflow architecture;dave grossman (game developer);disk staging;generativecomponents;hybrid algorithm;java;kramer graph;list of astronomical catalogues;parallel computing;programmer;reachability;run time (program lifecycle phase);self-modifying code;static program analysis;symposium on principles of programming languages;theory of cryptography conference;type system	Samuel N. Kamin;Baris Aktemur;Michael Katelman	2006		10.1145/1173706.1173708	parallel computing;real-time computing;computer science;programming language;static timing analysis;static analysis;algorithm;code generation	PL	-18.67674609419004	32.21920425271321	72718
fdd9a8250163af838cb0f6949f148ffbc9214f7f	on maintaining dynamic information in a concurrent environment (preliminary version)	digital disk;algorithm;dynamic information;digital region;compactness;digital convexity	processes under a shared memory model. Abstract data structures have proven to be very helpful in the design of sequential algorithms. Having an arsenal of data structures and efficient implementations makes it easier to formalize problems and to solve them. We hope that the same will be true for concurrent computation.	abstract data type;algorithm;computation;concurrency (computer science);concurrent computing;data structure;shared memory	Udi Manber	1984		10.1145/800057.808691	mathematical optimization;discrete mathematics;control theory;mathematics;compact space;algorithm	Theory	-13.773580524423018	21.700202452924486	72804
37b647c37c781d1c6654ea2889fdb367b7cf40d8	a process algebraic view of linda coordination primitives	coordination language;coordination languages;semantics of linda;asynchronous communication;process algebra;behavioral equivalences	The main Linda coordination primitives (asynchronous communication, read operation, nonblocking in/rd predicates) are studied in a process algebraic setting. A lattice of eight languages is proposed, where its bottom element L is a process algebra differing from CCS only for the asynchrony of the output operation, while all the other languages in the lattice are obtained as extension of this basic language by adding some of the Linda coordination primitives. The observational semantics for these languages are all obtained as the coarsest congruences contained in the barbed semantics, where only tuples are observable. The lattice of the eight languages collapses to a smaller four-points lattice of different bisimulation-based semantics. Notably, for L this semantics is the standard notion of strong bisimulation, where inputs and outputs/tuples are treated symmetrically.	asynchronous i/o;bisimulation;linda (coordination language);linear algebra;observable;process calculus	Nadia Busi;Roberto Gorrieri;Gianluigi Zavattaro	1998	Theor. Comput. Sci.	10.1016/S0304-3975(97)00149-7	combinatorics;process calculus;computer science;theoretical computer science;asynchronous communication;mathematics;programming language;algorithm;algebra	PL	-11.336696136029536	20.90012428861986	72810
ba8147a7f6563a66dd21f7c1a466497928cedbcc	using higher order logic and functional languages to synthesize correct hardware	functional form;hardware synthesis;program verification;functional programming;theorem proving;cad hardware synthesis specification languages higher order logic functional languages hol proof checker scheme hardware synthesis functions pezaris like array multipliers equivalence preserving transformations gate level interconnection lists functional specifications correctness theorems;hardware circuit synthesis logic arrays analytical models design methodology virtual manufacturing logic design voltage control synthetic aperture sonar computer aided software engineering;specification languages;formal logic;design verification;functional language;theorem proving formal logic functional programming program verification specification languages;higher order logic	Higher-order logic (HOL), the HOL proof checker, and the functional language SCHEME have been used to describe and verify several hardware synthesis functions, including one which synthesizes Pezaris-like array multipliers. The synthesis functions are shown to be equivalence preserving transformations. The synthesis functions produce functional forms corresponding to gate level interconnection lists. Proofs of theorems relating the synthesized functional forms to functional specifications are developed within HOL. Unlike simulation-based methods, which require exhaustive case analysis for each implementation, these theorems assert the corrections of all implementations produced by the synthesis functions. The combinations of machine executable synthesis functions and correctness theorems are additional features which would logically extend CAD systems for design synthesis and design verification. >	functional programming	Shiu-Kai Chin;Edward P. Stabler;Kevin J. Greene	1988		10.1109/ICCL.1988.13089	logic synthesis;logic optimization;formal methods;higher-order logic;formal verification;hol;computer science;theoretical computer science;logic simulation;functional logic programming;high-level verification;automated theorem proving;hardware description language;high-level synthesis;programming language;functional programming;higher-order function;logic;intelligent verification;register-transfer level;algorithm;functional verification	Logic	-14.88621453517057	29.882881075092435	72843
508c4ee4256f93614906d2d8b08aa4228ddcc325	decision procedures for automating termination proofs	abstraction;decision procedure;decidability	Automated termination provers often use the following schema to prove that a program terminates: construct a relational abstraction of the program’s transition relation and then show that the relational abstraction is well-founded. The focus of current tools has been on developing sophisticated techniques for constructing the abstractions while relying on known decidable logics (such as linear arithmetic) to express them. We believe we can significantly increase the class of programs that are amenable to automated termination proofs by identifying more expressive decidable logics for reasoning about wellfounded relations. We therefore present a new decision procedure for reasoning about multiset orderings, which are among the most powerful orderings used to prove termination. We show that, using our decision procedure, one can automatically prove termination of natural abstractions of programs.	automated theorem proving;boolean satisfiability problem;decision problem;dershowitz–manna ordering;karp's 21 np-complete problems;np (complexity);quantifier (logic);satisfiability modulo theories;termination analysis	Ruzica Piskac;Thomas Wies	2011		10.1007/978-3-642-18275-4_26	decidability;computer science;abstraction;programming language;algorithm	Logic	-14.875207020396905	22.478501098648675	72918
19c1738a9ed715f9535df425856f9cc740e4be23	a multi-encoding approach for ltl symbolic satisfiability checking	errors;specifications;temporal logic;systems engineering;proving;coding;automata theory;consistency	Formal behavioral specifications written early in the syste m-design process and communicated across all design phases have been show to increase the efficiency, consistency, and quality of the system under develo pment. To prevent introducing design or verification errors, it is crucia l to test specifications for satisfiability. Our focus here is on specifications expressed in linear temp oral logic (LTL). We introduce a novel encoding of symbolic transition-based Büchi automata and a novel, “sloppy,” transition encoding, both of which resul t in improved scalability. We also define novel BDD variable orders based on tree dec omposition of formula parse trees. We describe and extensively test a new m ulti-encoding approach utilizing these novel encoding techniques to create 30 encoding variations. We show that our novel encodings translate to significant, so metimes exponential, improvement over the current standard encoding for symboli c LTL satisfiability checking.	automata theory;büchi automaton;parsing;scalability;test and evaluation master plan;time complexity	Kristin Y. Rozier;Moshe Y. Vardi	2011		10.1007/978-3-642-21437-0_31	temporal logic;computer science;theoretical computer science;automata theory;coding;consistency;algorithm	SE	-12.978451356899175	26.324660163823065	72927
692b24b8f903fcf380ce169a36a0f7a90007d260	agent planning programs	automated planning;reasoning about action and change;synthesis of reactive systems;agent oriented programming	a r t i c l e i n f o a b s t r a c t Keywords: Agent-oriented programming Automated planning Reasoning about action and change Synthesis of reactive systems This work proposes a novel high-level paradigm, agent planning programs, for modeling agents behavior, which suitably mixes automated planning with agent-oriented programming. Agent planning programs are finite-state programs, possibly containing loops, whose atomic instructions consist of a guard, a maintenance goal, and an achievement goal, which act as precondition-invariance-postcondition assertions in program specification. Such programs are to be executed in possibly nondeterministic planning domains and their execution requires generating plans that meet the goals specified in the atomic instructions, while respecting the program control flow. In this paper, we define the problem of automatically synthesizing the required plans to execute an agent planning program, propose a solution technique based on model checking of two-player game structures, and use it to characterize the worst-case computational complexity of the problem as EXPTIME-complete. Then, we consider the case of deterministic domains and propose a different technique to solve agent planning programs, which is based on iteratively solving classical planning problems and on exploiting goal preferences and plan adaptation methods. Finally, we study the effectiveness of this approach for deterministic domains through an experimental analysis on well-known planning domains.	agent-oriented programming;automated planning and scheduling;best, worst and average case;computational complexity theory;control flow;exptime;formal specification;high- and low-level;model checking;postcondition;precondition;programming paradigm	Giuseppe De Giacomo;Alfonso Gerevini;Fabio Patrizi;Alessandro Saetti;Sebastian Sardiña	2016	Artif. Intell.	10.1016/j.artint.2015.10.001	simulation;computer science;artificial intelligence	AI	-14.539775779649482	23.34546311836386	73217
239e2e0381fb0eca5afedb57c195928571f1ba43	critical pair analysis in nominal rewriting		Nominal rewriting (Fernández, Gabbay & Mackie, 2004; Fernández & Gabbay, 2007) is a framework that extends first-order term rewriting by a binding mechanism based on the nominal approach (Gabbay & Pitts, 2002; Pitts, 2003). In this paper, we investigate confluence properties of nominal rewriting, following the study of orthogonal systems in (Suzuki et al., 2015), but here we treat systems in which overlaps of the rewrite rules exist. First we present an example where choice of bound variables (atoms) of rules affects joinability of the induced critical pairs. Then we give a proof of the critical pair lemma, and illustrate some of its applications including confluence results for non-terminating systems.	artificial neuron;automated theorem proving;confluence;critical pair (logic);divergence (computer science);first-order predicate;free variables and bound variables;gabbay's separation theorem;knuth–bendix completion algorithm;linear system;newman's lemma;rewrite (programming);rewriting;walter pitts	Takaki Suzuki;Kentaro Kikuchi;Takahito Aoto;Yoshihito Toyama	2016			combinatorics;discrete mathematics;lemma (mathematics);critical pair;confluence;rewriting;mathematics	Logic	-10.348870860168239	18.298170645017052	73438
99437c9481e62158ef51eca36be991739fc6e633	on the equivalence problem for letter-to-letter top-down tree transducers	constraint based reasoning;reconocimiento lenguaje;traduction langage;relation equivalence;reconnaissance langage;automate arbre;top down;language theory;regle production;language translation;forma normal;intelligence artificielle;teoria lenguaje;language recognition;algorithme;algorithm;automate a pile;codificacion;equivalence relation;reecriture;coding;tree automata;normal form;artificial intelligence;forme normale;decidibilidad;inteligencia artificial;push down automaton;rewriting;decidabilite;relacion equivalencia;lenguaje formal;theorie langage;formal language;automata a pila;raisonnement avec contrainte;reescritura;production rule;codage;decidability;regla produccion;algoritmo;langage formel	Letter to letter top-down tree transducers are investigated in this paper. Informally, trees which appear in the rules of such transducers are reduced to one letter in the right-hand side as in the left one. With an encoding of the tree transformations induced by such transducers into recognizable forests, we recently established the decidability of equivalence for linear top-down transducers. Here, in order to capture the non-linearity of top-down transducers, we introduce new classes of tree automata with equivalence constraints between direct subterms for which equivalence is decidable. We then show that the equivalence problem for non-linear top-down transducers can be reduced to the equivalence problem of automata with equivalence constraints.	automata theory;nonlinear system;top-down and bottom-up design;transducer;tree automaton;turing completeness	Yves Andre;Francis Bossut	1998	Theor. Comput. Sci.	10.1016/S0304-3975(97)00080-7	decidability;formal language;discrete mathematics;boundary-value analysis;rewriting;computer science;equivalence partitioning;philosophy of language;top-down and bottom-up design;mathematics;coding;equivalence relation;algorithm	DB	-6.3245787977527845	19.34722954029902	73456
8686b68b19ce063044a52152653c2ec80aa020d8	program generation in the equivalent transformation computation model using the squeeze method	rewrite rule;computer model;program synthesis;program generation;background knowledge;equivalent transformation	In the equivalent transformation (ET) computation model, a specification provides background knowledge in a problem domain, a program is a set of prioritized rewriting rules, and computation consists in successive reduction of problems by rule application. As long as meaning-preserving rewriting rules, called ET rules, with respect to given background knowledge are used, correct computation results are guaranteed. In this paper, a general framework for program synthesis in the ET model is described. The framework comprises two main phases: (1) equivalent transformation of specifications, and (2) generation of a program from an obtained specification. A method for program generation in the second phase, called the squeeze method, is presented. It constructs a program by accumulation of ET rules one by one on demand, with the goal of producing a correct, efficient, and non-redundant program.	model of computation;problem domain;program synthesis;rewriting;tree accumulation	Kiyoshi Akama;Ekawit Nantajeewarawat;Hidekatsu Koike	2006		10.1007/978-3-540-70881-0_7	theoretical computer science;mathematics;algorithm	AI	-17.20827437052346	22.941285785874715	73474
ee6354b7ac506ffd5de12979c5396d23fceebe3a	using fairness constraints in process-algebraic verification	modelizacion;sistema infinito;occupation time;verificacion modelo;vivacidad;verification modele;composicionalidad;program verification;methode algebrique;vivacite;equite;modelisation;equidad;verificacion programa;equity;temps occupation;model checking;algebra proceso;compositionnalite;compositionality;algebraic method;tiempo ocupacion;algebre processus;liveness;metodo algebraico;compositional analysis;process algebra;verification programme;modeling;systeme infini;infinite system	Although liveness and fairness have been used for a long time in classical model checking, with process-algebraic methods they have seen far less use. One problem is that it is difficult to combine fairness constraints with the compositionality of process algebra. Here we show how a class of fairness constraints can be applied in a consistent way to processes in the compositional setting. We use only ordinary, but possibly infinite, LTSs as our models of processes. In many cases the infinite LTSs are part of a larger system, which can again be represented as a finite LTS. We show how this finiteness can be recovered, namely, we present an algorithm that checks whether a finite representation exists and, if it does, constructs a finite LTS that is equivalent to the infinite system. Even in the negative case, the system produced by the algorithm is a conservative estimate of the infinite system. Such a finite representation can be placed as a component in further compositional analysis just like any other LTS.	fairness measure;process calculus	Antti Puhakka	2005		10.1007/11560647_36	model checking;combinatorics;process calculus;discrete mathematics;systems modeling;computer science;mathematics;programming language;equity;algorithm;principle of compositionality;liveness	Logic	-9.539985486003829	25.28554406414086	73554
2bb7e029b50af446a494cccfb32c482e66fe2365	probabilistic symbolic execution	software analysis;exact results;symbolic execution;decision procedure;proceedings international;computer algebra;empirical evaluation	The continued development of efficient automated decision procedures has spurred the resurgence of research on symbolic execution over the past decade. Researchers have applied symbolic execution to a wide range of software analysis problems including: checking programs against contract specifications, inferring bounds on worst-case execution performance, and generating path-adequate test suites for widely used library code.   In this paper, we explore the adaptation of symbolic execution to perform a more quantitative type of reasoning --- the calculation of estimates of the probability of executing portions of a program. We present an extension of the widely used Symbolic PathFinder symbolic execution system that calculates path probabilities. We exploit state-of-the-art computational algebra techniques to count the number of solutions to path conditions, yielding exact results for path probabilities. To mitigate the cost of using these techniques, we present two optimizations, PC slicing and count memoization, that significantly reduce the cost of probabilistic symbolic execution. Finally, we present the results of an empirical evaluation applying our technique to challenging library container implementations and illustrate the benefits that adding probabilities to program analyses may offer.	best, worst and average case;computer algebra system;independence day: resurgence;memoization;symbolic execution	Jaco Geldenhuys;Matthew B. Dwyer;Willem Visser	2012		10.1145/2338965.2336773	symbolic computation;computer science;theoretical computer science;software analysis pattern;software engineering;programming language;concolic testing;symbolic trajectory evaluation;algorithm	SE	-16.66301146842964	25.955805375213	73886
bcfa1a537e5b737e121611a523f9283c82578a09	transformation rules for automated design of circuits by theorem-proving techniques. transformation from recursion equations to circuit descriptions	concepcion asistida;automated design;computer aided design;integrated circuit;circuit vlsi;transformacion;langage evolue;circuito integrado;theorem proving;algorithme;algorithm;vlsi circuit;lenguaje descripcion;conception assistee;lenguaje evolucionado;transformation;circuito vlsi;high level language;langage description;circuit integre;algoritmo;description language	Abstract#R##N##R##N#One of the important aspects in the design of automation system is to establish some automatic transformation method from the specification by high-level language, which is suitable for designer to the description at the circuit level. This paper formulates the circuit design as the transformation, from the recursion equation, which is a high-level description, to the functional network FN, which is a circuit model based on the data-flow operation. The purpose of this paper is to clarify the transformation rules for the automatic circuit design based on the theorem-proving techniques. First, the recursion equation is formulated as the specification language, and the circuit description language for FN is designed. The theorem-proving system for the automatic design of the circuit can be formulated as a logic system in which the axioms are the predicate expressions for the elements, and the inference rules are the transformation rules among elements. The transformation rules from the recursion equation to the circuit representation are constructed using the concept of schema corresponding to the type of the recursion equation. Since the transformation includes heuristic aspects, some transformation techniques from the schema to the circuit description (called schema transformation) as well as the decomposition-synthesis rules in the circuit representation, are presented. A logic system based on the obtained result is presented, indicating that a theorem-proving system for the automatic circuit design can be constructed.	recursion	Masateru Harao;Koji Iwanuma	1989	Systems and Computers in Japan	10.1002/scj.4690200110	transformation;physical design;computer science;artificial intelligence;theoretical computer science;integrated circuit;computer aided design;mathematics;automated theorem proving;programming language;high-level programming language;algorithm	EDA	-14.505230765060704	30.209339509564032	74066
2c36d01240164e53bfb58250d6c38a7d27275447	optimality properties of planning via petri net unfolding: a formal analysis	reasoning about action;optimality;petri net unfoldings;theoretical analysis;directed unfolding;planning;petri nets;formal analysis;petri net	We provide a theoretical analysis of planning via Petri net unfolding, a novel technique for synthesising parallel plans. Parallel plans are generally valued for their execution flexibility, which manifests as alternative choices for the ordering of operators and potentially faster plan executions. Being a relatively new approach, the flexibility properties of plans synthesised via unfolding, and even the concurrency semantics supported by this technique, are particularly unclear and only understood at an informal level. In this paper, we first formally characterise the concurrency semantics of planning via unfolding as a further restriction on the standard notion of independence. More importantly, we then prove that plans obtained using this approach are optimal deorderings and optimal reorderings in terms of the number of ordering constraints on operators and plan execution time, respectively. These results provide objective guarantees on the quality of plans obtained by the unfolding technique.	automated planning and scheduling;cobham's thesis;concurrency (computer science);concurrency semantics;forward error correction;langton's loops;net (polyhedron);norm (social);panorama tools;parallel computing;persistent data structure;petri net;portable document format;precondition;pro tools;run time (program lifecycle phase);scheduling (computing);temporal logic;the australian;unfolding (dsp implementation)	Sarah L. Hickmott;Sebastian Sardiña	2009			discrete mathematics;computer science;management;petri net;algorithm	AI	-14.035707353714757	23.313429817267295	74114
c87770708660a02994425ec10430d7b4e5a050c3	using graph transformations and graph abstractions for software verification	programming language;software verification;graph transformation;model checking;transition systems;dynamic memory allocation	  In this abstract we present an overview of our intended approach for the verification of software written in imperative programming  languages. This approach is based on model checking of graph transition systems (GTS), where each program state is modeled  as a graph and the exploration engine is specified by graph transformation rules. We believe that graph transformation [13]  is a very suitable technique to model the execution semantics of languages with dynamic memory allocation. Furthermore, such  representation provides a clean setting to investigate the use of graph abstractions, which can mitigate the space state explosion  problem that is inherent to model checking techniques.    		Eduardo Zambon;Arend Rensink	2010	ECEASST	10.14279/tuj.eceasst.38.560	model checking;wait-for graph;software verification;computer science;theoretical computer science;graph;c dynamic memory allocation;programming language;graph rewriting	PL	-17.937278744295416	28.012977023366897	74468
4e2648c231d69686456c95d1372c8b72e1087d59	foundations of compositional program refinement - safety properties	safety properties;compositional program refinement;assertional methods;refinement;communication;behavior;pre-congruence;safety property;simulation;compositionality~ algebraic process the- ory;implementation;concurrency;shared variables;full abstractness.;com- pleteness;transition system	K e y w o r d s : ref inement , imp lemen ta t ion , concurrency, compositionality~ algebraic process theory, t r ans i t i on sys t em, s imula t ion , asser t ional me t hods , communica t ion , sha red variables, comple teness , (pre)congruence , behavior , full abs t rac tness .	assertion (software development);automata theory;combinatory logic;concurrency (computer science);flight instruments;linear algebra;liveness;refinement (computing);refinement calculus;simulation;specification language;transition system;verification and validation	Rob Gerth	1989		10.1007/3-540-52559-9_87	real-time computing;computer science;theoretical computer science;programming language	Logic	-18.893855108869747	26.97319276683441	74513
33522023fd621316fb04ef8d84a4d94daede7175	scenario formulation in an algebraic modelling language	modelling language;linear program;stochastic modelling	Algebraic modelling languages have simplified management of many types of large linear programs but have not specifically supported stochastic modelling. This paper considers modelling language support for multistage stochastic linear recourse problems with finite distributions. We describe basic language requirements for formulation of finite event trees in algebraic modelling languages and show representative problems in AMPL using three commonly used scenario types.	modeling language	Horand I. Gassmann;A. M. Ireland	1995	Annals OR	10.1007/BF02031743	natural language processing;mathematical optimization;algebraic modeling language;computer science;linear programming;stochastic modelling;theoretical computer science;continuous modelling;mathematics;discrete modelling;algorithm;empirical modelling	Vision	-5.680282738720154	24.92326715527385	74903
5a801d0f9407dcdd8c8a34b3f0361ad4bc27a03d	formal verification of an iterative low-power x86 floating-point multiplier with redundant feedback	clock gating;hardware architecture;theorem proving;formal verification;low power;model checking;logic in computer science;mathematical software;floating point	We present the formal verification of a low-power x86 floating -point multiplier. The multiplier operates iteratively and feeds back intermediate results in r edundant representation. It supports x87 and SSE instructions in various precisions and can block the issuing of new instructions. The design has been optimized for low-power operation and has not been c onstrained by the formal verification effort. Additional improvements for the implementation we re identified through formal verification. The formal verification of the design also incorporates the i mplementation of clock-gating and control logic. The core of the verification effort was based on AC L2 theorem proving. Additionally, model checking has been used to verify some properties of the floating-point scheduler that are relevant for the correct operation of the unit.	automated theorem proving;clock gating;formal verification;iteration;iterative method;level design;low-power broadcasting;model checking;scheduling (computing);streaming simd extensions;verilog;x86;x87	Peter-Michael Seidel	2011		10.4204/EPTCS.70.6	model checking;computer architecture;parallel computing;formal methods;formal verification;software verification;computer science;floating point;theoretical computer science;hardware architecture;formal equivalence checking;high-level verification;automated theorem proving;runtime verification;programming language;clock gating;intelligent verification;algorithm;functional verification	Logic	-16.291187392439948	30.12364656094871	74913
f2c4439ac8fd1d7c737c9a8fb58c75e52c9bdf46	a methodology for the formal verification of fft algorithms in hol	algorithme rapide;teoria demonstracion;concepcion asistida;high order logic;computer aided design;diseno circuito;numero complejo;theorie preuve;formal specification;modele arithmetique;algorithmique;registro rtl;proof theory;circuit design;metodo formal;methode formelle;abstraction;punto fijo;abstraccion;transformacion fourier rapida;formal method;fixed point;specification formelle;theorem proving;fast fourier transform;logica orden superior;modelo aritmetico;especificacion formal;demonstration theoreme;theorem prover;formal verification;algorithmics;algoritmica;point fixe;fast algorithm;niveau transfert registre;conception assistee;arithmetic model;fixed point arithmetic;verification formelle;complex number;conception circuit;floating point;coma flotante;error redondear;logique ordre superieur;demostracion teorema;transformation fourier rapide;register transfer level;algoritmo rapido;fix point;nombre complexe;rounding error;fast fourier transformation;virgule flottante;erreur arrondi	This paper addresses the formal specification and verification of fast Fourier transform (FFT) algorithms at different abstraction levels based on the HOL theorem prover. We make use of existing theories in HOL on real and complex numbers, IEEE standard floating-point, and fixed-point arithmetics to model the FFT algorithms. Then, we derive, by proving theorems in HOL, expressions for the accumulation of roundoff error in floatingand fixed-point FFT designs with respect to the corresponding ideal real and complex numbers specification. The HOL formalization and proofs are found to be in good agreement with the theoretical paper-and-pencil counterparts. Finally, we use a classical hierarchical proof approach in HOL to prove that the FFT implementations at the register transfer level (RTL) implies the corresponding high level fixed-point algorithmic specification.	algorithm;automated theorem proving;best, worst and average case;bridging (networking);decimation (signal processing);digital signal processing;fast fourier transform;fixed-point arithmetic;formal methods;formal specification;formal verification;hol (proof assistant);high-level programming language;principle of abstraction;register-transfer level;round-off error;simulation;tree accumulation	Behzad Akbarpour;Sofiène Tahar	2004		10.1007/978-3-540-30494-4_4	fast fourier transform;discrete mathematics;formal methods;hol;computer science;computer aided design;mathematics;automated theorem proving;programming language;algorithmics;algorithm	Logic	-15.221234796071904	28.096151729578818	74964
1d269999125a3d64a0002ecf9e9849f61ce770b0	the rational numbers as an abstract data type	verification;modelizacion;fonction rationnelle;algebraic specification;anneau commutatif;fraction rationnelle;computability;axiomatic;abstract data types;semantics;abstract data type;semantica;semantique;universiteitsbibliotheek;division;term rewrite system;computable algebras;modelisation;rational numbers;axiomatico;campo numero;field;rewriting systems;fraccion racional;languages rational numbers;specification algebrique;calculabilite;wijsbegeerte;type abstrait;number field;meadow;corps nombre;rational number;tipo abstracto;funcion racional;rational fraction;axiomatique;anillo conmutativo;rational function;computer algebra;modeling;division by zero;total versus partial functions;commutative ring;systeme reecriture;initial algebra;calculabilidad	We give an equational specification of the field operations on the rational numbers under initial algebra semantics using just total field operations and 12 equations. A consequence of this specification is that 0−1 = 0, an interesting equation consistent with the ring axioms and many properties of division. The existence of an equational specification of the rationals without hidden functions was an open question. We also give an axiomatic examination of the divisibility operator, from which some interesting new axioms emerge along with equational specifications of algebras of rationals, including one with the modulus function. Finally, we state some open problems, including: Does there exist an equational specification of the field operations on the rationals without hidden functions that is a complete term rewriting system?	abstract data type;existential quantification;graph coloring;initial algebra;modulus of continuity;rewriting	Jan A. Bergstra;J. V. Tucker	2007	J. ACM	10.1145/1219092.1219095	combinatorics;discrete mathematics;pure mathematics;mathematics;semantics;abstract data type;algorithm;rational number;algebra	PL	-9.475076382119886	19.076463221466348	75004
6ffa1921d5b465c7bdfa655e5cb7872bdd32de0a	model checking ctl properties of pushdown systems	arbre graphe;machine turing;tree graph;structure arborescente;logic;turing machine;automate a pile;computation tree logic;model checking;estructura arborescente;tâche controle;tree structure;checking task;tarea control;push down automaton;arbol grafo;maquina turing;logique;logica;automata a pila	A pushdown system is a graph G(P ) of configurations of a pushdown automaton P . The model checking problem for a logic L is: given a pushdown automaton P and a formula α ∈ L decide if α holds in the vertex of G(P ) which is the initial configuration of P . Computation Tree Logic (CTL) and its fragment EF are considered. The model checking problems for CTL and EF are shown to be EXPTIME-complete and PSPACE-complete, respectively.	computation tree logic;exptime;entity framework;model checking;pspace-complete;pushdown automaton;stack (abstract data type)	Igor Walukiewicz	2000		10.1007/3-540-44450-5_10	model checking;deterministic pushdown automaton;discrete mathematics;computation tree logic;computer science;turing machine;theoretical computer science;mathematics;ctl*;tree structure;logic;pushdown automaton;embedded pushdown automaton;algorithm	Logic	-5.089844335041355	21.429190457157105	75026
605261a3e1cce65b680084e4c971305e5084df8a	the quest for average response time		Responsiveness -the requirement that every request to a system be eventually handled- is one of the fundamental liveness properties of a reactive system and lies at the heart of all methods for specifying and verifying liveness. Average response time is a quantitative measure for the responsiveness requirement used commonly in performance evaluation. The static computation of average response time has proved remarkably elusive even for finite-state models of reactive systems. We present, for the first time, a robust formalism that allows the specification and computation of quantitative temporal properties including average response time. The formalism is based on nested weighted automata, which can serve as monitors for measuring the response time of a reactive system. We show thatquantitative properties specified by nested weighted automatacan be computed in exponential space for nondeterministic finite-state models of reactive systems and in polynomial time for probabilistic finite-state models.The specific property of average response time can be computed in polynomial time in both cases.  This work is joint with Krishnendu Chatterjee and Jan Otop.	automata theory;computation;expspace;finite-state transducer;formal specification;formal system;jan bergstra;liveness;mccarthy formalism;performance evaluation;polynomial;response time (technology);responsiveness;semantics (computer science);time complexity;verification and validation	Thomas A. Henzinger	2017		10.1145/3127041.3131364	time complexity;real-time computing;nondeterministic algorithm;computation;computer science;exponential function;liveness;response time;probabilistic logic;reactive system	Embedded	-9.695645932835088	26.028546762102064	75574
0baa8313c5fa39d9e81a4c7fed7e9e2118f2f08e	bandera: extracting finite-state models from java source code	program specialization bandera finite state models java source code finite state verification techniques model checking hardware design defects executable behavior software system best practice exponential complexity verification algorithms integrated collection program analysis program transformation components automatic extraction program source code input language verification tools verifier outputs correctness properties java programs program verification model extraction abstract interpretation;java hardware explosions data mining computer languages mathematical model computer science logic humans manufacturing;system monitoring program verification java finite state machines;model extraction;java programming;best practice;system monitoring;software systems;program verification;program specialization;programming model;finite state machines;slicing;model checking;cost effectiveness;hardware design;source code;program analysis;abstract interpretation;java	Finite-state verification techniques, such as model checking, have shown promise as a cost-effective means for finding defects in hardware designs. To date, the application of these techniques to software has been hindered by several obstacles. Chief among these is the problem of constructing a finite-state model that approximates the executable behavior of the software system of interest. Current best-practice involves hand-construction of models which is expensive (prohibitive for all but the smallest systems), prone to errors (which can result in misleading verification results), and difficult to optimize (which is necessary to combat the exponential complexity of verification algorithms). In this paper, we describe an integrated collection of program analysis and transformation components, called Bandera, that enables the automatic extraction of safe, compact finite-state models from program source code. Bandera takes as input Java source code and generates a program model in the input language of one of several existing verification tools; Bandera also maps verifier outputs back to the original source code. We discuss the major components of Bandera and give an overview of how it can be used to model check correctness properties of Java programs.	algorithm;correctness (computer science);executable;java;map;model checking;program analysis;software system;time complexity	James C. Corbett;Matthew B. Dwyer;John Hatcliff;Shawn Laubach;Corina S. Pasareanu;Robby;Hongjun Zheng	2000		10.1145/337180.337234	program analysis;model checking;system monitoring;real-time computing;cost-effectiveness analysis;computer science;theoretical computer science;operating system;software engineering;programming paradigm;finite-state machine;programming language;java;best practice;software system;source code	SE	-16.50763960152184	29.6158269370229	75616
a669b0d20f15c77e6eaf76b556a29b8412837898	improved net reductions for ltl $$\setminus $$ \ x model checking	reduction rules;model checking;1 safe petri nets;synchronization;ltl setminus x	A set of reduction rules for LTL   $$\setminus $$       \       X model checking of 1-safe Petri nets are presented in this paper. Compared with the rules available, more original transitions and places could be removed from the synchronization of Buchi automata obtained from LTL   $$\setminus $$       \       X formulae and 1-safe Petri nets with the new proposed rules. As a result, a compact synchronization is generated. This is useful in improving efficiency of LTL   $$\setminus $$       \       X model checking of 1-safe Petri nets.	model checking	Ya Shi;Zhenhua Duan;Cong Tian;Hua Yang	2013		10.1007/978-3-319-04915-1_4	discrete mathematics;real-time computing;mathematics;algorithm	Logic	-11.751059920865595	24.809131869682183	75894
39c0ab81815958ffff2d1c475181527b268e9460	using symbolic model checking to verify the railway stations of hoorn-kersenboogerd and heerhugowaard	variable etat;outil logiciel;software tool;hardware verification;logica temporal;langage vlc;temporal logic;state variable;preuve stalmarck;railway network;reseau ferroviaire;experimental result;symbolic model;formal verification;modele symbolique;decision procedure;herramienta controlada por logicial;robustesse;state space;resultado experimental;variable estado;hardware design;verification formelle;robustness;computer hardware;resultat experimental;red ferroviaria;symbolic model checking;materiel informatique;material informatica;logique temporelle;robustez	Stalmarck's proof procedure is a method of tautology checking that has been used to verify railway interlocking software. Recently, it has been proposed [SS98] that the method has potential to increase the capacity of formal verification tools for hardware. In this paper, we examine this potential in light of an experiment in the opposite direction: the application of symbolic model checking to railway interlocking software previously verified with Stalmarck's method. We show that these railway systems share important characteristics which distinguish them from most hardware designs, and that these differences raise some doubts about the applicability of Stalmarck's method to hardware verification.	model checking	Cindy Eisner	1999		10.1007/3-540-48153-2_9	simulation;temporal logic;formal verification;computer science;state space;artificial intelligence;theoretical computer science;programming language;algorithm;robustness;state variable	Logic	-10.041567174776816	27.378537249867723	75897
190c7fb6f2ca1294e251a23c3499905c18eb6106	a fixedpoint approach to implementing (co)inductive definitions	inductive definition	1 I n t r o d u c t i o n Several theorem provers provide commands for formalizing recursive data structures, like lists and trees. Examples include Boyer and Moore's shell principle [4] and Melham's recursive type package for the HOL system [11]. Such data structures are called datatypes below, by analogy with d a t a e y p e definitions in Standard ML. A datatype is but one example of an inductive definition. This specifies the least set closed under given rules [2]. The collection of theorems in a logic is inductively defined. A structural operational semantics [9] is an inductive definition of a reduction or evaluation relation on programs. A few theorem provers provide commands for formalizing inductive definitions; these include Coq [ 15] and again the HOL system [5]. The dual notion is that of a coinductive definition. This specifies the greatest set closed under given rules. Important examples include using bisimulation relations to formalize equivalence of processes [ 13] or lazy functional programs [ 1]. Other examples include lazy lists and other infinite data structures; these are called codatatypes below. Not all inductive definitions are meaningful. Monotone inductive definitions are a large, well-behaved class. Monotonicity can be enforced by syntactic conditions such as 'strictly positive,' but this could lead to monotone definitions being rejected on the grounds of their syntactic form. More flexible is to formalize monotonicity within the logic and allow users to prove it. This paper describes a package based on a fixedpoint approach. Least fixedpoints yield inductive definitions; greatest fixedpoints yield coinductive definitions. The package has several advantages: It allows reference to any operators that have been proved monotone. Thus it accepts all provably monotone inductive definitions, including iterated definitions. It accepts a wide class of datatype definitions, though at present restricted to finite branching. It handles coinductive and codatatype definitions. Most of the discussion below applies equally to inductive and coinductive definitions, and most of the code is shared. To my knowledge, this is the only package supporting coinductive definitions. Definitions may be mutually recursive. The package is implemented in Isabelle [ 19], using ZF set theory [20, 21]. However, the fixedpoint approach is independent of Isabelle. The recursion equations are specified as introduction rules for the mutually recursive sets. The package transforms these rules into a mapping over sets, and attempts to prove that the ~ J. Grundy and S. Thompson made detailed comments; the referees were also helpful. Research funded by SERC grants GR/G53279, GR/H40570 and by the ESPRIT Project 6453 'Types'.	bisimulation;coinduction;coq (software);data structure;display resolution;hol (proof assistant);inductive reasoning;inductive type;isabelle;iteration;lazy evaluation;mutual recursion;operational semantics;recursive data type;recursive definition;recursive set;reduction (complexity);standard ml;supercomputer education research centre;turing completeness;zermelo–fraenkel set theory;monotone	Lawrence C. Paulson	1994		10.1007/3-540-58156-1_11	computer science	PL	-15.663158592811044	18.800495416346614	75935
3ab6972ef07c21c79103a3613e1949f66e3f2a74	a p systems flat form preserving step-by-step behaviour	p systems;behavioral equivalence;p system;reactive system;normal form;membrane computing;normal forms	Starting from a compositional operational semantics of tra nsition P Systems we have previously defined, we face the problem of developing an axio matization that is sound and complete with respect to some behavioural equivalence. To achieve th is goal, we propose to transform the systems into a normal form with an equivalent semantics. As a fir t step, we introduce axioms which allow the transformation of membrane structures into flat me mbranes. We leave as future work the further step that leads to the wanted normal form.	a-normal form;beta normal form;operational semantics;p system;turing completeness	Roberto Barbuti;Andrea Maggiolo-Schettini;Paolo Milazzo;Simone Tini	2008	Fundam. Inform.		combinatorics;mathematical analysis;discrete mathematics;reactive system;computer science;membrane computing;database normalization;mathematics;algorithm;p system	PL	-10.451183076746851	20.464756415782812	76090
d334329000f83d222289c02b4ad5d314241c86fb	achieving fault-tolerance and safety of discrete-event systems through learning	learning based algorithm fault tolerance safety requirement discrete event system des finite automaton model fault diagnosis;safety discrete event systems fault diagnosis fault tolerant control finite automata learning artificial intelligence;fault tolerance fault tolerant systems fault diagnosis safety algorithm design and analysis learning automata automata;learning automata;automata;fault tolerant systems;fault tolerance;safety;algorithm design and analysis;fault diagnosis	A system is said to be fault-tolerant if it remains functional even after a fault occurs. By describing faults as unpredicted events, we study the active fault-tolerance of discrete-event systems (DES) while ensuring safety requirements. Starting from a finite automaton model of the uncontrolled plant, our proposed control framework consists of nominal supervision, fault diagnosis and active post-fault control reconfiguration. First a nominal supervisor is designed with respect to the nominal mode to ensure the control specification prior to the occurrence of faults. Second, a learning-based algorithm is proposed to compute a diagnoser that can detect the occurrence of a fault. Necessary and sufficient conditions under which a post-fault safety-enforcing control reconfiguration is feasible are explored, and a second learning-based design algorithm for the post-fault supervisor is presented by using the limited lookahead policies. Effectiveness the proposed framework is examined through an example.	algorithm;automaton;control reconfiguration;fault tolerance;finite-state machine;ibm notes;object storage;parsing;requirement;uncontrolled format string	Jin Dai;Ali Karimoddini;Hai Lin	2016	2016 American Control Conference (ACC)	10.1109/ACC.2016.7526118	control engineering;reliability engineering;algorithm design;fault tolerance;real-time computing;fault coverage;computer science;engineering;artificial intelligence;stuck-at fault;control reconfiguration;fault model;automaton;software fault tolerance	Robotics	-6.002547247021624	28.555335877493658	76247
7d4885632794dd601fec2d154ef344458e71ab06	hybrid verification integrating hol theorem proving with mdg model checking	verification;modelizacion;langage description materiel informatique;decision diagrams;high order logic;logica temporal;tool support;temporal logic;higher order logic hol;modelo hibrido;hardware description languages;diagramme decision;modele hybride;modeling language;hybrid model;theorem proving;logica orden superior;modelisation;demonstration theoreme;theorem prover;formal verification;model checking;first order temporal logic;multiway decision graphs mdg;verification formelle;computer hardware;logique ordre superieur;multiway decision graphs;verificacion;demostracion teorema;modeling;materiel informatique;higher order logic;logique temporelle;hardware	In this paper, we describe a hybrid tool for hardware formal verification that links the HOL (higher-order logic) theorem prover and the MDG (multiway decision graphs) model checker. Our tool supports abstract datatypes and uninterpreted function symbols available in MDG, allowing the verification of high-level specifications. The hybrid tool, HOL–MDG, is based on an embedding in HOL of the grammar of the hardware modeling language, MDG-HDL, as well as an embedding of the first-order temporal logic Lmdg used to express properties for the MDG model checker. Verification with the hybrid tool is faster and more tractable than using either tools separately. We hence obtain the advantages of both verification paradigms. r 2006 Elsevier Ltd. All rights reserved.	automated theorem proving;binary decision diagram;cobham's thesis;divergence (computer science);embedded system;first-order predicate;formal equivalence checking;formal verification;hol (proof assistant);hol light;hardware description language;high- and low-level;model checking;modeling language;temporal logic;turing completeness;uninterpreted function	Rabeb Mizouni;Sofiène Tahar;Paul Curzon	2006	Microelectronics Journal	10.1016/j.mejo.2006.07.019	theoretical computer science;automated theorem proving;algorithm	Logic	-15.715416161781263	27.752831689589012	76460
aac0a5b51b82c5643b78ed95bc7004409a71be90	composing strand spaces	no determinismo;simultaneidad informatica;safety properties;congruencia;concurrency;non determinism;non determinisme;stand space;simultaneite informatique;event structures;congruence;security protocol;protocole securite	The strand space model for the analysis of security protocols is known to have some limitations in the patterns of nondeterminism it allows and in the ways in which strand spaces can be composed. Its successful application to a broad range of security protocols may therefore seem surprising. This paper gives a formal explanation of the wide applicability of strand spaces. We start with an extension of strand spaces which permits several operations to be defined in a compositional way, forming a process language for building up strand spaces. We then show, under reasonable conditions how to reduce the extended strand spaces to ones of the traditional kind. For security protocols we are mainly interested in their safety properties. This suggests a strand-space equivalence: two strand spaces are equivalent if and only if they have essentially the same sets of bundles. However this equivalence is not a congruence with respect to the strand-space operations. By extending the notion of bundle we show how to define the strand-space operations directly on “bundle spaces”. This leads to a characterisation of the largest congruence within the strand-space equivalence. Finally, we relate strand spaces to event structures, a well known model for concurrency.	concurrency (computer science);congruence of squares;cryptographic protocol;spaces;strand (programming language);turing completeness	Federico Crazzolara;Glynn Winskel	2002		10.1007/3-540-36206-1_10	discrete mathematics;concurrency;computer science;congruence;mathematics;algorithm	Security	-9.10689214081504	21.521990217191814	76482
7acdc3d511d96c1be5df61c9ba4fa9c808732e55	structured formal development in isabelle	monads;theorem provers;functional programming;specification language;theorem prover;transformation;algorithm design;program development	General purpose theorem provers provide advanced facilities for proving properties about specifications, and may therefore be a valuable tool in formal program development. However, these provers generally lack many of the useful structuring mechanisms found in functional programming or specification languages. This paper presents a constructive approach to adding theory morphisms and parametrisation to theorem provers, while preserving the proof support and consistency of the prover. The approach is implemented in Isabelle and illustrated by examples of an algorithm design rule and of the modular development of computational effects for imperative language features based on monads. ACM CCS	algorithm design;autonomous robot;colette rolland;command language;computation;denotational semantics;functional programming;imperative programming;isabelle;lawrence a. hyland;logical framework;monad (functional programming);monad transformer;problem domain;specification language;transformers	Maksym Bortin;Einar Broch Johnsen;Christoph Lüth	2006	Nord. J. Comput.		transformation;algorithm design;discrete mathematics;specification language;computer science;mathematics;automated theorem proving;programming language;functional programming;monad;algorithm	PL	-16.979593422961667	18.909642486173126	76592
d4072286ee1d8a40e8539f5468dee2ed10ff0a7d	what are the odds?: probabilistic programming in scala	probabilistic inference;scala;probability monad;edsl;probabilistic programming;languages	Probabilistic programming is a powerful high-level paradigm for probabilistic modeling and inference. We present Odds, a small domain-specific language (DSL) for probabilistic programming, embedded in Scala. Odds provides first-class support for random variables and probabilistic choice, while reusing Scala's abstraction and modularity facilities for composing probabilistic computations and for executing deterministic program parts. Odds accurately represents possibly dependent random variables using a probability monad that models committed choice. This monadic representation of probabilistic models can be combined with a range of inference procedures. We present engines for exact inference, rejection sampling and importance sampling with look-ahead, but other types of solvers are conceivable as well. We evaluate Odds on several non-trivial probabilistic programs from the literature and we demonstrate how the basic probabilistic primitives can be used to build higher-level abstractions, such as rule-based logic programming facilities, using advanced Scala features.	computation;digital subscriber line;domain-specific language;embedded system;high- and low-level;importance sampling;logic programming;monad (functional programming);programming paradigm;randomized algorithm;rejection sampling;sampling (signal processing);scala	Sandro Stucki;Nada Amin;Manohar Jonnalagedda;Tiark Rompf	2013		10.1145/2489837.2489848	probabilistic analysis of algorithms;probabilistic ctl;computer science;theoretical computer science;probabilistic logic;programming language;probabilistic argumentation;probabilistic logic network;algorithm	PL	-7.070803680441118	21.28103341050864	76609
0b6ee852853918d179072999ec8d718061220d52	model checking meets performance evaluation	performance measure;performance guarantee;performance evaluation;numerical technique;satisfiability;model checking;information processing;markov chain	Markov chains are one of the most popular models for the evaluation of performance and dependability of information processing systems. To obtain performance measures, typically long-run or transient state probabilities of Markov chains are determined. Sometimes the Markov chain at hand is equipped with rewards and computations involve determining long-run or instantaneous reward probabilities.This note summarises a technique to determine performance and dependability guarantees of Markov chains. Given a precise description of the desired guarantee, all states in the Markov chain are determined that surely meet the guarantee. This is done in a fully automated way. Guarantees are described using logics. The use of logics yields an expressive framework that allows to express well-known measures, but also (new) intricate and complex performance guarantees. The power of this technique is that no matter how complex the logical guarantee, it is automatically checked which states in the Markov chain satisfy it. Neither manual manipulations of Markov chains (or their high-level descriptions) are needed, nor the knowledge of any numerical technique to analyze them efficiently. This applies to any (time-homogeneous) Markov chain of any structure specified in any high-level formalism.	algorithm;computation;deadlock;dependability;high- and low-level;information processing;knuth reward check;logical framework;markov chain;markov decision process;model checking;numerical analysis;performance evaluation;semantics (computer science);semiconductor industry;stochastic process;transient state	Christel Baier;Boudewijn R. Haverkort;Holger Hermanns;Joost-Pieter Katoen	2005	SIGMETRICS Performance Evaluation Review	10.1145/1059816.1059819	markov decision process;model checking;markov chain;maximum-entropy markov model;real-time computing;partially observable markov decision process;information processing;computer science;theoretical computer science;markov algorithm;markov process;markov model;statistics;satisfiability	Logic	-10.218803197779682	26.462530728742504	76847
73e96f79b316a5a27ae4ce9a3651a6a2fe8e30b4	narrowing failure in functional logic programming	logical programming;functional programming;programmation logique;defaillance;programmation fonctionnelle;failures;functional logic programming;logic programs;programacion logica;programacion funcional;fallo	Negation as failure is an important language feature within the logic programming paradigm. The natural notion generalizing negation as failure in a functional logic setting is that of finite failure of reduction. In previous works we have shown the interest of using such programming construct when writing functional logic programs, and we have given a logical status to failure by means of proof calculi designed to deduce failures from programs. In this paper we address the problem of the operational mechanism for the execution of functional logic programs using failure. Our main contribution is the proposal of a narrowing relation able to deal with failures, which is constructive in the usual sense of the term in the context of negation, that is, narrowing is able to find substitutions for variables even in presence of failures. As main technical results, we prove correctness and completeness of the narrowing relation with respect to the proof-theoretic semantics.	approximation;beta normal form;computable function;computation;correctness (computer science);curry;functional logic programming;indexed grammar;language binding;negation as failure;nondeterministic algorithm;programming paradigm;proof calculus;prototype;theory;windows fundamentals for legacy pcs	Francisco Javier López-Fraguas;Jaime Sánchez-Hernández	2002		10.1007/3-540-45788-7_13	discrete mathematics;declarative programming;horn clause;stable model semantics;reactive programming;computer science;negation as failure;functional logic programming;mathematics;programming paradigm;inductive programming;programming language;functional programming;prolog;logic programming;multimodal logic;algorithm;autoepistemic logic	PL	-16.09157008978595	18.84733617898667	76938
36fee052507a71188585261e589bd0dc4259ded3	non-disjunctive numerical domain for array predicate abstraction	predicate abstraction;static analysis	We present a numerical abstract domain to infer invariants on (a possibly unbounded number of) consecutive array elements using array predicates. It is able to represent and compute affine equality relations over the predicate parameters and the program variables, without using disjunctions or heuristics. It is the cornerstone of a sound static analysis of oneand two-dimensional array manipulation algorithms. The implementation shows very good performance on representative benchmarks. Our approach is sufficiently robust to handle programs traversing arrays and matrices in various ways.	algorithm;array data structure;benchmark (computing);disjunctive normal form;heuristic (computer science);numerical analysis;polyhedron;predicate abstraction;static program analysis;turing completeness	Xavier Allamigeon	2008		10.1007/978-3-540-78739-6_14	computer science;theoretical computer science;predicate;programming language;static analysis;algorithm	PL	-18.354570485156014	24.970028140407326	77001
4dda87291bcbc9f217390ef94e72633469a34501	codiagnosability analysis of bounded petri nets		"""In this paper, we propose a novel approach to perform codiagnosability analysis of labeled bounded Petri nets. A set of sites observe the system evolution, each one with its own observation mask. Sites do not exchange information with each other but communicate with a coordinator. The coordinator is able to detect a fault if and only if at least one site is able to do that. In a previous work by some of us, it has been proven that a necessary and sufficient condition for codiagnosability under such a framework is the absence of sequences that are “ambiguous” with respect to all sites and whose length may grow indefinitely after the occurrence of some fault. The novelties of the approach consist in using the notion of basis markings to avoid exhaustive enumeration of the set of reachable markings, and in the construction of an automaton, called Verifier, that allows one to detect the presence of ambiguous sequences. Finally, we introduce the notion of <inline-formula><tex-math notation=""""LaTeX"""">$K$</tex-math> </inline-formula>-codiagnosability: a system is <inline-formula><tex-math notation=""""LaTeX"""">$K$</tex-math> </inline-formula>-codiagnosable if and only if faults can be detected in the above framework within at most <inline-formula><tex-math notation=""""LaTeX"""">$K$</tex-math></inline-formula> observations after their occurrence. An algorithm is provided to compute the smallest value of <inline-formula><tex-math notation=""""LaTeX"""">$K$</tex-math> </inline-formula> such that the system is <inline-formula><tex-math notation=""""LaTeX"""">$K$</tex-math></inline-formula> -codiagnosable."""	algorithm;ambiguous grammar;automaton;petri net	Ning Ran;Hongye Su;Alessandro Giua;Carla Seatzu	2018	IEEE Transactions on Automatic Control	10.1109/TAC.2017.2742659	mathematical optimization;discrete mathematics;enumeration;petri net;if and only if;mathematics;integer programming;bounded function	SE	-7.3015264223224525	27.508049774316643	77035
da2eba8da9e4d8977e8add13152c0f96ddedc2b6	on the faithful regular extensions of iterative algebras	reachability problem;vector addition system;fixed point;petri net;fixed point property;decidability	In ([6]), Tiuryn proved the existence of extensions of algebras with the unique fixed point property (iterative algebras) to ordered algebras with the least fixed point property (regular algebras), extensions preserving the fixed point solutions.  The aim of this paper is to prove that whenever the extension is “faithful”, i.e. obtained without collapsing elements of the carrier, the new regular algebra is again iterative.  In Section 1 we fix some notations and definitions and state Tiuryn's result. Section 2 contains the formulation of the problem and a sketch of the proof. Section 3 deals with the main construction.	fixed point (mathematics);fixed-point property;iterative method;kleene algebra;least fixed point	Francesco Parisi-Presicce	1981		10.1145/800076.802491	decidability;combinatorics;discrete mathematics;mathematics;fixed point;petri net;algorithm;fixed-point property;least fixed point;algebra	Logic	-9.69287697493959	18.44725000284398	77246
12e52982f89dc688ae58f9acbf862f3fd7280e5f	two approaches to bounded model checking for a soft real-time epistemic computation tree logic		We tackle two symbolic approaches to bounded model checking (BMC) for an existential fragment of the soft real-time epistemic computation tree logic (RTECTLK) interpreted over interleaved interpreted systems. We describe a BDD-based BMC method for RTECTLK, and provide its experimental evaluation and comparison with a SAT-based BMC method. Moreover, we have attempted a comparison with MCMAS on several benchmarks.		Artur Meski;Bozena Wozna;Agnieszka Zbrzezny;Andrzej Zbrzezny	2013		10.1007/978-3-319-00551-5_58	probabilistic ctl;fair computational tree logic;multimodal logic	Logic	-14.449827978436959	22.793991264562127	77866
539d0362d474e9aa0fede66397b6d87c54089ef1	on qualitative analysis of fault trees using structurally persistent nets	analytical models;petri nets fault trees linear programming;logic gates fault trees safety computational modeling petri nets algorithm design and analysis analytical models;computational modeling;logic gates;safety;linear programming techniques fault trees structurally persistent nets ft analysis minimal cut sets mcs minimal path set mps reliability engineering structural analysis petri net pn;qualitative evaluation cut sets fault trees fts linear programming lp petri nets pns;petri nets;algorithm design and analysis;fault trees	A fault tree (FT) defines an undesired top event, characterizing it using logic combinations of lower-level undesired events. In this paper, we focus on coherent FTs, i.e., the logic is restricted to AND/OR formulas. FT analysis is used to identify and assess the minimal cut sets (MCSs) of an FT, which define the minimal set of events leading to the undesired state. The dual of MCS is minimal path set (MPS). MCS and MPS are commonly used for qualitative evaluation of FTs in safety and reliability engineering. This paper explores computation of the MCS/MPS of an FT by means of structural analysis (namely, computation of minimal p-semiflows) of a Petri net (PN) that represents the FT. To this end, we propose a formal definition of a coherent FT and a transformation from this model to a PN subclass (namely, structurally persistent nets). We also prove the relationship between minimal p-semiflows and MCS/MPS in an FT. In addition, we propose an algorithm that uses linear programming techniques to compute the MCS/MPS in an FT. Finally, we put our findings into practice by qualitatively evaluating the FT of a pressure tank system.	and gate;algorithm;analysis of algorithms;coherence (physics);computation;decision problem;fault tree analysis;linear programming;logic gate;max-flow min-cut theorem;multi categories security;petri net;polynomial;reliability engineering;structural analysis;substitution-permutation network;test engineer;time complexity;xml	Ricardo J. Rodríguez	2016	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2015.2437360	algorithm design;discrete mathematics;fault tree analysis;logic gate;computer science;theoretical computer science;mathematics;computational model;petri net;algorithm	SE	-7.9553835966885265	26.67040285386113	77993
0090349749a32f5d419d111770bbaed54aeef0e7	modularizing contexted constraints	smaller group;constraint formula;redundant information;exponential amount;non-interacting group;constraint-based grammar;dependent disjunction;efficient form;contexted constraint	This paper describes a nlethod for compiling a constraint-based g rammar into a potentially inore efficient form for processing. This method takes dependent disjunctions within a constraint formula and factors them into non-interacting groups whenever possibh; by determining their independence. When a group of dependent disjunctions is split into slnaller groups, an exponential amount of redundant information is reduced. At runtime, this ineans that all exponential alnount of processing can be saved as well. Since the performance of an algorithm ibr processing constraints with dependent disjmmtions is highly deterxnined by its input, the transformatioll presented in this paper should prove beneficial for all such algorithms.	algorithm;compiler;interaction;time complexity	John Griffith	1996			algorithm	ML	-16.760743872412363	20.00111686012361	78004
149ef788ef6a7fbaad69450012b2ef2bc24564df	look for the proof to find the program: decorated-component-based program synthesis		We introduce a technique for component-based program synthesis that relies on searching for a target program and its proof of correctness simultaneously using a purely constraint-based approach, rather than exploring the space of possible programs in an enumerate-and-check loop. Our approach solves a synthesis problem by checking satisfiability of an (exists exists ) constraint (phi ), whereas traditional program synthesis approaches are based on solving an (exists forall ) constraint. This enables the use of SMT-solving technology to decide (phi ), resulting in a scalable practical approach. Moreover, our technique uniformly handles both functional and nonfunctional criteria for correctness. To illustrate these aspects, we use our technique to automatically synthesize several intricate and non-obvious cryptographic constructions.	abstract interpretation;assertion (software development);component-based software engineering;constraint programming;correctness (computer science);cryptography;encode;enumerated type;linear algebra;program synthesis;scalability;weak duality	Adrià Gascón;Ashish Tiwari;Brent Carmer;Umang Mathur	2017		10.1007/978-3-319-63390-9_5	theoretical computer science;program synthesis;correctness;cryptography;computer science;satisfiability;scalability	PL	-15.444323628293274	26.537953503806644	78167
92ea3c89c11a8158357836e4ff8b0b481bcba8c8	from logarithmic advice to single-bit advice	separations among complexity classes;machines that take advice	Building on Barak’s work (Random’02), Fortnow and Santhanam (FOCS’04) proved a time hierarchy for probabilistic machines with one bit of advice. Their argument is based on an implicit translation technique, which allow to translate separation results for short (say logarithmic) advice (as shown by Barak) into separations for a single-bit advice. In this note, we make this technique explicit, by introducing an adequate translation lemma.	advice (programming);probabilistic turing machine	Oded Goldreich;Madhu Sudan;Luca Trevisan	2004		10.1007/978-3-642-22670-0_13	machine learning;mathematics;algorithm	PL	-6.1210200933781955	20.91552383779501	78444
06102de93ed3ec24db121c690cd74900184242e6	model checking dynamic pushdown networks		A Dynamic Pushdown Network (DPN) is a set of pushdown systems (PDSs) where each process can dynamically create new instances of PDSs. DPNs are a natural model of multi-threaded programs with (possibly recursive) procedure calls and thread creation. Thus, it is important to have model-checking algorithms for DPNs. We consider in this work model-checking DPNs against single-indexed LTL and CTL properties of the form ∧ fi s.t. fi is a LTL/CTL formula over the PDS i. We consider the model-checking problems w.r.t. simple valuations (i.e, whether a configuration satisfies an atomic proposition depends only on its control location) and w.r.t. regular valuations (i.e., the set of the configurations satisfying an atomic proposition is a regular set of configurations). We show that these model-checking problems are decidable. We propose automatabased approaches for computing the set of configurations of a DPN that satisfy the corresponding single-indexed LTL/CTL formula.	algorithm;atomic sentence;linear temporal logic;model checking;recursion;single-index model;solid modeling;stack (abstract data type);thread (computing)	Fu Song;Tayssir Touili	2013		10.1007/978-3-319-03542-0_3	distributed computing;programming language;algorithm	Logic	-11.96733497804388	23.197384735541437	78458
0e38de8058362f29e70acd29fc4dd01142691add	static analysis of clp programs over numeric domains	static analysis	Constraint logic programming (CLP) is a generalization of the pure logic programming paradigm, having similar model-theoretic, fixpoint and operational semantics [9]. Since the basic operational step in program execution is a test for solvability of constraints in a given algebraic structure, CLP has in addition an algebraic semantics. CLP is then a general paradigm which may be instantiated on various semantic domains, thus achieving a good expressive power. One relevant feature is the distinction between testing for solvability and computing a solution of a given constraint formula. In the logic programming case, this corresponds to the unification process, which tests for solvability by computing a solution (a set of equations in solved form or most general unifier). In CLP, the computation of a solution of a constraint is left to a constraint solver, which does not affect the semantic definition of the language. This allows different computational domains, e.g. real arithmetic, to be considered without requiring complicated encodings of data objects as first order terms. Since the fundamental linguistic aspects of CLP can be separated from the details specific to particular constraint systems, it seems natural to parameterize the semantics of CLP languages with respect to the underlying constraint system [16]. For example, considering a domain of “abstract constraints” instead of the “concrete constraints” that are actually manipulated during program execution, we obtain for free a formal treatment of abstract interpretation of CLP programs: this provides a foundation for dataflow analysis and program manipulation of CLP programs.	abstract interpretation;admissible rule;algebraic equation;algebraic semantics (computer science);computation;constraint logic programming;data-flow analysis;dataflow;fixed point (mathematics);operational semantics;programming paradigm;solver;static program analysis;theory;unification (computer science)	Roberto Bagnara;Roberto Giacobazzi;Giorgio Levi	1992			discrete mathematics;static analysis;mathematics	PL	-16.68777961790864	20.578962989797304	78637
07c497ba855a346e72a1e8e348751e5679a2267f	transitive closure and recursive datalog implemented on clusters	cost saving;polynomial fringe property;datalog;recursion;map reduce;transitive closure;recursive algorithm	"""Implementing recursive algorithms on computing clusters presents a number of new challenges. In particular, we consider the endgame problem: later rounds of a recursion often transfer only small amounts of data, causing high overhead for interprocessor communication. One way to deal with the endgame problem is to use an algorithm that reduces the number of rounds of the recursion. Especially, in an application like transitive closure (""""TC"""") there are several recursive-doubling algorithms that use a logarithmic, rather than linear, number of rounds. Unfortunately, recursive-doubling algorithms can deduce many more facts than the linear TC algorithms, which could negate the cost savings from the elimination of the overhead due to the proliferation of small files. We are thus led to consider TC algorithms that, like the linear algorithms, have the unique decomposition property that assures paths are discovered only once. We find that many such algorithms exist, and we show that they are incomparable, in that any of them could prove best on some data --- even lower in cost than the linear algorithms in some cases. The recursive-doubling approach to TC extends to other recursions as well. However, it is not acceptable to reduce the number of rounds at the expense of a major increase in the number of facts that are deduced. In this paper, we prove it is possible to implement any Datalog program of right-linear chain rules with a logarithmic number of rounds and no order-of-magnitude increase in the number of facts deduced. On the other hand, there are linear recursions for which the two goals of reducing the number of rounds and maintaining the total number of deduced facts cannot be met simultaneously. We show that the reachability problem cannot be solved in logarithmic rounds without using a binary predicate, thus squaring the number of potential facts to be deduced. We also show that the samegeneration recursion cannot be solved in logarithmic rounds without using a predicate of arity three."""	algorithm;datalog;inter-process communication;numerical integration;overhead (computing);period-doubling bifurcation;pointer jumping;reachability problem;recursion;transitive closure	Foto N. Afrati;Jeffrey D. Ullman	2012		10.1145/2247596.2247613	recursion;computer science;theoretical computer science;datalog;programming language;transitive closure;algorithm;recursion	DB	-16.26281597595187	23.448580366513152	78719
6a2572958b05d0bf0e65c179340b20d91ac0c787	trace partitioning in abstract interpretation based static analyzers	lenguaje programacion;analyse statique;renemen t;programming language;semantics;coordination language;interpretacion abstracta;program verification;semantica;semantique;analisis estatica;refinement method;langage coordination;verificacion programa;lenguaje coordinacion;langage programmation;interpretation abstraite;methode raffinement;static analysis;abstract interpretation;verification programme;metodo afinamiento	When designing a tractable static analysis, one usually needs to approximate the trace semantics. This paper proposes a systematic way of regaining some knowledge about the traces by performing the abstraction over a partition of the set of traces instead of the set itself. This systematic refinement is not only theoretical but tractable: we give automatic procedures to build pertinent partitions of the traces and show the efficiency on an implementation integrated in the Astrée static analyzer, a tool capable of dealing with industrial-size software.	abstract interpretation;approximation algorithm;astrée (static analysis);binary space partitioning;cobham's thesis;control flow;control point (mathematics);disjunctive normal form;disk partitioning;refinement (computing);relevance;static program analysis;time complexity;tracing (software)	Laurent Mauborgne;Xavier Rival	2005		10.1007/978-3-540-31987-0_2	computer science;theoretical computer science;semantics;programming language;static analysis;algorithm	PL	-18.696008042299663	24.540232978901884	79074
581cd5789f43e5c8a80c54067a0ec3e46281121b	local action systems and dpo graph transformation	parallelisme;simultaneidad informatica;graph transformation;parallelism;transformation graphe;concurrency;paralelismo;graph rewriting;rewriting systems;simultaneite informatique;systeme reecriture	A comparison between DPO graph rewriting and Local Action Systems is presented. It is shown that, as far as the sequential behaviour is concerned, each Local Action Systems can be simulated by a set of Double Pushout productions and vice versa. The encoding used is fairly straightforward, and it is easy to give conditions under which it preserves the finiteness of the sets of productions involved. As far as the sequential behaviour is concerned, it is shown that the situation is more complicated, and that the constructions presented are not satisfactory in the sense that atomic steps which are parallel independent in one system do not give rise to parallel independent steps in the simulating system.	graph rewriting	Dirk Janssens	2002		10.1007/3-540-45711-9_10	discrete mathematics;computer science;mathematics;programming language;algorithm;graph rewriting	Robotics	-9.644557404739643	22.32052499781562	79121
321b1b6db11912147f2c8f3d768046f076b0c45d	mechanical construction of type-checking predicates for extensible data types	predicate;type constructor;type checking;extensible types	The intent of this paper is to propose a simple mechanical procedure by which type checking predicates can be constructed for extensible sets of data types. Type checking under these circumstances is nontrivial because the potentially infinite (or at least very large) number of types makes it impossible (or at least very expensive) to encode type information into a fixed length tag that could be associated with each data item. To give substance to the discussion, the proposal is developed in terms of several basic data types and constructors for recursively defining aggregate data types of arbitrary structural complexity. A predicate for an aggregate type is defined in terms of the predicates for its various constituent types. It is hoped that the procedure discussed here could be exploited in a type-checking mechanism for any environment in which new data types can be synthesized out of existing ones.	aggregate data;data item;encode;predicate (mathematical logic);recursion;structural complexity (applied mathematics);type system	Clifford R. Hollander	1974		10.1145/800182.810389	composite data type;discrete mathematics;type family;algebraic data type;database;mathematics;type constructor;generalized algebraic data type;algorithm;product type	PL	-13.962484423174121	18.652940944770677	79175
4e1a46836609356930901e69184527537f374069	layered composition for timed automata	audio video;input output;state space;timed automata;collision avoidance;composition operator;real time systems;partial order	We investigate layered composition for real-time systems modelled as (networks of) timed automata (TA). We first formulate the principles of layering and transition independence for TA, and demonstrate the validity of the communication closed layer (CCL) laws in such a setting, by means of an operator for layered composition that is intermediate between parallel and sequential composition. Next, we introduce the principles of input/output (i/o) and partial-order (po) equivalences, and show that such equivalences are preserved when the layered composition operator is replaced by sequential composition within the expressions appearing in the CCL laws. Finally, we proceed to show that such layering (together with equivalences obtained through the CCL laws) can be useful in the design and verification of dense real-time systems that consist of a network of interacting components, by bringing about a reduction of the state-space through the exploitation of transition independence. This is illustrated by considering a collision avoidance protocol developed for an audio/video system of Bang and Olufsen.	apriori algorithm;automata theory;bang file;feedback;input/output;interaction;norm (social);partial order reduction;process calculus;real-time clock;real-time computing;state space;timed automaton;verification and validation	Ernst-Rüdiger Olderog;Mani Swaminathan	2010		10.1007/978-3-642-15297-9_18	partially ordered set;input/output;real-time computing;simulation;computer science;state space;artificial intelligence;composition operator;operating system;mathematics;algorithm	Embedded	-10.004274262082264	24.376214072613088	79239
56f38c1a31c0242864429938784f2d3596c2aa9b	using tilco for specifying real-time systems	temporal logic;real time systems logic design automatic logic units history councils;model checking;specifications tilco real time systems temporal interval logic with compositional operators temporal logics time intervals temporal relationships;composition operator;real time systems	The Temporal Interval Logic with Compositional Operators (TILCO) has been especially designed for the specification and validation of real-time systems. TILCO extends the classical temporal logics based on the operators eventually, and henceforth by using time intervals in order to allow the specification of both qualitative and quantitative temporal relationships. The use of time intervals also supports an abstract and synthetic style for specifying real-time systems. The validation of TILCO specifications is supported by means of property proving; direct execution of specifications and model-checking of system histories are also possible. Therefore, the TILCO model can be considered as a dual approach. This paper describes TILCO and shows with an example its application for the specification and validation of real-time systems. Highlights about the TILCO axiomatization are given and the tools available for proving properties of specifications and for specifications execution are discussed.	real-time operating system;real-time transcription	Riccardo Mattolini;Paolo Nesi	1996		10.1109/ICECCS.1996.558324	model checking;real-time computing;temporal logic;computer science;artificial intelligence;composition operator;algorithm	Embedded	-11.55781549122662	25.581507201357127	79279
fa6fd843939a454796f31a651383752c912cd505	automata for modeling real-time systems	modeling real-time systems;real-time system	To model the behavior of finite-state asynchronous real-time systems we propose the notion of timed Büchi automata (TBA). TBAs are Büchi automata coupled with a mechanism to express constant bounds on the timing delays between system events. These automata accept languages of timed traces, traces in which each event has an associated real-valued time of occurrence.	automaton;real-time operating system;real-time transcription	Rajeev Alur;David L. Dill	1990		10.1007/BFb0032042	discrete mathematics;temporal logic;automaton;theoretical computer science;asynchronous communication;computer science;büchi automaton	Embedded	-10.919594950564328	25.585479214383998	79292
2bf4b09f6ecdafb7a381f4d659230fe90b7a2123	a serialization graph construction for nested transactions	sequences;nested transaction;systems engineering;satisfiability;graphs;fault tolerant computing;system design;algorithms;reasoning;read write memories;construction	This paper makes three contributions. First, we present a proof technique that offers system designers the same ease of reasoning about nested transaction systems as is given by the classical theory for systems without nesting, and yet can be used to verify that a system satisfies the robust “user view” definition of correctness of [10]. Second, as applications of the technique, we verify the correctness of Moss' read/write locking algorithm for nested transactions, and of an undo logging algorithm that has not previously been presented or proved for nested transaction systems. Third, we make explicit the assumptions used for this proof technique, assumptions that are usually made implicitly in the classical theory, and therefore we clarify the type of system for which the classical theory itself can reliably be used.	algorithm;concurrency (computer science);concurrency control;correctness (computer science);directed acyclic graph;lock (computer science);nested transaction;serialization;ues (cipher);undo	Alan Fekete;Nancy A. Lynch;William E. Weihl	1990		10.1145/298514.298547	construction;computer science;theoretical computer science;sequence;database;distributed computing;graph;programming language;reason;nested transaction;satisfiability;systems design	DB	-17.721371343900504	29.72770435785343	79380
897dc58edaad8169bc94968d9e0fe9c7d0356088	rewrite rules and simplification of matrix expressions	rewrite rule;grobner basis;non commutative;automated theorem proving	This paper concerns the automated simplification of expressions which involve non-commuting variables. The technology has been applied to the simplification of matrix and operator theory expressions which arise in engineering applications. The non-commutative variant of the Gröbner Basis Algorithm is used to generate rewrite rules. We will also look at the phenomenon of infinite bases and implications for automated theorem proving.	algorithm;automated theorem proving;gröbner basis;level of detail;rewrite (programming);rewriting;text simplification	John J. Wavrik	1996	The Computer Science Journal of Moldova		discrete mathematics;computer science;gröbner basis;mathematics;automated theorem proving;programming language;algorithm;algebra	Logic	-18.865515017254573	18.859301823501845	79474
ab05b5c0948b6dead0a83563c5e7d8817027c9f3	abstract interpretation of mobile ambients	object oriented language;mobile ambients;control flow analysis;ambient calculus;abstract interpretation	We show how abstract interpretation can be expressed in a constraint-based formalism that is becoming increasingly popular for the analysis of functional and object-oriented languages. This is illustrated by developing analyses for the ambient calculus.The first step of the development constructs an analysis for counting occurrences of processes inside other processes; we show that the analysis is semantically correct and that the set of acceptable solutions constitutes a Moore family. The second step considers a previously developed control flow analysis and shows how to induce it from the counting analysis; we show that its properties can be derived from those of the counting analysis using general results about abstract interpretation for constraint-based analyses.	abstract interpretation;ambient calculus	Flemming Nielson;René Rydhof Hansen;Hanne Riis Nielson	2003	Sci. Comput. Program.	10.1016/S0167-6423(02)00131-4	ambient calculus;computer science;theoretical computer science;programming language;object-oriented programming;algorithm;control flow analysis	PL	-16.3956226439572	22.68866291990517	79496
8ed7c386ff38eb06b8f160aa84e61491e1baec3b	sequence types for the pi-calculus	sequence type;encodings;union types;polymorphism;process modelling;mobile processes;intersection types	We introduce channel sequence types to study finitary polymorphism in the context of mobile processes modelled in the @p-calculus. We associate to each channel a set of exchange types, and we require that output processes send values of one of those types, and input processes accept values of any type in the set. Our type assignment system enjoys subject reduction and guarantees the absence of communication errors. We give several examples of polymorphism, and we encode the @l-calculus with the strict intersection type discipline.	π-calculus	Sergio Maffeis	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.06.012	polymorphism;combinatorics;discrete mathematics;type family;computer science;algebraic data type;process modeling;mathematics;programming language;algorithm	Logic	-8.923246709591421	20.071102094382518	79505
6790dcff5b115996d619f3dadba048c8d5e787b5	online modeling refinement for discrete event systems	finite automata discrete event systems refinement calculus computational complexity;discrete event systems context modeling system identification mathematical model learning automata formal languages councils state estimation automatic testing system testing;system modeling;discrete event system;computational complexity;refinement calculus;finite automata;discrete event systems;deds model online modeling refinement algorithm discrete event systems machine identification externally observed sample path fixed length sample path np complete accumulated observed sample path;structural similarity	"""Machine identification of discrete event systems (DES) addresses the issue of identifying an unhown system based on externally observed sample path of the unknown system. Online Modeling Refinement studies the continuing machine identification process in the context when the observed sample path is updated incrementally. While machine identification problem for fixed length sample path is NP-complete, the computational requirement for the proposed online modeling refinement algorithm is maintained at minimal by taking the structure similarity between successive accumulated observed sample paths. I n addition to the computational advantage, the proposed algorithm also guarantees the identification results of the system models """"converge"""" to the unknown DEDS model as the incrementally observed sequence get """"long"""" and """"rich"""" enough."""	algorithm;computation;converge;np-completeness;refinement (computing)	Sheng-Luen Chung;Chung-Lun Li;Jun-Chin Wu;Shih-Tung Wang	2003		10.1109/ICSMC.2003.1244299	refinement calculus;discrete mathematics;systems modeling;discrete event dynamic system;computer science;theoretical computer science;structural similarity;machine learning;discrete system;mathematics;finite-state machine;computational complexity theory	AI	-7.3496569533020635	27.121215488892222	79637
3184c842ef27672390467695a8c74575efa32045	flatterms, discrimination nets, and fast term rewriting	term rewrite system;first order;indexation;term rewriting	We describe a new representation for first-order terms which is amenable to simple and fast traversal and matching operations. In addition, we describe some efficient discrimination net indexing algorithms which use the new term representation. We have implemented these ideas in a term rewriting system called HIPER, and have obtained substantial speedups.	algorithm;first-order predicate;rewriting;tree traversal	Jim Christian	1993	Journal of Automated Reasoning	10.1007/BF00881866	discrete mathematics;computer science;theoretical computer science;first-order logic;mathematics;programming language;confluence;algorithm	PL	-15.838909127601704	22.60216838710794	79828
25975d8750a1928143ac99336596c3e9a70c1ace	regular functions, cost register automata, and generalized min-cost problems	cost function;shortest path algorithm;regular language;decision problem;formal verification;automata theory;logic in computer science;algorithm design;formal language	Motivated by the successful application of the theory of regular languages to formal verification of finite-state systems, there is a renewed interest in developing a theory of analyzable functions from strings to numerical values that can provide a foundation for analyzing quantitative properties of finitestate systems. In this paper, we propose a deterministic model for associating costs with strings that is parameterized by operations of interest (such as addition, scaling, and min), a notion of regularity that provides a yardstick to measure expressiveness, and study decision problems and theoretical properties of resulting classes of cost functions. Our definition of regularity relies on the theory of string-to-tree transducers, and allows associating costs with events that are conditional upon regular properties of future events. Our model of cost register automata allows computation of regular functions using multiple “write-only” registers whose values can be combined using the allowed set of operations. We show that classical shortest-path algorithms as well as algorithms designed for computing discounted costs, can be adopted for solving the min-cost problems for the more general classes of functions specified in our model. Cost register automata with min and increment give a deterministic model that is equivalent to weighted automata, an extensively studied nondeterministic model, and this connection results in new insights and new open problems.	algorithm;automata theory;automaton;computation;decision problem;finite-state transducer;formal verification;image scaling;maxima and minima;numerical analysis;regular language;shortest path problem;write-only documentation	Rajeev Alur;Loris D'Antoni;Jyotirmoy V. Deshmukh;Mukund Raghothaman;Yifei Yuan	2011	CoRR		algorithm design;formal language;discrete mathematics;nondeterministic finite automaton;dijkstra's algorithm;regular language;formal verification;quantum finite automata;computer science;nested word;theoretical computer science;decision problem;automata theory;ω-automaton;mathematics;programming language;algorithm	Logic	-9.07272971622414	24.73571867401157	80047
8d99b92100c7e6d86b46435a11eeaa9b4c79bfc5	extracting and implementing list homomorphisms in parallel program development	parallelisme;distributed system;algoritmo paralelo;methode diviser pour regner;hypercube;systeme reparti;formal specification;parallel algorithm;complexite calcul;programacion paralela;equational reasoning;metodo dividir para vencer;parallel programming;ingenieria logiciel;software engineering;algorithme parallele;homomorphism;specification formelle;algorithme;algorithm;especificacion formal;complejidad computacion;parallelism;sistema repartido;paralelismo;computational complexity;divide and conquer method;homomorphisme;genie logiciel;parallel implementation;homomorfismo;parallel programs;divide and conquer;program development;parallel processing;programmation parallele;algoritmo;hipercubo	Homomorphisms are functions that match the divide-and-conquer pattern and are widely used in parallel programming. Two problems are studied for homomorphisms on lists: (1) parallelism extraction: nding a homo-morphic representation of a given function; (2) parallelism implementation: deriving an eecient parallel program that computes the function. The proposed approach to parallelism extraction starts by writing two sequential programs for the function, on traditional cons lists and on dual snoc lists; the parallel program is obtained by generalizing sequential programs as terms. For almost-homomorphic functions, e.g., the maximum segment sum problem, our method provides a systematic embedding into a homomorphism. The implementation problem is addressed by introducing the class of distributable homomorphisms and deriving for them a common parallel program schema. The derivation is based on equa-tional reasoning in the Bird-Meertens formalism, which guarantees the correctness of the parallel target program. The approach is illustrated with the function scan (parallel preex), for which the combination of our two systematic methods yields the optimal hypercube algorithm, usually presented ad hoc in the literature.	algorithm;bird–meertens formalism;boolean satisfiability problem;correctness (computer science);dependence analysis;formal proof;hoc (programming language);internationalization and localization;jeremy gibbons;morphic (software);newman's lemma;parallel computing;performance prediction;program transformation;regular expression;rewriting;spmd;semantics (computer science);structured programming;thread (computing)	Sergei Gorlatch	1999	Sci. Comput. Program.	10.1016/S0167-6423(97)00014-2	homomorphism;parallel processing;divide and conquer algorithms;computer science;theoretical computer science;formal specification;parallel algorithm;programming language;computational complexity theory;algorithm;hypercube	PL	-18.653790523856703	24.70588900306225	80063
5cb7f9b566fb4f1544f5b23be80b30a826a6d464	proving termination of imperative programs using max-smt	termination analysis;invariant synthesis;conference report;program correctness	We show how Max-SMT can be exploited in constraint-based program termination proving. Thanks to expressing the generation of a ranking function as a Max-SMT optimization problem where constraints are assigned different weights, quasi-ranking functions -functions that almost satisfy all conditions for ensuring well-foundedness- are produced in a lack of ranking functions. By means of trace partitioning, this allows our method to progress in the termination analysis where other approaches would get stuck. Moreover, Max-SMT makes it easy to combine the process of building the termination argument with the usually necessary task of generating supporting invariants. The method has been implemented in a prototype that has successfully been tested on a wide set of programs.	binary space partitioning;imperative programming;invariant (computer science);mathematical optimization;optimization problem;prototype;ranking (information retrieval);termination analysis	Daniel Larraz;Albert Oliveras;Enric Rodríguez-Carbonell;Albert Rubio	2013	2013 Formal Methods in Computer-Aided Design		computer science;theoretical computer science;termination analysis;programming language;algorithm	Logic	-17.253097650877844	24.69389940143343	80149
52087cc7ffba6d251f9b3f84ebf0f6af9f3de294	an efficient deadlock prevention policy for fmss using reduction method and theory of regions	flexible manufacturing systems;synchronisation flexible manufacturing systems petri nets reachability analysis;law;firing;system recovery mathematical model law equations firing petri nets;synchronisation;system recovery;deadlock prevention;reduction method petri nets theory of regions deadlock prevention;mathematical model;theory of regions deadlock prevention policy fms reduction method petri nets synchronization crucial marking transition separation instances reachability graph analysis cmtsi;theory of regions;petri nets;reduction method;reachability analysis	Petri nets have been recognized as one of the most powerful tools for modeling FMSs. The reason is that PNs are suited well to represent FMS characteristics such as present relations, concurrence, conflict and synchronization. On the other hand, it is well known that the marking/transition-separation instances (MTSIs) method with the theory of regions has been recognized as the best (i.e. maximally permissive) policy in deadlock problems. However, its major shortcoming is the state explosion and redundant inequalities problem since the reachability graph of a plant model has to be generated when one wants to find all MTSIs. For improving these drawbacks, this paper uses the reduction method and proposes a novel concept of the crucial marking/transition-separation instances (CMTSI) which is the key of MTSIs based on Petri nets and the theory of regions. According to our experimental results, our deadlock prevention policy is more efficient in existing literatures based on the theory of regions.	deadlock;item unique identification;national supercomputer centre in sweden;petri net;reachability	Yen-Liang Pan;Yi-Sheng Huang;MuDer Jeng	2011	2011 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/ICSMC.2011.6083771	synchronization;discrete mathematics;stochastic petri net;computer science;mathematical model;distributed computing;process architecture;petri net;deadlock prevention algorithms;algorithm	Robotics	-6.886614976740669	29.35754871734475	80237
8f1ff39b25f0f201bd62a5c696d2e5150c79cc0a	formal probabilistic analysis: a higher-order logic based approach	theorem proving;statistical properties;probabilistic analysis;random variable;higher order logic;large scale problem	Traditionally, simulation is used to perform probabilistic analysis. However, it provides less accurate results and cannot handle large-scale problems due to the enormous CPU time requirements. Recently, a significant amount of formalization has been done in higherorder logic that allows us to conduct precise probabilistic analysis using theorem proving and thus overcome the limitations of the simulation. Some major contributions include the formalization of both discrete and continuous random variables and the verification of some of their corresponding probabilistic and statistical properties. This paper describes the infrastructures behind these capabilities and their utilization to conduct the probabilistic analysis of real-world systems.	automated theorem proving;central processing unit;coq (software);hol (proof assistant);higher-order function;isabelle;markov chain;mathematical optimization;model checking;open research;probabilistic analysis of algorithms;prototype verification system;randomness;requirement;simulation;statistical model;stochastic process;theory;world-system	Osman Hasan;Sofiène Tahar	2010		10.1007/978-3-642-11811-1_2	discrete mathematics;probabilistic analysis of algorithms;probabilistic ctl;probabilistic relevance model;theoretical computer science;mathematics;probabilistic logic;probabilistic argumentation;probabilistic logic network;algorithm;divergence-from-randomness model	Logic	-10.692794179225192	28.816109757539902	80310
29f0e49e58ed6ef6c8eddb505f8e2cecf5f96301	model checking for nominal calculi	developpement logiciel;modelizacion;outil logiciel;model based reasoning;analyse symbolique;movilidad;software tool;verificacion modelo;raisonnement base sur modele;web service discovery;process calculi;formal model;mobility;analisis simbolico;service web;verification modele;automaton;mobilite;web service;program verification;specification language;garbage collection;sistema reactivo;intergiciel publication souscription;modelisation;automata;semantic model;verificacion programa;model checking;intergicial editor suscriptor;desarrollo logicial;contexto;automate;tâche controle;software development;reactive system;contexte;systeme reactif;checking task;tarea control;symbolic analysis;verification programme;petri net;herramienta software;modeling;publish subscribe middleware;context;servicio web	Nominal calculi have been shown very effective to formally m odel a variety of computational phenomena. The models of nominal c alculi have often infinite states, thus making model checking a difficult task. In this note we survey some of the approaches for model checking nominal calculi. T hen, we focus on History-Dependent automata , syntax-free automaton-based model of mobility. History-Dependent automata have provided the formal basis to design and implement some existing verification toolkits. We then introduce a novel syntax-free setting to model the symbolic semantics of a nominal calculu s. Our approach relies on the notions of reactive systems and observed borrowe d contexts introduced by Leifer and Milner, and further developed by Sassone, Lack and Sobocinski. We argue that the symbolic semantics model based on borrowed contexts can be conveniently applied to web service discovery and binding.	automaton;bisimulation;computation;list of toolkits;model checking;process calculus;programming paradigm;reduction strategy (code optimization);service discovery;transition system;web service	Gian Luigi Ferrari;Ugo Montanari;Emilio Tuosto	2005		10.1007/978-3-540-31982-5_1	computer science;artificial intelligence;theoretical computer science;database;automaton;programming language;mobile computing;algorithm	Logic	-9.530336979182369	26.64889226158409	80319
c4b2ef806b186a14625b270c20a7ece57ec91928	composite model-checking: verification with type-specific symbolic representations	software systems;automated verification;binary decision diagrams;model checking;presburger arithmetic;protocol specification;hardware design;symbolic model checking;software specification;equivalence checking;requirement specification;data structure;finite state machine;symbolic representation;binary decision diagram	There has been a surge of progress in automated verification methods based on state exploration. In areas like hardware design, these technologies are rapidly augmenting key phases of testing and validation. To date, one of the most successful of these methods has been symbolic model-checking, in which large finite-state machines are encoded into compact data structures such as Binary Decision Diagrams (BDDs), and are then checked for safety and liveness properties. However, these techniques have not realized the same success on software systems. One limitation is their inability to deal with infinite-state programs, even those with a single unbounded integer. A second problem is that of finding efficient representations for various variable types. We recently proposed a model-checker for integer-based systems that uses arithmetic constraints as the underlying state representation. While this approach easily verified some subtle, infinite-state concurrency problems, it proved inefficient in its treatment of boolean and (unordered) enumerated types—which are not efficiently representable using arithmetic constraints. In this article we present a new technique that combines the strengths of both BDD and arithmetic constraint representations. Our composite model merges multiple type-specific symbolic representations in a single model-checker. A system's transitions and fixpoint computations are encoded using both BDD (for boolean and enumerated types) and arithmetic constraints (for integers) representations, where the choice depends on the variable types. Our composite model-checking strategy can be extended to other symbolic representations provided that they support operations such as intersection, union, complement, equivalence checking, and relational image computation. We also present conservative approximation techniques for composite representations to address the undecidability of model-checking on infinite-state systems. We demonstrate the effectiveness of our approach by analyzing two example software specifications which include a mixture of booleans, integers, and enumerated types. One of them is a requirements specification for the control software of a nuclear reactor's cooling system, and the other one is a protocol specification.	approximation;binary decision diagram;computation;computer cooling;concurrency (computer science);data structure;data validation;enumerated type;finite-state machine;fixed point (mathematics);formal equivalence checking;liveness;model checking;reactor (software);requirement;requirements analysis;software requirements specification;software system;turing completeness	Tevfik Bultan;Richard Gerber;Christopher League	2000	ACM Trans. Softw. Eng. Methodol.	10.1145/332740.332746	model checking;software requirements specification;data structure;computer science;theoretical computer science;presburger arithmetic;formal equivalence checking;programming language;binary decision diagram;symbolic trajectory evaluation;algorithm;software system	SE	-13.597128251033228	25.702901087216226	80415
5dbc94fdbd49d7c602201431f0b03821b173f7e1	automated reasoning in extensions of theories of constructors with recursively defined functions and homomorphisms	004;decision procedures recursive datatypes recursive functions homomorphisms verification cryptography	We study possibilities of reasoning about extensions of base theories #R##N#with functions which satisfy certain recursion and homomorphism #R##N#properties.  Our focus is on emphasizing possibilities of hierarchical #R##N#and modular reasoning in such extensions and combinations thereof. #R##N##R##N#\begin{itemize}#R##N#item[(1)] We show that the theory of absolutely free constructors is local, #R##N#and locality is preserved also in the presence of selectors. These results are #R##N#consistent with existing decision procedures for this theory (e.g. by Oppen). #R##N##R##N#item[(2)] We show that, under certain assumptions,  extensions of the theory of #R##N#absolutely free constructors with functions satisfying a certain type of recursion axioms #R##N#satisfy  locality properties, and show that for functions with values in an ordered domain  #R##N#we can combine recursive definitions with boundedness axioms  without sacrificing locality. #R##N#We also address the problem of only considering models whose data part is #R##N#the {em initial} term algebra of such theories. #R##N##R##N#item[(3)] We analyze conditions which ensure that similar results can be obtained #R##N#if we relax some assumptions about the absolute freeness of the underlying #R##N#theory of data types, and illustrate the ideas on an example from cryptography.#R##N#end{itemize}#R##N#The locality results we establish allow us to reduce the task of reasoning about the #R##N#class of recursive functions we consider to reasoning in the underlying theory of #R##N#data structures (possibly combined with the theories associated with #R##N#the co-domains of the recursive functions). #R##N##R##N#As a by-product, the methods we use provide a possibility of presenting in a different light #R##N#(and in a different form) locality phenomena studied in cryp-to-gra-phy; we believe that #R##N#they will allow to better separate rewriting from proving, and thus to give simpler proofs.	automated reasoning;recursion;recursive definition	Viorica Sofronie-Stokkermans	2009			discrete mathematics;mathematics;algorithm	AI	-14.291228324677117	18.868997496626527	80528
1f29998c7696631c548915f704c153ca3088e807	on unique decomposition of processes in the applied π-calculus	factorization;weak bisimilarity;cancellation;normal form;unique decomposition;strong bisimilarity;applied π calculus	Unique decomposition has been a subject of interest in process algebra for a long time (for example in BPP [2] or CCS [10, 12]), as it provides a normal form with useful cancellation properties. We provide two parallel decomposition results for subsets of the Applied π-Calculus: We show that a closed finite process P can be decomposed uniquely into prime factors Pi with respect to weak labeled bisimilarity, i.e. such that P ≈l P1| . . . |Pn. We also prove that closed normed processes (i.e. processes with a finite shortest trace) can be decomposed uniquely with respect to strong labeled bisimilarity.	a-normal form;bpp (complexity);bisimulation;calculus of communicating systems;process calculus;π-calculus	Jannik Dreier;Cristian Ene;Pascal Lafourcade;Yassine Lakhnech	2013		10.1007/978-3-642-37075-5_4	combinatorics;mathematical analysis;discrete mathematics;mathematics;factorization	Logic	-6.641798483860197	19.919280169413188	80534
530fe3c41d970ddc0f9923b39596e36f65ef63e8	termination proofs using gpo ordering constraints	term rewrite system;genetics;constraint solving;term rewriting;data structure	We present here an algorithm for proving termination of term rewriting systems by gpo ordering constraint solving. The algorithm gives, as automatically as possible, an appropriate instance of the gpo generic ordering proving termination of a given system. Constraint solving is done eeciently thanks to a DAG shared term data structure.	algorithm;constraint satisfaction problem;data structure;directed acyclic graph;rewriting;termination analysis	Thomas Genet;Isabelle Gnaedig	1997		10.1007/BFb0030601	discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-14.925771979061054	22.126234704709425	80837
ad18976f6bac7040d492e509e705f43a1ef64f00	a new des control synthesis approach based on structural model properties	complexity theory mathematical model control system synthesis discrete event systems computational modeling supervisory control software engineering;partial reachability analysis des control synthesis approach structural model property shop floor level discrete event systems modular modeling formalism uncontrolled plant behavior petri net components transition event interconnections transition invariant graph cyclic plant behavior admissible trajectory lab scaled manufacturing system;net condition event systems control synthesis discrete event systems invariants;control system synthesis;discrete event systems;reachability analysis control system synthesis discrete event systems petri nets;petri nets;reachability analysis	This work proposes a novel approach for controller synthesis on the shop-floor level for discrete-event systems. The approach is based on a modular modeling formalism and structural properties of models of uncontrolled plant behavior that are designed using this modeling formalism. The approach takes advantage of the transition invariants of the underlying Petri net components of the model and uses the event interconnections of transitions that are part of the transition invariants. The result is the so-called transition invariant graph that represents a new structural property of the modeled system. The synthesis procedure takes models of the uncontrolled plant behavior and formal specifications of the desired cyclic plant behavior under control and forbidden states as well. From these ingredients, the transition invariant graph is computed from which the admissible trajectories are extracted. This is performed by partial reachability analysis. An example taken from a lab-scaled manufacturing system illustrates the methodology and shows the application. The complexity of the synthesis procedure is significantly reduced. That makes it feasible to be used for even larger systems of real industrial scale.	algorithm;closed-loop transfer function;complexity;embedded system;formal methods;formal specification;petri net;reachability;reduction (complexity);semantics (computer science);uncontrolled format string	Hans-Christian Lapp;Hans-Michael Hanisch	2013	IEEE Transactions on Industrial Informatics	10.1109/TII.2012.2215615	control engineering;discrete mathematics;stochastic petri net;discrete event dynamic system;computer science;control theory;mathematics;petri net	Robotics	-7.020956902158743	28.132403559983498	80917
22e7cc984fb38d4771440080342006f8aa2aecf4	conformance testing in the presence of multiple faults	convex hulls;controlled perturbation;oating point computation;safety properties;conformance testing;randomized incremental construction;directed graph;randomized algorithm;delaunay triangulations;randomized incremental algorithm;lower bound	Conformance testing is the problem of determining if a black-box implementation I is equivalent to a specification S, where both are modeled as finite state Mealy machines. The problem involves constructing a checking sequence based on the specification, which is a sequence of inputs that detects all faulty machines. Traditionally conformance testing algorithms have assumed that the number of states in the implementation does not exceed that in the specification. This is because it is known that, in the absence of this assumption, the length of the checking sequence needs to be at least exponential in the number of extra states in the implementation [41]. However, this has limited the applicability of these techniques in practice where the implementation typically has many more states than the specification.In this paper we relax the constraints on the size of the implementation and investigate the existence of polynomial length checking sequences for implementations with extra states, under the promise that they either have multiple faults or no faults at all. We present randomized algorithms to construct checking sequences that catch faulty implementations with at most Δ extra states, having at least r faults (where Δ and r are parameters to the algorithm), and pass all correct implementations. We demonstrate the near optimality of our algorithms by presenting lower bounds for this problem. One of the main technical lemmas used in our proof is an estimate of the probability that a random walk on directed graphs will reach a large target set. We believe that this lemma will be of independent interest in the context of verifying safety properties.	black box;conformance testing;directed graph;finite-state machine;mealy machine;polynomial;randomized algorithm;time complexity;verification and validation	Viraj Kumar;Mahesh Viswanathan	2005			mathematical optimization;combinatorics;discrete mathematics;directed graph;conformance testing;mathematics;upper and lower bounds;randomized algorithm;algorithm	Theory	-12.84853001085393	28.094386949110504	80946
76fc470e7d0059208c5604a92255c136365fb122	specification and verification using dependent types	computational implementation theorem proving dependent types veritas sup specification logic numerals iterative structures modeling functional metalanguage;specification logic;formal specification;formalization;logic;specification;qa 76 software;veritas sup;ingenieria logiciel;program verification;indexing terms;type dependent;software engineering;iterative structures;specification language;theorem proving;computer programming;iterative methods;verificacion programa;numerals;formal verification;specification and verification;computational logic;especificacion;functional metalanguage;computational implementation;genie logiciel;formalizacion;specification languages logic design formal verification logic programming arithmetic computer languages writing h infinity control calculus councils;lenguaje especificacion;dependent types;formalisation;verification programme;modeling;langage specification;logique;theorem proving formal specification iterative methods modelling;logica	VERITAS/sup +/, a specification logic based on dependent types, is described. The overall aim is to demonstrate how the use of dependent types together with subtypes and datatypes allows the writing of specifications that are clear, concise, and generic. The development of theories of arithmetic, numerals, and iterative structures is described, and the proof of a theorem that greatly simplifies the formal verification of iterative arithmetic structures is outlined. The VERITAS/sup +/ logic is defined by modeling it as a partial algebra within a purely functional metalanguage. Aspects of the computational implementation of the logic and its associated toolset are briefly described. >	dependent type	Keith Hanna;Neil Daeche;Mark Longley	1990	IEEE Trans. Software Eng.	10.1109/32.58783	specification language;formal verification;computer science;theoretical computer science;computational logic;formal specification;computer programming;programming language;logic;specification;algorithm	SE	-16.86035309531659	19.34978452201149	81024
958c5388f6262c00137d66260c42749fae78542e	implementing a stochastic process algebra within the möbius modeling framework	seguridad funcionamiento;estructura mobius;mobius structure;evaluation performance;surete fonctionnement;stochastic process;performance evaluation;evaluacion prestacion;algebra proceso;stochastic process algebra;dependability;processus stochastique;algebre processus;structure mobius;resource availability;proceso estocastico;process algebra	Many formalisms and solution methods exist for performance and dependability modeling. However, different formalisms have different advantages and strengths, and no one formalism is universally used. The Möbius tool was built to provide multi-formalism multi-solution modeling, and allows the modeler to develop models in any supported formalism. A formalism can be implemented in Möbius if a mapping can be provided to the Möbius Abstract Functional Interface, which includes a notion of state and a notion of how state changes over time. We describe a way to map PEPA, a stochastic process algebra, to the abstract functional interface. This gives Möbius users the opportunity to make use of stochastic process algebra models in their performance and dependability models.	dependability;pepa;process calculus;semantics (computer science);stochastic process	Graham Clark;William H. Sanders	2001		10.1007/3-540-44804-7_13	stochastic process;combinatorics;process calculus;discrete mathematics;computer science;dependability;mathematics;programming language;statistics	SE	-9.508707529507882	27.92619185238376	81268
5b9356e32c9b1fef590b2ff255e7e73b05a5cc03	matrix interpretations for proving termination of term rewriting	matrix interpretations;satisfiability;boolean satisfiability;article letter to editor;termination;sat solver;term rewriting	We present a new method for automatically proving termination of term rewriting. It is based on the well-known idea of interpretation of terms where every rewrite step causes a decrease, but instead of the usual natural numbers we use vectors of natural numbers, ordered by a particular nontotal well-founded ordering. Function symbols are interpreted by linear mappings represented by matrices. This method allows us to prove termination and relative termination. A modification of the latter, in which strict steps are only allowed at the top, turns out to be helpful in combination with the dependency pair transformation. By bounding the dimension and the matrix coefficients, the search problem becomes finite. Our implementation transforms it to a Boolean satisfiability problem (SAT), to be solved by a state-of-the-art SAT solver.	benchmark (computing);boolean satisfiability problem;coefficient;linear logic;polynomial;rewrite (programming);rewriting;search problem;semi-thue system;solver;termination analysis;the matrix;whole earth 'lectronic link	Jörg Endrullis;Johannes Waldmann;Hans Zantema	2007	Journal of Automated Reasoning	10.1007/s10817-007-9087-9	discrete mathematics;computer science;mathematics;boolean satisfiability problem;algorithm	Logic	-14.16349969469962	21.19615393671202	81279
0fb607f8d02e6b0fe45582375218996fc615912d	eliminating false data dependences using the omega test	program transformation;decision procedure;data dependence;technical report;integer program	Array data dependence analysis methods currently in use generate false dependences that can prevent useful program transformations. These false dependences arise because the questions asked are conservative approximations to the questions we really should be asking. Unfortunately, the questions we really should be asking go beyond integer programming and require decision procedures for a sublcass of Presburger formulas. In this paper, we describe how to extend the Omega test so that it can answer these queries and allow us to eliminate these false data dependences. We have implemented the techniques described here and believe they are suitable for use in production compilers.	approximation;compiler;data dependency;dependence analysis;integer programming;omega;presburger arithmetic;program transformation	William Pugh;David G. Wonnacott	1992		10.1145/143095.143129	computer science;technical report;theoretical computer science;algorithm	PL	-17.29536685571888	31.393601816519606	81306
d7ec6584d20bf9d6c925cc2dd7f3bd9661167771	clarifying the priority specification of gspn: detached priorities	gspn;turing machines;stochastic processes switches petri nets inhibitors turing machines delay effects;stochastic process;detached priorities gspn gspn priority specification global multilevel priority definition stochastic process syntactical subclass;detached priorities gspn;concurrency theory petri nets;delay effects;syntactical subclass;stochastic processes;global multilevel priority definition;petri nets;switches;inhibitors;priority specification;concurrency theory	The global multilevel priority definition in GSPN, while being most convenient with many respects, poses some modelling problems related with confusion and indirect conflicts, which may lead to undesired effects in the definition of the underlying stochastic process. While some of these problems have been recognized and dealt with in previous works on the definition of GSPN, through the introduction of extended conflict sets, some others, that will be illustrated in this paper, were not covered. To overcome all these problems, in this paper a syntactical subclass of GSPN, called detached priorities GSPN (dpGSPN), is proposed. In dpGSPN no confusion or indirect conflicts are possible, and according to our experience, modelling power is not substantially sacrificed. To facilitate the modelling, a possible method to derive a suitable global priority definition from local information is outlined.		Enrique Teruel;Giuliana Franceschinis;Massimiliano De Pierro	1999		10.1109/PNPM.1999.796558	real-time computing;computer science;distributed computing;algorithm	Logic	-10.23490982189694	21.97205496185135	81663
c45cafcf9acf388c1b1fffb6032a640b058d601b	program transformation templates for tupling based on term rewriting	lambda calculus	Chiba et al. (2006) proposed a framework of program transformation of term rewriting systems by developed templates. Contrast to the previous framework of program transformation by templates based on lambda calculus, this framework provides a method to verify the correctness of transformation automatically. Tupling (Bird, 1980) is a well-known technique to eliminate redundant recursive calls for improving efficiency of programs. In Chiba et al.’s framework, however, one can not use tuple symbols to construct developed templates. Thus their framework is not capable of tupling transformations. In this paper, we propose a more flexible notion of templates so that a wider variety of transformations, including tupling transformations, can be handled. key words: program transformation, tupling, term rewriting	correctness (computer science);lambda calculus;program transformation;recursion (computer science);rewriting	Yuki Chiba;Takahito Aoto;Yoshihito Toyama	2010	IEICE Transactions		computer science;theoretical computer science;lambda calculus;mathematics;programming language;algorithm	PL	-17.142206362709025	22.040328799871723	81908
1adaa89ca2955e6184f81aa4031304dbe88a60da	logic programming applied to hardware design specification and verification	prolog system;horn clause logic;novel feature;logic programming specification;logic programming technique;hardware design specification;hardware design;logic programming;executable specification;predicate logic programming language;hardware description language;hardware system;vlsi;incremental development;microarchitecture;pattern matching;rom;microcode;control store	This paper proposes the use of logic programming techniques in the specification and verification of hardware designs. Logic programming specifications are formal and directly executable. The advantages of executable specifications are: (1) the specification is itself a prototype of the specified system, (2) incremental development of specifications is possible, (3) behavior exhibited by the specification when executed can be used to check conformity of the specification with requirements. We discuss how Horn clause logic, which has a procedural interpretation, and predicate logic programming language, Prolog, can be used as a hardware description language to specify and verify the correctness of hardware systems. The Prolog system possesses a backtracking mechanism and a powerful pattern-matching feature which is based on unification. A novel feature of the proposed approach is that it can be used to answer interesting questions about a hardware design without resorting to simulation.	backtracking;conformity;correctness (computer science);executable;hardware description language;horn clause;iterative and incremental development;logic programming;pattern matching;programming language;prolog;prototype;requirement;simulation;unification (computer science)	Deepinder P. Sidhu	1984		10.1145/800016.808239	computer architecture;verification;formal methods;logic family;formal verification;reactive programming;theoretical computer science;functional logic programming;high-level verification;hardware description language;inductive programming;programming language;prolog;programming language specification;intelligent verification;register-transfer level;functional verification;specification pattern	PL	-15.6339650032772	29.07575797294906	81914
a085029c1252b083f8c1833bad3b56052ec65d4f	inner loops in flowgraphs and code optimization	code optimization	A criterion is developed to define a hierarchy of inner loops in a program which constitute sections of the program which take up large proportions of the execution time; this hierarchy lends a dynamic loop structure to the program. It is assumed that the program has been given a flowgraph representation in which each vertex corresponds to a statement or a set of statements and the flow in each edge corresponds to the frequency of passage of control from one statement or set of statements to another. While developing this criterion an attempt is made to guarantee that moving a loop invariant statement from an inner loop to a point outside of the loop would always yield a more optimal code.	inner loop;loop invariant;mathematical optimization;program optimization;run time (program lifecycle phase);signal-flow graph	Sridhar Vasudevan	1982	Acta Informatica	10.1007/BF00288967	loop fusion;while loop;real-time computing;loop-invariant code motion;loop fission;computer science;program optimization;do while loop;loop invariant;programming language;algorithm;inner loop	PL	-15.892771777449786	32.1519641134338	81979
07bed86cda15d61999e44d6d4167a90d48fa51a5	a classification of symbolic transition systems	transitions;symbolic transition system;temporal logic;temporal logics;classification;boolean algebra;boolean operation;infinite state model checking;model checking;linear temporal logic;symbolic algorithms;state space;hybrid system;logic in computer science;hybrid automata;state equivalences;hybrid systems	We define five increasingly comprehensive classes of infinite-state systems, called STS1--STS5, whose state spaces have finitary structure. For four of these classes, we provide examples from hybrid systems.STS1 These are the systems with finite bisimilarity quotients. They can be analyzed symbolically by iteratively applying predecessor and Boolean operations on state sets, starting from a finite number of observable state sets. Any such iteration is guaranteed to terminate in that only a finite number of state sets can be generated. This enables model checking of the μ-calculus.STS2 These are the systems with finite similarity quotients. They can be analyzed symbolically by iterating the predecessor and positive Boolean operations. This enables model checking of the existential and universal fragments of the μ-calculus.STS3 These are the systems with finite trace-equivalence quotients. They can be analyzed symbolically by iterating the predecessor operation and a restricted form of positive Boolean operations (intersection is restricted to intersection with observables). This enables model checking of all ω-regular properties, including linear temporal logic.STS4 These are the systems with finite distance-equivalence quotients (two states are equivalent if for every distance d, the same observables can be reached in d transitions). The systems in this class can be analyzed symbolically by iterating the predecessor operation and terminating when no new state sets are generated. This enables model checking of the existential conjunction-free and universal disjunction-free fragments of the μ-calculus.STS5 These are the systems with finite bounded-reachability quotients (two states are equivalent if for every distance d, the same observables can be reached in d or fewer transitions). The systems in this class can be analyzed symbolically by iterating the predecessor operation and terminating when no new states are encountered (this is a weaker termination condition than above). This enables model checking of reachability properties.	bisimulation;boolean algebra;boolean operations on polygons;hybrid system;iteration;linear temporal logic;modal μ-calculus;model checking;newman's lemma;observable;reachability;terminate (software);turing completeness	Thomas A. Henzinger;Rupak Majumdar;Jean-François Raskin	2000	ACM Trans. Comput. Log.	10.1145/1042038.1042039	combinatorics;discrete mathematics;computer science;artificial intelligence;mathematics;algorithm;hybrid system	Logic	-9.883924649089375	23.746170630864484	82138
51cf2352e554e012a14aa3575df33a21aeb055ed	counterexample generation for markov chains using smt-based bounded model checking	bounded model checking;markov chain	Generation of counterexamples is a highly important task in the model checking process. In contrast to, e. g., digital circuits where counterexamples typically consist of a single path leading to a critical state of the system, in the probabilistic setting counterexamples may consist of a large number of paths. In order to be able to handle large systems and to use the capabilities of modern SAT-solvers, bounded model checking (BMC) for discrete-time Markov chains was established. In this paper we introduce the usage of SMT-solving over linear real arithmetic for the BMC procedure. SMT-solving, extending SAT with theories in this context on the one hand leads to a convenient way to express conditions on the probability of certain paths and on the other hand allows to handle Markov reward models. We use the former to find paths with high probability first. This leads to more compact counterexamples. We report on some experiments, which show promising	bisimulation;boolean satisfiability problem;cycle detection;digital electronics;dijkstra's algorithm;experiment;intelligent platform management interface;markov chain;mathematical optimization;model checking;preprocessor;reachability;shortest path problem;time complexity;with high probability	Bettina Braitling;Ralf Wimmer;Bernd Becker;Nils Jansen;Erika Ábrahám	2011		10.1007/978-3-642-21461-5_5	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-12.80075293911495	26.16820867362	82206
74c24827af0245b62d17196682a0ab4506f0df4a	an application of temporal projection to interleaving concurrency	temporal projection;interval temporal logic;interleaving concurrency;time granularities	We revisit the earliest temporal projection operator $$\mathrm \Pi $$ in discrete-time Propositional Interval Temporal Logic PITL and use it to formalise interleaving concurrency. The logical properties of $$\mathrm{\Pi }$$ as a normal modality and a way to eliminate it in both PITL and conventional point-based Linear-Time Temporal Logic LTL, which can be viewed as a PITL subset, are examined. We also formalise concurrency without $$\mathrm{\Pi }$$, and relate the two approaches. Furthermore, $$\mathrm{\Pi }$$ and another standard PITL projection operator are interdefinable and both suitable for reasoning about different time granularities. We mention other mostly interval-based temporal logics with similar forms of projection, as well as some related applications and international standards.	concurrency control;forward error correction	Ben C. Moszkowski;Dimitar P. Guelev	2015		10.1007/978-3-319-25942-0_10	discrete mathematics;real-time computing;theoretical computer science;mathematics	DB	-12.377908884016483	21.334719986870944	82282
f119eb1c1940393d190d835b2d0adbe1bb9753a0	a conformant planner with explicit disjunctive representation of belief states	disjunctive normal form;incomplete information;conformant planning	This paper describes a novel and competitive complete conformant planner. Key to the enhanced performance is an efficient encoding of belief states as disjunctive normal form formulae and an efficient procedure for computing the successor belief state. We provide experimental comparative evaluation on a large pool of benchmarks. The novel design provides great efficiency and enhanced scalability, along with the intuitive structure of disjunctive normal form representations.	adder (electronics);benchmark (computing);best-first search;color gradient;computation;computational complexity theory;disjunctive normal form;experiment;scalability	Son Thanh To;Enrico Pontelli;Tran Cao Son	2009			mathematical optimization;discrete mathematics;computer science;mathematics;disjunctive normal form;complete information;algorithm	AI	-14.747874230271037	23.504101492121944	82303
bec2f6583a8d9f63a706baefdca71de18ff4368f	constraints in term algebras: an overview of constraint solving techniques	constraint solving	We will give a very brief overview on three methods for solving constraints over term algebras, namely formula rewriting, automata techniques and combination techniques. For results which illustrate the specific methods, we give literature pointers (which may be indirect ones, i.e., to more extensive surveys).		Hubert Comon-Lundh	1994		10.1007/3-540-59155-9_4	constraint logic programming;constraint programming;binary constraint;constraint satisfaction;constraint learning;computer science;constraint graph;constraint satisfaction dual problem;constraint;local consistency	Logic	-18.60593498044625	19.655904455717604	82743
595dcc0a1e275e5a79fb1b84beb0397018dbd1f8	completeness for the modal μ-calculus: separating the combinatorics from the dynamics		The modal mu-calculus is a very expressive formalism extending basic modal logic with least and greatest fixpoint operators. In the seminal paper introducing the formalism in the shape known today, Dexter Kozen also proposed an elegant axiom system, and he proved a partial completeness result with respect to the Kripke-style semantics of the logic. The problem of proving Kozen’s axiom system complete for the full language remained open for about a decade, until it was finally resolved by Igor Walukiewicz. Walukiewicz’ proof is notoriously difficult however, and the result has remained somewhat isolated from the standard theory of completeness for modal (fixpoint) logics. Our aim in this paper is to develop a framework that will let us clarify and simplify parts of Walukiewicz’s proof. We hope that this will also help to facilitate future research into completeness of modal fixpoint logics, including fragments, variants and extensions of the modal mu-calculus. Our main contribution is to take the automata-theoretic viewpoint, already implicit in Walukiewicz’s proof, much more seriously by bringing automata explicitly into the proof theory. Thus we further develop the theory of modal parity automata as a mathematical framework for proving results about the modal mu-calculus. Once the connection between automata and derivations is in place, large parts of the completeness proof can be reformulated as purely automata-theoretic theorems. From a conceptual viewpoint, our automata-theoretic approach lets us distinguish two key aspects of the mu-calculus: the one-step dynamics encoded by the modal operators, and the combinatorics involved in dealing with nested fixpoints. This “deconstruction” allows us to work with these two features in a largely independent manner. More in detail, prominent roles in our proof are played by two classes of modal automata: next to the disjunctive automata that are known from the work of Janin & Walukiewicz, we introduce here the class of semi-disjunctive automata that roughly correspond to the fragment of the mu-calculus for which Kozen proved completeness. We will establish a connection between the proof theory of Kozen’s system, and two kinds of games involving modal automata: a satisfiability game involving a single modal automaton, and a consequence game relating two such automata. In the key observations on these games we bring the dynamics and combinatorics of parity automata together again, by proving some results that witness the nice behaviour of disjunctive and semi-disjunctive automata in these games. As our main result we prove that every formula of the modal mu-calculus provably implies the translation of a disjunctive automaton; from this the completeness of Kozen’s axiomatization is immediate. Mathematics Subject Classification (MSC2010): 03B45; 03B70; 68Q60; 91A43.	automaton;axiomatic system;dexter (malware);dexter kozen;disjunctive normal form;fixed point (mathematics);formal system;generative grammar;igor muttik;least fixed point;mathematics subject classification;modal μ-calculus;semantics (computer science);semiconductor industry;ω-automaton	Sebastian Enqvist;Fatemeh Seifan;Yde Venema	2018	Theor. Comput. Sci.	10.1016/j.tcs.2018.03.001	witness;proof theory;combinatorics;discrete mathematics;modal operator;axiom;satisfiability;modal μ-calculus;parity (mathematics);modal logic;mathematics	Logic	-12.347448735777306	19.512797495065342	82923
01e16407e260d34f184c83dad1de95353e9dd9d4	on the effect of counters in guard conditions when state-based multi-objective testing	multiobjective genetic algorithm guard condition multiobjective software testing test case generation extended finite state machine efsm finite state graph;software testing;extended finite state machines;multi objective optimization;counter problem;software testing counter problem extended finite state machines genetic algorithms multi objective optimization;program testing finite state machines genetic algorithms graph theory;genetic algorithms;radiation detectors testing biological cells genetic algorithms search problems automata optimization	During test case generation from an extended finite state machine (EFSM), the counter problem is caused by the presence of guard conditions that refer to counter variables. Because such variables are initialized and updated by transitions in the EFSM, every traversal of the state machine graph is not necessarily feasible, i.e., executable. The problem manifests itself by the fact that a transition, a sequence of transitions, or a more complex behavior in the state machine, has to be repeatedly triggered to eventually trigger a specific behavior (another transition). In this paper we define different manifestations of the counter problem and experiment with a new search based solution for that problem. We also investigate how the counter problem affects a multi-objective genetic algorithm that generates test suites from an EFSM. We evaluate our solution and compare it with an existing one, using three different case studies.	executable;extended finite-state machine;genetic algorithm;guard (computer science);test case;tree traversal	Nesa Asoudeh;Yvan Labiche	2015	2015 IEEE International Conference on Software Quality, Reliability and Security - Companion	10.1109/QRS-C.2015.26	extended finite-state machine;computer science;theoretical computer science;machine learning;algorithm	SE	-10.05318788166429	30.753139862229634	82981
bceab7e5163908d5e8016b2a77f8b612c45b7213	probabilistic symbolic model checking with prism: a hybrid approach	model specification;analyse symbolique;diagrama binaria decision;logica temporal;chaine markov;diagramme binaire decision;cadena markov;analisis simbolico;temporal logic;prism;mode hybride;verification modele;decision markov;discrete time;program verification;continuous time markov chain;probabilistic model;hybrid approach;verificacion programa;discrete time markov chain;binary decision diagrams;model checking;specification modele;especificacion modelo;probabilistic model checking;calcul numerique;numerical computation;estructura datos;calculo numerico;modele probabiliste;markov decision;structure donnee;symbolic analysis;markov decision process;tiempo discreto;temps discret;symbolic model checking;modo hibrido;verification programme;data structure;sparse matrices;logique temporelle;modelo probabilista;markov chain;binary decision diagram;hybrid mode	In this paper we present efficient symbolic techniques for probabilistic model checking. These have been implemented in PRISM, a tool for the analysis of probabilistic models such as discrete-time Markov chains, continuous-time Markov chains and Markov decision processes using specifications in the probabilistic temporal logics PCTL and CSL. Motivated by the success of model checkers such as SMV which use BDDs (binary decision diagrams), we have developed an implementation of PCTL and CSL model checking based on MTBDDs (multi-terminal BDDs) and BDDs. Existing work in this direction has been hindered by the generally poor performance of MTBDD-based numerical computation, which is often substantially slower than explicit methods using sparse matrices. The focus of this paper is a novel hybrid technique which combines aspects of symbolic and explicit approaches to overcome these performance problems. For typical examples, we achieve a dramatic improvement over the purely symbolic approach. In addition, thanks to the compact model representation using MTBDDs, we can verify systems an order of magnitude larger than with sparse matrices, while almost matching or even beating them for speed.	binary decision diagram;citation style language;computation;markov chain;markov decision process;model checking;numerical analysis;out-of-core algorithm;prism (surveillance program);parallel computing;probabilistic ctl;sparse matrix;statistical model	Marta Z. Kwiatkowska;Gethin Norman;David Parker	2004	International Journal on Software Tools for Technology Transfer	10.1007/s10009-004-0140-2	markov decision process;model checking;statistical model;markov chain;discrete time and continuous time;data structure;sparse matrix;temporal logic;computer science;continuous-time markov chain;artificial intelligence;machine learning;prism;symbolic data analysis;binary decision diagram;specification;symbolic trajectory evaluation;algorithm	Logic	-10.153768105315203	29.11457136832366	83093
8e39c34ad11cf669e92a0c503a9907cd785c848d	the descriptive complexity of parity games	second order;fixed point;polynomial time	We study the logical definablity of the winning regions of parity games. For games with a bounded number of priorities, it is wellknown that the winning regions are definable in the modal μ-calculus. Here we investigate the case of an unbounded number of priorities, both for finite game graphs and for arbitrary ones. In the general case, winning regions are definable in guarded second-order logic (GSO), but not in least-fixed point logic (LFP). On finite game graphs, winning regions are LFP-definable if, and only if, they are computable in polynomial time, and this result extends to any class of finite games that is closed under taking bisimulation quotients.	bisimulation;computable function;computational complexity theory;descriptive complexity theory;fixed point (mathematics);international conference on functional programming;least fixed point;modal μ-calculus;polynomial;time complexity	Anuj Dawar;Erich Grädel	2008		10.1007/978-3-540-87531-4_26	combinatorial game theory;time complexity;combinatorics;discrete mathematics;computer science;mathematics;fixed point;second-order logic;algorithm	Logic	-5.54006838772396	19.21790979336949	83223
cf28c4f025c17e1e221897f8fafdb87eb9f612d6	infinite-state high-level mscs: model-checking and realizability	message sequence chart;model checking;asynchronous communication;communication protocol	We consider three natural classes of infinite-state HMSCs: globally-cooperative, locally-cooperative and local-choice HMSCs. We show first that model-checking for globally-cooperative and locally-cooperative HMSCs has the same complexity as for the class of finite-state (bounded) HMSCs. Surprisingly, model-checking local-choice HMSCs turns out to be exponentially more efficient in space than for locally-cooperative HMSCs. We also show that locally-cooperative and local-choice HMSCs can be always implemented by communicating finite states machines, provided we allow some additional (bounded) message data. Moreover, the implementation of local-choice HMSCs is deadlock-free and of linear-size.	microsoft cluster server;model checking	Blaise Genest;Anca Muscholl;Helmut Seidl;Marc Zeitoun	2002		10.1007/3-540-45465-9_56	model checking;communications protocol;discrete mathematics;computer science;theoretical computer science;asynchronous communication;distributed computing;message sequence chart;algorithm	Logic	-10.484382442157283	25.353805505470508	83627
5d68675e09efc4453bb6e92d7951918b250f09d8	test generation for specifications modeled by input/output automata	test generation;output automata;input output	In this paper, we consider conformance testing of communication systems modeled by I/O automata. A framework is proposed for testing I/O automata with full fault coverage for implementations with at most m states. The notion of state identification, which was originally is defined in the realm of I/O finite state machines, is applied. Based on this notion, a test derivation algorithm is given for test suites which guarantee fault coverage. This algorithm is an analogue of the so-called FSM-based HSI-method.	automaton;input/output	Q. M. Tan;Alexandre Petrenko	1998			discrete mathematics;real-time computing;computer science;algorithm	Logic	-10.961339830013317	26.463149256518328	83792
0fcc26b1106ee42e82ebade61518fa900f4717a5	2ls: memory safety and non-termination - (competition contribution)		2LS is a C program analyser built upon the CPROVER infrastructure. 2LS is bit-precise and it can verify and refute program assertions and termination. 2LS implements template-based synthesis techniques, e.g. to find invariants and ranking functions, and incremental loop unwinding techniques to find counterexamples and k-induction proofs. New features in this year’s version are improved handling of heap-allocated data structures using a template domain for shape analysis and two approaches to prove program non-termination.	memory safety	Viktor Malík;Stefan Marticek;Peter Schrammel;Mandayam K. Srivas;Tomás Vojnar;Johanan Wahlang	2018		10.1007/978-3-319-89963-3_24	discrete mathematics;theoretical computer science;counterexample;analyser;memory safety;mathematical proof;shape analysis (digital geometry);invariant (mathematics);ranking;data structure;computer science	ML	-16.400819910141898	24.755833786453223	84084
b0cd4af1806c7c2a7f05311cf6c0d262feba94dd	bisimulations for delimited-control operators		We propose a survey of the behavioral theory of an untyped lambda-calculus extended with the delimited-control operators shift and reset. We define a contextual equivalence for this calculus, that we then aim to characterize with coinductively defined relations, called bisimilarities. We study different styles of bisimilarities (namely applicative, normal-form, and environmental), and we give several examples to illustrate their respective strengths and weaknesses. We also discuss how to extend this work to other delimited-control operators.	applicative programming language;bisimulation;delimiter;lambda calculus;observational equivalence;turing completeness	Dariusz Biernacki;Sergueï Lenglet;Piotr Polesiuk	2013	CoRR		discrete mathematics;pure mathematics;mathematics	AI	-12.120129750726607	18.353687745826797	84305
56e4e5ce17a1abb14ea6205d6169194481525b3e	an intrinsic characterization of approximate probabilistic bisimilarity	markov process;probability measure	In previous work we have investigated a notion of approximate bisimilarity for labelled Markov processes. We argued that such a notion is more realistic and more feasible to compute than (exact) bisimilarity. The main technical tool used in the underlying theory was the Hutchinson metric on probability measures. This paper gives a more fundamental characterization of approximate bisimilarity in terms of the notion of (exact) similarity. In particular, we show that the topology of approximate bisimilarity is the Lawson topology with respect to the simulation preorder. To complement this abstract characterization we give a statistical account of similarity, and by extension, of approximate bisimilarity, in terms of the process testing formalism of Larsen and Skou.	approximation algorithm;audio feedback;bisimulation;covert channel;domain theory;formal system;interference (communication);internet leak;lawson topology;markov chain;markov property;non-interference (security);nondeterministic algorithm;simulation preorder	Franck van Breugel;Michael W. Mislove;Joël Ouaknine;James Worrell	2003		10.1007/3-540-36576-1_13	combinatorics;discrete mathematics;probability measure;computer science;mathematics;markov process;algorithm	Logic	-9.713020324393572	21.992049515479597	84354
7ac250459a288ab7bd806a4c91ceff13a286d408	a generalization of acp using belnap's logic	parallel composition;process algebra	Abstract   An overview is given of ACP with conditional composition (i.e.,  if-then-else ) over Belnap's four-valued logic. The interesting thing is that much of ACP can be analyzed using this logic. For example, both the choice operation + and  δ  (deadlock) can be seen as instances of conditional composition, and the axiom   x  +  δ  =  x   follows from this perspective. Furthermore, parallel composition can be generalized to conditional parallel composition, which has sequential composition as an instance, next to common parallel composition, pure interleaving and synchronous ACP.  This article is an extended abstract of [A. Ponse, M.B. van der Zwaag. A generalization of ACP using Belnap's logic. Submitted to Journal of Logic and Algebraic Programming]. The full article contains all proofs and some examples on parallel scheduling in GACP.		Alban Ponse;Mark van der Zwaag	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.12.102	process calculus;discrete mathematics;computer science;theoretical computer science;mathematics;programming language;algorithm	Logic	-11.46391544380476	20.58403088553076	84400
2bd319d3af0f86187977f4e8198f663d9dddae9c	probabilistic symbolic model checking with prism: a hybrid approach	hybrid approach;markov decision process;sparse matrix;probabilistic temporal logics pctl;csl model checking;probabilistic system;continuous-time markov chain;probabilistic model checker;probabilistic symbolic model checking;multi-terminal bdds;discrete-time markov chain;model checking specification;probabilistic model;continuous time markov chain;model checking;temporal logic;binary decision diagram;sparse matrices	In this paper we present efficient symbolic techniques for probabilistic model checking. These have been implemented in PRISM, a tool for the analysis of probabilistic models such as discrete-time Markov chains, continuous-time Markov chains and Markov decision processes using specifications in the probabilistic temporal logics PCTL and CSL. Motivated by the success of model checkers such as SMV, which use BDDs (binary decision diagrams), we have developed an implementation of PCTL and CSL model checking based on MTBDDs (multi-terminal BDDs) and BDDs. Existing work in this direction has been hindered by the generally poor performance of MTBDD-based numerical computation, which is often substantially slower than explicit methods using sparse matrices. The focus of this paper is a novel hybrid technique which combines aspects of symbolic and explicit approaches to overcome these performance problems. For typical examples, we achieve a dramatic improvement over the purely symbolic approach. In addition, thanks to the compact model representation using MTBDDs, we can verify systems an order of magnitude larger than with sparse matrices, whilst almost matching or even beating them for speed.	binary decision diagram;citation style language;computation;markov chain;markov decision process;model checking;numerical analysis;out-of-core algorithm;prism (surveillance program);parallel computing;probabilistic ctl;sparse matrix;statistical model;while	Marta Z. Kwiatkowska;Gethin Norman;David Parker	2002		10.1007/3-540-46002-0_5	markov decision process;model checking;statistical model;markov chain;discrete time and continuous time;data structure;sparse matrix;temporal logic;computer science;continuous-time markov chain;artificial intelligence;machine learning;prism;symbolic data analysis;binary decision diagram;specification;symbolic trajectory evaluation;algorithm	Logic	-10.171881135330384	29.212072034223702	84442
140546f0706683848880254203d01bc4bf052379	a theory of may testing for actors	-calculus;large body;detailed comparison;widespread adoption;communication protocol;model checking technique;may testing;certain discipline;alternate characterization;hardware design;actor model;reduction semantics;direct comparison;subtle defect;concurrent software;interaction paths;possible approach;actor property;effective mean;actors;increased use;embedded application;asynchronous pi-calculus;type system;π calculus	The Actor model and π-calculus have served as the basis of a large body of research on concurrency. We represent the Actor model as a typed asynchronous π-calculus, called Aπ. The type system imposes a certain discipline on the use of names to capture actor properties such as uniqueness and persistence. We investigate the notion of may testing in Aπ and give a trace based characterization of it. Such a characterization simplifies reasoning about actor configurations as it does not involve quantification over all environments. We compare our characterization with that of asynchronous π-calculus, and highlight the differences that arise due to actor properties.	actor model;concurrency (computer science);persistence (computer science);type system	Prasanna Thati;Reza Ziaei;Gul A. Agha	2002			type system;π-calculus;computer science;distributed computing;programming language;algorithm	PL	-11.267510733563409	21.467281021605906	84673
4dfcec08e0a46bb7144dab2f5b6273f28b68cdda	testing protocols modeled as fsms with timing parameters	verification;rural chinese postman problem;timing constraints;protocole transmission;protocol specification and testing;maquina estado finito;implementation;optimal method;reseau ordinateur;optimization method;generation essai;chino;metodo optimizacion;computer network;ejecucion;protocolo transmision;test case generation;conformance testing;red ordenador;methode optimisation;protocol specification;test generation;verificacion;minimum length;machine etat fini;chinois;chinese;finite state machine;longueur minimale;timing;transmission protocol;time constraint	An optimization method is introduced for generating minimum-length test sequences taking into account timing constraints for FSM models of communication protocols. Due to active timers in many of today's protocols, the number of consecutive self-loops that can be traversed in a given state before a timeout occurs is limited. A test sequence that does not consider timing constraints will likely be unrealizable in a test laboratory, thereby potentially resulting in the incorrect failing of valid implementations (or, vice versa). The solution uses a series of augmentations for a protocol's directed graph representation. The resulting test sequence is proven to be of minimum-length while not exceeding the tolerable limit of consecutive self-loops at each state. Although UIO sequences are used for state veri cation method, the results also are applicable to test generation that uses distinguishing or characterizing sequences.	directed graph;failure;graph (abstract data type);loop (graph theory);mathematical optimization;timeout (computing);timer	M. Ümit Uyar;Mariusz A. Fecko;Adarshpal S. Sethi;Paul D. Amer	1999	Computer Networks	10.1016/S1389-1286(99)00007-9	real-time computing;verification;simulation;telecommunications;computer science;conformance testing;finite-state machine;implementation;chinese;algorithm;computer network	Metrics	-10.470221876454655	30.667453906650113	84741
365f9647ba08cf9fa35cf4f70f24211f6da25a19	formal verification of numerical programs: from c annotated programs to mechanical proofs		Numerical programs may require a high level of guarantee. This can be achieved by applying formal methods, such as machine-checked proofs. But these tools handle mathematical theorems while we are interested in C code, in which numerical computations are performed using floating-point arithmetic, whereas proof tools typically handle exact real arithmetic. To achieve this high level of confidence on C programs, we use a chain of tools: Frama-C, its Jessie plugin, Why and provers among Coq, Gappa, Alt-Ergo, CVC3 and Z3. This approach requires the C program to be annotated: each function must be precisely specified, and we prove the correctness of the program by proving both that it meets its specifications and that no runtime error may occur. The purpose of this paper is to illustrate, on various examples, the features of this approach.	alt-ergo;computation;coq (software);correctness (computer science);ergo proxy;formal methods;formal verification;frama-c;high-level programming language;numerical analysis;run time (program lifecycle phase);satisfiability modulo theories	Sylvie Boldo;Claude Marché	2011	Mathematics in Computer Science	10.1007/s11786-011-0099-9	combinatorics;computer science;theoretical computer science;mathematics;programming language;algorithm	Logic	-16.809035212553862	25.479916342749146	84749
03e93ff60af4e9a5cc057a0668905b58dec5f7df	efficient processing of simple temporal networks with uncertainty: algorithms for dynamic controllability verification	datavetenskap datalogi;computer science	Temporal formalisms are essential for reasoning about actions that are carried out over time. The exact durations of such actions are generally hard to predict. In temporal planning, the resulting uncertainty is often worked around by only considering upper bounds on durations, with the assumption that when an action happens to be executed more quickly, the plan will still succeed. However, this assumption is often false: if we finish cooking too early, the dinner will be cold before everyone is ready to eat. Using simple temporal networks with uncertainty (STNU), a planner can correctly take both lower and upper duration bounds into account. It must then verify that the plans it generates are executable regardless of the actual outcomes of the uncertain durations. This is captured by the property of dynamic controllability (DC), which should be verified incrementally during plan generation. Recently a new incremental algorithm for verifying dynamic controllability was proposed: EfficientIDC, which can verify if an STNU that is DC remains DC after the addition or tightening of a constraint (corresponding to a new action being added to a plan). The algorithm was shown to have a worst case complexity of $$O(n^4)$$ O ( n 4 ) for each addition or tightening. This can be amortized over the construction of a whole STNU for an amortized complexity in $$O(n^3)$$ O ( n 3 ) . In this paper we improve the EfficientIDC algorithm in a way that prevents it from having to reprocess nodes. This improvement leads to a lower worst case complexity in $$O(n^3)$$ O ( n 3 ) .	algorithm;amortized analysis;automated planning and scheduling;best, worst and average case;executable;formal verification;verification and validation;worst-case complexity	Mikael Nilsson;Jonas Kvarnström;Patrick Doherty	2015	Acta Informatica	10.1007/s00236-015-0248-8	real-time computing;simulation;computer science;mathematics;algorithm	AI	-12.316839824827516	27.65838001498892	84783
5b028d70360a7eaa6f75d4f94ce7811cab9dbf9e	state equations for discrete state systems		A method for specifying the behavior and architecture of discrete state systems such as digital electronic devices and software. The method draws on state machine theory, automata products, and recursive functions and is ordinary working mathematics, not involving formal methods or any foundational or meta-mathematical techniques. Systems in which there are levels of components that may operate in parallel or concurrently are specified in terms of function composition. An example of a network of computing devices communicating via message exchange is provided.	computer program	Victor Yodaiken	2016	CoRR		discrete mathematics;discrete system;control theory	Arch	-7.554718773727322	25.1051049479262	84813
2f5349519965823a02b0987df24b125b6aa41f67	optimal supervisor synthesis for petri nets with uncontrollable transitions: a bottom-up algorithm	supervisory control;discrete event system;forbidden state problem;petri net	Petri nets are a widely used tool to model, analyze and control discrete event systems that arise from automated production, intelligent transportation, and workflow management. For a class of Petri nets with uncontrollable transitions, this paper proposes a bottom-up algorithm to transform a given generalized mutual exclusion constraint into an optimal admissible one. Based on the transformation, a design method is proposed to synthesize an optimal supervisor. Compared with the existing methods that require the computation of exponential complexity, the proposed one can obtain an optimal supervisor with polynomial complexity. © 2015 Elsevier Inc. All rights reserved.	algorithm;computation;mutual exclusion;petri net;polynomial;subnetwork;time complexity	Shouguang Wang;Dan You;Chengying Wang	2016	Inf. Sci.	10.1016/j.ins.2015.11.003	stochastic petri net;computer science;control theory;distributed computing;process architecture;supervisory control;petri net	AI	-6.681906276091385	28.700084060214085	84981
37a77ba239a68e04230270f90a77a55a07a977c2	deciding bisimilarities on distributions	decision procedure;weak setting;bisimulation relation;algorithmic treatment;weak distribution bisimulation;equivalent state-based characterisation;usual bisimulation relation;carrier set;probability distribution;equivalent state-based reformulation	Probabilistic automata (PA) are a prominent compositional concurrency model. As a way to justify property-preserving abstractions, in the last years, bisimulation relations over probability distributions have been proposed both in the strong and the weak setting. Different to the usual bisimulation relations, which are defined over states, an algorithmic treatment of these relations is inherently hard, as their carrier set is uncountable, even for finite PAs. The coarsest of these relation, weak distribution bisimulation, stands out from the others in that no equivalent state-based characterisation is known so far. This paper presents an equivalent state-based reformulation for weak distribution bisimulation, rendering it amenable for algorithmic treatment. Then, decision procedures for the probability distribution-based bisimulation relations are presented.	automata theory;bisimulation;concurrency (computer science);probabilistic automaton	Christian Eisentraut;Holger Hermanns;Julia Krämer;Andrea Turrini;Lijun Zhang	2013		10.1007/978-3-642-40196-1_6	combinatorics;discrete mathematics;bisimulation;mathematics;algorithm	Logic	-10.617262541401884	20.4296906926399	85078
dcd59e4756aaa3e736b7ef372ae5a2fe35662899	universal model simulation: bg and extended bg as examples		This paper focuses on simulations as a means of deriving the relative power of distributed computing models. We describe an abstract simulation algorithm that enables reducing the question of solvability of a generic distributed task in one model to an equivalent question in another model. The technique implies simple equivalents to the fundamental reduction by Borowsky and Gafni, known as BG simulation, as well as to Extended BG, a more recent extension of it to colored tasks. We also sketch how the parameters of our technique can be tuned to derive recent equivalence results for models that use, in addition to basic read-write memory, k-set agreement or k-process consensus objects, or make assumptions on active resilience.		Petr Kuznetsov	2013		10.1007/978-3-319-03089-0_2	simulation;theoretical computer science;mathematics;algorithm	Theory	-14.91769077035065	20.629136360311207	85109
d6999ec2eb298607916ca806ef5e71a9dbab8d8f	are there domain specific languages?	expressiveness;domain specific languages	Turing complete languages can express unbounded computations over unbounded structures, either directly or by a suitable encoding. In contrast, Domain Specific Languages (DSLs) are intended to simplify the expression of computations over structures in restricted contexts. However, such simplification often proves irksome, especially for constructing more elaborate programs where the domain, though central, is one of many considerations. Thus, it is often tempting to extend a DSL with more general abstractions, typically to encompass common programming tropes, typically from favourite languages. The question then arises: once a DSL becomes Turing complete, then in what sense is it still domain specific?	computation;digital subscriber line;domain-specific language;level of detail;turing completeness	Greg J. Michaelson	2016		10.1145/2889420.2892271	computer science;theoretical computer science;programming language;algorithm	PL	-12.841767812133442	19.415720597958817	85132
f4a8253650a513a13d6805ddd8ed523b0d9ace70	generalizing the ds-methods for testing non-deterministic fsms		There exists a significant body of work devoted to so-called complete tests which guarantee the detection of all the faults in a given fault domain. Several methods for generating complete tests for finite state machines (FSMs) which are based on a distinguishing sequence (DS) have been proposed. These methods even if extended to use adaptive DSs apply only to deterministic FSMs and the question arises whether they can be extended to non-deterministic FSMs to test for trace inclusion. In this paper, we generalize the notion of DS to a so-called total state separator, which is an adaptive experiment distinguishing states in any FSM that is trace included into the specification FSM. We then propose a method to test non-deterministic FSMs for trace inclusion. State separator is a key means of the proposed method, which has two phases: in the first phase, a preset test is constructed, which should be repeatedly applied to a non-deterministic implementation, thus requiring its resetting; in the second phase, the implementation is tested online and no reset is required. To the best of our knowledge, this is the first method which tests non-deterministic FSMs for the trace inclusion conformance relation, while avoiding resetting implementations to re-execute tests for transition verification.	conformance testing;experiment;finite-state machine;multiple homing;the computer journal	Alexandre Petrenko;Adenilso da Silva Simão	2015	Comput. J.	10.1093/comjnl/bxu113	theoretical computer science;generalization;computer science	Logic	-12.716644446229585	28.14487692500359	85211
2205de8d28099c26ab5593471fbabc4bf559f03a	on verifying fair lossy channel systems	interconnection;algorithmique;protocole transmission;automata estado finito;maquina estado finito;computer model;fifo system;interconexion;protocolo transmision;systeme fifo;algorithmics;algoritmica;canal transmission avec perte;interconnexion;sistema fifo;communication protocol;decidibilidad;finite automaton;automate fini;decidabilite;machine etat fini;lossy channel;finite state machine;large classes;decidability;transmission protocol	Lossy channel systems are systems of finite state automata that communicate via unreliable unbounded fifo channels. They are an important computational model because of the role they play in the algorithmic verification of communication protocols. In this paper, we show that fair termination is decidable for a large class of these systems.	automata theory;computational model;decision problem;fair-share scheduling;finite-state machine;formal verification;lossy compression;multiplexing;scheduling (computing);undecidable problem	Benoît Masson;Philippe Schnoebelen	2002		10.1007/3-540-45687-2_45	fifo;decidability;communications protocol;real-time computing;telecommunications;computer science;interconnection;finite-state machine;algorithm	Logic	-8.911755252718837	26.57061639492907	85256
16e0bb11dca43b3c09eb62671f4bdef1a905e3f5	feasible proofs of szpilrajn's theorem - a proof-complexity framework for concurrent automata		The aim of this paper is to propose a proof-complexity framework for concurrent automata. Since the behavior of concurrent processes can be described with partial orders, we start by formalizing proofs of Szpilrajn’s theorem. This theorem says that any partial order may be extended to a total order. We give two feasible proofs of the finite case of Szpilrajn’s theorem. The first proof is formalized in the logical theory LA extended to ordered rings; this yields a TC Frege derivation. The second proof is formalized in the logical theory ∃LA and yields a P/poly Frege derivation. Although TC is a much smaller complexity class than P/poly, the trade-off is that the P/poly proof is algebraically simpler—it requires the algebraic theory LA over the simplest of rings: Z2.	algorithm;automata theory;automaton;bell's theorem;complexity class;concurrency (computer science);frege;frege's propositional calculus;linear algebra;p/poly;proof complexity;tc0;tracing (software)	Michael Soltys	2011	Journal of Automata, Languages and Combinatorics	10.25596/jalc-2011-037	proof complexity;discrete mathematics;combinatorics;mathematics;mathematical proof	Logic	-11.49545030575501	20.798031859538558	85467
2e943cb5b338a5a8579726a72e6573ff31099737	conformance testing for real-time systems	on the fly algorithms;partial observability;system under test;conformance testing;specification and verification;on the fly;test generation;partial observation;timed automata;coverage;reaction time;real time systems	We propose a new framework for black-box conformance testing of real-time systems. The framework is based on the model of partially-observable, non-deterministic timed automata. We argue that partial observability and non-determinism are essential features for ease of modeling, expressiveness and implementability. The framework allows the user to define, through appropriate modeling, assumptions on the environment of the system under test (SUT) as well as on the interface between the tester and the SUT. We consider two types of tests: analog-clock tests and digital-clock tests. Our algorithm to generate analogclock tests is based on an on-the-fly determinization of the specification automaton during the execution of the test, which in turn relies on reachability computations. The latter can sometimes be costly, thus problematic, since the tester must quickly react to the actions of the system under test. Therefore, we provide techniques which allow analog-clock testers to be represented as deterministic timed automata, thus minimizing the reaction time to a simple state jump. We provide algorithms for static or on-the-fly generation of digitalclock tests. These tests measure time only with finite-precision, digital clocks, another essential condition for implementability. We also propose a technique for location, edge and state coverage of the specification, by reducing the problem to covering a symbolic reachability graph. This avoids having to generate too many tests. We report on a prototype tool TTG and two case studies: a lighting device and the Bounded Retransmission Protocol. Experimental results obtained by applying TTG on the Bounded Retransmission Protocol show that only a few tests suffice to cover thousands of reachable symbolic states in the specification.	automata theory;black box;bounded retransmission protocol;computation;conformance testing;nondeterministic algorithm;observable;powerset construction;prototype;reachability;real-time clock;real-time computing;retransmission (data networks);system under test;timed automaton	Moez Krichen;Stavros Tripakis	2009	Formal Methods in System Design	10.1007/s10703-009-0065-1	mental chronometry;real-time computing;simulation;computer science;conformance testing;system under test;algorithm	Logic	-12.178789544161882	28.75288420688425	85519
3fa8729d29b0ac3763172dae330824f2f5670240	modular transformations of clp programs		~ this paper we propose an unfold/fold transformation system for Constraint Log1c Pr.ograms. The framework is inspired by the one of Ta.maki and Sato for pure logtc ~rograms 1.18]. The use of CLP permits a more concise definition for the folcling operat1on. We provide conrutions for applying the system in a modul~ way .. un~er these ~~nditions the &ystem is correct WTt a semantics (nsemantlcs) which 1s composJtJOnal WTt the union of programs. As corollaries we also pr~ve the correctness of the non-modular system wrt. the answer constraint semantJcs ~d the ]east model semantics. Finally, we show how these results can be applied to t.he logic programming case.	correctness (computer science);logic programming;truth-table reduction	Sandro Etalle;Maurizio Gabbrielli	1994			modular design;computer architecture;computer science	AI	-17.318480356421837	20.290021560706414	85537
da961913d998e3668c922c8a68ceafa5d733f536	structural properties of lpv to lfr transformation: minimality, input-output behavior and identifiability		In this paper, we introduce and study important properties of the transformation of Affine Linear ParameterVarying (ALPV) state-space representations into Linear Fractional Representations (LFR). More precisely, we show that (i) state minimal ALPV representations yield minimal LFRs, and vice versa, (ii) the input-output behavior of the ALPV represention determines uniquely the input-output behavi or of the resulting LFR, (iii) structurally identifiable ALPVs yield structurally identifiable LFRs, and vice versa. We then characterize LFR models which correspond to equivalent ALPV models based on their input-output maps. As illustrated all along the paper, these results have important consequences for identification and control of systems described by LFRs.	linear logic;map;state space;z-order curve	Ziad Alkhoury;Mihály Petreczky;Guillaume Mercère	2016	CoRR			SE	-8.208481717488173	21.13330497243557	85580
49921f2ea269f491a691f171978042b446bf2f50	synthesizing concurrent programs using answer set programming.	answer set programming;synthesis;settore ing inf 05 sistemi di elaborazione delle informazioni;concurrent programs	We address the problem of the automatic synthesis of concurrent programs within a framework based on Answer Set Programming (ASP). The concurrent program to be synthesized is specified by providing both the behavioural and the structural properties it should satisfy. Behavioural properties, such as safety and liveness properties, are specified by using formulas of the Computation Tree Logic, which are encoded as a logic program. Structural properties, such as the symmetry of processes, are also encoded as a logic program. Then, the program which is the union of these two encodings, is given as input to an ASP system which returns as output a set of answer sets. Finally, each answer set is decoded into a synthesized program that, by construction, satisfies the desired behavioural and structural properties.	answer set programming;atomicity (database systems);automatic control;computation tree logic;concurrent computing;formal specification;kripke semantics;liveness;logic programming;solver;stable model semantics	Emanuele De Angelis;Alberto Pettorossi;Maurizio Proietti	2011	Fundam. Inform.	10.3233/FI-2012-758	discrete mathematics;stable model semantics;computer science;artificial intelligence;theoretical computer science;answer set programming;mathematics;programming language;algorithm	Logic	-13.660112464375688	22.57452217789381	85746
f2562c6fbe8fe8bf999952147f070296fa64dd03	the linear time - branching time spectrum ii	real time;spectrum;linear time	This paper studies semantic equivalences and preorders for sequential systems with silent moves, restricting attention to the ones that abstract from successful termination, stochastic and real-time aspects of the investigated systems, and the structure of the visible actions systems can perform. It provides a parameterized definition of such a preorder, such that most such preorders and equivalences found in the literature are obtained by a suitable instantiation of the parameters. Other instantiations yield preorders that combine properties from various semantics. Moreover, the approach shows several ways in which preorders that were originally only considered for systems without silent moves, most notably the ready simulation, can be generalized to an abstract setting, and how preorders that were originally only considered for for systems without divergence, such as the coupled simulation, can be extended to divergent systems. All preorders come with—or rather as—a modal characterization, and when possible also a relational characterization. The paper concludes with some pros and cons of the preorders.	time complexity	Rob J. van Glabbeek	1993		10.1007/3-540-57208-2_6	time complexity;spectrum;computer science	Theory	-10.310188597216449	20.491925180536796	85895
8d33c7b7d21ba9c45830179f71a06d1246e87989	one unary function says less than two in existential second order logic	second order;existential second order logic;complexite calcul;logique mathematique;formal languages;logica matematica;theorie modeles;spectrum;mathematical logic;computational complexity;teoria modelos;finite model theory;formal language;model theory;langage formel	We show that the set of square numbers is the spectrum of an F 1 1;1-sentence (existential second order logic with quantiication over unary function variables) with two unary function variables, but it is not the spectrum of F 1 1;1-sentences with only one unary function variable.	unary operation	Bernd Loescher	1997	Inf. Process. Lett.	10.1016/S0020-0190(96)00198-6	predicate logic;formal language;discrete mathematics;computer science;mathematics;algorithm	Logic	-6.043917460409412	18.96412063908685	85989
f511df8b2f443aed5557e7cda925763bc4531efd	model checking c programs using f-soft	semantic networks;program verification;verification complexity c program model checking f soft verification platform formal verification technique equivalence checking automatic verification software program verification finite state systems sat based model checking bdd based model checking;finite state machines;c language;binary decision diagrams;model checking;computational complexity;object oriented modeling hardware circuit testing protocols concrete chaotic communication national electric code laboratories computer science formal verification;semantic networks program verification c language finite state machines binary decision diagrams computational complexity	With the success of formal verification techniques like equivalence checking and model checking for hardware designs, there has been growing interest in applying such techniques for formal analysis and automatic verification of software programs. This paper provides a brief tutorial on model checking of C programs. The essential approach is to model the semantics of C programs in the form of finite state systems by using suitable abstractions. The use of abstractions is key, both for modeling programs as finite state systems and for reducing the model sizes in order to manage verification complexity. We provide illustrative details of a verification platform called F-Soft, which provides a range of abstractions for modeling software, and uses customized SAT-based and BDD-based model checking techniques targeted for software.	formal equivalence checking;formal verification;model checking;turing completeness	Franjo Ivancic;Ilya Shlyakhter;Aarti Gupta;Malay K. Ganai;Vineet Kahlon;Chao Wang;Zijiang Yang	2005	2005 International Conference on Computer Design	10.1109/ICCD.2005.77	model checking;verification and validation of computer simulation models;verification;formal verification;software verification;computer science;theoretical computer science;formal equivalence checking;high-level verification;runtime verification;finite-state machine;semantic network;programming language;computational complexity theory;abstraction model checking;intelligent verification;symbolic trajectory evaluation;algorithm;functional verification	SE	-15.554058591848024	29.242608217884985	86047
8704e5b59dc121d7b80381a0f54bf3ccd57c59e9	anzu: a tool for property synthesis	formal specification;linear temporal logic	We present the tool A NZU. ANZU takes a formal specification of a design and generates a functionally correct system if one exis ts. The specification is given as a set of linear temporal logic (LTL) formulas belo nging to the class of generalized reactivityof rank 1. Such formulas cover the majority of the formulas used in practice. A NZU is an implementation of the symbolic reactive(1) approach to synthesis by Piterman, Pnueli, and Sa’ar. If the specification isrealizableANZU provides the user with a Verilog module that represents a cor re t finite-state system.	formal specification;linear temporal logic;verilog	Barbara Jobstmann;Stefan J. Galler;Martin Weiglhofer;Roderick Bloem	2007		10.1007/978-3-540-73368-3_29	linear temporal logic;computer science;theoretical computer science;formal specification;programming language;algorithm	Logic	-15.866796293672015	28.309283620161903	86054
6a3c4949a9e8e1998db9f1a5a0901db189209f7c	pruning, pushdown exception-flow analysis	pushdown analysis;large scale standard java benchmarks pruning pushdown exception flow analysis static reasoning control flow analysis points to analysis object oriented language full featured exceptions abstract garbage collection object oriented programs liveness analysis reachability computation enhancement;resource management;semantics;abstract garbage collection;abstracts semantics concrete java context resource management algorithm design and analysis;abstracts;exception flow analysis;abstract garbage collection static analysis exception flow analysis pushdown analysis;static analysis;context;algorithm design and analysis;storage management data flow analysis java object oriented languages object oriented programming parallel languages reachability analysis;concrete;java	Statically reasoning in the presence of exceptions and about the effects of exceptions is challenging: exception-flows are mutually determined by traditional control-flow and points-to analyses. We tackle the challenge of analyzing exception-flows from two angles. First, from the angle of pruning control-flows (both normal and exceptional), we derive a pushdown framework for an object-oriented language with full-featured exceptions. Unlike traditional analyses, it allows precise matching of throwers to catchers. Second, from the angle of pruning points-to information, we generalize abstract garbage collection to object-oriented programs and enhance it with liveness analysis. We then seamlessly weave the techniques into enhanced reach ability computation, yielding highly precise exception-flow analysis, without becoming intractable, even for large applications. We evaluate our pruned, pushdown exception-flow analysis, comparing it with an established analysis on large scale standard Java benchmarks. The results show that our analysis significantly improves analysis precision over traditional analysis within a reasonable analysis time.	computation;control flow;control flow analysis;data-flow analysis;dataflow;dyck language;exception handling;garbage collection (computer science);java;lambda calculus;live variable analysis;pointer analysis;program analysis;stack (abstract data type)	Shuying Liang;Weibin Sun;Matthew Might;Andrew W. Keep;David Van Horn	2014	2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation	10.1109/SCAM.2014.44	algorithm design;concrete;computer science;resource management;theoretical computer science;database;semantics;programming language;java;static analysis	PL	-18.88994995916323	29.62360664047955	86305
87ec7db85d252832b31263e0050b2f08aab1bafa	session types and subtyping for orchestrated interactions		Abstract In the setting of the π -calculus with binary sessions, we aim at relaxing the notion of duality of session types by the concept of retractable compliance developed in contract theory. This leads to extending session types with a new type operator of “speculative selection” including choices not necessarily offered by a compliant partner. We address the problem of selecting successful communicating branches by means of an operational semantics based on orchestrators, which has been shown to be equivalent to the retractable semantics of contracts, but clearly more feasible. A type system, sound with respect to such a semantics, is hence provided. The introduction of subtyping when interactions are orchestrated naturally leads to explicit subtyping, where coercions are functors on orchestrators. Besides, priority-governed selection policies (either at type- or process-level) are investigated in order to get rid of nondeterministic behaviours but those of the partner processes of the interactions.		Franco Barbanera;Ugo de'Liguoro	2019	J. Log. Algebr. Meth. Program.	10.1016/j.jlamp.2018.10.001	operator (computer programming);discrete mathematics;theoretical computer science;nondeterministic algorithm;semantics;duality (optimization);mathematics;functor;contract theory;operational semantics;subtyping	SE	-13.062095078678793	20.363435976324112	86315
331a997f40ed78d79579a599e6f5e782d8a72799	beginner's luck: a language for property-based generators	publikationer;konferensbidrag;random testing;property based testing;artiklar;rapporter;domain specific language;constraint solving;narrowing	Property-based random testing à la QuickCheck requires building efficient generators for well-distributed random data satisfying complex logical predicates, but writing these generators can be difficult and error prone. We propose a domain-specific language in which generators are conveniently expressed by decorating predicates with lightweight annotations to control both the distribution of generated values and the amount of constraint solving that happens before each variable is instantiated. This language, called Luck, makes generators easier to write, read, and maintain.   We give Luck a formal semantics and prove several fundamental properties, including the soundness and completeness of random generation with respect to a standard predicate semantics. We evaluate Luck on common examples from the property-based testing literature and on two significant case studies, showing that it can be used in complex domains with comparable bug-finding effectiveness and a significant reduction in testing code size compared to handwritten generators.	cognitive dimensions of notations;constraint satisfaction problem;domain-specific language;linear algebra;procedural generation;quickcheck;random testing;randomness;semantics (computer science)	Leonidas Lampropoulos;Diane Gallois-Wong;Catalin Hritcu;John Hughes;Benjamin C. Pierce;Li-yao Xia	2017		10.1145/3009837.3009868	random testing;computer science;domain-specific language;theoretical computer science;programming language;algorithm	PL	-19.087943742526154	24.873779144574915	86603
78d7a13b5a9b3d4457a09a63c758cf501b2fb6f2	recovering representations of systems with repetitive subfunctions from observations	qa076 computer software;qa075 electronic computers computer science	This paper proposes an algorithm for the construction of an MSC graph from a given set of actual observations of an existing concurr ent system which has repetitive subfunctions. When a design representing th e current functionality of the existing system is desired, such a graph can be checked for safe realizability and be used as input to existing synthesis techniques to construct the design for the system functionality.	algorithm;graph (discrete mathematics)	Guy-Vincent Jourdan;Hüsnü Yenigün	2016	Multiple-Valued Logic and Soft Computing		simulation;computer science;artificial intelligence;theoretical computer science;machine learning	EDA	-12.464634016301408	28.450695054456073	86905
4a8f7aedf37a4468682f6160936cbd6d20f8372e	initial semantics in logics with constructors		The constructor-based logics constitute the logical foundation of the so-called OTS/CafeOBJ method, a modeling, specification and verification method of the observational transition systems. It is well known the important role played in algebraic specifications by the initial algebras semantics. Free models along presentation morphisms provide semantics for the modules with initial denotation in structured specification languages. Following Goguen and Burstall, the notion of logical system over which we build specifications is formalized as an institution. The present work is an institution-independent study of the existence of free models along sufficient complete presentation morphisms in logics with constructors in the signatures.	antivirus software;complex system;constructor (object-oriented programming);diagram;first-order predicate;formal system;initial algebra;intensional logic;interpolation;linear algebra;network address translation;semiconductor industry;specification language	Daniel Gâinâ;Kokichi Futatsugi	2015	J. Log. Comput.	10.1093/logcom/exs044	t-norm fuzzy logics	Logic	-13.559673726302462	19.383371401126688	87011
542b0c39bf104597090fe86d6de9bea79ca8358f	decidability of a temporal logic problem for petri nets	temporal logic;petri net	Abstract   The paper solves an open problem from [4] by showing a decision algorithm for a temporal logic language  L ( Q ′, GF). It implies the decidability of the problem of the existence of an infinite weakly fair occurence sequence for a given Petri net; thereby an open problem from [2] is solved.	petri net;temporal logic	Petr Jancar	1990	Theor. Comput. Sci.	10.1016/0304-3975(90)90006-4	combinatorics;discrete mathematics;linear temporal logic;stochastic petri net;concurrency;temporal logic;computer science;mathematics;petri net;algorithm	Logic	-8.975888773467748	23.83069968349491	87140
dfcf3c200fe76cd55af1c379495c4e357028c984	verification of floating point programs	mathematical and computer sciences;artificial intelligence;computer science	This copy of the thesis has been supplied on condition that anyone who consults it is understood to recognise that its copyright rests with its author and that no quotation from the thesis and no information derived from it may be published without proper acknowledgement. Thesis Summary In this thesis we present an approach to automated verification of floating point programs. Existing techniques for automated generation of correctness theorems are extended to produce proof obligations for accuracy guarantees and absence of floating point exceptions. A prototype automated real number theorem prover is presented, demonstrating a novel application of function interval arithmetic in the context of subdivision-based numerical theorem proving. The prototype is tested on correctness theorems for two simple yet non-trivial programs, proving exception freedom and tight accuracy guarantees automatically. The prover demonstrates a novel application of function interval arithmetic in the context of subdivision-based numerical theorem proving. The experiments show how function intervals can be used to combat the information loss problems that limit the applicability of traditional interval arithmetic in the context of hard real number theorem proving. Acknowledgements I thank my supervisor Michal Konečn´y, for his patience in guiding me through the theory of computing, and for his availability whenever advise or guidance was needed. It kept me motivated and made the research experience most enjoyable. I thank Dr. Amin Farju-dian for his collaboration and inspiring discussions. I thank Dr. Alan Barnes, for providing accommodation and stimulating conversations. I extend special thanks to Dr. Patchara Punyamoonwongsa, she has been a great friend and provided valuable advise and motivation. It gives me great pleasure to thank my dear parents Anna and Andrzej and brother Adam. During the course of my studies they have provided me with all I could wish for and it is thanks to them that I have had the peace of mind to focus completely on my studies.ick Chapman, the project liaison at Praxis, for providing generous access to training and other resources at the company.	altran praxis;automated theorem proving;correctness (computer science);experiment;interval arithmetic;mind;numerical analysis;prototype;subdivision surface;theory of computing;unix signal	Jan Andrzej Duracz	2010			discrete mathematics;theoretical computer science;automated proof checking;mathematics;automated theorem proving;algorithm	PL	-18.983719279721072	18.411519771944178	87381
598234347ca43e64c84bf1a735c5ce4fdcec4286	verification of concurrent quantum protocols by equivalence checking	research outputs;research publications	We present a tool which uses a concurrent language for describing quantum systems, and performs verification by checking equivalence between specification and implementation. In general, simulation of quantum systems using current computing technology is infeasible. We restrict ourselves to the stabilizer formalism, in which there are efficient simulation algorithms. In particular, we consider concurrent quantum protocols that behave functionally in the sense of computing a deterministic input-output relation for all interleavings of the concurrent system. Crucially, these input-output relations can be abstracted by superoperators, enabling us to take advantage of linearity. This allows us to analyse the behaviour of protocols with arbitrary input, by simulating their operation on a finite basis set consisting of stabilizer states. Despite the limitations of the stabilizer formalism and also the range of protocols that can be analysed using this approach, we have applied our equivalence checking tool to specify and verify interesting and practical quantum protocols from teleportation to secret sharing.	algorithm;basis set (chemistry);bisimulation;cobham's thesis;concurrency (computer science);correctness (computer science);formal equivalence checking;formal system;forward error correction;input/output;modeling language;parallel computing;quantum key distribution;quantum state;quantum system;secret sharing;semantics (computer science);sequential consistency;shor's algorithm;simulation;stabilizer code;superoperator;triune continuum paradigm;turing completeness	Ebrahim Ardeshir-Larijani;Simon J. Gay;Rajagopal Nagarajan	2014		10.1007/978-3-642-54862-8_42	discrete mathematics;computer science;theoretical computer science;mathematics;algorithm	Logic	-15.1025669256957	20.602962835512233	87458
23a6c1cf3a28d622ecb77c40e368ab3c73dd767b	the complexity of set constraints	computational complexity;technical report;program analysis;computer science;type inference	Set constraints are relations between sets of terms They have been used extensively in various applications in program analysis and type inference We present several results on the computational complexity of solving systems of set constraints The systems we study form a natural complexity hierarchy depending on the form of the constraint language	computational complexity theory;program analysis;type inference	Alexander Aiken;Dexter Kozen;Moshe Y. Vardi;Edward L. Wimmers	1993		10.1007/BFb0049320	program analysis;computational problem;parameterized complexity;complexity;decision tree model;computer science;technical report;theoretical computer science;type inference;computational resource;worst-case complexity;programming language;computational complexity theory;asymptotic computational complexity;algorithm;descriptive complexity theory	Logic	-15.455312548950618	23.044084649095243	87649
2b4f0ac33276b28b0f16c60891418fd3648653bf	automatic complexity analysis for programs extracted from coq proof	proof assistant;symbolic computation;time complexity;complexity analysis;automatic generation;computer algebra system;theorem proving;recurrence relation;program extraction	We describe an automatic complexity analysis mechanism for programs extracted from proofs carried out with the proof assistant Coq. By extraction, we mean the automatic generation of MiniML code [3]. By complexity analysis, we mean the automatic generation of a description of the time-complexity of a MiniML program in terms of the number of steps needed for its execution. This description can be a natural number for closed program, that is, programs coming along with their actual inputs. For programs per se, the description is given in terms of a set of recurrence relations which relate the number of steps of a computation in terms of the size of the inputs. Going from these recurrence relation to actual complexity functions is a hard task that requires the use of sophisticated tools for symbolic computations. This part is not implemented for the moment although we have manually used the MAPLE computer algebra system in some cases.	analysis of algorithms;computer algebra system;coq (software);maple;proof assistant;recurrence relation;symbolic computation;time complexity	Jean-Pierre Jouannaud;Weiwen Xu	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.08.005	time complexity;discrete mathematics;symbolic computation;recurrence relation;computer science;theoretical computer science;mathematics;automated theorem proving;proof assistant;programming language;proof complexity;algorithm	Logic	-16.958570633332204	24.97286319771364	87822
3242227d375df672a0b90ffb9403bfc68890d8e5	symbolic supervisory control of infinite transition systems under partial observation using abstract interpretation	controller synthesis;symbolic transition systems;partial observation;abstract interpretation	We propose algorithms for the synthesis of state-feedback controllers with partial observation of infinite state discrete event systems modelled by Symbolic Transition Systems. We provide models of safe memoryless controllers both for potentially deadlocking and deadlock free controlled systems. The termination of the algorithms solving these problems is ensured using abstract interpretation techniques which provide an overapproximation of the transitions to disable. We then extend our algorithms to controllers with memory and to online controllers. We also propose improvements in the synthesis of controllers in the finite case which, to our knowledge, provide more permissive solutions than what was previously proposed in the literature. Our tool SMACS gives an empirical validation of our methods by showing their feasibility, usability and efficiency.	abstract interpretation;algorithm;deadlock;distributed computing;usability	Gabriel Kalyon;Tristan Le Gall;Hervé Marchand;Thierry Massart	2012	Discrete Event Dynamic Systems	10.1007/s10626-011-0101-3	control engineering;real-time computing;computer science;control theory	Logic	-8.70895140389971	28.836113640816414	87910
10c5d3878344894299b76d1c2eb20fc02593be56	a theoretical evaluation of selected backtracking algorithms	constraint satisfaction;search trees;backtracking;constraint satisfaction problem	In recent years, many new backtracking algorithms for solving constraint satisfaction problems have been proposed. The algorithms are usually evalu­ ated by empirical testing. This method, however, has its limitations. Our paper adopts a different, purely theoretical approach, which is based on char­ acterizations of the sets of search tree nodes visited by the backtracking algorithms. A notion of in­ consistency between instantiations and variables is introduced, and is shown to be a useful tool for char­ acterizing such well-known concepts as backtrack, backjump, and domain annihilation. The charac­ terizations enable us to: (a) prove the correctness of the algorithms, and (b) partially order the algo­ rithms according to two standard performance mea­ sures: the number of nodes visited, and the number of consistency checks performed. Among other re­ sults, we prove the correctness of Backjumping and Conflict-Directed Backjumping, and show that For­ ward Checking never visits more nodes than Backjumping. Our approach leads us also to propose a modification to two hybrid backtracking algo­ rithms, Backmarking with Backjumping (BMJ) and Backmarking with Conflict-Directed Backjumping (BM-CBJ), so that they always perform fewer con­ sistency checks than the original algorithms.	algorithm;backjumping;backmarking;backtracking;constraint satisfaction problem;correctness (computer science);emoticon;eventual consistency;search tree;two-hybrid screening	Grzegorz Kondrak;Peter van Beek	1995		10.1016/S0004-3702(96)00027-6	mathematical optimization;constraint satisfaction;constraint learning;computer science;theoretical computer science;backjumping;mathematics;constraint satisfaction problem;algorithm;backtracking	AI	-15.153709467204834	24.31801227586949	87930
0a971fee0bb9893944ee0c64d7860b0d7e1ebcdd	termination orderings for rippling	rewrite rule;term rewriting	Rippling is a special type of rewriting developed for inductive theorem proving. Bundy et. al. have shown that rippling terminates by providing a well-founded order for the annotated rewrite rules used by rippling. Here, we simplify and generalize this order, thereby enlarging the class of rewrite rules that can be used. In addition, we extend the power of rippling by proposing new domain dependent orders. These extensions elegantly combine rippling with more conventional term rewriting. Such combinations o er the exibility and uniformity of conventional rewriting with the highly goal directed nature of rippling. Finally, we show how our orders simplify implementation of provers based on rippling.	automated theorem proving;circuit complexity;experiment;heuristic;mutual recursion;naruto shippuden: clash of ninja revolution 3;newman's lemma;rewrite (programming);rewriting;rippling;schematic;unfolding (dsp implementation)	David A. Basin;Toby Walsh	1994		10.1007/3-540-58156-1_34	discrete mathematics;computer science;mathematics;programming language;algorithm;rippling	AI	-16.432582095896905	21.554739739291623	87947
802ad33b486bb2995fb4b7bdc7a748110ab05985	modifying the simplex algorithm to a constraint solver	constraint logic programs;simplex algorithm;constraint satisfaction;logic programs	Constraint Logic Programming (CLP) tries to unify the best from Logic Programming and Constraint Satisfaction. However, implementors of languages of the CLP class such as the CLP(R) must solve some unique problems such as constraint backtracking and devising an incremental constraint solver. This paper describes how the Simplex algorithm was adapted to serve as a constraint solver in a prototype CLP(R) system. The algorithm can handle equations as well as inequalities. The Simplex algorithm can be incrementalized easily by dividing it into invariant preserving steps. Constraint backtracking can be implemented efficiently by changing slack variable types.	backtracking;clp(r);constraint satisfaction;prototype;simplex algorithm;slack variable;solver	Juhani Jaakola	1990		10.1007/BFb0024178	constraint logic programming;concurrent constraint logic programming;mathematical optimization;constraint programming;discrete mathematics;binary constraint;ac-3 algorithm;constraint satisfaction;constraint learning;constraint graph;constraint satisfaction dual problem;complexity of constraint satisfaction;satisfiability modulo theories;simplex algorithm;slack variable;constraint satisfaction problem;algorithm;difference-map algorithm;hybrid algorithm;local consistency;backtracking	AI	-15.877607323879868	23.22251829284647	87964
111d97258890461b46bbe170729f3a14048da99f	verification of logic programs with delay declarations	programming language;logic programs;dynamic networks	Logic programs augmented with delay declarations form a higly expressive programming language in which dynamic networks of processes that communicate asynchronously by means of multiparty channels can be easily created. In this paper we study correctness these programs. In particular, we propose proof methods allowing us to deal with occur check freedom, absence of deadlock, absence of errors in presence of arithmetic relations, and termination. These methods turn out to be simple modiications of the corresponding methods dealing with Prolog programs. This allows us to derive correct delay declarations by analyzing Prolog programs. Finally, we point out diiculties concerning proofs of termination.	correctness (computer science);deadlock;logic programming;occurs check;programming language;prolog	Krzysztof R. Apt;Ingrid Luitjes	1995		10.1007/3-540-60043-4_47	dynamic logic;description logic;horn clause;computer science;programming language implementation;theoretical computer science;functional logic programming;signature;hardware description language;programming language;prolog;logic programming;multimodal logic;algorithm	PL	-16.6405527653657	21.661958373641017	88053
0fc7fb21e97c8a9dd03e68c37e0bf28de157c448	the theory and practice of salt	real time operators;runtime verification;practical use;execution time;temporal specification;real time;real time properties;keywords assertion language;web interface;specification patterns;test drive;conference paper;model checking;regular expressions;language development;theoretical foundations;reactive system;real time logic;general purpose;theoretical foundation;regular expression;run time verification	Salt is a general purpose specification and assertion language developed for creating concise temporal specifications to be used in industrial verification environments. It incorporates ideas of existing approaches, such as PSL or Specification Patterns, in that it provides operators to express scopes and exceptions, as well as support for a subset of regular expressions. On the one hand side, Salt exceeds specific features of these approaches, for example, in that it allows the nesting of scopes and supports the specification of real-time properties. On the other hand, Salt is fully translatable to LTL, if no real-time operators are used, and to TLTL (also known as state-clock logic), if real-time operators appear in a specification. The latter is needed in particular for verification tasks to do with reactive systems imposing strict execution times and deadlines. Salt’s semantics is defined in terms of a translation to temporal (real-time) logic, and a compiler is freely available from the project web site, including an interactive web interface to test drive the compiler. This tutorial paper details on the theoretical foundations of Salt as well as its practical use in applications such as model checking and runtime verification.		Andreas Bauer;Martin Leucker	2011		10.1007/978-3-642-20398-5_3	real-time computing;computer science;software engineering;runtime verification;programming language;regular expression;algorithm	Embedded	-18.972285846146296	27.769538775089497	88268
f2eee2df3241821e93ed24db799c0ce15fa36301	more on testing bcd words with fortran	fortran	"""authors make the following statement with respect to the final or output phase of the program: """"The precedence list for the operators is employed in this algorithm in order to avoid the insertion of redundant parentheses into the output string."""" If one uses, in output, the precedence list given in the article, parentheses will not always be inserted correctly. The expression (X • A-(A. X-4-B))/XP2 when broken down into the parentheses-free form by the Ershov algorithm and then reconstructed by the output algorithm (without differentiating) becomes (X. A-A. X + B)/XP2. = p()) > p(~) = p(~). In passing, it should be noted that in the flow chart given in Fig. 3, there should be a (j ~j-1) immediately after the test (j >~ +1). An Algebraic Compiler for the FORTRAN Assembly Program Dear Editor: An algebraic compiler has been written which may be added to the FORTRAN Assembly Program. This compiler will expand all algebraic statements with the following operations: addition, subtraction, multiplication and division. It will compile multi-level expressions in floating-point arithmetic (this can easily be revised to fixed-point). It is called for by a pseudo-operation (EXPR) requesting a compilation of the expression found in the variable field. The method of compiling has been fashioned after that described by D. J. Dijkstra [1]. In brief, the expression is translated into a Polish string and the string is then compiled. The compilation is in floating point and the number of arguments is a function of table lengths. One additional feature of the compiler is the ability to compile indirect addressing into an expression. At the suggestion of W. H. Wattenburg, the character string left parenthesis, name, right parenthesis, i.e., (name), can uniquely define indirect addressing on the name within the parentheses. This feature has been incorporated into the compiler. It has been thought that it will be useful since the compiler is contained in an assembly program. Another feature is the ability to handle multiple assignments in a number of ways. The following expressions are equivalent: EXPR A=B=C+])/E EXPR A=(B=C+D/E) EXPR A=C+D/E=B Also, there is no necessity for an assignment character. If there is no assignment, the result will be left in the accumulator register. A by-product of multiple assignments is that a portion of an expression may be stored. Thus: EXPR A= (B=C+D)/E It is felt that this will be a useful tool in scientific computation. issue of …"""	accumulator (computing);addressing mode;algorithm;assembly language;binary-coded decimal;compiler;computation;computational science;expr;fixed-point arithmetic;flowchart;fortran;insertion sort;like button;linear algebra;list of programming languages by type;name;string (computer science)	C. A. Oster	1962	Commun. ACM	10.1145/368996.368999	computer architecture;parallel computing;computer science;programming language	PL	-14.724336459924185	32.164153525583195	88401
03b7f4b7dc5d4e7ddf7c14d3551970faac1244b6	a polynomial translation of π-calculus (fcp) to safe petri nets	model checking;π calculus;finite control process;petri net	We develop a polynomial translation from finite control π-calculus processes to safe low-level Petri nets. To our knowledge, this is the first such translation. It is natural in that there is a close correspondence between the control flows, enjoys a bisimulation result, and is suitable for practical model checking.	bisimulation;control flow;fibre channel protocol;high- and low-level;model checking;petri net;polynomial;π-calculus	Roland Meyer;Victor Khomenko;Reiner Hüchting	2012		10.1007/978-3-642-32940-1_31	model checking;discrete mathematics;π-calculus;stochastic petri net;computer science;theoretical computer science;process architecture;programming language;petri net;algorithm	Logic	-10.960672189738082	24.196921832314995	88555
fb32186088f10fb46df92f7e3426cff11291a5c9	diagnosis of continuous valued systems in transient operating regions	topology;value system;transient response fault diagnosis reliability theory monitoring topology;fast breeder reactor continuous valued system diagnosis transient operating regions embedded systems continuous processes digital processors reliability monitoring fault isolation capabilities prediction abrupt faults complex dynamic systems transient behavior parsimonious topological system models predicted transient effects faulty behavior signatures high order time derivatives dynamic effects progressive monitoring scheme transient analysis mechanisms steady state analysis secondary sodium cooling loop;system modeling;reliability theory;embedded system;transient analysis;higher order;transient response;monitoring;complex dynamical systems;present day;fast breeder reactor;steady state fault detection monitoring transient analysis control systems redundancy parameter estimation fault diagnosis context modeling embedded system;steady state analysis;fault isolation;fault diagnosis;transient behavior	The complexity of present day embedded systems (continuous processes controlled by digital processors), and the increased demands on their reliability motivate the need for monitoring and fault isolation capabilities in the embedded processors. This paper develops monitoring, prediction, and fault isolation methods for abrupt faults in complex dynamic systems. The transient behavior in response to those faults is analyzed in a qualitative framework using parsimonious topological system models. Predicted transient effects of hypothesized faults are captured in the form of signatures that specify future faulty behavior as higher order time-derivatives. The dynamic effects of faults are analyzed by a progressive monitoringscheme till transient analysis mechanisms have to be suspended in favor of steady state analysis. This methodology has been successfully applied to monitoring of the secondary sodium cooling loop of a fast breeder reactor.	antivirus software;breeder (cellular automaton);central processing unit;computer cooling;dynamical system;embedded system;fault detection and isolation;microprocessor;occam's razor;reactor (software);steady state;transient state	Pieter J. Mosterman;Gautam Biswas	1999	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/3468.798059	real-time computing;systems modeling;higher-order logic;reliability theory;computer science;control theory;value system;steady state;transient response;fault detection and isolation	Embedded	-4.548160965183516	28.52239211357869	88720
08acf581c8de66cd850e08897439ec7b8a23f3bf	drat-trim: efficient checking and trimming using expressive clausal proofs		The DRAT-trim tool is a satisfiability proof checker based on the new DRAT proof format. Unlike its predecessor, DRUP-trim, all presently known SAT solving and preprocessing techniques can be validated using DRAT-trim. Checking time of a proof is comparable to the running time of the proof-producing solver. Memory usage is also similar to solving memory consumption, which overcomes a major hurdle of resolution-based proof checkers. The DRAT-trim tool can emit trimmed formulas, optimized proofs, and new TraceCheck dependency graphs. We describe the output that is produced, what optimizations have been made to check RAT clauses, and potential applications of the tool.	automated proof checking;boolean satisfiability problem;horn clause;preprocessor;solver;time complexity	Nathan Wetzler;Marijn Heule;Warren A. Hunt	2014		10.1007/978-3-319-09284-3_31	computer science;theoretical computer science;mathematics;programming language;algorithm	Logic	-15.06190772893959	24.79850943596173	88917
007dde517a572090a6021aae2f1c4dd4ba9e51a5	ltl model checking for recursive programs	control flow graph;boolean satisfiability;programming model;model checking;linear temporal logic	We propose a complete algorithm to model check LTL (Linear Temporal Logic) formulas with recursive programs. Our program models are control flow graphs extended with procedure calls. The LTL formulas may then be used to specify constraints on the global variables and the local variables in the current scope. Our algorithm is based on semi-symbolic simulation of control-flow graphs to search for counter-examples. We apply post-dominance relation to reduce the number of the exploration traces. The existence of counter-examples is reduced to Boolean satisfiability while the termination of the exploration is reduced to Boolean unsatisfiability. We report our implementation and experiment.	model checking;recursion (computer science)	Geng-Dian Huang;Lin-Zan Cai;Farn Wang	2009		10.1007/978-3-642-04761-9_28	model checking;and-inverter graph;discrete mathematics;linear temporal logic;boolean expression;computer science;maximum satisfiability problem;theoretical computer science;mathematics;programming paradigm;boolean satisfiability problem;programming language;algorithm;control flow graph	Logic	-15.995805767832833	24.25847590243306	88953
7943f7114a33dcf2e04f35da6d5d9ebc5f080788	the expressive power of valued constraints: hierarchies and collapses	effondrement;hierarchy;cost function;expressibility;fractional polymorphisms;03d55;coaccion;contrainte;constraint satisfaction;funcion coste;feasibility;expressive power;satisfaction contrainte;puissance expressive;valued constraint satisfaction;constraint;informatique theorique;polymorphism;jerarquia;feasibility polymorphisms;coste;fonction cout;polymorphisme;polimorfismo;polymorphisms;satisfaccion restriccion;collapse;hierarchie;desmoronamiento;large classes;practicabilidad;faisabilite;max closed cost functions;computer theory;cout;informatica teorica	In this paper, we investigate the ways in which a fixed collection of valued constraints can be combined to express other valued constraints. We show that in some cases, a large class of valued constraints, of all possible arities, can be expressed by using valued constraints over the same domain of a fixed finite arity. We also show that some simple classes of valued constraints, including the set of all monotonic valued constraints with finite cost values, cannot be expressed by a subset of any fixed finite arity, and hence form an infinite hierarchy.	expressive power (computer science)	David A. Cohen;Peter Jeavons;Stanislav Zivny	2008	Theor. Comput. Sci.	10.1016/j.tcs.2008.08.036	polymorphism;feasibility study;combinatorics;discrete mathematics;constraint satisfaction;input/output;computer science;mathematics;constraint;programming language;expressive power;algorithm;hierarchy;collapse	ECom	-7.308018063419335	18.980959197213746	89087
927338a5d3559a59d5c4372f2737cbfbf2859ebb	towards the use of model checking for performing data consistency evaluation and cleansing			model checking	Mario Mezzanzanica;Mirko Cesarini;Fabio Mercorio;Roberto Boselli	2012			model checking;database;data consistency;computer science	SE	-18.074497964970632	26.94610245233408	89115
cdcbc446204b9bc21041b4939d4abf31da9816fd	a multiparty multi-session logic		Recent work on the enhancement of typing techniques for multiparty sessions with logical annotations enables, not only the validation of structural properties of the conversations and on the sorts of the messages, but also properties on the actual values exchanged. However, a specification and verification of mutual effects of multiple cross-session interactions are still an open problem. We introduce a multiparty logical system with virtual states that enables the tractable specification and validation of fine-grained inter-session correctness properties of processes participating in several interleaved sessions. We present a sound and relative complete static verification method, and justify its expressiveness by giving a sound and complete embedding into Hennessy-Milner logic.	cobham's thesis;correctness (computer science);formal proof;interaction;proof (truth);proof calculus;software verification	Laura Bocchi;Romain Demangeon;Nobuko Yoshida	2012		10.1007/978-3-642-41157-1_7	computer science;theoretical computer science;distributed computing;programming language;algorithm	PL	-13.091913658968757	21.288668207527326	89134
2d3485197cbe37073a1df71718303c7ab12999c7	arrays, bounded quantification and iteration in logic and constraint logic programming	optimisation;constraint logic programs;optimizacion;bounded quantification;algoritmo recursivo;coaccion;contrainte;logical programming;universiteitsbibliotheek;resolucion problema;iteraccion;constraint;algorithme recursif;programmation logique;dynamic data structure;estructura datos;iteration;optimization;structure donnee;recursive algorithm;logic programs;programacion logica;data structure;problem solving;resolution probleme	We claim that programming within the logic programming paradigm suffers from lack of attention given to iteration and arrays. To convince the reader about their merits we present several examples of logic and constraint logic programs which use iteration and arrays instead of explicit recursion and lists. These programs are substantially simpler than their counterparts written in the conventional way. They are easier to write and to understand, are guaranteed to terminate and their declarative character makes it simpler to argue about their correctness. Iteration is implemented by means of bounded quantification.	constraint logic programming;iteration	Krzysztof R. Apt	1995		10.1016/0167-6423(95)00020-8	constraint logic programming;concurrent constraint logic programming;mathematical optimization;constraint programming;iteration;horn clause;data structure;bounded quantification;computer science;computational logic;constraint;programming language;prolog;logic programming;algorithm;recursion	Logic	-18.115210254109012	20.97683657071212	89158
9982b25befcb28a72bd84bf97ac8ebefa3df16bf	diagnosability in concurrent probabilistic systems	logic of knowledge;computational complexity;concurrent probabilistic system;diagnosis;probabilistic reasoning	Diagnosability is a key attribute of systems to enable the detection of failure events by partial observations. This paper addresses the diagnosability in concurrent probabilistic systems. Four different notions (L-, P-, A-, and AA-diagnosability) are characterised by formulas of a logic of knowledge, time and probability. Also, we investigate the computational complexities of verifying them: the L-diagnosability is NL-complete, the A-diagnosability is PTIMEcomplete, and the P-diagnosability is in PSPACE.	aa tree;algorithm;analysis of algorithms;computation;experiment;nl (complexity);nl-complete;pspace;verification and validation	Xiaowei Huang	2013			probabilistic analysis of algorithms;probabilistic ctl;computer science;theoretical computer science;medical diagnosis;probabilistic logic;computational complexity theory;probabilistic argumentation;algorithm	AI	-8.157991833705584	27.589906874143416	89231
d8b907a25ff5494790f5ba59b6dd926e163d1117	an assume/guarantee based compositional calculus for hybrid csp	hoare logic;hcsp;duration calculus;assume guarantee;compositionality;hybrid systems	HCSP (Hybrid CSP) extends CSP to describe interacting continuous and discrete dynamics. The concurrency with synchronous communications, timing constructs, interrupts, differential equations, and so on, make the behavior of HCSP difficult to specify and verify. In this paper, we propose a Hoare-style calculus for reasoning about HCSP. The calculus includes Duration Calculus formulas to record process execution history and reason about real-time properties and continuous evolution, and dedicated predicate symbols to specify communication traces and readiness of process actions in a way which enables synchronisation to be handled compositionally by using assume/guarantee reasoning. keywords: Hybrid Systems, Duration Calculus, Hoare Logic, HCSP, Compositionality, Assume/Guarantee	concurrency (computer science);deadlock;duration calculus;formal specification;hoare logic;hybrid system;interaction;interrupt;real-time clock;real-time transcription;tracing (software);verification and validation	Shuling Wang;Naijun Zhan;Dimitar P. Guelev	2012		10.1007/978-3-642-29952-0_13	duration calculus;process calculus;discrete mathematics;real-time computing;computer science;hoare logic;programming language;algorithm;principle of compositionality;hybrid system	Logic	-11.05236679968687	25.729332798398453	89351
38bbb3f578f76de92f528995d78836b8784d8d14	modular construction and partial order semantics of petri nets	partial order semantics;petri net	modular construction and partial order semantics of petri nets What to say and what to do when mostly your friends love reading? Are you the one that don't have such hobby? So, it's important for you to start having that hobby. You know, reading is not the force. We're sure that reading will lead you to join in better concept of life. Reading will be a positive activity to do every time. And do you know our friends become fans of modular construction and partial order semantics of petri nets as the best book to read? Yeah, it's neither an obligation nor order. It is the referred book that will not make you feel disappointed.	modular design;petri net;powerset construction	Walter Vogler	1992		10.1007/3-540-55767-9	discrete mathematics;stochastic petri net;theoretical computer science;mathematics;process architecture;operational semantics;petri net;denotational semantics;algorithm	Logic	-5.140389808620369	22.723931904873844	89490
5336eca7845f4d16cb7a42910a45be0d75b0307c	termination of narrowing revisited	reachability;search space;confluencia;resolution math;term rewriting systems;confluence;term rewrite system;unification;reachability problems;borne electrique;informatique theorique;asequibilidad;borne electrico;systeme reecriture terme;termination;atteignabilite;resolucion matematica;narrowing;terminaison;solving;equational unification;unificacion;computer theory;informatica teorica	This paper describes several classes of term rewriting systems (TRS’s) where narrowing has a finite search space and is still (strongly) complete as a mechanism for solving reachability goals. These classes do not assume confluence of the TRS. We also ascertain purely syntactic criteria that suffice to ensure the termination of narrowing and include several subclasses of popular TRS’s such as right-linear TRS’s, almost orthogonal TRS’s, topmost TRS’s, and left-flat TRS’s. Our results improve and/or generalize previous criteria in the literature regarding narrowing termination.	confluence;np-completeness;reachability;rewriting	María Alpuente;Santiago Escobar;José Iborra	2009	Theor. Comput. Sci.	10.1016/j.tcs.2009.07.037	combinatorics;discrete mathematics;computer science;unification;mathematics;programming language;reachability;confluence;algorithm	AI	-8.351494616076316	19.611671901336862	89527
78d7a1f04fac63f731bd5793668c23535f6bdc5b	model checking a logic over systems with regular sets of processes		ough systems with process creation give rise to unboundedly many processes, their names are systematically generated and typically form a regular set. When we consider modal logics to specify properties of such systems, it is natural to consider quantication over such regular sets. ese are in the realm of term modal logics, which are usually undecidable. We consider themonodic variant, in which there is only one free variable in the scope of any modality, and present a model checking algorithm for this logic.	algorithm;dynamic logic (modal logic);exptime;free variables and bound variables;modal logic;model checking;solid modeling;term algebra;time complexity;undecidable problem;verification and validation	MS Padmanabha AnanthaPadmanabha;R Ramanujam	2017			discrete mathematics;model checking;mathematics	Logic	-11.164500131137634	22.389782670332316	89651
941139b8ef1d0fb4167ddf1b905ef6a2ce32180a	a generic technique for synthesizing bounded finite-state controllers	generalized planning;synthesis;finite state controllers	Finite-state controllers are a compact and effective plan representation for agent widely used in AI. In this paper, we propose a generic framework and related solver for synthesizing bounded finite-state controllers, and show its instantiations to three different applications, including generalized planning, planning programs and service composition under partial observability and controllability. We show that our generic solver is sound and complete, and amenable to heuristics that take into account the structure of the specific target instantiation. Experiments show that instantiations of our solver to the problems above often outperform tailored approaches in the literature. This suggests that our proposal is a promising base point for future research on finite-state controller synthesis.	heuristic (computer science);home automation;nondeterministic finite automaton;service composability principle;solver;universal instantiation;web service	Yuxiao Hu;Giuseppe De Giacomo	2013			mathematical optimization;discrete mathematics;mathematics	AI	-13.831777899529452	26.555436450897155	89795
2b3cb169d24962cbe3a7f48cfb93bca11ae76264	generation of implied constraints for automaton-induced decompositions	implied constraints;invariants;automata;generalised arc consistency;constraint programming;datavetenskap datalogi;automata theory;constraint handling;computer science;global constraints	Automata, possibly with counters, allow many constraints to be expressed in a simple and high-level way. An automaton induces a decomposition into a conjunction of already implemented constraints. Generalised arc consistency is not generally maintained on decompositions induced by counter automata with more than one state or counter. To improve propagation of automaton-induced constraint decompositions, we use automated tools to derive loop invariants from the constraint checker corresponding to the given automaton. These loop invariants correspond to implied constraints, which can be added to the decomposition. We consider two global constraints and derive implied constraints to improve propagation even to the point of maintaining generalised arc consistency.	automated reasoning;automaton;cognitive dimensions of notations;constraint (mathematics);constraint graph;database;experiment;global optimization;graph property;high- and low-level;invariant (computer science);local consistency;loop invariant;program counter;propagator;reasoning system;ring counter;sicstus prolog;sanity check;software propagation;solver	María Andreína Francisco Rodríguez;Pierre Flener;Justin Pearson	2013	2013 IEEE 25th International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2013.160	mathematical optimization;constraint programming;combinatorics;discrete mathematics;computer science;artificial intelligence;invariant;automata theory;mathematics;automaton;constraint;local consistency	Robotics	-15.03154140755135	23.239463611841504	89814
92cc3153e98b972d18bb0529e19eddc30c9cd4ec	modular properties of composable term rewriting systems	term rewrite system;rewrite systems	In this paper we prove several new modularity results for unconditional and conditional term rewriting systems. Most of the known modularity results for the former systems hold for disjoint or constructor-sharing combinations. Here we focus on a more general kind of combination: so-called composable systems. As far as conditional term rewriting systems are concerned, all known modularity result but one apply only to disjoint systems. Here we investigate conditional systems which may share constructors. Furthermore , we refute a conjecture of Middeldorp (1990, 1993).	rewriting	Enno Ohlebusch	1994		10.1006/jsco.1995.1036	discrete mathematics;theoretical computer science;mathematics;confluence;algorithm	Logic	-10.358792485643043	19.698092799457974	89845
bb7349eaa775df5532fd34088b8a1d96cd68d79e	redundancy based controller reconfiguration for fault recovery of manufacturing systems	reconfiguration;manufacturing systems;structure methods;discrete manufacturing systems;system modeling;recovery;regulation control;redundancy based controller reconfiguration;redundancy control systems manufacturing systems petri nets error correction control theory manufacturing automation usa councils fault detection production systems;petri nets;algebraic technique;recovery petri nets reconfiguration regulation control manufacturing systems;petri net;manufacturing system;petri nets manufacturing systems;fault recovery;algebraic technique redundancy based controller reconfiguration fault recovery manufacturing systems discrete manufacturing systems petri nets	This work deals with fault recovery of discrete manufacturing systems modeled by Petri nets (PN). A technique for fault recovery that exploits the redundancies included in the PN model is proposed. This work presents two main contributions; the first one is a structural method for finding out the redundancies in the PN system model. Based on these redundancies, the second contribution is an algebraic technique used to reconfigure partially the controller when permanent faults occur in system components. This approach avoids the computation of a new controller every time a fault is detected.	algorithm;computation;discrete manufacturing;fault tolerance;online and offline;petri net;polynomial	Mildreth Alcaraz-Mejia;Ernesto López-Mellado;Antonio Ramírez-Treviño	2007	2007 IEEE International Conference on Automation Science and Engineering	10.1109/COASE.2007.4341816	control engineering;real-time computing;engineering;operations management	Robotics	-5.628629465407947	28.849966489763442	90079
1c7f2e3483d68dbf5bac491f66e584cfc6d5dd97	on the axiomatizability of impossible futures: preorder versus equivalence	inequational theory;finite element methods;inequational theory process algebra bccsp;equations computational modeling algebra logic functions computer science concurrent computing concrete system recovery testing carbon capture and storage;process algebra bccsp;computational modeling;algebra;transforms;mathematical model;process algebra;concurrent process;reactive power	We investigate the (in)equational theory of impossible futures semantics over the process algebra BCCSP. We prove that no finite, sound axiomatization for BCCSP modulo impossible futures equivalence is ground-complete. By contrast, we present a finite, sound, ground-complete axiomatization for BCCSP modulo impossible futures preorder. If the alphabet of actions is infinite, then this axiomatization is shown to be omega-complete. If the alphabet is finite, we prove that the in equational theory of BCCSP modulo impossible futures preorder lacks such a finite basis. We also derive non-finite axiomatizability results for nested impossible futures semantics.	axiomatic system;chaitin's constant;futures and promises;modulo operation;process calculus;turing completeness	Taolue Chen;Wan Fokkink	2008	2008 23rd Annual IEEE Symposium on Logic in Computer Science	10.1109/LICS.2008.13	process calculus;discrete mathematics;computer science;finite element method;mathematical model;mathematics;ac power;programming language;computational model;algorithm;algebra	Logic	-9.607290547727812	21.677435002466243	90187
d1c6d75c3bcf4dcc73c8048df152999dae8dbf49	concolic testing for deep neural networks		Concolic testing combines program execution and symbolic analysis to explore the execution paths of a software program. In this paper, we develop the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we utilise quantified linear arithmetic over rationals to express test requirements that have been studied in the literature, and then develop a coherent method to perform concolic testing with the aim of better coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples.		Youcheng Sun;Min Wu;Wenjie Ruan;Xiaowei Huang;Marta Z. Kwiatkowska;Daniel Kroening	2018		10.1145/3238147.3238172	computer science;theoretical computer science;software;concolic testing;artificial neural network;symbolic execution;symbolic data analysis	SE	-16.768081152632828	29.559818236465315	90225
d9d7026e9a946d29285e1cd0c0096b60222cf4de	a unifying view of abstract domain design	evaluation performance;performance evaluation;static method;evaluacion prestacion;semantics;abstract data type;semantica;semantique;analisis programa;complecion;type abstrait;tipo abstracto;program analysis;metodo estatico;analyse programme;completion;methode statique	ions can be compared with each other with respect to their precision of representation. This order corresponds in the most natural way to the usual functional pointwise order between closures (denoted by v), which makes the set uco(C) of all closure operators on C (or equivalently the set of all abstract interpretations of C) a complete lattice. We introduce the notion of domain refinement as a general scheme in order to formalize enhancing operators on abstract domains. A domain refinement is a mapping R: uco(C) 3 uco(C) such that for any abstract domain A: R(A) contains more information than A (i.e., R(A) v A), and R monotonically depends on the information contained in its argument, namely, it is monotonic; a last very reasonable requirement is that R upgrades all at once, namely, R is idempotent. This clearly defines refinements as lower closure operators on uco(C). A natural question that arises in this setting is whether it is possible to define the inverse of a domain refinement. For a given refinement R, the inverse R (if it exists) is a function mapping any domain A into the (unique) most abstract domain R(A) such that R(R (A)) 5 R(A). Whenever this happens, we say that R(A) is the optimal basis for the domain A and refinement R. The intuition is that the refined domain R(A) can be systematically reconstructed by applying the refinement R to the more abstract, and therefore simpler, domain R(A). This domain R(A), is, in fact, the most abstract domain for which this condition holds. Not all domain refinements are invertible. A simple and meaningful example is provided by the following notion of completion by complements: a refinement R¬ which, under certain hypotheses, upgrades a given abstract domain by adding denotations for the lattice-theoretic complements of its elements. If Sign, A1, and A2 in Figure 1 are the domains for sign analysis of the integer variables depicted, with their obvious meanings as upper closures on the subsets of integers `(Z), ordered by inclusion, then R¬(A1) 5 R¬(A2) 5 Sign, but the common abstraction of A1 and A2, denoted A1 t A2 , is {Z, 0y} and R¬({Z, 0y}) 5 {Z, 0y} Þ Sign. Thus R¬ does not exist.	abstract interpretation;closure (computer programming);emoticon;idempotence;refinement (computing);theory	Gilberto Filé;Roberto Giacobazzi;Francesco Ranzato	1996	ACM Comput. Surv.	10.1145/234528.234742	program analysis;completion;computer science;artificial intelligence;semantics;programming language;abstract data type;algorithm	Logic	-14.386660401506562	18.413470516762015	90579
00fb6127748ec5d0977a8e063cf234f36f3d5e59	bounded variable logic, parameterized logarithmic space, and savitch's theorem		We study the parameterized space complexity of model-checking first-order logic with a bounded number of variables. By restricting the number of the quantifier alternations we obtain problems complete for a natural hierarchy between parameterized logarithmic space and FPT. We call this hierarchy the tree hierarchy, provide a machine characterization, and link it to the recently introduced classes PATH and TREE. We show that the lowest class PATH collapses to parameterized logarithmic space only if Savitch’s theorem can be improved. Finally, we settle the complexity with respect to the tree-hierarchy of finding short undirected paths and small undirected trees.	dspace;first-order logic;first-order predicate;graph (discrete mathematics);l (complexity);model checking;nl (complexity);parameterized complexity;quantifier (logic);savitch's theorem	Yijia Chen;Moritz Müller	2014		10.1007/978-3-662-44522-8_16	savitch's theorem;combinatorics;mathematical analysis;discrete mathematics;mathematics;bounded inverse theorem	Theory	-5.28127853316999	20.384592609358066	90685
6253ece18330d5a80662e52f29e2646c5c5d963d	termination proofs for linear simple loops	disjunctive ranking relations;linear simple loops;termination;static analysis	Analysis of termination and other liveness properties of a program can be reduced to termination proof synthesis for simple loops, i.e., loops with only variable updates in the loop body. Among simple loops, the subset of linear simple loops (LSLs) is particularly interesting because it is common in practice and expressive in theory. Existing techniques can successfully synthesize a linear ranking function for an LSL if there exists one. However, when a terminating LSL does not have a linear ranking function, these techniques fail. In this paper, we describe an automatic method that generates proofs of (universal) termination for LSLs based on the synthesis of disjunctive ranking relations. The method repeatedly finds linear ranking functions on partitions of the state space and checks whether the transitive closure of the transition relation is included in the union of the ranking relations. Our method extends the work of Podelski and Rybalchenko (A complete method for the synthesis of linear ranking functions. In: Proceedings of the 5th international conference on VMCAI, Jan 2004, Venice, Italy, pp 239–251, 2004). We have implemented a prototype of the method and have shown experimental evidence of the effectiveness of our method.	approximation;convex function;disjunctive normal form;experiment;linear logic;liveness;newman's lemma;prototype;ranking (information retrieval);state space;termination analysis;test suite;transitive closure	Hong Yi Chen;Shaked Flur;Supratik Mukhopadhyay	2013	International Journal on Software Tools for Technology Transfer	10.1007/s10009-013-0288-8	computer science;static analysis;algorithm	Logic	-16.782355019571515	25.16266736043445	90713
9192dba67355ffc8ba6f7d91cfd23abaec5db0ad	an algebraic framework for compositional program analysis		The purpose of a program analysis is to compute an abstract meaning for a program which approximates its dynamic behaviour. A compositionalprogram analysis accomplishes this task with a divide-and-conquer strategy: the meaning of a program is co mputed by dividing it into sub-programs, computing their mea ning, and then combining the results. Compositional program anal yses are desirable because they can yield scalable (and easily pa rallelizable) program analyses. This paper presents algebraic framework for designing, imp lementing, and proving the correctness of compositional prog ram analyses. A program analysis in our framework defined by an al gebraic structure equipped with sequencing , choice, and iteration operations. From the analysis designperspective, a particularly interesting consequence of this is that the meaning of a loop is computed by applying the iteration operator to the loop body. Th is style of compositional loop analysis can yield interesting ways o f computing loop invariants that cannot be defined iteratively. W e identify a class of algorithms, the so-called path-expressionalgorithms [35, 37], which can be used to efficiently implement analyses in our framework. Lastly, we develop a theory for p oving the correctnessof an analysis by establishing an approximation relationsh ip between an algebra defining a concrete semantics and an algeb ra defining an analysis.	algorithm;approximation;computer programming;comstock–needham system;correctness (computer science);iteration;linear algebra;loop invariant;mesh analysis;program analysis;random-access memory;scalability;scarab of ra	Azadeh Farzan;Zachary Kincaid	2013	CoRR		program analysis;computer science;theoretical computer science;programming language;algorithm	PL	-17.79181483413102	23.90826599856296	90890
704423c2d249767a0d5be11bc491329fa492a4fb	logic programming in a fragment of intuitionistic temporal linear logic	logique lineaire;lenguaje programacion;time dependent;logica temporal;programming language;temporal logic;logical programming;logica lineal;programmation logique;intuitionistic logic;logique intuitionniste;langage programmation;logic programs;programacion logica;linear logic;data structure;resource management system;logique temporelle;logica intuicionista	Recent development of logic programming languages based on linear logic suggests a successful direction to extend logic programming to be more expressive and more efficient. The treatment of formulasas-resources gives us not only powerful expressiveness, but also efficient access to a large set of data. However, in linear logic, whole resources are kept in one context, and there is no straight way to represent complex data structures as resources. For example, in order to represent an ordered list and time-dependent data, we need to put additional indices for each resource formula. This paper describes a logic programming language, called TLLP, based on intuitionistic temporal linear logic. This logic, an extension of linear logic with some features from temporal logics, allows the use of the modal operators ‘©’(next-time) and ‘2’(always) in addition to the operators used in intuitionistic linear logic. The intuitive meaning of modal operators is as follows: ©B means that B can be used exactly once at the next moment in time; 2 B means that B can be used exactly once any time; ! B means that B can be used arbitrarily many times (including 0 times) at any time. We first give a proof theoretic formulation of the logic of the TLLP language. We then present a series of resource management systems designed to implement not only interpreters but also compilers based on an extension of the standard WAM model.	compiler;data structure;intuitionistic logic;learning relationship management;linear logic;logic programming;modal operator;programming language;prototype;theory;warren abstract machine	Mutsunori Banbara;Kyoung-Sun Kang;Takaharu Hirai;Naoyuki Tamura	2001		10.1007/3-540-45635-X_29	predicate logic;dynamic logic;zeroth-order logic;linear logic;discrete mathematics;linear temporal logic;description logic;higher-order logic;horn clause;data structure;temporal logic;many-valued logic;computation tree logic;intuitionistic logic;computer science;intermediate logic;artificial intelligence;bunched logic;computational logic;mathematics;programming language;logic programming;substructural logic;multimodal logic;algorithm	Logic	-17.71380785789537	20.70185013790803	90959
0b84fb0ec9739e04f9b0fcbe040718d9f735200f	cartesian hoare logic for verifying k-safety properties	relational hoare logic;safety hyper properties;automated verification;product programs	Unlike safety properties which require the absence of a “bad” program trace, k-safety properties stipulate the absence of a “bad” interaction between k traces. Examples of k-safety properties include transitivity, associativity, anti-symmetry, and monotonicity. This paper presents a sound and relatively complete calculus, called Cartesian Hoare Logic (CHL), for verifying k-safety properties. We also present an automated verification algorithm based on CHL and implement it in a tool called DESCARTES. We use DESCARTES to analyze user-defined relational operators in Java and demonstrate that DESCARTES is effective at verifying (or finding violations of) multiple k-safety properties.	algorithm;cartesian closed category;hoare logic;java;model checking;operator associativity;proof assistant;relational operator;time complexity;tracing (software);verification and validation;vertex-transitive graph	Marcelo Sousa;Isil Dillig	2016		10.1145/2908080.2908092	separation logic;theoretical computer science;hoare logic;programming language;algorithm	PL	-17.827292222860255	25.55483119447624	90999
8f84d74cc37e57dfa74e5db552404b59ff9c5949	what is decidable about perfect timed channels?		In this paper, we introduce the model of communicating timed automata (CTA) that extends the classical models of finite-state processes communicating through FIFO perfect channels and timed automata, in the sense that the finite-state processes are replaced by timed automata, and messages inside the perfect channels are equipped with clocks representing their ages. In addition to the standard operations (resetting clocks, checking guards of clocks) each automaton can either (1) append a message to the tail of a channel with an initial age or (2) receive the message at the head of a channel if its age satisfies a set of given constraints. In this paper, we show that the reachability problem is undecidable even in the case of two timed automata connected by one unidirectional timed channel if one allows global clocks (that the two automata can check and manipulate). We prove that this undecidability still holds even for CTA consisting of three timed automata and two unidirectional timed channels (and without any global clock). However, the reachability problem becomes decidable in the case of two automata linked with one unidirectional timed channel and with no global clock. Finally, we consider the bounded-context case, where in each context, only one timed automaton is allowed to receive messages from one channel while being able to send messages to all the other timed channels. In this case we show that the reachability problem is decidable.	append;fifo (computing and electronics);reachability problem;timed automaton	Parosh Aziz Abdulla;Mohamed Faouzi Atig;Shankara Narayanan Krishna	2017	CoRR			Logic	-10.671592727666006	25.11461780308182	91006
5067471b1b29a392e50200b558c21cc4964cb7b1	termination proof of term rewriting system with the multiset path ordering. a complete development in the system coq	term rewrite system;calculus of constructions	We propose a constructive termination proof in the Calculus of Constructions of any finite term rewriting systems whose rules can be oriented by the multiset path ordering. We propose a new proof which consists in an embedding of the rewrite relation into the standard ordering over natural numbers. Then, we show how to derive automatically constructive well-foundedness proofs of rewrite relations. This work has been completely formalised in the Coq system. Such a mechanization is not useless since there was some nontrivial mistakes in the previous proofs of Cichon and Weiermann. Furthermore this kind of development reflects the ability to formalise in Coq parts of nontrivial mathematics.	coq (software);path ordering (term rewriting);rewriting;termination analysis	François Leclerc	1995		10.1007/BFb0014061	calculus of constructions;computer science;normalization property;programming language;algorithm	Logic	-13.348820328790213	19.347816827678727	91284
447a2c963895e15bd843a7cd62392bc08d70021d	modeling, analyzing and slicing periodic distributed computations	recurrent computation;predicate detection;liveness violation;d diagram	The earlier work on predicate detection has assumed that the given computation is finite. Detecting violation of a liveness predicate requires that the predicate be evaluated on an infinite computation. In this work, we develop the theory and associated algorithms for predicate detection in infinite runs. In practice, an infinite run can be determined in finite time if it consists of a recurrent behavior with some finite prefix. Therefore, our study is restricted to such runs. We introduce the concept of d-diagram, which is a finite representation of infinite directed graphs. Given a d-diagram that represents an infinite distributed computation, we solve the problem of determining if a global predicate ever became true in the computation. The crucial aspect of this problem is the stopping rule that tells us when to conclude that the predicate can never become true in future. We also provide an algorithm to provide vector timestamps to events in the computation for determining the dependency relationship between any two events in the infinite run. Finally, we give an algorithm to compute a slice of a d-diagram which concisely captures all the consistent global states of the computation satisfying the given predicate.	advanced configuration and power interface;algorithm;computation;diagram;directed graph;distributed computing;liveness;newman's lemma;recurrent neural network;sensor	Vijay K. Garg;Anurag Agarwal;Vinit A. Ogale	2014	Inf. Comput.	10.1016/j.ic.2013.11.002	functional predicate;discrete mathematics;theoretical computer science;hard-core predicate;mathematics;predicate;algorithm	Logic	-13.125660637998845	28.92489176486232	91450
4bd8002b1a678bc9d8329f449a59e50b9b4ab9e5	efficient decision procedure for bounded integer non-linear operations using smt()	software verification;controlled experiment;linear constraint;decision problem;linear operator;decision procedure	For the verification of complex designs, one often needs to solve decision problems containing integer non-linear constraints. Due to the undecidability of the problem, one usually considers bounded integers and then either linearizes the problem into a SMT($\mathcal{LIA}$) problem (i.e., the theory of linear integer arithmetic with Boolean constraints) or bit-blasts into a SAT problem. We present a novel way of linearizing those constraints, and then show how the proposed encoding to a SMT($\mathcal{LIA}$) problem can be integrated into an incremental lazy bounding and refinement procedure ( LBR  ) that leverages on the success of the state-of-the-art SMT($\mathcal{LIA}$) solvers. The most important feature of our  LBR  procedure is that the formula need not be re-encoded at every step of the procedure but rather, only bounds on variables need to be asserted/retracted, which are very efficiently supported by the recent SMT($\mathcal{LIA}$) solvers. In a series of controlled experiments, we show the effectiveness of our linearization encoding and  LBR  procedure in reducing the SMT solve time. We observe similar effectiveness of  LBR  procedure when used in a software verification framework applied on industry benchmarks.	decision problem	Malay K. Ganai	2008		10.1007/978-3-642-01702-5_11	mathematical optimization;software verification;computer science;decision problem;linear map;programming language;algorithm	Logic	-14.648077734585447	25.930061721005774	91533
ab9a0d0c8e762a760569192b237b3073b9c8aa34	space-time interpolants		Reachability analysis is difficult for hybrid automata with affine differential equations, because the reach set needs to be approximated. Promising abstraction techniques usually employ interval methods or template polyhedra. Interval methods account for dense time and guarantee soundness, and there are interval-based tools that overapproximate affine flowpipes. But interval methods impose bounded and rigid shapes, which make refinement expensive and fixpoint detection difficult. Template polyhedra, on the other hand, can be adapted flexibly and can be unbounded, but sound template refinement for unbounded reachability analysis has been implemented only for systems with piecewise constant dynamics. We capitalize on the advantages of both techniques, combining interval arithmetic and template polyhedra, using the former to abstract time and the latter to abstract space. During a CEGAR loop, whenever a spurious error trajectory is found, we compute additional space constraints and split time intervals, and use these space-time interpolants to eliminate the counterexample. Space-time interpolation offers a lazy, flexible framework for increasing precision while guaranteeing soundness, both for error avoidance and fixpoint detection. To the best of out knowledge, this is the first abstraction refinement scheme for the reachability analysis over unbounded and dense time of affine hybrid systems, which is both sound and automatic. We demonstrate the effectiveness of our algorithm with several benchmark examples, which cannot be handled by other tools.	approximation algorithm;automata theory;benchmark (computing);complexity;experiment;fixed point (mathematics);hybrid automaton;hybrid system;interpolation;interval arithmetic;iteration;lazy evaluation;nondeterministic algorithm;nonlinear system;polyhedron;reachability;refinement (computing);scalability;subdivision surface;template metaprogramming	Goran Frehse;Mirco Giacobbe;Thomas A. Henzinger	2018		10.1007/978-3-319-96145-3_25	mathematical optimization;theoretical computer science;hybrid system;computer science;bounded function;interval arithmetic;reachability;piecewise;fixed point;affine transformation;soundness	Logic	-15.408723925236309	26.131990417282648	91568
b47c7bf7e0761c21b2aaa6b88503986d5206fcfa	domain compression for complete abstractions	abstract model checking;abstract domains;verification modele;program verification;analisis programa;verificacion programa;model checking;completitud;program analysis;completeness;interpretation abstraite;analyse programme;predicate abstraction;abstract interpretation;verification programme;completude;domain refinement	Transform domains in order to make them satisfy a property P. P doesn't hold [Giacobazzi et al. 2000] Left adjoint S(A)=S(C(A)) Base of A Simplification: E(A) A P doesn't hold P doesn't hold P holds: Core of A Transform domains in order to make them satisfy a property P. A P holds: Shell of A P doesn't hold [Giacobazzi et al. 2000] Left adjoint S(A)=S(C(A)) Base of A Simplification: E(A) A P doesn't hold P holds: Core of A P doesn't hold Transform domains in order to make them satisfy a property P. A P holds: Shell of A P doesn't hold Right adjoint S(A)=S(B(A)) Base of A	p (complexity);text simplification	Roberto Giacobazzi;Isabella Mastroeni	2003		10.1007/3-540-36384-X_14	program analysis;model checking;domain;completeness;computer science;domain model;programming language;algorithm	Logic	-16.71396113068354	26.59747084111488	91710
41d5e8786b5138cae173123331450a0e66da3f42	reasoning in the bernays-schoenfinkel-ramsey fragment of separation logic		Separation Logic ( SL) is a well-known assertion language used in Hoare-style modular proof systems for programs with dynami c lly allocated data structures. In this paper we investigate the fragment of firs t-o derSL restricted to the Bernays-Schönfinkel-Ramsey quantifier prefix ∃∗∀∗, where the quantified variables range over the set of memory locations. When this s et is uninterpreted (has no associated theory) the fragment is PSPACE-complete , which matches the complexity of the quantifier-free fragment [7]. However, SL becomes undecidable when the quantifier prefix belongs to ∃∗∀∗∃∗ instead, or when the memory locations are interpreted as integers with linear arithmet ic constraints, thus setting a sharp boundary for decidability within SL. We have implemented a decision procedure for the decidable fragment of ∃∗∀∗SL as a specialized solver inside a DPLL(T) architecture, within the CVC4 SMT solver. The evaluation o f our implementation was carried out using two sets of verification c ditions, produced by(i) unfolding inductive predicates, and (ii) a weakest pr econdition-based verification condition generator. Experimental data shows that automated quantifier instantiation has little overhead, compared to manual mode l-based instantiation.	assertion (software development);bernays–schönfinkel class;data structure;decision problem;hoare logic;inductive reasoning;overhead (computing);pspace;pspace-complete;quantifier (logic);sl (complexity);satisfiability modulo theories;separation logic;solver;undecidable problem;unfolding (dsp implementation);universal instantiation;universal quantification;verification condition generator	Andrew Reynolds;Radu Iosif;Cristina Serban	2017		10.1007/978-3-319-52234-0_25	discrete mathematics;theoretical computer science;mathematics;programming language;algorithm	Logic	-16.411676781402956	23.07289616635336	91825
0897167ff5b7e5828a2600f92e73b3065d727fcd	leveraging abstraction to establish out-of-nominal safety properties		Digital systems in an out-of-nominal environment (e.g., one causing hardware bit flips) may not be expected to function correctly in all respects but may be required to fail safely. We present an approach for understanding and verifying a system’s out-of-nominal behavior as an abstraction of nominal behavior that preserves designated critical safety requirements. Because abstraction and refinement are already widely used for improved tractability in formal design and proof techniques, this additional way of viewing an abstraction can potentially verify a system’s out-of-nominal safety with little additional work. We illustrate the approach with a simple model of a turnstile controller with possible logic faults (formalized in the temporal logic of actions and NuSMV), noting how design choices can be guided by the desired out-of-nominal abstraction. Principles of robustness in complex systems (specifically, Boolean networks) are found to be compatible with the formal abstraction approach. This work indicates a direction for broader use of formal methods in safety-critical systems.		Jackson Mayo;Robert C. Armstrong;Geoffrey C. Hulette	2015		10.1007/978-3-319-29510-7_10	reliability engineering;data mining;computer security	Logic	-14.75294324300664	27.582049032084953	92041
0a2f77f70add8d4534ee6677e52e902f98bd96d6	verification of spatial and temporal modalities in biochemical systems	spatial and temporal modalities;open access;biochemical systems;linear logic	Biochemical systems such as metabolic and signaling pathways tend to be arranged in a physical space: the product of one reaction must be in the right place to become the reactant for the subsequent reaction in the pathway. Moreover, in some cases, the behavior of the systems can depend on both, the location of the reactants as well as on the time needed for the reaction to occur. We address the problem of specifying and verifying properties of biochemical systems that exhibit both temporal and spatial modalities at the same time. For that, we use as specification language a fragment of intuitionistic linear logic with subexponentials (SELL). The subexponential signature allows us to capture the spatial relations among the different components of the system and the timed constraints for reactions to occur. We show that our framework is general enough to give a declarative semantics to P-Systems and we show that such logical characterization has a strong level of adequacy. Hence, derivations in SELL follow exactly the behavior of the modeled system.	formal specification;gene regulatory network;linear logic;p system;specification language;verification and validation	Davide Chiarugi;Moreno Falaschi;Diana Hermith;Carlos Olarte	2015	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2015.06.009	linear logic;discrete mathematics;computer science;mathematics;programming language;algorithm	Logic	-10.37036895851642	22.411836866392857	92112
56c70c7c989221dcd2534771d84adf8b269c50bb	parallel sat solving in bounded model checking	bounded model checking	Bounded model checking (BMC) is an incremental refutation technique to search for counterexamples of increasing length. The existence of a counterexample of a fixed length is expressed by a first-order logic formula that is checked for satisfiability using a suitable solver. We apply communicating parallel solvers to check satisfiability of the BMC formulae. In contrast to other parallel solving techniques, our method does not parallelize the satisfiability check of a single formula, but the parallel solvers work on formulae for different counterexample lengths. We adapt the method of constraint sharing and replication of Shtrichman, originally developed for sequential BMC, to the parallel setting. Since the learning mechanism is now parallelized, it is not obvious whether there is a benefit from the concepts of Shtrichman in the parallel setting. We demonstrate on a number of benchmarks that adequate communication between the parallel solvers yields the desired results.	boolean satisfiability problem;model checking	Erika Ábrahám;Tobias Schubert;Bernd Becker;Martin Fränzle;Christian Herde	2011	J. Log. Comput.	10.1093/logcom/exp002	combinatorics;discrete mathematics;computer science;mathematics;algorithm	Logic	-11.821164840923121	20.92714976742462	92431
4b013abe3886eb2ac9b1564e2bb41f0114a4384f	incremental inductive verification of parameterized timed systems	verification;ic3;networks of timed automata;pdr;parameterized timed systems	We propose and extend an approach for the verification of safety properties for parameterized timed systems modeled as networks of timed automata. For this task, we introduce an incremental workflow that is based on our algorithm IC3 with Zones. It proceeds in a cycle in which single models of the system are verified, and the verification results are employed for the reasoning about the entire system. Starting with the smallest instances, the verification of the safety property is carried out fast and efficient. On successful verification, the algorithm produces an inductive strengthening of the safety property. We reuse this result and try to reason about the entire parameterized timed system. To this end, we extrapolate the inductive strengthening into a candidate for the next-larger model. In case this candidate is a valid inductive strengthening for the next larger model, our main theorem reasons about all models of the parameterized timed system, stating that the safety property holds true for all models. Otherwise, the main cycle starts over with the verification of the next larger model. This workflow is iterated indefinitely, until able to reason about the entire parameterized timed system, until a counterexample trace is found, or until the single models become too large to be handled in the verification. We reuse the intermediate results in a Feedback-loop in order to accelerate the verification runs for the single models. Furthermore, we consider an extended formalism in comparison to our previous publications.	algorithm;automata theory;experiment;extrapolation;feedback;inductive reasoning;iteration;semantics (computer science);timed automaton;type system;utility	Tobias Isenberg	2015	2015 15th International Conference on Application of Concurrency to System Design	10.1145/2984640	real-time computing;verification;computer science;theoretical computer science;algorithm;functional verification	Logic	-14.235138873034117	26.824049388046287	92518
0afe8331aba3b2442809810becf4481e681658bf	discovering applications of higher order functions through proof planning	programa;automatic;compilateur;algorithmique;program;paralelisacion;automatico;standard ml;compiler;functional programming;theorem proving;demonstration theoreme;planificacion;algorithmics;algoritmica;informatique theorique;parallelisation;algorithmic skeletons;parallelization;programme;automatique;proof search;planning;higher order functions;planification;demostracion teorema;automated theorem proving;compilador;computer theory;informatica teorica	The close association between higher order functions (HOFs) and algorithmic skeletons is a promising source of automatic parallelisation of programs. A theorem proving approach to discovering HOFs in functional programs is presented. Our starting point is proof planning, an automated theorem proving technique in which high-level proof plans are used to guide proof search. We use proof planning to identify provably correct transformation rules that introduce HOFs. The approach has been implemented in the λ Clam proof planner and tested on a range of examples. The work was conducted within the context of a parallelising compiler for Standard ML.	algorithmic skeleton;automated theorem proving;compiler;correctness (computer science);high- and low-level;parallel computing;standard ml	Andrew Cook;Andrew Ireland;Greg J. Michaelson;Norman Scaife	2004	Formal Aspects of Computing	10.1007/s00165-004-0054-5	computer-assisted proof;computer science;theoretical computer science;mathematics;automated theorem proving;programming language;functional programming;algorithmics;proof complexity;algorithm	PL	-18.70439123248578	24.131308574701876	92732
56d3bd3e5607c40536a02a55732d07c036beac52	mobile petri nets	computacion informatica;ciencias basicas y experimentales;matematicas;grupo a;petri net	We add mobility to Place-Transition Petri Nets: tokens are names for places, and an input token of a transition can be used in its postset to specify a destination. Mobile Petri Nets are then further extended to Dynamic Nets, by adding the possibility of creating new nets during the firing of a transition. In this way, starting from Petri Nets, we define a simple hierarchy of nets with increasing degrees of dynamicity. For each class in this hierarchy we provide its encoding in the former class. Our work has been largely inspired by the join-calculus of Fournet and Gonthier, that turns out to be a (well motivated) particular case of Dynamic Petri Nets. The main difference is that, in the preset of a transition, we allow for both non-linear patterns (name unification) and (locally) free names for input places (i.e. we remove the locality constraint, and preserve reflexion).	color;concurrency (computer science);distributed computing;join-calculus;locality of reference;machine code;nonlinear system;petri net;recursion;recursive definition;semantics (computer science);specification language;unification (computer science)	Andrea Asperti;Nadia Busi	2009	Mathematical Structures in Computer Science	10.1017/S0960129509990193	discrete mathematics;stochastic petri net;computer science;artificial intelligence;process architecture;petri net;algorithm	Logic	-10.87595503357458	22.534158770082733	92828
07088f5de3a523b07b1fc1fe68db18f55710416b	modelling, reduction and analysis of markov automata	petri net-like formalisms;effective analysis;efficient ma modelling;continuous-time compositional modelling formalism;analysis tool;markov automaton;generalised stochastic petri nets;quantitative analysis;expressive power;static analysis	Markov automata (MA) constitute an expressive continuoustime compositional modelling formalism. They appear as semantic backbones for engineering frameworks including dynamic fault trees, Generalised Stochastic Petri Nets, and AADL. Their expressive power has thus far precluded them from effective analysis by probabilistic (and statistical) model checkers, stochastic game solvers, or analysis tools for Petri net-like formalisms. This paper presents the foundations and underlying algorithms for efficient MA modelling, reduction using static analysis, and most importantly, quantitative analysis. We also discuss implementation pragmatics of supporting tools and present several case studies demonstrating feasibility and usability of MA in practice.	algorithm;automata theory;automaton;average-case complexity;correctness (computer science);expressive power (computer science);fault tree analysis;markov chain;model checking;petri net;reachability;scalability;semantics (computer science);static program analysis;statistical model;timed event system;toolchain;usability;user interface	Dennis Guck;Hassan Hatefi;Holger Hermanns;Joost-Pieter Katoen;Mark Timmer	2013		10.1007/978-3-642-40196-1_5	computer science;theoretical computer science;programming language;algorithm	Logic	-11.18049219512269	28.25740046900392	92914
efeb0caad68e7293d40163be6a49a10a3d43ee66	an introduction to function rank	apl implementation;rank operator;language design;sharp apl;simple introduction;function rank	This paper gives a simple introduction to the concepts of Function Rank and the Rank Operator as they are defined in SHARP APL, and presents examples of their use. It shows the benefits, both in language design and in practice, of these concepts. Comparisons are made with other APL implementations, which do not use these concepts.	apl;apache axis;clutter;kahn process networks;parallel computing;programmer;rank (j programming language)	Robert Bernecky	1987		10.1145/55626.55632	mean reciprocal rank	PL	-11.327741863427654	32.10856087257557	92955
428e811cb1b8c935626cef4cb7bc4e0058f1aec1	verification condition generation for hybrid systems	semantics;program verification;computational modeling hybrid power systems automata differential equations semantics safety;automata;computational modeling;hybrid power systems;safety;differential equations;state transition diagram verification condition generation hybrid system sequential program verification automatic theorem prover smt solver nonlinear arithmetic vcg procedure hybrid program	Verification condition generators (VCGs) can reduce overall correctness statements about sequential programs to verification conditions (VCs) that can then be proved independently by automatic theorem provers like SMT solvers. SMT solvers became not only more powerful in recent years in that they can now solve much bigger problems than before, they can now also solve problems of less restricted logics, for example, by covering non-linear arithmetic as required by some hybrid systems. However, there is so far still no VCG procedure that could generate VCs of hybrid programs for these SMT solvers. We therefore propose in this paper a first VCG procedure for hybrid systems that is based on induction proofs on the strongly connected components (SCCs) of the underlying state transition diagrams. Given the right invariants for a safety property, the VCs can be automatically generated for the considered hybrid system. The validity of the VCs is then independently proved by SMT solvers and implies the correctness of the considered safety property.	correctness (computer science);diagram;formal verification;hybrid system;mathematical induction;nonlinear system;satisfiability modulo theories;simultaneous multithreading;state transition table;strongly connected component	Xian Li;Klaus Schneider	2015	2015 ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE)	10.1109/MEMCOD.2015.7340491	computer science;theoretical computer science;semantics;automaton;programming language;computational model;differential equation;algorithm	Logic	-13.107995732114963	26.085551487676753	93199
1a80175f42d74d3d8c0d0380fb7cfc8f60ee5707	structure-preserving binary relations for program abstraction	galois connection;modele kripke;analyse statique;correspondance galois;abstraction;abstraccion;analisis programa;kripke model;structure preservation;binary relation;modelo kripke;program analysis;analyse programme;static analysis	An abstraction is a property-preserving contraction of a program’s model into a smaller one that is suitable for automated analysis. An abstraction must be sound, and ideally, complete. Soundness and completeness arguments are intimately connected to the abstraction process, and approaches based on homomorphisms and Galois connections are commonly employed to define abstractions and prove their soundness and completeness. This paper develops Mycroft and Jones’s proprosal that an abstraction should be stated as a form of structure-preserving binary relation. Mycroft-Jones-style relations are defined, developed, and employed in characterizations of the homomorphism and Galois-connection approaches to abstraction.	jones calculus;kripke structure (model checking);model checking;prith banerjee;soundness (interactive proof);string operations;temporal logic	David A. Schmidt	2002		10.1007/3-540-36377-7_12	program analysis;discrete mathematics;computer science;binary relation;mathematics;abstraction;abstraction model checking;static analysis;algorithm	PL	-14.049702616822946	21.78942178305829	93250
7eaa8e49c544d8664deae388015ce84ebc49b0db	quantitative automata-based controller synthesis for non-autonomous stochastic hybrid systems	probabilistic reachability;formal verification;stochastic hybrid systems;stochastic optimal control;approximate abstractions	This work deals with Markov processes that are defined over an uncountable state space (possibly hybrid) and embedding non-determinism in the shape of a control structure. The contribution looks at the problem of optimization, over the set of allowed controls, of probabilistic specifications defined by automata - in particular, the focus is on deterministic finite-state automata. This problem can be reformulated as an optimization of a probabilistic reachability property over a product process obtained from the model for the specification and the model of the system. Optimizing over automata-based specifications thus leads to maximal or minimal probabilistic reachability properties. For both setups, the contribution shows that these problems can be sufficiently tackled with history-independent Markov policies. This outcome has relevant computational repercussions: in particular, the work develops a discretization procedure leading into standard optimization problems over Markov decision processes. Such procedure is associated with exact error bounds and is experimentally tested on a case study.	applicative programming language;automata theory;automaton;autonomous robot;büchi automaton;control flow;discretization;experiment;finite-state machine;hybrid system;ibm power systems;markov chain;markov decision process;markov property;mathematical optimization;maximal set;nondeterministic algorithm;optimizing compiler;reachability;state space	Ilya Tkachev;Alexandru Mereacre;Joost-Pieter Katoen;Alessandro Abate	2013		10.1145/2461328.2461373	probabilistic-based design optimization;mathematical optimization;combinatorics;discrete mathematics;quantum finite automata;mathematics	Logic	-10.818451036015361	27.807711478724755	93321
2387a3f9cb7cb106684c9874fc57d0f95a76b7fa	a case study of theorem proving by the knuth-bendix method: discovering that x³=x implies ring commutativity	word problem;satisfiability;theorem proving	  An automatic procedure was used to discover the fact that x    3   = x implies ring commutativity. The proof of this theorem was posed as a challenge problem by W.W. Bledsoe in a 1977 article.  The only previous atttomated proof was by Robert Veroff using the Argonne National Laboratory — Northern Illinois University  theorem-proving system. The technique used to prove this theorem was the Knuth-Bendix completion method with associative and/or  commutative unification. This was applied to a set of reductions consisting of a complete set of reductions for free rings  plus the reduction x × x × x → and resulted in the purely forward-reasoning derivation of x × y = y × x. An important extension to this methodology, which made solution of the x    3   = x ring problem feasible, is the use of cancellation laws to simplify derived reductions. Their use in the Knuth-Bendix method  can substantially accelerate convergence of complete sets of reductions. A second application of the Knuth-Bendix method to  the set of reductions for free rings plus the reduction x × x × x → x, but this time with the commutativity of multiplication assumed, resulted in the discovery of a complete set of reductions  for free rings satisfying x    3    = x. This complete set of reductions can be used to decide the word problem for the x    3   = x ring.    	knuth–bendix completion algorithm	Mark E. Stickel	1984		10.1007/978-0-387-34768-4_15	word problem;combinatorics;discrete mathematics;no-go theorem;computer science;fundamental theorem;mathematics;automated theorem proving;picard–lindelöf theorem;programming language;full employment theorem;satisfiability	Logic	-9.356136723880947	18.67999611387004	93583
be3f72cb160e18efab9e51e31eadbca30ac7e951	minimal coverability set for petri nets: karp and miller algorithm with pruning		This paper presents the Monotone-Pruning algorithm (MP) for computing the minimal coverability set of Petri nets. The original Karp and Miller algorithm (K&M) unfolds the reachability graph of a Petri net and uses acceleration on branches to ensure termination. The MP algorithm improves the K&M algorithm by adding pruning between branches of the K&M tree. This idea was first introduced in the Minimal Coverability Tree algorithm (MCT), however it was recently shown to be incomplete. The MP algorithm can be viewed as the MCT algorithm with a slightly more aggressive pruning strategy which ensures completeness. Experimental results show that this algorithm is a strong improvement over the K&M algorithm as it dramatically reduces the exploration tree.	dijkstra's algorithm;mobile data terminal;petri net;reachability;monotone	Pierre-Alain Reynier;Frédéric Servais	2011		10.1007/978-3-642-21834-7_5	computer science;artificial intelligence;theoretical computer science;machine learning;algorithm	EDA	-6.08532064652747	23.801900173143135	93844
a58992a420bca209d49ad25453e733a8bef7ad94	functional kleene closures		We present a derivation of a purely functional version of Kleene’s closure algorithm for Kleene algebras (with tests) that contain a subset where the closure is already known. In particular, our result is applicable to the Kleene algebra of square matrices over a given Kleene algebra. Our approach is based solely on laws imposed on Kleene algebras and Boolean algebras. We implement our results in the functional programming language Haskell for the case of square matrices and discuss a general implementation. In this process we incorporate purely algebraic improvements like the use of commutativity to obtain a concise and optimised functional program. Our overall focus is on a functional program and the computational structures from which it is composed. Finally, we discuss our result particularly in light of alternative approaches.	algorithm;computation;functional programming;haskell;kleene algebra;kleene's recursion theorem;linear algebra;programming language	Nikita Danilenko	2014		10.1007/978-3-319-17822-6_14	discrete mathematics;boolean algebra;computer science;algebraic number;algorithm;functional programming;haskell;closure (computer programming);derivation;kleene algebra;commutative property	PL	-13.892219519370336	18.98832134527234	93963
cb2928fc9969d9039affc21834646c36050a1579	distributive-law semantics for cellular automata and agent-based models	500 naturwissenschaften und mathematik	We present an effort to give formal semantics to the popular but theoretically rather unreflected scientific modelling paradigm of agentor individual-based models. To this end, we give a generic formalization of two-dimensional cellular automata with flexible topology as the abstract basis of such models. The semantic approach of structural operational semantics a la Turi and Plotkin [7], based on bialgebras and distributive laws, leads in this case to a natural separation of the concerns of spatial structure, temporal behavior and local interaction. We give a generic distributive law for local behavior of automata and prove the equivalence to a more traditional, array-based formalization.	agent-based model;automata theory;cellular automaton;linear algebra;operational semantics;plotkin bound;programming paradigm;semantics (computer science);turi;turing completeness	Baltasar Trancón y Widemann;Michael Hauhs	2011		10.1007/978-3-642-22944-2_24	discrete mathematics;computer science;mathematics;algorithm	Logic	-11.38204785385053	19.799209120657572	93987
3ef028d3a918fa42dcfa9f646c6bf4e3c12ba829	characterizations of classes of programs by three-valued operators	circonscription;program transformation;logical programming;transformation programme;circumscription;transformacion programa;programmation logique;programme recursif;programa recursivo;recursive program;logic programs;analisis semantico;analyse semantique;programacion logica;semantic analysis;circonscripcion	Several important classes of normal logic programs includ ing the classes of acyclic acceptable and locally hierarchical programs have the property that every program in the class has a unique two valued supported model In this paper we call such classes unique sup ported model classes We analyse and characterize these classes by means of operators on three valued logics Our studies will motivate the de ni tion of a larger unique supported model class which we call the class of accessible programs Finally we show that the class of accessible programs is computationally adequate in that every partial recursive function can be implemented by such a program Proceedings of the th International Conference on Logic Programmingand Non Monotonic Reasoning LPNMR El Paso Texas December Springer Lecture Notes in Arti cial Intelligence Vol pp	computable function;directed acyclic graph;recursion (computer science);springer (tank);μ-recursive function	Pascal Hitzler;Anthony Karel Seda	1999		10.1007/3-540-46767-X_26	discrete mathematics;computer science;artificial intelligence;mathematics;circumscription;algorithm	DB	-18.944833673540085	20.48158328110382	94020
f740ae61dcc60409b539d136955dcd742433983b	decentralized optimal control of distributed interdependent automata with priority structure	optimal control;automata;indexes;computational modeling;process control;production;optimization	For distributed discrete-event systems (DESs), which are specified by a set of coupled automata, the centralized synthesis for a composed plant model is often undesired due to a high computational effort and the need to subsequently split the result into local controllers. This paper proposes modeling and synthesis procedures to obtain optimal decentralized controllers in state-feedback form for distributed DES. In particular, this paper addresses the DES with priority structures, in which subsystems with high priorities are supplied with the output of subsystems with lower priority. If the subsystem dependencies have linear or treelike structures, the synthesis of the subsystem controllers can be accomplished separately. Any local controller is computed by algebraic computations, it communicates with controllers of adjacent subsystems, and it aims at transferring the corresponding subsystem into goal states with a minimal sum of transfer costs. As is shown for an example, the computational effort can be significantly reduced compared with the synthesis of centralized controllers following the composition of all subsystem models.	algorithm;automata theory;automaton;centralized computing;computation;dynamic programming;finite-state machine;interdependence;linear algebra;mathematical optimization;optimal control;sparse matrix	Olaf Stursberg;Christian Hillmann	2017	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2017.2669893	control engineering;database index;mathematical optimization;optimal control;computer science;artificial intelligence;process control;control theory;distributed computing;automaton;computational model;algorithm	Logic	-6.091867637954523	29.118957805903328	94031
e9ebacfe6e83a09e6f1cc97523350a73f6ac5582	logical reasoning for higher-order functions with local state	verification.;languages;theory;higher order functions;data structure;hoare logic;higher order	We introduce an extension of Hoare logic for call-by-value higher-order func- tions with ML-like local reference generation. Local references may be generated dynam- ically and exported outside their scope, may store higher-order functions and may be used to construct complex mutable data structures. This primitive is captured logically using a predicate asserting reachability of a reference name from a possibly higher-order datum and quantifiers over hidden references. We explore the logic's descriptive and reasoning power with non-trivial programming examples combining higher-order procedures and dynami- cally generated local state. Axioms for reachability and local invariant play a central role for reasoning about the examples.		Nobuko Yoshida;Kohei Honda;Martin Berger	2007		10.1007/978-3-540-71389-0_26	discrete mathematics;higher-order logic;data structure;computer science;mathematics;hoare logic;programming language;higher-order function;algorithm	Logic	-17.04229544040679	20.37728978246622	94243
a86d04b24e203ab96b20b4b6d03d05dc1920a6b7	using abstraction to verify arbitrary temporal properties	system recovery software engineering electronic mail safety sufficient conditions concrete;electronic mail;arbitrary temporal properties;time permitting arbitrary temporal properties finitary state abstraction methods ranking abstraction safety properties;joints;program verification;sufficient conditions;software engineering;safety properties;time permitting;ranking function;system recovery;safety;temporal properties;predicate abstraction;finitary state abstraction methods;ranking abstraction	Summary form only given. It is a known fact that finitary state abstraction methods (i.e. methods in which the abstract domain is finite), such as predicate abstraction, are inadequate for verifying general liveness properties or even termination of sequential programs. In this talk we will present an abstraction approach called ranking abstraction which is sound and complete for verifying all temporally specified properties, including all liveness properties. We will start by presenting a general simple framework for state abstraction emphasizing that, in order to get soundness, it is necessary to apply an over-approximating abstraction to the system and an under-approximating abstraction to the (temporal) property. We show that finitary version of this abstraction are complete for verifying all safety properties. We also show examples of simple programs whose termination provably cannot be established by finitary abstraction. We then consider abstraction approaches to the verification of deadlock freedom, presenting some sufficient conditions guaranteeing that deadlock freedom is inherited from the concrete to the abstract. Finally, we introduce the method of ranking abstraction and illustrate its application to the verification of termination and more general liveness properties. In this presentation we emphasize the similarity between predicate abstraction and its extension into ranking abstraction. In particular, the fact that the user does not have to provide a full ranking function but only to specify the ingredients from which such a function can be constructed. We also sketch how abstraction refinement can be applied to ranking abstraction, thus opening the way to a CEGAR-like methodology. Time permitting, we will present a brief comparison between ranking abstraction and the methods of transition abstraction developed by Podelski, Rybalchenko, and Cook which underly the Terminator system. The talk is based on results obtained through joint research with I. Balaban, Y. Kesten, and L.D. Zuck.	deadlock;liveness;predicate abstraction;ranking (information retrieval);refinement (computing);temporal logic;the terminator;verification and validation	Amir Pnueli	2008	2008 15th Asia-Pacific Software Engineering Conference	10.1109/APSEC.2008.76	abstraction inversion;computer science;theoretical computer science;software engineering;abstraction model checking;algorithm	Logic	-16.787035811668105	25.951738330223556	94330
8da1b479ae6fc1bd23ff953554941eb2839fabe9	tuplix calculus specifications of financial transfer networks	universiteitsbibliotheek;logic in computer science;organizational structure	We study the application of Tuplix Calculus in modular financial budget design. We formalize organizational structure using financial transfer networks. We consider the notion of flux of money over a network, and a way to enforce the matching of influx and outflux for parts of a network. We exploit so-called signed attribute notation to make internal streams visible through encapsulations. Finally, we propose a Tuplix Calculus construct for the definition of data functions.	money	Jan A. Bergstra;Sanne Nolst Trenité;Mark van der Zwaag	2008	CoRR		organizational structure;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;algorithm	PL	-9.408345539136148	20.61461330860561	94381
05b2549823df254b567e0568068df72dcaaeb5f6	declarative semantics for functional languages: compositional, extensional, and elementary		We present a semantics for an applied call-by-value λ-calculus that is compositional, extensional, and elementary. We present four dierent views of the semantics: 1) as a relational (big-step) semantics that is not operational but instead declarative, 2) as a denotational semantics that does not use domain theory, 3) as a non-deterministic interpreter, and 4) as a variant of the intersection type systems of the Torino group. We prove that the semantics is correct by showing that it is sound and complete with respect to operational semantics on programs and that is sound with respect to contextual equivalence. We have not yet investigated whether it is fully abstract. We demonstrate that this approach to semantics is useful with three case studies. First, we use the semantics to prove correctness of a compiler optimization that inlines function application. Second, we adapt the semantics to the polymorphic λ-calculus extended with general recursion and prove semantic type soundness. ird, we adapt the semantics to the call-by-value λ-calculus with mutable references. All of the denitions and proofs in this paper are mechanized in Isabelle in under 3,000 lines.	correctness (computer science);declarative programming;denotational semantics;domain theory;functional programming;immutable object;isabelle;lambda calculus;mathematical optimization;observational equivalence;operational semantics;optimizing compiler;recursion;turing completeness;type safety;type system	Jeremy G. Siek	2017	CoRR		programming language;extensional definition;functional programming;natural language processing;semantics;artificial intelligence;computer science	PL	-17.11136722074847	20.660727343366787	94523
e74621ea26ab100a678b0bfd9cdfa8983b28d866	short cut fusion: proved and improved	fixed point theorem;descomposicion funcion;substitution;program transformation;data type;transformation programme;functional programming;theoreme point fixe;teorema punto fijo;higher order;fixed point;transformacion programa;decomposition fonction;polymorphism;estructura datos;polymorphisme;structure donnee;programmation fonctionnelle;polimorfismo;fusion rule;programacion funcional;data structure;contextual equivalence;substitucion;function decomposition	Short cut fusion is a particular program transformation technique which uses a single, local transformation — called the foldr-build rule — to remove certain intermediate lists from modularly constructed functional programs. Arguments that short cut fusion is correct typically appeal either to intuition or to “free theorems” — even though the latter have not been known to hold for the languages supporting higher-order polymorphic functions and fixed point recursion in which short cut fusion is usually applied. In this paper we use Pitts’ recent demonstration that contextual equivalence in such languages is relationally parametric to prove that programs in them which have undergone short cut fusion are contextually equivalent to their unfused counterparts. The same techniques in fact yield a much more general result. For each algebraic data type we define a generalization augment of build which constructs substitution instances of its associated data structures. Together with the well-known generalization cata of foldr to arbitrary algebraic data types, this allows us to formulate and prove correct for each a contextual equivalence-preserving cata-augment fusion rule. These rules optimize compositions of functions that uniformly consume algebraic data structures with functions that uniformly produce substitution instances of them.	algebraic data type;correctness (computer science);data structure;fixed point (mathematics);fold (higher-order function);lambda calculus;linear algebra;observational equivalence;program transformation;recursion;strict function;turing completeness;whole earth 'lectronic link	Patricia Johann	2001		10.1007/3-540-44806-3_4	functional decomposition;polymorphism;discrete mathematics;higher-order logic;data structure;data type;computer science;artificial intelligence;mathematics;fixed point;fixed-point theorem;programming language;functional programming;algorithm	PL	-13.322758395418612	18.67493807297485	94689
7e952fce3ef4eaa8b1ffd369d493d8bcff26d84a	a coq library for internal verification of running-times		This paper presents a Coq library that lifts an abstract yet precise notion of running-time into the type of a function. Our library is based on a monad that counts abstract steps. The monad’s computational content, however, is simply that of the identity monad so programs written in our monad (that recur on the natural structure of their arguments) extract into idiomatic OCaml code. We evaluated the expressiveness of the library by proving that red-black tree insertion and search, merge sort, insertion sort, various Fibonacci number implementations, iterated list insertion, various BigNum operations, and Okasaki’s Braun Tree algorithms all have their expected running times.	algorithm;arbitrary-precision arithmetic;coq (software);insertion sort;iteration;merge sort;monad (functional programming);ocaml;red–black tree	Jay A. McCarthy;Burke Fetscher;Max S. New;Daniel Feltey;Robert Bruce Findler	2016		10.1007/978-3-319-29604-3_10	computer science;theoretical computer science;programming language;algorithm	PL	-15.494957866179956	20.99694895857883	94712
2de5980adbc4ef81004583074f04f27e5d445507	reasoning about co-büchi tree automata	expressive power;tree automata	We consider co–Büchi tree automata along with both alternating and generalized paradigms, as a characterization of the class of languages whose complement is accepted by generalized Büchi tree automata. We first prove that for alternating generalized co–Büchi tree automata the simulation theorem does not hold and the generalized acceptance does not add to the expressive power of the model. Then, we show that the emptiness problem for this class is EXPTIME-complete. For the class of languages whose complement is accepted by deterministic generalized Büchi tree automata, we get better complexity bounds: we give a characterization of this class in terms of generalized co–Büchi tree automata that yields an algorithm for checking the emptiness that takes time linear in the product of the number of states and the number of sets in the acceptance condition. Finally, we compare the classes of languages whose complement is respectively accepted by deterministic and nondeterministic Büchi tree automata with the main classes studied in the literature.	abstract syntax tree;algorithm;assertion (software development);automata theory;computation;divergence (computer science);exptime;requirement;simulation;steiner tree problem;time complexity;tree automaton;ω-automaton	Salvatore La Torre;Aniello Murano	2004		10.1007/978-3-540-31862-0_37	combinatorics;discrete mathematics;quantum finite automata;computer science;nested word;mathematics;programming language;expressive power;algorithm	Theory	-6.101909698679404	21.96017855472408	94762
2cc476733a0c2d6712628ed8bf6f61cf58fb9be1	software is discrete mathematics	predicate logic;discrete mathematics;formal methods;software engineering;functional programming;formal method;functional language;correctness proof;correctness proofs	A three-year study collected information bearing on the question of whether studying mathematics improves programming skills. An analysis of the data revealed significant differences in the programming effectiveness of two populations of students: (1) those who studied discrete mathematics through examples focused on reasoning about software and (2) those who studied the same mathematical topics illustrated with more traditional examples. Functional programming played a central role in the study because it provides a straightforward framework for the presentation of concepts such as predicate logic and proof by induction. Such topics can be covered in depth, staying almost entirely within the context of reasoning about software. The intricate complexities in logic that mutable variables carry with them need not arise, early on, to confuse novices struggling to understand new ideas. In addition, because functional languages provide useful and compact ways to express mathematical concepts, and because the choice of notation in mathematics courses is often at the discretion of the instructor (in contrast to the notational restrictions often fiercely guarded by the faculty in programming courses), discrete mathematics courses, as they are found in most computer science programs, provide an easy opportunity to enhance the education of students by exposing them to functional programming concepts.	computer science;discrete mathematics;functional programming;immutable object;mathematical induction;population	Rex L. Page	2003		10.1145/944705.944713	predicate logic;formal methods;reactive programming;computer science;theoretical computer science;functional logic programming;programming paradigm;inductive programming;programming language theory;programming language;functional programming;algorithm	PL	-18.22222144277361	19.263292889596084	94774
7b9eef1c8072ef590fcbf4c88bc65db28531c0b6	truthful monadic abstractions	proof search;unprovable subgoal;classical first-order logic;undecidable logic;truth-preserving abstraction;intuitionistic first-order logic;direct proof search;truthful monadic abstraction;intuitionistic sequent calculus;monadic fragment	In intuitionistic sequent calculi, detecting that a sequent is unprovable is often used to direct proof search. This is for instance seen in backward chaining, where an unprovable subgoal means that the proof search must backtrack. In undecidable logics, however, proof search may continue indefinitely, finding neither a proof nor a disproof of a given subgoal. In this paper we characterize a family of truth-preserving abstractions from intuitionistic first-order logic to the monadic fragment of classical first-order logic. Because they are truthful, these abstractions can be used to disprove sequents in intuitionistic first-order logic.	automated theorem proving;backtracking;backward chaining;decision problem;experiment;first-order logic;first-order predicate;intuitionistic logic;library (computing);method of analytic tableaux;parallel computing;sensor;sequent calculus;twelf;undecidable problem	Taus Brock-Nannestad;Carsten Schürmann	2012		10.1007/978-3-642-31365-3_10	discrete mathematics;cut-elimination theorem;mathematics;structural proof theory;algorithm	Logic	-13.799135570446676	20.480016154207707	95130
4021fbd69cebc0941d20f7a18c8a5539db30a4d9	compositional design of stochastic timed automata		In this paper, we study the model of stochastic timed automata and we target the definition of adequate composition operators that will allow a compositional approach to the design of stochastic systems with hard real-time constraints. This paper achieves the first step towards that goal. Firstly, we define a parallel composition operator that (we prove) corresponds to the interleaving semantics for that model; we give conditions over probability distributions, which ensure that the operator is well-defined; and we exhibit problematic behaviours when this condition is not satisfied. We furthermore identify a large and natural subclass which is closed under parallel composition. Secondly, we define a bisimulation notion which naturally extends that for continuous-time Markov chains. Finally, we importantly show that the defined bisimulation is a congruence w.r.t. the parallel composition, which is an expected property for a proper modular approach to system design.	automata theory;bisimulation;congruence of squares;forward error correction;markov chain;real-time clock;real-time computing;stochastic process;systems design;timed automaton	Patricia Bouyer;Thomas Brihaye;Pierre Carlier;Quentin Menet	2016		10.1007/978-3-319-34171-2_9	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-9.882385058288186	22.913783352133105	95201
063206895b4e5a42f43f8d85364b9ed51fcd7da6	algebraic methods in the compositional analysis of logic programs	similarity solution;full abstraction;algebraic method;logic programs;compositional analysis	The compositionality of the semantics of logic programs with respect to (different varieties of) program union has been studied recently by a number of researchers. The approaches used can be considered quite ad-hoc in the sense that they provide, from scratch, the semantic constructions needed to ensure compositionality and, in some cases, full abstraction in the given framework. In this paper, we study the application of general algebraic methods for obtaining, systematically, this kind of results. In particular, the method proposed consists in defining the adequate institution for describing the given class of logic programs and, then, in using general institution-independent results to prove compositionality and full abstraction. This is done in detail for the class of definite logic programs, where the associated institution is defined in such a way that initial algebra semantics is equivalent to computed answer substitution semantics. Then a similar solution is sketched for definite logic programs with constraints and equality and for normal logic programs with constructive negation.	denotational semantics;hoc (programming language);initial algebra;linear algebra;logic programming	Fernando Orejas;Elvira Pino;Hartmut Ehrig	1994		10.1007/3-540-58338-6_62	discrete mathematics;theoretical computer science;mathematics;signature;algorithm	PL	-14.758478326165273	19.02855966612287	95581
27c11b3d113b3fe063d5eaf8e4fb22188a9c99b9	expression reduction from programs in a symbolic binary executor		Symbolic binary execution is a dynamic analysis method which explores program paths to generate test cases for compiled code. Throughout execution, a program is evaluated with a bit-vector theorem prover and a runtime interpreter as a mix of symbolic expressions and concrete values. Left untended, these symbolic expressions grow to negatively impact interpretation performance. We describe an expression reduction system which recovers sound, contextinsensitive expression reduction rules at run time from programs during symbolic evaluation. These rules are further refined offline into general rules which match larger classes of expressions. We demonstrate that our optimizer significantly reduces the number of theorem solver queries and solver time on hundreds of commodity programs compared to a default ad-hoc optimizer from a popular symbolic interpreter.	automated theorem proving;bit array;compiler;hoc (programming language);interpreter (computing);mathematical optimization;online and offline;rewriting;run time (program lifecycle phase);s-expression;solver;symbolic execution;test case	Anthony Romano;Dawson R. Engler	2013		10.1007/978-3-642-39176-7_19	theoretical computer science;distributed computing;algorithm	PL	-19.08409096806915	29.58931647167885	95595
f6db2d5e54b2800e29cbdab244f13f1b80245ea6	modelling real-time behaviour with an interval time calculus	real time	We present an extension of Milner's CCS [Mil89] with interval time. The notion of time is introduced in terms of time intervals which specify when actions are allowed to occur. We define three equivalences: strong, timed weak, and weak bisimulation equivalence. The strong bisimulation equivalence refines the corresponding relation in CCS by requiring strongly bisimilar processes to have the same timing behavior, the weak bisimulation equivalence is in essence the weak bisimulation equivalence of CCS, while the timed weak equivalence lays strictly between strong and weak equivalence, since it in addition to weak bisimularity also considers the timing of observable actions.  We define a refinement relation for processes specified in our calculus, which can be used to order processes according to their real-time behavior. This relation can be interpreted as having a more precise timing specification than. For example, the refinement relation can be used to show that an implementation (which normally has more precise time requirements) meets an abstract specification in which the time requirements are specified more generously.    	real-time locating system	Mats Daniels	1992		10.1007/3-540-55092-5_4	discrete mathematics;equivalence (measure theory);bisimulation;weak equivalence;observable;mathematics	Logic	-10.57181130857321	24.662387055421277	95779
a009107b499869f181fe1708cee950343699b670	covarieties of coalgebras: comonads and coequations	transition state;morphisme;morfismo;sistema transicion;state transition system;closure;behavioral analysis;bisimulacion;bisimulation;transition system;systeme transition;estado transitorio;analyse comportementale;estructura datos;coalgebra;coalgebre;analisis conductual;structure donnee;modele donnee;morphism;cerradura;data structure;etat transition;fermeture;data models	Coalgebras provide effective models of data structures and state-transition systems. A virtual covariety is a class of coalgebras closed under coproducts, images of coalgebraic morphisms, and subcoalgebras defined by split equalisers. A covariety has the stronger property of closure under all subcoalgebras, and is behavioural if it is closed under domains of morphisms, or equivalently under images of bisimulations. There are many computationally interesting properties that define classes of these kinds. We identify conditions on the underlying category of a comonad G which ensure that there is an exact correspondence between (behavioural/virtual) covarieties of G-coalgebras and subcomonads of G defined by comonad morphisms to G with natural categorical properties. We also relate this analysis to notions of coequationally defined classes of coalgebras.	bisimulation;data structure	Ranald Clouston;Robert Goldblatt	2005		10.1007/11560647_19	data modeling;combinatorics;discrete mathematics;topology;data structure;computer science;bisimulation;closure;mathematics;transition state;programming language;morphism	Logic	-8.350297103446247	22.09977022979603	95782
2315f6a30ab309dff982419bb9fa3c46bcde5cb9	infer precise program invariant using abstract interpretation with recurrence solving		Program invariant is formal description of properties that should hold at certain program location in every valid execution. It is very useful for program analysis and verification. In this paper, we introduce an abstraction interpretation approach for generating program invariant efficiently and precisely. A polynomial interval domain is proposed for representing abstract state and precise loop effect is summarized by recognizing and solving recurrence relations. Our method has implemented and its effectiveness is shown in various kinds of cases. Experiment results show that our approach generates more accurate program invariants quickly.	abstract interpretation;formal verification;invariant (computer science);polynomial;program analysis;recurrence relation	Zhenpeng Fang;Xibin Zhao;Min Zhou	2017	2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2017.149	discrete mathematics;real-time computing;program analysis;computer science;recurrence relation;theoretical computer science;algorithm design;polynomial;invariant (computer science);invariant (mathematics);abstract interpretation;abstraction	SE	-17.929692399594625	28.41291107460001	95790
b38baeede95492b727df21f488aef64bb455ac53	infinite normal forms for non-linear term rewriting systems	term rewrite system;normal form	Abstract Nonterminating rewrite relations have recently been studied in order to set a framework within which infinite terms can be seen as limits of infinite converging derivations. Results about the existence of infinite normal forms have been given only for orthogonal term rewriting systems, namely left-linear and nonoverlapping systems. In this paper we show that some of those results can be extended to a particular class of nonorthogonal term rewriting systems. We deal with systems in which the nonterminating rules are unfolding rules that model the operational semantics of a recursive operator. The left-linearity requirement is replaced by a retraction property of the supporting term algebra, that allows the definition of a rewrite relation modulo a congruence relation induced on the set of terms by the unfolding rules.	normal form (abstract rewriting);rewriting;term (logic)	Paola Inverardi;Monica Nesi	1991		10.1007/3-540-54345-7_66	computer science;normalization property;mathematics;normal-form game;confluence	Logic	-10.0493338456329	18.466271399889187	95936
f334331908442ec86f54e05bd83a77daa472f7cb	dataflow programs for parallel computations of logic programs and their semantics	answer sets;direct product;parallel computer;logic programs	This paper deals with a method of constructing a dataflow program computing a given logic program in parallel. The dataflow program for a given logic program is a recursion equation set expressing the sequences of answer substitutions provided by the finite computation of the original logic program. It is defined over a sequence domain, which is the set of all finite and infinite sequences of substitutions. It is shown that the recursion equation set defines a continuous function from a direct product of a sequence domain to itself, therefore there exists a least fixpoint of the function. The fixpoint completely denotes the answer set for a parallel computation of the original logic program. In this sense, the fixpoint is interpreted as a semantics of the logic program.	dataflow programming;logic programming	Susumu Yamasaki	1989		10.1007/3-540-51285-3_36	computer architecture;stable model semantics;computer science;theoretical computer science;computational logic;programming language;axiomatic semantics	PL	-11.699019097242898	20.134028730316	96131
abce5790981f11fa1ed232648f3aadc2fa3d74e3	structured interactive scores: from a simple description of an interactive scenario to a real-time capable implementation with formal semantics	interactive scores;concurrent constraint programming;ntcc;temporal constraints	Technology has shaped the way on which we compose and produce music: Notably, the invention of microphones and computers pushed the development of new music styles in the 20th century. In fact, several artistic domains have been benefiting from such technology developments; for instance, Experimental music, non-linear multimedia, Electroacoustic music, and interactive multimedia. In this dissertation, we focus on interactive multimedia. Interactive multimedia deals with the design of scenarios where multimedia content and interactive events are handled by computer programs. Examples of such scenarios are multimedia art installations, interactive museum exhibitions, some Electroacoustic music pieces, and some Experimental music pieces. Unfortunately, most interactive multimedia scenarios are based on informal specifications, thus it is not possible to formally verify properties of such systems. We advocate the need of a general and formal model. Interactive scores is a formalism to describe interactive multimedia scenarios. We propose new semantics for interactive scores based on timed event structures. With such a semantics, we can specify properties for the system, in particular, properties about traces, which are difficult to specify as constraints. In fact, constraints are an important part of the semantic model of interactive scores because the formalism is based on temporal constraints among the objects of the scenario. We also present an operational semantics of interactive scores based on the non-deterministic timed concurrent constraint (ntcc) calculus and we relate such a semantics to the timed event structures semantics. With the operational semantics, we formally describe the behavior of a score whose temporal object durations can be arbitrary integer intervals. The operational semantics is obtained from the timed event structures semantics of the score. To provide such a translation, we first define the normal form of a timed event structure in which events related with zero-duration delays are collapsed into a single one. We also define the notion of dispatchable timed event structures: Event structures such that its constraint graph can be dispatched by relying only on local propagation. We believe that operational semantics in ntcc offers some advantages over existing Petri nets semantics for interactive scores; for instance, the duration of the temporal objects can be arbitrary integer intervals, whereas in previous models of interactive scores, such durations can only be intervals to represent equalities and inequalities. In this dissertation, we also introduce two extensions of the formalism of interactive scores: (1) one to handle audio processing using the Fast AUdio Stream (Faust) language and (2) another one to handle conditional branching, allowing designers to specify choices and loops. For the first extension, we present a timed event structures semantics and ideas on how to define operational semantics. For the second extension, we present an implementation and results comparing the average relative jitter of an implementation of an arpeggio based on Karplus-Strong with respect to existing implementations of Karplus written in Pure Data. We also define a XML file format for interactive scores and for the conditional branching extension. A file format is crucial to assure the persistence of the scores. Ntcc models of interactive scores are executed using Ntccrt, a real-time capable		Mauricio Toro	2012			semantics of logic;topology;mathematics	AI	-11.483401429293895	25.143256242340392	96253
92dfbf02fc68df56b7ae3b27188b0314797d072e	an introduction to proving the correctness of programs	conditional expectation;program verification;input output;upper bound;symbolic execution;proof of correctness;program correctness	"""This paper explains, in an introductory fashion, the method of specifying the correct behavior of a program by the use of input/output assertions and describes one method for showing that the program is correct with respect to those assertions. An initial assertion characterizes conditions expected to be true upon entry to the program and a final assertion characterizes conditions expected to be true upon exit from the program. When a program contains no branches, a technique known as symbolic execution can be used to show that the truth of the initial assertion upon entry guarantees the truth of the final assertion upon exit. More generally, for a program with branches one can define a symbolic execution tree. If there is an upper bound on the number of times each loop in such a program may be executed, a proof of correctness can be given by a simple traversal of the (finite) symbolic execution tree. However, for most programs, no fixed bound on the number of times each loop is executed exists and the corresponding symbolic execution trees are infinite. In order to prove the correctness of such programs, a more general assertion structure must be provided. The symbolic execution tree of such programs must be traversed inductively rather than explicitly. This leads naturally to the use of additional assertions which are called """"inductive assertions."""""""	assertion (software development);computer program;correctness (computer science);formal verification;input/output;recursive definition;symbolic execution;tree traversal	Sidney L. Hantler;James C. King	1976	ACM Comput. Surv.	10.1145/356674.356677	program analysis;input/output;correctness;conditional expectation;computer science;theoretical computer science;upper and lower bounds;programming language;program derivation;algorithm;statistics	PL	-14.000631647737809	27.86315943728692	96349
390dcd89861d7e1c3bbba13c23fc0a0846c9e438	formal test generation from uml models	modelizacion;distributed system;seguridad funcionamiento;raisonnement base sur cas;razonamiento fundado sobre caso;surete fonctionnement;systeme reparti;formal specification;system modeling;model system;redundancia;red petri;lenguaje uml;formal specification language;semantics;simultaneidad informatica;langage modelisation unifie;semantica;semantique;specification language;specification formelle;modelisation;especificacion formal;software architecture;concurrency;sistema repartido;redundancy;state space method;methode espace etat;object oriented;generation test;unified modelling language;state space;dependability;classe equivalence;architecture basee modele;test generation;equivalence classes;lenguaje especificacion;intencion;case based reasoning;petri net;modeling;simultaneite informatique;langage specification;generacion prueba;lenguaje formal;model driven architecture;intention;formal language;architecture logiciel;reseau petri;redondance;metodo espacio estado;arquitectura basada modelo;langage formel	In this paper we will explain our approach for generating test cases for a UML system model. Despite the fact that UML authors claim that UML semantics are precise enough to define non-ambiguous models, we find that the overlap of the different views makes it difficult to explore and make deduction on the state space of the modeled system in order to generate test cases. Our approach is thus based on a subset of UML (inspired from the Fondue approach) for which we have defined clear transformation semantics. We provide these semantics by delineating transformation rules using MDA (Model Driven Architecture) architecture as foundation. We transform UML models into CO-OPN (Concurrent Object Oriented Petri Nets) ones, CO-OPN being a formal specification language defined in our Laboratory. We have also defined a language for expressing test intentions for COOPN models. This language allows selecting interesting executions (tests cases) of a model by providing constraints over all possible traces of that model. By exploring the model’s semantics with the tools we have built for our CO-OPN language we are able to generate test cases based on those test intentions. We are also able to partially eliminate redundancy in the produced test cases by finding equivalence classes in the model operation’s inputs.	co-opn;commitment ordering;complexity;formal specification;mobile phone;model-driven architecture;natural deduction;petri net;redundancy (engineering);semantics (computer science);semiconductor industry;specification language;state space;test case;test set;tracing (software);turing completeness;unified modeling language	Didier Buchs;Luis Pedro;Levi Lucio	2006		10.1007/11808107_7	simulation;systems modeling;uml tool;computer science;artificial intelligence;applications of uml;semantics;programming language;algorithm;object constraint language	SE	-16.918539894366056	27.175714127508602	96415
9c85fc5b4f7c613232512bcba4e1be3bfdd09f6e	a formal verification of a subset of information-based access control based on extended weighted pushdown system	access control;model checking		access control;formal verification;stack (abstract data type)	Pablo Lamilla Alvarez;Yoshiaki Takata	2014	IEICE Transactions		model checking;discrete mathematics;computer science;access control;distributed computing;programming language;pushdown automaton;embedded pushdown automaton	Crypto	-10.76967056482879	23.601327764543957	96419
538fe54b974d236d42141966e7e5622bc829988a	continuations in possible-world semantics	lenguaje programacion;preuve programme;program proof;continuations;programming language;semantics;semantica;semantique;category theory;theorie categorie;prueba programa;langage programmation;algol;teoria categoria;possible worlds	Tennent, R.D. and J.K. Tobin, Continuations in possible-world semantics, Theoretical Computer Science 85 (1991) 283-303. Earlier work has shown that a form of possible-world semantics allows elegant solutons to certain difficult problems in the modelling of local-variable declarations and noninterference specific&ions in a generalization of Hoare’s logic suitable for ALGOL ho-like languages with procedures. In this work it is shown how jumps and block expressions can be treated in this framework. Those of us who have worked with continuations for some time have soon learned to think of them as natural and in fact often simpler than the earlier methods. -C. Strachey and C.P. Wadsworth (1974) If this is the best of possible worlds, what then are the others? -Voltaire, Candide (1759)		Robert D. Tennent;J. K. Tobin	1991	Theor. Comput. Sci.	10.1016/0304-3975(91)90184-4	computer science;continuation;mathematics;semantics;possible world;programming language;algorithm;category theory;algebra	PL	-18.220337143097467	20.980538387544005	96589
ef20e0d9399489ac4f6a2f277de75aff7c688c39	coordinating time-constrained multi-agent resource sharing with fault detection	resource allocation;fault detection uncertainty polynomials complexity theory clustering algorithms resource management schedules;polynomials;multi agent systems;computational complexity;decomposition algorithm;state space;fault detection;fault tolerance;resource sharing;polynomial time;resource allocation computational complexity fault tolerance multi agent systems polynomials;computational complexity time constrained multiagent resource sharing fault detection distributed multiagent environment state transition relation environmental factors central coordinator np complete problem polynomial size state space;state transition;environmental factor	Sharing common resources in a distributed multi-agent environment requires coordination to avoid faulty system states. The statuses of resources such as personnel, equipments, and environmental factors at a point in time determine the system state at that time. When an agent takes an action at any time point within a scheduled time interval, it becomes a state-transition event occurring at that time. For each event, the underlying state transition relation can be compactly encoded as causal rules, which describe how statuses of resources and environmental factors may change in different ways based on preconditions before the event occurs. The central coordinator needs to check in advance whether any of the possible event sequences consistent with a proposed schedule may end in faulty system states. This fault detection task is NP-complete even for a polynomial-size state space in general. In this paper, we investigate the computational complexity of the fault detection task when agents fairly constrain the maximal length of time intervals. We develop a decomposition algorithm to divide the fault detection task over all events into subtasks involving subsets of the events with overlapping time intervals. For each subtask, only a subspace with reduced dimensionality is involved instead of the whole original state space. When the maximal length of time intervals is constrained below a fair threshold, we prove that with probability approaching one as the size of the problem instance grows the algorithm can accomplish the fault detection task in polynomial time even if the original underlying state space is exponential in size.	algorithm;causal filter;computational complexity theory;fault detection and isolation;maximal set;multi-agent system;np-completeness;polynomial;precondition;state space;state transition table;time complexity	Shieu-Hong Lin	2011	2011 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2011.6118066	shared resource;time complexity;mathematical optimization;fault tolerance;real-time computing;resource allocation;computer science;state space;artificial intelligence;theoretical computer science;machine learning;multi-agent system;distributed computing;computational complexity theory;fault detection and isolation;polynomial	Robotics	-6.659800771983405	30.841492880339686	96756
bc708f035dbc74276fd80af3951bf6d2b18372aa	algorithmic differentiation of code with multiple context-specific activities	tangent linear;activity analysis;reverse mode;algorithmic differentiation;adjoint;software and its engineering automated static analysis;additional key words and phrases algorithmic differentiation;journal article;ccs concepts mathematics of computing automatic differentiation;source transformation;automatic differentiation;static analysis;source code generation	Algorithmic differentiation (AD) by source-transformation is an established method for computing derivatives of computational algorithms. Static dataflow analysis is commonly used by AD tools to determine the set of active variables, that is, variables that are influenced by the program input in a differentiable way and have a differentiable influence on the program output. In this work, a context-sensitive static analysis combined with procedure cloning is used to generate specialised versions of differentiated procedures for each call site. This enables better detection and elimination of unused computations and memory storage, resulting in performance improvements of the generated code, in both forward- and reverse-mode AD. The implications of this multi-activity AD approach on the static analysis of an AD tool is shown using dataflow equations. The worst-case cost of multi-activity AD on the differentiation process is analysed and practical remedies to avoid running into this worst case are presented. The method was implemented in the AD tool Tapenade, and we present its application to a 3D unstructured compressible flow solver, for which we generate an adjoint solver that performs significantly faster when multi-activity AD is used.	algorithm;automatic differentiation;best, worst and average case;call site;call stack;command-line interface;computation;context-sensitive grammar;control flow;data-flow analysis;dataflow;derivative code;directive (programming);goto;gradient;mathematical software;memory footprint;return statement;reverse computation;solver;source transformation;stack (abstract data type);static program analysis;subroutine	Jan Christian Hückelheim;Laurent Hascoët;Jens-Dominik Müller	2017	ACM Trans. Math. Softw.	10.1145/3015464	automatic differentiation;mathematical optimization;mathematical analysis;computer science;theoretical computer science;mathematics;distributed computing;programming language;algorithm;algebra	SE	-17.796906477607365	31.968228616876374	96810
9d3e7c2e075cdad80347342a34b1bdafaa28b8a2	validating the psl/sugar semantics using automated reasoning	logique lineaire;modelo dinamico;embedding;logica formal;formal verication;logica temporal;formal specification;validacion;temporal logic;sugar property language;dynamic model;semantics;automated reasoning;raisonnement;logica lineal;semantica;semantique;specification language;specification formelle;theorem proving;especificacion formal;demonstration theoreme;computation tree logic;formal verification;model checking;linear temporal logic;semantic embedding;2003;modele dynamique;ordre n;plongement;razonamiento;formal logic;orden n;verification formelle;validation;formal specication;lenguaje especificacion;inmersion;reasoning;demostracion teorema;n order;logique formelle;hol;linear logic;langage specification;higher order logic;lenguaje formal;regular expression;logique temporelle;spl;formal language;industrial design;plongement semantique;ltl;langage formel	The Accellera organisation selected Sugar, IBM’s formal specification language, as the basis for a standard to ‘drive assertion-based verification’ in the electronics industry. Sugar combines regular expressions, Linear Temporal Logic (LTL) and Computation Tree Logic (CTL) into a property language intended for both static verification (e.g. model checking) and dynamic verification (e.g. simulation). In 2003 Accellera decided to rename the evolving standard to ‘Accellera Property Specification Language’ (or ‘PSL’ for short). We motivate and describe a deep semantic embedding of PSL in the version of higher-order logic supported by the HOL 4 theorem-proving system. The main goal of this paper is to demonstrate that mechanised theorem proving can be a useful aid to the validation of the semantics of an industrial design language.	algorithm;assertion (software development);automated reasoning;automated theorem proving;computation tree logic;documentation;email;formal grammar;formal specification;hol (proof assistant);interval temporal logic;latex;learning relationship management;linear temporal logic;logic programming;model checking;parsing;property specification language;regular expression;rename (relational algebra);sanity check;semantics (computer science);semiconductor industry;simulation;software bug;software development process;software verification;sugar;theorem proving system;theory;web standards;while	Michael J. C. Gordon	2003	Formal Aspects of Computing	10.1007/s00165-003-0014-5	model checking;linear logic;formal language;discrete mathematics;linear temporal logic;industrial design;higher-order logic;specification language;temporal logic;formal verification;computation tree logic;hol;computer science;embedding;formal specification;mathematics;semantics;automated theorem proving;automated reasoning;programming language;logic;reason;regular expression;algorithm	Logic	-16.74429374798636	26.87781550128781	96830
6caa7d233f9b1e3051fd7227fa38705b587a3869	design and applications of an algorithm benchmark system in a computational problem solving environment	learning effectiveness;time complexity;benchmark;knowledge portal;algorithm visualization;common criteria;problem solving environment;open source	Benchmark tests are often used to evaluate the quality of products by a set of common criteria. In this paper we describe a computational problem solving environment based on open source codes and an algorithm benchmark system, which is embedded in the environment as a plug-in system. The algorithm benchmark system can be used to compare the performance of various algorithms or to evaluate the behavior of an algorithm with different input instances. The current implementation allows users to compare or evaluate algorithms written in C/C++. Some examples of the algorithm benchmark system that evaluates the memory utilization, time complexity and the output of algorithms are also presented. Algorithm benchmark impresses the learning effect; students can not only comprehend the performance of respective algorithms but also write their own programs to challenge the best known results.	algorithm;benchmark (computing);c++;code;common criteria;computation;computational problem;embedded system;open-source software;plug-in (computing);problem solving environment;time complexity	Ming-Yu Chen;Jyh-Da Wei;Jeng-Hung Huang;D. T. Lee	2006		10.1145/1140124.1140159	time complexity;sdet;probabilistic analysis of algorithms;simulation;benchmark;computer science;theoretical computer science;machine learning	EDA	-9.354586143526058	32.268953866029996	97064
5b92d54f7fb2f28ed1c183fa9592ef79f8aef9f5	extended symbolic finite automata and transducers	equivalence;string encoders;symbolic automata;symbolic transducers	Symbolic finite automata and transducers augment classic automata and transducers with symbolic alphabets represented as parametric theories. This extension enables to succinctly represent large and potentially infinite alphabets while preserving closure and decidability properties. Extended symbolic finite automata and transducers further extend these objects by allowing transitions to read consecutive input elements in a single step. In this paper we study the properties of these models. In contrast to the case of finite alphabets, we show how reading multiple symbols increases the expressiveness of the models, which causes some closure properties to stop holding and most decision problems to become undecidable. In particular we show how extended symbolic finite transducers are not closed under composition, and the equivalence problem is undecidable for both extended symbolic finite automata and transducers. We then introduce the subclass of Cartesian extended symbolic finite transducers in which guards are limited to conjunctions of unary predicates and we propose an equivalence algorithm for this subclass in the single-valued case. We also present a heuristic algorithm for composing extended symbolic finite transducers that works for many practical cases. Finally, we model real world programs with Cartesian extended symbolic finite transducers and use the proposed algorithms to prove their correctness.	automata theory;finite-state machine;transducer	Loris D'Antoni;Margus Veanes	2015	Formal Methods in System Design	10.1007/s10703-015-0233-4	equivalence;symbolic data analysis;programming language;symbolic trajectory evaluation;algorithm	Logic	-10.922223680103347	23.556173667165005	97303
9c5ff6b044df785bf07d9d2ebd97df76e3adf2d2	developing a consensus algorithm using stepwise refinement	verification;stepwise refinement;event b examples;event b;refinement;eprints newcastle university;consensus algorithms;open access;dr jeremy bryans	We give a formal development and proof of a known consensus algo rithm using stepwise refinement. We begin with an abstract specification of th e intended result of the algorithm. The algorithm is developed and proved co rrect over a number of refinement steps. The proof of correctness is per formed concurrently with the development. The development and proof make use of key fault and failure assumptions. The stepwise refinement approach allows us to introduce and prove each property at the most appropriate stage in the deve lopment, before detail irrelevant to that property has been added to the model. Fin a ly we introduce an abstract model of a possible network on which the algorithm c ould be executed.	chandra–toueg consensus algorithm;co-ment;consensus (computer science);correctness (computer science);integer factorization;refinement (computing);stepwise regression;telecommuting;top-down and bottom-up design	Jeremy Bryans	2011		10.1007/978-3-642-24559-6_37	verification;computer science;theoretical computer science;top-down and bottom-up design;data mining;refinement;programming language;algorithm	Logic	-17.81312527311467	25.866385920189778	97364
7679e3c57af7ba3a483e85d1ec2546ce4e42f2b7	auxiliary variables in data refinement	auxiliary variable;metodologia;program transformation;transformation programme;methodologie;transformacion programa;methodology	Abstract   A set of local variables in a program is auxiliary if its members occur only in assignments to members of the same set. Data refinement transforms a program, replacing one set of local variables by another set, in order to move towards a more efficient representation of data. Most techniques of data refinement give a direct transformation. But there is an indirect technique, using auxiliary variables, that proceeds in several stages. Usually, the two techniques are considered separately. It is shown that the several stages of the indirect technique are themselves special cases of the direct one, thus unifying the separate approaches. Removal of auxiliary variables is formalised incidentally.	refinement (computing)	Carroll Morgan	1988	Inf. Process. Lett.	10.1016/0020-0190(88)90227-X	discrete mathematics;methodology;data mining;mathematics;algorithm	DB	-16.643114591529162	22.486263768885053	97621
44a0707cb6e8318c9c0dfbc4af6295525423c501	krivine machines and higher-order schemes	krivine machine model;higher-order pushdown automaton;global model checking problem;pushdown automaton;higher-order recursive scheme;new proof;literature use automata model;new approach;higher-order scheme;simply typed lambda calculus	We propose a new approach to analysing higher-order recursive schemes. Many results in the literature use automata models generalising pushdown automata, most notably higher-order pushdown automata with collapse (CPDA). Instead, we propose to use the Krivine machine model. Compared to CPDA, this model is closer to lambdacalculus, and incorporates nicely many invariants of computations, as for example the typing information. The usefulness of the proposed approach is demonstrated with new proofs of two central results in the field: the decidability of the local and global model checking problems for higher-order schemes with respect to the mu-calculus.	automata theory;computation;invariant (computer science);modal μ-calculus;model checking;pushdown automaton;recursion;stack (abstract data type)	Sylvain Salvati;Igor Walukiewicz	2011		10.1007/978-3-642-22012-8_12	combinatorics;discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-10.76752830091642	21.228340924822493	97673
ed6dd1c3115410e67c12808ff4a00313d16dbcb5	static semantic analysis and theorem proving for casl	algebraic specification;abstract syntax tree;theorem proving;first order logic;semantic analysis	This paper presents a static semantic analysis for CASL, the Common Algebraic Speciication Language. Abstract syntax trees are generated including subsorts and overloaded functions and predicates. The static semantic analysis, through the implementation of an overload resolution algorithm, checks and qualiies these abstract syntax trees. The result is a fully qualiied CASL abstract syntax tree where the overloading has been resolved. This abstract syntax tree corresponds to a theory in the institution underlying CASL, subsorted partial rst-order logic with sort generation constraints (SubPCFOL). Two ways of embedding SubPCFOL in higher-order logic (HOL) of the logical framework Isabelle are discussed: the rst one from SubPFOL to HOL via PFOL (partial rst-order logic) rst drops subsorting and then partiality, and the second one is the counterpart via SubFOL (sub-sorted rst-order logic). The C in SubPCFOL stands for sort generation constraints, which are translated separately. Finally, we sketch an integration of the embedding of CASL into the UniForM Workbench.	abstract syntax tree;algorithm;automated theorem proving;function overloading;hol (proof assistant);isabelle;logical framework;operator overloading;parse tree;workbench	Till Mossakowski;Kolyang;Bernd Krieg-Brückner	1997		10.1007/3-540-64299-4_43	computer science;first-order logic;automated theorem proving;programming language;algorithm;abstract syntax tree	Logic	-18.40525634345037	19.67126941169719	97715
35efa21cccd45b236eb2f4b4b3dc195950b2ff97	a logic and decision procedure for predicate abstraction of heap-manipulating programs	verificacion modelo;lien hypertexte;mise a jour;linked data;securite;software model checking;enlace hipertexto;pervasive computing;cerradura transitiva;chainage donnee;verification modele;hyperlink;interpretacion abstracta;prise decision;program verification;safety properties;actualizacion;informatica difusa;verificacion programa;marcador;model checking;pointer;fermeture transitive;decision procedure;informatique diffuse;indecidibilidad;estructura datos;safety;data link;pointeur;transitive closure;structure donnee;undecidability;indecidabilite;interpretation abstraite;predicate abstraction;abstract interpretation;toma decision;verification programme;seguridad;data structure;updating;ligazon datos	An important and ubiquitous class of programs are heap-manipulating programs (HMP), which manipulate unbounded linked data structures by following pointers and updating links. Predicate abstraction has proved to be an invaluable technique in the field of software model checking; this technique relies on an efficient decision procedure for the underlying logic. The expression and proof of many interesting HMP safety properties require transitive closure predicates; such predicates express that some node can be reached from another node by following a sequence of (zero or more) links in the data structure. Unfortunately, adding support for transitive closure often yields undecidability, so one must be careful in defining such a logic. Our primary contributions are the definition of a simple transitive closure logic for use in predicate abstraction of HMPs, and a decision procedure for this logic. Through several experimental examples, we demonstrate that our logic is expressive enough to prove interesting properties with predicate abstraction, and that our decision procedure provides us with both a time and space advantage over previous approaches.	data structure;decision problem;host media processing;linked data;model checking;predicate abstraction;transitive closure	Jesse D. Bingham;Zvonimir Rakamaric	2006		10.1007/11609773_14	predicate logic;functional predicate;model checking;description logic;pointer;higher-order logic;data structure;data link;computer science;artificial intelligence;theoretical computer science;predicate functor logic;linked data;database;mathematics;hyperlink;predicate variable;predicate;programming language;transitive closure;algorithm	PL	-17.5528715200579	23.81247500047191	97804
6ae3008dd96d7272761b3e1d5095b36b3b3cca97	on combining formal and informal verification	algorithmique;resource use;algorithmics;algoritmica;informatique theorique;computer theory;informatica teorica	"""We propose algorithms which combine simulation with symbolic methods for the veriication of invariants. The motivation is twofold. First, there are designs which are too complex to be formally veri-ed using symbolic methods; however the use of symbolic techniques in conjunction with traditional simulation results in better \coverage"""" relative to the computational resources used. Additionally, even on designs which can be symbolically veriied, the use of a hybrid methodology often detects the presence of bugs faster than either formal veriication or simulation."""	algorithm;computation;computational resource;invariant (computer science);simulation;software bug	Jun Yuan;Jian Shen;Jacob A. Abraham;Adnan Aziz	1997		10.1007/3-540-63166-6_37	computer science;theoretical computer science;algorithmics;symbolic trajectory evaluation;algorithm	Logic	-16.44425362129124	27.657365640314726	98123
16aced4ac4c6046aeaa8a6bef2c0d017dbe543ac	finitary logics for coalgebras with branching	thesis	The purpose of this dissertation is to further previous work on coalgebras as infinite statebased transition systems and their logical characterisation with particular focus on infinite regular behaviour and branching. Finite trace semantics is well understood [DR95] for nondeterministic labelled transition systems, and has recently [Jac04, HJS06] been generalised to a coalgebraic level where monads act as branching types for instance, of nondeterministic choice. Finite trace semantics then arises through an inductive construction in the Kleisli-category of the monad. We provide a more comprehensive definition of finite trace semantics, allowing for finitary branching types in Chapter 5. In Chapter 6 we carry over the ideas behind our definition of finite trace semantics to define infinite trace semantics. Coalgebraic logics [Mos99] provide one approach to characterising states in coalgebras up to bisimilarity. Coalgebraic logics are Boolean logics with the modality ∇. We define the Boolean dual of ∇ in the negation-free fragment of finitary coalgebraic logics in Chapter 7, showing that finitary coalgebraic logics are essentially negation free. Our proof is largely based on the previously established completeness of finitary coalgebraic logics [KKV08]. Finite trace semantics induces the notion of finite trace equivalence. In Chapter 8 we define coalgebraic logics for many relevant branching and transition types characterising states of coalgebras with branching up to finite trace equivalence. Under further assumptions we show that these logics are expressive. Coalgebra automata allow us to state finitary properties over infinite structures essentially by a fix-point style construction. We use the dualisation of ∇ from Chapter 7 to iii iv ABSTRACT prove that coalgebra automata are closed under complementation in Chapter 10. This result completes a Rabin style [Rab69] correspondence between finitary coalgebraic logics and coalgebra automata for finitary transition types, begun in [Ven04, KV05]. The semantics of coalgebra automata is given in terms of parity graph games [GTW02]. In Chapter 9 we show how to structure parity graph games into rounds using the notion of players power [vB02] and how to normalise the interaction pattern between the players per round. From the latter we obtain the coinductive principle of game bisimulation. Languages accepted by coalgebra automata are called regular. Regularity is commonly [Sip96, HMU03] disproved using the pumping lemma for regular languages. We define regular languages of coalgebras and prove a pumping lemma for these languages in Chapter 11.	automata theory;automaton;bisimulation;coinduction;denotational semantics;description logic;interaction design pattern;monad (functional programming);pumping (computer systems);pumping lemma for context-free languages;pumping lemma for regular languages;regular language;t-norm fuzzy logics;turing completeness	Christian Kissig	2012			combinatorics;discrete mathematics;mathematics;algebra	Logic	-6.64119085637482	21.382809968677446	98317
e64bb0b7b6c5abaf0e195da33db413c2c74a2316	an smt approach to bounded model checking of design in state transition matrix	distributed system;satisfiability modulo theories bounded model checking design state transition matrix table based modeling language distributed systems functional correctness;simulation languages computability design distributed processing matrix algebra;state transition matrix;computability;prototypes;system analysis and design;distributed processing;distributed computing;invariant properties;semantics;matrix algebra;computer industry;surface mount technology;modeling language;bounded model checking;indexes;computer aided software engineering;formal verification;satisfiability modulo theories state transition matrix bounded model checking invariant properties;unified modeling language;message passing;functional correctness;design;simulation languages;surface mount technology encoding formal verification computer industry prototypes distributed computing cats message passing embedded software computer aided software engineering;distributed systems;satisfiability modulo theories;cats;table based modeling language;encoding;state transition;embedded software	State Transition Matrix (STM) is a table-based modeling language that has been frequently used in industry for specifying behavior of distributed systems. Functional correctness of a STM design (i.e., a design written in STM) could usually be expressed as invariant properties. In this paper, we first present a formalization of the static and dynamic aspects of a STM design. Consequentially, based on this formalization, we investigate a symbolic encoding approach for STM design, through which the design could be bounded model checked wrt. invariant properties by using Satisfiability Modulo Theories (SMT) solving technique. We have built a prototype implementation of the proposed encoding and the state-of-the-art SMT solver -- Yices is used in our experiments to evaluate the effectiveness of our approach.	correctness (computer science);distributed computing;experiment;invariant (computer science);model checking;modeling language;modulo operation;prototype;satisfiability modulo theories;software transactional memory;solver;state transition table;stochastic matrix	Weiqiang Kong;Tomohiro Shiraishi;Yuki Mizushima;Noriyuki Katahira;Akira Fukuda;Masahiko Watanabe	2010	2010 International Conference on Computational Science and Its Applications	10.1109/ICCSA.2010.57	unified modeling language;design;discrete mathematics;message passing;embedded software;formal verification;computer science;theoretical computer science;operating system;database;mathematics;semantics;computability;modeling language;programming language;satisfiability modulo theories;algorithm;encoding	SE	-16.327196207490925	29.744635274230617	98422
4164f5648a79a026604b01117645822479b2ec1e	on modal μ-calculus over finite graphs with small components or small tree width	strongly connected component;tree width;modal μ calculus	This paper is a continuation and correction of a paper presented by the same authors at the conference GANDALF 2010. We consider the Modal μ-calculus and some fragments of it. For every positive integer k we consider the class SCCk of all finite graphs whose strongly connected components have size at most k, and the class TWk of all finite graphs of tree width at most k. As upper bounds, we show that for every k, the temporal logic CTL collapses to alternation free μ-calculus in SCCk; and in TW1, the winning condition for parity games of any index n belongs to the level ∆2 of Modal μ-calculus. As lower bounds, we show that Büchi automata are not closed under complement in TW2 and coBüchi nondeterministic and alternating automata differ in TW1.	alternating finite automaton;automata theory;büchi automaton;continuation;modal logic;modal μ-calculus;strongly connected component;temporal logic;treewidth	Giovanna D'Agostino;Giacomo Lenzi	2012	Int. J. Found. Comput. Sci.	10.1142/S012905411240031X	modal μ-calculus;combinatorics;discrete mathematics;computer science;mathematics;treewidth;strongly connected component;algorithm	Logic	-5.06037836567605	19.981161831192523	98473
3a6d143b2dc1fb57c2d39fa1ba60c6ec2f5a8d6e	pkorat: parallel generation of structurally complex test inputs	automated testing;software testing;state space methods;formal specification;heart;parallel algorithm;complex structural constraints;program testing formal specification java;java programming;availability;search space;probability density function;automatic testing;complex structure;space exploration;automatic testing java space exploration software testing parallel algorithms state space methods sequential analysis heart availability multicore processing;sequential analysis;testing;satisfiability;data mining;binary trees;structural complexity;parallel generation;program testing;multicore processing;state space;complex structural constraints parallel generation automated testing pkorat java programs parallel algorithm;pkorat;constraint solving;java programs;program processors;java;parallel algorithms	Constraint solving lies at the heart of several specification-based approaches to automated testing. Korat is a previously developed algorithm for solving constraints in Java programs. Given a Java predicate that represents the desired constraints and a bound on the input size, Korat systematically explores the bounded input space of the predicate and enumerates inputs that satisfy the constraint. Korat search is largely sequential: it considers one candidate input in each iteration and it prunes the search space based on the candidates considered. This paper presents PKorat, a new parallel algorithm that parallelizes the Korat search. PKorat explores the same state space as Korat but considers several candidates in each iteration. These candidates are distributed among parallel workers resulting in an efficient parallel version of Korat. Experimental results using complex structural constraints from a variety of subject programs show significant speedups over the traditional Korat search.	central processing unit;constraint satisfaction problem;directed acyclic graph;imperative programming;information;iteration;java;large eddy simulation;multi-core processor;parallel algorithm;parallel computing;path expression;queue (abstract data type);reyes rendering;scalability;sequential algorithm;speedup;state space;symbolic execution;test automation	Junaid Haroon Siddiqui;Sarfraz Khurshid	2009	2009 International Conference on Software Testing Verification and Validation	10.1109/ICST.2009.48	real-time computing;computer science;theoretical computer science;operating system;software engineering;parallel algorithm;software testing;programming language	SE	-18.01929141259462	29.786057273639734	98724
aca5e5711dc126249d618e17c7e75152b517c49f	higher-order uncurrying	formal specification;operational semantics;program transformation;source term;higher order;polymorphism;program analysis;higher order functions;functional language;type system	We present a formal specification of unCurrying for a higherorder, functional language with ML-style let-polymorphism. This specification supports the general unCurrying of functions, even for functions which are passed as arguments or returned as values. The specification also supports partial unCurrying of any consecutive parameters of a function, rather than only unCurrying all of a function's parameters. We present the specification as a deductive system which axiomatizes a judgment relating a source term with an unCurried form of the term. We prove that this system relates only typable terms and that it is correct with respect to an operational semantics. We define a practical algorithm, based on algorithm W, which implements the unCurrying and prove this algorithm sound and complete with respect to the deductive system. This algorithm generates maximally unCurried forms of source terms. These results provide a declarative framework for reasoning about unCurrying and support a richer form of unCurrying than is currently found in compilers for functional languages.	algorithm;compiler;correctness (computer science);currying;formal specification;formal system;functional programming;hindley–milner type system;operational semantics	John Hannan;Patrick Hicks	1998		10.1145/268946.268947	program analysis;polymorphism;higher-order logic;type system;accidental release source terms;computer science;theoretical computer science;formal specification;programming language;functional programming;operational semantics;higher-order function;algorithm	PL	-17.21608994190106	21.40082730994543	99014
7749b57ce159da61c6543e9ef7a86f93a5276474	on the expressive power of temporal concurrent constraint programming languages (invited talk)	concurrent constraint programming;expressive power;finite domain	Abstract   The tcc paradigm is a formalism for timed concurrent constraint programming. Several tcc languages differing in their way of expressing infinite behavior have been proposed in the literature. In this work we study the expressive power of some of these languages. In particular, we show that: (1) recursive procedures with parameters can be encoded into parameterless recursive procedures with dynamic scoping, and viceversa. (2) replication can be encoded into parameterless recursive procedures with static scoping, and viceversa. (3) the languages from (1) are strictly more expressive than the languages from (2). Furthermore, we show that behavioral equivalence is undecidable for the languages from (1), but decidable for the languages from (2). The undecidability result holds even if the process variables take values from a fixed finite domain.  (Joint work with Mogens Nielsen and Frank D. Valencia)	concurrent constraint logic programming;constraint programming	Catuscia Palamidessi	2002	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80359-7	discrete mathematics;computer science;third-generation programming language;abstract family of languages;fifth-generation programming language;programming language;expressive power;comparison of multi-paradigm programming languages;algorithm	Logic	-11.783756049627723	20.166208456219028	99210
f67ee1759466c7cdbcb122a583e72370dc066c45	full abstraction in structural operational semantics (extended abstract)	full abstraction;extended abstract;structural operational semantics	This paper explores the connection between semantic equivalences for concrete sequential processes, represented by means of transition systems, and formats of transition system specifications using Plotkin’s structural approach. For several equivalences in the linear time — branching time spectrum a format is given, as general as possible, such that this equivalence is a congruence for all operators specifiable in that format. And for several formats it is determined what is the coarsest congruence with respect to all operators in this format that is finer than partial or completed trace equivalence.	denotational semantics;operational semantics	Rob J. van Glabbeek	1993			computer science;theoretical computer science;algorithm	Logic	-12.402869932430578	21.669275811591262	99319
8cad1d123ffd8993d5a139c6ed8b7ccb283ef168	conformance testing of boolean programs with multiple faults	complete test suite;finite automaton;incorrect implementation;erroneous implementation;r fault;conformance testing;multiple fault;boolean program;test suite;different output;correct implementation	Conformance testing is the problem of constructing a complete test suite of inputs based on a specification S such that any implementation I (of size less than a given bound) that is not equivalent to S gives a different output on the test suite than S. Typically I and S are assumed to be some type of finite automata. In this paper we consider the problem of constructing test suites for boolean programs (or more precisely modular visibly pushdown automata) that are guaranteed to catch all erroneous implementations that have at least R faults, and pass all correct implementations; if the incorrect implementation has fewer than R faults then the test suite may or may not detect it. We present a randomized algorithm for the construction of such test suites, and prove the near optimality of our test suites by proving lower bounds on the size of test suites.	automata theory;conformance testing;finite-state machine;model checking;pushdown automaton;randomized algorithm;stack (abstract data type);test suite;with high probability	Pavithra Prabhakar;Mahesh Viswanathan	2012		10.1007/978-3-642-30793-5_7	computer science;theoretical computer science;test case;algorithm	Logic	-12.882587247273129	28.088914155655438	99324
7023d7aa5a7aaf422de86edfdc03bd8eb6b8a6fb	verification of imperative programs by transforming constraint logic programs		We present a method for verifying partial correctness properties of imperative programs that manipulate integers and arrays by using techniques based on the transformation of constraint logic programs (CLP). We use CLP as a metalanguage for representing imperative programs, their executions, and their properties. First, we encode the correctness of an imperative program, say prog, as the negation of a predicate incorrect defined by a CLP program T . By construction, incorrect holds in the least model of T if and only if the execution of prog from an initial configuration eventually halts in an error configuration. Then, we apply to program T a sequence of transformations that preserve its least model semantics. These transformations are based on well-known transformation rules, such as unfolding and folding, guided by suitable transformation strategies, such as specialization and generalization. The objective of the transformations is to derive a new CLP program TransfT where the predicate incorrect is defined either by (i) the fact ‘incorrect.’ (and in this case prog is not correct), or by (ii) the empty set of clauses (and in this case prog is correct). In the case where we derive a CLP program such that neither (i) nor (ii) holds, we iterate the transformation. Since the problem is undecidable, this process may not terminate. We show through examples that our method can be applied in a rather systematic way, and is amenable to automation by transferring to the field of program verification many techniques developed in the field of program transformation.	computer programming;correctness (computer science);encode;formal verification;imperative programming;iteration;logic programming;partial template specialization;program transformation;terminate (software);undecidable problem;unfolding (dsp implementation);verification and validation	Emanuele De Angelis;Fabio Fioravanti;Alberto Pettorossi;Maurizio Proietti	2013			empty set;undecidable problem;negation;predicate (grammar);if and only if;algorithm;correctness;metalanguage;mathematics;program transformation	PL	-17.348606608556562	22.59037773843479	99801
a22e66f13cbf25002837329d344f35bfef97ef11	a little bit infinite? on adding data to finitely labelled structures (abstract)	004	Finite or infinite strings or trees with labels from a finite alphabet play an important role in computer science. They can be used to model many interesting objects including system runs in Automated Verification and XML documents in Database Theory. They allow the application of formal tools like logical formulas to specify properties and automata for their implementation. In this framework, many reasoning tasks that are undecidable for general computational models can be solved algorithmically, sometimes even efficiently. Nevertheless, the use of finitely labelled structures usually requires an early abstraction from the real data. For example, theoretical research on XML processing very often concentrates on the document structure (including labels) but ignores attribute or text values. While this abstraction has led to many interesting results, some aspects like key or other integrity constraints can not be adequately handled. In Automated Verification of software systems or communication protocols, infinite domains occur even more naturally, e.g., induced by program data, recursion, time, communication or by unbounded numbers of concurrent processes. Usually one approximates infinite domains by finite ones in a very early abstraction step. An alternative approach that has been investigated in recent years is to extend strings and trees by (a limited amount of) data and to use logical languages with a restricted expressive power concerning this data. As an example, in the most simple setting, formulas can only test equality of data values. The driving goal is to identify logical languages and corresponding automata models which are strong enough to describe interesting properties of data-enhanced structures while keeping decidability or even feasibility of automatic reasoning. The talk gives a basic introduction into data-enhanced finitely labelled structures, presents examples of their use, and highlights recent decidability and complexity results.	algorithm;automata theory;automated reasoning;automaton;computational model;computer science;data integrity;database theory;expressive power (computer science);key (cryptography);recursion;software system;string (computer science);undecidable problem;xml	Thomas Schwentick	2008		10.4230/LIPIcs.STACS.2008.1325	combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics;programming language;algorithm	PL	-13.246596970334	24.553989240173962	99864
953ae8ef6637376065213a990e995b033826fecf	a kernel language for algebraic specification and implementation - extended abstract	algebraic specification;extended abstract;kernel language	A kernel specification language called ASL is presented. ASL comprises five fundamental but powerful specification-building operations and has a simple semantics. Behavioural abstraction with respect to a set of observable sorts can be expressed, and (recursive) parameterised specifications can be defined using a more powerful and more expressive parameterisation mechanism than usual. A simple notion of implementation permitting vertical and horizontal composition (i.e. it is transitive and monotonic) is adopted and compared with previous more elaborate notions. A collection of identities is given which can provide a foundation for the development of programs by transformation.	algebraic specification	Donald Sannella;Martin Wirsing	1983		10.1007/3-540-12689-9_122	discrete mathematics;specification language;computer science;theoretical computer science;programming language;programming language specification	Theory	-14.035390210577022	19.65277583593298	99937
7a4849b54a82eb9c86710b320d628f5c35a7975a	a clausal resolution method for extended computation tree logic ectl	automated deduction;linear time temporal logic;resolution;temporal logic;temporal resolution;computation tree logic;branching time;program specification and verification;normal form;program specification	A temporal clausal resolution method was originally developed for linear time temporal logic and further extended to the branching-time framework of Computation Tree Logic (CTL). In this paper, following our general idea to expand the applicability of this efficient method to more expressive formalisms useful in a variety of applications in computer science and AI requiring branching time logics, we define a clausal resolution technique for Extended Computation Tree Logic (ECTL). The branching-time temporal logic ECTL is strictly more expressive than CTL in allowing fairness operators. The key elements of the resolution method for ECTL, namely the clausal normal form, the concepts of step resolution and a temporal resolution, are introduced and justified with respect to this new framework. Although in developing these components we incorporate many of the techniques defined for CTL, we need novel mechanisms in order to capture fairness together with the limit closure property of the underlying tree models. We accompany our presentation of the relevant techniques by examples of the application of the temporal resolution method. Finally, we provide a correctness argument and consider future work discussing an extension of the method yet further, to the logic CTL ∗ , the most powerful logic of this class.	computation tree logic;resolution (logic)	Alexander Bolotov;Artie Basukoski	2006	J. Applied Logic	10.1016/j.jal.2005.06.003	dynamic logic;discrete mathematics;linear temporal logic;description logic;probabilistic ctl;resolution;temporal logic;interval temporal logic;computation tree logic;computer science;artificial intelligence;temporal resolution;theoretical computer science;mathematics;fair computational tree logic;ctl*;programming language;substructural logic;multimodal logic;algorithm;temporal logic of actions	Logic	-13.072275332913465	21.902750320585447	99955
22184a85fcf5f2f9f5151c3a32bb007653f15a28	busy and lazy fp with infinite objects	rewrite rule;operational semantics;rewrite systems;data structure;functional programming language	The paper introduces a variant of Backus' functional programming language FP that has non-strict basic operations as well as non-strict language constructs.  The basic data structure of FP, finite nested sequences, is generalized to infinite trees by allowing a non-strict sequence constructor. Then infinite objects can be described as least solutions of recursion equations.  For this language variant we give a structured mathematical and operational semantics which employs rewriting rules on finite terms. Infinite objects are evaluated by repeated unfolding of their recursive definitions and subsequent simplifications according to a confluent and Noetherian rewriting system on the level of objects. Two algorithms are given that formalize the ideas of busy (data-driven) and lazy (demand-driven) evaluation.  Throughout the paper the semantic concepts of FP are explained; special emphasis is laid on the notion and the proper algebraic treatment of infinite objects.	algorithm;confluence (abstract rewriting);data structure;functional programming;lazy evaluation;linear algebra;operational semantics;recursion;rewriting;strict function;strict programming language;unfolding (dsp implementation)	Walter Dosch;Bernhard Möller	1984		10.1145/800055.802045	data structure;computer science;theoretical computer science;programming language;functional programming;operational semantics;algorithm	PL	-13.450415166045804	21.637481447487517	100109
2a8f1899f8e3cdec5117fddd92a3b34569300051	on the complexity of bisimilarity of normed probabilistic context-free processes	computational complexity;context-free languages;decidability;graph theory;minimisation;probability;pspace;bisimilarity complexity;context-free process graph;minimization;normed probabilistic context-free processes;polynomial-time hierarchy;probabilistic bisimulation equivalence	Selectivity p. 55 An O(n log[subscript 2] n) Hybrid Sorting Algorithm on 2-D Grid p. 60 Far Field Path Planning for Rotorcraft NOE Missions in a Mountainous Region p. 65 Interpretive Language Implementation from a Layered Operational Model p. 71 On Space Bounded Server Algorithms p. 77 An Efficient Parallel Recognition Algorithm of Parity Graphs p. 82 A Combinatorial View of Visibility Graphs of Simple Polygons p. 87 Analysis of Windowing Operations or R+ Trees p. 93 Minimize Linear Mutual Recursion by Rule Unfolding p. 98 Self-Stabilizing Fault Location p. 103 Fixed-Path Proxy-Logins with Assured Termination p. 111 On a Communication Software Generation Method from Communication Service Specifications Described by a Declarative Language p. 116	bisimulation;declarative programming;motion planning;mutual recursion;r language;selectivity (electronic);sorting algorithm;visibility graph	Dung T. Huynh;Lu Tian	1993			combinatorics;discrete mathematics;probabilistic analysis of algorithms;mathematics;algorithm	Theory	-4.802242278938998	23.274069375663192	100164
9f759f79ec1009a778b427c75e229a37856b8522	the complexity of automated addition of fault-tolerance without explicit legitimate states	automated formal methods;model repair;program synthesis;fault tolerance	Existing algorithms for automated model repair for adding fault-tolerance to fault-intolerant models incur an impediment that designers have to identify the set of legitimate states of the original model. This set determines states from where the original model meets its specification in the absence of faults. Experience suggests that of the inputs required for model repair, identifying such legitimate states is the most difficult. In this paper, we consider the problem of automated model repair for adding fault-tolerance where legitimate states are not explicitly given as input. We show that without this input, in some instances, the complexity of model repair increases substantially (from polynomial-time to NP-complete). In spite of this increase, we find that this formulation is relatively complete; i.e., if it was possible to perform model repair with explicit legitimate states, then it is also possible to do so without the explicit identification of the legitimate states. Finally, we show that if the problem of model repair can be solved with explicit legitimate states, then the increased cost of solving it without explicit legitimate states is very small. In summary, the results in this paper identify instances of automated addition of fault-tolerance, where the explicit knowledge of legitimate state is beneficial and where it is not very crucial.	algorithm;byzantine fault tolerance;complexity class;computation;fault tolerance;liveness;model checking;np-completeness;polynomial;time complexity;yet another	Fuad Abujarad;Yiyan Lin;Borzoo Bonakdarpour;Sandeep S. Kulkarni	2014	Distributed Computing	10.1007/s00446-014-0227-2	fault tolerance;simulation;computer science;artificial intelligence;distributed computing;algorithm	SE	-13.651182049797946	28.886696706408614	100200
50eaf177fc9afce3076c2dc7560e52199cf76377	context-bounded model checking with esbmc 1.17	qa75 electronic computers computer science	ESBMC is a context-bounded symbolic model checker for singleand multi-threaded ANSI-C code. It converts the verification conditions using d fferent background theories and passes them directly to an SMT solver.	model checking;satisfiability modulo theories;solver;theory;thread (computing)	Lucas C. Cordeiro;Jeremy Morse;Denis A. Nicole;Bernd Fischer	2012		10.1007/978-3-642-28756-5_42	computer architecture;computer science;theoretical computer science;programming language	Logic	-18.043630232163327	27.125265023827208	100322
2066b6088984aa87a751fbfb7274ace874ca986f	a new perspective on integrating functional and logic languages		Traditionally the integration of functional and logic languages is performed by attempting to integrate their semantic logics in some way. Many languages have been developed by taking this approach, but none manages to exploit fully the programming features of both functional and logic languages and provide a smooth integration of the two paradigms. We propose that improved integrated systems can be constructed by taking a broader view of the underlying semantics of logic programming. A novel integrated language paradigm, Deenitional Constraint Programming (DCP), is proposed. DCP generalises constraint logic programming by admitting user-deened functions via a purely functional subsystem and enhances it with the power to solve constraints over functional programs. This constraint approach to integration results in a homogeneous uniied system in which functional and logic programming features are combined naturally.	constraint logic programming;constraint programming;functional programming;programming paradigm	John Darlington;Yike Guo;Helen Pull	1992			and gate;functional logic programming;theoretical computer science;computer science;ontology language	PL	-17.41715301195911	19.22345540932461	100596
75e4a1272b00add2a14ddf69faec5e7f03683e43	model checking of open interval markov chains	public records;websearch;rwth publications	We consider the model checking problem for interval Markov chains with open intervals. Interval Markov chains are generalizations of discrete time Markov chains where the transition probabilities are intervals, instead of constant values. We focus on the case where the intervals are open. At first sight, open intervals present technical challenges, as optimal (min, max) value for reachability may not exist. We show that, as far as model checking (and reachability) is concerned, open intervals does not cause any problem, and with minor modification existing algorithms can be used for model checking interval Markov chains against	algorithm;artificial intelligence;closing (morphology);computation;computational complexity theory;convex hull;decision problem;european joint conferences on theory and practice of software;execution unit;existential theory of the reals;expert system;farmville;first-order logic;first-order reduction;lecture notes in computer science;linear algebra;linear equation;map;markov chain;markov decision process;maxima and minima;model checking;probabilistic ctl;reachability;semiconductor industry;springer (tank);strict function;system of linear equations;umc (computer)	Souymodip Chakraborty;Joost-Pieter Katoen	2015		10.1007/978-3-319-18579-8_3	markov chain;combinatorics;discrete mathematics;public records;computer science;mathematics;markov model;algorithm	Logic	-6.8970205071138615	24.37638343210871	100632
cf33343fc9fddfaabf78976134fac7585c4a8076	an experiment in ping-pong protocol verification by nondeterministic pushdown automata		An experiment is described that confirms the security of a well-studied class of cryptographic protocols (Dolev-Yao intruder model) can be verified by two-way nondeterministic pushdown automata (2NPDA). A nondeterministic pushdown program checks whether the intersection of a regular language (the protocol to verify) and a given Dyck language containing all canceling words is empty. If it is not, an intruder can reveal secret messages sent between trusted users. The verification is guaranteed to terminate in cubic time at most on a 2NPDA-simulator. The interpretive approach used in this experiment simplifies the verification, by separating the nondeterministic pushdown logic and program control, and makes it more predictable. We describe the interpretive approach and the known transformational solutions, and show they share interesting features. Also noteworthy is how abstract results from automata theory can solve practical problems by programming language means.	apl;array data structure;automata theory;control flow;cryptographic protocol;cryptography;declarative programming;dolev–yao model;electronic proceedings in theoretical computer science;flow chart language (fcl);flowchart;formal language;functional programming;fuzzy logic;general-purpose markup language;high- and low-level;imperative programming;machine code;model of computation;nondeterministic algorithm;nondeterministic finite automaton;nondeterministic programming;ping-pong scheme;polynomial;production (computer science);program transformation;programming language;programming model;pushdown automaton;recursion;stack (abstract data type);time complexity;transformer;usability;yao graph	Robert Glück	2018	CoRR		nondeterministic algorithm;theoretical computer science;ping (video games);regular language;pushdown automaton;dyck language;cryptographic protocol;computer science;automata theory	Logic	-19.106109960382042	25.75747528521556	100686
c97e073ffd7a7c57e42114277d0349a219f1df9d	an interactive verification system based on dynamic logic	dynamic logic;state transition	"""An interactive verification system based on dynamic logic is presented. This approach allows to strengthen the role of """"dynamic reasoning"""", i.e. reasoning in terms of state transitions caused by programs."""		Reiner Hähnle;Maritta Heisel;Wolfgang Reif;Werner Stephan	1986		10.1007/3-540-16780-3_99	dynamic logic;logic synthesis;real-time computing;logic optimization;computer science;artificial intelligence;theoretical computer science;high-level verification;multimodal logic	EDA	-12.76916601862681	23.62161159304001	100873
3f55368b3eb80ce2fdff8fb0d37fee9be44980a1	tableau-based automated deduction for duration calculus	automated deduction;systeme temps reel;logica temporal;tableau calculus;temporal logic;calcul tableau;logical programming;sistema reactivo;duration calculus;programmation logique;reactive system;systeme reactif;real time system;decidibilidad;sistema tiempo real;decidabilite;programacion logica;logique temporelle;decidability;real time systems	Duration Calculus is a temporal logic introduced to specify real-time systems. It is a very expressive but undecidable logic. In this paper we turn our attention to a decidable fragment for which we develop a tableau-based decision method taking into account some semantic restrictions.	automated theorem proving;duration calculus;method of analytic tableaux;natural deduction	Nathalie Chetcuti-Sperandio	2002		10.1007/3-540-45616-3_5	decidability;duration calculus;description logic;real-time operating system;temporal logic;reactive system;computer science;artificial intelligence;mathematics;proof calculus;situation calculus;natural deduction;algorithm	Logic	-9.83569773788482	25.599611783886324	101242
461e890ef6ccf750d4821f6ff3003fecb37dea23	polyhedral-based dynamic loop pipelining for high-level synthesis		Loop pipelining is one of the most important optimization methods in high-level synthesis (HLS) for increasing loop parallelism. There has been considerable work on improving loop pipelining, which mainly focuses on optimizing static operation scheduling and parallel memory accesses. Nonetheless, when loops contain complex memory dependencies, current techniques cannot generate high performance pipelines. In this paper, we extend the capability of loop pipelining in HLS to handle loops with uncertain dependencies (i.e., parameterized by an undetermined variable) and/or nonuniform dependencies (i.e., varying between loop iterations). Our optimization allows a pipeline to be statically scheduled without the aforementioned memory dependencies, but an associated controller will change the execution speed of loop iterations at runtime. This allows the augmented pipeline to process each loop iteration as fast as possible without violating memory dependencies. We use a parametric polyhedral analysis to generate the control logic for when to safely run all loop iterations in the pipeline and when to break the pipeline execution to resolve memory conflicts. Our techniques have been prototyped in an automated source-to-source code transformation framework, with Xilinx Vivado HLS, a leading HLS tool, as the RTL generation backend. Over a suite of benchmarks, experiments show that our optimization can implement optimized pipelines at almost the same clock speed as without our transformations, running approximately 3.7– $10{\times }$  faster, with a reasonable resource overhead.	benchmark (computing);clock rate;compiler;experiment;graphics pipeline;high- and low-level;high-level synthesis;iteration;loop invariant;mathematical optimization;overhead (computing);parallel computing;pipeline (computing);polyhedral;program transformation;regular expression;run time (program lifecycle phase);scheduling (computing)	Junyi Liu;John Wickerson;Samuel Bayliss;George A. Constantinides	2018	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2017.2783363	clock rate;real-time computing;control theory;computer science;scheduling (computing);parameterized complexity;high-level synthesis;pipeline (computing);parametric statistics;control logic	EDA	-18.384019112361376	31.497302768910036	101359
2fbd5b73a8e9d55d59b34041a8013f88df4709f1	regular expressions revisited: a coinductive approach to streams, automata, and power series	power series;lenguaje programacion;preuve programme;program proof;programming language;differential equation;formal power series;informatique theorique;finite automata;prueba programa;langage programmation;lenguaje formal;regular expression;formal language;computer theory;informatica teorica;langage formel	Regular expressions are a standard means for denoting formal languages that are recognizable by finite automata. Much less familiar is the use of syntactic expressions for (formal) power series. Power series generalize languages by assigning to words multiplicities in any semiring (such as the reals) rather than just Booleans. and include as a special case the set of streams (infinite sequences). Here we shall define an extended set of regular expressions with multiplicities in an arbitrary semiring. The semantics of such expressions will be defined coinductively, allowing for the use of a syntactic coinductive proof principle. To each expression will be assigned a nondeterministic automaton with multiplicities, which usually is a rather efficient representation of the power series denoted by the expression. Much of the above will be illustrated for the special case of streams of real numbers; other examples include automata and languages (sets of words), and task-resource systems (using the max-plus semiring). The coinductive definitions mentioned above take the shape of what we have called behavioural differential equations, on the basis of which we develop, as a motivating example, a theory of streams in a calculus-like fashion Our perspective is essentially coalgebraic. More precisely, the set of all formal power series, including the set of languages and the set of streams as special instances, is a final coalgebra. This fact is the basis for both the coinduction definition and the coinduction proof principle For general background on coalgebra, see [1] and [2]. The proceedings of the recently established workshop series CMCS (Coalgebraic Methods in Computer Science), contained in Volumes 11, 19, and 33 of Elsevier's Electronic Notes in Theoretical Computer Science, give a good impression of many of the latest developments in coalgebraic studies. References related to the theory summarized above are [3], dealing with automata and languages, and [4], on formal power series. A technical report on behavioural differential equations is in preparation.	automaton;coinduction;regular expression	Jan J. M. M. Rutten	2000		10.1007/10722010_7	formal language;discrete mathematics;computer science;artificial intelligence;machine learning;database;mathematics;distributed computing;formal power series;finite-state machine;programming language;power series;differential equation;regular expression;algorithm	Logic	-4.794273056372248	18.700032851582275	101419
9a78880fbd869ba2d4dee7d2a6f6159618df90c7	incremental realization of safety requirements: non-determinism vs. modularity		This paper investigates the impact of non-determinism and modularity on the complexity of incremental incorporation of safety requirements while preserving liveness (a.k.a. the problem of incremental synthesis). Previous work shows that realizing safety in non-deterministic programs under limited observability is an NP-complete problem (in the state space of the program), where limited observability imposes read restrictions on program components with respect to the local state of other components. In this paper, we present a surprising result that synthesizing safety remains an NP-complete problem even for deterministic programs! The results of this paper imply that non-determinism is not the source of the hardness of synthesizing safety in concurrent programs; instead, limited observability has a major impact on the complexity of realizing safety. We also provide a roadmap for future research on exploiting the benefits of modularization while keeping the complexity of incremental synthesis manageable.	non-functional requirement	Ali Ebnenasir	2015		10.1007/978-3-319-24644-4_11	program synthesis;computer science;theoretical computer science;distributed computing;state space;determinism;observability;modular programming;modularity;liveness	SE	-14.429003731472896	27.3330920890801	101446
8abc0ec1620c6e9835951d54a02d539ac00fea14	on the boundary between decidability and undecidability of asynchronous session subtyping		Session types are behavioural types for guaranteeing that concurrent programs are free from basic communication errors. Recent work has shown that asynchronous session subtyping is undecidable. However, since session types have become popular in mainstream programming languages in which asynchronous communication is the norm rather than the exception, it is crucial to detect significant decidable subtyping relations. Previous work considered extremely restrictive fragments in which limitations were imposed to the size of communication buffer (at most 1) or to the possibility to express multiple choices (disallowing them completely in one of the compared types). In this work, for the first time, we show decidability of a fragment that does not impose any limitation on communication buffers and allows both the compared types to include multiple choices for either input or output, thus yielding a fragment which is more significant from an applicability viewpoint. In general, we study the boundary between decidability and undecidability by considering several fragments of subtyping. Notably, we show that subtyping remains undecidable even if restricted to not using output covariance and input contravariance.	programming language;undecidable problem	Mario Bravetti;Marco Carbone;Gianluigi Zavattaro	2018	Theor. Comput. Sci.	10.1016/j.tcs.2018.02.010	discrete mathematics;undecidable problem;covariance and contravariance;decidability;subtyping;asynchronous communication;mathematics;covariance	PL	-10.733141886009937	20.710174757926282	102233
3c80bc60ac2d4b1f69b2c56d1e34916a45c28a75	time petri nets with action duration: a true concurrency real-time model	maximality semantics;time petri nets with action duration dtpn;durational action timed automata data;actions duration;real time systems	The design of real-time systems needs a high-level specification model supporting at the same time timing constraints and actions duration. We introduce in this paper an extension of Petri Nets called Time Petri Nets with Action Duration (DTPN) where time is associated with transitions. In DTPN, the firing of transitions is bound to a time interval and transitions represent actions which have explicit durations. We give an operational semantics for DTPN in terms of Durational Action Timed Automata (DATA). DTPN considers both timing constraints and durations under a true-concurrency semantics with an aim of better expressing concurrent and parallel behaviours of real-time systems.	concurrency (computer science);concurrency semantics;high- and low-level;operational semantics;petri net;real-time clock;real-time computing;real-time transcription;timed automaton	Nabil Belala;Djamel-Eddine Saïdouni;Radja Boukharrou;Ahmed-Chawki Chaouche;A. Seraoui;A. Chachoua	2013	IJERTCS	10.4018/jertcs.2013040104	real-time computing;computer science;algorithm	Embedded	-10.955814374885303	25.661136732512514	102427
3bd9e68a008dab1bcf5850f2a8dbf7bb4b9e1994	from i/o automata to timed i/o automata	i o automata	The model of timed I/O automata represents an extension of the model of I/O automata with the aim of reasoning about realtime systems. A number of case studies using timed I/O automata has been carried out, among them a treatment of the so-called Generalized Railroad Crossing (GRC). An already existing formalization of the metatheory of I/O automata within Isabelle/HOLCF allows for fully formal tool-supported verification using I/O automata. We present a modification of this formalization which accomodates for reasoning about timed I/O automata. The guiding principle in choosing the parts of the metatheory of timed I/O automata to formalize has been to provide all the theory necessary for formalizing the solution to the GRC. This leads to a formalization of the GRC, in which not only the correctness proof itself has been formalized, but also the underlying meta-theory of timed I/O automata, on which the correctness proof is based.	automata theory;canonical account;care-of address;correctness (computer science);governance, risk management, and compliance;input/output;isabelle;semiconductor industry;simulation;timed automaton	Bernd Grobauer;Olaf Müller	1999		10.1007/3-540-48256-3_18	quantum finite automata;computer science;theoretical computer science;automata theory;ω-automaton;mobile automaton;timed automaton;algorithm	Embedded	-17.357708238629762	29.528601325365745	102818
bbd3ae62ab4721db21c57334341c2659ca13d73b	on liveness and controlled siphons in petri nets	linear algebra;structuration theory;petri net	Abs t rac t . Structure theory of Petri nets investigates the relationship between the behavior and the structure of the net. Contrary to linear algebraic techniques, graph based techniques fully exploit the properties of the flow relation of the net (pre and post sets). Liveness of a Petri net is closely related to the validation of certain predicates on siphons. In this paper, we study thoroughly the connections between siphons structures and liveness. We define the controlled-siphon property that generalizes the well-known Commoner's property, since it involves both traps and invariants notions. We precise some structural conditions under which siphons cannot be controlled implying the structural non-liveness. These conditions based on local synchronization patterns cannot be captured by linear algebraic techniques. We establish a graph-theoretical characterization of the non-liveness under the controlled-siphon property. Finally, we prove that the controlled-siphon property is a necessary and suiticient liveness condition for simple nets and asymmetric choice nets. All these results are illustrated by significant examples taken from literature.	graph theory;invariant (computer science);linear algebra;liveness;petri net;whole earth 'lectronic link	Kamel Barkaoui;Jean-François Pradat-Peyre	1996		10.1007/3-540-61363-3_4	discrete mathematics;stochastic petri net;computer science;linear algebra;distributed computing;petri net;algorithm	Logic	-9.390236856181678	20.86250618875591	102862
c48be1bcd40108325d0c8f0906b59a17504945fb	deterministic and stochastic modeling of parallel garbage collection - towards real-time criteria	real time;functional programming;garbage collection;garbage collector;stochastic model;logic programs	The study of garbage collection for a logic programming language machine has exhibited fundamental differences with the more popular functional programming garbage collection. These differences yield behaviours that cannot be observed with classical models. We give two new models, (one is determistic and the other stochastic) which take into account these behaviours. We argue that the stochastic model is also suitable for more classical garbage collector, and that it overcomes deterministic models for the study of the real-time property for parallel garbage collector. Finally, we argue that the methodology used to build and solve the stochastic model can be applied to the stochastic modeling of other systems.	functional programming;garbage collection (computer science);programming language;real-time clock;real-time transcription;stochastic modelling (insurance)	Olivier Ridoux	1987		10.1145/30350.30365	garbage;real-time computing;computer science;distributed computing;data pre-processing;garbage collection;programming language;functional programming	PL	-18.164658772088895	31.310093171102857	102895
ae8f33ac64498e35f5f9da14f2454a375ad60476	correctness verification of generalized algebraic deadlock avoidance policies through mathematical programming	sequential resource allocation systems;correctness verification;generalized algebraic deadlock avoidance policies;resource allocation;automaton based representation correctness verification generalized algebraic deadlock avoidance policies mathematical programming sequential resource allocation systems computational complexity petri net;deadlock avoidance;program verification;mathematical programming;computational complexity;state space;petri nets;automaton based representation;petri net;system recovery mathematical programming digital audio players design methodology resource management automation usa councils state space methods computational complexity vehicles;resource allocation computational complexity mathematical programming petri nets program verification;design methodology	Generalized algebraic deadlock avoidance policies (DAPs) for sequential resource allocation systems (RAS) have recently been proposed as an interesting extension of the class of algebraic DAPs, that maintains the analytical representation and computational simplicity of the latter, while it guarantees completeness with respect to the maximally permissive DAP. The original work of S. Reveliotis, et al., (2007) that introduced these policies also provided a design methodology for them, but this methodology is limited by the fact that it necessitates the deployment of the entire state space of the considered RAS. Hence, this paper seeks the development of alternative computational tools that can support the synthesis of correct generalized algebraic DAPs while controlling the underlying computational complexity. From a conceptual standpoint, the presented results are motivated by and extend similar past results for the synthesis of correct algebraic DAPs. However, when viewed from a more technical standpoint, the presented developments are complicated by the fact that generalized algebraic DAPs do not admit a convenient representation in the Petri net (PN) modeling framework, that has been the primary vehicle for the aforementioned past developments, and therefore, the relevant analysis must be pursued in an alternative, automaton-based representation of the RAS behavior and the applied policy logic. We believe that this translation of the past results in this new representational framework is a significant contribution in itself, since it enables a more profound understanding of the past developments, and at the same time, it renders them more accessible to the practitioner.	correctness (computer science);deadlock;mathematical optimization	Spyros A. Reveliotis;Elzbieta Roszkowska;Jin Young Choi	2007		10.1109/COASE.2007.4341690	discrete mathematics;computer science;theoretical computer science;algorithm	Robotics	-12.774465481990147	20.61201498958865	102980
326b0588cce9eaee26b960a107201927dc21437e	a characterization of context-free languages	context free language	"""The developments in the theory of languages and automata in recent years have been towards defining and studying new grammars, automata and languages, abstract families of automata, languages, translations, and so on. In spite of this fact, the central position of regular sets and context-free languages (CFL's) in the theory of languages remains, and so one can still expect their further study to be useful and maybe stimulating for the general development of language theory. One of the important advantages in dealing with regular languages is the fact that each such language can be denoted explicitly by a regular expression, in contrast to the implicit representation of languages by grammars. A similar characterization of CFL's would perhaps be useful also. However, one can hardly expect to get a characterization of CFL's so elegant and so simple as the one we have developed for regular languages. In Section 2 an attempt is made to give a regular-like characterization of CFL's, using union, product, and a new operation-symbol iteration. In so doing, we are compelled to use some """"auxiliary"""" symbols. Their role and number are discussed in Section 3. The application of the results of Section 2 to semilinear languages and their subclasses is presented in Section 4."""	automata theory;automaton;commutation theorem;context-free language;courant–friedrichs–lewy condition;iteration;regular expression;regular language	Jozef Gruska	1971	J. Comput. Syst. Sci.	10.1016/S0022-0000(71)80023-5	synthetic language;computer science;extended affix grammar;context-free language;ontology language;abstract family of languages;natural language;fifth-generation programming language;programming language;second-generation programming language;comparison of multi-paradigm programming languages;algorithm	Theory	-12.007927105215794	18.724265385501006	103083
7b0fa10e43ffc4aabf7ddf49cd0bac9c8c8670ef	on the expressive power of temporal logic for infinite words	palabra infinita;logica temporal;mot infini;langage w star free;language theory;temporal logic;teoria lenguaje;infinite word;expressive power;informatique theorique;logique temporelle;theorie langage;computer theory;informatica teorica	We study the expressive power of linear propositional temporal logic interpreted on finite sequences or words. We first give a transparent proof of the fact that a formal language is expressible in this logic if and only if its syntactic semigroup is finite and aperiodic. This gives an effective algorithm to decide whether a given rational language is expressible. Our main result states a similar condition for the “restricted” temporal logic (RTL), obtained by discarding the “until” operator. A formal language is RTL-expressible if and only if its syntactic semigroup is finite and satisfies a certain simple algebraic condition. This leads to a polynomial time algorithm to check whether the formal language accepted by an n-state deterministic automaton is RTL-expressible. Temporal logic is a particular case of modal logic. It was introduced by Pnueli [16] in connection with applications to the specification, development and verification of possibly parallel or non-deterministic processes. This logical language admits several variations, one of them being propositional linear temporal logic (PTL). It uses three connectives suggestively called “next”, “eventually” and “until”. In this paper we are interested in the descriptive power of propositional linear temporal logic and of a restriction of temporal logic (RTL) obtained by considering only the operators “next” and “eventually”. In both cases, we interpret temporal logic on finite words only. In this case, a temporal formula defines a set of words (that is, a formal language) and our problem is to determine precisely which formal languages can be specified in this way. In the case of PTL, the solution has been known for some time, as a consequence of a series of deep results. Indeed, Kamp [6] has shown that PTL is expressively equivalent to first-order logic when interpreted on words. Next, McNaughton [10] proved that a formal language is first-order definable if and only if it is star-free. Finally, star-free languages are characterized by a ∗Research on this paper was partially supported by PRC “Mathématiques et Informatique”.	algorithm;deterministic automaton;expressive power (computer science);first-order logic;first-order predicate;formal language;linear algebra;linear temporal logic;logic programming;logical connective;modal logic;p (complexity);parallel computing;pass transistor logic;polynomial;probabilistically checkable proof;regular language;star-free language;syntactic monoid	Joëlle Cohen-Chesnot	1991	Theor. Comput. Sci.	10.1016/0304-3975(91)90281-6	natural language processing;temporal logic;computer science;philosophy of language;mathematics;programming language;expressive power;algorithm	Logic	-6.489833309478494	19.54558473878543	103096
5f7c988dc7e93b4e44711fe7d970e4a05f083669	transformation and debugging of functional logic programs	functional logic programming;partial evaluation;first order;satisfiability;generic algorithm	The Italian contribution to functional-logic programming has been significant and influential in a number of areas of semantics, and semantics-based program manipulation techniques. We survey selected topics, with a particular regard to debugging and transformation techniques. These results as usual depend on the narrowing strategy which is adopted and on the properties satisfied by the considered programs. In this paper, we restrict ourselves to first-order functional-logic languages without non-deterministic functions. We start by describing some basic classical transformation techniques, namely folding and unfolding. Then, we recall the narrowing-driven partial evaluation, which is the first generic algorithm for the specialization of functional logic programs. Regarding debugging, we describe a goal-independent approach to automatic diagnosis and correction which applies the immediate consequence operator modeling computed answers to the diagnosis of bugs in functional logic programs. A companion bug-correction program synthesis methodology is described that attempts to correct the erroneous components of the wrong code.		María Alpuente;Demis Ballis;Moreno Falaschi	2010		10.1007/978-3-642-14309-0_13	computer science;theoretical computer science;algorithmic program debugging;algorithm	PL	-18.016667061047126	22.166234192308703	103149
1621c91578efb1b325911eb9530d66c9eee68612	simplifying pointer kleene algebra		Pointer Kleene algebra has proved to be a useful abstraction for reasoning about reachability properties and correctly deriving pointer algorithms. Unfortunately it comes with a complex set of operations and defining (in)equations which exacerbates its practicability with automated theorem proving systems but also its use by theory developers. Therefore we provide an easier access to this approach by simpler axioms and laws which also are more amenable to automatic theorem proving systems.	algorithm;automated theorem proving;kleene algebra;pointer (computer programming);reachability	Han-Hing Dang;Bernhard Möller	2011			kleene's recursion theorem;discrete mathematics;kleene star;pointer (computer programming);reachability;axiom;kleene algebra;algorithm;abstraction;automated theorem proving;mathematics	Logic	-17.569522670420845	24.097355829644187	103326
71fec0cbfe0558b0db3c0a4a37ccf829341528a5	analysis of discrete time deterministic and stochastic petri nets			petri net	Robert Zijal	1997				Logic	-5.902413124505414	25.09932914220084	103499
e681af62ac7361f1f277ab3e56efe34d4575e198	programming with singular and plural non-deterministic functions	non deterministic functions;maude;semantics;program transformation;functional programming;lenguajes de programacion;term rewriting	Non-strict non-deterministic functions are one of the most distinctive features of functional-logic languages. Traditionally, two semantic alternatives have been considered for this kind of functions: call-time choice and run-time choice. While the former is the standard choice of modern implementations of FLP, the latter lacks some basic properties--mainly compositionality--that have prevented its use in practical FLP implementations. Recently, a new compositional plural semantics for FLP has been proposed. Although this semantics allows an elegant encoding of some problems--in particular those with an implicit manipulation of sets of values--, call-time choice still remains the best option for many common programming patterns.  In this paper we explore the expressive possibilities of the combination of singular and plural non-determinism. After formalizing the intended semantics by means of a logic calculus, several significant examples exploiting the capabilities of the semantics are presented. These examples have been tested and developed in a Maude-based prototype whose implementation is outlined.	maude system;nondeterministic algorithm;prototype;strict function;windows fundamentals for legacy pcs	Adrián Riesco;Juan Rodríguez-Hortalá	2010		10.1145/1706356.1706373	computer science;theoretical computer science;semantics;programming language;well-founded semantics;functional programming;operational semantics;denotational semantics;algorithm	PL	-17.481100519274143	19.604752398090817	103829
c29198b8084913db3d37f11459da533e09a7690e	complexity of logical theories involving coprimality	limite superieure;nombre naturel;nombres relativement premiers;plenitud;complexite calcul;complejidad calculo;limite inferior;computing complexity;natural number;upper bound;theory;teoria;completeness;limite superior;completude;limite inferieure;lower bound;theorie	It is well known that complete number theory, i.e. the theory of the structure (FU, =, +, x ), is undecidable. When we drop one of the operations + or x , the theories Th(N, =, +) and Th(N, =, x) we obtain are decidable [19,16, 251. The computational complexity of these theories has been studied [3,4,8-l 1, 17, 18,20,21], eventually showing that Th( N, =, + ) is complete for uC,0 ATIME-ALT(2”“, n), and Th( N, =, x ) is complete for UC,0 ATIME-ALT(222’M, n) [S]. Many relations and functions can be defined in ( PY, =, +, x ). First, consider the following weakenings of +: the linear ordering <, which is definable in (N, =, + ), and the successor function S, which is definable in (N, < ). Next consider the following weakenings of x : the partial ordering 1 of divisibility, which is definable in (N, =, x ), and the binary relation I of coprimality, which is definable in (N,j). Note that = is definable in ( tV, < ) and ( N, I). What happens to the theory Th( N, =, <, 1, I, S, +, x ) if we drop some of these relations or functions? Robinson [23] has proved that in (N, S, / ) we can define all relations and functions of complete number theory. Woods [27] has proved the same	axiomatic system;computational complexity theory;emoticon;pspace-complete;python;stat (system call);successor function;uc browser;undecidable problem	Pascal Michel	1992	Theor. Comput. Sci.	10.1016/0304-3975(92)90250-J	calculus;mathematics;upper and lower bounds;algorithm;statistics	Logic	-6.200184019401218	18.950468975381114	103874
278addb9e85f869abe8c2d6ead76024c0c89f460	translation of turner combinators in o(n log n) space	n log n;turner combinators;complexity;lambda calculus;functional programming	Abstract   A practical method for representing Turner Combinators is presented, which needs only O(n log n) space in the worst case for translating lambda expressions of length n. No precomputation is necessary in our translation, which should be contrasted with Burton's proposal. The runtime system can be implemented with virtually no essential change to Turner's reduction machine.	combinatory logic;fixed-point combinator	Kohei Noshita	1985	Inf. Process. Lett.	10.1016/0020-0190(85)90066-3	theoretical computer science;mathematics;programming language;algorithm	DB	-15.000036985473947	21.39926667211122	103910
8778a628b61eee8f8be9168e74d852353f028ae0	fairness in lotos	specification language;indexation;computer model	"""Fairness is an important concept related to specification languages which are based on concurrent and non-deterministic computation models; it is related to liveness. In this paper we formally introduce fairness to the LOTOS specification language by employing the standard LOTOS semantics together with a formalism which states restrictions on fair infinite execution sequences. We extend three fairness concepts of CSP, namely process, guard and channel fairness, to LOTOS. Certain features of LOTOS, such as the dynamic creation of processes, the dynamic relation between gates and processes, and related membership in multi-way rendezvous, not present in CSP, make the definition of fairness difficult. We introduce the concept of """"transition groups"""", which leads to a general notion of fairness, and use LOTOS action indexes to define the concepts of process, alternative and channel for LOTOS. We explain how a fair execution model for LOTOS can be obtained, and demonstrate the use of these concepts by showing how fairness assumptions can be used to prove liveness properties for a given LOTOS specification."""	action potential;centralized computing;communicating sequential processes;complex systems;computation;deterministic automaton;fairness measure;language of temporal ordering specification;liveness;semantics (computer science);specification language;temporal logic	Cheng Wu;Gregor von Bochmann	1991			computer simulation;real-time computing;specification language;computer science;theoretical computer science;programming language;algorithm	PL	-10.915632560365465	21.707693608044007	104036
5d81c1cecf1e0351d81064b65bbd79e3f7ee34bd	design proof assistant (abstract)	design proof assistant;proof assistant	type are inconsistent Abstract type implies existential (this is not enough) { type t; val x : t; ... } ' ∃t .(t × . . . )type implies existential (this is not enough) { type t; val x : t; ... } ' ∃t .(t × . . . ) A fact: ∃ and ∀ over types in types are inconsistent in HOL Existential type unnecessary ! I We have specification to replace them I With existential type one can not extend library Abstract type are inconsistenttype are inconsistent Abstract type implies existential (this is not enough) { type t; val x : t; ... } ' ∃t .(t × . . . )type implies existential (this is not enough) { type t; val x : t; ... } ' ∃t .(t × . . . ) A fact: ∃ and ∀ over types in types are inconsistent in HOL Existential type unnecessary ! I We have specification to replace them I With existential type one can not extend library Abstract type are inconsistenttype are inconsistent Abstract type implies existential (this is not enough) { type t; val x : t; ... } ' ∃t .(t × . . . )type implies existential (this is not enough) { type t; val x : t; ... } ' ∃t .(t × . . . ) A fact: ∃ and ∀ over types in types are inconsistent in HOL Existential type unnecessary ! I We have specification to replace them I With existential type one can not extend library Inductive type are inconsistent Same fact: μ and ν (fix-points) over types are inconsistent in HOL Solution: No solution using sets for: α = α → β Inductive type are inconsistent Same fact: μ and ν (fix-points) over types are inconsistent in HOL Solution: No solution using sets for: α ⊃ α → β But solutions for α ⊂ α → β Inductive type are inconsistent Same fact: μ and ν (fix-points) over types are inconsistent in HOL Solution: From typing constraints, construct interpreted after order ( , ) α ⊃ β ⇒ α β α ⊃ β → γ ⇒ α β, γ α ⊃ β × γ ⇒ α β, γ α ⊂ β → γ ⇒ > α ⊂ β × γ ⇒ > Reject program if is cyclic Object encoding is preserved ! Fixpoint of negation:	abstract type;existential quantification;fixed point (mathematics);hol (proof assistant);inductive type;proof assistant;type system	Gérard P. Huet	1996		10.1007/3-540-61464-8_49	proof assistant	PL	-15.67998347127634	19.41553809234043	104250
9a18c6e6e7fbf0e9da082b5f66d8ca27f614b821	coalgebraic aspects of bidirectional computation		We have previously shown that several state-based bx formalisms can be captured using monadic functional programming, using the state monad together with possibly other monadic effects, giving rise to structures we have called monadic bx (mbx). In this paper, we develop a coalgebraic theory of state-based bx, and relate the resulting coalgebraic structures (cbx) to mbx. We show that cbx support a notion of composition coherent with, but conceptually simpler than, our previous mbx definition. Coalgebraic bisimulation yields a natural notion of behavioural equivalence on cbx, which respects composition, and essentially includes symmetric lens equivalence as a special case. Finally, we speculate on the applications of this coalgebraic perspective to other bx constructions and formalisms.	bidirectional transformation;bisimulation;coherence (physics);combinatory logic;component-based software engineering;computation;congruence of squares;expectation propagation;functional programming;hoc (programming language);intensional logic;lambda lifting;mathematical model;monad (functional programming);monadic predicate calculus;plotkin bound;state space;turi;turing completeness	Faris Abou-Saleh;James McKinna;Jeremy Gibbons	2015	Journal of Object Technology	10.5381/jot.2017.16.1.a1	programming language;discrete mathematics;equivalence (measure theory);computation;rotation formalisms in three dimensions;bisimulation;functional programming;special case;computer science;monad (functional programming)	PL	-11.352874438736517	19.35758580503066	104608
0b8732d6a42ee7e49b05370e75bd0c8645b81ec4	optimal constructions for active diagnosis	controller synthesis;game and automata theory;partial observation;diagnosis	Diagnosis is the task of detecting fault occurrences in a partially observed system. Depending on the possible observations, a discrete-event system may be diagnosable or not. Active diagnosis aims at controlling the system to render it diagnosable. Past research has proposed solutions for this problem, but their complexity remains to be improved. Here, we solve the decision and synthesis problems for active diagnosability, proving that (1) our procedures are optimal with respect to computational complexity, and (2) the memory required for our diagnoser is minimal. We then study the delay between a fault occurrence and its detection by the diagnoser. We construct a memory-optimal diagnoser whose delay is at most twice the minimal delay, whereas the memory required to achieve optimal delay may be highly greater. We also provide a solution for parametrized active diagnosis, where we automatically construct the most permissive controller respecting a given delay.	alternating finite automaton;automata theory;automatic control;automation;bertrand (programming language);büchi automaton;chaitin's constant;computational complexity theory;deadlock;fault tolerance;finite-state machine;fundamenta informaticae;general protection fault;haar wavelet;informatics;lecture notes in computer science;observable;petri net;quantum finite automata;quiesce;quiescence search;sensor;source-to-source compiler;springer (tank);stack (abstract data type);symposium on foundations of computer science;theoretical computer science;thomas–fermi model;time complexity;ω-automaton	Stefan Haar;Serge Haddad;Tarek Melliti;Stefan Schwoon	2017	J. Comput. Syst. Sci.	10.1016/j.jcss.2016.04.007	discrete mathematics;medical diagnosis;control theory;mathematics;algorithm	EDA	-7.66559384134104	27.761700502858897	104653
cfb089b218bf4a62c9c94aa0e7fb2767b74026d5	branching time and abstraction in bisimulation semantics	action refinement;logique mathematique;language theory;semantics;abstraction;simultaneidad informatica;bisimulation;logica matematica;teoria lenguaje;mathematical logic;abstraccion;semantica;semantique;concurrency;branching time;simultaneite informatique;concurrent process;process algebra semantic equivalence;lenguaje formal;theorie langage;formal language;langage formel	In comparative concurrency semantics, one usually distinguishes between linear time and branching time semantic equivalences. Milner's notion of observatin equivalence is often mentioned as the standard example of a branching time equivalence. In this paper we investigate whether observation equivalence really does respect the branching structure of processes, and find that in the presence of the unobservable action τ of CCS this is not the case. Therefore, the notion of branching bisimulation equivalence is introduced which strongly preserves the branching structure of processes, in the sense that it preserves computations together with the potentials in all intermediate states that are passed through, even if silent moves are involved. On closed CCS-terms branching bisimulation congruence can be completely axiomatized by the single axion scheme: a.(τ.(y+z)+y)=a.(y+z) (where a ranges over all actions) and the usual loaws for strong congruence. We also establish that for sequential processes observation equivalence is not preserved under refinement of actions, whereas branching bisimulation is. For a large class of processes, it turns out that branching bisimulation and observation equivalence are the same. As far as we know, all protocols that have been verified in the setting of observation equivalence happen to fit in this class, and hence are also valid in the stronger setting of branching bisimulation equivalence.	bisimulation;computation;concurrency (computer science);concurrency semantics;congruence of squares;refinement (computing);time complexity;turing completeness	Rob J. van Glabbeek;W. P. Weijland	1996	J. ACM	10.1145/233551.233556	logical equivalence;combinatorics;mathematical logic;formal language;discrete mathematics;concurrency;computer science;bisimulation;philosophy of language;mathematics;semantics;abstraction;algorithm	Logic	-9.316611014098152	21.820267192018786	104807
d05a92a685d62b4a17aae4c371b4f851865d973c	computing minimal siphons in petri net models of resource allocation systems: a parallel solution	flexible manufacturing systems;resource allocation;concurrent computing resource management system recovery petri nets manufacturing systems production systems flexible manufacturing systems np complete problem routing linear programming;siphon computation;resource management;deadlock prevention method;siphons;resource allocation systems;parallel computation;petri net models;resource allocation concurrency control flexible manufacturing systems parallel processing petri nets;concurrency control;parallel computer;parallel computation petri net models resource allocation systems deadlock prevention method siphon computation;parallel implementation;petri nets;petri net;parallel processing;structural properties;structural properties parallel computation petri nets siphons	Siphons are related to the liveness properties of Petri net models. This relation is strong in the case of resource allocation systems (RASs). Siphons can be used in these systems in order to both characterize and prevent/avoid deadlock situations. However, the computation of these structural components can be very time consuming or, even, impossible. Moreover, if, in general, the complete enumeration of the set of minimal siphons must be avoided (there can exist an exponential number of such components), some deadlock prevention methods rely on its (complete or partial) computation and enumeration. The special syntactical constraints of some classes of RASs can help in developing specific algorithms to compute siphons in a more efficient way. In this work, a known method for siphon computation is adapted to get advantage of the special (syntactical) structure of a class of RASs; a parallel implementation is proposed and some experimental results are presented	algorithm;computation;deadlock;liveness;petri net;time complexity	Fernando Tricas García;Joaquín Ezpeleta	2006	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2005.855751	parallel processing;real-time computing;computer science;resource management;theoretical computer science;distributed computing;petri net	Robotics	-6.914954275764473	29.252472363009087	104820
a68c202563280a8cba257c59947880cef6b53b3e	factoring stochastic relations	theorie automate;procesamiento informacion;004;stochastic system;concurrency;congruences;informatique theorique;information processing;automata theory;teoria automata;stochastic model;sistema estocastico;traitement information;stochastic relations;factor systems;systeme stochastique;computer theory;informatica teorica	When a system represented through a stochastic model is observed, the equivalence of behavior is described through the observation that equivalent inputs lead to equivalent outputs. This paper has a look at the systems that arise when the stochastic model is factored through the congruence. Congruences may refine each other, and we show that this refinement is reflected through factoring. We also show that factoring a factor does not give rise to any new constructions, since we are kept in the realm of factors for the original system. Thus we cannot have infinite long chains of factors, so that no new behavior can arise from the original system upon factoring (a system and its factors are bisimilar, after all).	bisimulation;congruence of squares;integer factorization;ka band;markov chain;modal logic;refinement (computing);state space;turing completeness	Ernst-Erich Doberkat	2004	Inf. Process. Lett.	10.1016/j.ipl.2004.02.009	combinatorics;discrete mathematics;concurrency;information processing;computer science;stochastic modelling;congruence relation;automata theory;mathematics;algorithm	Logic	-5.003455633920509	21.73647582402826	105125
27c96d02b4514a5c071905f7b44436ff97910422	lazy shape analysis	software model checking shape analysis counterexample guided abstraction refinement interpolation predicate abstraction;verificacion modelo;interpolation;004;software model checking;counterexample guided abstraction refinement;nccr mics cl2;morfoscopia;shape analysis;nccr mics;verification modele;abstraction;interpretacion abstracta;program verification;forma geometrica;recursive data structure;abstraccion;refinement method;verificacion programa;marcador;analisis morfologico;model checking;pointer;compact representation;morphoscopie;estructura datos;geometrical shape;morphological analysis;control flow;analyse morphologique;pointeur;structure donnee;forme geometrique;interpretation abstraite;methode raffinement;predicate abstraction;abstract interpretation;verification programme;metodo afinamiento;data structure	Many software model checkers are based on predicate abstraction. Values of variables in branching conditions are represented abstractly using predicates. The strength of this approach is its path-sensitive nature. However, if the control flow depends heavily on the values of memory cells on the heap, the approach does not work well, because it is difficult to find ‘good’ predicate abstractions to represent the heap. In contrast, shape analysis can lead to a very compact representation of data structures stored on the heap. In this paper, we combine shape analysis with predicate abstraction, and integrate it into the software model checker BLAST. Because shape analysis is expensive, we do not apply it globally. Instead, we ensure that shapes are computed and stored locally, only where necessary for proving the verification goal. To achieve this, we extend lazy abstraction refinement, which so far has been used only for predicate abstractions, to shapes. This approach does not only increase the precision of model checking and shape analysis taken individually, but also increases the efficiency of shape analysis (we do not compute shapes where not necessary). We implemented the technique by extending BLAST with calls to TVLA , and evaluated it on several C programs manipulating data structures, with the result that the combined tool can now automatically verify programs that are not verifiable using either shape analysis or predicate abstraction on its own. Classification: D.2.4 Software Engineering — Software/Program Verification F.3.1 Logics and Meanings of Programs — Specifying and Verifying and Reasoning about Programs	blast;control flow;data structure;formal specification;formal verification;lazy evaluation;model checking;predicate abstraction;refinement (computing);shape analysis (digital geometry);software engineering	Dirk Beyer;Thomas A. Henzinger;Grégory Théoduloz	2006		10.1007/11817963_48	model checking;pointer;data structure;interpolation;morphological analysis;computer science;recursive data type;theoretical computer science;shape analysis;abstraction;programming language;control flow;algorithm	Logic	-18.245924068635368	25.45395462469789	105292
c0f902236d4c6331fca18974468b3ea2f8d12f54	eliminating concurrency bugs in multithreaded software: a new approach based on discrete-event control	multi threading;deadlock avoidance;concurrent software;optimal control;control system synthesis;concurrency control;system recovery software concurrent computing petri nets optimal control multicore processing software algorithms;discrete event systems;redundant control logic concurrency bugs elimination multithreaded software discrete event control uniprocessor architecture multicore architecture parallel software conventional serial software circular wait deadlock gadara project petri nets gadara nets lock allocation operation release operation optimal control synthesis methodology siphon analysis;petri nets concurrent software deadlock avoidance liveness enforcement optimal control;program debugging;petri nets;program debugging concurrency control control system synthesis discrete event systems multi threading optimal control petri nets;liveness enforcement	Computer hardware is moving from uniprocessor to multicore architectures. One problem arising in this evolution is that only parallel software can exploit the full performance potential of multicore architectures, and parallel software is far harder to write than conventional serial software. One important class of failures arising in parallel software is circular-wait deadlock in multithreaded programs. In our ongoing Gadara project, we use a special class of Petri nets, called Gadara nets, to systematically model multithreaded programs with lock allocation and release operations. In this paper, we propose an efficient optimal control synthesis methodology for ordinary Gadara nets that exploits the structural properties of Gadara nets via siphon analysis. Optimality in this context refers to the elimination of deadlocks in the program with minimally restrictive control logic. We formally establish a set of important properties of the proposed control synthesis methodology, and show that our algorithms never synthesize redundant control logic. We conduct experiments to evaluate the efficiency and scalability of the proposed methodology, and discuss the application of our results to real-world concurrent software.	algorithm;computer hardware;concurrency control;control theory;correctness (computer science);deadlock;dynamical system;experiment;incidence matrix;input/output;item unique identification;iteration;iterative method;linear algebra;linear inequality;liveness;logic synthesis;loop (graph theory);maximal set;multi-core processor;optimal control;petri net;scalability;social inequality;software bug;structural analysis;uniprocessor system;weight function	Hongwei Liao;Yin Wang;Jason Stanley;Stéphane Lafortune;Spyros A. Reveliotis;Terence Kelly;Scott A. Mahlke	2013	IEEE Transactions on Control Systems Technology	10.1109/TCST.2012.2226034	parallel computing;real-time computing;multithreading;optimal control;computer science;concurrency control;control theory;distributed computing;petri net	SE	-6.379976896144594	29.829702915172376	105378
c13bc003c98b1e0d04cfe0bb792c4ab4785b01e7	deciding safety properties in infinite-state pi-calculus via behavioural types	pi calculus;satisfiability;safety properties;behavioural types;safety;transition systems;spatial logic;type system;decidability	In the pi-calculus, we consider decidability of safety properties expressed in a simple spatial logic. We first introduce a behavioural type system that, given a (in general infinitecontrol) process P, tries to extract a spatial-behavioural type T , in the form of a CCS term that is logically equivalent to the given process. Using techniques based on well-structured transition systems (WSTS), we prove that satisfiability (T |= φ) is decidable for types over an interesting fragment of the logic. The WSTS techniques we use require first endowing the considered transition system with a well-quasi order, then defining a finite basis for the denotation of each formula. This is achieved by viewing types as forests, with a wellquasi order that essentially corresponds to forest embedding. Finally, as a consequence of the logical equivalence between types and processes, we obtain the decidability of the considered fragment of the logic for well-typed pi-processes.	algorithm;calculus of communicating systems;concurrency (computer science);decision problem;distributed object;interaction;lambda calculus;model checking;petri net;reachability;sequent calculus;simulation;sorting;stationary process;turing completeness;type system;well-structured transition system;π-calculus	Lucia Acciai;Michele Boreale	2009		10.1007/978-3-642-02930-1_3	decidability;combinatorics;discrete mathematics;type system;π-calculus;computer science;mathematics;programming language;algorithm;satisfiability	Logic	-10.439636596894754	22.473174122205624	105385
a117d7de6e5995167bba7aa658b0868ae972a864	active diagnosis with observable quiescence	control systems;control systems discrete event systems observers automata europe production semantics;observability control system synthesis discrete event systems game theory;semantics;active diagnosis fault detection synthesis problem decision problem game based construction design discrete event system observable quiescence;observers;automata;discrete event systems;production;europe	Active diagnosis of a discrete-event system consists in controlling the system such that faults can be detected. Here we extend the framework of active diagnosis by introducing modalities for actions and states and a new capability for the controller, namely observing that the system is quiescent. We design a game-based construction for both the decision and the synthesis problems that is computationally optimal. Furthermore we prove that the size and the delay provided by the active diagnoser (when it exists) are almost optimal.	observable;quiesce;quiescence search	Stanislav Böhm;Stefan Haar;Serge Haddad;Piotr Hofman;Stefan Schwoon	2015	2015 54th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2015.7402449	control engineering;real-time computing;discrete event dynamic system;computer science;control system;control theory;mathematics;semantics;automaton	Robotics	-7.335168210662431	27.74575520873472	105503
2c7af45bb17cd6f3a1eed911f9cd9f504d9db7c8	automatic parallelization of recursive functions using quantifier elimination	quantifier elimination;automatic generation;parallel computer;parallel programs;computational efficiency;automatic parallelization;generic programming	Although the recent popularity of parallel-computing environments has called for parallel programs, it is difficult for nonspecialists to develop those that are efficient. What is required are parallelization methods that can automatically generate efficient parallel programs from sequential ones. In this paper, we propose an automatic method of parallelization for recursive functions. The key is a quantifier-elimination-based derivation of an operator that shrinks function closures representing partial computations. Once we obtain such an operator, we can split the input structure and perform computation on each part in parallel. Our method has several features: it does not require any human help, it guarantees computational efficiency of generated programs, and it deals with complicated recursive functions such as those that are nonlinear recursive, non-self recursive, and accumulative.	automatic parallelization;quantifier (logic);recursion (computer science)	Akimasa Morihata;Kiminori Matsuzaki	2010		10.1007/978-3-642-12251-4_23	quantifier elimination;computer science;theoretical computer science;programming language;generic programming;algorithm;automatic parallelization	Logic	-14.943872027292086	24.381304164158287	105701
d88cc73f0918504b2267aae055cad1ac959d65d4	algebraic model checking	model checking	Several more or less algebraic approaches to model checking are presented and compared with each other with respect to their range of applications and their degree of automation. All of them have been implemented and tested in our Haskell-based formal-reasoning system Expander2. Besides realizing and integrating state-of-the art proof and computation rules the system admits rarely restricted specifications of the models to be checked in terms of rewrite rules and functional-logic programs. It also offers flexible features for visualizing and even animating models and computations. Indeed, this paper does not present purely theoretical work. Due to the increasing abstraction potential of programming languages like Haskell the boundaries between developing a formal system and implementing it or making it ‘user-friendly’ as well as between systems developed in different communities become more and more obsolete. The individual topics discussed in the paper reflect this observation.	computation;formal system;haskell;linear algebra;model checking;programming language;reasoning system;rewriting	Peter Padawitz	2010	ECEASST	10.14279/tuj.eceasst.26.359	model checking;computer science;theoretical computer science;programming language;algorithm	PL	-18.214473458176986	18.28493053423928	105777
0dc7dc78201dc2dca8c3bf2a01245fa146898cdd	animating tla specifications	logica temporal;canonical form;temporal logic;complejidad programa;intelligence artificielle;logical programming;specification programme;program verification;satisfiability;verificacion programa;first order;model checking;temporal logic of actions;specification and verification;programmation logique;linear temporal logic;animation;reactive system;tla;artificial intelligence;system development;validation;inteligencia artificial;program complexity;verification programme;program specification;programacion logica;logique temporelle;especificacion programa;complexite programme	TLA (the Temporal Logic of Actions) is a linear temporal logi c for specifying and reasoning about reactive systems. We define a subset of TLA whose formulas are amenable to validation by animation, wit h the intent to facilitate the communication between domain and solution exp erts in the design of reactive systems.	algorithm;automata theory;backtracking;büchi automaton;concurrent metatem;database;deadlock;exptime;executable;finite-state machine;first-order predicate;initial condition;java;linear temporal logic;model f keyboard;operational semantics;pass transistor logic;prolog;prototype;tla+;temporal logic of actions	Yassin Mokhtari;Stephan Merz	1999		10.1007/3-540-48242-3_7	model checking;anime;canonical form;linear temporal logic;real-time computing;temporal logic;reactive system;computer science;artificial intelligence;first-order logic;algorithm;temporal logic of actions;satisfiability	Logic	-16.641299640781433	27.00885434905213	105808
09e9c7532db634d977c66e326f755b207a0087fa	on fixed-points of multivalued functions on complete lattices and their application to generalized logic programs	multivalued functions;68q55;multivalued function;aplicacion;06b23;logique programme;fonction monotone;virgule fixe;37c25;punto fijo;fonction multivoque;verite;potencia;carta de datos;funcion monotona;coma fija;47h10;funcion multivoca;fixed point;logic programming;point fixe;mappage;xed point;monotonic function;puissance;03bxx;value function;mapping;68n17;06bxx;logic programs;complete lattice;application;fix point;power;fixed points;treillis complet;complete lattices	Unlike monotone single-valued functions, multivalued mappings may have zero, one, or (possibly infinitely) many minimal fixed-points. The contribution of this work is twofold. First, we overview and investigate the existence and computation of minimal fixed-points of multivalued mappings, whose domain is a complete lattice and whose range is its power set. Second, we show how these results are applied to a general form of logic programs, where the truth space is a complete lattice. We show that a multivalued operator can be defined whose fixed-points are in one-to-one correspondence with the models of the logic program.	computation;fixed point (mathematics);logic programming;one-to-one (data model);monotone	Umberto Straccia;Manuel Ojeda-Aciego;Carlos Viegas Damásio	2009	SIAM J. Comput.	10.1137/070695976	combinatorics;mathematical analysis;discrete mathematics;mathematics;fixed point;fourth normal form	Theory	-7.005764918926786	18.557885504136735	105828
79b5db7a986ea2392741a479909b4458b15734ca	classification-based parameter synthesis for parametric timed automata		Parametric timed automata are designed to model timed systems with unknown parameters, often representing design uncertainties of external environments. In order to design a robust system, it is crucial to synthesize constraints on the parameters, which guarantee the system behaves according to certain properties. Existing approaches suffer from scalability issues. In this work, we propose to enhance existing approaches through classification-based learning. We sample multiple concrete values for parameters and model check the corresponding non-parametric models. Based on the checking results, we form conjectures on the constraint through classification techniques, which can be subsequently confirmed by existing model checkers for parametric timed automata. In order to limit the number of model checker invocations, we actively identify informative parameter values so as to help the classification converge quickly. We have implemented a prototype and evaluated our idea on 24 benchmark systems. The result shows our approach can synthesize parameter constraints effectively and thus improve parametric verification.	timed automaton	Jiaying Li;Jun Sun;Bo Gao;Étienne André	2017		10.1007/978-3-319-68690-5_15	scalability;theoretical computer science;model checking;timed automaton;computer science;parametric statistics	Logic	-14.460172585454668	28.2504795868303	105868
567e06ee582cdf4081ca9e5e77e65744ca8b2339	lost in abstraction: monotonicity in multi-threaded programs (extended technical report)		Monotonicity in concurrent systems stipulates that, in any global state, extant system actions remain executable when new processes are added to the state. This concept is not only natural and common in multi-threaded software, but also useful: if every thread’s memory is finite, monotonicity often guarantees the decidability of safety property verification even when the number of running threads is unknown. In this paper, we show that the act of obtaining finite-data thread abstractions for model checking can be at odds with monotonicity: Predicate-abstracting certain widely used monotone software results in non-monotone multi-threaded Boolean programs — the monotonicity is lost in the abstraction. As a result, well-established sound and complete safety checking algorithms become inapplicable; in fact, safety checking turns out to be undecidable for the obtained class of unbounded-thread Boolean programs. We demonstrate how the abstract programs can be modified into monotone ones, without affecting safety properties of the non-monotone abstraction. This significantly improves earlier approaches of enforcing monotonicity via overapproximations.	algorithm;concurrency (computer science);executable;model checking;thread (computing);undecidable problem;monotone	Alexander Kaiser;Daniel Kroening;Thomas Wahl	2014		10.1007/978-3-662-44584-6_11	discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-15.349306350039429	26.53400917799501	105882
f353e86d69e11b82678fc7c0c120e2379744281a	inference of termination conditions for numerical loops in prolog	programming language;prolog;semantics;program transformation;transformation programme;semantica;semantique;transformacion programa;calcul numerique;logique ordre 1;numerical computation;calculo numerico;logic in computer science;floating point;logic programs;first order logic;logica orden 1	We present a new approach to termination analysis of numerical computations in logic programs. Traditional approaches fail to analyse them due to non well-foundedness of the integers. We present a technique that allows overcoming these difficulties. Our approach is based on transforming a program in a way that allows integrating and extending techniques originally developed for analysis of numerical computations in the framework of querymapping pairs with the well-known framework of acceptability. Such an integration not only contributes to the understanding of termination behaviour of numerical computations, but also allows us to perform a correct analysis of such computations automatically, by extending previous work on a constraint-based approach to termination. Finally, we discuss possible extensions of the technique, including incorporating general term orderings.	prolog	Alexander Serebrenik;Danny De Schreye	2001		10.1007/3-540-45653-8_45	computer science;floating point;artificial intelligence;theoretical computer science;first-order logic;mathematics;semantics;programming language;prolog;algorithm	HPC	-17.780132061986993	21.160034522143473	106121
5a4e6b85646f51c3912ebbd3b00bf4ea0820cc08	outils pour une comparaison sans a priori entre arithmétique logarithmique et arithmétique flottante	logarithmic number system;field programmable gate array;arithmetique ordinateur;representacion sistema;lns;fpga;red puerta programable;reseau porte programmable;funcion logaritmica;computer arithmetic;logarithmic function;verilog hardware description language;representation systeme;vhdl;fonction logarithmique;system representation;aritmetica ordenador;arithmetic;floating point;coma flotante;hardware operators;virgule flottante	This paper describes a library of parameterized arithmetic operators for manipulating high-dynamic numbers on FPGA. It support both floati ng-point and logarithmic representations. Along with its direct interest, that library allow application-specific comparisons of those two number representation systems. It is unbi ased in the sense that it tends to reflect the state-of-the-art for both number arithmetic s ystems, and is freely available at MOTS-CLÉS :arithmétique, virgule flottante, système logarithmique, L NS, opérateurs matériels, FPGA, VHDL.	field-programmable gate array;sans institute;vhdl	Jérémie Detrey;Florent de Dinechin	2005	Technique et Science Informatiques	10.3166/tsi.24.625-643	computer science;theoretical computer science;operating system;algorithm;field-programmable gate array	Logic	-9.168134508162163	31.652391102262918	106341
f779f08b9474203e17a3c8672cae8c9a3c3688ff	higher-order interpretations and program complexity	lambda calculus;term rewriting systems;implicit computational complexity;type systems	Polynomial interpretations and their generalizations like quasi-interpretations have been used in the setting of first-order functional languages to design criteria ensuring statically some complexity bounds on programs 10. This fits in the area of implicit computational complexity, which aims at giving machine-free characterizations of complexity classes. In this paper, we extend this approach to the higher-order setting. For that we consider simply-typed term rewriting systems 35, we define higher-order polynomial interpretations for them, and we give a criterion ensuring that a program can be executed in polynomial time. In order to obtain a criterion flexible enough to validate interesting programs using higher-order primitives, we introduce a notion of polynomial quasi-interpretations, coupled with a simple termination criterion based on linear types and path-like orders.		Patrick Baillot;Ugo Dal Lago	2016	Inf. Comput.	10.1016/j.ic.2015.12.008	combinatorics;discrete mathematics;ph;computer science;structural complexity theory;lambda calculus;mathematics;programming language;algorithm;descriptive complexity theory;algebra	Logic	-12.766353941799903	19.505581227721514	106485
e4b968628d0bf90d9dbc70ea7f8b22d30a8c8749	eliminating spurious transitions in reachability with support functions	verification;reachability;tools;hybrid systems	Computing an approximation of the reachable states of a hybrid system is a challenge, mainly because overapproximating the solutions of ODEs with a finite number of sets does not scale well. Using template polyhedra can greatly reduce the computational complexity, since it replaces complex operations on sets with a small number of optimization problems. However, the use of templates may make the over-approximation too conservative. Spurious transitions, which are falsely considered reachable, are particularly detrimental to performance and accuracy, and may exacerbate the state explosion problem. In this paper, we examine how spurious transitions can be avoided with minimal computational effort. To this end, detecting spurious transitions is reduced to the well-known problem of showing that two convex sets are disjoint by finding a hyperplane that separates them. We generalize this to flowpipes by considering hyperplanes that evolve with time in correspondence to the dynamics of the system. The approach is implemented in the model checker SpaceEx and demonstrated on examples.	approximation;computation;computational complexity theory;convex set;hybrid system;mathematical optimization;model checking;polyhedron;reachability;scalability;sensor;whole earth 'lectronic link	Goran Frehse;Sergiy Bogomolov;Marius Greitschus;Thomas Strump;Andreas Podelski	2015		10.1145/2728606.2728622	combinatorics;discrete mathematics;mathematics;algorithm	AI	-11.606279357200993	27.988952565515984	106570
7f84c0c8acd2f1a80e9fd1b44a00a52d690ae4ef	a game-theoretic approach to branching time abstract-check-refine process	errors;program verification computers;semantics;detection;software engineering;error analysis;branching mathematics;refining;checkout	Since the complexity of software systems continues to grow, most engineers face two serious problems: the state space explosion problem and the problem of how to debug systems. In this paper, we propose a game-theoretic approach to full branching time model checking on three-valued semantics. The three-valued models and logics provide successful abstraction that overcomes the state space explosion problem. The game style model checking that generates counterexamples can guide refinement or identify validated formulas, which solves the system debugging problem. Furthermore, output of our game style method will give significant information to engineers in detecting where errors have occurred and what the causes of the errors are.	algorithm;debugging;fuzzy logic;game theory;in the beginning... was the command line;model checking;refinement (computing);regular expression;sensor;software system;state space	Yi Wang;Tetsuo Tamai	2009			refining;computer science;theoretical computer science;semantics;programming language	SE	-16.64871803217282	29.022374364926844	106674
cc9f3960e53089d876bc0e337e240138051a53a0	lazy annotation revisited		Lazy Annotation is a method of software model checking that performs a backtracking search for a symbolic counterexample. When the search backtracks, the program is annotated with a learned fact that constrains future search. In this sense, the method is closely analogous to conflictdriven clause learning in SAT solvers. In this paper, we develop several improvements to the basic Lazy Annotation approach. The resulting algorithm is compared both conceptually and experimentally to two approaches based on similar principles but using different learning strategies: unfolding-based Bounded Model Checking and Property-Driven Reachability. Copyright 2014 Microsoft Research. All rights reserved.	algorithm;backtracking;constraint learning;experiment;future search;lazy evaluation;microsoft research;model checking;reachability;unfolding (dsp implementation)	Kenneth L. McMillan	2014		10.1007/978-3-319-08867-9_16	computer science;theoretical computer science;machine learning;programming language;algorithm	AI	-15.178253900202504	25.014236748208994	106738
1164975c70f4075899d0cc04bde378890f67f45d	two new strategies for developing loop invariants and their applications	dynamic program;recurrence relation;algorithm design;problem solving	The loop invariants take a very important role in the design, proof and derivation of the algorithmic program. We point out the limitations of the traditional standard strategy for developing loop invariants, and propose two new strategies for proving the existing algorithmic program and developing new ones. The strategies use recurrence as vehicle and integrate some effective methods of designing algorithms, e. g. Dynamic Programming, Greedy and Divide- Conquer, into the recurrence relation of problem solving sequence. This lets us get straightforward an approach for solving a variety of complicated problems, and makes the standard proof and formal derivation of their algorithmic programs possible. We show the method and advantages of applying the strategies with several typical nontrivial examples.	algorithm design;david gries;dynamic programming;formal proof;goto;greedy algorithm;loop invariant;problem solving;recurrence relation	Jinyun Xue	1993	Journal of Computer Science and Technology	10.1007/BF02939477	algorithm design;mathematical optimization;recurrence relation;computer science;algorithm	PL	-16.895831545502766	24.37630380989102	106798
3193e51bbd2b1b67dd94a0c7197f6f218ff53614	efficient probabilistic model checking of systems with ranged probabilities	higher-order error term;sound over-approximation;efficient probabilistic model checking;affine arithmetic;interval arithmetic;error bound;reachability property;first-order error term;interval discrete-time markov chains;new technique	We introduce a new technique to model check reachability properties on Interval Discrete-Time Markov Chains (IDTMC). We compute a sound overapproximation of the probabilities of satisfying a given property where the accuracy is characterized in terms of error bounds. We leverage affine arithmetic to propagate the first-order error terms. Higher-order error terms are bounded using interval arithmetic.	affine arithmetic;first-order predicate;interval arithmetic;markov chain;model checking;reachability;statistical model;time complexity	Khalil Ghorbal;Parasara Sridhar Duggirala;Vineet Kahlon;Franjo Ivancic;Aarti Gupta	2012		10.1007/978-3-642-33512-9_10	combinatorics;discrete mathematics;theoretical computer science;affine arithmetic;mathematics	Logic	-11.215382418431991	27.918886215743505	106821
52d2ddd4b6a529e144176d773fb7e1f88a52317b	liveness checking as safety checking to find shortest counterexamples to linear time properties		Temporal logic is widely used for specifying hardware and software systems. Typically two types of properties are distinguished, safety and liveness properties. While safety can easily be checked by reachability analysis, and many efficient checkers for safety properties exist, more sophisticated algorithms have always been considered to be necessary for checking liveness. In this dissertation we describe an efficient translation of liveness checking problems into safety checking problems for finite state systems. More precisely, fair repeated reachability in a fair Kripke structure K is formulated as reachability in a transformed Kripke structure K. A fair loop in K is detected in K by saving a previously visited state in an additional state-recording component, waiting until a fair state has been seen, and checking a loop closing condition. The approach extends to all ω-regular properties. We show that the size of the state space, the reachable state space, the transition relation, and its transitive closure grow by a factor of |S| in the transformed model, where |S| is the size of the state space in the original model. Radius and diameter increase by a small, constant factor. We discuss optimizations that limit the overhead of our translation. We have implemented the approach for BDD-based model checkers of the SMV family. Experimental results show not only that the approach is feasible for complex examples, but that it may lead to faster verification if the property turns out to be false. For one example even an exponential speed-up can be observed. We finally show that a similar reduction can be applied to a number of infinite state systems, namely, (ω-) regular model checking, pushdown systems, and timed automata. Counterexamples as produced by a model checker for a failing property help developers to understand the problem in a faulty design. The shorter a counterexample, the easier it is typically to understand. The length of a counterexample, as reported by a model checker, depends on both the algorithm used for state space exploration and the way the property is encoded. We provide necessary and sufficient criteria for a Büchi automaton to accept shortest counterexamples. Extending a notion introduced by Kupferman and Vardi we call a Büchi automaton that accepts shortest counterexamples tight. We prove that Büchi automata constructed using the approach of Kesten et al. (KPR), which is essentially the same as the construction by Lichtenstein and Pnueli, are tight for future time LTL formulae, while an automaton generated with the algorithm of Gerth et al. (GPVW) may lead to unnecessary long counterexamples. Optimality is lost in the first case as soon as past time operators are included. We show that potential excess length is in both cases at most linear in the length of the specification. Using a recently proposed encoding for bounded model checking of LTL with past by Latvala et al., we construct a Büchi automaton that accepts shortest counterexamples for full LTL. The construction adapts the idea of virtual unrolling by Benedetti and Cimatti to Büchi automata. Its generalization gives a method to make an arbitrary Büchi automaton accept shortest counterexamples. We use our method of translating liveness into safety to find shortest counterexamples with a BDD-based symbolic model checker without modifying the model checker itself. Though	algorithm;automata theory;büchi automaton;closing (morphology);computer hardware;exponential time hypothesis;failure;kripke structure (model checking);liveness;model checking;offset binary;overhead (computing);reachability;software system;stack (abstract data type);state space;temporal logic;time complexity;timed automaton;transitive closure	Viktor Schuppan	2006			real-time computing;mathematics;distributed computing;algorithm	Logic	-14.040697197411049	26.968813341110085	106890
9e88c65b0682f0d8bf078a1a8b0d3717d03c1061	a mathematical approach to nondeterminism in data types	modelizacion;lenguaje programacion;observational equivalence;programming language;language theory;abstract data type;data type;teoria lenguaje;systeme non deterministe;equivalence;input output;modelisation;non deterministic system;cartesian product;equivalence relation;informatique theorique;type abstrait;langage programmation;tipo abstracto;sistema no determinista;modeling;equivalencia;theorie langage;computer theory;informatica teorica	The theory of abstract data types is generalized to the case of nondeterministic operations (set-valuedfunctions). Since the nondeterminism of operations may be coupled, signatures are extended so that operations can have results in Cartesian products. Input/output behavior is used to characterize implementation of one model by another. It is described by means of accumulated arrows, which form a generalization of the term algebra. Morphisms of nondeterministic models are introduced. Both innovations prove to be powerful tools in the analysis of input/output behavior. Extraction equivalence and observable equivalence of values are investigated. Quotient models for such equivalence relations are constructed. The equivalence relations are compared with each other, with separation of values by means of experiments, and with the separation property that characterizes a terminal model. Examples are given to show that the four concepts are different. In deterministic models the concepts coincide.	abstract data type;cartesian closed category;experiment;input/output;observable;term algebra;turing completeness;type signature	Wim H. Hesselink	1988	ACM Trans. Program. Lang. Syst.	10.1145/42192.42194	logical equivalence;equivalence;input/output;systems modeling;data type;computer science;philosophy of language;cartesian product;equivalence relation;programming language;abstract data type;algorithm	Logic	-8.427852715831511	21.65755382136933	106911
8d00053b633f962cceeae36c0d2644efaad6cab3	translation-based compositional reasoning for software systems	developpement logiciel;sistema experto;formal specification;base composition;composition;semantica formal;complexite calcul;composicion;software systems;verification modele;intelligence artificielle;formal semantics;program verification;specification formelle;semantique formelle;especificacion formal;software architecture;complejidad computacion;verificacion programa;model checking;computational complexity;desarrollo logicial;contexto;software development;message passing;contexte;artificial intelligence;inteligencia artificial;systeme expert;verification programme;correctness proof;context;compositional reasoning;architecture logiciel;expert system	Software systems are often model checked by translating them into a directly model-checkable formalism. Any serious software system requires application of compositional reasoning to overcome the computational complexity of model checking. This paper presents Translation-Based Compositional Reasoning (TBCR), an approach to application of compositional reasoning in the context of model checking software systems through model translation. In this approach, given a translation from a software semantics to a directly model-checkable formal semantics, a compositional reasoning rule is established in the software semantics and mapped to an equivalent rule in the formal semantics based on the translation. The correctness proof of the composition reasoning rule in the software semantics is established based on this mapping and the correctness proof of the equivalent rule in the formal semantics. The compositional reasoning rule in the software semantics is implemented and applied based on the translation from the software semantics to the formal semantics and reusing the implementation of the equivalent rule in the formal semantics. TBCR has been realized for a commonly used software semantics, the Asynchronous Interleaving Message-passing semantics. TBCR is illustrated by two applications of this realization.	aim alliance;computational complexity theory;correctness (computer science);denotational semantics;forward error correction;machine translation;message passing;model checking;semantics (computer science);software system	Fei Xie;James C. Browne;Robert P. Kurshan	2003		10.1007/978-3-540-45236-2_32	model checking;composition;software architecture;message passing;formal semantics;action semantics;formal verification;failure semantics;computer science;theoretical computer science;software development;software engineering;formal semantics;formal specification;programming language;well-founded semantics;operational semantics;expert system;denotational semantics;algorithm;computational semantics;static program analysis	PL	-16.91112830641118	26.98422884387189	107051
f5f01a02415ea05a47d62ebdc37825f2db649619	automatic techniques for detecting and exploiting symmetry in model checking	qa75 electronic computers computer science	Model checking is an increasingly popular technique for the formal verification of concurrent systems. The application of model checking is limited due to the statespace explosion problem as the number of components represented by a model increases, the worst case size of the associated state-space grows exponentially. As such, models of realistic systems are often too large to feasibly check. Over the last 15 years, symmetry reduction techniques for model checking have been developed and, in a restricted setting, have been shown to be effective in reducing the statespace explosion problem. Current techniques can handle limited kinds of symmetry, e. g. full symmetry between identical components in a concurrent system. They avoid the problem of automatic symmetry detection by requiring the user to specify the presence of symmetry in a model (explicitly, or by annotating the associated specification using additional language keywords), or by restricting the input language of a model checker so that only symmetric systems can be specified. Additionally, computing unique representatives for each symmetric equivalence class is easy for these limited kinds of symmetry. We present a theoretical framework for symmetry reduction which can be applied to explicit state model checking. The framework includes techniques for autonzatic symmetry detection using computational group theory, which can be applied with no additional user input. These techniques detect structural symmetries induced by the topology of a concurrent system, so our framework includes exact and approximate techniques to efficiently exploit arbitrary symmetry groups which may arise in this way. These techniques are also based on computational group	approximation algorithm;best, worst and average case;computation;computational group theory;concurrency (computer science);formal verification;model checking;sensor;state space;turing completeness	Alastair F. Donaldson	2007			combinatorics;discrete mathematics;theoretical computer science;mathematics	Logic	-13.46490109395825	27.01742736690057	107163
ebea214518eeccf65ba7b60aaf92fbc105d88890	a stratified semantics of general references a stratified semantics of general references	static checking;inference mechanisms;logic object oriented modeling safety data structures java arithmetic computer languages computer science;semantic model;proof carrying code;formal logic;twelf metalogic stratified semantics general references higher order logic semantic model mutable memory cells von neumann machine proofcarrying code system frame axiom introduction rule reasoning;higher order logic;inference mechanisms formal logic	We demonstrate a semantic model of general references - that is, mutable memory cells that may contain values of any (statically-checked) closed type, including other references. Our model is in terms of execution sequences on a von Neumann machine; thus, it can be used in a Proof-Carrying Code system where the skeptical consumer checks even the proofs of the typing rules. The model allows us to prove a frame-axiom introduction rule that allows locality of specification and reasoning, even in the event of updates to aliased locations. Our proof is machine-checked in the Twelf metalogic.		Amal G Ahmed;Andrew W. Appel;Roberto Virga	2002		10.1109/LICS.2002.1029818	semantic data model;description logic;metalogic;higher-order logic;epistemology;computer science;theoretical computer science;computational logic;mathematics;proof calculus;programming language;logic;algorithm;philosophy of logic	PL	-16.70346287242294	19.440338559034803	107274
100e50ad2ce79ff5a02a54657165c912486b8e46	list homomorphism with accumulation.	sequential pattern;parallel programs	This paper introduces accumulation into list homomorphisms for systematic development of both efficient and correct parallel programs. New parallelizable recursive pattern called H-homomorphism is given, and transformations from sequential patterns in the H-form and Hform into (H-)homomorphism are shown. We illustrate the power of our formalization by developing a novel and general parallel program for a class of interesting and challenging problems, known as maximum marking problems.	item unique identification;parallel programming model;recursion;tree accumulation	Kazuhiko Kakehi;Zhenjiang Hu;Masato Takeichi	2003			computer science;algorithm	HPC	-13.622971637555448	24.069085526109856	107431
b9e26fae53ff082eb05586624bb11803405bad81	side effects and aliasing can have simple axiomatic descriptions	lenguaje de programacion;programming language;construction langage;definition axiomatique;side effect;langage programmation;completeness;completude;algol 68	We present a different style of axiomatic definition for programming languages. It is oriented toward imperative languages, such as Algol 68, that do not distinguish between statements and expressions. Rather than basing the logic on a notion of pre- or postcondition, we use the value of a programming language expression as the underlying primitive. A number of language constructs are examined in this framework. We argue that this style of definition gives us a significantly different view of the notion of “easy axiomatixability.” Side effects in expressions as well as aliasing between variables are shown to be “easily axiomatizable” in our system.	algol 68;apl;aliasing;axiomatic system;imperative programming;postcondition;programming language	Helene Böhm	1985	ACM Trans. Program. Lang. Syst.	10.1145/4472.4474	completeness;computer science;theoretical computer science;third-generation programming language;programming language;side effect;algorithm	PL	-15.67118349273547	18.74424470467542	107694
e94534164ebad38bcdd3855d403a15aeb6f60455	specifying message passing systems requires extending temporal logic	temporal logic;message passing;data structure	We prove that it is impossible to express asynchronous message passing within the framework of first-order temporal logic with both future and past operators (as studied by Kamp). This is an extension of a result of Sistla et al. that unbounded buffers cannot be expressed in linear time temporal logic. In our analysis the source of this inexpressiveness is the impossibility to couple each message that is delivered by a message passing system to auniquemessage accepted by that system. This result seems to necessitate the enrichment of TL-based formalisms, e.g. with auxiliary data structures or histories as done, respectively, by Lamport and Hailpern. Observe that Lamport employs a hybrid formalism (TL+Data Structures), and that in Hailpernu0027s method similar systems, such as FIFO and LIFO, do not have similar specifications. We shall prove that no such enrichment is logically required. This is done by introducing an additional axiom within TL which formalizes the assumption that messages accepted by the system can be uniquely identified. In this way, no extraneous formalisms are introduced, and both FIFO and LIFO are expressible with equal ease.	message passing;temporal logic	Ron Koymans	1987		10.1007/3-540-51803-7_28	linear temporal logic;message passing;real-time computing;concurrency;data structure;temporal logic;interval temporal logic;computer science;theoretical computer science;distributed computing;sequential logic;programming language;temporal logic of actions	Theory	-14.486364849803316	20.515471001440336	107893
ffeafc5c36812942505425d62412dc44ed5ca4af	the input-output control of real-time discrete event systems	automatic control;control systems;input output controller;deterministic automata;supervisory control;input output control;deterministic limited automata input output control real time discrete event systems controller synthesis problem hard real time deadlines finite timed traces necessary and sufficient conditions supervisory control theory closed loop specification;control systems real time systems discrete event systems automata control system synthesis sufficient conditions automatic control supervisory control polynomials timing;real time;discrete time systems;supervisory control theory;controller synthesis problem;finite timed traces;sufficient conditions;polynomials;deterministic limited automata;input output;real time discrete event systems;controller synthesis;automata;control problem;discrete event system;control system synthesis;necessary and sufficient condition;controllers;discrete event systems;necessary and sufficient conditions;hard real time deadlines;closed loop specification;real time systems controllers deterministic automata discrete time systems;hard real time;real time systems;timing	We formulate and analyze a controller synthesis problem for a plant which must meet certain h a d real-time deadlines. In this timed anput-output control problem, the plant is modeled by finite tamed traces. We provide necessary and sufficient conditions for the existence of an timed anput-output controller. The results are based in part on the supervisory control theory of Ramadge and Wonham. When the plant and the closed-loop specification are represented by deterministic tamed automata the synthesis problem can be solved. The synthesis procedure and the synthesized supervisor are polynomial in the number of automata states and exponential in the timing information.	automata theory;automaton;control theory;input/output;polynomial;real-time clock;time complexity;tracing (software)	G. Hoffmann;Howard Wong-Toi	1992		10.1109/REAL.1992.242655	input/output;real-time computing;computer science;control system;automatic control;automaton;supervisory control;timed automaton;polynomial	Embedded	-6.24493116718634	28.464399987046097	107989
ba50e05e19a9759f1f9d74c0495d54ee2e23a25f	semantic subtyping for non-strict languages		Semantic subtyping is an approach to define subtyping relations for type systems featuring union and intersection type connectives. It has been studied only for strict languages, and it is unsound for non-strict semantics. In this work, we study how to adapt this approach to non-strict languages: in particular, we define a type system using semantic subtyping for a functional language with a call-by-need semantics. We do so by introducing an explicit representation for divergence in the types, so that the type system distinguishes expressions that are results from those which are computations that might diverge.	computation;functional programming;lazy evaluation;logical connective;strict function;type inference;type system	Tommaso Petrucciani;Giuseppe Castagna;Davide Ancona;Elena Zucca	2018	CoRR		theoretical computer science;functional programming;divergence;computation;semantics;expression (mathematics);computer science;subtyping	PL	-13.25665221594687	18.396229810872256	108034
7fe765e63f0e33c472f348b634ab7f13c11648b8	filter models for conjunctive-disjunctive lambda-calculi		The distinction between the conjunctive nature of non-determinism as opposed to the disjunctive character of parallelism constitutes the motivation and the starting point of the present work. λ-calculus is extended with both a non-deterministic choice and a parallel operator; a notion of reduction is introduced, extending β-reduction of the classical calculus. We study type assignment systems for this calculus, together with a denotational semantics which is initially defined constructing a set semimodel via simple types. We enrich the type system with intersection and union types, dually reflecting the disjunctive and conjunctive behaviour of the operators, and we build a filter model. The theory of this model is compared both with a Morris-style operational semantics and with a semantics based on a notion of capabilities.	disjunctive normal form	Mariangiola Dezani-Ciancaglini;Ugo de'Liguoro;Adolfo Piperno	1996	Theor. Comput. Sci.	10.1016/S0304-3975(96)80703-1		ECom	-12.151334150936187	19.451568114928097	108061
27f34e017a4413f359962dc426f29abbf6ac4d8f	event structures as presheaves -two representation theorems	morphisme;morfismo;categorisation;relation ordre partiel;presheaf model;theoreme representation;categorizacion;causalite;pomset;partial ordering;characterization;preservation;representation theorem;morphism;relacion orden parcial;caracterisation;structure evenement;preservacion;caracterizacion;event structures;categorization;causality;causalidad;partial order	The category of event structures is known to embed fully and faithfully in the category of presheaves over pomsets. Here a characterisation of the presheaves represented by event structures is presented. The proof goes via a characterisation of the presheaves represented by event structures when the morphisms on event structures are “strict” in that they preserve the partial order of causal dependency.	causal filter	Glynn Winskel	1999		10.1007/3-540-48320-9_37	partially ordered set;combinatorics;discrete mathematics;topology;mathematics	Logic	-8.210097011333811	18.617842358428543	108075
442d897f1771181f17a7e8901b640c8ca40a3772	minimizing the number of inputs while applying adaptive test cases	adaptive testing;software engineering;minimization of test inputs;adaptive test cases	State-based formalisms such as Finite State Machine and its derivatives have been used extensively for the specification of the externally observable behavior of a wide range of reactive systems [3–7]. A use of such specifications is to construct a set of test cases to be employed during testing of potential implementations of the specified system. Test cases constructed from such a specification are in the form of sequences of pairs of test input and the corresponding expected output (as in preset testing), unless it is recognized that there are more than one valid expected response and thus the next test input depends on the actual output produced in response to the current input (as in adaptive testing). If the latter is the case, then the test case	test case	Guy-Vincent Jourdan;Hasan Ural;Nejib Zaguia	2005	Inf. Process. Lett.	10.1016/j.ipl.2005.01.011	real-time computing;computer science;theoretical computer science;mathematics;computerized adaptive testing	DB	-12.735689877354334	28.56210815123309	108299
6ca6d73387fbcfc83c2fbdbb08be51400afbeddf	quantitative separation logic and programs with lists	separation logic;program verification;list structures;first order logic	This paper presents an extension of a decidable fragment of Separation Logic for singly-linked lists, defined by Berdine et al. (2004). Our main extension consists in introducing atomic formulae of the form ls k (x, y) describing a list segment of length k, stretching from x to y, where k is a logical variable interpreted over positive natural numbers, that may occur further inside Presburger constraints. We study the decidability of the full first-order logic combining unrestricted quantification of arithmetic and location variables. Although the full logic is found to be undecidable, validity of entailments between formulae with the quantifier prefix in the language $\exists^* \{\exists_{\bf \mathbb{N}}, \forall_{\bf \mathbb{N}}\}^*$ is decidable. We provide here a model theoretic method, based on a parametric notion of shape graphs. We have implemented our decision technique, providing a fully automated framework for the verification of quantitative properties expressed as pre- and post-conditions on programs working on lists and integer counters.	atomic formula;first-order logic;first-order predicate;linked list;presburger arithmetic;quantifier (logic);separation logic;theory;undecidable problem;unrestricted grammar	Marius Bozga;Radu Iosif;Swann Perarnau	2008	Journal of Automated Reasoning	10.1007/s10817-010-9179-9	combinatorics;discrete mathematics;separation logic;computer science;first-order logic;mathematics;programming language;algorithm	Logic	-13.769545206380295	21.794568163958367	108306
0103c48857ef8f75a9f7ba77057189a2cd031670	symmetry-breaking answer set solving	answer sets;answer set programming;search space;graph automorphism;graph automorphism problem;symmetry breaking;logic in computer science;disjunctive logic programming	We investigate the role of symmetry detection and symmetry breaking in answer set programming to eliminate symmetric parts of the search space and, thereby, simplify the solution process. We reduce symmetry detection to a graph automorphism problem which allows us to extract symmetries of a logic program from the symmetries of the constructed coloured graph. The correctness of our reduction is proven. We also propose an encoding of symmetry-breaking constraints in terms of permutation cycles and use only generators in this process to implicitly represent symmetries with exponential compression. These ideas are formulated as preprocessing and implemented in a completely automated flow that first detects symmetries from a given answer set program, adds symmetrybreaking constraints, and can be applied to any existing answer set solver. We demonstrate computational impact on benchmarks versus direct application of the solver.	answer set programming;benchmark (computing);computation;correctness (computer science);experiment;feasible region;graph automorphism;logic programming;overhead (computing);preprocessor;search algorithm;solver;stable model semantics;symmetry breaking;symmetry-breaking constraints;time complexity	Christian Drescher;Oana Tifrea-Marciuska;Toby Walsh	2011	AI Commun.	10.3233/AIC-2011-0495	symmetry breaking;stable model semantics;computer science;artificial intelligence;answer set programming;graph automorphism	AI	-15.056127632604397	23.822981703960632	108307
8dfe33be19512d8825bb858b3879c7a69de6a362	model checking of systems employing commutative functions	verificacion modelo;mise a jour;classe etat;optimization technique;verification modele;simultaneidad informatica;concurrent program;interpretacion abstracta;program verification;actualizacion;verificacion programa;standard model;concurrency;model checking;directed graph;state class;programa competidor;clase estado;concurrent programs;state explosion;interpretation abstraite;abstract interpretation;verification programme;simultaneite informatique;updating;programme concurrent	The paper presents methods for model checking a class of possibly infinite state concurrent programs using various types of bisimulation reductions. The proposed methods work for the class of programs in which the functions that update the variables are mutually commutative. A number of bi-simulation relations are presented for such systems. Explicit state model checking methods that employ on-the-fly reductions with respect to these bi-simulations are given. Some of these methods have been implemented and have been used to verify some well known protocols that employ integer variables.	automata theory;bi-directional text;bisimulation;computer aided verification;diagram;hybrid automaton;ieee transactions on software engineering;integer factorization;lecture notes in computer science;liveness;model checking;reachability problem;simulation;springer (tank);static program analysis;turing completeness	A. Prasad Sistla;Min Zhou;Xiaodong Wang	2005		10.1007/978-3-540-30579-8_17	model checking;standard model;directed graph;concurrency;computer science;theoretical computer science;programming language;abstraction model checking;algorithm	Logic	-17.36207084264506	26.789503737306333	108309
3b3a65179e91b571b5536c84e300276718f87a73	boolean operations and inclusion test for attribute-element constraints	esquema;xml schema;closure;blow up;difference operator;language theory;estrategia;automaton;teoria lenguaje;algorithme test;interseccion;schema;strategy;automata;boolean operation;informatique theorique;automate;xml;inclusion;decidibilidad;cerradura;formal language theory;intersection;decidabilite;strategie;scheme;divide and conquer;lenguaje formal;theorie langage;formal language;fermeture;decidability;computer theory;informatica teorica;langage formel	"""The history of schema languages for XML is (roughly) an increase of expressiveness. While early schema languages mainly focused on the element structure, Clark first paid an equal attention to attributes by allowing both element and attribute constraints in a single constraint expression (we call his mechanism """"attribute-element constraints""""). In this paper, we investigate intersection and difference operations and inclusion test for attribute-element constraints, in view of their importance in static typechecking for XML processing programs. The contributions here are (1) proofs of closure under intersection and difference as well as decidability of inclusion test and (2) algorithm formulations incorporating a """"divide-and-conquer"""" strategy for avoiding an exponential blowup for typical inputs."""	boolean algebra	Haruo Hosoya;Makoto Murata	2006	Theor. Comput. Sci.	10.1016/j.tcs.2006.05.004	combinatorics;discrete mathematics;computer science;xml schema;mathematics;automaton;programming language;algorithm;algebra	Logic	-8.459640421237554	18.691073142190763	108342
bf5b6b5fa1e05be37b4b97c24896e67d4bc30cb7	industrial-strength formally certified sat solving	automotive industry;satisfiability;boolean satisfiability;artificial intelligent;theorem prover;logic in computer science;sat solver;correctness proof	Boolean Satisfiability (SAT) solvers are now routinely used in the verification of large industrial problems. However, their application in safety-critical domains such as the railways, avionics, and automotive industries requires some form of assurance for the results, as the solvers can (and sometimes do) have bugs. Unfortunately, the complexity of modern, highly optimized SAT solvers renders impractical the development of direct formal proofs of their correctness. This paper presents an alternative approach where an untrusted, industrial-strength, SAT solver is plugged into a trusted, formally certified, SAT proof checker to provide industrial-strength certified SAT solving. The key novelties and characteristics of our approach are (i) that the checker is automatically extracted from the formal development, (ii), that the combined system can be used as a standalone executable program independent of any supporting theorem prover, and (iii) that the checker certifies any SAT solver respecting the agreed format for satisfiability and unsatisfiability claims. The core of the system is a certified checker for unsatisfiability claims that is formally designed and verified in Coq. We present its formal design and outline the correctness proofs. The actual standalone checker is automatically extracted from the the Coq development. An evaluation of the certified checker on a representative set of industrial benchmarks from the SAT Race Competition shows that, albeit it is slower than uncertified SAT checkers, it is significantly faster than certified checkers implemented on top of an interactive theorem prover.	automated proof checking;automated theorem proving;avionics;benchmark (computing);boolean satisfiability problem;chaff algorithm;coq (software);correctness (computer science);executable;expectation propagation;formal methods;hol (proof assistant);isabelle;ocaml;proof assistant;rendering (computer graphics);server (computing);software bug;solver;usability	Ashish Darbari;Bernd Fischer;Joao Marques-Silva	2009	CoRR		discrete mathematics;#sat;computer science;artificial intelligence;theoretical computer science;boolean satisfiability problem;programming language;algorithm	Logic	-16.457035325965467	25.49173000902792	108480
1fc3ab04a495e27551498570df5e6ee0e7300747	mona implementation secrets	decision procedure;monadic second order logic;the mona tool;finite state automata;tree automata;data structure	The Mona tool provides an implementation of automaton-based decision procedures for the logics WS1S and WS2S. It has been used for numerous applications, and it is remarkably efficient in practice, even though it faces a theoretically non-elementary worst-case complexity. The implementation has matured over a period of six years. Compared to the first naive version, the present tool is faster by several orders of magnitude. This speedup is obtained from many different contributions working on all levels of the compilation and execution of formulas. We present an overview of Mona and a selection of implementation “secrets” that have been discovered and tested over the years, including formula reductions, DAGification, guided tree automata, three-valued logic, eager minimization, BDD-based automata representations, and cache-conscious data structures. We describe these techniques and quantify their respective effects by experimenting with separate versions of the Mona tool that in turn omit each of them.	best, worst and average case;compiler;data structure;decision problem;experiment;no silver bullet;speedup;three-valued logic;tree automaton;worst-case complexity	Nils Klarlund;Anders Møller;Michael I. Schwartzbach	2002	Int. J. Found. Comput. Sci.	10.1142/S012905410200128X	data structure;computer science;artificial intelligence;theoretical computer science;finite-state machine;programming language;algorithm	PL	-15.070600260243106	25.262716817820763	108512
6603fc8036193c89d21e73a1fccda66906e53115	soft session types	session types;programming language;logic in computer science;linear logic;type system	We show how systems of session types can enforce interactions to be bounded for all typable processes. The type system we propose is based on Lafont’s soft linear logic and is strongly inspired by recent works about session types as intuitionistic linear logic formulas. Our main result is the existence, for every typable process, of a polynomial bound on the length of any reduction sequence starting from it and on the size of any of its reducts.	high- and low-level;interaction;lambda calculus;linear logic;polynomial;recursion;server (computing);type inference;type system	Ugo Dal Lago;Paolo Di Giamberardino	2011		10.4204/EPTCS.64.5	linear logic;type system;computer science;theoretical computer science;mathematics;distributed computing;programming language;algorithm	Logic	-10.843226045245602	22.496071962539354	108613
641f57430fdc7fe29c448b504f09c7df7c607f2e	modleing and checking networks of communicating real-time process	modelizacion;diagrama binaria decision;entrada salida;diagramme binaire decision;protocole transmission;etude theorique;maquina estado finito;telecommunication network;real time processing;abstraction;abstraccion;experimental result;input output;modelisation;tratamiento tiempo real;protocolo transmision;traitement temps reel;red telecomunicacion;estudio teorico;reseau telecommunication;resultado experimental;theoretical study;resultat experimental;machine etat fini;modeling;finite state machine;entree sortie;binary decision diagram;transmission protocol	In this paper we present a new modeling formalism that is well suited for modeling real-time systems in different application areas and on various levels of abstraction. These I/O-interval structures extend interval structures by a new communication method, where input sensitive transitions are introduced. The transitions can be labeled time intervals as well as with communication variables. For interval structures, efficient model checking techniques based on MTBDDs exist. Thus, after composing networks of I/O-interval structures, efficient model checking of interval structures is applicable. The usefulness of the new approach is demonstrated by various real-world case studies, including experimental results.	real-time transcription	Jürgen Ruf;Thomas Kropf	1999		10.1007/3-540-48153-2_20	input/output;systems modeling;telecommunications;computer science;artificial intelligence;abstraction;finite-state machine;binary decision diagram;abstraction model checking;algorithm;telecommunications network	Embedded	-9.66900382096899	27.050569729970796	108688
005c4c63b795a388d0fcc1d7d3ada4025ba2bc48	lr-parsing derived	developpement logiciel;lenguaje programacion;no determinismo;automata aceptor;programming language;language theory;sistema informatico;computer system;gramatica cf;teoria lenguaje;exactitude programme;acceptor automaton;grammaire cf;non determinism;exactitud programa;non determinisme;context free grammar;desarrollo logicial;informatique theorique;software development;analizador sintaxico;langage programmation;systeme informatique;parser;finite automaton;automate accepteur;analyseur syntaxique;angelique;program development;theorie langage;computer theory;informatica teorica;program correctness	The LR(k)-parsing algorithm is derived, i.e., presented and proved as an interplay between program development and parsing theory. The program development uses invariants and the new concept of weakest angelic precondition. The parsing theory involved relates rightmost derivability to three other transitive relations on strings. The usual stack of item sets and the finite automaton appear in an optimisation of the abstract algorithm.	algorithm;angelic layer;automaton;finite-state machine;invariant (computer science);lr parser;mathematical optimization;parsing;precondition	Wim H. Hesselink	1992	Sci. Comput. Program.	10.1016/0167-6423(92)90007-X	computer science;artificial intelligence;philosophy of language;software development;finite-state machine;context-free grammar;programming language;algorithm	Logic	-5.744470499763011	21.40368539280707	108810
b272a331f37d9ef5d8b980a39eba42bd2fcd1b5a	parametric (co)iteration vs. primitive direcursion	higher order;denotational semantic	Freyd showed that in certain CPO-categories, locally continuous functors have minimal invariants, which possess a structure that he termed dialgebra. This gives rise to a category of dialgebras and homomorphisms, where the minimal invariants are initial, inducing a powerful recursion scheme (direcursion) on a cpo. In this paper, we identify a problem appearing when translating (co)iterative functions (on a fixed parameterised datatype) to direcursion (on the same datatype), and present a solution to this problem as a recursion scheme (primitive direcursion), generalising and symmetrising primitive (co)recursion for endofunctors. To this end, we give a uniform technique for translating (co)iterative maps into direcursive maps. This immediately gives a plethora of examples of direcursive functions, improving on the situation in the literature where only a few examples have appeared. Moreover, a technical trick proposed in a POPL paper is avoided for the translated maps. We conclude the paper by applying the results to a denotational semantics of Abadi and Cardelli’s typed object calculus, and linking them to previous work on higher-order coalgebra and to bisimulations.	applicative programming language;bernhard schölkopf;circular layout;denotational semantics;embedded system;exemplification;initial algebra;linear algebra;map;object-based language;primitive recursive function;quicksort;recursion;symposium on principles of programming languages	Johan Glimming	2007		10.1007/978-3-540-73859-6_18	discrete mathematics;higher-order logic;computer science;pure mathematics;mathematics;programming language	PL	-13.328324513879666	18.717963859076526	108839
71375ad8269b1d272807e7bf487b092281d30df4	approach for minimal-siphon computation in s4pr		The efficient siphon computation is the key to the development of siphon-based deadlock control strategies with good performance. This work studies the computation of minimal siphons in a class of Petri nets called S4PR. Firstly, we propose a function with polynomial complexity to determine whether a resource subset can generate a minimal siphon. Next, using the technique of problem partitioning, a new approach is developed to compute all minimal siphons in S4PR. Finally, an example is given to illustrate the proposed approach.	computation;deadlock;petri net;time complexity	Dan You;Shouguang Wang;Wenzhan Dai;Wenhui Wu	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8123214	siphon;discrete mathematics;deadlock;machine learning;computation;petri net;computer science;artificial intelligence;polynomial	Robotics	-6.923093816505517	28.957995512514962	108857
7bfd5b5a6d49bea147df33c930b98e7913a04c5e	a lazy unbounded model checker for event-b	formal specification language;theorem prover;state space	Formal specification languages are traditionally supported by theorem provers, but recently model checkers have proven to be useful tools. In this paper we present Eboc, an explicit state model checker for Event-B. Eboc is based on lazy techniques that allow it to fairly perform an exhaustive state space search without bounding the size of the sets used in the specification. We describe the implementation of Eboc and provide a preliminary comparison with ProB, an existing bounded model checker for classical B.	algorithm;b-method;enumerated type;event-driven programming;formal methods;formal specification;invariant (computer science);lazy evaluation;microsoft outlook for mac;model checking;software engineering;state space search;systems architecture	Paulo J. Matos;Bernd Fischer;Joao Marques-Silva	2009		10.1007/978-3-642-10373-5_25	computer science;state space;automated theorem proving;programming language;algorithm	Logic	-17.774483392353858	27.346779130491836	109020
5aab9c25aa3e0d254c736ca625920e3d2b03f788	a framework for protein and membrane interactions	labelled transition system;reactive system;logic in computer science;protein interaction;quantitative method;meta model;protein level	We introduce the Bioβ Framework, a meta-model for both protein-level and membrane-level interactions of living cells. This formalism aims to provide a formal setting where to encode, compare and merge models at different abstraction levels; in particular, higher-level (e.g. membrane) activities can be given a formal biological justification in terms of low-level (i.e., protein) interactions. A Bioβ specification provides a protein signature together a set of protein reactions, in the spirit of the κ-calculus. Moreover, the specification describes when a protein configuration triggers one of the only two membrane interaction allowed, that is “pinch” and “fuse”. In this paper we define the syntax and semantics of Bioβ , analyse its properties, give it an interpretation as biobigraphical reactive systems, and discuss its expressivity by comparing with κcalculus and modelling significant examples. Notably, Bioβ has been designed after a bigraphical metamodel for the same purposes. Hence, each instance of the calculus corresponds to a bigraphical reactive system, and vice versa (almost). Therefore, we can inherith the rich theory of bigraphs, such as the automatic construction of labelled transition systems and behavioural congruences.	bigraph;bisimulation;dhrystone;docking (molecular);encode;formal system;gillespie algorithm;high- and low-level;interaction;metamodeling;simulation;synthetic biology;synthetic intelligence;type system;zeller's congruence	Giorgio Bacci;Davide Grohmann;Marino Miculan	2009		10.4204/EPTCS.11.2	metamodeling;quantitative research;reactive system;computer science;artificial intelligence;mathematics;algorithm	Logic	-11.775196996562338	19.820999498358358	109105
51576c50a12bdff3e65d8238f17343ddeee37566	a program transformation for faster goal-directed search	concurrent computing instruments programming silicon context syntactics instruction sets;goal directed verifiers program transformation goal directed search reachability call graph source to source transformation input program entry procedure output program concurrent programs;reachability analysis program verification	A goal-directed search attempts to reveal only relevant information needed to establish reachability (or unreachability) of the goal from the initial state of the program. The further apart the goal is from the initial state, the harder it can get to establish what is relevant. This paper addresses this concern in the context of programs with assertions that may be nested deeply inside its call graph - thus, far away interprocedurally from main. We present a source-to-source transformation on programs that lifts all assertions in the input program to the entry procedure of the output program, thus, revealing more information about the assertions close to the entry of the program. The transformation is easy to implement and applies to sequential as well as concurrent programs. We empirically validate using multiple goal-directed verifiers that applying this transformation before invoking the verifier results in significant speedups, sometimes up to an order of magnitude.	algorithm;call graph;heuristic (computer science);program transformation;reachability;source lines of code;source transformation	Akash Lal;Shaz Qadeer	2014	2014 Formal Methods in Computer-Aided Design (FMCAD)	10.1109/FMCAD.2014.6987607	real-time computing;computer science;distributed computing;programming language;algorithm	PL	-18.242847675959784	30.835519808596196	109204
7b2fc140750ee0f186cbf3bf7a083d090efb1479	guarded transitions in evolving specifications	machine abstraite;developpement logiciel;algebraic specification;modele mathematique;maquina abstracta;simultaneidad informatica;state machine;modelo matematico;abstract machine;concurrency;desarrollo logicial;specification algebrique;software development;mathematical model;simultaneite informatique	We represent state machines in the category of speci cations, where assignment statements correspond exactly to interpretations between theories [7, 8]. However, the guards on an assignment require a special construction. In this paper we raise guards to the same level as assignments by treating each as a distinct category over a shared set of objects. A guarded assignment is represented as a pair of arrows, a guard arrow and an assignment arrow. We give a general construction for combining arrows over a factorization system, and show its specialization to the category of speci cations. This construction allows us to de ne the ne structure of state machine morphisms with respect to guards. Guards de ne the ow of control in a computation, and how they may be translated under re nement is central to the formal treatment of safety, liveness, concurrency, and determinism.	concurrency (computer science);finite-state machine;indeterminacy in concurrent computation;liveness;ne (complexity);partial template specialization;theory	Dusko Pavlovic;Douglas R. Smith	2002		10.1007/3-540-45719-4_28	discrete mathematics;concurrency;computer science;artificial intelligence;software development;mathematical model;database;mathematics;distributed computing;abstract machine;programming language;algorithm	Logic	-11.434016488273844	21.889631877392667	109606
fe440b29629d60f9bd9f547381703861beeca099	the equational theory of pomsets	modelizacion;relation ordre partiel;programmation;red petri;programacion paralela;parallel programming;concurrent program;programacion;modelisation;partial ordering;informatique theorique;programa competidor;relacion orden parcial;petri net;modeling;programming;reseau petri;programme concurrent;programmation parallele;computer theory;informatica teorica	Abstract   Pomsets have been introduced as a model of concurrency. Since a pomset is a string in which the total order has been relaxed to be a partial order, in this paper we view them as a generalization of strings, and investigate their algebraic properties. In particular, we investigate the axiomatic properties of pomsets, sets of pomsets and ideals of pomsets, under such operations as concatenation, parallel composition, union and their associated closure operations. We find that the equational theory of sets, pomsets under concatenation, parallel composition and union is finitely axiomatizable, whereas the theory of languages under the analogous operations is not. A similar result is obtained for ideals of pomsets, which incorporate the notion of subsumption which is also known as augmentation. Finally, we show that the addition of any closure operation (parallel or serial) leads to nonfinite axiomatizability of the resulting equational theory.		Jay L. Gischer	1988	Theor. Comput. Sci.	10.1016/0304-3975(88)90124-7	partially ordered set;programming;combinatorics;discrete mathematics;systems modeling;computer science;mathematics;petri net;algorithm;algebra	ECom	-7.6213243500373915	21.72260579341879	109791
85c865dbe51e30300518727b696d779a1184bd95	analyzing demand in non-strict functional programming languages		ion, 18, 67, 81, 82, see also λx.s accept, 114 application, 18, 52, 66, 67, 115, 126, 131–133, 228, 229, 254, 255, 257, see also rule application approximation, 10, 88, 89, 93, 94, 123, 150 set-based, 14 argument, 87 argument context, 76, 76, 77, 78 blank symbol, 113, 114 case, 18 co-domain, 20, 100, 206 combinator, 23 fixpoint, 34, 35, 79, 81 complete, 163, 171, 177, 181 externally, 182, 193–195 internally, 181 complete lattice, 10, 89, 90, 96 component, 62, 87, 87, 97, 98, 100, 107, 122– 124, 150, 152, 164, 167, 204–206, 212, 217, 223, 256 of a union, 189, 201 computation, 8, 9, 14, 15, 114, 114, 115, 117– 119, 121, 122, 261 accepting, 122 infinite, 199 concretization, 10, 11, 85, 105, 105, 108, 109, 126, 143, 144, 147, 167, 177, 179, 180, 227, 229 configuration, 114, 114, 119 final, 114, 114 start, 114, 114, 116 constructor, 17, 18, 18, 32, 50–53, 56, 59, 67,	approximation;computation;fixed point (mathematics);functional programming;programming language;strict function	Marko Schütz	2000			comparison of multi-paradigm programming languages;theoretical computer science;programming language theory;second-generation programming language;functional logic programming;fourth-generation programming language;programming paradigm;fifth-generation programming language;third-generation programming language;computer science	SE	-12.129342540421069	18.57891237458373	109805
6822eb7f896333fd6c064f5de7c383036913b719	reo connectors and components as tagged signal models		Tagged Signal Model (TSM) is a denotational framework and a meta-model to study certain properties of models of computation. To study the behavior of Reo connectors in a closed system, we propose two denotational semantics for Reo using TSM. TSM is very similar to the coalgebraic model of Timed Data Streams (TDS), the first formal semantics and the basis for most of the other formal semantics of Reo. There is a direct mapping between the time – data pairs of TDS, and tag – value of TSM. This work shows how treating tags to be either totally or partially ordered has a direct consequence on the results. We looked into five primitive connectors of Reo in both these settings and discuss the determinacy of systems.	rca connector	Marjan Sirjani;Fatemeh Ghassemi;Bahman Pourvatan	2018		10.1007/978-3-319-90089-6_11	discrete mathematics;denotational semantics;data stream mining;determinacy;semantics of logic;model of computation;mathematics	Logic	-9.444398047577916	19.275761751597305	109876
069e21471aaafb054fa99a683034a703376d9e72	first-order and temporal logics for nested words	nested words;three- variable property;temporal logic;temporal logics;nested word automata;model checking.;la russell;martin-lof type theory;equality judgement;first-order expressive completeness;nested word;dependent function type;satisfiability;logic design;state machine;boolean functions;inspection;navigation;tree data structures;first order logic;computer science;model checking;first order;tree structure;automata;xml	Nested words are a structured model of execution paths in procedural programs, reflecting their call and return nesting structure. Finite nested words also capture the structure of parse trees and other tree-structured data, such as XML. We provide new temporal logics for finite and infinite nested words, which are natural extensions of LTL, and prove that these logics are first-order expressivelycomplete. One of them is based on adding a ”within” modality, evaluating a formula on a subword, to a logic CaRet previously studied in the context of verifying properties of recursive state machines. The other logic is based on the notion of a summary path that combines the linear and nesting structures. For that logic, both model-checking and satisfiability are shown to be EXPTIME-complete. Finally, we prove that first-order logic over nested words has the three-variable property, and we present a temporal logic for nested words which is complete for the twovariable fragment of first-order.		Rajeev Alur;Marcelo Arenas;Pablo Barceló;Kousha Etessami;Neil Immerman;Leonid Libkin	2007		10.1109/LICS.2007.19	combinatorics;discrete mathematics;nested set model;computer science;nested word;first-order logic;mathematics;finite-state machine;programming language;algorithm	Logic	-13.275668523285448	22.43547008538328	110021
8b7bc06150721201f267024a090eecd8fac85e7e	efficient implementation of rewriting revisited technical report	complexity analysis;turing machine;term rewrite system;graph rewriting;efficient implementation;computational complexity;rewrite systems;logic in computer science	Recently, many techniques have been introduced that allow the (automated) classification of the runtime complexity of term rewrite systems (TRSs for short). In earlier work, the authors have shown that for confluent TRSs, innermost polynomial runtime complexity induces polytime computability of the functions defined. In this paper, we generalise the above result to full rewriting. Following our previous work, we exploit graph rewriting. We give a new proof of the adequacy of graph rewriting for full rewriting that allows for a precise control of the resources copied. In sum we completely describe an implementation of rewriting on a Turing machine (TM for short). We show that the runtime complexity of the TRS and the runtime complexity of the TM is polynomially related. Our result strengthens the evidence that the complexity of a rewrite system is truthfully represented through the length of derivations. Moreover our result allows the classification of non-deterministic polytime-computation based on runtime complexity analysis of rewrite systems.	analysis of algorithms;computability;computation;graph rewriting;polynomial;rewrite (programming);time complexity;turing machine	Martin Avanzini;Georg Moser	2010	CoRR		discrete mathematics;computer science;turing machine;theoretical computer science;mathematics;computational complexity theory;confluence;algorithm;graph rewriting	Logic	-14.548819363062787	24.85971304664453	110027
4d75b36b63a2dbfeab1465d37e5228d3f9a44275	an algebraic system of temporal structures	linear structures;formal specification;temporal logic;formal verification algebra formal specification;model expressions temporal logic linear structures;formal verification;algebra;model expressions;algebraic system model checking real time specifications continuous systems first order equivalence linear temporal structures leonard lauchli;games algebra computational modeling syntactics continuous time systems complexity theory cognition	Lauchli and Leonard, in 1966, described a series of operations which are able to build all linear temporal structures up to first order equivalence. More recently these operations have been used to describe executions of continuous systems for the purposes of model checking real-time specifications. In this paper we present an algebra over these operations and show that it is both sound and complete, in that it can generate all equivalences over these models.	computation;computational model;model checking;real-time clock;recursion;recursively enumerable set;regular expression;theory;turing completeness	Tim French;John Christopher McCabe-Dansted;Mark Reynolds	2013	2013 20th International Symposium on Temporal Representation and Reasoning	10.1109/TIME.2013.18	discrete mathematics;formal verification;theoretical computer science;pure mathematics;mathematics	Logic	-12.14815180926572	22.93065667835668	110132
33ee6772a5c8ee24820832b7e856a6489590aa28	automating induction with an smt solver	natively support induction;program verifier;mechanical proof assistant;simple inductive theorem;smt solvers;inductive proof;smt solver;program verification;simple tactic;proof assistant;automating induction	Mechanical proof assistants have always had support for inductive proofs. Sometimes an alternative to proof assistants, satisfiability modulo theories (SMT) solvers bring the hope of a higher degree of automation. However, SMT solvers do not natively support induction, so inductive proofs require some encoding into the SMT solver’s input. This paper shows a surprisingly simple tactic—a rewriting strategy and a heuristic for when to apply it—that has shown to be useful in verifying simple inductive theorems, like those that can occur during program verification. The paper describes the tactic and its implementation in a program verifier, and reports on the positive experience with using the tactic.	brute-force search;formal verification;heuristic;isabelle;mathematical induction;modulo operation;proof assistant;rewriting;satisfiability modulo theories;solver;verification and validation	K. Rustan M. Leino	2012		10.1007/978-3-642-27940-9_21	computer science;theoretical computer science;programming language;algorithm	AI	-16.389362849153684	25.041440005651285	110395
01023844ed5ea2f9987a4a4abecdefd83e8d676e	backward deterministic büchi automata on infinite words		This paper describes how backward deterministic Büchi automata are defined, what their main features are, and how they can be applied to solve problems in temporal logic. 1998 ACM Subject Classification automata over infinite objects, modal and temporal logics	automata theory;büchi automaton;modal logic;temporal logic	Thomas Wilke	2017		10.4230/LIPIcs.FSTTCS.2017.6	discrete mathematics;algebra;automaton;computer science;temporal logic	Logic	-10.257798269318721	23.621275349937793	110399
7af80d5ffec2c4b49d1582eafc7f4e980e8bbdf9	the higher-order recursive path ordering	rewrite rule;lambda calculus;monomorphic instances higher order recursive path ordering termination proof techniques reduction orderings higher order setting typed lambda calculus polymorphic higher order function symbols polymorphic typing function symbols higher order calculi higher order rewrite rules godel s recursor termination property;higher order;theorem proving;expressive power;pattern matching large scale integration functional programming logic programming design methodology;formal verification;rewriting systems;polymorphism;type theory;strong normalization;higher order functions;recursive path ordering;typed lambda calculus;formal verification lambda calculus theorem proving rewriting systems type theory	This paper extends the termination proof techniques based on reduction orderings to a higher-order setting, by adapting the recursive path ordering definition to terms of a typed lambda-calculus generated by a signature of polymorphic higher-order function symbols. The obtained ordering is well-founded, compatible with -reductions and with polymorphic typing, monotonic with respect to the function symbols, and stable under substitution. It can therefore be used to prove the strong normalizationpropert y of higher-order calculi in which constants can be defined by higher-order rewrite rules. For example, the polymorphic version of Gödel’s recursor for the natural numbers is easily oriented. And indeed, our ordering is polymorphic, in the sense that a single comparison allows to prove the termination property of all monomorphic instances of a polymorphic rewrite rule. Several other non-trivial examples are given which examplify the expressive power of the ordering.	calculus of constructions;computable function;gödel;higher-order function;intuitionistic type theory;path ordering (term rewriting);perturbation theory;recursion;rewrite order;rewriting;termination analysis;typed lambda calculus	Jean-Pierre Jouannaud;Albert Rubio	1999		10.1109/LICS.1999.782635	polymorphism;typed lambda calculus;discrete mathematics;higher-order logic;formal verification;computer science;lambda calculus;mathematics;automated theorem proving;programming language;higher-order function;type theory;expressive power;algorithm	Logic	-14.80896518113184	20.674132441208457	110550
3bb93b46eb54f488e1a3cd401923a8b7a283cfea	ordinary interactive small-step algorithms, iii	postulates;behavioral equivalence;global step;ordinary interactive small-step algorithm;abstract state machines;interaction;abstract state machine;sequential algorithms;interactive algorithm;arbitrary algorithm;small-step algorithm;asm semantics;behaviorally equivalent;abstract state machine thesis;bounded amount;equivalence of algorithms	This is the third in a series of three articles extending the proof of the Abstract State Machine thesis---that arbitrary algorithms are behaviorally equivalent to abstract state machines---to algorithms that can interact with their environments during a step, rather than only between steps. As in the first two articles of the series, we are concerned here with ordinary, small-step, interactive algorithms. This means that the algorithms:  (1) proceed in discrete, global steps,  (2) perform only a bounded amount of work in each step,  (3) use only such information from the environment as can be regarded as answers to queries, and  (4) never complete a step until all queries from that step have been answered.  After reviewing the previous articles' definitions of such algorithms, of behavioral equivalence, and of abstract state machines (ASMs), we prove the main result: Every ordinary, interactive, small-step algorithm is behaviorally equivalent to an ASM.  We also discuss some possible variations of and additions to the ASM semantics.	abstract state machines;algorithm;turing completeness	Andreas Blass;Yuri Gurevich	2007	ACM Trans. Comput. Log.	10.1145/1243996.1243999	randomized algorithms as zero-sum games;discrete mathematics;computer science;artificial intelligence;analysis of parallel algorithms;mathematics;programming language;algorithm;abstract state machines;super-recursive algorithm	Theory	-6.596196048274464	22.70087883446223	110568
2cbfcf1c0fee21d206a80b084c09f1c0f05d4671	many-sorted high-level nets	petri nets australia concurrent computing telecommunications inhibitors concrete power system modeling performance analysis laboratories algebra;abstract data type;satisfiability;algebraic nets many sorted high level nets abstract data types petri nets algebraic framework inhibitor arcs many sorted algebra colored petri nets place capacities p nets many sorted versions predicate transition;petri nets data structures;levels of abstraction;data structures;colored petri net;petri nets;petri net	Many-sorted high-level nets (h4HLNs) combine abstract data types and Petri nets within the same algebraic framework, and include inhibitor arcs and place capacities. Manysorted signatures are used to define inscriptions. h4HLNs are defined at two different levels of abstraction. At an abstract level markings and capacities are defined by terms. This is suitable for specifying classes of systems. At the concrete level, a many-sorted algebra satisfying the signature, is used for markings and capacities. Both abstract and concrete h4HLNs can be given an interpretation in terms of Coloured Petri Nets extended by place capacities and inhibitors, known as P-nets. A hierarchy of high-level nets, including many-sorted versions of Predicate-Transition (PrT) nets and Algebraic nets, is developed and differences with their single-sorted versions are discussed.	abstract data type;antivirus software;coloured petri net;high- and low-level;linear algebra;principle of abstraction	Jonathan Billington	1989		10.1109/PNPM.1989.68550	discrete mathematics;stochastic petri net;computer science;theoretical computer science;process architecture;petri net;algorithm	Logic	-12.27783236422655	22.23644218089929	111016
62a8025241fd5646bfa652814f4bab3a47eba061	towards a categorical representation of reversible event structures		We study categories for reversible computing, focussing on reversible forms of event structures. Event structures are a well-established model of true concurrency. There exist a number of forms of event structures, including prime event structures, asymmetric event structures, and general event structures. More recently, reversible forms of these types of event structures have been defined. We formulate corresponding categories and functors between them. We show that products and coproducts exist in many cases. In most work on reversible computing, including reversible process calculi, a cause-respecting condition is posited, meaning that the cause of an event may not be reversed before the event itself. Since reversible event structures are not assumed to be cause-respecting in general, we also define cause-respecting subcategories of these event structures. Our longer-term aim is to formulate event structure semantics for reversible process calculi.	cascading style sheets;causal filter;concurrency (computer science);data structure;distributed transaction;expectation propagation;liveness;process calculus;reversible computing;undefined behavior	Eva Graversen;I. Phillips;Nobuko Yoshida	2017		10.4204/EPTCS.246.9	discrete mathematics;pattern recognition	Logic	-9.92713093819096	21.387264998481076	111213
b63464b90e4c500768198d98ca3002db40cfb18a	improving the results of program analysis by abstract interpretation beyond the decreasing sequence		The classical method for program analysis by abstract interpretation consists in computing first an increasing sequence using an extrapolation operation, called widening, to correctly approximate the limit of the sequence. Then, this approximation is improved by computing a decreasing sequence without widening, the terms of which are all correct, more and more precise approximations. It is generally admitted that, when the decreasing sequence reaches a fixpoint, it cannot be improved further. As a consequence, most efforts for improving the precision of an analysis have been devoted to improving the limit of the increasing sequence. In a previous paper, we proposed a method to improve a fixpoint after its computation. This method consists in computing from the obtained solution a new starting value from which increasing and decreasing sequences are computed again. The new starting value is obtained by projecting the solution onto well-chosen components. The present paper extends and improves the previous paper: the method is discussed in view of some example programs for which it fails. A new method is proposed to choose the restarting value: the restarting value is no longer a simple projection, but is built by gathering and combining information backward the widening nodes in the basic solution. Experiments show that the new method properly solves all our examples, and improves significantly the results obtained on a classical benchmark.	abstract interpretation;approximation algorithm;benchmark (computing);computation;experiment;extrapolation;fixed point (mathematics);numerical analysis;overhead (computing);polyhedron;program analysis	Rémy Boutonnet;Nicolas Halbwachs	2018	Formal Methods in System Design	10.1007/s10703-017-0310-y	mathematical optimization;program analysis;computer science;extrapolation;theoretical computer science;computation;abstract interpretation;fixed point;basic solution	PL	-12.265015577129688	30.523682796906506	111263
a43448b53386f0237bb9c5602ca71acf3a2d4d2e	static analysis of intensional databases in u-datalog	static analysis	Static analysis of declarative languages deals with the detection, at compile time, of program properties that can be used to better understand the program semantics and to improve the efficiency of the program evaluation. In logical update languages, an interesting problem is the detection of situations that may lead to inconsistent updates (insertion and deletion of the same fact), generating non-deterministic behavior. The analysis of this problem for transactions based on set-oriented updates is not a simple task. In this paper, we investigate this topic in the context of the UDatalog language, a set-oriented update language for deductive databases [BMM92]. We first formally define relevant properties of UDatalog programs, mainly related to conflicts leading to non-deterministic results. Then} we prove that the presented prop erties are decidable. Our results are based on an analysis tool, the querg tree, first used in [LS92].	compile time;compiler;datalog;debugging;decision problem;deductive database;intensional logic;programming paradigm;semantics (computer science);static program analysis	Elisa Bertino;Barbara Catania	1996		10.1145/237661.237711	computer science;theoretical computer science;data mining;mathematics;static analysis;algorithm	DB	-17.927260672483946	21.951871731025278	111312
0ee3dc20a7bfa451af6558fcc8421b93160b8d50	failures semantics for a simple process language with refinement	process algebra;partial order	We study a suitable semantic theory based on the standard failures preorder [BHR84] for a simple process algebra which includes operators for the refinement of actions by processes. We present a model-theoretic and a behavioural characterization of the largest precongruence associated with failures preorder,  F c , over the language we consider.  The model-theoretic characterization is in terms of a pomset failures model which is shown to be fully abstract with respect to  F c . Pomset failures (labelled partial orders with refusal sets) allow us to express the nonsequential as well as the nondeterministic characteristics of  F c . The behavioural characterization is in terms of a variation on the standard failures preorder based on ideas of ST-semantics, [Gla90].    		Luca Aceto;Uffe Engberg	1991		10.1007/3-540-54967-6_63	partially ordered set;process calculus;computer science;theoretical computer science;programming language;algorithm	PL	-10.677829200495994	21.459024074840368	111332
6c5a4c33b93f0328f410202fa85351a95b0133f6	addressing state explosion in behavior protocol verification	software components;behavior protocols;state explosion;parse trees;formal verification;software component;state space;proof of concept	A typical problem formal verification faces is the size of the model of a system being verified. Even for a small system, the state space of the model tends to grow exponentially (state explosion). In this paper, we present a new representation of state spaces suitable for implementing operations upon behavior protocols of software components [1]. The proposed representation is linear in length of the source behavior protocol. By trading space for time, it allows handling behavior protocols of “practical size”. As a proof of concept, a verification tool for behavior protocols is discussed.	component-based software engineering;formal verification;jan dietz;mathematical optimization;parse tree;program test authority;requirement;sofa;state space;tree (data structure);tree automaton	Martin Mach;Frantisek Plasil	2004			proof of concept;component-based software engineering;distributed computing;theoretical computer science;state space;formal verification;computer science	SE	-12.782715849389424	29.496682793317444	111617
05669dcb45755624bcf49550b5e3c7232c948db9	footprints in local reasoning	programming language;separation logic;standard model	Local reasoning about programs exploits the natural local behaviour common in programs by focussing on the footprint that part of the resourc e accessed by the program. We address the problem of formally characterising and analysing the no tio of footprint for abstract local functions introduced by Calcagno, O’Hearn and Yang. With our defi nition, we prove that the footprints are the only essential elements required for a complete spec ification of a local function. We formalise the notion of small specifications in local reasoning and sho w that, for well-founded resource models, a smallest specification always exists that only includ es the footprints. We also present results for the non-well-founded case. Finally, we use this theory o f fo tprints to investigate the conditions under which the footprints correspond to the smallest safe s t t s. We present a new model of RAM in which, unlike the standard model, the footprints of every program correspond to the smallest safe states. We also identify a general condition on the primitiv e commands of a programming language which guarantees this property for arbitrary models.	apl;computation;framing (world wide web);html element;locality of reference;programming language;random-access memory;spec#;surround sound;yang	Mohammad Raza;Philippa Gardner	2008	Logical Methods in Computer Science	10.2168/LMCS-5(2:4)2009	natural language processing;standard model;separation logic;horn clause;programming domain;computer science;machine learning;functional logic programming;programming paradigm;fifth-generation programming language;programming language;prolog;logic programming	Logic	-16.786633339333275	20.783599530681677	111779
ea9d4d3fe59f25e261c354e75932021f4db539b1	non-monotonic refinement of control abstraction for concurrent programs	concurrent programs;experimental evaluation	ion for Concurrent Programs Ashutosh Gupta, Corneliu Popeea, and Andrey Rybalchenko Technische Universität München Abstract. Verification based on abstraction refinement is a successful technique for checking program properties. Conventional abstraction refinement schemes increase precision of the abstraction monotonically, and therefore cannot recover from overly precise refinement decisions. This problem is exacerbated in the context of multi-threaded programs, where keeping track of all control locations in concurrent threads is the inevitably discovered abstraction and is prohibitively expensive. In contrast to the conventional (partition refinement-based) approaches, nonmonotonic abstraction refinement schemes rely on re-partitioning and have promising potential for avoiding excess of precision. In this paper, we propose a non-monotonic refinement scheme for the control abstraction (of concurrent programs). Our approach employs a constraint solver to discover re-partitioning at each refinement step. An experimental evaluation of our non-monotonic control abstraction refinement on a collection of multi-threaded verification benchmarks indicates its effectiveness in practice. Verification based on abstraction refinement is a successful technique for checking program properties. Conventional abstraction refinement schemes increase precision of the abstraction monotonically, and therefore cannot recover from overly precise refinement decisions. This problem is exacerbated in the context of multi-threaded programs, where keeping track of all control locations in concurrent threads is the inevitably discovered abstraction and is prohibitively expensive. In contrast to the conventional (partition refinement-based) approaches, nonmonotonic abstraction refinement schemes rely on re-partitioning and have promising potential for avoiding excess of precision. In this paper, we propose a non-monotonic refinement scheme for the control abstraction (of concurrent programs). Our approach employs a constraint solver to discover re-partitioning at each refinement step. An experimental evaluation of our non-monotonic control abstraction refinement on a collection of multi-threaded verification benchmarks indicates its effectiveness in practice.	abstraction (software engineering);concurrent computing;non-monotonic logic;offset binary;partition refinement;refinement (computing);solver;subdivision surface;thread (computing)	Ashutosh Gupta;Corneliu Popeea;Andrey Rybalchenko	2010		10.1007/978-3-642-15643-4_15	real-time computing;abstraction inversion;computer science;theoretical computer science;refinement;abstraction model checking;algorithm	PL	-17.15560205360859	28.810889910044242	111849
3fd3db7403db24a14f636db3b201819b7823157c	depth-first search satisfiability of the μ-calculus with converse over trees	complexity theory;boolean functions;automata;radio frequency;data structures;calculus;cognition	The μ-calculus is a modal logic with least and greatest fixed-point operators, encompassing many temporal, program and description logics such as LTL, PDL, CTL and ALCQIO reg . Many decision procedures have been proposed for the μ-calculus, however few implementations have been shown useful in practice. In this paper, we propose a satisfiability algorithm for the μ-calculus with converse interpreted on finite unranked trees. In contrast with current state of the art algorithms, mostly automata-based, we propose an algorithm based on a depth-first search. We prove the algorithm to be correct (sound and complete) and optimal (EXPTIME). We also provide an implementation, which shows significant performance improvement with respect to a known breadth-first search based algorithm.	depth-first search;modal μ-calculus	Yensen Limón;Everardo Bárcenas;Edgard Benítez-Guerrero;María Auxilio Medina Nieto	2017		10.1109/CONIELECOMP.2017.7891827	discrete mathematics;cognition;data structure;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;automaton;boolean function;radio frequency;algorithm	NLP	-12.257471893095936	23.160563923692056	111901
d71d02d38f313bcf38344dd59290724e23db33d3	symbolic model checking with partitioned transistion relations	asynchronous circuit	 We significantly reduce the complexity of BDD-based symbolic verificationby using partitioned transition relations to represent state transition graphs.This method can be applied to both synchronous and asynchronous circuits.The times necessary to verify a synchronous pipeline and an asynchronousstack are both bounded by a low polynomial in the size of the circuit. Wewere able to handle stacks with over 1050reachable states and pipelines withover 10120reachable states.1  	model checking	Jerry R. Burch;Edmund M. Clarke;David E. Long	1991			model checking;abstraction model checking;theoretical computer science;discrete mathematics;computer science;asynchronous circuit;symbolic trajectory evaluation	Logic	-12.596378727351231	26.68217255575697	111948
3bb9929517e2f38684177e94b063bf19019af5e4	an overview of marq		MarQ is a runtime monitoring tool for specifications written as quantified event automata, an expressive automata-based specification language based on the notion of parametric trace slicing. MarQ has performed well in the runtime verification competition and implements advanced indexing and redundancy elimination techniques. This overview describes the basic structure and functionality provided by MarQ and gives a brief description of how to use the tool.	automata theory;automaton;runtime verification;specification language	Giles Reger	2016		10.1007/978-3-319-46982-9_34	redundancy (engineering);theoretical computer science;computer science;specification language;search engine indexing;parametric statistics;slicing;runtime verification	Logic	-18.250562283494617	27.854032897073488	112018
6541aaa142ab202dde8e67941495c2777ca412a9	inheritance and cofree constructions	algebraic specification	"""The coalgebraic view on classes and objects is elaborated to include inheritance. Inheritance in coalgebraic speciication (of classes) will be understood dually to parametrization in algebraic speciication. That is, inheritance involves restriction (specialization), where parametrization involves extension. And cofree constructions are \best"""" restrictions, like free constructions are \best"""" extensions. To make this view on inheritance precise we need a suitable notion of behaviour preserving morphism between classes, which will be deened as a \coalgebra map up-to-bisimulation""""."""	algebraic equation;bisimulation;freetype;generic programming;partial template specialization	Bart Jacobs	1996		10.1007/BFb0053063	computer science	PL	-13.064814740048755	18.708733617066606	112190
e3cf80e5f4b9cbb3addbec072a2a8feb132a4f98	on the first-order equivalence of call-by-name and call-by-value	functional programming;first order	Within the framework of (first–order) recursive applicative program schemes we prove the parameter–passing mechanisms call–by– name and call–by–value to be of the same computational power, thus solving an open problem in the theory of functional programming. The equivalence proof is given constructively by a detour through flowchart program schemes which operate on pushdown stores. This result is in contrast to the non–deterministic (i.e., language–theoretic) case where the outermost (OI) and the innermost (IO) expansion strategy of macro grammars lead to incomparable classes of string languages.	applicative programming language;first-order predicate;flowchart;functional programming;higher-order function;plotkin bound;recursion;run time (program lifecycle phase);simulation;stack (abstract data type);theory;time complexity;turing completeness	Thomas Noll	1994		10.1007/BFb0017486	equivalence partitioning	PL	-11.835045186702077	19.586731939801	112212
73342cc7fed9b48005595d3d920e4d4d25c73789	about the decision of reachability for register machines	verification;modelizacion;sistema infinito;non decidabilite;configuracion;problem;trajectoire;machine registre;reachability;limit trajectory;order;red petri;fonction;fonction affine;machine;vecteur;probleme;registre;dynamical system;infinite state systems;modelisation;systeme dynamique;maquina;iteraccion;trajectory;asequibilidad;affine function;trajectoire limite;decision;ordre;iteration;atteignabilite;trayectoria;vector;decidibilidad;undecidability;problema;verificacion;sistema dinamico;register machine;decidabilite;configuration;petri net;modeling;systeme infini;functions;registro;register;reseau petri;orden;decidability;infinite system	We study the decidability of the following problem: given p affine functions f 1 ,..., f p  over N k  and two vectors v 1 , v 2  E N k , is v 2  reachable from v 1  by successive iterations of f 1 ,...,f p  (in this given order)? We show that this question is decidable for p = 1, 2 and undecidable for some fixed p.	reachability;register machine	Véronique Cortier	2002	ITA	10.1051/ita:2003001	decidability;combinatorics;machine;discrete mathematics;verification;systems modeling;iteration;vector;order;computer science;trajectory;dynamical system;affine transformation;mathematics;configuration;reachability;petri net;function;algorithm;register machine	Logic	-8.23169605349499	25.69460269860141	112268
2064106c003b2dbb535e8599cca4663d19eab608	analysis of deterministic and stochastic petri nets	program diagnostics;protocols;program diagnostics petri nets markov processes protocols reachability analysis;transition probability;stochastic petri net;efficient algorithm;simple communication protocol deterministic petri nets stochastic petri nets execution policies deterministic transitions subordinated markov chain embedded markov chain transition probabilities memory requirements;communication protocol;markov processes;stochastic processes petri nets fires delay electromagnetic compatibility sliding mode control steady state transient analysis closed form solution research and development;petri nets;reachability analysis;embedded markov chain;steady state;markov chain	We present a time and space efficient algorithm for computing steady state solutions of deterministic and stochastic Petri nets (DSPNs) with both stochastic and structural extensions. The algorithm can deal with different execution policies associated with deterministic transitions of a DSPN. The definition of a subordinated Markov chain (SMC) is refined to reduce the computational cost of deriving the transition probabilities of the embedded Markov chain (EMC) underlying a DSPN. Closed-form expressions of these transition probabilities are presented for some SMC topologies. Moreover, we propose to make use of the reward structure defined on the DSPN to reduce memory requirements. The usefulness of the proposed extensions and the steps of the solution algorithm are illustrated using a DSPN of a simple communication protocol.	algorithm;algorithmic efficiency;communications protocol;embedded system;markov chain;requirement;steady state;stochastic petri net	Gianfranco Ciardo;Christoph Lindemann	1993		10.1109/PNPM.1993.393454	markov chain;discrete mathematics;real-time computing;stochastic petri net;computer science;distributed computing;process architecture;markov model;petri net;variable-order markov model	AI	-10.862495729898102	29.133323466409923	112524
951690ee1ea75c9a222aed46dc07075daeec9c7e	proof and refutation in formal software development	theorem proving;model checking;automatic theorem proving;software development	In this paper we describe investigations into the use of automatic theorem proving technology in the refutation of proof obligations. Specifically, we discuss the use of resolution based theorem proving and model checking to find false obligations and counterexamples. These techniques can be used as basis of an automatic method for finding faults in design during the formal development of software. This approach is complementary to verifcation by proof as such proofs can only be completed when all faults have been corrected. We give a simple example using the B formal development method to demonstrate its potential.	automated theorem proving;formal methods;model checking;software development	Juan Bicarregui;Brian Matthews	1999			formal proof;model checking;computer science;software development;automated proof checking;automated theorem proving;programming language;proof complexity;algorithm	SE	-15.407138875639992	27.338120219635183	112586
79a06fc8c56a5bafcb4190ef6ff9ff0689c0d1ab	equivalence between answer-set programs under (partially) fixed input		Answer Set Programming has become an increasingly popular formalism for declarative problem solving. Among the huge body of theoretical results, investigations of different equivalence notions between logic programs play a fundamental role for understanding modularity and optimization. While strong equivalence between two programs holds if they can be faithfully replaced by each other in any context (facts and rules), uniform equivalence amounts to equivalent behavior of programs under any set of facts. Both notions (as well as several variants thereof) have been extensively studied. However, the somewhat reverse notion of equivalence which holds if two programs are equivalent under the addition of any set of proper rules (i.e., all rules except facts) has not been investigated yet. In this paper, we close this gap and give a thorough study of this notion, which we call rule equivalence , and its parameterized version where we allow facts over a given restricted alphabet to appear in the context. This notion of equivalence is thus a relationship between two programs whose input is (partially) fixed but where additional proper rules might still be added. Such a notion might be helpful in debugging of programs. We give full characterization results and a complexity analysis for the propositional case of rule equivalence and its relativized versions. Moreover, we address the problem of program simplification under rule equivalence. Finally, we show that rule equivalence is decidable in the non-ground case.	analysis of algorithms;answer set programming;co-np;debugging;mathematical optimization;problem solving;rule 184;rule 90;semantics (computer science);stable model semantics;strong duality;symbolic computation;time complexity;turing completeness	Bernhard Bliem;Stefan Woltran	2016		10.1007/978-3-319-30024-5_6	logical equivalence;algorithm	AI	-12.74302089336509	19.623914916113605	112828
002b75e0613a76e99fabf6564cee676a96e29cb7	towards logical frameworks in the heterogeneous tool set hets	logic translation;mere proof checking;model theoretic focus;model checker;efficient proof support;automated theorem provers;new logic;proof theoretic;towards logical framework;model finder;integration tool;heterogeneous tool set hets	LF is a meta-logical framework that has become a standard tool for representing logics and studying their properties. Its focus is proof theoretic, employing the Curry-Howard isomorphism: propositions are represented as types, and proofs as terms. Hets is an integration tool for logics, logic translations and provers, with a model theoretic focus, based on the meta-framework of institutions, a formalisation of the notion of logical system. In this work, we combine these two worlds. The benefit for LF is that logics represented in LF can be (via Hets) easily connected to various interactive and automated theorem provers, model finders, model checkers, and conservativity checkers thus providing much more efficient proof support than mere proof checking as is done by systems like Twelf. The benefit for Hets is that (via LF) logics become represented formally, and hence trustworthiness of the implementation of logics is increased, and correctness of logic translations can be mechanically verified. Moreover, since logics and logic translations are now represented declaratively, the effort of adding new logics or translations to Hets is greatly reduced. This work is part of a larger effort of building an atlas of logics and translations used in computer science and mathematics.	antivirus software;automated proof checking;automated theorem proving;best practice;computer science;correctness (computer science);curry;curry–howard correspondence;eclipse xtext;first-order logic;formal system;hard coding;interconnection;isabelle;logical framework;model checking;modular programming;proof calculus;scalability;soundness (interactive proof);spec#;synergy;theory;time complexity;trust (emotion);twelf	Mihai Codescu;Fulya Horozal;Michael Kohlhase;Till Mossakowski;Florian Rabe;Kristina Sojakova	2010		10.1007/978-3-642-28412-0_10	t-norm fuzzy logics;algorithm	Logic	-15.391492961064351	19.18254159277836	112875
10b9a084eca0003b91bb4c7ca59cbd0139ba0131	mops: an infrastructure for examining security properties of software	verification;security properties;safety properties;model checking;finite state automaton;program analysis;program analysis tool;static analysis;security	We describe a formal approach for finding bugs in security-relevant software and verifying their absence. The idea is as follows: we identify rules of safe programming practice, encode them as safety properties, and verify whether these properties are obeyed. Because manual verification is too expensive, we have built a program analysis tool to automate this process. Our program analysis models the program to be verified as a pushdown automaton, represents the security property as a finite state automaton, and uses model checking techniques to identify whether any state violating the desired security goal is reachable in the program. The major advantages of this approach are that it is sound in verifying the absence of certain classes of vulnerabilities, that it is fully interprocedural, and that it is efficient and scalable. Experience suggests that this approach will be useful in finding a wide range of security vulnerabilities in large programs efficiently.	algorithm;data compaction;encode;experiment;finite-state machine;model checking;modulo operation;personal digital assistant;program analysis;pushdown automaton;reachability;scalability;software bug;software design pattern;software verification;stack (abstract data type);verification and validation;vulnerability (computing)	Hao Chen;David A. Wagner	2002		10.1145/586110.586142	program analysis;computer security model;model checking;real-time computing;verification;computer science;information security;theoretical computer science;finite-state machine;computer security;static analysis	Security	-18.673412169389557	28.154503887340784	113023
104da3f3895f0d49654b06dfa1dbe5dd212ded87	branching time temporal logic	linear time temporal logic;temporal logic;satisfiability;monadic second order;expressive power;model checking;decision procedure;linear time;tree automata;parallel computer;concurrent programs	K e y w o r d s : Modal and Temporal Logic: Branching time temporal logic, linear time temporal logic, dynaimic logics, expressiveness, axiomatics, decidability, decision procedures, satisfiability, model checking; Logics of Programs: Reasoning about concurrent programs, program specification, program verification, specification of and reasoning about fairness; Software Engineering: Specification techniques, mechanical synthesis, automated verification techniques; Computational Complexity; Automata Theory: Finite-state automata on infinite objects, tree automata. ?This work was supported in part by NSF grant DCR-8511354, ONR URI contract N00014--86-K-0763, and Netherlands N'vVO grant nf-3/nfb 62-500.	automata theory;computational complexity theory;expressive power (computer science);fairness measure;finite-state machine;formal specification;formal verification;ibm notes;linear temporal logic;modal logic;model checking;software engineering;time complexity;tree automaton	E. Allen Emerson;Jai Srinivasan	1988		10.1007/BFb0013022	dynamic logic;discrete mathematics;linear temporal logic;description logic;concurrency;interval temporal logic;computation tree logic;intermediate logic;theoretical computer science;mathematics;fair computational tree logic;monadic predicate calculus;substructural logic;multimodal logic;algorithm;temporal logic of actions	Logic	-12.979592168607514	22.607602187820255	113034
04ffb0950114ce47eab5f5cc31e571779eece023	quasi-static scheduling of embedded software using equal conflict nets	tarea concurrente;concepcion sistema;red petri;transition;theorie quasi statique;algorithme;algorithm;transicion;systeme t pondere;system design;scheduling;reseau conflit egal;ordonamiento;teoria cuasiestatica;tâche concurrente;petri net;concurrent task;conception systeme;ordonnancement;reseau petri;ordonnancement quasi statique;quasi static theory;embedded software;algoritmo	Embedded system design requires the use of eecient scheduling policies to execute on shared resources, e.g. the processor, algorithms that consist of a set of concurrent tasks with complex mutual dependencies. Scheduling techniques are called static when the schedule is computed at compile time, dynamic when some or all decisions are made at run-time. The choice of the scheduling policy mainly depends on the speciication of the system to be designed. For speciications containing only data computation, it is possible to use a fully static scheduling technique, while for speciications containing data-dependent control structures, like the if-then-else or while-do constructs, the dynamic behaviour of the system cannot be completely predicted at compile time and some scheduling decisions are to be made at run-time. For such applications we propose a Quasi-static scheduling (QSS) algorithm that generates a schedule in which run-time decisions are made only for data-dependent control structures. We use Equal Connict (EC) nets as underlying model, and deene quasi-static schedulability for EC nets. We solve QSS by reducing it to a decomposition of the net into connict-free components. The proposed algorithm is complete, in that it can solve QSS for any EC net that is quasi-statically schedulable.	atm turbo;algorithm;compile time;compiler;computation;conditional (computer programming);control flow;data dependency;embedded software;embedded system;mutual exclusion;naruto shippuden: clash of ninja revolution 3;overhead (computing);schedule (computer science);scheduling (computing);systems design	Marco Sgroi;Luciano Lavagno;Yosinori Watanabe;Alberto L. Sangiovanni-Vincentelli	1999		10.1007/3-540-48745-X_13	fair-share scheduling;fixed-priority pre-emptive scheduling;real-time computing;earliest deadline first scheduling;embedded software;dynamic priority scheduling;computer science;rate-monotonic scheduling;transition;operating system;two-level scheduling;database;distributed computing;least slack time scheduling;round-robin scheduling;scheduling;petri net;algorithm;systems design	Embedded	-7.7745184759210675	31.4940931997105	113099
8b88a9829f4bfacbd9cf6df68f471dd3e01195f7	bounded model checking for past ltl	temporal logic;bounded model checking;formal verification;model checking;linear temporal logic;reactive system;experimental evaluation	The introduction of Past Operators enables to produce more n atural formulation of a wide class of properties of reactive system s, compared totraditional pure future temporal logics. For this reason, past temporal logics are gaining increasing interest in several application areas, r nging from Requirement Engineering to Formal Verification and Model Checking. We show how SAT-based Bounded Model Checking techniques can be extende d to deal with Linear Temporal Logics with Past Operators (PLTL). Though a pparently simple, this task turns out to be absolutely non-trivial when tackle d in its full generality. We discuss a bounded semantics for PLTL, we show that it i s correct (and complete), and propose an encoding scheme able to cope with P LTL formulas. Finally, we implement the encoding in NuSMV, and present a fir st experimental evaluation of the approach.	formal verification;line code;model checking;newton–cotes formulas;nusmv;temporal logic	Marco Benedetti;Alessandro Cimatti	2003		10.1007/3-540-36577-X_3	model checking;discrete mathematics;linear temporal logic;temporal logic;formal verification;reactive system;computation tree logic;computer science;artificial intelligence;theoretical computer science;mathematics;programming language;algorithm	Logic	-14.280283987108074	25.711596636530267	113218
3b4df4ebce48c3432b899a38bd336d1a5ad5cdde	reversing computations modelled by coloured petri nets		Reversible computation is an unconventional form of computing where any sequence of performed operations can be executed in reverse order at any point during computation. It has recently been attracting increasing attention as on the one hand it promises low-power computation and on the other hand it is inherent or of interest in a variety of applications. In this paper we propose a structural way of translating reversing Petri nets (RPNs), a formalism that embeds the three main forms of reversibility (backtracking, causal reversing and out-of-causalorder reversing), to Coloured Petri Nets (CPNs), an extension of traditional Petri Nets, where tokens carry data values. The translation into the CPN model uses additional places and transitions in order to capture the machinery employed in the RPN framework and demonstrates that the abstract model of RPNs, and thus the principles of reversible computation, can be emulated in CPNs. The transformation can be automated and utilized for the analysis of reversible systems using CPN Tools.	backtracking;causal filter;coloured petri net;computation;computer science;cycle (graph theory);encode;emulator;low-power broadcasting;reverse polish notation;reversible computing;reversing: secrets of reverse engineering;semantics (computer science);whole earth 'lectronic link	Kamila Barylska;Anna Gogolinska;Lukasz Mikulski;Anna Philippou;Marcin Piatkowski;Kyriaki Psara	2018			discrete mathematics;computation;reversing;petri net;computer science	SE	-12.762555687841175	26.670143770089158	113300
c00cb1bee4c5bcaefc1613b149f28b57a936cbbc	time abstracted bisimiulation: implicit specifications and decidability	systeme temps reel;computability and decidability;calculabilite decidabilite;raisonnement;razonamiento;reasoning;real time systems	In the last few years a number of real-time process calculi have emerged with the purpose of capturing important quantitative aspects of real-time systems. In addition, a number of process equivalences sensitive to time-quantities have been proposed, among these the notion of timed (bisimulation) equivalence. In this paper, we introduce a time-abstracting (bisimulation) equivalence and investigate its properties with respect to the real-time process calculus of Wang (Real-time behaviour of asynchronous agents, in Proceedings of CONCUR90, Lecture Notes in Computer Science, Vol. 458, Springer-Verlag, Berlin/New York, 1990). Seemingly, such an equivalence would yield very little information (if any) about the timing properties of a process. However, time-abstracted reasoning about a composite process may yield important information about the relative timing-properties of the components of the system. In fact, we show as a main theorem that such implicit reasoning will reveal all timing aspects of a process. More precisely, we prove that two processes are interchangeable in any context up to time-abstracted equivalence precisely if the two processes are themselves timed equivalent. As our second main theorem, we prove that time-abstracted equivalence is decidable for the calculus of Wang, using classical methods based on a finite-state symbolic, structured operational semantics.		Kim G. Larsen;Wang Yi	1993		10.1007/3-540-58027-1_8	logical equivalence;discrete mathematics;mathematics;algorithm	Logic	-10.44577188831021	24.552283961831858	113318
5afff51f4cf495c02793f65d2e47ab1ea1487811	using mtbdds for discrete timed symbolic model checking	verification;directed graphs;mtbdd;quantitative computation tree logic;symbol manipulation;discrete timed symbolic model checking;state space methods;logic design;boolean functions;multi valued binary decision diagram;clocks;temporal logic;logic;qctl semantics;discrete time systems;timing properties;discrete time;diagrams;diagrams formal verification real time systems timing temporal logic logic design directed graphs discrete time systems symbol manipulation decision theory;embedded system;algorithm;computation tree logic;formal verification;model checking;data structures;fault tolerance;decision theory;iteration;boolean functions data structures logic timing clocks fault tolerance real time systems automation formal verification state space methods;validation;real time system;symbolic model checking;qctl semantics mtbdd discrete timed symbolic model checking verification timing properties validation embedded system real time system temporal logic automation multi valued binary decision diagram algorithm quantitative computation tree logic iteration;real time systems;binary decision diagram;timing;automation	The verification of timing properties is an important task in the validation process of embedded and real time systems. Temporal logic model checking is one of the most successful techniques as it allows the complete automation of the verification. In this paper, we present a new approach to symbolic QCTL (Quantitative CTL) model checking. In contrast to previous approaches we use an intuitive QCTL semantics, provide an efficient model representation and the new algorithms require less iteration steps compared to translating the QCTL problem into CTL and using standard CTL model checking techniques. The new model checking algorithm is based on a MTBDD representation. Some experimental results show the efficiency of the new approach.	algorithm;embedded system;iteration;model checking;temporal logic	Thomas Kropf;Jürgen Ruf	1997		10.1109/EDTC.1997.582356	model checking;discrete mathematics;computer science;theoretical computer science;ctl*;abstraction model checking;symbolic trajectory evaluation;algorithm	EDA	-12.691008661312475	27.25142970201547	113374
4930614459a41c136b909904efc752975f73a450	some consequences of the linear approximation of the stochastic context-free calculus	context free grammars;finite state approximations linear approximation stochastic context free calculus stochastic pushdown computation finite cut point;stochastic grammars and languages finite state automata chomsky hierarchy;approximation theory;stochastic processes;production grammar automata stochastic processes calculus linear approximation;stochastic processes approximation theory context free grammars	We introduce a technique of linear approximation for the stochastic pushdown computation with finite cut point. Some fundamental consequences of the approximation result are discussed. The technique is extended so to characterize the stochastic pushdown calculus in terms of convergent sequences of finite-state approximations.	computation;linear approximation;stack (abstract data type)	Marco Carpentier	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.519	stochastic process;stochastic calculus;stochastic approximation;combinatorics;discrete mathematics;deterministic context-free grammar;quantum stochastic calculus;continuous-time stochastic process;mathematics;context-free grammar;stochastic;algorithm;malliavin calculus;statistics;approximation theory	Robotics	-5.106266088513639	24.20509367036012	113614
4edbf83c75b86fa3d99edb4ddfc671082d5a671f	deadlock prevention based on structure reuse of petri net supervisors for flexible manufacturing systems	optimal solution;control systems;optimisation;flexible manufacturing systems;resource allocation;computer model;resource allocation condition monitoring discrete event systems flexible manufacturing systems optimal control optimisation petri nets redundancy;inequality constraint;satisfiability;flexible manufacturing system fms;optimal control;artificial neural networks;the theory of regions;control system;discrete event system;computational modeling;system recovery;deadlock prevention;redundancy;condition monitoring;monitoring;flexible manufacturing system;the theory of regions deadlock prevention discrete event system flexible manufacturing system fms petri net;discrete event system petri net supervisors automated flexible manufacturing systems automated fms resource utilization np hard problem deadlock prevention method computational tractability structural analysis optimal live controlled system siphon constraint redundancy condition identification plant net marking deadlock free controlled system;discrete event systems;petri nets;petri net;system recovery control systems monitoring petri nets computational modeling artificial neural networks educational institutions;artificial neural network;structure analysis	Deadlocks are an undesirable situation in automated flexible manufacturing systems (FMS). Their occurrences often deteriorate the utilization of resources and may lead to catastrophic results. Finding an optimal supervisor is NP-hard. A computationally efficient method often ends up with a suboptimal one. This paper develops a deadlock prevention method that makes a good tradeoff between optimality and computational tractability for a class of Petri nets, which can model many FMS. The theory of regions guides our efforts toward the development of near-optimal solutions for deadlock prevention. Given a plant net, a minimal initial marking is first decided by structural analysis, and an optimal live controlled system is computed. Then, a set of inequality constraints is derived with respect to the markings of monitors and the places in the model such that no siphon can be insufficiently marked. A method is proposed to identify the redundancy condition for constraints. For a new initial marking of the plant net, a deadlock-free controlled system can be obtained by regulating the markings of the monitors such that the inequality constraints are satisfied, without changing the structure of the controlled system. The near-optimal performance of a controlled net system via the proposed method is shown through several examples.	algorithmic efficiency;constraint (mathematics);deadlock;flexible-fuel vehicle;item unique identification;np-hardness;petri net;social inequality;structural analysis	Zhiwu Li;GaiYun Liu;Hans-Michael Hanisch;Mengchu Zhou	2012	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2011.2147308	real-time computing;computer science;control system;machine learning;petri net;artificial neural network	Robotics	-6.494123228254496	29.07444914822282	113627
862b42bddfebb3282445bdab3decc446fae3e926	proving forall-µ-calculus properties with sat-based model checking	distributed system;diagrama binaria decision;verificacion modelo;systeme reparti;diagramme binaire decision;modele kripke;metodo formal;methode formelle;verification modele;logique propositionnelle;mu calculo;program verification;constraint satisfaction;formal method;kripke model;satisfaction contrainte;verificacion programa;sistema repartido;model checking;propositional logic;modelo kripke;satisfaccion restriccion;logica proposicional;verification programme;mu calculus;mu calcul;binary decision diagram	In this paper, we present a complete bounded model checking algorithm for the universal fragment of μ-calculus. The new algorithm checks the completeness of bounded proof of each property on the fly and does not depend on prior knowledge of the completeness thresholds. The key is to combine both local and bounded model checking techniques and use SAT solvers to perform local model checking on finite Kripke structures. Our proof-theoretic approach works for any property in the specification logic and is more general than previous work on specific properties. We report experimental results to compare our algorithm with the conventional BDD-based algorithm.	model checking	Bow-Yaw Wang	2005		10.1007/11562436_10	model checking;discrete mathematics;formal methods;constraint satisfaction;computer science;mathematics;propositional calculus;binary decision diagram;abstraction model checking;algorithm	Logic	-15.436334243951933	26.791210884067393	114112
537bd4a88bf80033b84feb34b383d536483db3a6	interactive and probabilistic proof of mobile code safety	interactive proof system;software verification;software systems;interactive proof systems;safety properties;proof carrying code;interactive proofs;mobile code	This paper proposes a new proof-based approach to safe evolution of distributed software systems. Specifically, it extends the simple certification mechanism of proof-carrying code (PCC) to make it interactive and probabilistic, thereby devising interactive proof-carrying code (iPCC). With iPCC, a code consumer is convinced, with overwhelming probability, of the existence and validity of a safety proof of a transmitted code through interaction with a code producer. The iPCC mechanism theoretically solves the problem of proof explosion with PCC and can be used to efficiently prove a greater variety of safety properties that may require longer proofs. Technically, the class (PSPACE) of safety properties that are efficiently provable by iPCC is larger than the class (NP) efficiently provable by PCC. To illustrate the power of iPCC, this paper demonstrates that the verification of certain basic safety properties of typical machine instruction codes needs co-NP-complete computation, and shows how these safety properties can be efficiently verified by the iPCC mechanism.	co-np;co-np-complete;computation;distributed computing;interactive proof system;machine code;np-completeness;pspace;portable c compiler;proof-carrying code;provable security;software system	Yasuyuki Tsukada	2005	Automated Software Engineering	10.1007/s10515-005-6207-9	interactive proof system;software verification;computer science;theoretical computer science;software engineering;proof assistant;computer security;algorithm;software system	PL	-15.32022750073906	25.80238519287964	114484
1484a5f8d22bd57be806cf989f9cb1f29d91cb6d	framing the frame problem	slight variation;difficult problem;cognitive science;recent attempt;frame problem	Some parts of the software verification process require human annotation, but as much as possible of the rest should be automatic. An excellent candidate for full automation is change analysis, also known as the frame problem: how to determine which program properties remain unchanged under a given operation. The problem is particularly delicate in the case of programs using pointers or references, in particular object-oriented programs. The double frame inference strategy automates both frame specification and frame verification. On the specification side, it deduces the set of permissible changes of a routine (its “modifies clause”) from a simple examination of its postcondition. On the implementation side, it applies the change calculus, itself based on the alias calculus, to determine the set of expressions whose values the routine can actually change. Frame verification then consists of ascertaining that the actual change set is a subset of the permissible change set.	frame language;frame problem;framing (social sciences);pointer (computer programming);postcondition;software verification	Bertrand Meyer	2015		10.3233/978-1-61499-495-4-193	frame problem;philosophy;epistemology;mathematics;side effect;algorithm	Logic	-17.584178019594397	21.940457972291252	114505
540e79b244e6f16192b3c8584e2d317c5d37be76	construction and deduction methods for the formal development of software	type theory;software development;software development methods;higher order functions;calculus of constructions	In this paper we present an approach towards a framework based on the type theory ECC Extended Calculus of Constructions in which speci cations programs and operators for modular development by stepwise re nement can be formally described and reasoned about We show that generic software development steps can be expressed as higher order functions and demonstrate that proofs about their asserted e ects can be carried out in the underlying logical calculus For transformations requiring syntactic manipulations of objects a two level system comprising a Meta and an Object level is provided and it is shown how transformations can be formalized that faithfully represent operators on the object level	calculus of constructions;formal system;higher-order function;natural deduction;software development;stepwise regression;type theory	Friedrich W. von Henke;Axel Dold;Harald Ruess;Detlef Schwier	1995		10.1007/BFb0015465	discrete mathematics;formal methods;software framework;software development;software design description;pure mathematics;software construction;mathematics;goal-driven software development process;software development process	SE	-13.939266731425102	19.507493856948685	114530
06d065c4b5996ad08411f9b751390a081f1094b6	maintaining doubly-linked list invariants in shape analysis with local reasoning	shape analysis	This paper presents a novel shape analysis algorithm with local reasoning that is designed to analyze heap structures with structural invariants, such as doubly-linked lists. The algorithm abstracts and analyzes one single heap cell at a time. In order to maintain the structural invariants, the analysis uses a local heap abstraction that models the sub-heap consisting of one cell and its immediate neighbors. The proposed algorithm can successfully analyze standard doublylinked list manipulations.	algorithm;data structure;doubly linked list;shape analysis (digital geometry)	Sigmund Cherem;Radu Rugina	2007		10.1007/978-3-540-69738-1_17	computer science;theoretical computer science;binary heap;shape analysis;algorithm	PL	-17.840539792125334	25.20161293058583	114585
eee7c5c4bb5cf7e207ca8e376f2417654e583983	constrained dynamic partial order reduction		The cornerstone of dynamic partial order reduction (DPOR) is the notion of independence that is used to decide whether each pair of concurrent events p and t are in a race and thus both (p cdot t) and (t cdot p) must be explored. We present constrained dynamic partial order reduction (CDPOR), an extension of the DPOR framework which is able to avoid redundant explorations based on the notion of conditional independence—the execution of p and t commutes only when certain independence constraints (ICs) are satisfied. ICs can be declared by the programmer, but importantly, we present a novel SMT-based approach to automatically synthesize ICs in a static pre-analysis. A unique feature of our approach is that we have succeeded to exploit ICs within the state-of-the-art DPOR algorithm, achieving exponential reductions over existing implementations.	algorithm;exponential time hypothesis;partial order reduction;programmer;simultaneous multithreading;time complexity	Elvira Albert;Miguel Gómez-Zamalloa;Miguel Isabel;Albert Rubio	2018		10.1007/978-3-319-96142-2_24	implementation;theoretical computer science;parallel computing;computer science;partial order reduction;exponential function;programmer;exploit	PL	-15.504514402013111	26.56444446756675	114590
25d17d1750daa0e9ddf5d90477c96f68e9368885	ockhamist computational logic: past-sensitive necessitation in ctl	computational logic	The framework underlying CTL* is extended in order to include past operators Recent developments in areas related to concurrent program specification and verification, as well as database and information systems specification, justify the interest of such extension The semantics for the language so obtained is defined according to the ockhamist approach to non-deterministic time The differences between this semantics and the original semantics for CTL* are analysed. A sound axiomatization is proposed for such logic and its completeness is proved	axiomatic system;ctl*;computational logic;concurrent computing;dtime;formal specification;information system;ntime	Alberto Zanardo;José Carmo	1993	J. Log. Comput.	10.1093/logcom/3.3.249	linear temporal logic;computer science;computational logic;ctl*	Logic	-12.85065558102016	21.82444337838877	114733
33c49cf5dd2a1d2a90ed76c00b6fd580d40036ef	depth-bounded bottom-up evaluation of logic program	query language;evaluation performance;bottom up;performance evaluation;complexite calcul;top down;evaluacion prestacion;evaluation method;logical programming;lenguaje interrogacion;algorithme;algorithm;complejidad computacion;programmation logique;computational complexity;langage interrogation;systeme gestion base donnee;query answering;logic programs;programacion logica;sistema gestion base datos;database management system;article;algoritmo	> We present here a depth-bounded bottom-up evaluation algorithm for logic programs. We show that it is sound, complete, and terminating for nite-answer queries if the programs are syntactically restricted to Datalog nS , a class of logic programs with limited function symbols. Datalog nS is an extension of Datalog capable of representing innnite phenomena. Predicates in Datalog nS can have arbitrary unary and limited n-ary function symbols in one distinguished argument. We precisely characterize the computational complexity of depth-bounded evaluation for Datalog nS and compare depth-bounded evaluation with other evaluation methods, top-down and Magic Sets among others. We also show that universal safety ((niteness of query answers for any database) is decidable for Datalog nS. <	algorithm;bottom-up parsing;computational complexity theory;datalog;logic programming;newman's lemma;top-down and bottom-up design;unary operation	Jan Chomicki	1995	J. Log. Program.	10.1016/0743-1066(94)00030-A	computer science;top-down and bottom-up design;database;datalog;programming language;algorithm	DB	-7.497310476403311	19.221067820033	114761
90d416c084941bbcf5c01531300b8822b64948ae	deriving polymorphic type dependencies for logic programs using multiple incarnations of prop	polymorphism;logic programs	This paper illustrates the application of abstract compilation using multiple incarnations of the domain Prop in deriving type dependencies for logic programs. We illustrate how dependencies can be derived in the presence of both monomorphic and polymorphic type information. Type dependencies generalize the recently proposed notion of directional types as well as the more common notion of groundness dependencies. Directional types have proven useful in a number of applications such as in proving termination. These applications, however, are based on type declarations. The main contribution of this paper is in the simplicity in which non-trivial type dependencies are inferred using abstract compilation and by associating each type with an incarnation of Prop. We illustrate the use of a semantics for open logic programs in maintaining space e cient analyses. Time e ciency is also maintained due to approximation of the type domain in a boolean lattice calling on results of universal algebra.	approximation;compiler;logic programming;termination analysis;type system	Michael Codish;Bart Demoen	1994		10.1007/3-540-58485-4_47	polymorphism;computer science;theoretical computer science;programming language;algorithm	PL	-16.480917976902276	20.794411355553603	114844
7e04193866a0c619e93018da1c4b046c36750472	non-deterministic recursive ascent parsing	functional implementation;simple correctness proof;recursive ascent;recursive descent parser;non-deterministic recursive ascent parsing;compact representation;simple modification;kruseman aretz;normal cf grammar;extended cf grammar;non-lr grammar;regular expression;time complexity	A purely functional implementation of LR-parsers is given, together with a simple correctness proof. It is presented as a generalization of the recursive descent parser. For non-LR grammars the time-complexity of our parser is cubic if the functions that constitute the parser are implemented as memo-functions, i.e. functions that memorize the results of previous invocations. Memo-functions also facilitate a simple way to construct a very compact representation of the parse forest. For LR(0) grammars, our algorithm is closely related to the recursive ascent parsers recently discovered by Kruse-man Aretz [1] and Roberts [2]. Extended CF grammars (grammars with regular expressions at the right hand side) can be parsed with a simple modification of the LR-parser for normal CF grammars. 1 Introduction In this paper we give a purely functional implementation of LR-parsers, applicable to general CF grammars. It will be obtained as a generalization of the well-known recursive descent parsing technique. For LR(0) grammars , our result implies a deterministic parser that is closely related to the recursive ascent parsers discovered by Kruseman Aretz [1] and Roberts [2]. In the general non-deterministic case, the parser has cubic time complexity if the parse functions are implemented as memo-functions [3], which are functions that memorize and re-use the results of previous invocations. Memo-functions are easily implemented in most programming languages. The notion of memo-functions is also used to define an algorithm that constructs a cubic representation for the parse forest, i.e. the collection of parse trees. It has been claimed by Tomita that non-deterministic LR-parsers are useful for natural language processing. In [4] he presented a discussion about how to do non-deterministic LR-parsing, with a device called a graph-structured stack. With our parser we show that no explicit stack manipulations are needed; they can be expressed implicitly with the use of appropriate programming language concepts. Most textbooks on parsing do not include proper correctness proofs for LR-parsers, mainly because such proofs tend to be rather involved. The theory of LR-parsing should still be considered underdeveloped, for this reason. Our presentation, however, contains a surprisingly simple correctness proof. In fact, this proof is this paper's major contribution to parsing theory. One of its lessons is that the CF grammar class is often the natural one to proof parsers for, even if these parsers are devoted to some special class of grammars. If the gram-marlis restricted in some way, a parser for general CF …	algorithm;call stack;correctness (computer science);cubic function;earley parser;finite-state machine;glr parser;graph-structured stack;high- and low-level;indexed grammar;lr parser;memoization;natural language processing;parse tree;parsing;polynomial;programming language;recursion (computer science);recursive acronym;recursive ascent parser;recursive descent parser;regular expression;time complexity;times ascent;whole earth 'lectronic link	René Leermakers	1991			left recursion;natural language processing;time complexity;parser combinator;tree-adjoining grammar;indexed grammar;l-attributed grammar;deterministic context-free grammar;lalr parser;parsing expression grammar;computer science;theoretical computer science;glr parser;context-free grammar;programming language;recursive descent parser;regular expression;algorithm;lr parser;simple lr parser	PL	-13.454303465620063	19.252699098836146	114924
f3f60c0b9f7b7d947d684431552c030bfa612e89	relatively complete pushdown analysis of escape continuations		Escape continuations are weaker than full, first-class continuations but nevertheless can express many common control operators. Although language and compiler designs profitably leverage escape continuations, all previous approaches to analyze them statically in a higher-order setting have been ad hoc or imprecise. We present (mathrm {MCCFA}2), a generalization of (mathrm {CFA}2) that analyzes them with pushdown precision in their most-general form. In particular, the summarization algorithm of (mathrm {MCCFA}2) is both sound and complete with respect to a conservative extension of (mathrm {CFA}2)’s abstract semantics. We also present an continuation age analysis as a client of (mathrm {MCCFA}2) that reveals critical function call optimizations.		Kimball Germane;Matthew Might	2019		10.1007/978-3-030-11245-5_10	continuation;operator (computer programming);automatic summarization;algebra;theoretical computer science;compiler;subroutine;semantics;computer science;conservative extension	Logic	-18.159710342523745	25.854970779081775	114994
a7c1da03b4d47dd957684fd7b530b603262e4531	revising distributed unity programs is np-complete	theorems;fault tolerant;transformations;unity;formal methods;computations;distributed programs;complexity;automatic programming;safety properties;computer logic;formal method;automated revision;decision theory;polynomial time;algorithms;transformation;repair	We focus on automated revision techniques for adding Unity properties to distributed programs. We show that unlike centralized programs where multiple safety properties and one progress property can be added in polynomial-time, addition of a safety or a progress Unity property to distributed programs is significantly more difficult. Precisely, we show that such addition is NP-complete in the size of the given program’s state space. We also propose an efficient symbolic heuristic for addition of a leads-to property to distributed programs, which has applications in automated program synthesis.	advanced configuration and power interface;algorithm;centralized computing;computation;experiment;fault tolerance;heuristic;heuristic (computer science);np-completeness;polynomial;state space;time complexity;unity	Borzoo Bonakdarpour;Sandeep S. Kulkarni	2008		10.1007/978-3-540-92221-6_26	transformation;transformation;time complexity;fault tolerance;complexity;theorem;formal methods;decision theory;computer science;theoretical computer science;computation;distributed computing;programming language;algorithm	SE	-13.703052416106884	28.6192299939707	114995
22090505d1e173ee3cefa73aee1b2fd2f774ef20	deciding life-cycle inheritance on petri nets	herencia;arbre recherche;ciclo desarrollo;acuerdo;object oriented model;life cycle;red petri;heritage;agreement;search trees;arbol investigacion;t invariants;object oriented;backtracking;cycle developpement;object orientation;oriente objet;workflow;programming techniques programming;life cycle inheritance;object oriented modelling;petri nets;search tree;inheritance;petri net;orientado objeto;branching bisimilarity;accord;reseau petri;structural properties;dynamic behavior	One of the key issues of object-oriented modeling is inheritance. It allows for the definition of a subclass that inherits features from some superclass. When considering the dynamic behavior of objects, as captured by their life cycles, there is no general agreement on the meaning of inheritance. Basten and Van der Aalst introduced the notion of life-cycle inheritance for this purpose. Unfortunately, the search tree needed for deciding life-cycle inheritance is in general prohibitively large. This paper presents a backtracking algorithm to decide life-cycle inheritance on Petri nets. The algorithm uses structural properties of both the base life cycle and the potential sub life cycle to prune the search tree. Test cases show that the results are promising.	backtracking;bisimulation;business architecture;computable function;esa;experiment;liquid contact indicator;petri net;search algorithm;search tree;state diagram;unified modeling language;von luschan's chromatic scale	H. M. W. Verbeek;Twan Basten	2003		10.1007/3-540-44919-1_7	computer science;artificial intelligence;theoretical computer science;distributed computing;programming language;petri net;algorithm	AI	-16.646118939367998	26.829660358313667	115091
1437a00c932e893cc277cbf271b3ef457036d04a	simulation based computation of certificates for safety of dynamical systems		In this paper, we present an algorithm for synthesizing certificates for safety of continuous time dynamical systems, so-called barrier certificates. Unlike the usual approach of using constraint solvers to compute the certificate from the system dynamics, we synthesize the certificate from system simulations. This makes the algorithm applicable even in cases where the dynamics is either not explicitly available, or too complicated to be analyzed by constraint solvers, for example, due to the presence of transcendental function symbols. The algorithm itself allows the usage of heuristic techniques in which case it does not formally guarantee correctness of the result. However, in cases that do allow rigorous constraint solving, the computed barrier certificate can be rigorously verified, if desired. Hence, in such cases, our algorithm reduces the problem of finding a barrier certificate to the problem of formally verifying a given barrier certificate.	algorithm;barrier function;computation;constraint programming;constraint satisfaction problem;correctness (computer science);dynamical system;heuristic;simulation;system dynamics;usability;verification and validation	Stefan Ratschan	2017		10.1007/978-3-319-65765-3_17	mathematical optimization;dynamical systems theory;theoretical computer science;mathematics;certificate;system dynamics;computation;transcendental function	Logic	-11.18467695042628	28.071465326691055	115195
d67c2f14e89180ec5702df0fbdea536ad0a67738	converging from branching to linear metrics on markov chains		We study two well known linear-time metrics on Markov chains (MCs), namely, the strong and strutter trace distances. Our interest in these metrics is motivated by their relation to the probabilistic LTL-model checking problem: we prove that they correspond to the maximal differences in the probability of satisfying the same LTL and LTL (LTL without next operator) formulas, respectively. The threshold problem for these distances (whether their value exceeds a given threshold) is NP-hard and not known to be decidable. Nevertheless, we provide an approximation schema where each lower and upper-approximant is computable in polynomial time in the size of the MC. The upper-approximants are bisimilarity-like pseudometrics (hence, branching-time distances) that converge point-wise to the linear-time metrics. This convergence is interesting in itself, because it reveals a nontrivial relation between branching and linear-time metric-based semantics that does not hold in equivalence-based semantics.	approximation;bisimulation;computable function;converge;markov chain;maximal set;model checking;np-hardness;operational semantics;polynomial;time complexity;turing completeness	Giorgio Bacci;Giovanni Bacci;Kim G. Larsen;Radu Mardare	2015		10.1007/978-3-319-25150-9_21	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-6.282803606667339	19.8910637820098	115671
ff55a8795bdb0b493db360887dad641da807494d	functional plus logic programming with built-in and symbolic constraints	lenguaje programacion;programming language;logical programming;functional programming;langage seta;programmation logique;langage programmation;programmation fonctionnelle;logic programs;programacion logica;programacion funcional	In this paper we propose a lazy functional logic language, named SETA, which allows to handle multisets, built-in arithmetic constraints over the domain of real numbers, as well as various symbolic constraints over datatypes. As main theoretical results, we have proved the existence of free term models for all SETA programs and we have developed a correct and complete goal solving mechanism.	canonical account;computation;computational model;lazy evaluation;logic programming;parsing;theory	Puri Arenas;Francisco Javier López-Fraguas;Mario Rodrúguez-Arteljo	1999		10.1007/10704567_9	computer science;artificial intelligence;functional logic programming;mathematics;programming language;functional programming;algorithm	AI	-17.56105731775077	20.87796234280637	115772
17ea77dbce70c0c10b726e47b678a67b16ce5cb4	an algorithm deciding functional equivalence in a new class of program schemes	functional equivalence	Equivalence of program schemes is an undecidable problem even for a very rP.qtricted class of schemes [5,4]. I-Iowever, more interesting from the practical point of view, are algorithms deciding the functional (or more strongly) equivalence in some “good” classes of schemes. These positive results help us to develop the theory of program optimization and verification. The most remarkable decidability results proved in [ 101 and [l], give us complete knowledge of all the possibilities of equivalent transformations in schemes under consideration. In this paper, we propose an algorithm deciding the functional equivalence in the class of schemes we call through schemes. A characteristic feature of computation of a through scheme under a free interpretation is that the transition from a current test to the next one is done without losing the values of the terms on which the predicate is checked in the current test. In addition, each predicate is checked on a collecti c 3;; cd di”ferent terms corresponding to the different variables. The decision algorithm described will be useful for constructing methods for search of program invariants, which are necessary in program optimization, transformation and verification.	algorithm;computation;mathematical optimization;program optimization;turing completeness;undecidable problem	Viktor K. Sabelfeld	1990	Theor. Comput. Sci.	10.1016/0304-3975(90)90201-R	combinatorics;mathematical analysis;discrete mathematics;boundary-value analysis;computer science;equivalence partitioning;mathematics	Logic	-15.490762176584226	21.818568289897538	115816
6c8aad412bedddf67b6ef706083772c4d063fcb2	ltl model checking under fairness in prob		Model checking of liveness properties often results in unrealistic, unfair infinite behaviors as counterexamples. Fairness is a notion where the search is constrained to infinite paths that do not ignore infinitely the execution of a set of enabled actions. In this work we present an implementation for efficient checking of LTL formulas under strong and weak fairness in ProB, available for model checking B, Event-B, Z, CSP and CSP‖B models. The fairness checking algorithm can cope with both weak and strong fairness conditions, where the respective fairness conditions can be joined by means of the logical operators for conjunction and disjunction, which makes setting up and checking fairness to a property more flexible. We evaluate the implementation on various CSP models and compare it to the fairness implementation of the PAT tool.	algorithm;b-method;fairness measure;liveness;logical connective;model checking	Ivaylo Dobrikov;Michael Leuschel;Daniel Plagge	2016		10.1007/978-3-319-41591-8_14	computer science;operator (computer programming);real-time computing;model checking;counterexample;discrete mathematics;liveness	Logic	-11.53495248633498	21.048952433382976	116059
97e4bb53ea4aa9e0c84fa1917ed43333227eac48	the poor man's proof assistant: using prolog to develop formal language theoretic proofs	proof assistant;prolog;shuffle	While proving a theorem from a set of axioms is undecidable in first order logic, recent development has produced several tools which serve as automated theorem provers. However, often these systems are too complex for a given problem. Their usefulness is outweighed by the difficulty of learning a new tool or translating results into computer-readable form.   I describe tools developed in Prolog to partially characterize the shuffle-inclusion problem. These tools allowed for rapid development of proofs with little intellectual overhead. While focused around a specific problem, the techniques described are general, and well suited to many problems on discrete structures.	formal language;poorman;prolog;proof assistant;theory	Joey Eremondi	2013		10.1145/2508075.2508088	computer science;theoretical computer science;proof assistant;programming language;prolog;algorithm	PL	-18.16253366482707	19.337680137297014	116101
2f9ed1ea1dc225ebd57359e55e2b482b76b0c22c	distributed synthesis for regular and contextfree specifications		We consider the distributed realizability problem for systems with regular and deterministic contextfree local specifications. We characterize exactly the architectures for which the realizability problem is decidable. This extends known results on local specifications in two directions. First, architectures with cycles are allowed instead of just acyclic ones and second, deterministic contextfree specifications are considered.	directed acyclic graph	Wladimir Fridman;Bernd Puchala	2014	Acta Informatica	10.1007/s00236-014-0194-x	combinatorics;discrete mathematics;mathematics;algorithm	DB	-9.528428873566448	23.45089413303678	116142
96169d73b6fc0595de7ba1abac0ecc3b972a775c	interval temporal logics model checking	mathematics;physics;computational modeling;model checking;cognition;computer science;labeling	Model checking is a successful technique widely used in formal verification. Given a model of a system and a formula specifying a desired property of it, one can verify whether the system satisfies the property by checking the formula against the model. Distinctive features of model checking are: (i) it is a fully automatic process, (ii) it exaustively checks all the possible behaviours of the system, and (iii) it produces a counterexample, in case the property is violated. Systems are usually modeled as (finite) Kripke structures, that is, state-transition systems, and their properties are specified by formulas of point-based temporal logics, such as LTL, CTL, and the like. These logics allow one to express requirements on computation states and their relationships; however, they are not well suited to specify conditions on computation stretches, which come into play when dealing with, for instance, actions with duration, accomplishments, and temporal aggregations. To overcome the limitations of point-based logics, one can resort to interval temporal logics (ITLs), that assume time intervals,instead of time points, as their primitive entities. The most well-known ITL is Halpern and Shoham's modal logic of time intervals HS [4], which features one modality for each possible ordering relation between a pair of intervals, apart from equality. The satisfiability problem for HS has been studied in [4], and it turns out to be highly undecidable forall relevant (classes of) linear orders. The same holds for most fragments of it [2]; luckily, some meaningful exceptions exist, including the logic of temporal neighbourhood and the temporal logic of sub-intervals.	aggregate function;boolean satisfiability problem;computation;entity;formal verification;interval temporal logic;kripke structure (model checking);linear temporal logic;modal logic;modality (human–computer interaction);model checking;requirement;undecidable problem	Angelo Montanari	2016	2016 23rd International Symposium on Temporal Representation and Reasoning (TIME)	10.1109/TIME.2016.32	discrete mathematics;computation tree logic;mathematics;algorithm	Logic	-11.803578930230126	24.52074414779557	116237
f9675b5cb55b7612fc01ecabdfd37bcf2c021788	on the product form solution for stochastic petri nets	queueing network;stochastic petri net;exact solution;state space;petri net	The combinatorial explosion of the state space of Stochastic Petri Nets (SPNs) is a well known problem that inhibits the exact solution of large SPNs, and therefore a broad use of this kind of Petri Nets as a modelling tool. The same problem exists also for other modelling formalisms like for example Queueing Networks (QNs). In [13, 3] a class of QNs whose solution can be computed in an easy way was defined. For this class of models the solution can be factorized into terms that refer to each single queue of the network. This solution is known as Product Form Solution (PFS).  In this paper we compare two different approaches to PFS for SPNs. In both proposals the solution is obtained as a product form of terms, each term corresponding to a place in the SPN.      The first approach (by Lazar and Robertazzi) allows the PFS to be detected at state space level by inspecting the structure of the reachability graph. The second one (by Henderson, Lucic and Taylor) allows the PFS to be detected at structural level, that is to say without inspection of the reachability graph. In this paper we try to put the two approaches into a common framework and to show the important role played by T-invariants.    		Susanna Donatelli;Matteo Sereno	1992		10.1007/3-540-55676-1_9	discrete mathematics;stochastic petri net;computer science;state space;artificial intelligence;distributed computing;petri net	Robotics	-8.211566378845054	25.33784253823608	116339
892499871d94f10758e17f42764d3d96cdb36942	a theoretical framework for the declarative debugging of datalog programs	query language;theoretical framework;logic programs;deductive databases	The logic programming language Datalog has been extensively researched as a query language for deductive databases. Although similar to Prolog, the Datalog operational mechanisms are more intricate, leading to computations quite hard to debug by traditional approaches. In this paper, we present a theoretical framework for debugging Datalog programs based on the ideas of declarative debugging. In our setting, a debugging session starts when the user detects an unexpected answer for some query, and ends with the debugger pointing to either an erroneous predicate or to a set of mutually recursive predicates as the cause of the unexpected answer. Instead of representing the computations by means of trees, as usual in declarative debugging, we propose graphs as a more convenient structure in the case of Datalog, proving formally the soundness and completeness of the debugging technique. We also present a debugging tool implemented in the publicly available deductive database system DES following this theoretical framework.	algorithmic program debugging;computation;datalog;debugger;declarative programming;deductive database;experiment;logic programming;mutual recursion;predicate (mathematical logic);programming language;prolog;query language;sql;usability;workbench	Rafael Caballero;Yolanda García-Ruiz;Fernando Sáenz-Pérez	2008		10.1007/978-3-540-88594-8_8	computer science;theoretical computer science;database;algorithmic program debugging;datalog;programming language	DB	-18.001445446408496	22.133545075831748	116345
5c0a80718f9082aa6b6eac815edd36a7e26f18c4	specifying hardware in temporal logic & efficient synthesis of state-diagrams using prolog	temporal logic;state diagram		prolog;temporal logic	Masahiro Fujita;Hidehiko Tanaka;Tohru Moto-Oka	1984			linear temporal logic;computer architecture;dynamic logic (modal logic);interval temporal logic;horn clause;register-transfer level;theoretical computer science;computer science;temporal logic of actions;logic programming;prolog	EDA	-15.810756855378028	28.453139804519108	116432
d006557cfcced7c44177877386c1ce9cde54b663	imperative program specialisation: an approach using clp	declarative programming;programmation logique avec contrainte;semantica operacional;constraint logic programs;programming language;operational semantics;programacion logica con restriccion;program transformation;transformation programme;analisis programa;transformacion programa;semantique operationnelle;informatique theorique;declarative languages;directed graph;partial evaluation;constraint logic programming;program analysis;analyse programme;specialisation programme;computer theory;informatica teorica	The semantics of an imperative programming language can be expressed as a program in a declarative constraint language. Not only does this render the semantics executable, but it opens up the possibility of applying to imperative languages the advances made in program analysis and transformation of declarative languages. We propose a method for carrying out partial evaluation of imperative programs, using partial evaluation in a declarative language, but returning the results in the syntax of the imperative program which is to be partially evaluated. The approach uses a special form of the semantics and program points to aid partial evaluation in the reconstruction of a specialised imperative program from a partially evaluated semantics program. Constraints provide a means through which information is propagated inside both branches of a conditional, the body of a loop, and along chains of def-use chains in the program. The method provides a framework for constructing a partial evaluator for any imperative programming language, by writing down its semantics as a declarative program (a constraint logic program, in the approach shown here).	imperative programming	Julio C. Peralta;John P. Gallagher	1999		10.1007/10720327_7	program analysis;constraint logic programming;constraint programming;declarative programming;directed graph;computer science;artificial intelligence;theoretical computer science;fifth-generation programming language;programming language;operational semantics;partial evaluation;algorithm;semantics	SE	-18.893528764674425	23.485144249922403	116728
e1b8a17ccd9ebda286ffcfaff467c6194f605c22	silence is golden: branching bisimilarity is decidable for context-free processes	context-free processes;branching bisimilarity	We show that the branching bisimulation equivalence introduced by Rob van Glabbeek is decidable for the class of normed, recursively defined BPA processes with silent actions, thus generalizing the decidability result for strong bisimilarity by Baeten, Bergstra, and Klop.	bisimulation	Hans Hüttel	1991		10.1007/3-540-55179-4_2	computer science;distributed computing;algorithm	Logic	-5.386008881649696	22.571556876928685	116759
3a010312b66f3adb76055df557fa24dce454388f	some remarks on definability of process graphs	file attente;distributed system;graphe infini;systeme reparti;simultaneidad informatica;queue;basic parallel processes;graph connectivity;concurrency;fonction densite;sistema repartido;algebra proceso;infinite graph;density function;funcion densidad;conectividad grafo;algebre processus;grafo infinito;process algebra;connectivite graphe;simultaneite informatique;article in monograph or in proceedings;fila espera;density functional	We propose the notions of “density” and “connectivity” of infinite process graphs and investigate them in the context of the wellknown process algebras BPA and BPP. For a process graph G, the density function in a state s maps a natural number n to the number of states of G with distance less or equal to n from s. The connectivity of a process graph G in a state s is a measure for how many different ways “of going from s to infinity” exist in G. For BPA-graphs we discuss some tentative findings about the notions density and connectivity, and indicate how they can be used to establish some non-definability results, stating that certain process graphs are not BPA-graphs, and stronger, not even BPA-definable. For BPPgraphs, which are associated with processes from the class of Basic Parallel Processes (BPP), we prove that their densities are at most polynomial. And we use this fact for showing that the paradigmatic process Queue is not expressible in BPP.	bpp (complexity);map;oracle bpa suite;polynomial;process calculus	Clemens Grabmayer;Jan Willem Klop;Bas Luttik	2006		10.1007/11817949_2	probability density function;combinatorics;process calculus;discrete mathematics;concurrency;computer science;artificial intelligence;connectivity;mathematics;distributed computing;programming language;queue;algorithm	ML	-5.239948267713783	22.835648659578553	116850
1090fd5fa9589802318adfefb60d581dcc4e70c9	assertion-based debugging of higher-order (c)lp programs	debugging;program verification;higher order;logic programming;assertions;constraint logic programming;error detection;run time verification	Higher-order constructs extend the expressiveness of first-order (Constraint) Logic Programming ((C)LP) both syntactically and semantically. At the same time assertions have been in use for some time in (C)LP systems helping programmers detect errors and validate programs. However, these assertion-based extensions to (C)LP have not been integrated well with higher-order to date. This paper contributes to filling this gap by extending the assertion-based approach to error detection and program verification to the higher-order context within (C)LP. We propose an extension of properties and assertions as used in (C)LP in order to be able to fully describe arguments that are predicates. The extension makes the full power of the assertion language available when describing higher-order arguments. We provide syntax and semantics for (higher-order) properties and assertions, as well as for programs which contain such assertions, including the notions of error and partial correctness. We also discuss several alternatives for performing run-time checking of such programs.	assertion (software development);constraint logic programming;correctness (computer science);debugging;error detection and correction;first-order predicate;formal verification;programmer;run time (program lifecycle phase);runtime error detection	Nataliia Stulova;José F. Morales;Manuel V. Hermenegildo	2014		10.1145/2643135.2643148	constraint logic programming;error detection and correction;higher-order logic;computer science;theoretical computer science;programming language;debugging;logic programming;algorithm	PL	-17.846171238016513	23.13343745415639	116967
7d35a94134b6b2993d9c23a672ad57ebdda79534	test generation for model-based diagnosis	system modeling;model based diagnosis;dual problem;polynomial hierarchy;test generation	This article formalises the dual problem to model-based diagnosis (MBD), i.e., generatingteststo isolate multiple simultaneous faults. Using a standard propositional MBD framework, we first define a test of minimal size that can isolate multiple simultaneous faults of an arbitrary nature. Second, we prove complexity results for multiplefault tests of minimal size in propositional system models, showing such problems have complexity similar to those of MBD problems, i.e., complexity at the second level of the polynomial hierarchy.	approximation algorithm;digital electronics;duality (optimization);embedded system;model-based definition;polynomial hierarchy;stochastic approximation	Gregory M. Provan	2008		10.3233/978-1-58603-891-5-199	duality;systems modeling;algorithm	AI	-8.015521420742958	27.47640658755504	117110
47b071302bfaa12f0132551eb12f13c2810260e9	stochastic programs and hybrid automata for (biological) modeling	concurrent constraint programming;hybrid automata;stochastic programming	We present a technique to associate to stochastic programs written in stochastic Concurrent Constraint Programming a semantics in terms of a lattice of hybrid automata. The aim of this construction is to provide a framework to approximate the stochastic behavior by a mixed discrete/continuous dynamics with a variable degree of discreteness.	approximation algorithm;automata theory;concurrent constraint logic programming;constraint programming;hybrid automaton;semiconductor industry	Luca Bortolussi;Alberto Policriti	2009		10.1007/978-3-642-03073-4_5	stochastic cellular automaton;concurrent constraint logic programming;stochastic programming;mathematical optimization;discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-5.680275559915431	24.805900883304982	117251
be2bb7209fe44ee5d14f10f16caf563d0a6640a4	distinguishing tests for nondeterministic finite state machines	nondeterministic finite state machines;distinguishing tests;finite state machine	In this paper, testing of deterministic implementations of nondeterministic specification FSMs is considered. Given two nondeterministic FSMs, a black box deterministic FSM is known to be a correct implementation of at least one them. We want to derive a test that determines whether this black box is a correct implementation of the first NDFSM. No upper bound on the number of states of the black box is known. The necessary and sufficient conditions for test existence are found. A method for constructing a conditional test of a minimal length is proposed. Upper bounds of multiplicity, length and overall length close to minimal are obtained.	finite-state machine	Sergiy Boroday	1998			extended finite-state machine;nondeterministic finite automaton with ε-moves;combinatorics;discrete mathematics;nondeterministic finite automaton;state diagram;deterministic finite automaton;generalized nondeterministic finite automaton	EDA	-12.497485523673669	28.17292958227813	117318
3d848d7ad9df77d6be6db8c61c6e76af03c771b3	compositional abstraction techniques for probabilistic automata	abstract state;constraint function;aggressive abstraction technique;common combined-transitions;probabilistic automaton;abstraction technique;compositional w;discrete probabilistic;concrete state;abstract pa;compositional abstraction technique	We present aggressive abstraction techniques for probabilistic automata (PA), a state-based model involving discrete probabilistic and nondeterministic branching. Our abstractions yield abstract PA in which transitions are typed “possible” or “required”—as in modal transition systems—and have constraint functions as target. The key idea is to focus on identifying common combined-transitions from concrete states and putting them as required ones in the abstract state. We prove the correctness of our abstraction techniques, study their relationship, and show that they are compositional w.r.t. parallel composition. We also show the preservation of probabilistic and expected reachability properties for PA.	automata theory;congruence of squares;correctness (computer science);modal logic;probabilistic automaton;reachability;refinement (computing)	Falak Sher;Joost-Pieter Katoen	2012		10.1007/978-3-642-33475-7_23	discrete mathematics;computer science;theoretical computer science;algorithm	Logic	-11.205446509379716	22.967516634736604	117499
0f344948db7a3f9e65f4809b6c7ccea4fa77ec07	symbolic constraints for meta-logic programming	logic programs	Logic programming, with its declarative bias as well as unification and the direct representation of linguistic structures, is well qualified for meta-programming, i.e., programs working with representations of other programs as their data. However, constraint techniques seem necessary in order to fully exploit this paradigm. In the DEMOII system, the language of constraint handling rules (CHRs) has been used in order to provide a functionality that appears difficult to obtain without such means. For example, reversibility of a meta-interpreter, which can be obtained by means of constraints, turns it into a powerful program generator; in the same way, negation-as-failure implemented by means of constraints provides an incremental evaluation of integrity constraints. This paper focuses on the design of such constraints and their implementation by means of CHR.	logic programming	Henning Christiansen;Davide Martinenghi	2000	Applied Artificial Intelligence	10.1080/088395100117034	constraint logic programming;constraint programming;constraint satisfaction;computer science;theoretical computer science;machine learning;constraint;algorithm	AI	-17.971352417829326	20.15328249368393	117634
1857a94f25e28fd799a9862a05e090ea4ec37b62	output events for human-system interaction modeling	human computer interaction;delays complexity theory equations mathematical model petri nets firing temperature;zoomed output event output event types human system interaction modeling delayed output events	In this paper, a new approach to define output actions associated with output event types for human-system interaction modeling is presented. In previous works, output events are used to affect a signal in a basic way: increment, decrement or set its value in the next time instant. This work proposes a set of new types of events to define different dependencies and behaviors for output signals. These events affect the associated signal with a specific behavior, which can be defined using specific characteristics including a function, a time window, and/or a final value. The concepts of delayed output events and zoomed output event are already proposed. Delayed events postpone the event contribution in a specific number of steps. Zoomed events change the number of steps between each contribution, allowing expanding and contracting the associated behavior. These concepts can be applied to previous types of events. Finally, the proposals are discussed, presenting some examples of usage for each type of events.	automatic programming;code generation (compiler);increment and decrement operators;linear function	Rogério Campos-Rebelo;Anikó Costa;Luís Gomes	2014	2014 7th International Conference on Human System Interactions (HSI)	10.1109/HSI.2014.6860486	control engineering;real-time computing;computer science;control theory	Visualization	-18.9429837139297	30.900691699645414	117644
e548a1dc46c57beeed5509f152864f47fd7c06ad	model checking for infinite state systems using data abstraction, assumption-commitment style reasoning and theorem proving	linear time temporal logic;theorem proving;model checking;data abstraction;reactive system	A method combining data abstraction, model checking and theorem proving is presented. It provides a semi-automatic, formal framework for proving arbitrary linear time temporal logic properties of infinite state reactive systems. The paper contains a complete case study to prove safety and liveness of an implementation of a scheduler for the readers/writers problem which uses unbounded queues and sets. We argue that the proposed framework could be automated to a very large extent making this approach feasible in an industrial environment.	abstraction (software engineering);automated theorem proving;correctness (computer science);liveness;model checking;scheduling (computing)	Jürgen Dingel;Thomas Filkorn	1995		10.1007/3-540-60045-0_40	model checking;reactive system;computer science;theoretical computer science;automated proof checking;automated theorem proving;programming language;abstraction model checking;algorithm	Logic	-12.129072406135641	26.616569605779514	117750
a52282a8c49502e7fbf578a507c1403f75e812e4	model reduction using the orthogonality between overapproximate slicing and abstract	graph theory;graph theory formal specification formal verification;object oriented model;formal specification;computational modeling object oriented modeling reduced order systems software software engineering aerospace electronics safety;computer model;satisfiability;software engineering;formal verification;model reduction;model checking;data dependence;state space;static slicing model reduction overapproximate slicing abstract orthogonality state space reduction abstract state graph data dependence relations	The orthogonality between static slicing and abstract method has been used to furtherly reduce the state space in model checking. However, static slicing can not always guarantee a slicing model with an desired size. This paper proposes a new approach which compute the over approximate slicing of an abstract state graph other than a counterpart of a static slicing. An overapproximate slicing is obtained by only considering the data dependence relations between predicates, which will always lead to a slice with an ideal size for verification. Though the overapproximate slice only has a weak property resistance power, it is an super set of the abstract state graph, which guarantees if a property ϕ is satisfied on the overapproximate slice, then the original specification is a model of ϕ. And if there appears a spurious counterexample, then it increases the precision of the overapproximate slice by refinement to keep the verification cost as low as possible. We also provide sufficient proof for the correctness of our method. The experimental result shows that our method improves the scalability of model checking remarkably and scales better to a larger system.	approximation algorithm;array slicing;correctness (computer science);data dependency;method (computer programming);model checking;program slicing;refinement (computing);scalability;state space	Hongtao Huang;Shaobin Huang;Zhiyuan Chen;Tao Zhang	2011	2011 4th International Conference on Biomedical Engineering and Informatics (BMEI)	10.1109/BMEI.2011.6098696	model checking;program slicing;discrete mathematics;formal verification;computer science;state space;graph theory;theoretical computer science;formal specification;algorithm;satisfiability	SE	-14.26659430899282	29.538289172568557	117764
34639ef4690df16d4e65ce589cd1b470192e43e5	mechanical analysis of program complexity	fp;program transformation;functional programming;mechanical analysis;recursion induction principle	There has been a great deal of research done which investigates the problem of evaluating the complexity of particular algorithms; little effort however has been applied to the mechanization of this evaluation. This paper presents the ACE (for Automatic Complexity Evaluator) system which is able to analyse reasonably large programs like sorting programs or numerical programs in a fully mechanical way. A complexity function is derived from the initial program. This function is then automatically transformed into its non-recursive equivalent according to MacCarthy's recursion induction principle, using a pre-defined library of recursive definitions (for example id, length, exponential ...). As the execution time is not a decidable property, this transformation will not be possible in all cases. The richer the pre-defined library is, the more likely the system is to succeed. The paper presents the reasons for mechanizing complexity calculus and the problems involved. It describes the operations performed by ACE and its implementation; limitations and further improvements are discussed in conclusion.		Daniel Le Métayer	1985	SIGPLAN Notices	10.1145/17919.806828	fp;computer science;programming language;functional programming;algorithm	PL	-17.78075073237669	22.936019911767687	117792
92beac0b34836d332044ef0e777f316741a78a38	enhancing the fault-tolerance of nonmasking programs	distributed algorithms;triple modular redundant;distributed programming fault tolerant computing distributed algorithms formal specification computational complexity;nonmasking program;formalmethods;formal specification;automatic addition of fault tolerance;fault tolerant;program synthesis nonmasking program formal specification atomicity program distributed algorithm fault tolerance triple modular redundancy byzantine agreement;byzantine agreement;program transformation;distributed programs;triple modular redundancy;program synthesis;satisfiability;software engineering;formal method;atomicity program;fault tolerance safety fault tolerant systems software engineering laboratories computer science automation costs redundancy engineering profession;fault tolerant computing;redundancy;fault tolerant systems;computational complexity;engineering profession;distributed programming;fault tolerance;safety;computer science;distributed algorithm;reading and writing;automation	In this paper, we focus on automated techniques to enhance the fault-tolerance of a nonmasking fault-tolerant program to masking. A masking program continually satisfies its specification even if faults occur. By contrast, a nonmasking program merely guarantees that after faults stop occurring, the program recovers to states from where it continually satisfies its specification. Until the recovery is complete, however, a nonmasking program can violate its (safety) specification. Thus, the problem of enhancing fault-tolerance from nonmasking to masking requires that safety be added and recovery be preserved. We focus on this enhancement problem for high atomicity programs –where each process can read all variables– and for distributed programs –where restrictions are imposed on what processes can read and write. We present a sound and complete algorithm for high atomicity programs and a sound algorithm for distributed programs. We also argue that our algorithms are simpler than previous algorithms, where masking fault-tolerance is added to a fault-intolerant program. Hence, these algorithms can partially reap the benefits of automation when the cost of adding masking fault-tolerance to a fault-intolerant program is high. To illustrate these algorithms, we show how the masking fault-tolerant programs for triple modular redundancy and Byzantine agreement can be obtained by enhancing the fault-tolerance of the corresponding nonmasking versions. We also discuss how the derivation of these programs is simplified when we begin with a nonmasking fault-tolerant	algorithm;atomicity (database systems);byzantine fault tolerance;data recovery;heuristic;polynomial;time complexity;triple modular redundancy	Sandeep S. Kulkarni;Ali Ebnenasir	2003		10.1109/ICDCS.2003.1203494	distributed algorithm;fault tolerance;real-time computing;computer science;theoretical computer science;operating system;database;distributed computing;programming language;computer security	PL	-13.8580864983477	28.963845177659632	117828
4e89a82c22e4162e08defc0b65269ea1ff6e0999	synthesis of switching controllers using approximately bisimilar multiscale abstractions	switched systems;switched system;multiscale abstractions;satisfiability;controller synthesis;optimal control;timing optimization;dc dc converter;state space;self triggered controllers;approximate bisimulation	When available, discrete abstractions provide an appealing approach to controller synthesis. Recently, an approach for computing discrete abstractions of incrementally stable switched systems has been proposed, using the notion of approximate bisimulation. This approach is based on sampling of time and space where the sampling parameters must satisfy some relation in order to achieve a certain precision. Particularly, the smaller the sampling period, the finer the lattice approximating the state-space and the larger the number of states in the abstraction. This renders the use of these abstractions for synthesis of fast switching controllers computationally prohibitive. In this paper, we present a novel class of multiscale discrete abstractions for switched systems that allows us to deal with fast switching while keeping the number of states in the abstraction at a reasonable level. The transitions of our abstractions have various durations: for transitions of longer duration, it is sufficient to consider abstract states on a coarse lattice; for transitions of shorter duration, it becomes necessary to use finer lattices. These finer lattices are effectively used only on a restricted area of the state-space where the fast switching occurs. We show how to use these abstractions for multiscale synthesis of self-triggered switching controllers for reachability specifications under time optimization. We illustrate the merits of our approach by applying it to the boost DC-DC converter.	approximation algorithm;bisimulation;computation;fast user switching;mathematical optimization;optimal control;reachability;rendering (computer graphics);sampling (signal processing);state space;thyristor;time complexity	Javier Cámara;Antoine Girard;Gregor Gößler	2011		10.1145/1967701.1967730	control engineering;real-time computing;control theory;mathematics	Logic	-9.285711901953997	29.12400958785256	117924
c1e3a40078d22b106621fed499f3c08974d2ace7	formal probabilistic analysis of a wsn-based monitoring framework for iot applications		Internet of Things (IoT) has been considered as an intuitive evolution of sensing systems using Wireless Sensor Networks (WSN). In this context, energefficiency is considered as one of the most critical requirement. For that purpose, the randomized node scheduling approach is largely applied. The randomness feature in the node scheduling together with the unpredictable deployment make probabilistic techniques much more appropriate to evaluate the coverage properties of WSNs. Classical probabilistic analysis techniques, such as simulation and model checking, do not guarantee accurate results, and thus are not suitable for analyzing mission-critical WSN applications. Based on the most recently developed probability theory, available in the HOL theorem prover, we develop the formalizations of the key coverage performance attributes: the coverage intensity of a specific point and the expected value of the network coverage intensity. The practical interest of our higher-order-logic developments is finally illustrated through formally analyzing the asymptotic coverage behavior of an hybrid monitoring framework for environmental IoT.	probabilistic analysis of algorithms	Maissa Elleuch;Osman Hasan;Sofiène Tahar;Mohamed Abid	2016		10.1007/978-3-319-53946-1_6	software deployment;randomness;probabilistic analysis of algorithms;model checking;wireless sensor network;theoretical computer science;probabilistic logic;scheduling (computing);hol;computer science	EDA	-10.743968268409812	28.912610379700222	118234
07e52404a0f11790dc6cb6ac9fc7bed3cc34c352	a realizability model for impredicative hoare type theory	separation logic;hoare type theory;abstract data type;monograph or book;dependent type theory;polymorphism	We present a denotational model of impredicative Hoare Type Theory, a very expressive dependent type theory in which one can specify and reason about mutable abstract data types. The model ensures soundness of the extension of Hoare Type Theory with impredicative polymorphism; makes the connections to separation logic clear, and provides a basis for investigation of further sound extensions of the theory, in particular equations between computations and types.	abstract data type;computation;dependent type;hoare logic;immutable object;impredicativity;parametric polymorphism;separation logic;type theory	Rasmus Lerchedahl Petersen;Lars Birkedal;Aleksandar Nanevski;J. Gregory Morrisett	2008		10.1007/978-3-540-78739-6_26	polymorphism;parametric polymorphism;separation logic;computer science;hoare logic;programming language;abstract data type;algorithm	PL	-15.946310767895174	19.9090929175938	118398
fc553ce6ffeb2bf46403c6940f27ad5ff2488e93	model checking mobile ad hoc networks	mobile ad hoc networks;model checking;multi-hop network constraints;constrained labeled transition systems	Modeling arbitrary connectivity changes within mobile ad hoc networks (MANETs) makes application of automated formal verification challenging. We use constrained labeled transition systems as a semantic model to represent mobility. To model check MANET protocols with respect to the underlying topology and connectivity changes, we introduce a branching-time temporal logic. The path quantifiers are parameterized by multi-hop constraints over topologies, to discriminate the paths over which the temporal behavior should be investigated; the paths that violate the multi-hop constraints are not considered. A model checking algorithm is presented to verify MANETs that allow arbitrary mobility, under the assumption of reliable communication. It is applied to analyze a leader election protocol.	algorithm;formal verification;hoc (programming language);leader election;mcrl2;maude system;model checking;temporal logic;transition system;verification and validation	Fatemeh Ghassemi;Wan Fokkink	2016	Formal Methods in System Design	10.1007/s10703-016-0254-7	optimized link state routing protocol;computer science;theoretical computer science;distributed computing;mobility model	SE	-11.687634084843971	30.111279893718105	118438
d2ce375cdee9339b171095ab31b1e3750fe845fb	bisimulation-invariant ptime and higher-dimensional µ-calculus	modele kripke;algorithm complexity;temps polynomial;complexite calcul;logique mathematique;complejidad algoritmo;calcul propositionnel;logica matematica;theorie modeles;satisfiability;mathematical logic;kripke model;complejidad computacion;modal logic;complexite algorithme;model checking;algebra proceso;computational complexity;logique modale;propositional calculus;transition systems;polynomial time;algebre processus;logica modal;logic in computer science;calculo proposicional;descriptive complexity;process logics;modelo kripke;process algebra;teoria modelos;finite model theory;model theory;tiempo polinomial	Consider the class of all those properties of worlds in finite Kripke structures (or of states in finite transition systems), that are l recognizable in polynomial time, and l closed under bisimulation equivalence. It is shown that the class of these bisimulation-invariant PTIME queries has a natural logical characterization. It is captured by the straightforward extension of propositional p-calculus to arbitrary finite dimension. Bisimulation-invariant PTIME, or the modal fragment of PTIME, thus proves to be one of the very rare cases in which a logical characterization is known in a setting of unordered structures. It is also shown that higher-dimensional p-calculus is undecidable for satisfiability in finite structures, and even Et-hard over general structures. @ 1999 Elsevier Science B.V. All rights reserved.	automatic vectorization;bisimulation;boolean satisfiability problem;computer science;descriptive complexity theory;kripke structure (model checking);l (complexity);modal logic;monadic predicate calculus;operational semantics;p (complexity);polynomial;recursion;strong duality;time complexity;turing completeness;undecidable problem;whole earth 'lectronic link;yet another;π-calculus	Martin Otto	1999	Theor. Comput. Sci.	10.1016/S0304-3975(98)00314-4	modal logic;time complexity;model checking;mathematical logic;process calculus;discrete mathematics;finite model theory;computer science;mathematics;propositional calculus;computational complexity theory;algorithm;descriptive complexity theory;model theory;satisfiability	Logic	-7.119590785059126	20.24325548474026	118515
35436404ac84ec5d5a642b8bef1740659063899d	polynomial interrupt timed automata		Interrupt Timed Automata (ITA) form a subclass of stopwatch automata where reachability and some variants of timed model checking are decidable even in presence of parameters. They are well suited to model and analyze real-time operating systems. Here we extend ITA with polynomial guards and updates, leading to the class of polynomial ITA (PolITA). We prove the decidability of the reachability and model checking of a timed version of CTL by an adaptation of the cylindrical decomposition method for the first-order theory of reals. Compared to previous approaches, our procedure handles parameters and clocks in a unified way. Moreover, we show that PolITA are incomparable with stopwatch automata. Finally additional features are introduced while preserving decidability.	audio feedback;automata theory;connected component (graph theory);din connector;dave grossman (game developer);decision problem;expressive power (computer science);first-order logic;first-order reduction;formal methods;hybrid automaton;hybrid system;issac;information and computation;interrupt;journal of symbolic computation;lambda lifting;lecture notes in computer science;linear algebra;model checking;p (complexity);polynomial;quantifier (logic);reachability;real-time cmix;real-time clock;real-time computing;real-time operating system;soap with attachments;springer (tank);temporal logic;timed automaton	Béatrice Bérard;Serge Haddad;Claudine Picaronny;Mohab Safey El Din;Mathieu Sassolas	2015		10.1007/978-3-319-24537-9_3	discrete mathematics;real-time computing;mathematics;timed automaton;algorithm	Logic	-10.882587705626591	24.139001096219133	118534
a8793b6bab5160254a0a9f620efee62f29069f5c	one way of estimating frequencies of jumps in a program	program graph;jump frequencies estimation;jump frequencies;frequency estimation;optimal program segmentation;markov chain program correspondence;object program reduction;supervisor overheading decreasing;control transfers estimation;locally estimated jump frequencies;one entry subgraph;supervisor calls decreasing;program segmentation algorithm;markov chain	"""For the segmentation of a program it is useful to have a reasonable estimation of the values of SiC, where S<~ is the mean value of the number of jumps from the ith instruction on to the i'.h instruction in the run time. In the cases where the S~j are estimated directly, the structure of the whole program must be generally taken into account; therefore it is very difficult for the programmer and/or the translator to obtain a good estimation of 'lhe Sij'. It is easier to estimate not S~j but the quantities ~i;' = S¢j'c.i/~5~-I S~., where ci is an arbitrary positive constant for each i. Although the ~i¢ are, for each i, proportional to Sis, the estimation of ~i~ is easier, because we must estimate only lhe """"probabilities"""" of events where instruction I~ is executed after instruction I i . This estimation can often be done without considering the structure of the whole program. In the first part of the paper, using the theory of the Markov chains, an algorithm for the computation of the S~i from the ~ is found, and some ways of obtaining estimates of the ~s are given. In the second part a variant of this algorithm is derived, avoiding the necessity of computation involving large matrices."""	algorithm;computation;markov chain;programmer;run time (program lifecycle phase)	Jaroslav Král	1968	Commun. ACM	10.1145/363397.363400	markov chain;mathematical optimization;real-time computing;simulation;mathematics;statistics	EDA	-15.580474459010537	31.838410485321923	118556
65aa940fb8d8d058c65b5a4cbff1fe6b37c8aac3	observing partial order runs of petri nets	petri net;partial order	A reformulation of the standard causal semantics for Petri nets is proposed which allows one to reconstruct a partial order  run from sequential observations without any knowledge of the underlying net structure or its current marking. It provides  a new solution to how Mazurkiewicz's trace theory can be generalized to unbounded nets.  	petri net	Astrid Kiehn	1997		10.1007/BFb0052091	partially ordered set;stochastic petri net;computer science;mathematics;petri net	Logic	-9.696308451589028	23.021781511405717	118613
69723c5a972771229dc32b5839780e297f68f207	the cogent case for property-based testing		Property-based testing can play an important role in reducing the cost of formal verification: It has been demonstrated to be effective at detecting bugs and finding inconsistencies in specifications, and thus can eliminate effort wasted on fruitless proof attempts. We argue that in addition, property-based testing enables an incremental approach to a fully verified system, by allowing replacement of automatically generated tests of properties stated in the specification by formal proofs. We demonstrate this approach on the verification of systems code, discuss the implications on systems design, and outline the integration of property-based testing into the Cogent framework.	formal verification;quickcheck;sensor;software bug;systems design	Zilin Chen;Liam O'Connor;Gabriele Keller;Gerwin Klein;Gernot Heiser	2017		10.1145/3144555.3144556	computer science;formal methods;theoretical computer science;mathematical proof;systems design;formal verification	SE	-15.536395802352148	27.547041868128762	118778
404913bfac286514e1a1ee6bb9b8a37378005935	linear complexity assertions for sorting	partition function;verificateur chien garde;program diagnostics;permutation assertion;control flow error;execution time;partition functions;watchdog checker;sorting;programs correctness checking;programming theory sorting program debugging program diagnostics program verification;ingenieria logiciel;tria;program verification;indexing terms;order assertion;sorting programs;linear complexity;software engineering;assertion;program execution;order sum assertion;sorting computer errors error correction hardware costs change detection algorithms computer aided instruction testing counting circuits registers;ascending order;programming theory;particion;output data;input data;triage;flot commande;control flow;partition;sorted data;genie logiciel;error checking;linear complexity assertions;program debugging;error detection;flujo control;descending order;erreur flot commande;partition theory;erreur donnee;watchdog checker linear complexity assertions sorting programs programs correctness checking program execution order assertion permutation assertion sorted data descending order ascending order output data input data error detection execution time order sum assertion partition theory partition functions error checking;data error	AbstructCorrectness of the execution of sorting programs can be checked by two assertions: the order assertion and the permutation assertion. The order assertion checks if the sorted data is in ascending or descending order. The permutation assertion checks if the output data produced by sorting is a permutation of the original input data. Permutation and order assertions are sufficient for the detection of errors in the execution of sorting programs; however, in terms of execution time these assertions cost the same as sorting programs. An assertion, called the order-sum assertion, that has lower execution cost than sorting programs is derived from permutation and order assertions. The reduction in cost is achieved at the expense of incomplete checking. Some metrics are derived to quantify the effectiveness of order-sum assertion under various error models. A natural connection between the effectiveness of the ordersum assertion and the partition theory of numbers is shown. Asymptotic formulae for partition functions are derived.	assertion (software development);run time (program lifecycle phase);sorting	Nirmal R. Saxena;Edward J. McCluskey	1994	IEEE Trans. Software Eng.	10.1109/32.295891	partition;computer science;sorting;theoretical computer science;operating system;invariant;programming language;algorithm	Embedded	-17.2114745833708	31.243900870383783	118897
8348a27ad99833e81aba0392df8715bad8e9db50	compiler analysis of the value ranges for variables	weak interpretation;optimizing compiler;range analysis;constant propagation optimizing compiler program analysis proof of correctness weak interpretation;data representation;testing algorithm design and analysis information analysis performance analysis flow graphs optimizing compilers tracking loops tires data flow computing;proof of correctness;program analysis;constant propagation;program correctness	Programs can be analyzed to determine bounds on the ranges of values assumed by variables at various points in the program. This range information can then be used to eliminate redundant tests, verify correct operation, choose data representations, select code to be generated, and provide diagnostic information. Sophisticated analyses involving the proofs of complex assertions are sometimes required to derive accurate range information for the purpose of proving programs correct. The performance of such algorithms may be unacceptable for the routine analysis required during the compilation process. This paper presents a discussion of mechanical range analysis employing techniques practical for use in a compiler. This analysis can also serve as a useful adjunct to the more sophisticated techniques required for program proving.	algorithm;formal verification;optimizing compiler	William H. Harrison	1977	IEEE Transactions on Software Engineering	10.1109/TSE.1977.231133	program analysis;compiler correctness;computer science;theoretical computer science;optimizing compiler;external data representation;live variable analysis;programming language;constant folding;algorithm	SE	-17.431950323288433	30.961344967358308	118928
b42dcfbc3ca01117246bce49d4ebd057284db96c	reasoning about hybrid systems based on a nonstandard model	nonstandard analysis;linear temporal logic;hybrid system;hybrid automata	In this paper, we propose to introduce a nonstandard analysis into a logical modeling of continuous dynamics and present a new framework called hyper-finite hybrid automaton (HHA). HHA is a non-standard interpretation of hybrid automata in the domain of * R. We also enlarge the linear temporal logic LTL to * LTL to describe the system specification. By using this framework, we examine the validation of the system consistency of the hybrid system, especially the existence and reachability of Zeno point.	hybrid system	Katsunori Nakamura;Akira Fusaoka	2007		10.1007/978-3-540-76928-6_87	discrete mathematics;linear temporal logic;computer science;mathematics;non-standard analysis;algorithm;hybrid system	AI	-11.778748224261836	23.11436272287563	119063
288c7f96a6e89bfd20f7a3829ec9b9b9635c0794	higher order generalization and its application in program verification	program verification;higher order;first order;inductive inference	Generalization is a fundamental operation of inductive inference. While first order syntactic generalization (anti–unification) is well understood, its various extensions are often needed in applications. This paper discusses syntactic higher order generalization in a higher order language λ2 [1]. Based on the application ordering, we prove that least general generalization exists for any two terms and is unique up to renaming. An algorithm to compute the least general generalization is also presented. To illustrate its usefulness, we propose a program verification system based on higher order generalization that can reuse the proofs of similar programs.	algorithm;formal verification;inductive reasoning;unification (computer science)	Jianguo Lu;John Mylopoulos;Masateru Harao;Masami Hagiya	2000	Annals of Mathematics and Artificial Intelligence	10.1023/A:1018952121991	generalization;discrete mathematics;higher-order logic;computer science;inductive reasoning;first-order logic;mathematics;programming language;algorithm;universal generalization	AI	-13.609670578456049	18.302324169890817	119465
119b863678a9eb27b4fb083f7a10d6d339ea53ed	from outermost termination to innermost termination	programming language;program verification;term rewrite system;termination analysis;functional programming language	Rewriting is the underlying evaluation mechanism of functional programming languages. Therefore, termination analysis of term rewrite systems (TRSs) is an important technique for program verification. To capture the evaluation mechanism of a programming language one has to take care of the evaluation strategy, where we focus on the outermost strategy. As there are only few techniques available to analyze outermost termination of TRSs directly, we introduce a new transformation such that a TRS is outermost terminating iff the transformed TRS is innermost terminating. In this way all of the several techniques that have been developed to investigate innermost termination become applicable to analyze outermost termination, too. We have implemented the transformation and successfully evaluated it on a large collection of TRSs.	apl;care-of address;formal verification;functional programming;newman's lemma;programming language;rewrite (programming);rewriting;termination analysis	René Thiemann	2009		10.1007/978-3-540-95891-8_48	real-time computing;computer science;termination analysis;programming language;functional programming;algorithm	PL	-16.93451770260284	22.582935757261623	119513
05910acd07715b7ad338d70a21ff1c63096374e6	source-to-source optimizing transformations of prolog programs based on abstract interpretation	operational semantics;program transformation;indexation;source code;logic programs;static analysis;abstract interpretation	Making a Prolog program more efficient by transforming its source code, without changing its operational semantics, is not an obvious task. It requires the user to have a clear understanding of how the Prolog compiler works, and in particular, of the effects of ‘impure’ features like the cut. The way a Prolog code is written e.g., the order of clauses, the order of literals in a clause, the use of cuts or negations influences its efficiency. Furthermore, different optimisation techniques may be redundant or conflicting when they are applied together, depending on the way a procedure is called e.g., inserting cuts and enabling indexing. We present an optimiser, based on abstract interpretation, that automatically performs safe code transformations of Prolog procedures in the context of some class of input calls. The method is more effective if procedures are annotated with additional information about modes, types, sharing, number of solutions and the like. Thus the approach is similar to Mercury. It applies to any Prolog program, however.	abstract interpretation;code;compiler;heuristic (computer science);literal (mathematical logic);mathematical optimization;mercury;operational semantics;prolog;run time (program lifecycle phase)	Francçois Gobert;Baudouin Le Charlier	2007	CoRR		computer science;theoretical computer science;programming language;operational semantics;static analysis;algorithm;source code	PL	-19.037066428072748	23.17057884248513	119525
89f7cc3d18f0e70268df58b14b1c0ea63cc47b89	a road map of interval temporal logics and duration calculi	expressiveness;interval temporal logic;duration calculus;axiomatic system;decidability	We survey main developments, results, and open problems on interval temporal logics and duration calculi. We present various formal systems studied in the literature and discuss their distinctive features, emphasizing on expressiveness, axiomatic systems, and (un)decidability results.	axiomatic system;formal system;interval temporal logic	Valentin Goranko;Angelo Montanari;Guido Sciavicco	2004	Journal of Applied Non-Classical Logics	10.3166/jancl.14.9-54	axiomatic system;decidability;duration calculus;discrete mathematics;interval temporal logic;computer science;expressivity;mathematics;programming language;algorithm	Logic	-11.705867721505513	22.041250313678443	119643
63213f6c1c172effe641e2f2a8cc1eb98529402a	bidirectional nested weighted automata		Nested weighted automata (NWA) present a robust and convenient automata-theoretic formalism for quantitative specifications. Previous works have considered NWA that processed input words only in the forward direction. It is natural to allow the automata to process input words backwards as well, for example, to measure the maximal or average time between a response and the preceding request. We therefore introduce and study bidirectional NWA that can process input words in both directions. First, we show that bidirectional NWA can express interesting quantitative properties that are not expressible by forward-only NWA. Second, for the fundamental decision problems of emptiness and universality, we establish decidability and complexity results for the new framework which match the best-known results for the special case of forward-only NWA. Thus, for NWA, the increased expressiveness of bidirectionality is achieved at no additional computational complexity. This is in stark contrast to the unweighted case, where bidirectional finite automata are no more expressive but exponentially more succinct than their forward-only counterparts.	automata theory;automaton;bi-directional text;computational complexity theory;decision problem;finite-state machine;finite-state transducer;maximal set;semantics (computer science);universality probability	Krishnendu Chatterjee;Thomas A. Henzinger;Jan Otop	2017		10.4230/LIPIcs.CONCUR.2017.5	finite-state machine;theoretical computer science;computer science;automaton;computational complexity theory;decision problem;universality (philosophy);decidability;formalism (philosophy);special case	Logic	-8.187194201015545	20.861186748461307	119679
e31ccc52df8334974d7664f83e364a288e35674b	testing finite state machines presenting stochastic time and timeouts	investigacion operativa;finite state machine	In this paper we define a formal framework to test implementations that can be represented by the class of finite state machines introduced in [10]. First, we introduce an appropriate notion of test. Next, we provide an algorithm to derive test suites from specifications such that the constructed test suites are sound and complete with respect to two of the conformance relations introduced in [10]. In fact, the current paper together with [10] constitute a complete formal theory to specify and test the class of systems covered by the before mentioned stochastic finite state machines.	algorithm;conformance testing;finite-state machine;test suite	Mercedes G. Merayo;Manuel Núñez;Ismael Rodríguez	2007		10.1007/978-3-540-75211-0_8	real-time computing;computer science;finite-state machine;programming language;algorithm	SE	-10.969472611210286	26.54898601153905	119710
c51a4244c55f3211d5bd101a1dc672381dd7f429	extending fairness expressibility of ectl+: a tree-style one-pass tableau approach		Temporal logic has become essential for various areas in computer science, most notably for the specification and verification of hardware and software systems. For the specification purposes rich temporal languages are required that, in particular, can express fairness constraints. For linear-time logics which deal with fairness in the linear-time setting, one-pass and two-pass tableau methods have been developed. In the repository of the CTL-type branching-time setting, the well-known logics ECTL and ECTL+ were developed to explicitly deal with fairness. However, due to the syntactical restrictions, these logics can only express restricted versions of fairness. The logic CTL, often considered as “the full branching-time logic” overcomes these restrictions on expressing fairness. However, this logic itself, is extremely challenging for the application of verification techniques, and the tableau technique, in particular. For example, there is no onepass tableau construction for this logic, while it is known that one-pass tableau has an additional benefit enabling the formulation of dual sequent calculi that are often treated as more “natural” being more friendly for human understanding. Based on these two considerations, the following problem arises are there logics that have richer expressiveness than ECTL+ yet “simpler” than CTL for which a one-pass tableau can be developed? In this paper we give a solution to this problem. We present a tree-style one-pass tableau for a sub-logic of CTL that we call ECTL#, which is more expressive than ECTL+ allowing the formulation of a new range of fairness constraints with “until” operator. The presentation of the tableau construction is accompanied by an algorithm for constructing a systematic tableau, for any given input of admissible branching-time formulae. We prove the termination, soundness and completeness of the method. As tree-shaped one-pass tableaux are well suited for the automation and are amenable for the implementation and for the formulation of sequent calculi, our results also open a prospect of relevant developments of the automation and implementation of the tableau method for ECTL#, and of a dual sequent calculi. 2012 ACM Subject Classification Theory of computation → Modal and temporal logics 1 The author would like to thank the University of Westminster for supporting the sabbatical in 2017. 2 This author has been partially supported by Spanish Projects TIN2013-46181-C2-2-R and TIN201786727-C2-2-R, and by the University of the Basque Country under Project LoRea GIU15/30. 3 This author has been partially supported by Spanish Projects TIN2013-46181-C2-2-R and TIN201786727-C2-2-R, and by the University of the Basque Country under Project LoRea GIU15/30. © Alexander Bolotov, Montserrat Hermo, and Paqui Lucio; licensed under Creative Commons License CC-BY 25th International Symposium on Temporal Representation and Reasoning (TIME 2018). Editors: Natasha Alechina, Kjetil Nørvåg, and Wojciech Penczek; Article No. 5; pp. 5:1–5:22 Leibniz International Proceedings in Informatics Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany 5:2 Extending Fairness Expressibility of ECTL+	algorithm;computer science;fairness measure;formal verification;informatics;long division;method of analytic tableaux;runge–kutta methods;sequent calculus;software system;soundness (interactive proof);temporal logic;theory of computation;time complexity;whole earth 'lectronic link	Alexander Bolotov;Montserrat Hermo;Paqui Lucio	2018		10.4230/LIPIcs.TIME.2018.5		Logic	-13.008601944022605	22.36522174515325	119714
05ef40bb31c24157cc34730dfc5b9024c069f1fe	on the use of a high-level fault model to check properties incompleteness	description validation;coverage methodology;fault simulation;behavioralchecking completeness;model checking;coverage metrics;paper proposesa coverage methodology;formal property;automatic test pattern generation;system specification;properties incompleteness;dynamic verification;automatictest pattern generation;behavioral checking;systems analysis;high-level fault simulation;incompleteness estimation;digital system;high-level fault model;high-level fault modelallows;combination ofmodel checking;formal specification;formal verification;property completeness checking;mathematical model;algorithm design and analysis;logic design;fault model	The use of model checking to validate descriptions ofdigital systems lacks a coverage metrics. The set of provenproperties can be incomplete, thus not guaranteeing the behavioralchecking completeness of the digital system implementationwith respect to the specification. This paper proposesa coverage methodology based on a combination ofmodel checking, high-level fault simulation and automatictest pattern generation, to estimate the incompleteness of aset of formal properties. The adopted high-level fault modelallows to join dynamic and formal verification.	byzantine fault tolerance;digital electronics;fault model;formal verification;high- and low-level;model checking;simulation	Franco Fummi;Graziano Pravadelli;Andrea Fedeli;Umberto Rossi;Franco Toto	2003	First ACM and IEEE International Conference on Formal Methods and Models for Co-Design, 2003. MEMOCODE '03. Proceedings.		model checking;systems analysis;real-time computing;formal methods;fault coverage;formal verification;computer science;theoretical computer science;formal specification;formal equivalence checking;fault model;runtime verification;programming language;abstraction model checking	EDA	-15.42805859044163	29.084290368815495	119895
7bb41d77850e37cfedd824d060668e65f64353db	resource constrained failure management in networked computing systems	distributed computing resource constrained failure management networked computing system fault detection markov chain model fitting approach dynamic programming optimal admissible policy usage limitation constraint distributed storage;failure management fault detection dynamic programming budgeted estimation;resource allocation distributed processing dynamic programming fault tolerant computing markov processes	We examine the problem of fault detection in networked computing systems and highlight the tradeoff between diagnosing/reacting to potentially harmful real-time events and minimizing the number of times the system is reset or scanned for malicious activity. The various health states of a system are modeled as states in a Markov chain, and we use a model fitting approach to estimate the transitions between these states. We proceed by considering a scenario in which a system is to be deployed over a fixed horizon but with a limit on the number of times that the health state can be scanned and the system can be reset. Each health state is assigned a cost according to the performance of the system while in that state. Dynamic Programming is then used to find an optimal admissible policy (one that obeys the usage limitation constraints) which achieves the lowest expected aggregate cost. Finally, we examine some properties of the solution.	aggregate data;approximation algorithm;backward induction;computer simulation;curve fitting;dynamic programming;fault detection and isolation;image scanner;malware;markov chain;real-time clock;scheduling (computing);state space	Praveen Bommannavar;Nicholas Bambos	2012	2012 IEEE Global Communications Conference (GLOBECOM)	10.1109/GLOCOM.2012.6503390	mathematical optimization;real-time computing;computer science;distributed computing	Metrics	-5.020342553246442	31.01712103736419	119988
34b59c77fb3b5ec1889242643b4ea52edfdb3592	asynchronous automata-theoretic characterization of aperiodic trace languages	asynchronous automaton;automata aceptor;automate asynchrone;langage ordre 1;gossiping all to all;automata asincrono;first order language;acceptor automaton;first order;todos a todos;informatique theorique;automate accepteur;lenguaje orden 1;echange total;computer theory;informatica teorica	We characterize aperiodic distributed behaviours, modelled as Mazurkiewicz traces in terms of a very natural cascade product of the gossip automaton with a counter-free asynchronous automaton. The characterization strengthens the fundamental results of Schutzenberger and, McNaughton and Papert and implies that star-free, equivalently, first-order-definable trace languages admit counter-free asynchronous acceptors modulo the gossip automaton.	automaton	Bharat Adsul;Milind A. Sohoni	2004		10.1007/978-3-540-30538-5_8	büchi automaton;computer science;artificial intelligence;theoretical computer science;two-way deterministic finite automaton;probabilistic automaton;deterministic automaton;first-order logic;programming language;timed automaton;pushdown automaton;algorithm	Theory	-5.456446852108004	22.130240995324094	120130
d908e913048c892ba2d37e1e040976ab86208b61	goto elimination in program algebra	cobol;universiteitsbibliotheek;goto elimination;restructuring;program algebra;correctness proof;pga	This paper shows that program algebra (PGA) [J.A. Bergstra, M.E. Loots, Program algebra for sequential code, J. Logic Algebraic Programm. 51 (2002) 125-156] offers a mathematical and systematic framework for reasoning about correctness and equivalence of algorithms and transformation rules for goto removal. We study correctness and equivalence for the algorithm proposed by Cooper for goto elimination with additional boolean variables. To remove goto statements without the use of additional variables, we propose a technique to get rid of head-to-head crossings and subsequently employ the results of Peterson et al. and Ramshaw. Finally, we provide formal correctness proofs in the setting of PGA for industrial transformation rules given recently by Veerman for restructuring Cobol programs in real-life applications. We hereby show that PGA can explain goto elimination with mathematical rigor to a larger public.	goto	Thuy Duong Vu	2008	Sci. Comput. Program.	10.1016/j.scico.2008.07.002	computer science;theoretical computer science;restructuring;cobol;programming language;algorithm	Logic	-18.610515623613658	20.143018535951494	120164
1778f47f122b15015b67e62a24e7a5779737b2bd	d-flat2: subset minimization in dynamic programming on tree decompositions made easy		Many problems from the area of AI have been shown tractable for bounded treewidth. In order to put such results into practice, quite involved dynamic programming (DP) algorithms on tree decompositions have to be designed and implemented. These algorithms typically show recurring patterns that call for tasks like subset-minimization. In this paper we present D-FLATˆ2, a system that allows one to obtain DP algorithms (specified in ASP) from simpler principles, where the DP formalization of subset-minimization is performed automatically. We illustrate the method at work by providing several DP algorithms – given in form of ASP programs – that are more space-efficient than existing solutions, while featuring improved readability, reuse and therefore maintainability of ASP code. Experiments show that our approach also yields a significant improvement in runtime performance.	algorithm;cobham's thesis;dynamic programming;experiment;mathematical optimization;polynomial hierarchy;run time (program lifecycle phase);treewidth	Bernhard Bliem;Günther Charwat;Markus Hecher;Stefan Woltran	2016	Fundam. Inform.	10.3233/FI-2016-1397	mathematical optimization;combinatorics;discrete mathematics	AI	-15.51935578828497	24.287645505967077	120279
dfbf460c669994650f6aabc6bbd5b14aa77c04d6	coalgebraic specifications and models of deterministic hybrid systems	hybrid system	"""Coalgebraic speciication and semantics, as used earlier for object-oriented programming, is extended with temporal aspects. The (non-temporal) expression s:meth expressing that method meth is applied in state s is extended to an expression s:meth@, where is a time parameter. It means: in state s let the state evolve for units of time, and then apply method meth. With this formalism we specify various (elementary) deterministic hybrid systems (and give a few simulations). We also deene a notion of model for such a speciication, and deene what it means for a model to be terminal. Terminal models are \optimal"""" in the sense that they involve a minimal set of states, as will be illustrated in a number of examples. This shows that standard model theory can be applied to temporal (coalgebraic) speciications."""	elementary;hybrid system;semantics (computer science);simulation	Bart Jacobs	1996		10.1007/BFb0014337	computer science;hybrid system	Logic	-10.421768179435881	22.9082267323891	120301
d01abcccb59fd17f11fc54d19ccf363835adbe01	toward synthesis from english descriptions	hardware design languages;design automation;design engineering;prototypes;vocabulary;natural languages;data engineering;design representation;counting circuits;permission;control system synthesis;digital systems;model integration;counting circuits control system synthesis design automation hardware design languages natural languages prototypes permission vocabulary data engineering design engineering;semantic analysis	This paper reports on a research project to design a system for automatically interpreting English specifications of digital systems in terms of design representation formalisms currently employed in CAD systems. The necessary processes involve the machine analysis of English and the synthesis of models from the specifications. The approach being investigated is interactive and consists of syntactic scanning, semantic analysis, interpretation generation, and model integration.	computer-aided design;digital electronics;semantic analysis (compilers);syntactic predicate	Walling R. Cyre	1989	26th ACM/IEEE Design Automation Conference	10.1145/74382.74519	natural language processing;information engineering;electronic design automation;idef4;computer science;computer-automated design;theoretical computer science;prototype;natural language;programming language;algorithm	EDA	-14.299393214652675	30.701434167822825	120341
d6d85e472999e32244473d44125e110be8c7d421	model checking freeze ltl over one-counter automata	low complexity;satisfiability;model checking	We study complexity issues related to the model-checking problem for LTL with registers (a.k.a. freeze LTL) over one-counter automa ta. We consider several classes of one-counter automata (mainly deterministic vs. nondeterministic) and several syntactic fragments (restriction on the number of re gisters and on the use of propositional variables for control locations). The log ic has the ability to store a counter value and to test it later against the current counter valu . By introducing a non-trivial abstraction on counter values, we show that model checking LTL with registers over deterministic one-counter automata is PS PACEcomplete with infinite accepting runs. By constrast, we prove that model ch ecking LTL with registers over nondeterministic one-counter automata is Σ 1 -complete [resp.Σ 1 -complete] in the infinitary [resp. finitary] case even if only one register is used and with no propositional variable. This makes a difference with the facts that several verification problems for one-counter automata are know n to be decidable with relatively low complexity, and that finitary satisfiability for LTL with a unique register is decidable. Our results pave the way for model-check ing LTL with registers over other classes of operational models, such as rever al-bounded counter machines and deterministic pushdown systems.	automata theory;automaton;counter machine;model checking;nondeterministic algorithm;ps-algol;pspace;philippe kruchten;processor register;program counter;propositional variable;stack (abstract data type);undecidable problem	Stéphane Demri;Ranko Lazic;Arnaud Sangnier	2008		10.1007/978-3-540-78499-9_34	model checking;combinatorics;discrete mathematics;computer science;mathematics;algorithm;satisfiability	Logic	-11.676052970162717	23.90733838168177	120543
37435c6209f7f75265f0179110315f34d4454270	bounded communication reachability analysis of process rewrite systems with ordered parallelism	verification;parallel composition;synchronisation;rewrite systems;tree automata;program analysis;multithreaded programs with procedure calls;process algebra;reachability analysis;parallel processing	We define a new model called O-PRS that extends the Process Rewrite Systems formalism with a new associative operator, “ ”, that allows to model parallel composition while keeping the order between parallel processes. Indeed, sometimes, it is important to remember the order between the parallel processes. The reachability problem of O-PRS being undecidable, we develop tree automata techniques allowing to build polynomial finite representations of (1) the exact reachable configurations in O-PRS modulo various equivalences that omit the associativity of “ ”, and (2) underapproximations of the reachable configurations if the associativity of “ ” is considered. We show that these underapproximations are exact if the number of communications between ordered parallel processes is bounded. We implemented our algorithms in a tool that was used for the analysis of a concurrent lexer server.	algorithm;automata theory;lexical analysis;modulo operation;polynomial;procedural reasoning system;reachability problem;semantics (computer science);server (computing);tree automaton;undecidable problem	Mihaela Sighireanu;Tayssir Touili	2009	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2009.05.029	program analysis;parallel processing;synchronization;process calculus;discrete mathematics;verification;computer science;theoretical computer science;mathematics;distributed computing;programming language	Logic	-11.769331059884376	25.1122449102769	120553
076d78c60ff17f6547b4e2523f4cf6f1a9f12e20	weighted relational models for mobility	004;concurrency mobility denotational semantics	We investigate operational and denotational semantics for computational and concurrent systems with mobile names which capture their computational properties. For example, various properties of fixed networks, such as shortest or longest path, transition probabilities, and secure data flows, correspond to the “sum” in a semiring of the weights of paths through the network: we aim to model networks with a dynamic topology in a similar way. Alongside rich computational formalisms such as the λ-calculus, these can be represented as terms in a calculus of solos with weights from a complete semiring R, so that reduction associates a weight in R to each reduction path. Taking inspiration from differential nets, we develop a denotational semantics for this calculus in the category of sets and R-weighted relations, based on its differential and compact-closed structure, but giving a simple, syntax-independent representation of terms as matrices over R. We show that this corresponds to the sum in R of the values associated to its independent reduction paths, and that our semantics is fully abstract with respect to the observational equivalence induced by sum-of-paths evaluation. 1998 ACM Subject Classification F.3.2 Semantics of Programming Languages, F.4.1 Mathematical Logic	algorithm;concurrency (computer science);denotational semantics;directed acyclic graph;evaluation function;idempotence;input/output;lambda calculus;longest path problem;markov chain;observational equivalence;semantics (computer science);turing completeness	James Laird	2016		10.4230/LIPIcs.FSCD.2016.24	combinatorics;discrete mathematics;normalisation by evaluation;domain theory;mathematics;denotational semantics of the actor model;operational semantics;denotational semantics;algorithm	PL	-9.978322065183427	20.210166560670885	120588
abadb9f5f5d71eaf96d52b0d1c3e44511bc9791b	a hybrid approach to conjunctive partial evaluation of logic programs	offline;hybrid approach;logic programming;partial evaluation;on the fly;partial deduction;logic programs;capitulo de libro	Conjunctive partial deduction is a well-known technique for the partial evaluation of logic programs. The original formulation follows the so called online approach where all termination decisions are taken on-the-fly. In contrast, offline partial evaluators first analyze the source program and produce an annotated version so that the partial evaluation phase should only follow these annotations to ensure the termination of the process. In this work, we introduce a lightweight approach to conjunctive partial deduction that combines some of the advantages of both online and offline styles of partial evaluation.	logic programming;natural deduction;online and offline;partial evaluation	Germán Vidal	2010		10.1007/978-3-642-20551-4_13	computer science;theoretical computer science;programming language;logic programming;partial evaluation;algorithm	PL	-18.98840087679586	22.86668168231474	120672
3d6d2bc2427f1865444fb9007dc3395bcf95c06e	the quantitative linear-time--branching-time spectrum	quantitative verification system distance distance hierarchy linear time branching time;004;branching time;linear time;system distance;quantitative verification;distance hierarchy	We present a distance-agnostic approach to quantitative verification. Taking as input an unspecified distance on system traces, or executions, we develop a game-based framework which allows us to define a spectrum of different interesting system distances corresponding to the given trace distance. Thus we extend the classic linear-time–branching-time spectrum to a quantitative setting, parametrized by trace distance. We also prove a general transfer principle which allows us to transfer counterexamples from the qualitative to the quantitative setting, showing that all system distances are mutually topologically inequivalent.	time complexity;trace distance;tracing (software);type system	Ulrich Fahrenberg;Axel Legay;Claus R. Thrane	2011		10.4230/LIPIcs.FSTTCS.2011.103	time complexity;combinatorics;discrete mathematics;computer science;mathematics;geometry;distance;algorithm	ML	-9.38801090721182	22.59563438999147	120702
a752e589ad915152f2bf2f55673467880635ef1a	the constrained shortest path problem: a case study in using asms.	efficient implementation;shortest path problem	This paper addresses the correctness problem of an algorithm solving the constrained shortest path problem. We deene an abstract, nondeterministic form of the algorithm and prove its correctness from a few simple axioms. We then deene a sequence of natural reenements which can be proved to be correct and lead from the abstract algorithm to an eecient implementation due to Ulrich Lauther Lauther 1996] and based on Desrosiers et al. 1995]. Along the way, we also show that the abstract algorithm can be regarded as a natural extension of Moore's algorithm Moore 1957] for solving the shortest path problem.	angular resolution (graph drawing);correctness (computer science);nondeterministic algorithm;shortest path problem	Karl Stroetmann	1997	J. UCS	10.3217/jucs-003-04-0304	private network-to-network interface;canadian traveller problem;widest path problem;constrained shortest path first;longest path problem;shortest job next;computer science;pathfinding;euclidean shortest path;yen's algorithm;shortest path problem;k shortest path routing;shortest path faster algorithm	Logic	-13.607009042326354	20.577584496031044	120747
e62ed9708cf253f3dd1243707283fc9402de201b	a constructive algebraic hierarchy in coq	proof assistant;computacion informatica;satisfiability;article letter to editor;ciencias basicas y experimentales;dependent types;grupo a	We describe a framework of algebraic structures in the proof assistant Coq. We have developed this framework as part of the FTA project in Nijmegen, in which a constructive proof of the Fundamental Theorem of Algebra has been formalized in Coq. The algebraic hierarchy that is described here is both abstract and structured. Structures like groups and rings are part of it in an abstract way, defining e.g. a ring as a tuple consisting of a group, a binary operation and a constant that together satisfy the properties of a ring. In this way, a ring automatically inherits the group properties of the additive subgroup. The algebraic hierarchy is formalized in Coq by applying a combination of labeled record types and coercions. In the labeled record types of Coq, one can use dependent types: the type of one label may depend on another label. This allows to give a type to a dependent-typed tuple like 〈A, f, a〉, where A is a set, f an operation on A and a an element of A. Coercions are functions that are used implicitly (they are inferred by the type checker) and allow, for example, to use the structure A := 〈A, f, a〉 as synonym for the carrier set A, as is often done in mathematical practice. Apart from the inheritance and reuse of properties, the algebraic hierarchy has proven very useful for reusing notations.	algebraic equation;coq (software);dependent type;fault tree analysis;linear algebra;proof assistant;type system;utility functions on indivisible goods	Herman Geuvers;Randy Pollack;Freek Wiedijk;Jan Zwanenburg	2002	J. Symb. Comput.	10.1006/jsco.2002.0552	discrete mathematics;dependent type;mathematics;proof assistant;algorithm;algebra;satisfiability	PL	-13.703148972810506	18.700766481656274	120950
30523d5553ffd0fe14e2ce94c8520a432e61dbcf	the existence of $\omega$-chains for transitive mixed linear relations and its applications	timed automaton;generation time	We show that it is decidable whether a transitive mixed linear relation has an ω-chain. Using this result, we study a number of liveness verification problems for generalized timed automata within a unified framework. More precisely, we prove that (1) the mixed linear liveness problem for a timed automaton with dense clocks, reversal-bounded counters, and a free counter is decidable, and (2) the Presburger liveness problem for a timed automaton with discrete clocks, reversal-bounded counters, and a pushdown stack is decidable.	automata theory;liveness;presburger arithmetic;software verification;stack (abstract data type);timed automaton;transitive closure;unified framework	Zhe Dang	2001	CoRR		discrete mathematics;computer science;mathematics;distributed computing;generation time;timed automaton;pushdown automaton;algorithm;liveness	Logic	-10.81930055893954	24.193671553670022	121098
efc52227b42c1c2ff1c8276503340be8273b034a	exploiting common subexpressions in numerical csps	directed acyclic graph;common subexpression elimination;vu;interval arithmetic;system of equations	It is acknowledged that the symbolic form of the equations is crucial for interval-based solving techniques to efficiently handle systems of equations over the reals. However, only a few automatic transformations of the system have been proposed so far. Vu, Schichl, Sam-Haroud, Neumaier have exploited common subexpressions by transforming the equation system into a unique directed acyclic graph. They claim that the impact of common subexpressions elimination on the gain in CPU time would be only due to a reduction in the number of operations. This paper brings two main contributions. First, we prove theoretically and experimentally that, due to interval arithmetics, exploiting certain common subexpressions might also bring additional filtering/contraction during propagation. Second, based on a better exploitation of n-ary plus and times operators, we propose a new algorithm I-CSE that identifies and exploits all the “useful” common subexpressions. We show on a sample of benchmarks that I-CSE detects more useful common subexpressions than traditional approaches and leads generally to significant gains in performance, of sometimes several orders of magnitude.	algorithm;benchmark (computing);cascading style sheets;central processing unit;cryptographic service provider;directed acyclic graph;experiment;interval arithmetic;limbo;numerical analysis;performance;software propagation	Ignacio Araya;Bertrand Neveu;Gilles Trombettoni	2008		10.1007/978-3-540-85958-1_23	system of linear equations;mathematical optimization;discrete mathematics;common subexpression elimination;computer science;theoretical computer science;mathematics;interval arithmetic;programming language;directed acyclic graph;algorithm	AI	-15.096703093467909	24.09017060996908	121175
3db8ec2171426d73ea8d8880c4a828c217e212b8	model checking constraint ltl over trees		Constraint automata are an adaptation of Büchi-automata that process data words where the data comes from some relational structure S. Every transition of such an automaton comes with constraints in terms of the relations of S. A transition can only be fired if the current and the next data values satisfy all constraints of this transition. These automata have been used in the setting where S is a linear order for deciding constraint LTL with constraints over S. In this paper, S is the infinitely branching infinite order tree T. We provide a PSPACE algorithm for emptiness of T-constraint automata. This result implies PSPACEcompleteness of the satisfiability and the model checking problem for constraint LTL with constraints over T.	algorithm;büchi automaton;constraint automaton;model checking;pspace;relational model	Alexander Kartzow;Thomas Weidner	2015	CoRR		combinatorics;discrete mathematics;mathematics;constraint;algorithm	Logic	-10.375280475188372	23.794645481146183	121397
301c4a0dacdb582e124b0146ffc6356bd258e052	analyzing automata with presburger arithmetic and uninterpreted function symbols	presburger arithmetic;data structure	Abstract   We study a class of extended automata defined by guarded commands over Presburger arithmetic with uninterpreted functions. On the theoretical side, we show that the bounded reachability problem is decidable in this model. On the practical side, the class is useful for modeling programs with unbounded data structures, and the reachability procedure can be used for symbolic simulation, testing, and verification.	automaton;presburger arithmetic;uninterpreted function	Vlad Rusu;Elena Zinovieva	2001	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)00186-0	discrete mathematics;data structure;computer science;presburger arithmetic;mathematics;programming language;algorithm	Logic	-12.27542977509906	24.01661784666678	121593
f626c32c07efb77cad91eef04dc8ae9b6121a4ad	partial mutual exclusion for infinitely many processes	parallel	Partial mutual exclusion is the drinking philosophers problem for complete graphs. It is the problem that a process may enter a critical section CS of its code only when some finite set nbh of other processes are not in their critical sections. For each execution of CS, the set nbh can be given by the environment. We present a starvation free solution of this problem in a setting with infinitely many processes, each with finite memory, that communicate by asynchronous messages. The solution has the property of first-come first-served, in so far as this can be guaranteed by asynchronous messages. For every execution of CS and every process in nbh, between three and six messages are needed. The correctness of the solution is argued with invariants and temporal logic. It has been verified with the proof assistant PVS.	correctness (computer science);critical section;dining philosophers problem;invariant (computer science);mutual exclusion;proof assistant;temporal logic	Wim H. Hesselink	2011	CoRR		critical section;temporal logic;mutual exclusion;computer science;correctness;distributed computing;asynchronous communication;proof assistant;finite set;invariant (mathematics)	PL	-9.471342548563308	25.128519115230574	121774
d59cd50941d985371f6dccf6c43556b8d78e3668	inferring invariants by symbolic execution	fixed point	In this paper we propose a method for inferring invariants for loops in Java programs. An example of a simple while loop is used throughout the paper to explain our approach. The method is based on a combination of symbolic execution and computing fixed points via predicate abstraction. It reuses the axiomatisation of the Java semantics of the KeY system. The method has been implemented within the KeY system which allows to infer invariants and perform verification within the same environment. We present in detail the results of a non-trivial example.	approximation;axiomatic system;correctness (computer science);experiment;fixed point (mathematics);fixed-point iteration;formal verification;heuristic;invariant (computer science);java;key;predicate abstraction;programming paradigm;refinement (computing);speculative execution;static program analysis;symbolic execution;while loop	Peter H. Schmitt;Benjamin Weiß	2007			predicate abstraction;theoretical computer science;symbolic trajectory evaluation;computer science;concolic testing;symbolic programming;fixed point;symbolic execution;while loop;java	SE	-18.833559403984065	27.088815161351857	121866
75cd283079de118ff374c35c1c73792d1f47906f	control flow refinement and symbolic computation of average case bound		This paper presents a new technique for refining the complex control structure of loops that occur in imperative programs. We first introduce a new way of describing program execution patterns – ( + ,·)-path expressions, which is a subset of conventional path expressions with the operators ∨ and ∗ eliminated. The programs induced by ( + ,·)-path expressions have no path interleaving or skipping-over inner loops, which are the two main issues that cause impreciseness in program analysis. Our refinement process starts from a conventional path expression (mathcal{E}) obtained from the control flow graph, and aims to calculate a finite set of ( + ,·)-path expressions ({mathfrak{e}_1, ..., mathfrak{e}_n}) such that the language generated by path expression (mathcal{E}) is equivalent to the union of the languages generated by each ( + ,·)-path expressions (mathfrak{e}_i). In theory, a conventional path expression can potentially generate an infinite set of ( + ,·)-path expressions. To remedy that, we use abstract interpretation techniques to prune the infeasible paths. In practice, the refinement process usually converges very quickly.	control flow;symbolic computation	Hong Yi Chen;Supratik Mukhopadhyay;Zheng Lu	2013		10.1007/978-3-319-02444-8_24	combinatorics;discrete mathematics;theoretical computer science;mathematics	Logic	-17.52139261595385	24.145788733660353	121874
a8f954a8d3f013ccde299369505240fa5f7c80c9	accelerating coverage estimation through partial model checking	coverage estimation time accelerating coverage estimation system design system behavior system model state of the art coverage estimation method mutation based coverage estimation partial model checking technique pmc technique system states pmc method state graph manipulators model checker;mutation partial model checking coverage estimation formal verification model checking;system analysis and design;formal methods;software engineering;model checking estimation synchronization acceleration system analysis and design computational modeling;acceleration;computational modeling;formal verification;partial model checking;model checking;estimation;synchronization;graph theory estimation theory formal verification;software program verification;software software engineering;coverage estimation;mutation	In model checking a system design against a set of properties, coverage estimation is frequently used to measure the amount of system behavior being checked by the properties. A popular coverage estimation method is to mutate the system model and check if the mutation can be detected by the given properties. For each mutation and each property, a full model check is required by some state-of-the-art coverage estimation methods. With such repeated model checking, mutation-based coverage estimation becomes significantly time-consuming. To alleviate this problem, a partial model checking (PMC) technique is proposed to recheck only those system states that were affected by a mutation, thus unnecessary rechecking of a large portion of the system states is avoided and time is saved. The PMC method has been integrated into the State Graph Manipulators model checker. Applying the proposed method to several examples showed that PMC has a saving of 50% to 70% in the coverage estimation time, and a reduction of 90% in mode visits.	model checking;systems design	Yean-Ru Chen;Jia-Jen Yeh;Pao-Ann Hsiung;Sao-Jie Chen	2014	IEEE Transactions on Computers	10.1109/TC.2013.63	acceleration;mutation;model checking;synchronization;estimation;real-time computing;formal methods;formal verification;computer science;theoretical computer science;programming language;computational model;structured systems analysis and design method	SE	-14.028945090019945	29.870642452457815	121901
4cd09caa1937f0df63d99e2e1986e16308823a38	nominal rewriting with name generation: abstraction vs. locality	α conversion;programming language design;binders;name generation;locality;first and higher order rewriting;confluence;higher order;first order;binders α conversion;higher order functions;localized state	Nominal rewriting extends first-order rewriting with Gabbay-Pitts abstractors: bound entities are named, matching respects α-conversion and can be directly implemented thanks to the use of freshness constraints. In this paper we study two extensions to nominal rewriting. First we introduce a NEW quantifier for modelling name generation and restriction. This allows us to model higher-order functions involving local state, and has also applications in concurrency theory. The second extension introduces new constraints in freshness contexts. This allows us to express strategies of reduction and has applications in programming language design and implementation. Finally, we study confluence properties of nominal rewriting and its extensions.	concurrency (computer science);confluence;entity;first-order predicate;higher-order function;local variable;locality of reference;programming language design and implementation;quantifier (logic);replay attack;rewriting;walter pitts	Maribel Fernández;Murdoch James Gabbay	2005		10.1145/1069774.1069779	higher-order logic;computer science;theoretical computer science;first-order logic;programming language;higher-order function;confluence;algorithm	PL	-16.660604333141727	20.639147051043132	121984
0930967d0557a91f4f267b31f746d3deb460c71f	a symmetric modal lambda calculus for distributed computing	foundational language;thepossible world;powerful propositions-as-types interpretation;novel system;intuitionistic modal logic;corresponding program;operational semantics;elimination rule;symmetric modal lambda calculus;natural deduction;distributed computing;lambda calculus;possible worlds;modal logic;type theory;computer languages;scientific computing;computer networks;curry howard isomorphism;type;distributed programming;internet;programming languages;calculus;logic programming	We present a foundational language for spatially distributed programming, called Lambda 5, that addresses both mobility of code and locality of resources. In order to construct our system, we appeal to the powerful propositions-as-types interpretation of logic. Specifically, we take the possible worlds of the intuitionistic modal logic IS5 to be nodes on a network, and the connectives /spl square/ and /spl diams/ to reflect mobility and locality, respectively. We formulate a novel system of natural deduction for IS5, decomposing the introduction and elimination rules for /spl square/ and /spl diams/, thereby allowing the corresponding programs to be more direct. We then give an operational semantics to our calculus that is type-safe, logically faithful, and computationally realistic.	code mobility;computation;curry;curry–howard correspondence;distributed computing;lambda calculus;locality of reference;logical connective;loop-invariant code motion;modal logic;natural deduction;operational semantics;possible world;programming language;s5 (modal logic);sequent calculus;type safety;type theory	Tom Murphy Vii;Karl Crary;Robert Harper;Frank Pfenning	2004	Proceedings of the 19th Annual IEEE Symposium on Logic in Computer Science, 2004.	10.1109/LICS.2004.1319623	modal logic;the internet;computer science;theoretical computer science;lambda calculus;mathematics;possible world;curry–howard correspondence;programming language;natural deduction;logic programming;operational semantics;type theory;algorithm	Logic	-15.888070704668708	19.706922409146376	122121
28b5e90dbc3254a17832bfe1c562b33154f0002d	proof by consistency in constructive systems with final algebra semantics	final algebra semantics;constructive systems	In this paper we study nal algebra semantics for constructive equational systems. A class of models of a constructive system is described, and proven to have a nal algebra. Then we develop a method for proof by consistency with respect to the nal model. Finally we show that the method contains the proof methods of Musser [11], Goguen [2], and Huet and Hullot [5] as special cases.	constructor (object-oriented programming);david musser;test template framework	Olav Lysne	1992		10.1007/BFb0013832	discrete mathematics;pure mathematics;mathematics;constructive proof	DB	-13.005513215438413	19.316427848236295	122145
bfd07e8ef734768f477814690670ca42a7c2e9a7	abstract interpretation-based static safety for actors		Interpretation-based Static Safety for Actors Pierre-Loïc Garoche, Marc Pantel, and Xavier Thirioux Institut de Recherche en Informatique de Toulouse, France Email: tgaroche,pantel,thiriouxu@enseeiht.fr Abstract— The actor model eases the definition of concurrent programs with non uniform behaviors. We present here an encoding of a higher-order actor calculus, CAP, into an abstract interpretation-based framework for the static analysis of mobile systems. Then, we prove that a CAP term and its encoding are bisimilar. Non-trivial properties are captured using existing abstract domains, as well as new ones such as our linearity abstract domain. As far as we know, it is one of the first analyzes that deals with behavioral and occurrence counting properties on a higher-order calculus. The actor model eases the definition of concurrent programs with non uniform behaviors. We present here an encoding of a higher-order actor calculus, CAP, into an abstract interpretation-based framework for the static analysis of mobile systems. Then, we prove that a CAP term and its encoding are bisimilar. Non-trivial properties are captured using existing abstract domains, as well as new ones such as our linearity abstract domain. As far as we know, it is one of the first analyzes that deals with behavioral and occurrence counting properties on a higher-order calculus.	abstract interpretation;actor model;bisimulation;cap computer;dataflow;dead code;email;feret (facial recognition technology);higher-order function;information flow (information theory);process calculus;static program analysis;type system	Pierre-Loïc Garoche;Marc Pantel;Xavier Thirioux	2007	JSW		process calculus;real-time computing;simulation;actor model and process calculi;denotational semantics of the actor model;computer security;algorithm	PL	-10.24502336166584	22.62286679223844	122161
12e0351adbcd0201e758d6f9491dbd63d9b5214c	iterative-free program analysis	dynamic programming;optimal solution;catamorphism;bottom up;sp term;register allocation;top down;dynamic program;control flow graph;tree width;fixed point;control flow;transition systems;control flow analysis;structural control;program analysis;iterative solution	Program analysis is the heart of modern compilers. Most control flow analyses are reduced to the problem of finding a fixed point in a certain transition system, and such fixed point is commonly computed through an iterative procedure that repeats tracing until convergence.This paper proposes a new method to analyze programs through recursive graph traversals instead of iterative procedures, based on the fact that most programs (without spaghetti GOTO) have well-structured control flow graphs, graphs with bounded tree width. Our main techniques are; an algebraic construction of a control flow graph, called SP Term, which enables control flow analysis to be defined in a natural recursive form, and the Optimization Theorem, which enables us to compute optimal solution by dynamic programming.We illustrate our method with two examples; dead code detection and register allocation. Different from the traditional standard iterative solution, our dead code detection is described as a simple combination of bottom-up and top-down traversals on SP Term. Register allocation is more interesting, as it further requires optimality of the result. We show how the Optimization Theorem on SP Terms works to find an optimal register allocation as a certain dynamic programming.	bottom-up proteomics;compiler;control flow analysis;control flow graph;data-flow analysis;dead code;dynamic programming;fixed point (mathematics);goto;iteration;iterative method;linear algebra;optimization problem;program analysis;program optimization;recursion;register allocation;structured programming;top-down and bottom-up design;transition system;treewidth	Mizuhito Ogawa;Zhenjiang Hu;Isao Sasano	2003		10.1145/944705.944716	mathematical optimization;computer science;top-down and bottom-up design;programming language;algorithm	PL	-16.74371721002235	24.325643231088275	122235
ee6817356983c87705a659303dc766173405c423	inductive temporal formula specifications for system verification	system verification;inductive specification;computation tree logic;strong weak temporal order relation			Chikatoshi Yamada;Yasunori Nagata;Zensho Nakao	2005	JACIII	10.20965/jaciii.2005.p0321	linear temporal logic;interval temporal logic;computation tree logic;computer science;theoretical computer science;high-level verification;runtime verification;algorithm	Logic	-12.210345702372148	23.16467580028641	122514
5139de6a49c837590cfc5b66c60d3a052b4b4704	a flexible approach for finding optimal paths with minimal conflicts		Complex systems are usually modelled through a combination of structural and behavioural models, where separate behavioural models make it easier to design and understand partial behaviour. When partial models are combined, we need to guarantee that they are consistent, and several automated techniques have been developed to check this. We argue that in some cases it is impossible to guarantee total consistency, and instead we want to find execution paths across such models with minimal conflicts with respect to a certain metric of interest. We present an efficient and scalable solution to find optimal paths through a combination of the theorem prover Isabelle with the constraint solver Z3. Our approach has been inspired by a healthcare problem, namely how to detect conflicts between medications taken by patients with multiple chronic conditions, and how to find preferable alternatives automatically.	application domain;automated theorem proving;cognitive dimensions of notations;complex systems;computation;consistency model;correctness (computer science);directed graph;experiment;formal verification;isabelle;map;mathematical optimization;run time (program lifecycle phase);satisfiability modulo theories;scalability;solver;z3 (computer)	Juliana Küster Filipe Bowles;Marco B. Caminati	2017		10.1007/978-3-319-68690-5_13	theoretical computer science;complex system;scalability;multiple chronic conditions;computer science;automated theorem proving;constraint satisfaction problem	SE	-12.924304941700154	30.77156186450811	122627
ab4ab4feac8a0c6426ad326118a1e786ab6aef5f	model construction for implicit specifications in model logic	model construction;model logic;implicit specifications	In top-down design of reactive systems, implicit specifications of the form C(P 1, ..., Pn) sat F are often encountered, where C(P 1, ..., Pn) is a system containing the (unknown) processes P 1, ..., Pn, and F is a specification. We present a method for constructing the processes P 1, ..., Pn (as labelled transition systems) when C is given as a context of process algebra (such as CCS), and F is given as a formula of Hennessy-Milner Logic extended with maximal recursion. The main contribution is the treatment of the simultaneous construction of several processes which together act as a model for the specification. We have implemented two prototype tools (a semi-automatic as well as an automatic one) which are based on the presented theory.		Ole Høgh Jensen;Jarl Tuxen Lang;Christian Jeppesen;Kim G. Larsen	1993		10.1007/3-540-57208-2_18	discrete mathematics;programming language;algorithm	Logic	-14.137757579520104	22.309569444450013	122663
ea1f383e33548d9742882194e1acea7e4256447d	automatic termination analysis of logic programs	termination analysis	This paper describes a system implemented in SICStus Prolog for automatically checking left termination of logic programs. Given a program and query, the system answers either that the query terminates or that there may be non-termination. The system can use any norm of a wide family of norms. It can handle automatically most of the examples found in the literature on termination of logic programs, and about half of the programs in the benchmarks of 5]. The algorithm employed by the system consists of three main parts: instantiation analysis (i.e., rigidity analysis), constraint inference, and construction of the query-mapping pairs associated with the program and query. Each of these parts generalizes earlier work related to termination analysis.	algorithm;benchmark (computing);constraint inference;divergence (computer science);logic programming;sicstus prolog;termination analysis;universal instantiation	Naomi Lindenstrauss;Yehoshua Sagiv	1997			real-time computing;termination analysis;computer science	Logic	-19.06785241520734	23.215527112486242	122693
7fc7979c461bf08cb1f11e475bbba9ed7ba79e7d	model finding for recursive functions in smt		SMT solvers have recently been extended with techniques for finding models in presence of universally quantified formulas in some restricted fragments. This paper introduces a translation which reduces axioms specifying a large class of recursive functions, including well-founded (terminating) functions, to universally quantified formulas for which these techniques are applicable. An empirical evaluation confirms that the approach improves the performance of existing solvers on benchmarks from two sources. The translation is implemented as a preprocessor in the solver CVC4.	benchmark (computing);corecursion;newman's lemma;paradox (database);preprocessor;recursion (computer science);satisfiability modulo theories;solver;universal instantiation;universal quantification;z3 (computer)	Andrew Reynolds;Jasmin Christian Blanchette;Simon Cruanes;Cesare Tinelli	2016		10.1007/978-3-319-40229-1_10	discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-15.878719778812995	22.727682049105482	122747
44162094205fafe4347df656a8bc90e3a9a32f62	free applicative functors		Applicative functors [6] are a generalisation of monads. Both allow the expression of effectful computations into an otherwise pure language, like Haskell [5]. Applicative functors are to be preferred to monads when the structure of a computation is fixed a priori. That makes it possible to perform certain kinds of static analysis on applicative values. We define a notion of free applicative functor, prove that it satisfies the appropriate laws, and that the construction is left adjoint to a suitable forgetful functor. We show how free applicative functors can be used to implement embedded DSLs which can be statically analysed.	applicative functor;applicative programming language;computation;embedded system;haskell;monad (functional programming);static program analysis	Paolo Capriotti;Ambrus Kaposi	2013		10.4204/EPTCS.153.2	adjoint functors;discrete mathematics;functor category;mathematics;derived functor;exact functor;algorithm;algebra	PL	-13.83917214234741	18.721303026564353	123189
0828939f7dd9a354cf3842da86843bc50ad8252c	parametric regular path queries	lenguaje programacion;graph theory;program graph;sistema transicion;query language;expresion regular;optimisation;verificacion modelo;teoria grafo;algorithm complexity;camino grafo;algorithm analysis;optimizacion;programming language;graph path;complejidad algoritmo;programa control;query formulation;interrogation base donnee;interrogacion base datos;complexity analysis;verification modele;performance comparison;formulacion pregunta;data tructures;program verification;theorie graphe;formulation question;software engineering;lenguaje interrogacion;analisis programa;transition system;semistructured data;verificacion programa;semi structured data;systeme transition;complexite algorithme;model checking;precomputation;dato semi estructurado;regular expressions;graphe programme;data structures;estructura datos;graph representation;checking program;transition systems;chemin graphe;genie logiciel;programme controle;langage programmation;expression reguliere;algorithms;graph query languages;regular path queries;optimization;analyse algorithme;structure donnee;langage interrogation;propagacion trayecto multiple;program analysis;multipath propagation;analyse programme;verification programme;grafo programa;memoization;data structure;ingenieria informatica;algorithms and data structure;database query;regular expression;analisis algoritmo;verification model;propagation trajet multiple;donnee semistructuree	Regular path queries are a way of declaratively expressing queries on graphs as regular-expression-like patterns that are matched against paths in the graph. There are two kinds of queries: existential queries, which specify properties about individual paths, and universal queries, which specify properties about all paths. They provide a simple and convenient framework for expressing program analyses as queries on graph representations of programs, for expressing verification (model-checking) problems as queries on transition systems, for querying semi-structured data, etc. Parametric regular path queries extend the patterns with variables, called parameters, which significantly increase the expressiveness by allowing additional information along single or multiple paths to be captured and relate.This paper shows how a variety of program analysis and model-checking problems can be expressed easily and succinctly using parametric regular path queries. The paper describes the specification, design, analysis, and implementation of algorithms and data structures for efficiently solving existential and universal parametric regular path queries. Major contributions include the first complete algorithms and data structures for directly and efficiently solving existential and universal parametric regular path queries, detailed complexity analysis of the algorithms, detailed analytical and experimental performance comparison of variations of the algorithms and data structures, and investigation of efficiency tradeoffs between different formulations of queries.	algorithm;analysis of algorithms;data structure;model checking;program analysis;regular expression;semi-structured data;semiconductor industry	Yanhong A. Liu;Tom Rothamel;Fuxiang Yu;Scott D. Stoller;Nanjun Hu	2004		10.1145/996841.996868	data structure;computer science;theoretical computer science;database;programming language;regular expression;algorithm	PL	-18.402242200717254	24.975153455544717	123221
693a24a5c38c53382bb6381cc4efc7921a8f70ed	innocuous constructor-sharing combinations	rewrite systems	We investigate conditions under which confluence and/or termination are preserved for constructor-sharing and hierarchical combinations of rewrite systems, one of which is left-linear and convergent.	naruto shippuden: clash of ninja revolution 3;rewrite (programming)	Nachum Dershowitz	1997		10.1007/3-540-62950-5_71	computer science	ML	-15.565494602211917	20.686426376822133	123336
4542ff434bef82dc76adbe2e47b890cced525515	trends in formal verification techniques for c-based hardware designs	formal verification;system level formal verification;hardware design	Three formal verification approaches targeting C language based hardware designs, which are the central verification technologies for C-based hardware design flows, are presented. First approach is to statically analyze C design descriptions to see if there is any inconsistency/inadequate usages, such as array overbounds accesses, uses of values of variables before initialization, deadlocks, and others. It is based on local analysis of the descriptions and hence applicable to large design descriptions. The key issue for this approach is how to reason about various dependencies among statements as precisely as possible with as short time as possible. Second approach is to model check C design descriptions. Since simple model checking does not work well for large descriptions, automatic abstractions or reductions of descriptions and their refinements are integrated with model checking methods such that reasonably large designs can be processed. By concentrating on particular types of properties, there can be large reductions of design sizes, and as a result, real life designs could be model checked. The last approach is to check equivalence between two C design descriptions. It is based on symbolic simulations of design descriptions. Since there can be large numbers of execution paths in large design descriptions, various techniques to reduce the numbers of execution paths to be examined are incorporated. All of the presented methods use dependence analysis on data, control, and others as their basic analysis techniques. System dependence graph for programming languages are extended to deal with C based hardware designs that have structural hierarchy as well. With those techniques, reasonably large design descriptions can be checked.	boolean satisfiability problem;deadlock;dependence analysis;formal equivalence checking;formal verification;hardware description language;model checking;programming language;real life;simulation;software verification;symbolic computation;turing completeness	Masahiro Fujita	2009	IPSJ Trans. System LSI Design Methodology	10.2197/ipsjtsldm.2.2	computer architecture;concepts;verification;formal methods;formal verification;software verification;computer science;formal specification;formal equivalence checking;refinement;high-level verification;runtime verification;intelligent verification;functional verification	EDA	-16.185413855036696	29.00906890494387	123460
dc8b59706db0d25def17d82a8eec2bf2964955f0	incresing the versatility of heuristic based theorem provers	conference paper;theorem prover	Heuristic based theorem proving systems typically impose a fixed ordering on the strategies which they embody. The ordering reflects the general experience of the system designer. As a consequence, there will exist a variety of specific instances where the fixed ordering breaks down. We present an approach which liberates such systems by introducing a more versatile framework for organising proof strategies.	automated theorem proving;declarative programming;heuristic;network address translation;nqthm;precondition;systems design	Alistair Manning;Andrew Ireland;Alan Bundy	1993		10.1007/3-540-56944-8_53	computer science;artificial intelligence;automated theorem proving;programming language;algorithm	Logic	-18.38633761993112	19.302766622891173	123465
22f942035c293c44b8b595379a0c61d5ff37c4b7	verification of data paths using unbounded integers: automata strike back	satisfiability;safety properties;bounded model checking;presburger arithmetic;decision procedure;propositional logic;levels of abstraction;finite automata;polynomial time;hardware design	We present a decision procedure for quantifier-free Presburger arithmetic that is based on a polynomial time translation of Presburger formulas to alternating finite automata (AFAs). Moreover, our approach leverages the advances in SAT solving by reducing the emptiness problem of AFAs to satisfiability problems of propositional logic. In order to obtain a complete decision procedure, we use an inductive style of reasoning as originally proposed for proving safety properties in bounded model checking. Besides linear arithmetic constraints, our decision procedure can deal with bitvector operations that frequently occur in hardware design. Thus, it is well-suited for the verification of data paths at a high level of abstraction.	automaton;bit array;boolean satisfiability problem;cobham's thesis;emoticon;fractional fourier transform;model checking;netlist;presburger arithmetic;quantifier (logic);sequence read archive;software upgrade protocol;software bug	Tobias Schüle;Klaus Schneider	2006		10.1007/978-3-540-70889-6_5	time complexity;computer science;presburger arithmetic;propositional calculus;finite-state machine;algorithm;satisfiability	Logic	-14.190543687512	25.398271941084573	123665
f4426ec1fb47d9715f7ff9eab12506eda3aa6985	towards a formal representation of interactive systems	verification;network algebra;2 dimensional languages;kleene theorem;specification;parallel programming;regular algebra;regular expressions;structured interactive programming;finite interactive systems;interactive programming;relational semantics	Powerful algebraic techniques have been developed for classical sequential computation. Many of them are based on regular expressions and the associated regular algebra. For parallel and interactive computation, extensions to handle 2-dimensional patterns are often required. Finite interactive systems, a 2-dimensional version of finite automata, may be used to recognize 2-dimensional languages. In this paper we present a blueprint for getting a formal representation of parallel, interactive programs and of their semantics. It is based on a recently introduced approach for getting regular expressions for 2-dimensional patterns, particularly using words of arbitrary shapes and powerful control mechanisms on composition. We extend the previously defined class of expressions n2RE with new control features, progressively increasing the expressive power of the formalism up to a level where a procedure for generating the words accepted by finite interactive systems may be obtained. Targeted applications come from the area of modelling, specification, analysis and verification of structured interactive programs via the associated scenario semantics.	interactivity	Iulia Teodora Banu-Demergian;Gheorghe Stefanescu	2014	Fundam. Inform.	10.3233/FI-2014-1017	discrete mathematics;verification;computer science;theoretical computer science;programming language;generalized star height problem;specification;regular expression;algorithm;algebra	HCI	-12.935154666188575	22.016237766347775	123670
af0961b25de9af959a4b8fd5a8dd280381154bfc	building proofs in context	building proofs	When reasoning with implicitly deened contexts or theories, a general notion of proof in context is more appropriate than classical uses of reeection rules. Proofs in a multicontext framework can still be carried out by switching to a context, reasoning within it, and exporting the result. Context switching however does not correspond to reeection or reiication but involves changing the level of nesting of theory within another theory. We introduce a generalised rule for proof in context and a convenient notation to express nesting of contexts, which allows us to carry out reasoning in and across contexts in a safe and natural way.	theory	Giuseppe Attardi;Maria Simi	1994		10.1007/3-540-58792-6_25	implicit personality theory;theoretical computer science;rule of inference;context switch;notation;mathematical proof;natural deduction;computer science;reification (marxism)	PL	-13.992579242245096	19.90496730492391	123841
8ecc995e3daa6110e15550e5378571bd31d00b08	implementing groundness analysis with definite boolean functions	constraint logic programs;boolean function;upper bound;a general works;qa75 electronic computers computer science;abstract interpretation	The domain of deenite Boolean functions, Def , can be used to express the groundness of, and trace grounding dependencies between, program variables in (constraint) logic programs. In this paper, previously unexploited computational properties of Def are utilised to develop an eecient and succinct groundness analyser that can be coded in Prolog. In particular, entailment checking is used to prevent unnecessary least upper bound calculations. It is also demonstrated that join can be deened in terms of other operations, thereby eliminating code and removing the need for preprocessing formulae to a normal form. This saves space and time. Furthermore, the join can be adapted to straightforwardly implement the downward closure operator that arises in set sharing analyses. Experimental results indicate that the new Def implementation gives favourable results in comparison with BDD-based groundness analyses.	a-normal form;binary decision diagram;boolean algebra;join (sql);preprocessor;prolog	Jacob M. Howe;Andy King	2000		10.1007/3-540-46425-5_13	computer science;theoretical computer science;boolean function;upper and lower bounds;programming language;algorithm	DB	-16.573930493396336	23.165323110382317	123971
6a659685bff877d39cb4d5a20fcdbbf42151b84f	the mathsat5 smt solver	mathsat5 smt solver;sat solvers;smt formula;smt application;full support;industrial partner;smt-lib theory;floating point;state-of-the-art smt tool;formal verification;improved incrementality support	MATHSAT is a long-term project, which has been jointly carried on by FBK-IRST and University of Trento, with the aim of develop ing and maintaining a state-of-the-art SMT tool for formal verification (and other applications). MATHSAT5 is the latest version of the tool. It supports most of the SMT-LIB theories and their combinations, and provides many functio al ties (like e.g. unsat cores, interpolation, AllSMT). MATHSAT5 improves its predecessor M ATHSAT4 in many ways, also providing novel features: first, a muc h improved incrementality support, which is vital in SMT applications; seco nd, a full support for the theories of arrays and floating point; third, sound SAT-s tyle Boolean formula preprocessing for SMT formulae; finally, a framework allowi ng users for plugging their custom tuned SAT solvers. M ATHSAT5 is freely available, and it is used in numerous internal projects, as well as by a number of i ndustrial partners.	ext js javascript framework;formal verification;interpolation;mathematical optimization;non-deterministic turing machine;nonlinear system;preprocessor;satisfiability modulo theories;simultaneous multithreading;solver;theory	Alessandro Cimatti;Alberto Griggio;Bastiaan Joost Schaafsma;Roberto Sebastiani	2013		10.1007/978-3-642-36742-7_7	real-time computing;computer science;artificial intelligence;theoretical computer science;programming language;algorithm	Logic	-15.904858680230223	25.235872094148753	124028
78aa53bba166e9aa458a726512540fcf1d6d7e3e	stream differential equations: specification formats and solution methods	computer science formal languages and automata theory;syntactic solution method;f 1 1;f 3 2;stream differential equations;f 4 3;abstract gsos;stream operations;computer science logic in computer science;infinite sequences	Streams, or infinite sequences, are infinite objects of a very simple type, yet they have a rich theory partly due to their ubiquity in mathematics and computer science. Stream differential equations are a coinductive method for specifying streams and stream operations, and their theory has been developed in many papers over the past two decades. In this paper we present a survey of the many results in this area. Our focus is on the classification of different formats of stream differential equations, their solution methods, and the classes of streams they can define. Moreover, we describe in detail the connection between the so-called syntactic solution method and abstract GSOS.	coinduction;computer science;gs/os;streams	Helle Hvid Hansen;Clemens Kupke;Jan J. M. M. Rutten	2014	Logical Methods in Computer Science		computer science;theoretical computer science;mathematics;programming language;algorithm	Theory	-4.958112062877152	18.66150147693063	124162
dd65ecd67b3f9bce19a4f83112548619bed6cd43	a coinductive calculus for asynchronous side-effecting processes	coinductive calculus;concurrent extension;equational calculus;generic mutual exclusion scheme;monadic metalanguage;monadic base;corecursion scheme;base language;concurrent process;abstract framework;monadic encapsulation;mutual exclusion;side effect	We present an abstract framework for concurrent processes in which atomic steps have generic side effects, handled according to the principle of monadic encapsulation of effects. Processes in this framework are potentially infinite resumptions, modelled using final coalgebras over the monadic base. As a calculus for such processes, we introduce a concurrent extension of Moggi’s monadic metalanguage of effects. We establish soundness and completeness of a natural equational axiomatisation of this calculus. Moreover, we identify a corecursion scheme that is explicitly definable over the base language and provides flexible expressive means for the definition of new operators on processes, such as parallel composition. As a worked example, we prove the safety of a generic mutual exclusion scheme using a verification logic built on top of the equational calculus.	axiomatic system;coinduction;encapsulation (networking);monad (functional programming);monadic predicate calculus;mutual exclusion;soundness (interactive proof)	Sergey Goncharov;Lutz Schröder	2011		10.1007/978-3-642-22953-4_24	discrete mathematics;mathematics;monadic predicate calculus;algorithm;algebra	PL	-11.131566653086145	21.249301707939793	124318
ed771bd93e17b331a596bf52d2fc491f45e4025b	termination proofs and derivation lengths in term rewriting systems			rewriting	Dieter Hofbauer	1992				Logic	-14.318972674874253	20.54624439483349	124373
fcad274ec96f5ded99709864332daba2ed3cfba2	games with opacity condition	word problem;imperfect information;system security;discrete event system;interactive system;automata theory	We describe the class of games with opacity condition, as an adequate model for security aspects of computing systems. We study their theoretical properties, relate them to reachability perfect information games and exploit this relation to discuss a search approach with heuristics, based on the directing-word problem in automata theory.	automata theory;heuristic (computer science);reachability;search algorithm;synchronizing word	Bastien Maubert;Sophie Pinchinat	2009		10.1007/978-3-642-04420-5_16	discrete mathematics;simulation;theoretical computer science;mathematics	Logic	-6.291201453340457	23.298818631529457	124505
506dab48684a61d2446fb741117763205d7a2b01	algorithmic program synthesis with partial programs and decision procedures	domain theory;program synthesis;theorem prover;model checking;decision procedure;data structure	"""Program synthesizer can derive programs that are efficient, even surprising, but it must be first """"programmed"""" with human insights about the domain and its implementation tricks. In deductive synthesis, the insights are captured by domain theories, often elusive and always requiring formal expertise. To bring synthesis to everyday programmers, we have been exploring algorithmic synthesis, which is to deductive synthesis what model checking is to deductive verification: Rather than deducing a program with a theorem prover, algorithmic synthesis systematically finds the program in a space of candidate implementations. If we help programmers turn their insights into descriptions of candidates, we have a chance for a practical synthesizer.#R##N##R##N#I will show how sketches-partial programs that syntactically define the candidate space-allow programmers to express their insight while eliding tedious code fragments. These fragments are filled in by CEGIS, our counterexample-guided inductive synthesis algorithm that exploits recent advances in automated decision procedures. I will also show how these decision procedures allow us to implement an oracle that helps the programmer refine and formalize his insight about a problem. Finally, I will describe the linguistic support for synthesis in our SKETCH language and show how we synthesized complex implementations of ciphers, scientific codes, and concurrent lock-free data-structures."""	program synthesis	Rastislav Bodík	2009		10.1007/978-3-642-03237-0_1	model checking;data structure;computer science;theoretical computer science;domain theory;automated theorem proving;programming language;algorithm	Logic	-18.806013581501137	25.06177259641292	124596
d9768e79a66eb3125d0bfe68c3f06e2eae5a2c7d	towards breaking more composition symmetries in partial symmetry breaking		The paper proposes a dynamic method, Recursive Symmetry Breaking During Search (ReSBDS), for efficient partial symmetry breaking. We first demonstrate how Partial Symmetry Breaking During Search (ParSBDS) misses important pruning opportunities when given only a subset of symmetries to break. The investigation pinpoints the culprit and in turn suggests rectification. The main idea is to add extra symmetry breaking constraints during search recursively to prune also symmetric nodes of some pruned subtrees. Thus, ReSBDS can break extra symmetry compositions, but is carefully designed to break only the ones that are easy to identify and inexpensive to break. We present theorems to guarantee the soundness and termination of our approach, and compare our method with popular static and dynamic methods. When the variable (value) heuristic is static, ReSBDS is also complete in eliminating all interchangeable variables (values) given only the generator symmetries. We propose further a light version of ReSBDS method (LReSBDS), which has a slightly weaker pruning power of ReSBDS but with a reduced overhead. We give theoretical characterization on the soundness and termination of LReSBDS, and comparisons on pruning strengths against other symmetry breaking methods including ReSBDS. Extensive experimentations confirm the efficiency of ReSBDS and LReSBDS, when compared against state of the art methods.	heuristic;image rectification;overhead (computing);recursion (computer science);soundness (interactive proof);symmetry breaking;symmetry-breaking constraints;tree (data structure)	Jimmy Ho-Man Lee;Zichen Zhu	2017	Artif. Intell.	10.1016/j.artint.2017.07.006	rectification;explicit symmetry breaking;discrete mathematics;symmetry breaking;dynamic method;recursion;mathematics;homogeneous space;heuristic;soundness	AI	-14.879057856366257	26.25437580660955	124723
bd0b8577c22889b306215d1868186e812a0c5316	thread-modular counterexample-guided abstraction refinement	modular verification;static analysis	We consider the refinement of a static analysis method called thread-modular verification. It was an open question whether such a refinement can be done automatically. We present a counterexampleguided abstraction refinement algorithm for thread-modular verification and demonstrate its potential, both theoretically and practically.	algorithm;cartesian closed category;data structure;forward error correction;newton's method;refinement (computing);static program analysis;thread (computing)	Alexander Malkis;Andreas Podelski;Andrey Rybalchenko	2010		10.1007/978-3-642-15769-1_22	refinement calculus;computer science;theoretical computer science;refinement;high-level verification;programming language;static analysis;algorithm	Logic	-18.594631777599087	27.03762703321004	124742
20bd7811f04f49a72d655a2d684af75ac6156429	weak bisimulation for probabilistic timed automata	probabilistic automaton;timed systems;protocols;probability;sistema temporizado;loi probabilite;ley probabilidad;bisimulacion;timed system;bisimulation;protocolo;cryptographic protocol;68wxx;algorithme;probabilistic model;algorithm;protocole;informatique theorique;probability distribution;probabilidad;automate probabiliste;probabilite;modele probabiliste;systeme temporise;automata probabilista;weak bisimulation;timed automata;computer theory;timing attack;modelo probabilista;probabilistic timed automata;algoritmo;timing;informatica teorica	We are interested in describing timed systems that exhibit probabilistic behaviour. To this purpose, we consider a model of Probabilistic Timed Automata and introduce a concept of weak bisimulation for these automata, together with an algorithm to decide it. The weak bisimulation relation is shown to be preserved when either time, or probability is abstracted away. As an application, we use weak bisimulation for Probabilistic Timed Automata to model and analyze a timing attack on the dining cryptographers protocol. © 2010 Elsevier B.V. All rights reserved.	approximation algorithm;automata theory;complex systems;congruence of squares;dining cryptographers problem;list of cryptographers;model checking;observable;prism (surveillance program);probabilistic bisimulation;state space;static timing analysis;timed automaton	Ruggero Lanotte;Andrea Maggiolo-Schettini;Angelo Troina	2010	Theor. Comput. Sci.	10.1016/j.tcs.2010.09.003	probability distribution;statistical model;communications protocol;discrete mathematics;timing attack;computer science;bisimulation;theoretical computer science;probabilistic automaton;probability;cryptographic protocol;mathematics;timed automaton;algorithm;statistics	Logic	-9.450755031373212	26.37687060792881	125010
09e29a9c16f5abdc9da08a60211880382e7482ee	a verified abstract machine for functional coroutines		Functional coroutines are a restricted form of control mechanism, where each coroutine is represented with both a continuation and an environment. This restriction was originally obtained by considering a constructive version of Parigot’s classical natural deduction which is sound and complete for the Constant Domain logic. In this article, we present a refinement of de Groote’s abstract machine for functional coroutines and we prove its correctness. Therefore, this abstract machine also provides a direct computational interpretation of the Constant Domain logic.	abstract machine;continuation;coroutine;correctness (computer science);natural deduction;refinement (computing)	Tristan Crolard	2015		10.4204/EPTCS.212.1	discrete mathematics;mathematics;algorithm	Logic	-18.507333119638385	20.509689284275144	125101
546650e2f13515c67ebca4ae3c3f96fb14294d1c	a classification of the expressive power of well-structured transition systems	sistema transicion;computer engineering;68q55;language class;reachability;expressiveness;multiensemble;05bxx;red petri;labeled transition system;language theory;03d55;semantics;automaton;teoria lenguaje;semantica;semantique;classification;transition system;expressive power;automata;puissance expressive;systeme transition;rewriting systems;rewrite systems;informatique theorique;asequibilidad;well structured transition systems;automate;classe langage;transition systems;datavetenskap datalogi;atteignabilite;etiqueta;datorteknik;etiquette;computer science;label;petri net;systeme reecriture;clasificacion;theorie langage;reseau petri;computer theory;clase lenguaje;informatica teorica	We compare the expressive power of a class of well-structured transition systems that includes relational automata, (extensions of) Petri nets, lossy channel systems, constrained multiset rewriting systems, and data nets. For each one of these models we study the class of languages generated by labelled transition systems describing their semantics. We consider here two types of accepting conditions: coverability and reachability of a fixed a priori configuration. In both cases we obtain a strict hierarchy in which constrained multiset rewriting systems is the the most expressive model.	automata theory;expressive power (computer science);lossy compression;petri net;reachability;rewriting;well-structured transition system	Parosh Aziz Abdulla;Giorgio Delzanno;Laurent Van Begin	2011	Inf. Comput.	10.1016/j.ic.2010.11.003	computer science;artificial intelligence;theoretical computer science;semantics;automaton;programming language;algorithm	Logic	-5.062365679848466	21.329484858656613	125276
b8c9d00f6e82d80e58af2cd3483be015673f0c06	incorporating cardinality constraints and synonym rules into conditional functional dependencies	databases;probleme satisfiabilite;procesamiento informacion;fonctionnelle;algorithm analysis;complexite calcul;specification;coaccion;satisfiabilite;contrainte;database;base dato;68p15;functional dependency;specification language;probleme np;funcional;puissance expressive;complejidad computacion;constraint;especificacion;functional;specification languages;computational complexity;informatique theorique;problema satisfactibilidad;information processing;base de donnees;analyse algorithme;lenguaje especificacion;cardinalite;traitement information;langage specification;satisfiability problem;analisis algoritmo;68q60;computer theory;informatica teorica	We propose an extension of conditional functional dependencies (CFDs), denoted by CFDcs, to express cardinality constraints, domain-specific conventions, and patterns of semantically related constants in a uniform constraint formalism. We show that despite the increased expressive power, the satisfiability and implication problems for CFDcs remain NP-complete and coNP-complete, respectively, the same as their counterparts for CFDs. We also identify tractable special cases.	cardinality (data modeling);co-np;cobham's thesis;functional dependency;functional derivative;karp's 21 np-complete problems;semantics (computer science)	Wenguang Chen;Wenfei Fan;Shuai Ma	2009	Inf. Process. Lett.	10.1016/j.ipl.2009.03.021	specification language;information processing;computer science;artificial intelligence;mathematics;functional dependency;constraint;boolean satisfiability problem;computational complexity theory;specification;algorithm	DB	-7.2624955463419525	18.807597916500352	125703
8bae82809e2eacf0f48e6fde8f723468b8f20d6a	dependence analysis for recursive data	optimising compilers;data analysis transformers algorithm design and analysis program processors optimizing compilers information analysis high level languages computer science computer languages functional programming;efficient algorithms;dependence analyses;computer languages;high level languages;dependence analysis;grammar transformers;general regular free grammars;dead code analysis;program verification;trees mathematics;approximation operations;functional programming;power method;data analysis;grammars;regular tree grammars;program verification optimising compilers trees mathematics grammars;partially dead recursive data;approximation operation;recursive substructures;grammar operations;recursive data constructions;recursive substructure;recursive data;computer science;optimizing compilers;grammar operations recursive data dependence analysis recursive data constructions partially dead recursive data recursive substructures dependence analyses general regular free grammars mutually recursive grammar transformers approximation operation precise deterministic result;information analysis;transformers;program processors;algorithm design and analysis;precise deterministic result;projections;mutually recursive grammar transformers	This paper describes a general and powerful method for dependence analysis in the presence of recursive data constructions. The particular analysis presented is for identifying partially dead recursive data, but the general framework for representing and manipulating recursive substruc-tures applies to all dependence analyses. The method uses projections based on general regular tree grammars extended with notions of live and dead, and deenes the analysis as mutually recursive grammar transformers. To guarantee that the analysis terminates, we use carefully designed approximations. We describe how to approximate argument projections with grammars that can be computed without iterating and how to approximate resulting projections with a widening operation. We design an approximation operation that combines two grammars to give the most precise deterministic result possible. All grammar operations used in the analysis have eecient algorithms. The overall analysis yields signiicantly more precise results than other known methods.	approximation algorithm;dependence analysis;mutual recursion;recursion (computer science);recursive grammar;regular tree grammar;transformers	Yanhong A. Liu	1998		10.1109/ICCL.1998.674171	parsing expression grammar;computer science;theoretical computer science;data analysis;programming language;functional programming;algorithm;recursive partitioning	PL	-13.959090689922276	27.965200599853237	125715
d6b49b8f571531e77ab035c6e549e7b096b08df0	an automata theory dedicated towards formal circuit synthesis	hardware synthesis;automata theory;circuit synthesis	This is a technical report about a theory named Automata. Automata is an arithmetic for synchronous circuits. It provides means for representing and transforming circuit descriptions at the RT level and gate level in a mathemtical manner. Automata has been implemented in the HOL theorem proving environment. Preproven theorems are designed for performing standard synthesis steps such as state encoding, retiming and state minimization in a mathematical manner via logical derivation.	and gate;algorithm;automata theory;automated theorem proving;automaton;correctness (computer science);hol (proof assistant);mathematical optimization;principle of abstraction;retiming;scheduling (computing)	Dirk Eisenbiegler;Ramayya Kumar	1995		10.1007/3-540-60275-5_63	computer science;automata theory	Logic	-15.223004595701298	28.697643238401305	125810
b16ed36e5b681204f6fe6695a94cb60a9384d82a	a causal semantics for ccs via rewriting logic	causal semantics;communicating process;logic;operational semantics;semantics;transition;simultaneidad informatica;abstract data type;etiquetage;semantica;semantique;transition system;etiquetaje;proceso comunicante;transicion;concurrency;systeme transition;algebra proceso;causalite;reecriture;processus communicant;type abstrait;rewriting logic;transition systems;algebre processus;labelling;tipo abstracto;rewriting;process algebra;simultaneite informatique;logique;logica;reescritura;causality;causalidad	We consider two operational semantics for CCS defined in the literature: the first exploits proved transition systems (PTS) and the second rewriting logic (RL). We show that the interleaving interpretation of both semantics agree, in that they define the same transitions and exhibit the same non-deterministic structure. In addition, we study causality in CCS computations. We recall its treatment via PTS, exhibiting the notion of causality presented in the literature, and we show how to recast it in the RL semantics via suitable axioms. Also in this case, the two semantics agree.	causal filter;rewriting	Pierpaolo Degano;Fabio Gadducci;Corrado Priami	2002	Theor. Comput. Sci.	10.1016/S0304-3975(01)00165-7	discrete mathematics;rewriting;computer science;formal semantics;mathematics;semantics;programming language;well-founded semantics;operational semantics;algorithm	Logic	-9.350456779658005	21.381534656012857	125996
5dea500473d16d0ae8cab2d47eb9e8e73c3506de	abstract syntax for variable binders: an overview	labeled tree;program transformation;logical programming;transformation programme;program optimization;resolucion problema;theorem prover;transformacion programa;analyse syntaxique;first order;programmation logique;analisis sintaxico;abstract syntax;syntactic analysis;higher order abstract syntax;optimisation programme;programacion logica;problem solving;resolution probleme;optimizacion programa	A large variety of computing systems, such as compilers, interpreters, static analyzers, and theorem provers, need to manipulate syntactic objects like programs, types, formulas, and proofs. A common characteristic of these syntactic objects is that they contain variable binders, such as quantifiers, formal parameters, and blocks. It is a common observation that representing such binders using only first-order expressions is problematic since the notions of bound variable names, free and bound occurrences, equality up to alpha-conversion, substitution, etc., are not addressed naturally by the structure of first-order terms (labeled trees). This overview describes a higher-level and more declarative approach to representing syntax within such computational systems. In particular, we shall focus on a representation of syntax called higher-order abstract syntax and on a more primitive version of that representation called λ-tree syntax.	abstract syntax;encode;free variables and bound variables;logical connective;parse tree;parsing	Dale Miller	2000		10.1007/3-540-44957-4_16	abstract syntax;discrete mathematics;computer science;artificial intelligence;parsing;program optimization;first-order logic;abstract semantic graph;mathematics;automated theorem proving;programming language;homoiconicity;algorithm;abstract syntax tree;syntax error	NLP	-18.309512919496985	21.102182420030235	126142
ecbdc7a6e6bb7b80a0322d92c1719b871da328b9	automating modular verification	verification;automatic verification;modulo;user needs;reachability;controlabilidad;concepcion sistema;controllability;infinite state system;abstraction;abstraccion;transition system;controlabilite;systeme transition;system design;verification automatique;asequibilidad;state space;modular verification;atteignabilite;guidance;systeme etat infini;modular construction;verificacion;state explosion;regle verification modulaire;module;conception systeme;automation	Modular techniques for automatic verification attempt to ov ercome the state-explosion problem by exploiting the modular stru cture naturally present in many system designs. Unlike other tasks in the verificatio n of finite-state systems, current modular techniques rely heavily on user guida nce. In particular, the user is typically required to construct module abstraction s that are neither too detailed as to render insufficient benefits in state exploratio n, nor too coarse as to invalidate the desired system properties. In this paper, we construct abstract modules automatically, using reachability and controllabili ty information about the concrete modules. This allows us to leverage automatic veri fication techniques by applying them in layers: first we compute on the state space s of system components, then we use the results for constructing abstracti ons, and finally we compute on the abstract state space of the system. Our experimen tal r sults indicate that if reachability and controllability information is us ed in the construction of abstractions, the resulting abstract modules are often sig nificantly smaller than the concrete modules and can drastically reduce the space an d time requirements	reachability;requirement;signature block;state space	Rajeev Alur;Luca de Alfaro;Thomas A. Henzinger;Freddy Y. C. Mang	1999		10.1007/3-540-48320-9_8	module;discrete mathematics;verification;controllability;computer science;state space;theoretical computer science;automation;abstraction;reachability;algorithm;modulo;systems design	Logic	-16.478114609201995	27.93644439120561	126547
618ab79453709ea40d908c3d26643bcca7181d22	using boolean cardinality constraint for lts bounded model checking		Generally concurrent software is harder to find a bug than sequential one. Thus there have been a number of works on formal verification which exhaustively checks all behaviors of concurrent software. Bounded Model Checking (BMC) is widely used in formal verification of concurrent software systems and exhaustively checks whether some errors exist in execution traces of the given system or not within the given limit called as a bound. In this paper, we develop the tool LTS-BMC that accepts both LTS (Labeled Transition System) as a modeling language and FLTL (Fluent Linear Temporal Logic) as a specification language. And the specification is checked against the model using a SAT solver. To translate them into a set of CNF clauses which is the input format to most SAT solvers, we propose efficient CNF encoding techniques and apply to several case studies. As a result, LTS-BMC shows a good performance.	cardinality (data modeling);model checking	Sachoun Park;Gihwon Kwon	2008			modeling language;linear temporal logic;data mining;model checking;discrete mathematics;computer science;specification language;transition system;algorithm;formal verification;boolean satisfiability problem;bounded function	Logic	-14.290248563996219	25.96888094249067	126581
5306d799e8ecf286db2c4a302ba5279779d988c1	deciding branching bimiliarity of normed context-free processes is in \sigma^ p_2	minimisation;minimization;clase complejidad;context free;minimizacion;graphe processus;equivalence bisimulation ramifiee;classe complexite;complexity class;normed context free process;polynomial time;decidibilidad;processus cf norme;bisimilitude;decidabilite;decidability	We show that the problem of deciding branching bisimulation equivalence for normed context-free processes is in Σ 2  p , the second level of the polynomial-time hierarchy, and hence in PSPACE. We also show that minimization of normed context-free process graphs is in PSPACE		Didier Caucal;Dung T. Huynh;Lu Tian	1995	Inf. Comput.	10.1006/inco.1995.1069	decidability;time complexity;complexity class;minimisation;combinatorics;discrete mathematics;computer science;mathematics;algorithm	Logic	-6.282290223419448	20.60164430975743	126593
35ce28979f18d45ac2245911dc87b6a9bd8d46a4	better bounds for event sequencing testing	software testing;event sequence testing;covering array algorithm design and analysis program testing software testing event sequence testing combinatorial testing;program testing;combinatorial testing;covering array;upper bounds event sequencing testing problem event sequence permutation exponential behavior element cardinality subset lower bounds;algorithm design and analysis;vectors upper bound arrays conferences software testing sequential analysis	A permutation of a sequence of events is a common construction in many testing environments. Covering all possible permutations has clearly an exponential behavior; so one can ask for partial (easier) requirement, to cover all possible orders: permutations induced on of a small cardinality subset of elements. In our paper we show better (both lower and upper) bounds on this event sequencing testing problem. We also discuss another variant of the problem where we impose restrictions on the permutations. In this case we show how to achieve an exponential growth in the complexity of the problem. We also give solutions for specific cases of the problem.	calculus of constructions;real life;requirement;time complexity	Oded Margalit	2013	2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops	10.1109/ICSTW.2013.39	algorithm design;model-based testing;orthogonal array testing;all-pairs testing;computer science;software engineering;software testing;algorithm	SE	-10.488557009482566	30.58090572696035	126701
0e586d49df75713086a526cf38dbc038e0676410	verifying properties of hms machine specifications of real-time systems	safety properties;theorem proving;model checking;hard real time system;real time systems	A hierarchical multi-state (HMS) machine is an automaton in which multiple states may be true, multiple transitions may fire simultaneously, a state may be expanded into a lower-level HMS machine, and in which the transitions are controlled by predicates in a temporal interval logic called TIL. HMS machines provide a compact formalism for specifying and verifying the behavior of concurrent hard real-time systems. Two approaches to verification of properties of non-recursive HMS machines are presented: (1) an extension of tableau-based theorem proving that utilizes the logic TIL and the execution semantics of machines to verify safety properties, and (2) a variation of model checking that uses interacting parametric computation graphs. The first verification approach avoids the construction of a complete computation graph and the second approach permits the analysis of behavior of certain types of HMS machines using multiple, relatively simple computation graphs.	real-time operating system;real-time transcription	Armen Gabrielian;R. Iyer	1991		10.1007/3-540-55179-4_39	model checking;real-time computing;computer science;automated theorem proving;programming language;algorithm	Embedded	-11.129503886976034	25.850146904650696	126868
5af9243006992fa13c51a3493f8e9805869a6131	automatic symmetry breaking method combined with sat	search space;satisfiability;backtrack search;symmetry;hybrid approach;finite models;first order;computational logic;symmetry breaking;backtracking search;propositional satisfiability	Finding models of first-order formulas is an important and challenging problem in computational logic. Many satisfiable formulas have finite models. To find a finite model of a firstorder formula, one may use a backtracking search procedure directly, or transform the problem into propositional satisfiability. In the former approach, symmetries can be used effectively to prune the search space; while the latter approach benefits from the efficiency of the unit propagation rule. In this paper, a hybrid approach is proposed. The automatic symmetry breaking method is used as a preprocessing step, and then the propositional satisfiability procedure is called to complete the search. Experimental results show that the hybrid approach is better in some cases.	backtracking;boolean satisfiability problem;computational logic;first-order reduction;preprocessor;software propagation;symmetry breaking;unit propagation	Jian Zhang	2001		10.1145/372202.372206	symmetry breaking;computer science;first-order logic;computational logic;dpll algorithm;symmetry;algorithm;backtracking;satisfiability	AI	-15.041932771450538	23.75201095068434	127054
2d9392c288284ad3e412a37e6166d7d10450ba96	synthesising certificates in networks of timed automata	iterative refinement process;certificate synthesis;iterative process;homomorphic abstraction;red automata;verificacion modelo;calculateur embarque;analyse statique;sistema temporizado;real time;embedded real time systems;timed system;verification modele;abstraction;automate temporise;program verification;abstraccion;analisis de asequibilidad;analisis estatica;automata contemporizado;reseau automate;iterative methods;refinement method;embedded systems;proceso iterativo;processus iteratif;verificacion programa;forward reachability analysis;reachability analysis automata theory embedded systems formal verification iterative methods;formal verification;model checking;temps reel;embedded real time systems certificate synthesis timed automata homomorphic abstraction model checking forward reachability analysis backward reachability analysis iterative refinement process;boarded computer;data flow analysis;systeme temporise;tiempo real;automata theory;analyse flux donnee;analisis de flujo de datos;backward reachability analysis;timed automata;methode raffinement;static analysis;verification programme;metodo afinamiento;automaton network;calculador embarque;reachability analysis;probleme direct;problema directo;analyse atteignabilite;direct problem	The authors present an automatic method for the synthesis of certificates for components in embedded real-time systems. A certificate is a small homomorphic abstraction that can transparently replace the component during model checking: if the verification with the certificate succeeds, then the component is guaranteed to be correct; if the verification with the certificate fails, then the component itself must be erroneous. The authors give a direct construction, based on a forward and backward reachability analysis of the timed system, and an iterative refinement process, which produces a series of successively smaller certificates. In their experiments, model checking the certificate is several orders of magnitude faster than model checking the original system.	algorithm;approximation;automata theory;best, worst and average case;documentation;embedded system;experiment;information;interrupt;iterative method;iterative refinement;model checking;public key certificate;reachability;real-time clock;real-time computing;refinement (computing);requirement;time complexity;timed automaton;type system;worst-case complexity	Bernd Finkbeiner;Hans-Jörg Peter;Sven Schewe	2010	IET Software	10.1049/iet-sen.2009.0047	model checking;formal verification;computer science;theoretical computer science;data-flow analysis;iterative and incremental development;automata theory;abstraction;certificate;iterative method;programming language;static analysis;algorithm	Embedded	-16.802028333215798	27.69883043348983	127289
16f1df463c4303cfe174bcf47ca2bfdc5ddb19ea	verification of proofs of unsatisfiability for cnf formulas	computability;formal verification;theorem proving;cnf formula;sat algorithm;sat solver;conjunctive normal form;formal verification;proof verification;satisfiability problem;unsatisfiability	"""As SAT-algorithms become more and more complex, there is little chance of writing a SAT-solver that is free of bugs. So it is of great importance to be able to verify the information returned by a SAT-solver. If the CNF formula to be tested is satisfiable, solution verification is trivial and can be easily done by the user. However, in the case of unsatisfiability, the user has to rely on the reputation of the SAT-solver. We describe an efficient procedure for checking the correctness of unsatisfiability proofs. As a by-product, the proposed procedure finds an unsatisfiable core of the initial CNF formula. The efficiency of the proposed procedure was tested on a representative set of large """"real-life"""" CNF formulas from the formal verification domain."""	algorithm;boolean satisfiability problem;computational complexity theory;conjunctive normal form;correctness (computer science);formal verification;real life;software bug;solver;verification and validation	Eugene Goldberg;Yakov Novikov	2003			computer science;theoretical computer science;boolean satisfiability problem;algorithm	AI	-15.015421071386626	25.002370061798334	127465
5883faf93f0c297f923db12aa85506e1199c155c	efficiently parsable extensions to tree-local multicomponent tag	conference paper;strain localization	Recent applications of Tree-Adjoining Grammar (TAG) to the domain of semantics as well as new attention to syntactic phenomena have given rise to increased interested in more expressive and complex multicomponent TAG formalisms (MCTAG). Although many constructions can be modeled using tree-local MCTAG (TL-MCTAG), certain applications require even more flexibility. In this paper we suggest a shift in focus from constraining locality and complexity through treeand set-locality to constraining locality and complexity through restrictions on the derivational distance between trees in the same tree set in a valid derivation. We examine three formalisms, restricted NS-MCTAG, restricted Vector-TAG and delayed TL-MCTAG, that use notions of derivational distance to constrain locality and demonstrate how they permit additional expressivity beyond TLMCTAG without increasing complexity to the level of set local MCTAG.	locality of reference;nsb/appstudio;parsing;sl (complexity);transform, clipping, and lighting;tree-adjoining grammar	Rebecca Nesson;Stuart M. Shieber	2009		10.3115/1620754.1620768	computer science;algorithm	NLP	-11.48481988094276	19.464308734035786	127768
e69230c860a8a837ee5d884beef43836a754a9c7	on model checking techniques for randomized distributed systems	distributed system;system modeling;partial information;worst case analysis;model checking;parallel systems;linear time;polynomial time;graph algorithm;linear program;markov decision process;lower bound	"""The automata-based model checking approach for randomized distributed systems relies on an operational interleaving semantics of the system by means of a Markov decision process and a formalization of the desired event E by an ω-regular linear-time property, e.g., an LTL formula. The task is then to compute the greatest lower bound for the probability for E that can be guaranteed even in worst-case scenarios. Such bounds can be computed by a combination of polynomially time-bounded graph algorithm with methods for solving linear programs. In the classical approach, the """"worst-case"""" is determined when ranging over all schedulers that decide which action to perform next. In particular, all possible interleavings and resolutions of other nondeterministic choices in the system model are taken into account. The worst-case analysis relying on this general notion of schedulers is often too pessimistic and leads to extreme probability values that can be achieved only by schedulers that are unrealistic for parallel systems. This motivates the switch to more realistic classes of schedulers that respect the fact that the individual processes only have partial information about the global system states. Such classes of partial-information schedulers yield more realistic worst-case probabilities, but computationally they are much harder. A wide range of verification problems turns out to be undecidable when the goal is to check that certain probability bounds hold under all partial-information schedulers."""	distributed computing;model checking;randomized algorithm	Christel Baier	2010		10.1007/978-3-642-16265-7_1	time complexity;computer science;linear programming;theoretical computer science;programming language;algorithm	Logic	-9.375775016360407	26.017753614218737	127882
e878d2ebaa085787915751e339c579089786d9a5	on process equivalence = equation solving in ccs	automatic verification;calculus of communicating systems;unique fixpoint induction equational verification calculus of communicating systems;equational verification;inference rule;state space;unique fixpoint induction;journal of automated reasoning;equational verification calculus of communicating systems;ccs;article	Unique Fixpoint Induction (UFI) is the chief inference rule to prove the equivalence of recursive processes in the Calculus of Communicating Systems (CCS) (Milner 1989). It plays a major role in the equational approach to verification. Equational verification is of special interest as it offers theoretical advantages in the analysis of systems that communicate values, have infinite state space or show parameterised behaviour. We call these kinds of systems VIPSs. VIPSs is the acronym of Value-passing, Infinite-State and Parameterised Systems. Automating the application of UFI in the context of VIPSs has been neglected. This is both because many VIPSs are given in terms of recursive function symbols, making it necessary to carefully apply induction rules other than UFI, and because proving that one VIPS process constitutes a fixpoint of another involves computing a process substitution, mapping states of one process to states of the other, that often is not obvious. Hence, VIPS verification is usually turned into equation solving (Lin 1995a). Existing tools for this proof task, such as VPAM (Lin 1993), are highly interactive. We introduce a method that automates the use of UFI. The method uses middle-out reasoning (Bundy et al. 1990a) and, so, is able to apply the rule even without elaborating the details of the application. The method introduces meta-variables to represent those bits of the processes’ state space that, at application time, were not known, hence, changing from equation verification to equation solving. Adding this method to the equation plan developed by Monroy et al. (Autom Softw Eng 7(3):263–304, 2000a), we have implemented an automatic verification planner. This planner increases the number of verification problems that can be dealt with fully automatically, thus improving upon the current degree of automation in the field.	calculus of communicating systems;equation solving;fixed point (mathematics);formal verification;process substitution;recursion (computer science);state space;turing completeness;vips (software)	Raúl Monroy;Alan Bundy;Ian Green	2009	Journal of Automated Reasoning	10.1007/s10817-009-9125-x	discrete mathematics;computer science;state space;artificial intelligence;theoretical computer science;mathematics;programming language;calculus of communicating systems;algorithm;functional verification;rule of inference	Logic	-18.578342033924642	18.665876939098435	127957
f24a5b82c29d8e207368c9e1533fdf5ff953fd29	a logic for miranda	qa 76 software;program verification;functional programming;fixed point;computer programming;pattern matching;functional programming language	We formulate a logical description of the functional programming language Miranda. Distinctive features include a full treatment of pattern matching with repeated variables and the characterisation of various (sub-)domains, like the defined natural numbers and finite definite lists, by means of new quantifiers. These quantifiers are introduced by induction rules, and also carry elimination rules. We also discuss the rôle of fixed point induction and issues of modularisation and scale.	fixed point (mathematics);functional programming;mathematical induction;miranda;pattern matching;programming language	Simon J. Thompson	1989	Formal Aspects of Computing	10.1007/BF01887213	declarative programming;programming domain;reactive programming;functional reactive programming;computer science;theoretical computer science;functional logic programming;pattern matching;computer programming;mathematics;fixed point;programming paradigm;inductive programming;programming language;functional programming;algorithm	Logic	-16.504284538337505	19.590757542337208	128025
9b2cc0fec2c2b465aa4a1ab5ad705c6d56ad947e	bitwidth reduction via symbolic interval analysis for software model checking	program diagnostics;lower and upper bound;boolean functions;application software;software model checking;program verification;data mining;software engineering;state explosion problem bitwidth reduction symbolic interval analysis software model checking lower bound upper bound program variables;upper bound;model checking;data structures;program variables;lab on a chip explosions hardware data structures boolean functions context modeling upper bound application software software engineering data mining;state explosion problem;bitwidth reduction;lab on a chip;software engineering abstract interpretation interval analysis model checking program analysis;explosions;program analysis;state explosion;abstract interpretation;context modeling;symbolic interval analysis;lower bound;interval analysis;hardware;program verification program diagnostics	This paper presents a lightweight interval analysis technique for determining the lower and upper bounds for program variables and its application in improving software model checking techniques. The experiments demonstrate that it is an effective approach to alleviate the state explosion problem in software model checking.	experiment;interval arithmetic;model checking	Aleksandr Zaks;Zijiang Yang;Ilya Shlyakhter;Franjo Ivancic;Srihari Cadambi;Malay K. Ganai;Aarti Gupta;Pranav Ashar	2008	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2008.925777	model checking;real-time computing;data structure;computer science;theoretical computer science;upper and lower bounds;programming language;abstraction model checking;symbolic trajectory evaluation;algorithm	SE	-16.505285562479145	29.68088389255223	128148
21ecd77531ff53d48ff519284e846a79308ef4f2	prism: probabilistic symbolic model checker	symbolic computation;evaluation performance;diagrama binaria decision;logica temporal;chaine markov;diagramme binaire decision;cadena markov;performance evaluation;temporal logic;evaluacion prestacion;probabilistic system;probabilistic approach;program verification;continuous time markov chain;qualite service;calculo simbolico;probabilistic model;aleatorizacion;verificacion programa;model checking;enfoque probabilista;approche probabiliste;algorithme reparti;cell communication;randomisation;algoritmo repartido;markov decision process;sparse matrix;verification enumerative;quality of service;randomization;verification programme;distributed algorithm;calcul symbolique;polling system;sparse matrices;logique temporelle;service quality;calidad servicio;markov chain;binary decision diagram;workstation cluster	In this paper we describe PRISM, a tool being developed at the University of Birmingham for the analysis of probabilistic systems. PRISM supports three probabilistic models: discrete-time Markov chains, continuous-time Markov chains and Markov decision processes. Analysis is performed through model checking such systems against specifications written in the probabilistic temporal logics PCTL and CSL. The tool features three model checking engines: one symbolic, using BDDs (binary decision diagrams) and MTBDDs (multi-terminal BDDs); one based on sparse matrices; and one which combines both symbolic and sparse matrix methods. PRISM has been successfully used to analyse probabilistic termination, performance, dependability and quality of service properties for a range of systems, including randomized distributed algorithms [2], polling systems [22], workstation clusters [18] and wireless cell communication [17].	binary decision diagram;cell signaling;citation style language;dependability;distributed algorithm;markov chain;markov decision process;matrix method;model checking;prism (surveillance program);probabilistic ctl;quality of service;randomized algorithm;sparse matrix;workstation	Marta Z. Kwiatkowska;Gethin Norman;David Parker	2002		10.1007/3-540-46029-2_13	distributed algorithm;symbolic computation;sparse matrix;computer science;theoretical computer science;machine learning;database;algorithm;statistics	Logic	-9.458979129084772	27.517869489707564	128525
7e516b646bb1bd4f5069808e457e685da56f934c	a local algorithm for checking probabilistic bisimilarity	probabilistic labelled transition systems;local algorithm;labelled transition system;probabilistic labelled transition systems concurrency probabilistic bisimilarity local algorithm;probability;time complexity;probabilistic bisimilarity checking formal systems concurrency theory decision algorithms finite state systems global algorithms local algorithms system behaviour verification system state space generation polynomial time local algorithm time complexity;probability bisimulation equivalence computational complexity concurrency theory polynomials;polynomials;concurrency;computational complexity;state space;concurrent computing state space methods partitioning algorithms computer science polynomials finishing;polynomial time;bisimulation equivalence;probabilistic bisimilarity;concurrency theory	Bisimilarity is one of the most important relations for comparing the behaviour of formal systems in concurrency theory. Decision algorithms for bisimilarity in finite state systems are usually classified into two kinds: global algorithms are generally efficient but require to generate the whole state spaces in advance, and local algorithms combine the verification of a system's behaviour with the generation of the system's state space, which is often more effective to determine that one system fails to be related to another. Although local algorithms are well established in the classical concurrency theory, the study of local algorithms in probabilistic concurrency theory is not mature. In this paper we propose a polynomial time local algorithm for checking probabilistic bisimilarity. With mild modification, the algorithm can be easily adapted to decide probabilistic similarity with the same time complexity.	bisimulation;concurrency (computer science);formal system;local algorithm;polynomial;state space;time complexity	Yuxin Deng;Wenjie Du	2009	2009 Fourth International Conference on Frontier of Computer Science and Technology	10.1109/FCST.2009.37	time complexity;probabilistic analysis of algorithms;computer science;theoretical computer science;algorithm	Logic	-8.442574175086943	26.360367875736333	128527
255c030c03b8d1b1e5fa156d56f1e5d7d54a9422	axiomatising infinitary probabilistic weak bisimilarity of finite-state behaviours		In concurrency theory, weak bisimilarity is often used to relate processes exhibiting the same observable behaviour. The probabilistic environment gives rise to several generalisations; we study the infinitary semantics, which abstracts from a potentially unbounded number of internal actions being performed when something observable happens. Arguing that this notion yields the most desirable properties, we provide a sound and complete axiomatisation capturing its essence. Previous research has failed to achieve completeness in the presence of unguarded recursion, as only the finitary variant has been axiomatised, yet.	axiomatic system;bisimulation;concurrency (computer science);observable;recursion	Florian Fallegger;Rob van Glabbeek	2019	J. Log. Algebr. Meth. Program.	10.1016/j.jlamp.2018.09.006	completeness (statistics);discrete mathematics;argument;probabilistic logic;semantics;recursion;mathematics;concurrency;finitary;observable	Logic	-10.775879414550404	20.875825653478472	128611
87403f23c637aa164258c71c93de3abda09b75cf	counter-strategy guided refinement of gr(1) temporal logic specifications	formal specification;finite state machines;temporal logic	The reactive synthesis problem is to find a finite-state controller that satisfies a given temporal-logic specification regardless of how its environment behaves. Developing a formal specification is a challenging and tedious task and initial specifications are often unrealizable. In many cases, the source of unrealizability is the lack of adequate assumptions on the environment of the system. In this paper, we consider the problem of automatically correcting an unrealizable specification given in the generalized reactivity (1) fragment of linear temporal logic by adding assumptions on the environment. When a temporal-logic specification is unrealizable, the synthesis algorithm computes a counter-strategy as a witness. Our algorithm then analyzes this counter-strategy and synthesizes a set of candidate environment assumptions that can be used to remove the counter-strategy from the environment's possible behaviors. We demonstrate the applicability of our approach with several case studies.	algorithm;formal specification;linear temporal logic;refinement (computing)	Rajeev Alur;Salar Moarref;Ufuk Topcu	2013	2013 Formal Methods in Computer-Aided Design		discrete mathematics;linear temporal logic;temporal logic;interval temporal logic;computer science;artificial intelligence;theoretical computer science;formal specification;mathematics;finite-state machine;programming language;algorithm;temporal logic of actions	Logic	-14.10228812347552	26.799296004023823	128749
213ca52364aaa08543376f57fc7fa20ec7ee72d7	input-output conformance simulation (iocos) for model based testing		A new model based testing theory built on simulation semantics is presented. At the core of this theory there is an input-output conformance simulation relation (iocos). As a branching semantics iocos can naturally distinguish the context of local choices. We show iocos to be a finer relation than the classic ioco conformance relation. It turns out that iocos is a transitive relation and therefore it can be used both as a conformance relation and a refinement preorder. An alternative characterisation of iocos is provided in terms of testing semantics. Finally we present an algorithm that produces a test suite for any specification. The resulting test suite is sound and exhaustive for the given specification with respect to iocos.	algorithm;coinduction;computer performance;conformance testing;correctness (computer science);denotational semantics;input/output;internet backbone;model-based testing;online and offline;refinement (computing);simulation;test suite;whole earth 'lectronic link	Carlos Gregorio-Rodríguez;Luis Llana;Rafael Martínez-Torres	2013		10.1007/978-3-642-38592-6_9	discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-11.684037660658005	23.33241888067455	129031
f66912e2926a45f36fcabd597bc9adf6637c72ca	sufficient conditions for the marked graph realisability of labelled transition systems		Abstract This paper describes two results within the context of Petri net synthesis from labelled transition systems. We consider a set of structural properties of transition systems, and we show that, given such properties, it is possible to re-engineer a Petri net realisation into one which lies inside the set of marked graphs, a well-understood and useful class of Petri nets. The first result originates from Petri net based workflow specifications, where it is desirable that k customers can share a system without mutual interferences. In a Petri net representation of a workflow, the presence of k customers can be modelled by an initial k -marking, in which the number of tokens on each place is a multiple of k . For any initial k -marking with k ≥ 2 , we show that other desirable assumptions such as reversibility and persistence suffice to guarantee marked graph realisability. For the case that k = 1 , we show that the existence of certain cycles, along with other properties such as reversibility and persistence, again suffices to guarantee marked graph realisability.	marked graph	Eike Best;Thomas Hujsa;Harro Wimmel	2018	Theor. Comput. Sci.	10.1016/j.tcs.2017.10.006	discrete mathematics;combinatorics;petri net;realisation;multiple;workflow;persistence (computer science);mathematics;marked graph;stochastic petri net;graph	Logic	-7.30838749755487	27.140814219555	129443
1511e9339b5eba2146613b79d43c5e35accc6242	an smt-based approach to coverability analysis		Model checkers based on Petri net coverability have been used successfully in recent years to verify safety properties of concurrent shared-memory or asynchronous message-passing software. We revisit a constraint approach to coverability based on classical Petri net analysis techniques. We show how to utilize an SMT solver to implement the constraint approach, and additionally, to generate an inductive invariant from a safety proof. We empirically evaluate our procedure on a large set of existing Petri net benchmarks. Even though our technique is incomplete, it can quickly discharge most of the safe instances. Additionally, the inductive invariants computed are usually orders of magnitude smaller than those produced by existing solvers.	benchmark (computing);discharger;message passing;model checking;petri net;satisfiability modulo theories;shared memory;solver	Javier Esparza;Ruslán Ledesma-Garza;Rupak Majumdar;Philipp J. Meyer;Filip Niksic	2014		10.1007/978-3-319-08867-9_40	computer science;artificial intelligence;theoretical computer science;programming language;algorithm	Logic	-14.431432664869439	26.229718659302232	129446
169c790743187111c0d38213cd6ae4b4da0859e9	polytypic properties and proofs in coq	proof assistant;coq;polytypic programs;polytypic proofs;generic programming	We formalize proofs over Generic Haskell-style polytypic programs in the proof assistant Coq. This makes it possible to do fully formal (machine verified) proofs over polytypic programs with little effort. Moreover, the formalization can be seen as a machine verified proof that polytypic proof specialization is correct with respect to polytypic property specialization.	coq (software);generic programming;haskell;partial template specialization;proof assistant	Wendy Verbruggen;Edsko de Vries;Arthur Hughes	2009		10.1145/1596614.1596616	discrete mathematics;mathematics;programming language;algorithm	PL	-17.60053320880123	22.83932408581217	129476
94d03f1da6106608ee32dafdd1bebb135e77080d	verification of stochastic systems by stochastic satisfiability modulo theories with continuous domain (cssmt)			satisfiability modulo theories	Yang Gao	2015				AI	-5.9107628689576375	25.122211131394188	129486
b9c93de6a4810f809463853e5b0ed81a7f37c7c0	extrapolation-based path invariants for abstraction refinement of fifo systems	hardware verification;network protocol;empirical evaluation	The technique of counterexample-guided abstraction refinement (Cegar) has been successfully applied in the areas of software and hardware verification. Automatic abstraction refinement is also desirable for the safety verification of complex infinite-state models. This paper investigates Cegar in the context of formal models of network protocols, in our case, the verification of fifo systems. Our main contribution is the introduction of extrapolation-based path invariants for abstraction refinement. We develop a range of algorithms that are based on this novel theoretical notion, and which are parametrized by different extrapolation operators. These are utilized as subroutines in the refinement step of our Cegar semi-algorithm that is based on recognizable partition abstractions. We give sufficient conditions for the termination of Cegar by constraining the extrapolation operator. Our empirical evaluation confirms the benefit of extrapolation-based path invariants.	communications protocol;extrapolation;fifo (computing and electronics);genesis;level of measurement;lossy compression;re (complexity);refinement (computing);relevance;semiconductor industry;shinnar–le roux algorithm;subroutine;turing completeness	Alexander Heußner;Tristan Le Gall;Grégoire Sutre	2009		10.1007/978-3-642-02652-2_11	communications protocol;discrete mathematics;computer science;theoretical computer science;algorithm	Logic	-13.760087812512653	27.346840806070716	129579
cb21e28d1716193f111b7bd3cbaaf48c7b910b47	timing verification by successive approximation	parallel composition;satisfiability;temporal properties;time constraint	Apparatus for developing and verifying systems. The disclosed apparatus employs a computationally-tractable technique for verifying whether a system made up of a set of processes, each of which has at least one delay constraint associated with it, satisfies a given temporal property. The technique deals with the verification as a language inclusion problem, i.e., it represents both the set of processes and the temporal property as automata and determines whether there is a restriction of the set of processes such that the language of the automaton representing the restricted set of processes is included in the language of the automaton representing the temporal property. The technique is computationally tractable because it deals with the problem iteratively: it tests whether a current restriction of the set of processes is included, and if not, it employs a counter-example for the inclusion to either determine that the delay constraints render satisfaction of the given temporal property or to derive a new restriction of the set of processes. Further included in the disclosure are techniques for checking the timing consistency of the counter-example with respect to a delay constraint and techniques for finding the optimal delay constraint.	approximation;formal verification;static timing analysis	Rajeev Alur;Alon Itai;Robert P. Kurshan;Mihalis Yannakakis	1992		10.1007/3-540-56496-9_12	theoretical computer science;algorithm;satisfiability	Logic	-10.202694690381099	24.908613260945	129632
22ec2f005243afdae74512c34fe777beccb3259b	algebraic structure of combined traces	lexicographical canonical form;concurrent system;combined trace;fundamental notion;action subalphabets;infinite combined trace;algebraic structure;atomic action;formal model;formal language;step sequence equivalence	Traces – and their extension called combined traces (comtraces) – are two formal models used in the analysis and verification of concurrent systems. Both models are based on concepts originating in the theory of formal languages, and they are able to capture the notions of causality and simultaneity of atomic actions which take place during the process of a system’s operation. The aim of this paper is a transfer to the domain of comtraces and developing of some fundamental notions, which proved to be successful in the theory of traces. In particular, we introduce and then apply the notion of indivisible steps, the lexicographical canonical form of comtraces, as well as the representation of a comtrace utilising its linear projections to binary action subalphabets. We also provide two algorithms related to the new notions. Using them, one can solve, in an efficient way, the problem of step sequence equivalence in the context of comtraces. One may view our results as a first step towards the development of infinite combined traces, as well as recognisable languages of combined traces.	algebraic equation;algorithm;automata theory;causality;concurrency (computer science);digital footprint;formal language;indivisible;lexicography;linear algebra;linearizability;trace theory;tracing (software);turing completeness	Lukasz Mikulski	2013	Logical Methods in Computer Science	10.2168/LMCS-9(3:8)2013	discrete mathematics;computer science;artificial intelligence;mathematics;algorithm	Logic	-10.3021970642185	21.22145132378577	129636
e4dd3d6d39b9e0db2f50635852a7823399553a23	canonical finite models of kleene algebra with tests	regular language;canonical finite model;program optimization;kleene algebra with tests;syntactic semiring;completeness theorem	Kleene algebra with tests (KAT) was introduced by Kozen as an extension of Kleene algebra (KA). So far, the decidability of equational formulas (  p=q     p  =  q       ) and Horn formulas (  ∧ i p i =q i →p=q       ∧    i      p    i    =    q    i    →  p  =  q       ) in KAT has been investigated by several authors. Continuing this line of research, the current paper studies the decidability of existentially quantified equational formulas   ∃q∈P.(p=q)     ∃  q  ∈  P  .  (  p  =  q  )        in KAT, where P is a fixed collection of KAT terms and plays a role as a parameter of this decision problem. To design a systematic strategy of deciding problems of this form, given in this paper is an effective procedure of constructing from each KAT term  p    a finite KAT model   K(p)     K  (  p  )        that will be called the  canonical finite model  of the KAT term  p   . Applications of this construction are presented, proving the decidability of   ∃q∈P.(p=q)     ∃  q  ∈  P  .  (  p  =  q  )        for several non-trivial P.	kleene algebra	Takeo Uramoto	2016	J. Log. Algebr. Meth. Program.	10.1016/j.jlamp.2015.11.001	combinatorics;discrete mathematics;regular language;computer science;program optimization;mathematics;programming language;algorithm;gödel's completeness theorem	Logic	-10.171333947685147	18.97550045718324	129861
1a87f9a1ecb1e96c57e3ecc25d907815970ef9b2	validating software pipelining optimizations	verification;instruction level parallel;translation validation;compilers;pipeline processors;compiler optimization;optimization;software pipelining	The paper presents a method for translation validation of a specific optimization, software pipelining optimization, used to increase the instruction level parallelism in EPIC type of architectures. Using a methodology as in [15] to establish simulation relation between source and target based on computational induction, we describe an algorithm that automatically produces a set of decidable proof obligations. The paper also describes SPV, a prototype translation validator that automatically produces verification conditions for software pipelining optimizations of the SGI Pro-64 compiler. These verification conditions are further checked automatically by the CVC [12] checker.	algorithm;compiler;computation;instruction-level parallelism;mathematical optimization;parallel computing;pipeline (computing);prototype;satisfiability modulo theories;simulation;software pipelining;validator	Raya Leviathan;Amir Pnueli	2002		10.1145/581630.581676	software pipelining;computer architecture;compiler;parallel computing;verification;software verification;computer science;operating system;optimizing compiler;programming language	PL	-16.787111427745533	30.436425195862153	129971
aec86e13a7f4d83518d5d711504acf5abfabbdb3	decentralized diagnosis by petri nets and integer linear programming		This paper proposes a novel decentralized on-line fault diagnosis approach based on the solution of some integer linear programming problems for discrete event systems in a Petri net framework. The decentralized architecture consists of a set of local sites communicating with a coordinator that decides whether the system behavior is normal or subject to some possible faults. To this aim, some results allow defining the rules applied by the coordinator and the local sites to provide the global diagnosis results. Moreover, two protocols for the detection and diagnosis of faults are proposed: they differ for the information exchanged between local sites and coordinator and the diagnostic capability. In addition, a sufficient and necessary condition under which the second presented protocol can successfully diagnose a fault in the decentralized architecture is proved. Finally, some examples are presented to show the efficiency of the proposed approach.	.net framework;centralized computing;communications protocol;complex system;computational complexity theory;distributed computing;exploit (computer security);fault detection and isolation;integer programming;linear programming;observable;online and offline;petri net;sensor;systems architecture	Xuya Cong;Maria Pia Fanti;Agostino Marcello Mangini;Zhiwu Li	2018	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2017.2726108	mathematical optimization;architecture;fault detection and isolation;petri net;computer science;stochastic petri net;integer programming	Robotics	-7.156785589165339	27.89244963034802	130170
b7d2c0fc376df74210cde4c1946184631628fc0a	sound up-to techniques and complete abstract domains		Abstract interpretation is a method to automatically find invariants of programs or pieces of code whose semantics is given via least fixed-points. Up-to techniques have been introduced as enhancements of coinduction, an abstract principle to prove properties expressed via greatest fixed-points.  While abstract interpretation is always sound by definition, the soundness of up-to techniques needs some ingenuity to be proven. For completeness, the setting is switched: up-to techniques are always complete, while abstract domains are not.  In this work we show that, under reasonable assumptions, there is an evident connection between sound up-to techniques and complete abstract domains.	abstract interpretation;coinduction;denotational semantics;dijkstra's algorithm;invariant (computer science);library (computing);proof assistant;proof calculus	Filippo Bonchi;Pierre Ganty;Roberto Giacobazzi;Dusko Pavlovic	2018		10.1145/3209108.3209169	algorithm;discrete mathematics;completeness (statistics);mathematics;semantics;ingenuity;abstract interpretation;soundness;invariant (mathematics)	Logic	-13.350251666997837	21.212640184293473	130171
0266e2eae0541e1e3d8d8b27851c871f1c81fdcf	epistemic strategies and games on concurrent processes	probability;schedulers;game semantics;concurrency;epistemic logic;process algebra	We develop a game semantics for process algebra with two interacting agents. The purpose of our semantics is to make manifest the role of knowledge and information flow in the interactions between agents and to control the information available to interacting agents. We define games and strategies on process algebras, so that two agents interacting according to their strategies determine the execution of the process, replacing the traditional scheduler. We show that different restrictions on strategies represent different amounts of information being available to a scheduler. We also show that a certain class of strategies corresponds to the syntactic schedulers of Chatzikokolakis and Palamidessi, which were developed to overcome problems with traditional schedulers modelling interaction. The restrictions on these strategies have an explicit epistemic flavour.	adversary (cryptography);agent-based model;concurrency (computer science);control theory;distributed computing;epistemic modal logic;game semantics;information theory;interaction;lawrence a. hyland;linear algebra;mobile agent;moses;process calculus;scheduling (computing);shannon (unit);speculative execution;synchronization (computer science);transition system;turing completeness	Konstantinos Chatzikokolakis;Sophia Knight;Catuscia Palamidessi;Prakash Panangaden	2012	ACM Trans. Comput. Log.	10.1145/2362355.2362356	process calculus;discrete mathematics;epistemic modal logic;concurrency;computer science;game semantics;theoretical computer science;probability;mathematics;distributed computing;programming language;algorithm	Logic	-12.039474359712782	21.293578299336986	130377
5e2bd1822131e9718936e754b895d84009074cdb	using quasi ordered sets to model program properties denotationally	ordered set;partially ordered set	Properties of programs are often not expressible with the standard de-notational semantics approach. Annotated semantic domains and corresponding semantic functions capacitate us to express the properties, but the resulting structures are no longer partial ordered sets. Instead we obtain quasi ordered sets, which lack the antiisymmetry which is present in partially ordered sets. In order to cater with these structures, we develop a new xpoint theory for quasi ordered sets. This theory is also useful to denotationally model semantics for situations where nonnmonotonic operations occur.	3d xpoint	Markus Mohnen	1997		10.1007/978-3-642-60831-5_73	total order;ordered vector space	DB	-9.879004648428069	18.257148937366413	130479
255dfd827a185cd0e87577528a3a269cf63eaef1	model and program repair via sat solving	temporal logic;model checking;shared memory;sat solver;supervisory control;polynomial time;satisfiability	We consider the <i>subtractive model repair problem</i>: given a finite Kripke structure <i>M</i> and a CTL formula η, determine if <i>M</i> contains a substructure <i>M</i><sup>′</sup> that satisfies η. Thus, <i>M</i> can be “repaired” to satisfy <i>eta</i> by deleting some transitions and states. We map an instance ⟨ <i>M</i>,η ⟩ of model repair to a Boolean formula <i>repair</i>(<i>M</i>,η) such that ⟨ <i>M</i>,η ⟩ has a solution iff <i>repair</i>(<i>M</i>,η) is satisfiable. Furthermore, a satisfying assignment determines which states and transitions must be removed from <i>M</i> to yield a model <i>M</i><sup>′</sup> of η Thus, we can use any SAT solver to repair Kripke structures. Using a complete SAT solver yields a complete algorithm: it always finds a repair if one exists. We also show that CTL model repair is NP-complete. We extend the basic repair method in three directions: (1) the use of abstraction mappings, that is, repair a structure abstracted from <i>M</i> and then concretize the resulting repair to obtain a repair of <i>M</i>, (2) repair concurrent Kripke structures and concurrent programs: we use the pairwise method of Attie and Emerson to represent and repair the behavior of a concurrent program, as a set of “concurrent Kripke structures”, with only a quadratic increase in the size of the repair formula, and (3) repair hierarchical Kripke structures: we use a CTL formula to summarize the behavior of each “box,” and CTL deduction to relate the box formula with the overall specification.	abstraction layer;algorithm;boolean satisfiability problem;concurrent computing;graphical user interface;kripke semantics;kripke structure (model checking);np-completeness;natural deduction;solver;thinking outside the box	Paul C. Attie;Kinan Dak-Al-Bab;Mouhammad Sakr	2017	ACM Trans. Embedded Comput. Syst.	10.1145/3147426	maintenance engineering;time complexity;model checking;shared memory;programming;labeling theory;discrete mathematics;kripke structure;concurrent computing;temporal logic;computer science;theoretical computer science;mathematics;semantics;supervisory control;boolean satisfiability problem;programming language;computational model;algorithm;polynomial;satisfiability	Logic	-12.994945485532547	23.232085296579733	130655
859770f5358216b6a5a3c431cc0ac5d6de4ce67d	regulation control in interpreted petri nets using trace equivalence	petri nets automatic control control systems discrete event systems linear programming control system synthesis regulators manufacturing systems communication system traffic control traffic control;linear programming regulation petri nets discrete event systems;control problem;discrete event system;discrete event systems;linear programming;linear program;regulation;petri nets;petri net;specification system regulation control interpreted petri nets trace equivalence discrete event system linear programming	This paper is concerned with the control problem of discrete event systems (DES) when both, the system to be controlled (plant) and the required behavior (specification), are modeled by interpreted Petri nets (IPN). Trace equivalence is defined for IPN in order to determine when the behavior of a system is embedded into another one. Moreover, trace equivalence can be characterized by linear programming methods. This result is applied for building IPN regulator controllers, where the plant must track the imposed by a specification system.	embedded system;interplanet;linear programming;petri net;turing completeness	Jesus F. Sánchez-Blanco;Antonio Ramírez-Treviño;Alejandra Santoyo-Sanchez	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1399926	real-time computing;stochastic petri net;discrete event dynamic system;computer science;linear programming;control theory;process architecture;petri net	Robotics	-6.0685832417149665	28.4905325694755	130689
d8a036d2f4dcb36c3845747b2745400fa9995c86	computation for supremal simulation-based controllable and strong observable subautomata	observability;monotone operator;observability automata theory controllability lattice theory mathematical operators;control theory;lattice theory;electronic mail;complexity theory;lattices;controllability;nondeterministic systems;bisimulation;satisfiability;mathematical operators;automata;partial observation bisimulation lattice theory discrete event systems nondeterministic systems;observability lattices automata complexity theory educational institutions electronic mail controllability;discrete event systems;automata theory;partial observation;complete lattice;strong observable automaton supremal simulation based controllable subautomata strong observable subautomata lattice theory monotone operators simulation operator controllable operator strong observable operator complete lattice inequalities simulation based controllable set strong observable set supremal simulation based controllable subautomaton strong observable subautomaton supremal simulation based controllable automaton	In our previous work, we proposed simulation-based controllability and simulation-based observability as properties of the specification to guarantee the existence of a bisimilarity supervisor. However, a given specification may not satisfy these conditions. Then, a natural question is how to compute a feasible sub-specification. To answer this question, this paper investigates the computation of supremal simulation-based controllable and strong observable subautomata by using lattice theory. First, three monotone operators-simulation operator, controllable operator and strong observable operator are constructed upon a complete lattice. Based on these operators, inequalities are then formulated, whose solution is a simulation-based controllable and strong observable set. In particular, a sufficient condition is presented to guarantee the existence of a supremal simulation-based controllable and strong observable subautomaton. When such an existence condition holds, an algorithm is further proposed to compute the supremal simulation-based controllable and strong observable automaton.	algorithm;automata theory;automaton;bisimulation;computation;nondeterministic finite automaton;observable;simulation;monotone	Yajuan Sun;Hai Lin;Fuchun Liu	2012	Proceedings of the 31st Chinese Control Conference		mathematical optimization;combinatorics;discrete mathematics;lattice;control theory;mathematics	Logic	-7.084369649473058	25.814806903725007	130807
10724ed795a52cd1b31c591f742504278fba036e	proof pearl: regular expression equivalence and relation algebra	isabelle hol;decision procedure;regular expressions	We describe and verify an elegant equivalence checker for regular expressions. It works by constructing a bisimulation relation between (derivatives of) regular expressions. By mapping regular expressions to binary relations, an automatic and complete proof method for (in)equalities of binary relations over union, composition and (reflexive) transitive closure is obtained. The verification is carried out in the theorem prover Isabelle/HOL, yielding a practically useful decision procedure.	automata theory;automated theorem proving;bisimulation;decision problem;formal equivalence checking;hol (proof assistant);isabelle;kleene algebra;linear algebra;overhead (computing);regular expression;transitive closure;turing completeness	Alexander Krauss;Tobias Nipkow	2011	Journal of Automated Reasoning	10.1007/s10817-011-9223-4	discrete mathematics;computer science;mathematics;programming language;generalized star height problem;regular expression;algorithm	Logic	-14.380526651346477	20.52942938133722	130885
b2f9e07faea020594e92b2604c9eabd4665acf99	adjunction models for call-by-push-value with stacks	game semantics;operational semantics;denotational semantic;pattern matching;indexation;possible worlds	Call-by-push-value is a ”semantic machine code”, providing a set of simple primitives from which both the call-by-value and call-by-name paradigms are built. We present its operational semantics as a stack machine, suggesting a term judgement of stacks. We then see that CBPV, incorporating these stack terms, has a simple categorical semantics based on an adjunction between values and stacks. There are no coherence requirements. We describe this semantics incrementally. First, we introduce locally indexed categories and the opGrothendieck construction, and use these to give the basic structure for interpreting the three judgements: values, stacks and computations. Then we look at the universal property required to interpret each type constructor. We define a model to be a strong adjunction with countable coproducts, countable products and exponentials. We see a wide range of instances of this structure: we give examples for divergence, storage, erratic choice, continuations, possible worlds and games (with or without a bracketing condition), in each case resolving the strong monad from the literature into a strong adjunction. And we give ways of constructing models from other models. Finally, we see that call-by-value and call-by-name are interpreted within the Kleisli and co-Kleisli parts, respectively, of a call-by-push-value adjunction.	call-by-push-value;categorical logic;computation;continuation;machine code;monad (functional programming);operational semantics;possible world;requirement;stack machine;type constructor	Paul Blain Levy	2002	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80568-1	discrete mathematics;computer science;game semantics;pattern matching;mathematics;possible world;programming language;operational semantics;algorithm	PL	-11.342813546323947	18.328676203625715	130896
97871fa3474d7a0dd0a84138b6b58ebe7d9aa975	improving sat solver performance with structure-based preferential bumping		We present a method we call structure-based preferential bumping, a low-cost way to exploit formula structure in VSIDS-based SAT solvers. We show that the SAT solver Glucose, when modified with preferential bumping of certain easily identified structurally important variables, out-performs unmodified Glucose on the industrial formulas from recent SAT solver competitions.	boolean satisfiability problem;preferential entailment;solver	Sima Jamali;David Mitchell	2017			computational science;bumping;boolean satisfiability problem;computer science	AI	-14.893687284459288	23.93819841017564	131075
7ee37c116d5e6def1831772cf2265b168ea04766	leviathan: a new ltl satisfiability checking tool based on a one-pass tree-shaped tableau		The paper presents Leviathan, an LTL satisfiability checking tool based on a novel one-pass, treelike tableau system, which is way simpler than existing solutions. Despite the simplicity of the algorithm, the tool has performance comparable in speed and memory consumption with other tools on a number of standard benchmark sets, and, in various cases, it outperforms the other tableau-based tools.	algorithm;benchmark (computing);boolean satisfiability problem;embarrassingly parallel;heuristic (computer science);leviathan (cipher);long division;method of analytic tableaux;multi-core processor;preprocessor;software propagation;solver;test case;unit propagation	Matteo Bertello;Nicola Gigante;Angelo Montanari;Mark Reynolds	2016			discrete mathematics;mathematics;algorithm	AI	-14.716597400307146	25.117167091857084	131245
5ef5de674d62c5f6589f1bc2eda9374e01ec3f99	quantitative model checking of linear-time properties based on generalized possibility measures	model checking;linear temporal logic;fuzzy regular language;possibility theory;fuzzy finite automaton;generalized possibilistic kripke structure	Model checking of linear-time properties based on possibil ity measures was studied in previous work (Y. Li and L. Li, Model checking of linear -time properties based on possibility measure, IEEE Transactions on Fuzzy Sy stems, 21(5)(2013), 842-854). However, the linear-time properties considered in the previous work was classical and qualitative, possibility information of the systems was not considered at all. We shall study quantitative model checking o f fuzzy linear-time properties based on generalized possibility measures in th e paper. Both the model of the system, as well as the properties the system needs to ad here to, are described using possibility information to identify the uncertainty in the model/properties. The systems are modeled by generalized possibilistic Kripke structures (GPKS, in short), and the properties are described by fuzzy lineartime properties. Concretely, fuzzy linear-time properties about reachability , always reachability, constrain reachability, repeated reachability and persitenc e i GPKSs are introduced and studied. Fuzzy regular safety properties and fuzzy ω−regular properties in GPKSs are introduced, the verification of fuzzy regular safe ty properties and fuzzy ω−regular properties using fuzzy finite automata are thorough ly studied. It has been shown that the verification of fuzzy regular safet y properties and fuzzy ω−regular properties in a finite GPKS can be transformed into th e verification of (always) reachability properties and repeated reachabi lity (persistence) properties in the product GPKS introduced in this paper. Several ex mples are given to illustrate the methods presented in the paper.	automata theory;electronic paper;finite-state machine;kripke semantics;model checking;persistence (computer science);reachability;time complexity	Yongming Li	2017	Fuzzy Sets and Systems	10.1016/j.fss.2017.03.012	fuzzy logic;model checking;possibility theory;combinatorics;discrete mathematics;linear temporal logic;defuzzification;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy associative matrix;fuzzy set operations;algorithm	Logic	-11.443274609183218	22.292406781266802	131748
02a3d68abdd2480d4bbafa12e4acc425fb398a9e	program schemas with concurrency: execution time and hangups	high level languages;branch point;natural extension;automatic programming;set theoretic languages;program optimization;data structure choice;structural properties	"""A class of program schemas with concurrency is defined as a natural extension of the standard notion of sequential flow-chart-like schemas. The question is considered as to whether such a program schema may reach a premature termination (or """"hangup"""") for some interpretation. It is shown that in general this question is undecidable; however, it is shown to be decidable for the class of free program schemas. And an algorithm for testing this property is presented with an upper time bound that grows linearly with the size of the schema.Several structural properties are shown to be equivalent to the hangup-free conditon for free schemas. And we give a method for computing the expected execution time of a program schema if the expected frequencies of choices at the branch points are known."""	algorithm;branch (computer science);concurrency (computer science);concurrent computing;flowchart;run time (program lifecycle phase);termination analysis;undecidable problem	Bruce P. Lester	1975		10.1145/512976.512995	branch point;computer science;program optimization;database;programming language;high-level programming language;algorithm	DB	-16.163178523847375	21.757246764988228	131908
d3691255e747b0b14644569bf3a62d3a61a9b379	isar: an interactive system for algebraic implementation proofs	interactive system	1 Basic concepts Formal implementation notions are a necessary prerequisite for proving the correctness of software development steps. In order to be useful in practice formal implementation concepts should be supplied by appropriate proof methods and, even more important, by tools providing mechanical support for correctness proofs. In the following an interactive system, called ISAR, is described which provides an environment for proofs of algebraic implementation relations based on behavioural semantics of equational algebraic specifications. For the basic notions of algebraic specifications, such as signature Z, term algebra Wz(X), ground term algebra WZ etc. we refer to [2]. Then a behavioural specification SP = (Z, Obs, E) consists of a signature Z = (S, F), a subset Obs ~ S of observable sorts and a set E of ax/oms (here equations t = r with terms t, r e Wx(X)). The definition of our implementation concept is based on the assumption that from the software user's point of view a software product is a correct implementation if it satisfies the desired input/output behaviour. Hence a behavioural specification SP1 = (Z1, Obsl, El) is called behavioural implementation of SP = (Z, Obs, E) if SP1 respects the observable properties of SP. A precise formal definition of this intuitive notion on the model level is given in [4]. Since we are interested in automatic implementation proofs we will present here only the following proof theoretic characterization for behavioural implementations (cf. [4]) which is the theoretical basis of the ISAR-system. The characterization uses the notion of observable X-context which is any term C[Zs] of observable sort so e Obs over the signature Z of SP which contains a distinguished variable Zs of some sort s e S. The application of a context C[Zs] to a term t of sort s is defined by the substitution of Zs by t. It is denoted by c[t].	algebraic equation;correctness (computer science);input/output;interactivity;linear algebra;observable;software development;term algebra;theory;winzip	Bernhard Bauer;Rolf Hennicker	1992		10.1007/BFb0013084	computer science	Logic	-14.177691303186416	19.52802010888874	132372
b1f49c051b7027f9e9cba20849af4ca3388fa757	term graph narrowing		We introduce term graph narrowing as an approach for solving equations by transformations on term graphs. Term graph narrowing combines term graph rewriting with rst-order term uniication. Our main result is that this mechanism is complete for all term rewriting systems over which term graph rewriting is normalizing and connuent. This includes, in particular, all convergent term rewriting systems. Completeness means that for every solution of a given equation, term graph narrowing can nd a more general solution. The general motivation for using term graphs instead of terms is to improve eeciency: sharing common subterms saves space and avoids the repetition of computations.	abstract semantic graph;computation;equation solving;graph rewriting;naruto shippuden: clash of ninja revolution 3;rewrite (programming);source-to-source compiler;unification (computer science)	Annegret Habel;Detlef Plump	1996	Mathematical Structures in Computer Science		discrete mathematics;combinatorics;mathematics;abstract semantic graph	Logic	-14.746994918798926	20.899796748285222	132524
a6bac8004e6db4cfd7eda4a14d40255c2f5b973b	functional treewidth: bounding complexity in the presence of functional dependencies	modelizacion;base relacional dato;comptage;compilacion;formule cnf;interrogation base donnee;satisfiabilite;interrogacion base datos;relational database;satisfiability;constraint satisfaction;contaje;functional dependency;modelisation;satisfaction contrainte;formula cnf;complexity measure;dependance fonctionnelle;mesure complexite;non structural;counting;base donnee relationnelle;compilation;treewidth;anchura arbol;dependencia funcional;satisfaccion restriccion;conjunctive normal form;largeur arborescente;modeling;database query;medida complexidad;satisfactibilidad;functional dependence	Many reasoning problems in logic and constraint satisfaction have been shown to be exponential only in the treewidth of their interaction graph: a graph which captures the structural interactions among variables in a problem. It has long been observed in both logic and constraint satisfaction, however, that problems may be easy even when their treewidth is quite high. To bridge some of the gap between theoretical bounds and actual runtime, we propose a complexity parameter, called functional treewidth, which refines treewidth by being sensitive to non– structural aspects of a problem: functional dependencies in particular. This measure dominates treewidth and can be used to bound the size of CNF compilations, which permit a variety of queries in polytime, including clausal implication, existential quantification, and model counting. We present empirical results which show how the new measure can predict the complexity of certain benchmarks, that would have been considered quite difficult based on treewidth alone.	benchmark (computing);conjunctive normal form;constraint satisfaction;existential quantification;functional dependency;interaction;time complexity;treewidth	Yuliya Zabiyaka;Adnan Darwiche	2006		10.1007/11814948_14	conjunctive normal form;combinatorics;discrete mathematics;systems modeling;constraint satisfaction;relational database;computer science;mathematics;functional dependency;treewidth;counting;algorithm;tree decomposition;satisfiability	AI	-7.705640071273244	19.791670951804875	132559
706a6bb943dbf0223bfb69a67cac9f609b021caa	"""counterexamples to """"liveness-enforcing supervision of bounded ordinary petri nets using partial-order methods"""""""	unfolding;liveness enforcing supervision bounded ordinary petri nets partial order methods;deploiement;red petri;helium;vivacidad;despliegue;partial order methods;control system analysis petri nets;vivacite;system recovery;petri nets system recovery joining processes;bounded ordinary petri nets;liveness;joining processes;control system analysis;liveness enforcing supervision;petri nets;petri net;supervision;reseau petri;partial order	This note shows by means of simple counterexamples that some key results presented by He and Lemmon on the liveness verification and enforcing of Petri nets using unfolding are incorrect. As a result, the applicability of unfolding for Petri net supervision is still an open issue. 1 Introduction Recently, He and Lemmon have presented an original approach for the analysis and control of bounded Petri nets based on unfolding. This technique allows one to describe the set of reachable marking of a given net N by means of a finite occurrence net (called the unfolding of N) without the necessity of explicitly computing the reachability graph of N [4]. This often leads to significant computational advantages. In a series of papers [1-3] He and Lemmon have used unfolding for liveness verification and enforcing. We show in this note through a series of counterexamples that some key results of these papers are incorrect. As a result, although we still strongly believe that unfolding is an interesting and potentially fruitful technique for Petri net control, the applicability of unfolding for Petri net supervision is still an open issue. 2 Counterexamples for [1] Let us first consider the approach proposed in [1]. The unfolding βc used in this approach is an extension of the well-known McMillan unfolding [4] and includes cut-off transitions not preceding any other cut-off transitions. As an example, the unfolding βc of the net N1 in Figure 1.(a) is shown in part (b) of the same figure. Note that here we are conventionally using a double arrowed arc to represent two arcs with opposite directions. The approach proposed in [1] is based on three concepts: • deadlocked configurations; • cut graph; • cut cycles. (*) Paper to appear as a correspondence on the IEEE Transactions on Automatic Control, 2004.	automatic control;computation;cut (graph theory);emoticon;formal verification;item unique identification;liveness;petri net;reachability;scott r. lemmon;unfolding (dsp implementation);whole earth 'lectronic link	Xiaolan Xie;Alessandro Giua	2004	IEEE Trans. Automat. Contr.	10.1109/TAC.2004.831162	discrete mathematics;real-time computing;computer science;mathematics;distributed computing;petri net	Logic	-6.5013248286604775	26.73797418808809	132577
22a781923e43bac38b3cee89673c2a5635820ba3	improved method to generate path-wise test data	software testing;automated test data generation;test data generation;program slice;modeling language;automatic generation;dynamic data;data flow analysis;program slicing;data flow;program path;black box testing	Guptet al., proposed a method, which is referred to as the Iterative Relaxation Method, to generate test data for a given path in a program by linearizing the predicate functions. In this paper, a model language is presented and the properties of static and dynamic data dependencies are investigated. The notions in the Iterative Relaxation Method are defined formally. The predicate slice proposed by Guptaet al. is extended to path-wise static slice. The correctness of the constructional algorithm is proved afterward. The improvement shows that the constructions of predicate slice and input dependency set can be omitted. The equivalence of systems of constraints generated by both methods is proved. The prototype of path-wise test data generator is presented in this paper. The experiments show that our method is practical, and fits the path-wise automatic generation of test data for both white-box testing and black-box testing.	algorithm;black box;black-box testing;correctness (computer science);data dependency;dynamic data;experiment;fits;iterative method;linear programming relaxation;prototype;relaxation (iterative method);test data;turing completeness;white-box testing	Jinhui Shan;Ji Wang;Zhichang Qi;Jianping Wu	2003	Journal of Computer Science and Technology	10.1007/BF02948890	program slicing;computer science;theoretical computer science;operating system;database;programming language;algorithm	SE	-18.552054339151315	26.017443439525497	132647
5d0b8ba1879d678df867545ef6fa9d892015a976	the complexity of abduction for separated heap abstractions	research outputs;research publications	Abduction, the problem of discovering hypotheses that support a conclusion, has mainly been studied in the context of philosophical logic and Artificial Intelligence. Recently, it was used in a compositional program analysis based on separation logic that discovers (partial) pre/post specifications for un-annotated code which approximates memory requirements. Although promising practical results have been obtained, completeness issues and the computational hardness of the problem have not been studied. We consider a fragment of separation logic that is representative of applications in program analysis, and we study the complexity of searching for feasible solutions to abduction. We show that standard entailment is decidable in polynomial time, while abduction ranges from NP-complete to polynomial time for different subproblems.	abductive reasoning;artificial intelligence;computation;data structure;device driver;directed acyclic graph;erp;expect;linked list;microsoft research;np-completeness;np-hardness;p (complexity);polynomial;program analysis;requirement;separation logic;server (computing);slayer;source lines of code;time complexity	Nikos Gorogiannis;Max I. Kanovich;Peter W. O'Hearn	2011		10.1007/978-3-642-23702-7_7	computer science;artificial intelligence;theoretical computer science;programming language;algorithm	AI	-15.499116743027447	22.940965044237142	132682
92c328a5fec2604642582ddf88c367adabea86d9	computer-assisted verification of an algorithm for concurrent timestamps	verification;tool support;input output automata;input output;concurrent timestamps;validation and testing;uct;tools and tool support;larch	A formal representation and machine-checked proof are given for the Bounded Concurrent Timestamp (BCTS) algorithm of Dolev and Shavit. The proof uses invariant assertions and a forward simulation mapping to a corresponding Unbounded Concurrent Timestamp (UCTS) algorithm, following a strategy developed by Gawlick, Lynch, and Shavit. The proof was produced interactively, using the Larch Prover.	algorithm;interactivity;larch prover;simulation;unbounded nondeterminism	Tsvetomir P. Petrov;Anna Pogosyants;Stephen J. Garland;Victor Luchangco;Nancy A. Lynch	1996		10.1007/978-0-387-35079-0_2	input/output;real-time computing;verification;computer science;theoretical computer science;distributed computing	Logic	-17.4616898241255	26.655334524770705	132744
3ba2979df0feeb52c030f120025c2420a2c3e3bb	a generic modular data structure for proof attempts alternating on ideas and granularity	teoria demonstracion;proof assistant;grain size;gestion memoire;mathematics;theorie preuve;automatic proving;proof theory;maintenance;ingenierie connaissances;storage management;automatisation;demostracion automatica;program verification;automatizacion;demonstration automatique;gestion memoria;verificacion programa;grosor grano;matematicas;estructura datos;mantenimiento;structure donnee;verification programme;data structure;mathematiques;grosseur grain;knowledge engineering;automation	A practically useful mathematical assistant system requires the sophisticated combination of interaction and automation. Central in such a system is the proof data structure, which has to maintain the current proof state and which has to allow the flexible interplay of various components including the human user. We describe a parameterized proof data structure for the management of proofs, which includes our experience with the development of two proof assistants. It supports and bridges the gap between abstract level proof explanation and low-level proof verification. The proof data structure enables, in particular, the flexible handling of lemmas, the maintenance of different proof alternatives, and the representation of different granularities of proof attempts.	automated proof checking;data structure;high- and low-level;proof assistant	Serge Autexier;Christoph Benzmüller;Dominik Dietrich;Andreas Meier;Claus-Peter Wirth	2005		10.1007/11618027_9	data structure;computer science;artificial intelligence;automation;knowledge engineering;proof theory;mathematics;proof assistant;programming language;proof of concept;proof complexity;algorithm;grain size	PL	-19.106507149217183	21.562439525684017	132777
31af9dced770e9d0301009cf3ef6b3d2728dde8e	specification of reduction strategies in term rewriting systems	algebraic specification;formal specification;programming language;computer model;term rewrite system;formal method;graph rewriting;rewrite systems;parallel implementation;abstract interpretation;part of book or chapter of book;functional language	There is a growing interest in Term Rewriting Systems (TRS's), which are used as a conceptual basis for new programming languages such as functional languages and algebraic specification languages. TRS's serve as a computational model for (parallel) implementations of these languages. They also form the foundation for a calculus for Graph Rewriting Systems (GRS's). In Rewriting Systems reduction strategies play an important role because they control the actual rewriting process. Strategies determine the order of the rewriting and the rules to apply. Hence they have a great influence on the efficiency and the amount of parallelism in the computation. In ambiguous or non-deterministic TRS's, they even influence the outcome of the computation. Some of the reduction strategies used in TRS's are extremely complex algorithms. Unfortunately, there is no common formal specification method for reduction strategies yet.  In this paper three formal methods for specifying reduction strategies in TRS's are presented. In the first method the reduction strategy is encoded in the TRS itself. The original TRS is transformed to a so called annotation TRS in which the strategy is encoded using functions. This annotation TRS itself may use any normalizing reduction strategy. Unfortunately, compared with the number of rules of the original TRS, the annotation TRS may contain an exponential number of additional rules. In the second method this drawback is prevented, simply by using a priority TRS as annotation TRS. The desire to specify a strategy uniformly for all TRS's leads to the third method. A new TRS system is introduced that uses two basic primitives for matching and rewriting and that is build out of three separate TRS's. The use of this abstract-interpretation TRS is shown to be the most promising method.    	rewriting	Marko C. J. D. van Eekelen;Marinus J. Plasmeijer	1986		10.1007/3-540-18420-1_57	formal methods;object language;specification language;computer science;theoretical computer science;formal specification;programming language;programming language specification;confluence;algorithm;language of temporal ordering specification;graph rewriting	Logic	-16.063605969192157	22.09019998525437	132791
fac319a34a9a1b93cb772d4cdb42cdb8741f2edc	optimal implementation of conjunctive queries in relational data bases	relational data;conjunctive queries;relational database;finite automata;matrix multiplication	We define the class of conjunctive queries in relational data bases, and the generalized join operator on relations. The generalized join plays an important part in answering conjunctive queries, and it can be implemented using matrix multiplication. It is shown that while answering conjunctive queries is NP complete (general queries are PSPACE complete), one can find an implementation that is within a constant of optimal. The main lemma used to show this is that each conjunctive query has a unique minimal equivalent query (much like minimal finite automata).	automata theory;conjunctive query;database;finite-state machine;matrix multiplication;np-completeness;pspace;pspace-complete	Ashok K. Chandra;Philip M. Merlin	1977		10.1145/800105.803397	discrete mathematics;boolean conjunctive query;relational calculus;relational database;computer science;theoretical computer science;database;mathematics;conjunctive query;finite-state machine	DB	-6.581959809919836	18.348374448688602	132936
43dda151f247f98e8364ea638793deafb2eff8dc	probabilistic constraint handling rules	simulated annealing;constraint handling rules;lines of code	Classical Constraint Handling Rules (CHR) provide a powerful tool for specifying and implementing constraint solvers and programs. The rules of CHR rewrite constraints (non-deterministically) into simpler ones until they are solved. In this paper we introduce an extension of Constraint Handling Rules (CHR), namely Probabilistic CHRs (PCHR). These allow the probabilistic “weighting” of rules, specifying the probability of their application. In this way we are able to formalise various randomised algorithms such as for example Simulated Annealing. The implementation is based on source-to-source transformation (STS). Using a recently developed prototype for STS for CHR, we could implement probabilistic CHR in a concise way with a few lines of code in less than one hour.	algorithm;best, worst and average case;constraint (mathematics);constraint handling rules;constraint satisfaction problem;ergodicity;fairness measure;iteration;mathematical optimization;probabilistic database;prototype;randomized algorithm;rewrite (programming);rewriting;simulated annealing;solver;source lines of code;source transformation;time complexity	Thom W. Frühwirth;Alessandra Di Pierro;Herbert Wiklicky	2002	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80789-8	simulated annealing;computer science;theoretical computer science;machine learning;programming language;source lines of code;algorithm	AI	-16.033704468282494	24.24597951139828	133108
951f37259bc0a786fe8f2f9c1d84cd3c51ac08a4	control of continuous petri nets using on/off based method		Continuous Petri Nets (CPN) can be used to approximate classical discrete Petri nets which suffer from the state explosion problem. In this paper we focus on the control of timed CPN (TCPN), aiming to drive the system from an initial state to a desired final one. This problem is similar to the set-point control problem in a general continuous-state system. In a previous work, a simple and efficient ON/OFF controller was proposed for structurally persistent nets, and it is proved to be minimum-time. In this work the ON/OFF controller is extended to general TCPN, but in this case, the minimum-time evolution is not guaranteed. Three extensions are proposed, all of them are based on the ON/OFF strategy. Some comparisons of those controllers are given in terms of their applications to an assembly system.	approximation algorithm;coloured petri net;computational complexity theory;continuous integration;timed automaton	Liewei Wang;Cristian Mahulea;Jorge Júlvez;Manuel Silva Suárez	2012		10.3182/20121003-3-MX-4033.00011	control engineering;real-time computing;computer science;control theory;process architecture;petri net	Robotics	-8.562381089657979	29.067244939906487	133331
28376555e455e6758e72e63beee1aef451a17a1c	efficient real-time scheduling of integrated equipment in semiconductor fabrication	matrix representation;supervisory control;deadlock prone modules real time systems scheduling semiconductor fabrication automation supervisory control system machine failures deadlock management resource request matrix polynomial complexity;computational complexity computer aided production planning semiconductor device manufacture real time systems management production control matrix algebra;polynomial complexity;matrix algebra;scheduling algorithm;production control;computational complexity;real time scheduling;semiconductor device manufacture;computer aided production planning;computational efficiency;management;fabrication system recovery processor scheduling automation supervisory control computational efficiency scheduling algorithm matrix decomposition resource management polynomials;real time systems	This paper presents an efficient approach to real-time scheduling of integrated equipment in semiconductor fabrication. To accomplish high level of automation and flexibility, the supervisory control system of an integrated equipment should comprehend management of machine failures and deadlock while optimizing equipment utilization. This paper proposes a real-time scheduling approach that can perform scheduling and deadlock management in efficient way in spite of machine failures. First, we present a novel framework where we decompose deadlock management problem into sub-problems for computational efficiency, and then integrate them with scheduling algorithms. To realize the proposed framework, a Resource Request Matrix representation is introduced to identify deadlock-prone modules, and to apply deadlock management algorithms. We also present algorithms of polynomial complexity to identify deadlock-prone modules, and algorithms to manage deadlock considering machine failures.	algorithm;control system;deadlock;high-level programming language;matrix representation;real-time locating system;scheduling (computing);semiconductor device fabrication;time complexity	Hyun Joong Yoon;Doo Yong Lee	2002		10.1109/ROBOT.2002.1014272	real-time computing;matrix representation;computer science;distributed computing;supervisory control;computational complexity theory;scheduling;deadlock prevention algorithms	Embedded	-6.196917296940222	29.482359008809738	133483
003087b075d03912bc1f1083f69cedd38d20f477	invariant generation for parametrized systems using self-reflection - (extended version)	central challenge;inductive invariants;off-the-shelf invariant generator;candidate invariants;reflective abstraction;inferring invariants;important correspondence;iterative invariant generation procedure;invariant inference;parametrized system	We examine the problem of inferring invariants for parametrized systems. Parametrized systems are concurrent systems consisting of an a priori unbounded number of process instances running the same program. Such systems are commonly encountered in many situations including device drivers, distributed systems, and robotic swarms. In this paper we describe a technique that enables leveraging off-the-shelf invariant generators designed for sequential programs to infer invariants of parametrized systems. The central challenge in invariant inference for parametrized systems is that naı̈vely exploding the transition system with all interleavings is not just impractical but impossible. In our approach, the key enabler is the notion of a reflective abstraction that we prove has an important correspondence with inductive invariants. This correspondence naturally gives rise to an iterative invariant generation procedure that alternates between computing candidate invariants and creating reflective abstractions.	concurrency (computer science);device driver;distributed computing;invariant (computer science);iterative method;robot;self-reflection;transition system	Alejandro Sánchez;Sriram Sankaranarayanan;César Sánchez;Bor-Yuh Evan Chang	2012		10.1007/978-3-642-33125-1_12	theoretical computer science;algorithm	PL	-14.506114687676584	26.833335971422077	133603
01659b154b53c8007660416762f77668c296c2b1	when is containment decidable for probabilistic automata?		The containment problem for quantitative automata is the natural quantitative generalisation of the classical language inclusion problem for Boolean automata. We study it for probabilistic automata, where it is known to be undecidable in general. We restrict our study to the class of probabilistic automata with bounded ambiguity. There, we show decidability (subject to Schanuel’s conjecture) when one of the automata is assumed to be unambiguous while the other one is allowed to be finitely ambiguous. Furthermore, we show that this is close to the most general decidable fragment of this problem by proving that it is already undecidable if one of the automata is allowed to be linearly ambiguous. 2012 ACM Subject Classification Theory of computation→ Quantitative automata, Theory of computation → Probabilistic computation	ambiguous grammar;automata theory;probabilistic automaton;theory of computation;undecidable problem	Laure Daviaud;Marcin Jurdzinski;Ranko Lazic;Filip Mazowiecki;Guillermo A. Pérez;James Worrell	2018		10.4230/LIPIcs.ICALP.2018.121	conjecture;discrete mathematics;probabilistic automaton;ambiguity;undecidable problem;bounded function;decidability;automaton;computer science;generalization	Logic	-5.819223969402833	19.858607699414886	133802
541b002a9d2dbab6ae8b2c1f53e6bb122c07d418	a hoare calculus for graph programs	operational semantics;inference rule	We present Hoare-style axiom schemata and inference rules for verifying the partial correctness of programs in the graph programming language GP. The preand postconditions of this calculus are the nested conditions of Habel, Pennemann and Rensink, extended with expressions for labels in order to deal with GP’s conditional rule schemata and infinite label alphabet. We show that the proof rules are sound with respect to GP’s operational semantics.	branch (computer science);correctness (computer science);graph rewriting;hoare logic;operational semantics;postcondition;programming language;semantics (computer science);subroutine;termination analysis;transformation language;verification and validation	Christopher M. Poskitt;Detlef Plump	2010		10.1007/978-3-642-15928-2_10	combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics;programming language;operational semantics;algorithm;rule of inference	PL	-12.880861211470922	21.560832396108324	133820
5e527c2c11dd1a1296d13dfb1f7801095d4511f0	pvs#: streamlined tacticals for pvs	tacticals;proof languages;monads;pvs;theorem prover;proof search;point of view;tactics;strategies	The semantics of a proof language relies on the representation of the state of a proof after a logical rule has been applied. This information, which is usually meaningless from a logical point of view, is fundamental to describe the control mechanism of the proof search provided by the language. In this paper, we propose a datatype, called proof monad, to represent the state information of a proof and we illustrate its use in PVS. Furthermore, we show how this representation can be used to design a new set of powerful PVS tacticals that have simple and clear semantics. The implementation of these tacticals is called PVS#.	automated theorem proving;data structure;exception handling;formal system;monad (functional programming);proof assistant;prototype;synthetic intelligence	Florent Kirchner;César A. Muñoz	2007	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2006.10.057	discrete mathematics;strategy;computer science;mathematics;automated theorem proving;programming language;monad;algorithm	PL	-17.494211809345064	20.899146363135653	133845
cb618f72802b5eed91b526ff4e2860f12b5417aa	"""optimization of lr(k) """"reduced parsers"""""""	lenguaje programacion;optimisation;reduced tables;optimizacion;programming language;metodo reduccion;lr k parsers;canonical form;forme canonique;sistema informatico;lr k grammars;tablero;formal languages;computer system;tableau;parsing;grammaire lr;gramatica lr;forma canonica;analizador sintaxico;langage programmation;array;methode reduction;optimization;systeme informatique;parser;lr grammar;reduction method;analyseur syntaxique;lenguaje formal;formal language;parsing tables;langage formel	Abstract   Some optimization algorithms for LR( k ) tables presented by A.V. Aho and J.D. Ullman ( J. Comput. System Sci.  6 (1972) 573-602) are extended to a new type of parsing tables, called  Reduced Tables . The corresponding algorithms are compared and a sufficient condition which guarantees a lower space size of the new tables with respect to the canonical LR tables is introduced. The analysis of the grammar of some programming languages confirm the convenience of this approach.	lr parser;parsing	Massimo Ancona;Claudia Fassino;Vittoria Gianuzzi	1992	Inf. Process. Lett.	10.1016/0020-0190(92)90074-6	formal language;speech recognition;canonical lr parser;computer science;parsing;programming language;algorithm	DB	-5.817105542665633	20.213208948422267	133917
182d9f49c45a27e5e38f330bb825257da722e0a3	generalized, efficient array decision procedures	silicon;hardware verification;probability density function;software verification;satisfiability modulo theory;satisfiability modulo theory efficient array decision procedure hardware verification software verification symbolic analysis basic array theory combinatory array logic smt solver z3;satisfiability;data mining;theorem proving;inference rule;arrays;indexes;logic arrays arithmetic surface mount technology equations hardware constraint theory formal verification delay automata filters;finite domain;decision procedure;smt solver z3;decision theory;formal logic;mathematical model;basic array theory;combinatory array logic;symbolic analysis;efficient array decision procedure;context;theorem proving decision theory formal logic	The theory of arrays is ubiquitous in the context of software and hardware verification and symbolic analysis. The basic array theory was introduced by McCarthy and allows to symbolically representing array updates. In this paper we present combinatory array logic, CAL, using a small, but powerful core of combinators, and reduce it to the theory of uninterpreted functions. CAL allows expressing properties that go well beyond the basic array theory. We provide a new efficient decision procedure for the base theory as well as CAL. The efficient procedure serves a critical role in the performance of the state-of-the-art SMT solver Z3 on array formulas from applications.	cal;combinatory logic;computational complexity theory;decision problem;experiment;phigs;satisfiability modulo theories;set theory;solver;z3 (computer)	Leonardo Mendonça de Moura;Nikolaj Bjørner	2009	2009 Formal Methods in Computer-Aided Design	10.1109/FMCAD.2009.5351142	database index;probability density function;decision theory;software verification;computer science;theoretical computer science;mathematical model;automated theorem proving;symbolic data analysis;silicon;programming language;logic;algorithm;rule of inference;satisfiability	EDA	-12.943090422475796	27.36156630579727	133966
2b316501f56e95a7e11e630f915c9d14aaa53593	time processes for time petri-nets	asynchronous automaton;automate asynchrone;red petri;automata asincrono;time petri net;informatique theorique;systeme concurrent;timing analysis;petri net;reseau petri;computer theory;partial order;informatica teorica	Time Petri nets are Petri nets extended with a notion of time, where the occurrence time of a transition is constrained by a static interval. The objective of this work is to give time Petri nets a partial order semantics, based on the nonsequential processes semantics for untimed net systems. A time process of a time Petri net is deened as a traditionally constructed causal process that has a valid timing. This means that the events of the process are labeled with occurrence times which must satisfy speciic validness criteria. These criteria are obtained by analyzing how the timing constraints interact with the causal ordering of the events in the net. An eecient algorithm for checking then validness of a given timing is sketched. Interleavings of the time processes are deened as linearizations of the causal partial order of events where also the temporal ordering of events is preserved. The relationship between the ring schedules of a time Petri net and the interleavings of the time processes of the net is shown to be bijective. Also, a suucient condition is given for when the invalidity of timings for a process can be inferred from an initial subprocess. An alternative characterization for the validness of timings then results in an algorithm for constructing the set of all valid timings for a process. This set of all valid timings is presented as sets of alternative linear constraints, from which the existence of a valid timing can be decided.	algorithm;causal filter;child process;petri net	Tuomas Aura;Johan Lilius	1997		10.1007/3-540-63139-9_34	partially ordered set;stochastic petri net;computer science;artificial intelligence;process architecture;petri net;static timing analysis;algorithm	Logic	-8.731663551158132	24.15328562199244	134060
7f6a30e59f094463bd32c3f5158eb47a783aa63d	solving constraint satisfaction problems with sat modulo theories	csp computer program language;constraint programming computer science;solvers and tools;info eu repo semantics article;programacio per restriccions informatica;constraint programming;reformulation;computer algorithms;algorismes computacionals;satisfiability modulo theories;csp llenguatge de programacio	Due to significant advances in SAT technology in the last years, its use for solving constraint satisfaction problems has been gaining wide acceptance. Solvers for satisfiability modulo theories (SMT) generalize SAT solving by adding the ability to handle arithmetic and other theories. Although there are results pointing out the adequacy of SMT solvers for solving CSPs, there are no available tools to extensively explore such adequacy. For this reason, in this paper we introduce a tool for translating FLATZINC (MINIZINC intermediate code) instances of CSPs to the standard SMT-LIB language. We provide extensive performance comparisons between state-of-the-art SMT solvers and most of the available FLATZINC solvers on standard FLATZINC problems. The obtained results suggest that state-of-the-art SMT solvers can be effectively used to solve CSPs.	backjumping;bitwise operation;black box;boolean satisfiability problem;central processing unit;constraint programming;constraint satisfaction problem;cryptographic service provider;decision problem;existential quantification;gecode;heuristic (computer science);lazy evaluation;linear search;mathematical optimization;modulo operation;problem domain;satisfiability modulo theories;scheduling (computing);simultaneous multithreading;software propagation;solver;theory;unit propagation	Miquel Bofill;Miquel Palahí;Josep Suy;Mateu Villaret	2012	Constraints	10.1007/s10601-012-9123-1	mathematical optimization;constraint programming;computer science;theoretical computer science;mathematics;programming language;satisfiability modulo theories;algorithm	AI	-15.231714235650164	24.59173848127616	134265
0cb236f73cf97d23c4c4ac107f2ca78116931f85	generating path conditions for timed systems	semiautomatique;timed systems;software testing;semiautomatico;total order;formal specification;sistema temporizado;unit testing;real time;metodo formal;timed system;methode formelle;program verification;satisfiability;automatic generation;generacion automatica prueba;methode calcul;formal method;metodo calculo;verificacion programa;temps reel;semiautomatic;generation automatique test;systeme temporise;automatic test generation;tiempo real;verification programme;computing method;partial order;time constraint	We provide an automatic method for calculating the path condition for programs with real time constraints. This method can be used for the semiautomatic verification of a unit of code in isolation, i.e., without providing the exact values of parameters with which it is called. Our method can also be used for the automatic generation of test cases for unit testing. The current generalization of the calculation of path condition for the timed case turns out to be quite tricky, since not only the selected path contributes to the path condition, but also the timing constraints of alternative choices in the code.	test case;unit testing	Saddek Bensalem;Doron A. Peled;Hongyang Qu;Stavros Tripakis	2005		10.1007/11589976_2	partially ordered set;real-time computing;formal methods;fast path;computer science;artificial intelligence;formal specification;software testing;unit testing;programming language;total order;algorithm;satisfiability	SE	-16.718308648318974	27.80658126005642	134314
bef57d68264d11db2ff870902cc82264e4a406e8	stochastic extension for real time process algebra with urgency executing policy	process algebra;real time;stochastic extension;urgency;exponential distribution;data mining;computer science;algebra;servers;stochastic processes;probabilistic logic;real time systems	Real time systems have a natural executing policy of urgency. However, real time process algebras of nowadays cannot specify this basic executing policy which limits their expressiveness. There is only one default policy called “maximal progress” in process algebras which is not enough to specify the behaviors of real time systems. Based on this, we propose a real time process algebra with urgency executing policy which can specify the behaviors of real time systems.	bisimulation;maximal set;operational semantics;pepa;process calculus;real-time computing;theory;timer;weak ai	Wenbo Chen;Guang Zheng;Lian Li;Jinzhao Wu	2009	2009 International Conference on Computational Intelligence and Software Engineering		exponential distribution;stochastic process;process calculus;discrete mathematics;computer science;theoretical computer science;mathematics;distributed computing;probabilistic logic;server;statistics	Embedded	-10.441955683587626	25.950973007945993	134335
02e1f5d633ffcac546d6289eeafcf441fd7657f5	on the expressive power of movement and restriction in pure mobile ambients	communication process;movilidad;process calculi;eliminacion;mobility;mobile ambients;mobilite;definicion;proceso comunicacion;expressive power;process calculus;processus communication;definition;informatique theorique;decidibilidad;elimination;decidabilite;decidability;computer theory;informatica teorica	Pure mobile ambients is a process calculus suitable to focus on issues related to mobility, abstracting away from aspects concerning process communication. However, it incorporates name restriction (i.e. the (vn) binder) and ambient movement (i.e. the in and out capabilities) that can be seen as characteristics adapted, or directly borrowed, from the tradition of communication-based process calculi. For this reason, we retain that it is worth to investigate whether or not these features can be removed from pure mobile ambients without losing expressive power.To this aim, we consider two variants of pure mobile ambients which differ in the way infinite processes can be defined; the former exploits process replication, while the latter is more general and permits recursive process definition. We analyse whether or not the elimination of ambient movement and/or name restriction reduces the expressive power of these two calculi, using the decidability of process termination as a yardstick. We prove that name restriction can be removed from both calculi without reducing the expressive power. On the other hand, the elimination of both ambient movement and name restriction strictly reduces the expressive power of both calculi. As far as the elimination of only ambient movement is concerned, we prove an interesting discrimination result: process termination is undecidable under recursive process definition, while it turns out to be decidable under process replication.	ambient calculus	Nadia Busi;Gianluigi Zavattaro	2004	Theor. Comput. Sci.	10.1016/j.tcs.2003.10.040	process calculus;computer science;artificial intelligence;mathematics;programming language;mobile computing;algorithm	ECom	-9.079186709145123	21.33372899912368	134498
b38d621a72ff5d57f9fbfdc0482c7897f81a3eb5	hasta-la-vista: termination analyser for logic programs	program verification;termination analysis;constraint solving;logic programs;system architecture;type inference	Verifying termination is often considered as one of the most important aspects of program verification. Logic languages, allowing us to program declaratively, increase the danger of non-termination. Therefore, termination analysis received considerable attention in logic programming (see e.g. [7, 8, 10, 16]). Unfortunately, the majority of existing termination analysers, such as TermiLog [15], TerminWeb [7], and cTI [16] are restricted to pure logic programs and thus, leave many interesting real-world examples out of consideration. Therefore, in order to abridge the gap between programming practice and existing termination analysers real-world programming techniques should be considered. In this paper we present Hasta-La-Vista—a powerful tool for analysing termination of logic programs with integer computations. Hasta-La-Vista extends the constraints-based approach of Decorte et al. [9] by integrating the inference algorithm of [19]. Moreover, as explained in [19], in the integer case our approach is not limited to proving termination, but can also infer termination, i.e., find the set of queries terminating for a given program. System architecture. Conceptually, Hasta-La-Vista consists of three main parts: transformation, constraints generation and constraints solving. As a preliminary step, given a program and a set of atomic queries, type analysis of Janssens and Bruynooghe [14] computes the call set. We opted for a very simple type inference technique that provides us only with information whether some argument is integer or not. Based on the results of the type analysis the system decides whether termination of the given program can be dependent on the integer computation. In this case, the adorning transformation is applied [19]. The aim of the transformation is to discover bounded integer arguments and to make the bounds explicit. Intuitively, if a variable x is known to be bounded from above by n, then n x is always positive and thus, can be used as a basis for a definition of a levelmapping (a function from the set of atoms to the naturals). In order to prove termination we have to show that the level-mapping decreases while traversing a clause. This requirement can be translated into a set of constraints. Finally, this set of constraints is solved and depending on the solution termination is	abstract rewriting system;algorithm;basis (linear algebra);computation;constraint (mathematics);declarative programming;divergence (computer science);formal verification;hasta la vista, baby;linear algebra;logic programming;software design pattern;systems architecture;termination analysis;type inference	Alexander Serebrenik;Danny De Schreye	2003			computer science;theoretical computer science;programming language;algorithm	Logic	-17.133356519935234	22.73922398211593	134522
62ba27581bc60c882cc5e4ae194ef77c6f3bf5aa	system modeling and verification with uclid	uclid modeling language;microprocessors;protocols;computability formal verification simulation languages;floating point unit;sat solver system modeling formal verification infinite state systems uclid modeling language bit level model checker integer state variable function state variable propositional logic binary decision diagrams boolean satisfiability;boolean functions;system modeling;model system;computability;logic;buffer storage;power system modeling logic boolean functions formal verification microprocessors data structures hardware protocols engines buffer storage;instruction set architecture;modeling language;out of order;boolean satisfiability;infinite state systems;theorem prover;out of order execution;formal verification;binary decision diagrams;cache coherence protocol;engines;decision procedure;propositional logic;data structures;bit level model checker;cache coherence;industrial application;hardware design;simulation languages;functional unit;state explosion;sat solver;power system modeling;predicate abstraction;symbolic model checking;integer state variable;first order logic;embedded software;function state variable;hardware;binary decision diagram	Formal verification has had a significant impact on the semiconductor industry, particularly for companies that can devote significant resources to creating and deploying internally developed verification tools. If we look more closely, however, we see that the major industrial applications of formal verification have been either in verifying individual blocks, such as floating-point units and memories, or in verifying an abstracted representation of some aspect of the system, such as a cache coherence protocol. Attempting to verify overall system correctness is beyond the reach of current tools. For example, these tools are not capable of verifying that an out-of-order execution microprocessor correctly replicates the behavior of its sequential instruction set architecture (ISA) model. Most existing verification tools model system operation at a detailed bit level. Using powerful inference engines, such as Binary Decision Diagrams (BDDs) and Boolean satisfiability (SAT) checkers, symbolic model checkers [3, 5] and similar tools can analyze all possible behaviors of very large, finite-state systems. Modeling a system at the bit level makes it difficult to scale formal verification to systems that store and manipulate large amounts of data, such as microprocessors and many forms of embedded software. The many bits of state held in the various memories, queues, and caches lead to state explosion problems that overwhelm even the most advanced model checkers. Taking a cue from the hardware design principle of separating data from control, we believe that systems should be modeled and verified using a more abstract representation of data. If we assume individual functional units, such as ALUs and instruction decoders can be verified separately, then there is no need to track the value of every bit in the system. Instead, we can represent words of data as symbolic term values that are generated by function units, communicated among different subunits, and stored in different buffers and memories. For some term value x, we need not keep track of its bit width, its encoding, or even its actual value. With this term-level abstraction of data, verification can focus on the behavior of the control logic. Term-level abstraction has long been used by researchers using automatic theorem provers to verify hardware design [10]. Burch and Dill [4] were among the first to demonstrate that highly automated tools based on term-level models could be used to verify pipelined microprocessors. Rather than using Boolean inference engines, these tools make use of decision procedures for highly restricted subsets of firstorder logic. Over the years, these procedures have improved greatly in their speed [11] and the richness of the logic they can handle [1, 6]. We have developed UCLID [2], a prototype verifier for infinite-state systems. The UCLID modeling language extends that of SMV, a bit-level model checker, to include integer and function state variables, addition by constants, and relational operations. The underlying logic is expressive enough to model a wide range of systems, but it still permits a decision procedure where we transform the formula into propositional logic and then use either BDDs or a SAT solver. Most recently, we have developed powerful predicate abstraction methods that can automatically generate and prove system invariants using techniques similar those used in symbolic model checking [8]. UCLID has been used to verify a variety of hardware designs, including out-of-order microprocessors [7] and cache coherency protocols, as well as abstract synchronization protocols such as Lamport’s Bakery algorithm [9].	arithmetic logic unit;automated theorem proving;binary decision diagram;bit-level parallelism;boolean satisfiability problem;cache coherence;correctness (computer science);decision problem;embedded software;embedded system;formal verification;inference engine;lamport's bakery algorithm;microprocessor;model checking;modeling language;out-of-order execution;predicate abstraction;propositional calculus;prototype;semiconductor industry;solver;systems modeling;uclid;verification and validation	Randal E. Bryant	2004		10.1109/MEMCOD.2004.1459805	data structure;computer science;out-of-order execution;theoretical computer science;operating system;boolean satisfiability problem;programming language;algorithm	Logic	-15.059327291793105	30.391415783178417	134585
6aca6bb8e56dd79817ee6e78e4d31df09109a38a	incremental re-encoding for symbolic traversal of product machines		State space exploration of nite state machines is used to prove properties. The three paradigms for exploring reachable states, forward traversal, backward traversal and a combination of the two, reach their limits on large practical examples. Approximate techniques and combinational veri cation are far less expensive but these imply su cient, not strictly necessary conditions. Extending the applicability of the purely combinational check can be achieved through state minimization, partitioning, and re-encoding the FSMs to factor out their differences. This paper focuses on re-encoding presenting an incremental approach to re-encoding for sequential veri cation. Experimental results demonstrate the e ectiveness of this solution on medium{large circuits where other techniques may fail.	combinational logic;state (computer science);state space	Gianpiero Cabodi;Paolo Camurati;Luciano Lavagno;Stefano Quer;Robert K. Brayton;Ellen Sentovich	1996			discrete mathematics;shape;computer science;state space;theoretical computer science;space exploration;mathematics;finite-state machine;logic;algorithm	Logic	-13.95223545573393	26.615880365785316	134729
0124dc671fb7c36efc5162230d3b59f26d2e4051	formal verification of multi-paxos for distributed consensus		This paper describes formal specification and verification of Lamport’s Multi-Paxos algorithm for distributed consensus. The specification is written in TLA+, Lamport’s Temporal Logic of Actions. The proof is written and checked using TLAPS, a proof system for TLA+. Building on Lamport, Merz, and Doligez’s specification and proof for Basic Paxos, we aim to facilitate the understanding of Multi-Paxos and its proof by minimizing the difference from those for Basic Paxos, and to demonstrate a general way of proving other variants of Paxos and other sophisticated distributed algorithms. We also discuss our general strategies for proving properties about sets and tuples that helped the proof check succeed in significantly reduced time.	consensus (computer science);distributed algorithm;formal specification;formal verification;paxos (computer science);proof calculus;tla+;temporal logic of actions	Saksham Chand;Yanhong A. Liu;Scott D. Stoller	2016		10.1007/978-3-319-48989-6_8	lamport timestamps;paxos;computer science;theoretical computer science;lamport's bakery algorithm;distributed computing;programming language;algorithm	PL	-17.557480194692655	26.18517984368025	134825
349a53403af41bce13f0b5fef369e8f30b1b312d	revisiting parametricity: inductives and uniformity of propositions		Reynold’s parametricity theory captures the property that parametrically polymorphic functions behave uniformly: they produce related results on related instantiations. In dependently typed programming languages, such relations and uniformity proofs can be expressed internally, and generated as a program translation. We present a new parametricity translation for a significant fragment of Coq. Previous translations of parametrically polymorphic propositions allowed non-uniformity. For example, on related instantiations, a function may return propositions that are logically inequivalent (e.g. True and False). We show that uniformity of polymorphic propositions is not achievable in general. Nevertheless, our translation produces proofs that the two propositions are logically equivalent and also that any two proofs of those propositions are related. This is achieved at the cost of potentially requiring more assumptions on the instantiations, requiring them to be isomorphic in the worst case. Our translation augments the previous one for Coq by carrying and compositionally building extra proofs about parametricity relations. It is made easier by a new method for translating inductive types and pattern matching. The new method builds upon and generalizes previous such translations for dependently typed programming languages. Using reification and reflection, we have implemented our translation as Coq programs1. We obtain several stronger free theorems applicable to an ongoing compiler-correctness project. Previously, proofs of some of these theorems took several hours to finish.	best, worst and average case;circuit complexity;compiler correctness;coq (software);correctness (computer science);dependent type;graph isomorphism;inductive type;parametric polymorphism;parametricity;pattern matching;programming language;reification (knowledge representation)	Abhishek Anand;J. Gregory Morrisett	2017	CoRR		discrete mathematics;computer science;artificial intelligence;mathematics;algorithm	PL	-16.84491507182338	22.74261861929715	134879
370f4fd7816606c5f77063aaadd8cb4391b1e739	capturing complexity classes by fragments of second-order logic	second order;base relacional dato;temps polynomial;relacion orden;sistema informatico;clase complejidad;proposition horn;orden 2;ordering;computer system;logique propositionnelle;relational database;theorie point fixe;relation ordre;classe complexite;fermeture transitive;complexity class;propositional logic;logique ordre 1;polynomial time;base donnee relationnelle;transitive closure;systeme informatique;ordre 2;logica proposicional;first order logic;cierre transitivo;logica orden 1;tiempo polinomial	We investigate the expressive power of certain fragments of second-order logic on finite structures. The fragments are second-order Horn logic, second-order Krom logic as well as a symmetric and a deterministic version of the latter. It is shown that all these logics collapse to their existential fragments. In the presence of a successor relation they provide characterizations of polynomial time, deterministic and nondeterministic logspace and of the complement of symmetric logspace. Without successor relation these logics still can express certain problems that are complete in the corresponding complexity classes, but on the other hand they are strictly weaker than previously known logics for these classes and fail to express some very simple properties.	complexity class;expressive power (computer science);horn clause;l (complexity);nl (complexity);polynomial;sl (complexity);time complexity	Erich Grädel	1992	Theor. Comput. Sci.	10.1016/0304-3975(92)90149-A	predicate logic;time complexity;complexity class;combinatorics;order theory;relational database;computer science;first-order logic;mathematics;propositional calculus;transitive closure;second-order logic;algorithm	Logic	-7.645310655965802	19.207449028181834	134989
e152e70f1afd5de03e3b5f488a41bc186ab0fbd1	definability of recursive predicates in the induced subgraph order		Consider the set of all finite simple graphs $$\\mathcal {G}$$ ordered by the induced subgraph order $$\\le _i$$. Building on previous work by Wires [14] and Jezek and Mckenzie [5---8], we show that every recursive predicate over graphs is definable in the first order theory of $$\\mathcal {G},\\le _i, P_3$$ where $$P_3$$ is the path on 3 vertices.	induced subgraph;recursion;recursive language	Ramanathan S. Thinniyam	2017		10.1007/978-3-662-54069-5_16	induced subgraph isomorphism problem	Theory	-5.001862004348985	20.357361392655122	134998
abdf766e884eca65880f7437e7401ad50b1a7c2a	an algebraic structure for derivations in rewriting systems	rewrite systems	Abstract   In the present paper a uniform description of the algebraic properties of derivations in a rewriting system, and of their syntax and semantics is proposed on the base of a polycategory structure. Similarity relations between derivations are studied on syntax and semantic levels. Several canonical derivation forms are described.	rewriting	Yuri Velinov	1988	Theor. Comput. Sci.	10.1016/0304-3975(88)90039-4	abstract syntax;discrete mathematics;computer science;mathematics;confluence;algorithm;algebra	ECom	-12.714405184684148	18.596607857454437	135146
8b437e7c3fd61fd6cf643f5dab4fd9ceac8f229b	decomposing port automata	decomposition;a a;tuple centres;artifacts;automata;situatedness;respect;mas environment	Port automata are an operational model for component connectors in a coordination language such as Reo. They describe which sets of ports can synchronize in each state of the connector being modelled. This paper presents decomposition theorems for port automata, namely that all (finite) port automata can be generated from a small set of primitive port automata. Applying these results to component connectors means that all component connectors can be constructed from just two primitive connectors.	automata theory;automaton;causal filter;causality;constraint automaton;kinetic data structure;state (computer science)	Christian Krause;Dave Clarke	2009		10.1145/1529282.1529587	quantum finite automata;computer science;artificial intelligence;automaton;decomposition;algorithm	Embedded	-8.058840285117329	24.14403866146203	135294
03579598b3279728235bc6b06bb201f5970ec3d8	c-shore: a collapsible approach to higher-order verification	verification;collapsible pushdown systems;higher order;model checking;recursion schemes	Higher-order recursion schemes (HORS) have recently received much attention as a useful abstraction of higher-order functional programs with a number of new verification techniques employing HORS model-checking as their centrepiece. This paper contributes to the ongoing quest for a truly scalable model-checker for HORS by offering a different, automata theoretic perspective. We introduce the first practical model-checking algorithm that acts on a generalisation of pushdown automata equi-expressive with HORS called collapsible pushdown systems (CPDS). At its core is a substantial modification of a recently studied saturation algorithm for CPDS. In particular it is able to use information gathered from an approximate forward reachability analysis to guide its backward search. Moreover, we introduce an algorithm that prunes the CPDS prior to model-checking and a method for extracting counter-examples in negative instances. We compare our tool with the state-of-the-art verification tools for HORS and obtain encouraging results. In contrast to some of the main competition tackling the same problem, our algorithm is fixed-parameter tractable, and we also offer significantly improved performance over the only previously published tool of which we are aware that also enjoys this property. The tool and additional material are available from http://cshore.cs.rhul.ac.uk.	approximation algorithm;automata theory;backward induction;best, worst and average case;cobham's thesis;higher-order function;model checking;parameterized complexity;pushdown automaton;reachability;recursion;scalability;stack (abstract data type);verification and validation;while	Christopher H. Broadbent;Arnaud Carayol;Matthew Hague;Olivier Serre	2013		10.1145/2500365.2500589	model checking;verification;higher-order logic;computer science;theoretical computer science;programming language;algorithm	Logic	-14.738605484168664	25.999992780552493	135298
86ea9ec45e34b2934415d6bc1dbc9a9dfc62decf	automated theory exploration for interactive theorem proving: an introduction to the hipster system		Theory exploration is a technique for automatically discovering new interesting lemmas in a mathematical theory development using testing. In this paper I will present the theory exploration system Hipster, which automatically discovers and proves lemmas about a given set of datatypes and functions in Isabelle/HOL. The development of Hipster was originally motivated by attempts to provide a higher level of automation for proofs by induction. Automating inductive proofs is tricky, not least because they often need auxiliary lemmas which themselves need to be proved by induction. We found that many such basic lemmas can be discovered automatically by theory exploration, and importantly, quickly enough for use in conjunction with an interactive theorem prover without boring the user.	automated theorem proving;hol (proof assistant);isabelle;mathematical induction;parsing;prettyprint;printing;proof assistant;symbolic computation	Moa Johansson	2017		10.1007/978-3-319-66107-0_1	automation;algorithm;mathematical proof;programming language;computer science;lemma (mathematics);mathematical theory;proof assistant	AI	-19.02570382570309	18.7900954019094	135464
ee50ebb95640c8d7e92b03354009ca6872b0baf6	formula-dependent equivalence for compositional ctl model checking	model checking	 . We present a state equivalence that is defined with respectto a given CTL formula. Since it does not attempt to preserve all CTLformulas, like bisimulation does, we can expect to compute coarser equivalences.We use this equivalence to manage the size of the transition relationsencountered when model checking a system of interacting FSMs.Specifically, the equivalence is used to reduce the size of each componentFSM, so that their product will be smaller. We show how to applythe method,... 	model checking;turing completeness	Adnan Aziz;Thomas R. Shiple;Vigyan Singhal	1994		10.1007/3-540-58179-0_65	model checking;computer science;ctl*;programming language;algorithm	Logic	-11.892929263182882	24.020379094818036	135477
9c75b1524c665c883d55e22a14e7e3b9dd3e5e2d	losing functions without gaining data: another look at defunctionalisation	firstification;data type;higher order;first order;analysis pattern;defunctionalisation;haskell	We describe a transformation which takes a higher-order program, and produces an equivalent first-order program. Unlike Reynolds-style defunctionalisation, it does not introduce any new data types, and the results are more amenable to subsequent analysis operations. We can use our method to improve the results of existing analysis operations, including strictness analysis, pattern-match safety and termination checking. Our transformation is implemented, and works on a Core language to which Haskell programs can be reduced. Our method cannot always succeed in removing all functional values, but in practice is remarkably successful.	first-order predicate;haskell;library (computing);precomputation;strictness analysis;termination analysis	Neil Mitchell;Colin Runciman	2009		10.1145/1596638.1596641	higher-order logic;data type;computer science;first-order logic;programming language;algorithm	PL	-16.988393470139027	22.470261810726246	135538
13d1d33df37c252b14073caf03b841edd37ae5bf	incremental quantitative verification for markov decision processes	strongly connected component;markov processes formal verification;system modeling;incremental verification;probabilistic model checking quantitative verification incremental verification markov decision processes performance analysis;computer network;probabilistic model;formal verification;community networks;probabilistic model checking;markov processes tin probabilistic logic computational modeling numerical models analytical models data structures;analytical method;numerical computation;symbolic data structures incremental quantitative verification technique markov decision processes strongly connected components scc based approach;performance analysis;markov processes;markov decision process;wireless technology;data structure;markov decision processes;everyday life;quantitative verification;discrete event simulation;markov chain	Quantitative verification techniques provide an effective means of computing performance and reliability properties for a wide range of systems. However, the computation required can be expensive, particularly if it has to be performed multiple times, for example to determine optimal system parameters. We present efficient incremental techniques for quantitative verification of Markov decision processes, which are able to re-use results from previous verification runs, based on a decomposition of the model into its strongly connected components (SCCs). We also show how this SCC-based approach can be further optimised to improve verification speed and how it can be combined with symbolic data structures to offer better scalability. We illustrate the effectiveness of the approach on a selection of large case studies.	computation;data structure;markov chain;markov decision process;parallel computing;reliability (computer networking);scalability;strongly connected component;verification and validation	Marta Z. Kwiatkowska;David Parker;Hongyang Qu	2011	2011 IEEE/IFIP 41st International Conference on Dependable Systems & Networks (DSN)	10.1109/DSN.2011.5958249	markov decision process;data structure;computer science;theoretical computer science;machine learning;data mining;runtime verification;functional verification;statistics	SE	-10.73896787470999	28.780571845062187	135608
3a821948e8a0b8a4acb2d5c1a435c362d30a08a1	modeling sequences within the relview system	relation algebra	We use a relational characterization of binary direct sums to model sequences within the relation-algebraic manipulation and prototyping system RelView in a simple way. As an application we formally derive a RelView program for computing equivalence classes of an equivalence relation, where we combine relation-algebraic calculations with the so-called Dijkstra-Gries program development method. Also a re nement of the simple modeling is presented, which leads to the classical datatype of stacks, and a further application is sketched.	david gries;dijkstra's algorithm;linear algebra;stack (abstract data type);turing completeness	Rudolf Berghammer;Thorsten Hoffmann	2001	J. UCS	10.3217/jucs-007-02-0107	discrete mathematics;stack (abstract data type);data mining;equivalence class;equivalence relation;computer science;binary number;relation algebra	ML	-12.39722486941834	19.263401877365684	135633
3e3284dca26a707d4a41f73eb0ce6a0276dfcbdf	efficient algorithms for description problems over finite totally ordered domains	disjunction;68w40;total order;temps polynomial;efficient algorithm;logica multivalente;coaccion;contrainte;forma normal;approche algebrique;complexity;calculo automatico;vecteur;mediane;polynomial;median;68wxx;computing;calcul automatique;formula horn;disyuncion;logique multivalente;finite domain;constraint;polinomio;disjonction;polynomial time;algorithme polynomial;normal form;ensemble fini;forme normale conjonctive;complexity 68q25;algorithms;68q25;forme normale;vector;mediana;formule horn;multivalued logic;polynome;horn formula;description problems;68t27;tiempo polinomial	Given a finite set of vectors over a finite totally ordered domain, we study the problem of computing a constraint in conjunctive normal form such that the set of solutions for the produced constraint is identical to the original set. We develop an efficient polynomial-time algorithm for the general case, followed by specific polynomial-time algorithms producing Horn, dual Horn, and bijunctive formulas for sets of vectors closed under the operations of conjunction, disjunction, and median, respectively. Our results generalize the work of Dechter and Pearl on relational data, as well as the papers by Hébrard and Zanuttini. They complement the results of Hähnle et al. on multivalued logics and Jeavons et al. on the algebraic approach to constraints.	algorithm;conjunctive normal form;constraint (mathematics);linear algebra;newton–cotes formulas;polynomial;time complexity	Àngel J. Gil;Miki Hermann;Gernot Salzer;Bruno Zanuttini	2008	SIAM J. Comput.	10.1137/050635900	combinatorics;computing;complexity;vector;computer science;calculus;mathematics;constraint;median;total order;algorithm;polynomial	AI	-6.782170503155111	18.585825026649342	135787
196c16f7916917570f8ef240faaef355a3747a6f	the stuttering principle revisited	logique lineaire;hierarchy;depth;logica lineal;equivalence;principe begaiement;linear temporal logic;informatique theorique;jerarquia;profundidad;characterization;invariante;stuttering principle;profondeur;caracterisation;hierarchie;linear logic;caracterizacion;equivalencia;invariant;stuttering;ltl;computer theory;informatica teorica	It is known that LTL formulae without the ‘next’ operator are invariant under the so-called stutter equivalence of words. In this paper we extend this principle to general LTL formulae with given nesting depths of both ‘next’ and ‘until’ operators. This allows us to prove the semantical strictness of three natural hierarchies of LTL formulae, which are parametrized either by the nesting depth of just one of the two operators, or by both of them. Further, we provide an effective characterization of languages definable by LTL formulae with a bounded nesting depth of the ‘next’ operator.	information trust institute;model checking;theoretical computer science;turing completeness	Antonín Kucera;Jan Strejcek	2005	Acta Informatica	10.1007/s00236-005-0164-4	equivalence;linear logic;discrete mathematics;linear temporal logic;computer science;invariant;mathematics;programming language;algorithm;hierarchy	Logic	-9.946890183471067	19.50405821845798	135794
4a65dec417142c680f479cf09621926db116c97b	speeding up exact real arithmetic on fast binary cauchy sequences by using memoization based on quantized precision		Exact Real Arithmetic on Fast Binary Cauchy Sequences (FBCSs) provides us a simple and fairly fast way to obtain numerical results of arbitrary precision. The arithmetic on FBCSs can be implemented concisely in a lazy functional language with unlimited-length integer arithmetic, such that each FBCS is represented by a function that generates approximated values with respect to requested precisions. However, application of the arithmetic on FBCSs to programs such as matrix computations, that usually involve large amount of references to common subexpressions, requires care to avoid the blowup of the amount of computation caused by the fact that approximated values are not shared among multiple references. Although simple memoization might alleviate the situation, the effect would be limited since required precisions for subexpressions tend to be various. In this paper, we present an extended design of the arithmetic on FBCSs that enables the memoization based on quantized precision, that is expected to enlarge the reuse rate and reduce the amount of computation without sacrificing the properties of the arithmetic to be exact arithmetic. Numerical experiments by using our prototype libraries in Haskell demonstrated that our approach possesses the potential to outperform existing implementations by orders of magnitude in speed and memory consumption.	approximation algorithm;arbitrary-precision arithmetic;computation;experiment;functional programming;haskell;lazy evaluation;library (computing);memoization;numerical analysis;prototype	Hideyuki Kawabata	2017	JIP	10.2197/ipsjjip.25.494	theoretical computer science;computer science;memoization;quantization (physics);cauchy sequence;binary number	PL	-11.951072790430862	30.99967133782497	135849
ff3e2513450a7ca3f7d5a26c6aaa4506c40d67e2	using eternity variables to specify and prove a serializable database interface	auxiliary variable;linear time temporal logic;prophecy variables;implementation;specification;forward invariant;theorem prover;mechanical verification;serializability;backward invariant;scrializability;history variables	Eternity variables are introduced to specify and verify serializability of transactions of a distributed database. Eternity variables are a new kind of auxiliary variables. They do not occur in the implementation but are used in specification and verification. Elsewhere it has been proved that eternity variables in combination with history variables are semantically complete for proving refinement relations. An eternity variable can be thought of as an unknown constant that is determined by the behaviour (execution sequence). In the specification of the database, one eternity variable is used to enforce serialization. In the verification, an additional eternity variable is needed for the connection of the local data with the shared database. The formalism is based on linear-time temporal logic, but the analysis of behaviours is completely reduced to the next-state relation together with progress arguments using variant functions. Forward invariants (inductive predicates) are complemented with other, so-called backward, invariants. The proof has been verified with the first-order theorem prover NQTHM to give additional confidence in the result and in the feasibility of the approach.	automated theorem proving;distributed database;first-order predicate;invariant (computer science);linear temporal logic;nqthm;refinement (computing);semantics (computer science);serializability;serialization	Wim H. Hesselink	2004	Sci. Comput. Program.	10.1016/j.scico.2003.06.001	computer science;automated theorem proving;programming language;serializability;implementation;specification;algorithm	Logic	-17.723175572482855	22.12334814567961	135863
8bf4f2cf0a15c19353bd0f535e7a7cf7fce26fd1	formal techniques for performance analysis: blending san and pepa	stochastic automaton;continuous time;evaluation performance;continuous time markov process;performance evaluation;proceso markov;evaluacion prestacion;temps continu;stochastic automata networks;automata estocastico;tiempo continuo;automate stochastique;algebra proceso;informatique theorique;processus markov;analyse performance;performance analysis;markov process;algebre processus;performance evaluation process algebra;process algebra;computer theory;analisis eficacia;informatica teorica;performance modelling	In this paper we consider two performance modelling techniques from the perspectives of model construction, generation of an underlying continuous time Markov process, and the potential for reduction in the Markov process. Such careful comparison of modelling techniques allows us to appreciate the strengths and weaknesses of different approaches, and facilitates cross-fertilization between them. In the present case we take a characteristic of one formalism, functional rates in Stochastic Automata Networks, and introduce it to the other formalism, Performance Evaluation Process Algebra. We investigate the benefits of this cross-fertilization, particularly from the perspectives of Markov process generation and reduction.	alpha compositing;automaton;markov chain;pepa;performance evaluation;process calculus;profiling (computer programming);reduction (complexity);semantics (computer science)	Jane Hillston;Leïla Kloul	2006	Formal Aspects of Computing	10.1007/s00165-006-0011-6	process calculus;simulation;computer science;artificial intelligence;markov process;programming language;algorithm	HPC	-9.714651048962809	27.96786831971882	135872
f24d89dff3b29b2fce21733a6aaddff5f7c278d5	column-wise verification of multipliers using computer algebra		Verifying arithmetic circuits, and most prominently multipliers, is an important problem but in practice still requires substantial manual effort. Recent work tries to solve this issue using techniques from computer algebra. The most effective approach uses polynomial reasoning over pseudo boolean polynomials. In this paper we give a rigorous formalization of this approach and present a new column-wise verification technique for the correctness of gate-level multipliers which does not require the reduction of a full word-level specification. We formally prove soundness and completeness of our technique, making use of our precise formalization. Our experiments show that simple multipliers can be verified efficiently by using off-the-shelf computer algebra tools, while more complex and optimized multipliers require more sophisticated techniques. Further, our paper independently confirms the effectiveness of previous related work. We make all benchmarks and tools publicly available.	128-bit;benchmark (computing);boolean algebra;computer algebra system;correctness (computer science);experiment;linear algebra;polynomial;soundness (interactive proof);spec#;symbolic computation	Daniela Ritirc;Armin Biere;Manuel Kauers	2017	2017 Formal Methods in Computer Aided Design (FMCAD)	10.23919/FMCAD.2017.8102237	theoretical computer science;completeness (statistics);symbolic computation;computer science;correctness;polynomial;soundness	Logic	-16.122841584943178	25.207220208129822	136250
31e9ffdfc70a5c5930fc18c937c38c1e72f9c5e1	generalized planning: synthesizing plans that work for multiple environments	generalized planning;deterministic environment;representation formalism;finite set;formal definition;one-dimensional problem;infinite set;recursively enumerable;multiple environment;complexity-wise optimal technique;generalized plan;synthesizing plan	We give a formal definition of generalized planning that is independent of any representation formalism. We assume that our generalized plans must work on a set of deterministic environments, which are essentially unrelated to each other. We prove that generalized planning for a finite set of environments is always decidable and EXPSPACEcomplete. Our proof is constructive and gives us a sound, complete and complexity-wise optimal technique. We also consider infinite sets of environments, and show that generalized planning for the infinite “one-dimensional problems,” known in the literature to be recursively enumerable when restricted to finite-state plans, is EXPSPACE-decidable without sequence functions, and solvable by generalized planning for finite sets.	2-exptime;automata theory;computation;correctness (computer science);decision problem;expspace;hector levesque;holographic principle;linear temporal logic;recursively enumerable set;semantics (computer science)	Yuxiao Hu;Giuseppe De Giacomo	2011		10.5591/978-1-57735-516-8/IJCAI11-159	mathematical optimization;combinatorics;discrete mathematics;mathematics	AI	-6.531585580601997	24.27497431804176	136469
0421f79fe086a9c32162c095c7406c25d0352e93	preserving behavior in transition systems from event structure models		Two structurally di erent methods of associating transition system semantics to event structure models are distinguished in the literature. One of them is based on con gurations (event sets), the other on residuals (model fragments). In this paper, we consider three kinds of event structures (resolvable con ict structures, extended prime structures, stable structures), translate the other models into resolvable conict structures and back, provide the isomorphism results on the two types of transition systems, and demonstrate the preservation of some bisimulations on them.	naruto shippuden: clash of ninja revolution 3;transition system	Irina Virbitskaite;Nataliya Gribovskaya	2018				Logic	-9.644166343536993	20.848385666175567	136544
c50b8ac512e394a4648e2db5a5897afe61f8effa	toward an algebra of nondeterministic programs	prolog;program transformation;functional programming;extensible languages;logic programming;equational programming	Two major advantages of the FP Algebra of Programs are its mathematical tractability and the ease with which parallel evaluation may be introduced. Unfortunately, some aspects of parallelism involve nondeterministic computations which, at times, yield indeterminate results. It is possible to introduce special operators to express the indeterminacy; the augmented language is, however, less tractable than the original. Indeterminacy destroys referential transparency: program transformation is not applicable when, for example, the expression x&equil;x may not be identically true. In this paper, we extend the Algebra of Programs by introducing nondeterministic operators and formulating algebraic laws describing their behavior and the transformations applicable to them.	cobham's thesis;indeterminacy in concurrent computation;linear algebra;nondeterministic algorithm;parallel computing;program transformation;referential transparency	A. Toni Cohen;Thomas J. Myers	1982		10.1145/800068.802155	computer science;theoretical computer science;programming language;functional programming;prolog;logic programming;algorithm	PL	-17.439259072730238	19.828348335888382	136650
31e759afeebaf5e94c4d4da8466ed2e70294a9b3	monitoring functions on global states of distributed programs	modelizacion;distributed system;architecture systeme;systeme reparti;complexite calcul;programacion paralela;reseau ordinateur;distributed programs;parallel programming;computer network;algorithme;modelisation;algorithm;complejidad computacion;sistema repartido;computational complexity;red ordenador;arquitectura sistema;system architecture;modeling;programmation parallele;algoritmo	The domain of a global function is the set of all global states of an execution of a distributed program. We show how to monitor a program in order to determine if there exists a global state in which the sum x 1 +x 2 +: : :+x N exceeds some constant K, where x i is deened in process i. We examine the cases where x i is an integer variable and where x i is a boolean variable. For both cases we provide algorithms, prove their correctness and analyze their complexity.	advanced configuration and power interface;algorithm;correctness (computer science);global serializability	Alexander I. Tomlinson;Vijay K. Garg	1997	J. Parallel Distrib. Comput.	10.1006/jpdc.1996.1298	systems modeling;computer science;theoretical computer science;computational complexity theory;algorithm;systems architecture	Logic	-7.792562880746758	31.45325612665343	137094
1522d2eff4f25c9ddaa12e4d98de62d2ce189f70	an introduction to symbolic trajectory evaluation	developpement logiciel;verificacion modelo;blow up;publikationer;temporal logic;verification modele;program transformation;symbolic trajectory evaluation;konferensbidrag;transformation programme;transformacion programa;formal verification;model checking;desarrollo logicial;software development;artiklar;temporal properties;rapporter;hardware design;high performance;binary decision diagram	The rapid growth in hardware complexity has lead to a need for formal verification of hardware designs to prevent bugs from entering the final sili con. Model-checking [3] is by far the most popular technique for automatically verifyi ng properties of designs. In model-checking, a model of a design is exhaustively checked against a property, often specified in some temporal logic. Today, all major hardware c ompanies use modelcheckers in order to reduce the number of bugs in their design s. Most model-checking techniques are state-based . This means that some kind of representation of all reachable states of the design is used whe n checking that the temporal properties are fulfilled. One popular way of representing th e set of reachable states of a design is by using Binary Decision Diagrams (BDDs) [2]. A BD D is a canonical way of representing a boolean formula over a fixed set of varia bles. When the set of reachable states of a design can be calculated using BDDs, st ate-based model-checking techniques work very well. However, for some types of design s, it is very hard to represent all reachable states by BDDs; they grow exponentiall y in size and lead to a BDD blow-up. A different kind of model-checking technique is simulation-basedmodel checking. In simulation-based model-checking, some representa tio of the values that drive certain signals in the design is used, in order to calculate t he resulting values of other signals in the design. In this way, we do not need to represent th states of the design, but only the values that flow through each signal. Symbolic Trajectory Evaluation Symbolic Trajectory Evaluation (STE) is a highperformance simulation-based model checking technique, o riginally invented by Seger and Bryant [13]. STE uses a combination of three-valued simulation andsymbolic simulation. Let us look at a very simple example to understand what this m eans. The example is inspired by Harrison [5]. Take a look at the circuit in Fig. 1, where a designer has built a 7-input AND-gate out of primitive gates. Inverters are represented in the figu re by a block dot ( •). Suppose it is our job to verify that this circuit actually behaves lik e a 7-input AND-gate. One way to do this is to use standard boolean simulation. This means that we simulate the circuit for every possible combination of input val ues. Since we have 7 inputs, we have to perform2 = 128 simulation runs. An example of one of these can be described as follows:	abstraction layer;assertion (software development);binary decision diagram;blu-ray;boolean algebra;boolean satisfiability problem;formal verification;forte 4gl;graph (discrete mathematics);inverter (logic gate);linear temporal logic;mathematical induction;model checking;naruto shippuden: clash of ninja revolution 3;simulation;software bug;solver;symbolic trajectory evaluation;the european library;theorem proving system;ues (cipher);xfig	Koen Claessen;Jan-Willem Roorda	2006		10.1007/11757283_3	model checking;discrete mathematics;temporal logic;formal verification;computer science;artificial intelligence;theoretical computer science;software development;database;mathematics;programming language;binary decision diagram;symbolic trajectory evaluation;algorithm	Logic	-15.070798307879473	27.93017793239726	137181
0c594aa415d5862f3d32f7a9d9ccf2d289234ce5	soc: a slicer for csp specifications	control flow graph;software engineering;program slicing;data structure	This paper describes SOC, a program slicer for CSP specifications. In order to increase the precision of program slicing, SOC uses a new data structure called Context-sensitive Synchronized Control Flow Graph (CSCFG). Given a CSP specification, SOC generates its associated CSCFG and produces from it two different kinds of slices; which correspond to two different static analyses. We present the tool's architecture, its main applications and the results obtained from experiments conducted in order to measure the performance of the tool.	communicating sequential processes;control flow graph;data structure;experiment;program slicing;static program analysis	Michael Leuschel;Marisa Llorens;Javier Oliver;Josep Silva;Salvador Tamarit	2009		10.1145/1480945.1480969	embedded system;program slicing;real-time computing;data structure;computer science;programming language;control flow graph	SE	-18.96513302079657	28.896411725603297	137484
429c43d6f691da0dc8b7539d5759b67bc9c549a4	formalizing goal serializability for evaluation of planning features		Evaluation of the properties of various planning techniques such as completeness and termination plays an important role in choosing an appropriate planning technique for a particular planning problem. In this paper, we use the already existing formal specification of two well-known and classic state space planning techniques, forward state space planning and goal stack state space planning techniques, in Transaction Logic(( mathcal {TR} )) to study their completeness. Our study shows that using ( mathcal {TR} ), we can formally specify the serializability of planning problems and prove the completeness of ( textit{STRIPS}) planning problems for planning problems with serializable goals.	serializability	Reza Basseda;Michael Kifer	2016		10.1007/978-3-319-48758-8_33	reliability engineering;knowledge management;database	Robotics	-13.795630777821534	23.174284662471848	137640
8ec8fa3bc7e7b3fcd5dac73fa9fe9035483db47c	on observing dynamic prioritised actions in soc	process calculus;qa75 electronic computers computer science;high priority	We study the impact on observational semantics for SOC of priority mechanisms which combine dynamic priority with local pre-emption. We define manageable notions of strong and weak labelled bisimilarities for COWS, a process calculus for SOC, and provide alternative characterisations in terms of open barbed bisimilarities. These semantics show that COWS’s priority mechanisms partially recover the capability to observe receive actions (that could not be observed in a purely asynchronous setting) and that high priority primitives for termination impose specific conditions on the bisimilarities.	process calculus	Rosario Pugliese;Francesco Tiezzi;Nobuko Yoshida	2009		10.1007/978-3-642-02930-1_46	combinatorics;process calculus;real-time computing;computer science;artificial intelligence;mathematics;distributed computing;programming language;algorithm	Logic	-10.745354767259895	21.935442976647977	137682
b095c65982210a3f0cf5622b5fcb8c424ec98c7e	a traversal-based algorithm for higher-order model checking	software model checking;game semantics;functional programming;higher order;programming model;model checking;pattern matching;higher order programs;functional language;intersection types;type system	Higher-order model checking - the model checking of trees generated by higher-order recursion schemes (HORS) - is a natural generalisation of finite-state and pushdown model checking. Recent work has shown that it can serve as a basis for software model checking for functional languages such as ML and Haskell. In this paper, we introduce higher-order recursion schemes with cases (HORSC), which extend HORS with a definition-by-cases construct (to express program branching based on data) and non-determinism (to express abstractions of behaviours). This paper is a study of the universal HORSC model checking problem for deterministic trivial automata: does the automaton accept every tree in the tree language generated by the given HORSC? We first characterise the model checking problem by an intersection type system extended with a carefully restricted form of union types. We then present an algorithm for deciding the model checking problem, which is based on the notion of traversals induced by the fully abstract game semantics of these schemes, but presented as a goal-directed construction of derivations in the intersection and union type system. We view HORSC model checking as a suitable backend engine for an approach to verifying functional programs. We have implemented the algorithm in a tool called TravMC, and demonstrated its effectiveness on a test suite of programs, including abstract models of functional programs obtained via an abstraction-refinement procedure from pattern-matching recursion schemes.	algorithm;denotational semantics;experiment;functional programming;game semantics;haskell;model checking;pattern matching;recursion (computer science);refinement (computing);scalability;stack (abstract data type);test suite;tree automaton;tree traversal;type system;verification and validation	Robin P. Neatherway;Steven J. Ramsay;C.-H. Luke Ong	2012		10.1145/2364527.2364578	model checking;higher-order logic;type system;computer science;game semantics;theoretical computer science;automated proof checking;pattern matching;programming paradigm;programming language;functional programming;abstraction model checking;algorithm	PL	-14.474139786919896	24.289974856977274	138261
ba09bca77acc42d783bdd67c7559553e434c1dce	a domain-theoretic model for a higher-order process calculus	theoretical model;operational semantics;lambda calculus;model choice;higher order;process calculus;technical report;computer science	In this paper we study a higher-order process calculus, a restriction of one due to Boudol, and develop an abstract, model for it. By abstract we mean that the model is constructed domain-theoretically and reeects a certain conceptual viewpoint about observability. It is not constructed from the syntax of the calculus or from computation sequences. We describe a new powerdomain construction that can be given additional algebraic structure that allows one to model concurrent composition, in the same sense that Plotkin's powerdomain can have a continuous binary operation deened on it to model choice. We show that the model constructed this way is adequate with respect to the operational semantics. The model that we develop and our analysis of it is closely related to the work of Abramsky and Ong on the lazy lambda calculus.	computation;domain theory;lambda calculus;lazy evaluation;operational semantics;plotkin bound;power domains;process calculus	Radha Jagadeesan;Prakash Panangaden	1990		10.1007/BFb0032031	ambient calculus;fluent calculus;combinatorics;process calculus;typed lambda calculus;discrete mathematics;binary lambda calculus;normalisation by evaluation;higher-order logic;computer science;technical report;time-scale calculus;lambda calculus;simply typed lambda calculus;mathematics;programming language;church encoding;lambda cube;natural deduction;operational semantics;calculus of communicating systems;multivariable calculus;algorithm;tuple relational calculus	Logic	-11.897547519415964	19.070028240207673	138336
c60659bce32b5debe1dd1bd28efb8167bf987bcf	hoare logics for time bounds		We study three different Hoare logics for reasoning about time bounds of imperative programs and formalize them in Isabelle/HOL: a classical Hoare like logic due to Nielson, a logic with potentials due to Carbonneaux et al. and a separation logic following work by Atkey, Chaguerand and Pottier. These logics are formally shown to be sound and complete. Verification condition generators are developed and are shown sound and complete too. We also consider variants of the systems where we abstract from multiplicative constants in the running time bounds, thus supporting a big-O style of reasoning. Finally we compare the expressive power of the three systems.	hoare logic	Maximilian P. L. Haslbeck;Tobias Nipkow	2018	Archive of Formal Proofs		expressive power;discrete mathematics;multiplicative function;hol;separation logic;hoare logic;computer science	Theory	-13.844866973044818	20.845975060796647	138342
4e538c518850683ba1ddc8b4ab1dab7992e8b077	on proof rules for monitors	concurrency;operating system;semaphores;proof rules;monitor	An inadequacy is pointed out in the original proof rules for monitors and in later extended rules. This inadequacy gives rise to an anomaly in proving the invariant for a monitor simulating a counting semaphore. New proof rules are proposed and used to give a sound proof of the invariant.	anomaly detection;monitor (synchronization);semaphore (programming);simulation	J. Mack Adams;Andrew P. Black	1982	Operating Systems Review	10.1145/1041474.1041476	monitor;semaphore;concurrency;computer science;theoretical computer science;operating system;proof complexity;algorithm	Logic	-11.401510695274146	25.876604272634296	138640
3babbde875539348484223e0099e14c2e458b883	implementation of an array bound checker	grammar;automatic;errors;human interaction;machine coding;semantics;context free;program verification;automatic generation;computer programs;upper bound;arrays;parsing;theorem prover;programmers;linear time;left recursion;algorithms;cover;lower bound;computer program verification	This paper describes a system which checks correctness of array accesses automatically without any inductive assertions or human interaction. For each array access in the program a condition that the subscript is greater than or equal to the lower bound and a condition that the subscript is smaller than or equal to the upper bound are checked and the results indicating within the bound, out of bound, or undetermined are produced. It can check ordinary programs at about fifty lines per ten seconds, and it shows linear time complexity behavior.It has been long discussed whether program verification will ever become practical. The main argument against program verification is that it is very hard for a programmer to write assertions about programs. Even if he can supply enough assertions, he must have some knowledge about logic in order to prove the lemmas (or verification conditions) obtained from the verifier.However, there are some assertions about programs which must always be true no matter what the programs do; and yet which cannot be checked for all cases. These assertions include: integer values do not overflow, array subscripts are within range, pointers do not fall off NIL, cells are not reclaimed if they are still pointed to, uninitialized variables are not used.Since these conditions cannot be completely checked, many compilers produce dynamic checking code so that if the condition fails, then the program terminates with proper diagnostics. These dynamic checking code sometimes take up much computation time. It is better to have some checking so that unexpected overwriting of data will not occur, but it is still very awkward that the computation stops because of error. Moreover, these errors can be traced back to some other errors in the program. If we can find out whether these conditions will be met or not before actually running the program, we can benefit both by being able to generate efficient code and by being able to produce more reliable programs by careful examination of errors in the programs. Similar techniques can be used to detect semantically equivalent subexpressions or redundant statements to do more elaborate code movement optimization.The system we have constructed runs fast enough to be used as a preprocessor of a compiler. The system first creates logical assertions immediately before array elements such that these assertions must be true whenever the control passes the assertion in order for the access to be valid. These assertions are proved using similar techniques as inductive assertion methods. If an array element lies inside a loop or after a loop a loop invariant is synthesized. A theorem prover was created which has the decision capabilities for a subset of arithmetic formulas. We can use this prover to prove some valid formulas, but we can also use it to generalize nonvalid formulas so that we can hypothesize more general loop invariants.Theoretical considerations on automatic synthesis of loop invariants have been taken into account and a complete formula for loop invariants was obtained. We reduced the problem of loop invariant synthesis to the computation of this formula. This new approach of the synthesis of loop invariant will probably give more firmer basis for the automatic generation of loop invariants in general purpose verifiers.	array data structure;assertion (software development);automated theorem proving;automatic control;compiler;computation;correctness (computer science);for loop;formal verification;inductive reasoning;loop invariant;mathematical optimization;overwriting (computer science);preprocessor;programmer;time complexity;uninitialized variable	Norihisa Suzuki;Kiyoshi Ishihata	1977		10.1145/512950.512963	computer science;theoretical computer science;semantics;invariant;upper and lower bounds;programming language;algorithm	PL	-16.724934439886788	30.94562063830286	138674
5d27664035b9a9d09905c9227093f1f1464c2934	symbolic evaluation with structural recursive symbolic constants	symbolic evaluation;structural recursive symbolic constant	Symbolic evaluation is a technique used for many purposes: program analysis, program verification, program transformation. The paper focusses on a novel approach which allows one to symbolically evaluate programs with respect to predicates denoting subsets of a user-defined data domain. The data domain is assumed to be recursively defined and the predicates can be defined by structural induction on the data. An application of the technique to the textual reduction of recursive programs is sketched.	data domain;formal verification;program analysis;program transformation;recursion (computer science);recursive definition;structural induction;symbolic execution	Fosca Giannotti;Attilio Matteucci;Dino Pedreschi;Franco Turini	1987	Sci. Comput. Program.	10.1016/0167-6423(87)90031-1	computer science;theoretical computer science;symbolic data analysis;programming language;symbolic trajectory evaluation;algorithm	PL	-18.29879470908496	25.282316634652496	138836
c5d17617d5b057d7c0838be5a6305d8c9b5f6ed7	semantics of real-time trigger-response properties in event-b		Event-B is a formal method for system-level modelling and analysis, which uses logic and set theory to describe discrete labelled transition systems. Timed transition systems have been introduced to incorporate timing constraints on transitions to describe real-time behaviours of the system. This paper proposes an approach to modelling high level timing constraints between different transitions with a timed trigger-response property. We present trace semantics for the trigger-response property and timed trigger-response property. This semantics provides a precise definition of valid trigger-response behaviours in Event-B machines. Based on the semantics, we develop proof obligations on Event-B machines under which all the traces of a machine satisfy the trigger-response property and the timed trigger-response property.		Chenyang Zhu;Michael J. Butler;Corina Cîrstea	2018	2018 International Symposium on Theoretical Aspects of Software Engineering (TASE)	10.1109/TASE.2018.00028	theoretical computer science;semantics;formal methods;computer science;set theory	Logic	-11.300696830827059	25.64892866862293	139171
31054476bafa3ad68872cd4aa917c715f2e0744f	flattening and implication	learning algorithm;algorithm analysis;apprentissage inductif;algoritmo recursivo;algorithme apprentissage;logical programming;satisfiability;algorithme recursif;programmation logique;inductive learning;analyse algorithme;recursive algorithm;algoritmo aprendizaje;programacion logica;analisis algoritmo	Flattening is a method to make a definite clause functionfree. For a definite clause C, flattening replaces every occurrence of a term f(t1, · · · , tn) in C with a new variable v and adds an atom pf (t1, · · · , tn, v) with the associated predicate symbol pf with f to the body of C. Here, we denote the resulting function-free definite clause from C by flat(C). In this paper, we discuss the relationship between flattening and implication. For a definite program Π and a definite clause D, it is known that if flat(Π) |= flat(D) then Π |= D, where flat(Π) is the set of flat(C) for each C ∈ Π . First, we show that the converse of the above statement does not hold even if Π = {C}, that is, there exist definite clauses C and D such that C |= D but flat(C) |= flat(D). Furthermore, we investigate the conditions of C and D satisfying that C |= D if and only if flat(C) |= flat(D). Then, we show that, if (1) C is not self-resolving and D is not tautological, (2) D is not ambivalent, or (3) C is singly recursive, then the statement holds.	emoticon;existential quantification;horn clause;predicate variable;recursion	Kouichi Hirata	1999		10.1007/3-540-46769-6_13	combinatorics;computer science;mathematics;linguistics;programming language;algorithm;recursion;satisfiability	NLP	-7.8104682249425545	18.846461244174204	139222
1c6d8916ee5995a4dcb32b66187e02085d99a56e	unbounded lookahead in wmso+u games		Delay games are two-player games of infinite duration in which one player may delay her moves to obtain a lookahead on her opponent’s moves. We consider delay games with winning conditions expressed in weak monadic second order logic with the unbounding quantifier (WMSO+U), which is able to express (un)boundedness properties. It is decidable whether the delaying player is able to win such a game with bounded lookahead, i.e., if she only skips a finite number of moves. However, bounded lookahead is not always sufficient: we present a game that can be won with unbounded lookahead, but not with bounded lookahead. Then, we consider WMSO+U delay games with unbounded lookahead and show that the exact evolution of the lookahead is irrelevant. Finally, we investigate whether the winner of such a game with unbounded lookahead is effectively computable: we reduce this problem to a delayfree infinite game with WMSO+U winning condition, whose winner is effectively computable. However, we only obtain partial results on the effectiveness of the reduction, thereby leaving decidability also open.	algorithm;automata theory;computable function;embedded system;open-source video game;parsing;quantifier (logic);relevance;unbounded nondeterminism	Martin Zimmermann	2015	CoRR		discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-11.019978980291139	24.52229522939807	139237
341342e4447e269b8026a0f3513b41aedc6279e2	on reachability analysis of updatable timed automata with one updatable clock		As an extension of Timed Automata (TAs), Updatable Timed Automata (UTAs) proposed by Bouyer et al. have the ability to update clocks in a more elaborate way than simply reset them to zero. The reachability of general UTAs is undecidable, by regarding a pair of updatable clocks as counters updatable with incrementation and decrementation operations. This paper investigates the model of subclass of UTAs by restricting the number of updateable clocks. It is shown that the reachability of UTAs with one updatable clock (UTA1s) under diagonal-free constraints is decidable. The decidability is proved by treating a region of a UTA1 as an unbounded digiword, and encoding sets of digiwords that are accepted by a pushdown system where regions are generated on-the-fly on the stack.	reachability;timed automaton	Yunqing Wen;Guoqiang Li;Shoji Yuen	2015		10.1007/978-3-319-31220-0_11	database	Logic	-11.083036528287932	24.61548874434043	139548
140b9fd8a708ad9d8e4a6704fc6057effc523f7d	analyzing wmsol definable properties on sum-like weighted labeled trees	software;measurement semantics automata cost accounting computational modeling vocabulary software;measurement;vocabulary;semantics;automata;cost accounting;computational modeling;incremental verification hardware and software verification simulation testing coverage;trees mathematics finite state machines formal logic;hierarchical structures wmsol definable properties finite state machine weighted monadic second order logic quantitative properties sum like weighted labeled trees fsm behaviour decomposition structures disjoint unions generalization wmsol formula	Modern software and hardware designs are mostly hierarchical. Moreover, while the design specification is defined up-down, the design implementation and verification are done down-up. In such a case, as a rule, coverage properties for simulation-based verification are defined inconsistently for different stages of the design flow. The fact leads to the well known explosion of bug rate, when we pass from the lower design stage to the upper one. In this paper, we propose a new approach that allows propagation of quantitative properties from the upper stage of the design flow to the lower ones and their incremental computation on the components. The approach may be applied to any design, modeled as a Finite State Machine (FSM) or other formalisms, which eventually lead to weighted labeled trees. We use Weighted Monadic-Second Order Logic (WMSOL) to describe the desired families of quantitative properties and sum-like weighted labeled trees to describe the decomposition of the behaviour of the FSM. The last notion is based on a generalization of disjoint unions of structures with additional links between the components. Our approach shows how computation of a quantitative property, definable as a WMSOL formula on the upper stage of the design may be reduced for certain cost to incremental computations of effectively derivable WMSOL-definable properties on the components. We provide several examples of families of such properties and discuss different aspects, related to the applicability of our approach. The approach is new and provides a uniform theoretical basis for analyzing WMSOL-definable properties on hierarchical structures.	code coverage;computation;design flow (eda);dynamic problem (algorithms);expressive power (computer science);finite-state machine;incremental computing;microsoft outlook for mac;patch (computing);simulation;software propagation;value (ethics);whole earth 'lectronic link	Elena V. Ravve	2014	2014 16th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2014.57	discrete mathematics;computer science;artificial intelligence;theoretical computer science;machine learning;data mining;database;mathematics;semantics;automaton;linguistics;programming language;computational model;algorithm;measurement;cost accounting;algebra	Logic	-8.056200447200814	25.74366555165886	140241
6c667c2d63131979cfb7521e8ee8df9623607072	enhancing predicate pairing with abstraction for relational verification		ion-based Predicate Pairing (APP) P1whlP2whl(...) ← a, P1whl(...), P2whl(...) the defnition is augmented with a constraint a representing some relations among the arguments of P1whl and P2whl. The new constraint a is an abstraction of the constraint c, d, e (c, d, e) → a occurring in the clause obtained by unfolding: false ← c, d, P1whl(...) , e, P2whl(...) y Bounded Diference Shapes Octagonal Shapes Boxes Convex Polyhedra Enhancing predicate pairing abstract domains x >= 2 x <= 10 y >= 3 y <= 7 x-y >= 0 y <= 6 y >= 3 x <= 10 x+y >= 3 x+y <= 12 x >= 2 x <= 10 y <= 9 y >= 3 x+y <= 35 y >= x-5 7x <= y+95 3y >= x+7 y <= 4x The transformation strategy is parametric with respect to the abstract constraint domain for representing the relations among the atoms of the new predicate defnitions x y	abstract interpretation;polyhedron;unfolding (dsp implementation)	Emanuele De Angelis;Fabio Fioravanti;Alberto Pettorossi;Maurizio Proietti	2017	CoRR		theoretical computer science;predicate (grammar);horn clause;regular polygon;polyhedron;relational calculus;mathematics;pairing;abstraction;solver	Logic	-15.225870303934277	22.857202885754	140386
75373008ac7c703a3ce1f815c2c9eb9aa0f9c456	a simple cps transformation of control-flow information	program transformation;continuation passing style;control flow;control flow analysis;computer control;binding time analysis;program analysis	We build on Danvy and Nielsen’s first-order program transformation into continuation-passing style (CPS) to design a new CPS transformation of flow information that is simpler and more efficient than what has been presented in previous work. The key to simplicity and efficiency is that our CPS transformation constructs the flow information in one go, instead of first computing an intermediate result and then exploiting it to construct the flow information. More precisely, we show how to compute control-flow information for CPStransformed programs from control-flow information for direct-style programs and vice-versa. As a corollary, we confirm that CPS transformation has no effect on the control-flow information obtained by constraint-based control-flow analysis. The transformation has immediate applications in assessing the effect of the CPS transformation over other analyses such as, for instance, binding-time analysis.	continuation;continuation-passing style;control flow analysis;data-flow analysis;emoticon;first-order predicate;legendre transformation;mathematical optimization;name binding;partial evaluation;program analysis;program transformation;tail call	Daniel Damian;Olivier Danvy	2002	Logic Journal of the IGPL	10.1093/jigpal/10.5.501	program analysis;real-time computing;computer science;continuation-passing style;programming language;control flow;algorithm;control flow analysis	PL	-16.250697385031046	22.365506956196977	140532
9d4c0d17545b609d8b4fc137a2355baad7188119	proving properties of multidimensional recurrences with application to regular parallel algorithms	parallel algorithm;concurrent computing;very large scale integration;logic;computer applications;theorem proving;theorem prover;multidimensional systems parallel algorithms difference equations logic computer applications concurrent computing very large scale integration power system modeling data structures;difference equations;parallel systems;data structures;power system modeling;multidimensional systems;parallel algorithms	We present a set of verification methods to prove properties of parallel systems described by means of multidimensional affine recurrence equations. We use polyhedral analysis and transformation techniques together with theorem proving. Polyhedral techniques allow us to handle simple but otherwise costly proof steps, while theorem proving provides more expressivity and more complex proof techniques. This allows large, generic and structured systems to be verified. These methods are implemented in the MMAlpha environment using the PVS theorem prover.	automated theorem proving;computation;dec alpha;emoticon;functional specification;interaction;mathematical induction;matrix multiplication;model checking;network address translation;parallel algorithm;polyhedral;polytope model;property list;recurrence relation;refinement (computing);the matrix	David Cachera;Patrice Quinton;Sanjay V. Rajopadhye;Tanguy Risset	2001	Proceedings 15th International Parallel and Distributed Processing Symposium. IPDPS 2001	10.1109/IPDPS.2001.925133	parallel computing;concurrent computing;computer science;theoretical computer science;distributed computing;parallel algorithm;automated theorem proving;programming language;algorithm	Logic	-11.432176912892347	27.396706211104796	140805
0ac6bc105de0a38fc579ecd9c78981b93bc6939c	a semantics for a class of stratified production system programs	production system	We present our research on defining a correct semantics for forward chaining production systems (PS) programs. A correct semantics must ensure that the execution of the program is deterministic and that it will terminate. We define a class of function-free stratified PS programs, and develop a fixpoint semantics and a declarative semantics for these programs. A stratified PS program comprises an initial extensional database (EDB init) of facts, and a set of productions. We define the conditions for the productions in the PS program to be stratified and we define an operator TPS, which computes the fixpoint for the productions of the stratified PS program. The fixpoint is represented by an updated database EDBn. A corresponding logic program PS dd is derived from the stratified PS program. PS dd is stratified and has a standard minimal model MPS dd. The declarative semantics for the PS program is given by this model MPS dd. We prove that the declarative semantics given by the model MPS dd for PS dd is equivalent to the fixpoint EDBn for the productions in the PS program.	bus (computing);dd (unix);declarative programming;deterministic algorithm;fixed point (mathematics);forward chaining;init;logic programming;ps-algol;production system (computer science);terminate (software)	Louiqa Raschid	1994	J. Log. Program.	10.1016/0743-1066(94)90005-1	computer science;database;production system;programming language;algorithm	DB	-18.40151583450264	21.18074782680622	140819
31c8e994ea29c1541684e434de93ee757e516374	dynamic partitioning in linear relation analysis: application to the verification of reactive systems	synchronous programming;partitioning;program verification;state space;linear relation analysis;reactive system;program verication;reactive systems;abstract interpretation	We apply linear relation analysis [CH78, HPR97] to the verification of declarative synchronous programs [Hal98]. In this approach, state partitioning plays an important role: on one hand the precision of the results highly depends on the fineness of the partitioning; on the other hand, a too much detailed partitioning may result in an exponential explosion of the analysis. In this paper we propose to consider very general partitions of the state space and to dynamically select a suitable partitioning according to the property to be proved. The presented approach is quite general and can be applied to other abstract interpretations.	state space;time complexity	Bertrand Jeannet	2003	Formal Methods in System Design	10.1023/A:1024480913162	reactive system;computer science;theoretical computer science;programming language;algorithm	Logic	-12.80513051946011	25.637903940377484	140881
7720afec49ab0c32d3b0b36bd69ea7c67f728313	diagnosis of discrete event systems using satisfiability algorithms: a theoretical and empirical study	silicon;synchronization silicon computational modeling discrete event systems automata petri nets data mining;propositional satisfiability diagnosis discrete event systems;data mining;journal article;automata;computational modeling;synchronization;discrete event systems;petri nets	We propose a novel algorithm for the diagnosis of systems modelled as discrete event systems. Instead of computing all paths of the model that are consistent with the observations, we use a two-level approach: at the first level diagnostic questions are generated in the form does there exist a path from a given subset that is consistent with the observations?, whilst at the second level a satisfiability (SAT) solver is used to answer the questions. Our experiments show that this approach, implemented in SAT, can solve problems that we could not solve with other techniques.	algorithm;boolean satisfiability problem;constraint learning;existential quantification;experiment;hybrid system;nonlinear system;path (graph theory);simultaneous multithreading;software propagation;solver;unit propagation;while	Alban Grastien;Anbulagan;Jussi Rintanen;Elena Kelareva	2007	IEEE Transactions on Automatic Control	10.1109/TAC.2013.2275892	synchronization;discrete mathematics;computer science;theoretical computer science;discrete system;automaton;silicon;computational model;petri net;algorithm	SE	-8.297879384770267	27.61509271441337	140890
47f8898d3a76d53b8d48a2503b861b8a25a4208d	solos in concert	computability;analyse syntaxique;analisis sintaxico;informatique theorique;syntactic analysis;calculabilite;datavetenskap datalogi;computer science;analisis semantico;analyse semantique;calculabilidad;semantic analysis;computer theory;informatica teorica	We present a calculus of mobile processes without prefix or summation, and using two different encodings we show that it can express both action prefix and guarded summation. One encoding gives a strong correspondence but uses a match operator; the other yields a slightly weaker correspondence but uses no additional operators.	guarded command language;mobile agent	Cosimo Laneve;Björn Victor	1999		10.1007/3-540-48523-6_48	computer science;artificial intelligence;parsing;mathematics;computability;programming language;algorithm	NLP	-6.443709787672698	20.21900820492196	141068
ad1ccd993ee92d09b737121d6cd9d674a53bcc00	coalgebraic semantics and observational equivalences of an imperative class-based oo-language	observational equivalence;computer model;object oriented;contextual equivalence	Fickle is a class-based object oriented imperative language, which extends Java with object re-classification. In this paper, we introduce a natural observational equivalence on Fickle programs. This is a contextual equivalence on main methods with respect to a given sequence of class definitions, i.e. a program. To study it, we use the formal computational model for OO-programming based on coalgebras, which has recently emerged, whereby objects are taken to be equal when the actions of methods on them yield the same observations and equivalent next states. However, in order to deal with imperative features, we need to extend the original approach of H.Reichel and B.Jacobs in various ways. In particular, we introduce a coalgebraic description of objects (states of a class), which induces a coinductive behavioural equivalence on programs. For simplicity, we focus on Fickle objects whose methods do not take more than one object parameter as argument. Completeness results as well as problematic issues arising from binary methods are also discussed.	coinduction;completeness (knowledge bases);computational model;imperative programming;java;observational equivalence;turing completeness	Furio Honsell;Marina Lenisa;Rekha Redamalla	2004	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.08.024	computer simulation;discrete mathematics;computer science;mathematics;programming language;object-oriented programming;algorithm	AI	-13.847919559412338	19.27999632598056	141092
0b9a7ce3a0c1590b0c5acbcf89cdbc5d40d30eaa	graph types for monadic mobile processes	full abstraction;community structure	While types for name passing calculi have been studied extensively in the context of sorting of polyadic π-calculus [26, 7, 43, 11, 36, 41, 24, 42, 14, 22], the type abstraction on the corresponding level is not possible in the monadic setting, which was left as an open issue by Milner [26]. We solve this problem with an extension of sorting which captures dynamic aspects of process behaviour in a simple way. Equationally this results in the full abstraction of the standard encoding of polyadic π-calculus into the monadic one: the sorted polyadic π-terms are equated by the basic behavioural equality in the polyadic calculus if and only if their encodings are equated in the basic typed behavioural equality in the monadic calculus. This is the first result of this kind we know of in the context of the encoding of polyadic name passing, which is a typical example of translation of high-level communication structures into π-calculus. The construction is general enough to be extendable to encodings of calculi with more complex operational structures.	denotational semantics;extensibility;high- and low-level;mobile agent;sorting	Nobuko Yoshida	1996		10.1007/3-540-62034-6_64	combinatorics;discrete mathematics;computer science;mathematics;community structure;algorithm	PL	-10.864421223209094	19.804336317279436	141235
98ad6aa9248c3d39a2e362cbd8994c453b88634f	specifying monogenetic specializers by means of a relation between source and residual programs	driving;natural semantics;specialization;specification;inference rule;correctness;program equivalence;partial evaluation;functional language;supercompilation	A specification of a class of specializers for a tiny functional language in form of natural semantics inference rules is presented. The specification defines a relation between source and residual programs with respect to an initial configuration (a set of input data). The specification expresses the idea of what is to be a specialized program, avoiding where possible the details of how a specializer builds it. In particular, it abstracts from the difference between on-line and off-line specialization.#R##N##R##N#The class of specializers specified here is limited to monogenetic specializers, which includes specializers based upon partial evaluation as well as restricted supercompilation. The specification captures such supercompilation notions as configuration, driving, generalization of a configuration, and a simple case of splitting a configuration.#R##N##R##N#The proposed specification is an intensional definition of equivalence between source and residual programs. It provides a shorter cut for proving the correctness and other properties of specializers than usual reduction to the extensional equivalence of programs does.		Andrei V. Klimov	2006		10.1007/978-3-540-70881-0_22	discrete mathematics;mathematics;programming language;algorithm	SE	-15.286935508718543	19.090437723954693	141289
8c88dde643c25ca467227cc53442bb37cd6bc78a	a comparison of presburger engines for efsm reachability	arithmetique ordinateur;theorie automate;maquina estado finito;computer arithmetic;formal verification;presburger arithmetic;decision procedure;extended finite state machine;informatique theorique;aritmetica ordenador;automata theory;verification formelle;teoria automata;machine etat fini;finite state machine;reachability analysis;analyse atteignabilite;computer theory;informatica teorica	Implicit state enumeration for extended finite state machines relies on a decision procedure for Presburger arithmetic. We compare the performance of two Presburger packages, the automata-based Shasta package and the polyhedrabased Omega package. While the raw speed of each of these two packages can be superior to the other by a factor of 50 or more, we found the asymptotic performance of Shasta to be equal or superior to that of Omega for the experiments we performed.	automata theory;automaton;boolean algebra;computer performance;decision problem;experiment;finite-state machine;function representation;kerrison predictor;omega;polyhedron;presburger arithmetic;reachability;semantic-oriented programming	Thomas R. Shiple;James H. Kukula;Rajeev K. Ranjan	1998		10.1007/BFb0028752	extended finite-state machine;formal verification;computer science;theoretical computer science;presburger arithmetic;automata theory;finite-state machine;programming language;algorithm	Logic	-16.051419758699417	27.745670732044132	141337
cb332334b11c012a64637696eae907cbf715c414	driving a sound static software analyzer with branch-and-bound	manuals;program diagnostics;algorithm design and analysis abstracts approximation algorithms conferences approximation methods manuals vectors;approximation algorithms;program slicing sound static software analyzer branch and bound algorithm interval arithmetic simultaneous propagation approximations static analysis c programs sound value analyzer spalter frama c plug in skelboe moore algorithm;approximation theory;tree searching approximation theory c language program diagnostics;c language;vectors;abstracts;approximation methods;tree searching;algorithm design and analysis;conferences	During the last decade, static analyzers of source code have improved greatly. Today, precise analyzers that propagate values for the program's variables, for instance with interval arithmetic, are used in the industry. The simultaneous propagation of sets of values, while computationally efficient, is a source of approximations, and ultimately of false positives. When the loss of precision is detrimental to the user's goals, a user needs to provide some kind of manual guidance. Frama-C, a framework for the static analysis of C programs, provides a sound value analyzer. This analyzer can optionally be guided by skillfully placed user annotations. This article describes SPALTER, a Frama-C plug-in that uses a variation of the Skelboe-Moore algorithm from the field of interval arithmetic to guide Frama-C's value analyzer towards a high-level objective set by the user. SPALTER reproduces the results of a case study that used Frama-C's value analysis and required extensive manual guidance. In difference, our approach with SPALTER required no guidance, except preparation of the analyzed program by slicing.	abstract interpretation;algorithm;algorithmic efficiency;approximation;branch and bound;computation;frama-c;function pointer;high- and low-level;interval arithmetic;plug-in (computing);prototype;semiconductor industry;single-precision floating-point format;software propagation;static program analysis	Sven Mattsen;Pascal Cuoq;Sibylle Schupp	2013	2013 IEEE 13th International Working Conference on Source Code Analysis and Manipulation (SCAM)	10.1109/SCAM.2013.6648185	algorithm design;computer science;theoretical computer science;software engineering;programming language;approximation algorithm;algorithm;approximation theory	SE	-17.154282883955037	30.793645653854252	141416
4b172ec6efa426916c377cd6b14db80377965a43	coinductive interpreters for process calculi	langage fonctionnel;lenguaje programacion;process calculi;programming language;coinductive types;lenguaje funcional;simultaneidad informatica;functional programming;process calculus;coinductive type;concurrency;algebra proceso;algebre processus;langage programmation;type coinductif;programmation fonctionnelle;process algebra;functional language;programacion funcional;simultaneite informatique;functional programming language;applications	This paper suggests functional programming languages with coinductive types as suitable devices for prototyping process calculi. The proposed approach is independent of any particular process calculus and makes explicit the different ingredients present in the design of any such calculi. In particular structural aspects of the underlying behaviour model (e.g., the dichotomies such as active vs reactive, deterministic vs nondeterministic) become clearly separated from the interaction structure which defines the synchronisation discipline. The approach is illustrated by the detailed development in Charity of an interpreter for a family of process languages.	charity;coinduction;functional programming;process calculus;programming language;software prototyping	Luís Soares Barbosa;José Nuno Oliveira	2002		10.1007/3-540-45788-7_11	process calculus;computer science;mathematics;programming language;functional programming;algorithm	PL	-11.205124381188359	21.390516783040372	141676
4a71e9823929d248962699346fef345bd5889aa9	a call-by-need strategy for higher-order functional-logic programming.	functional programming;higher order;higher order functions;logic programs	We present an approach to truely higher order functional logic programming based on higher order narrowing Roughly speaking we model a higher order functional core language by higher order rewriting and extend it by logic variables For the integration of logic programs conditional rules are supported For solving goals in this framework we present a complete calcu lus for higher order conditional narrowing We develop several re nements that utilize the determinism of functional programs These re nements can be combined to a narrowing strategy which generalizes call by need as in functional programming where the dedicated higher order methods are only used for full higher order goals Furthermore we propose an implementa tional model for this narrowing strategy which delays computations until needed	computation;functional logic programming;functional programming;lazy evaluation;rewriting	Christian Prehofer	1995			concurrent constraint logic programming;constraint programming;declarative programming;programming domain;reactive programming;functional reactive programming;functional logic programming;programming paradigm;procedural programming;inductive programming;fifth-generation programming language	PL	-17.103554251496547	19.45944277036873	141680
49b17dc30133c50405d31f8b2a9f58f77e0e9f0d	a verification system for interval-based specification languages	timed interval calculus;formal model;formal specification language;discrete time;formal specification languages;specification language;theorem proving;theorem prover;duration calculus;prototype verification system;formal analysis;real time computing;real time systems	Interval-based specification languages have been used to formally model and rigorously reason about real-time computing systems. This usually involves logical reasoning and mathematical computation with respect to continuous or discrete time. When these systems are complex, analyzing their models by hand becomes error-prone and difficult. In this article, we develop a verification system to facilitate the formal analysis of interval-based specification languages with machine-assisted proof support. The verification system is developed using a generic theorem prover, Prototype Verification System (PVS). Our system elaborately encodes a highly expressive set-based notation, Timed Interval Calculus (TIC), and can rigorously carry out the verification of TIC models at an interval level. We validated all TIC reasoning rules and discovered subtle flaws in the original rules. We also apply TIC to model Duration Calculus (DC), which is a popular interval-based specification language, and thus expand the capacity of the verification system. We can check the correctness of DC axioms, and execute DC proofs in a manner similar to the corresponding pencil-and-paper DC arguments.	acm transactions on software engineering and methodology;automated theorem proving;cognitive dimensions of notations;computation;correctness (computer science);decision problem;duration calculus;entity–relationship model;experiment;expressive power (computer science);heuristic (computer science);high- and low-level;ieee transactions on software engineering;level of measurement;mathematical induction;numerical analysis;peano axioms;prototype verification system;real-time computing;real-time locating system;real-time transcription;requirement;specification language;universal instantiation	Chunqing Chen;Jin Song Dong;Jun Sun;Andrew P. Martin	2010	ACM Trans. Softw. Eng. Methodol.	10.1145/1734229.1734232	formal methods;specification language;formal verification;computer science;theoretical computer science;formal specification;automated theorem proving;programming language;intelligent verification;algorithm;language of temporal ordering specification	Logic	-18.129390425938723	27.56924670311408	141681
6248e2738b245f398502666dd20a199d6554e76c	on-stack replacement, distilled		On-stack replacement (OSR) is essential technology for adaptive optimization, allowing changes to code actively executing in a managed runtime. The engineering aspects of OSR are well-known among VM architects, with several implementations available to date. However, OSR is yet to be explored as a general means to transfer execution between related program versions, which can pave the road to unprecedented applications that stretch beyond VMs. We aim at filling this gap with a constructive and provably correct OSR framework, allowing a class of general-purpose transformation functions to yield a special-purpose replacement. We describe and evaluate an implementation of our technique in LLVM. As a novel application of OSR, we present a feasibility study on debugging of optimized code, showing how our techniques can be used to fix variables holding incorrect values at breakpoints due to optimizations.	adaptive optimization;algorithm;bisimulation;breakpoint;compiler;control flow;correctness (computer science);debugger;debugging;dynamic compilation;general-purpose markup language;graal;llvm;mathematical optimization;observational equivalence;open-source religion;petri net;programming language design and implementation;recursion;tiling window manager;turing completeness;z/vm	Daniele Cono D'Elia;Camil Demetrescu	2018		10.1145/3192366.3192396	theoretical computer science;breakpoint;computer science;implementation;constructive;debugging;adaptive optimization	PL	-18.22535854648488	30.664176521163498	141742
0619d282dff28b5101d4dce4d2024c7e944c20d3	bounding linear head reduction and visible interaction through skeletons		In this paper, we study the complexity of execution in higher-order programming languages. Our study has two facets: on the one hand we give an upper bound to the length of interactions between bounded P-visible strategies in Hyland-Ong game semantics. This result covers models of programming languages with access to computational effects like non-determinism, state or control operators, but its semantic formulation causes a loose connection to syntax. On the other hand we give a syntactic counterpart of our semantic study: a non-elementary upper bound to the length of the linear head reduction sequence (a low-level notion of reduction, close to the actual implementation of the reduction of higher-order programs by abstract machines) of simply-typed λ-terms. In both cases our upper bounds are proved optimal by giving matching lower bounds. These two results, although different in scope, are proved using the same method: we introduce a simple reduction on finite trees of natural numbers, hereby called interaction skeletons. We study this reduction and give upper bounds to its complexity. We then apply this study by giving two simulation results: a semantic one measuring progress in game-theoretic interaction via interaction skeletons, and a syntactic one establishing a correspondence between linear head reduction of terms satisfying a locality condition called local scope and the reduction of interaction skeletons. This result is then generalized to arbitrary terms by a local scopization transformation.	game semantics;game theory;high- and low-level;higher-order programming;interaction;lawrence a. hyland;local variable;locality of reference;nondeterministic algorithm;programming language;simulation	Pierre Clairambault	2015	Logical Methods in Computer Science	10.2168/LMCS-11(2:6)2015	combinatorics;discrete mathematics;computer science;mathematics;algorithm	PL	-10.675844544198828	20.131534634523486	141757
b6004d7077c78324c9436a2b2d5f463e99b681fe	precise thread-modular verification	polynomial complexity;modular verification;concurrent programs;high efficiency	Thread-modular verification is a promising approach for the verification of concurrent programs. Its high efficiency is achieved by abstracting the interaction between threads. The resulting polynomial complexity (in the number of threads) has its price: many interesting concurrent programs cannot be handled due to the imprecision of the abstraction. We propose a new abstraction algorithm for threadmodular verification that offers both high degree precision and polynomial complexity. Our algorithm is based on a new abstraction domain that combines Cartesian abstraction with exception sets, which allow one to handle particular thread interactions precisely. Our experimental results demonstrate the practical applicability of the algorithm.	algorithm;interaction;polynomial;time complexity	Alexander Malkis;Andreas Podelski;Andrey Rybalchenko	2007		10.1007/978-3-540-74061-2_14	computer science;theoretical computer science;high-level verification;programming language;algorithm;functional verification	Logic	-13.600155572804892	24.73555859825606	141994
f442b6cf42a2592fedf6ddd8df1be198f5b827e5	an approach to effective model-checking of real-time finite-state machines in mu-calculus	real time;fixed point;model checking;finite state machine	The paper presents two main results. The first one is a polynomial Model-Checker (PMC) for a new representative subclass of formulae of the propositional mu-calculus. Formulae in this class have some discipline of alternation of fixed points. The other result extends the model checking techniques to the so-called semilinear class of finite Kripke structures induced by Finite-State Machines with multiple clock functioning in real time.	finite-state machine;modal μ-calculus;model checking;real-time clock	Sergey Berezin;Nikolay V. Shilov	1994		10.1007/3-540-58140-5_6	model checking;combinatorics;discrete mathematics;real-time computing;computer science;mathematics;fixed point;finite-state machine;abstraction model checking;algorithm;virtual finite-state machine	Logic	-10.40615112426996	24.668229312899904	142120
0f55c5fc2dd9b6131672c5942cb36bf289a6d5e1	composition and decomposition of dpo transformations with borrowed context	distributed system;transformacion grafo;systeme reparti;labelled transition system;inductive definition;graph transformation;systeme transition etiquete;congruencia;transformation graphe;sistema repartido;system analysis;localized state;congruence;sistema transicion marcada	Double-pushout (DPO) transformations with borrowed context extend the standard DPO approach by allowing part of the graph needed in a transformation to be borrowed from the environment. The bisimilarity based on the observation of borrowed contexts is a congruence, thus facilitating system analysis. In this paper, focusing on the situation in which the states of a global system are built out of local components, we show that DPO transformations with borrowed context defined on a global system state can be decomposed into corresponding transformations on the local states and vice versa. Such composition and decomposition theorems, developed in the framework of adhesive categories, can be seen as a first step towards an inductive definition, in sos style, of the labelled transition system associated to a graph transformation system. As a special case we show how an ordinary DPO transformation on a global system state can be decomposed into local DPO transformations with borrowed context using the same production.	bisimulation;comparison of raster-to-vector conversion software;congruence of squares;gnutella2;graph rewriting;local variable;process calculus;recursive definition;springer (tank);symposium on logic in computer science;system analysis;transition system;wilfried brauer	Paolo Baldan;Hartmut Ehrig;Barbara König	2006		10.1007/11841883_12	discrete mathematics;calculus;congruence;mathematics;system analysis;algorithm	PL	-8.442597160940382	22.763502994445805	142165
25d73554adba0c7d3f4284bc3a78ac3c64cff6e2	knowledge for the distributed implementation of constrained systems		"""Problem to be solved: """" Given a centralized specification PN and a constraint Ψ, Derive a distributed implementation I for PN controlled by Ψ """" Our hypotheses: centralized specification PN: w.l.g. Petri Nets distributed setting: one process per location — can learn about each other only via communication mechanisms provided by the platform constraint Ψ: a safety constraint (here: priorities) Not considered in this talk:"""	centralized computing;petri net	Susanne Graf;Sophie Quinton	2013		10.1007/978-3-642-38613-8_6	implementation;theoretical computer science;computer science;correctness;mathematical proof	Embedded	-6.388982916965441	26.6700012963281	142261
00ac91016bc808a650e7804445aa2de49a947c23	branching-time versus linear-time - a cooperative and feasible approach	linear time	A new temporal logic called linear-time computation tree logic (LCTL) is obtained from computation tree logic (CTL) by adding some modified versions of the temporal operators of linear-time temporal logic (LTL). A theorem for embedding LCTL into CTL is proved. The model-checking, validity and satisfiability problems of LCTL are shown to be deterministic PTIME-complete, EXPTIME-complete and deterministic EXPTIMEcomplete, respectively.	computation tree logic;exptime;karp's 21 np-complete problems;linear temporal logic;model checking;p (complexity);time complexity	Norihiro Kamide	2010			time complexity;machine learning;artificial intelligence;computer science;branching (version control)	Logic	-6.799809229331584	23.39092199190836	142358
1c13d278ec8781e2d66179d5924f23d971bae87d	equilibrium and termination ii: the case of petri nets	computacion informatica;ciencias basicas y experimentales;matematicas;grupo a	This paper is concerned with the asymptotic properties of a restricted class of Petri nets equipped with stochastic mass action semantics. We establish a simple algebraic criterion for the existence of an equilibrium, that is to say an invariant probability that satisfies the detailed balance condition familiar from the thermodynamics of reaction networks. We also find that when such a probability exists, it can be described by a free energy function which combines an internal energy term and an entropy one. Under strong additional conditions, we show how the entropy term can be deconstructed using the finer-grained individual-token semantics of Petri nets.	action semantics;angular defect;approximation;axiomatic system;bellman equation;computable function;concurrency (computer science);confluence;detailed balance;entropy (information theory);linear algebra;logic programming;markov chain;mass action law (electronics);mathematical optimization;maximal set;petri net;reachability;rewriting;undecidable problem	Vincent Danos;Nicolas Oury	2013	Mathematical Structures in Computer Science	10.1017/S0960129512000126	discrete mathematics;calculus;mathematics;petri net;algorithm	Logic	-10.196074790697546	18.288396283344643	142699
fee34a910824be6ac64d730fd7efb6a8d3e1ef99	from jinja bytecode to term rewriting: a complexity reflecting transformation		Abstract In this paper we show how the runtime complexity of imperative programs can be analysed fully automatically by a transformation to term rewrite systems , the complexity of which can then be automatically verified by existing complexity tools. We restrict to well-formed Jinja bytecode programs that only make use of non-recursive methods. The analysis can handle programs with cyclic data only if the termination behaviour is independent thereof. We exploit a term-based abstraction of programs within the abstract interpretation framework. The proposed transformation encompasses two stages. For the first stage we perform a combined control and data flow analysis by evaluating program states symbolically, which is shown to yield a finite representation of all execution paths of the given program through a graph, dubbed computation graph . In the second stage we encode the (finite) computation graph as a term rewrite system. This is done while carefully analysing complexity preservation and reflection of the employed transformations such that the complexity of the obtained term rewrite system reflects on the complexity of the initial program. Finally, we show how the approach can be automated and provide ample experimental evidence of the advantages of the proposed analysis.	jinja (template engine);rewriting	Georg Moser;Michael Schaper	2018	Inf. Comput.	10.1016/j.ic.2018.05.007	discrete mathematics;mathematics;bytecode;abstract interpretation;computation;exploit;data-flow analysis;restrict;rewriting;abstraction	Logic	-17.53476921659414	23.324893836774915	142745
f47361a5e617e0a3239c81c7ced80a3a8d613dc5	application of a syntax driver to logic equation processing and data-control card scanning	design automation	This paper will describe the use of a syntax driver in writing a logic equation compiler (LOGCOM). By such a compiler or processor we mean a program which takes logic equations punched on cards and converts them into a tabular form which subsequently is used by other design automation programs. In the body of the paper it is assumed that the reader possesses a casual familiarity with syntax. This assumption is made mostly for organizational convenience and an appendix discussing syntax is included at the end of the paper. My terminology is summarized at the end of this appendix and the reader familiar with syntax can check there to see if his terminology and mine agree. Sample syntax applied to a specific problem is shown in figure 3 and is discussed in Section 2.	compiler;punched card;table (information)	Jock A. Rader	1968		10.1145/800167.805406	embedded system;abstract syntax;electronic engineering;electronic design automation;telecommunications;computer science;engineering;electrical engineering;compiler construction;mathematics;programming language;homoiconicity;algorithm;abstract syntax tree;syntax error	EDA	-13.896603043504365	31.654474806126768	143027
8070397529168b5bd1dd481780f1af1d7fc99eca	expert system benchmarks	expert systems;knowledge based systems;performance evaluation;program testing;lisp-based shell;pc plus;artificial;backward chaining;development environment;execution time;expert system shells;file size;inference engine;loading;memory requirements;real knowledge base;realistic benchmark;stylized benchmark knowledge base	Benchmarks for use with expert system shells are considered. Two approaches are pursued: running a shell with a real knowledge base on different machines, and running an artificial, stylized benchmark knowledge base with different shells on the same machine. The realistic benchmark uses PC Plus, a Lisp-based shell and development environment with an inference engine that uses backward chaining (forward chaining is also possible). Stylized knowledge bases have been used with different shells to compare loading and execution time, file size, and memory requirements. Some stylized-benchmark results are given.<<ETX>>	backward chaining;benchmark (computing);expert system;forward chaining;inference engine;knowledge base;lisp;requirement;run time (program lifecycle phase)	Larry Press	1989	IEEE Expert	10.1109/64.21898	data mining;knowledge-based systems;computer science;knowledge base;inference engine;lisp;expert system;forward chaining;legal expert system;backward chaining	HPC	-9.72291431181017	32.192933898495795	143129
6f0c20d22715670636ee7f7a87d31b0a97709b32	natural rewriting for general term rewriting systems	theorem proving;certain application;left-linear constructor system;conservative generalization;logic programming;general term;expressive power;functional programming language;equational language	We address the problem of an efficient rewriting strategy for general term rewriting systems. Several strategies have been proposed over the last two decades for rewriting, the most efficient of all being the natural rewriting strategy [9]. All the strategies so far, including natural rewriting, assume that the given term rewriting system is a left-linear constructor system. Although these restrictions are reasonable for some functional programming languages, they limit the expressive power of equational languages, and they preclude certain applications of rewriting to equational theorem proving and to languages combining equational and logic programming. In this paper, we propose a conservative generalization of natural rewriting that does not require the rules to be left-linear and constructor-based. We also establish the soundness and completeness of this generalization.	analysis of algorithms;automated theorem proving;compiler;computation;concurrency (computer science);correctness (computer science);database normalization;functional programming;logic programming;modulo operation;programming language;recursion;rewrite (programming);rewriting;serial digital video out	Santiago Escobar;José Meseguer;Prasanna Thati	2004		10.1007/11506676_7		Logic	-17.363732285029133	19.82734939442494	143278
49f4c1b9b6d072a14b439d42d6f6017567275c96	weak observable liveness and infinite games on finite graphs		The notion of observable liveness was introduced in the literature for 1-safe Petri net systems in which transitions are either observable or unobservable by a user and, among the observable ones, some are controllable, in the sense that they correspond to interactions with the user and cannot autonomously occur. An observable transition is observably live if a user can, from any reachable marking, force it to occur infinitely often by using controllable transitions. Here, we introduce a weaker version of this notion by considering the capability of the user, by means of controllable transitions, to force the considered observable transition to fire infinitely often, starting from the initial marking instead of considering each reachable marking. The main result of this paper is a method for checking weak observable liveness in state machine decomposable 1-safe nets whose transitions are observable. The introduced method is based on infinite games that are played on finite graphs. We transform the problem of weak observable liveness into a game between a system and a user, and we prove that a transition is weakly observably live if and only if the user has a winning strategy for the game.	liveness;observable	Luca Bernardinello;Görkem Kilinç;Lucia Pomello	2017		10.1007/978-3-319-57861-3_12	combinatorics;discrete mathematics;mathematics;distributed computing	Logic	-7.130172878430942	22.790410365949622	143593
9e0a9be9999494382942f2b1d1a8b864d2ef6f3e	approximate model-based diagnosis using preference-based compilation	modelizacion;metodo caso peor;compilacion;mode transfert asynchrone;model based diagnosis;interrogation base donnee;diagnostico;interrogacion base datos;abstraction;abstraccion;constraint satisfaction;modelisation;satisfaction contrainte;model approximation;inferencia;methode cas pire;preferencia;compilation;preference;constraint satisfaction problem;satisfaccion restriccion;diagnosis;modeling;worst case method;database query;inference;asynchronous transfer mode;diagnostic;modo transferencia asincrono	This article introduces a technique for improving the efficiency of diagnosis through approximate compilation. We extend the approach of compiling a diagnostic model, as is done by, for example, an ATMS, to compiling an approximate model. Approximate compilation overcomes the problem of space required for the compilation being worst-case exponential in particular model parameters, such as the path-width of a model represented as a Constraint Satisfaction Problem. To address this problem, we compile the subset of most “preferred” (or most likely) diagnoses. For appropriate compilations, we show that significant reductions in space (and hence on-line inference speed) can be achieved, while retaining the ability to solve the majority of most preferred diagnostic queries. We experimentally demonstrate that such results can be obtained in real-world problems.	approximation algorithm;best, worst and average case;compiler;constraint satisfaction problem;cryptographic service provider;experiment;online and offline;pathwidth;time complexity	Gregory M. Provan	2005		10.1007/11527862_13	mathematical optimization;simulation;systems modeling;constraint satisfaction;computer science;artificial intelligence;asynchronous transfer mode;abstraction;constraint satisfaction problem;algorithm	AI	-7.590117572203261	20.022466843967873	143998
8fe343fe569a889e4e7c50ef4b917a5fbb524db0	on hybrid systems and the modal µ-calculus	verification;systeme temps reel;analyse modale;formal specification;analisis sistema;sistema hibrido;dynamic systems theory;analisis modal;automata estado finito;temporal logic;value analysis;set theory;mathematical logic;computer logic;specification formelle;especificacion formal;automata;modal logic;mathematical models;systems analysis;state space;transition systems;hybrid system;formal logic;reactive system;system analysis;real time system;value function;analyse systeme;finite automaton;sistema tiempo real;hybrid automata;automate fini;modal analysis;verificacion;formal analysis;applied mathematics;computer program verification;hybrid systems;systeme hybride;real time systems;symbolic representation	"""We start from a basic and fruitful idea in current work on the formal analysis and veriication of hybrid and real-time systems: the uniform representation of both sorts of state dynamics { both continuous evolution within a control mode, and the eeect of discrete jumps between control modes { as abstract transition relations over a hybrid space X Q R n , where Q is a nite set of control modes. The resulting \machine"""" or transition system model is currently analyzed using the resources of concurrent and reactive systems theory and temporal logic veriication, abstracted from their original setting of nite state spaces and purely discrete transitions. One such resource is the propositional-calculus: a richly expressive formal logic of transition system models (of arbitrary cardinality), which subsumes virtually all temporal and modal logics. The key move here is to view the transition system models of hybrid automata not merely as some form of \discrete abstraction"""", but rather as a skeleton which can be eshed out by imbuing the state space with topological, metric tolerance or other structure. Drawing on the resources of modal logics, we give explicit symbolic representation to such structure in polymodal logics extending the modal-calculus. The result is a logical formalism in which we can directly and simply express continuity properties of transition relations and metric tolerance properties such as \being within distance """" of a set. Moreover, the logics have sound and complete deductive proof systems, so assumptions of continuity or tolerance can be used as hypotheses in deductive veri-cation. By also viewing transition relations in their equivalent form as set-valued functions, and drawing on the resources of set-valued analysis and dynamical systems theory, we open the way to a richer formal analysis of robustness and stability for hybrid automata and related classes of systems."""	automata theory;dynamical systems theory;formal system;hybrid automaton;hybrid system;modal logic;modal μ-calculus;propositional calculus;real-time clock;real-time computing;scott continuity;state space;temporal logic;transition system	Jennifer M. Davoren	1997		10.1007/3-540-49163-5_3	discrete mathematics;real-time operating system;computer science;artificial intelligence;mathematics;distributed computing;finite-state machine;algorithm;hybrid system	Logic	-10.271908435600738	23.21587945344069	144107
10647f8cb0dab5d8fc82466301d7460a3f587ec8	linear ordering on graphs, anti-founded sets and polynomial time computability	strongly extensional graphs;graph theory;graphe lineaire;linear order;teoria grafo;anti foundation axiom;algorithm complexity;temps polynomial;complexite calcul;complejidad algoritmo;language theory;computability;teoria conjunto;cerradura transitiva;linear ordering;theorie ensemble;bisimulation;teoria lenguaje;natural extension;set theory;theorie graphe;grafo lineal;complejidad computacion;complexite algorithme;fermeture transitive;computational complexity;calculabilite;polynomial time;bounded set theory;transitive closure;descriptive complexity;theorie langage;calculabilidad;linear graph;tiempo polinomial;capturing ptime	It is proved deenability in FO+IFP of a global linear ordering on vertices of strongly extensional (SE) nitely-branching graphs. In the case of nite SE graphs this also holds for FO+LFP. This gives capturing results for PTIME computability on the latter class of graphs by FO+LFP and FO+IFP, and also on the corresponding anti-founded universe HFA of hereditarily-nite sets by a language of a bounded set theory BSTA. Oracle PTIME computability over HFA is also captured by an appropriate extension of the language by predicate variables and a bounded 2-recursion schema. It is also characterized the type of corresponding linear ordering on the universe HFA and on its natural extension HFA 1 consisting of hereditarily-nite anti-founded sets with possibly innnite (unlike HFA) transitive closures.	computability;p (complexity);set theory;time complexity;transitive closure	Alexei Lisitsa;Vladimir Yu. Sazonov	1999	Theor. Comput. Sci.	10.1016/S0304-3975(98)00312-0	time complexity;combinatorics;discrete mathematics;computer science;bisimulation;philosophy of language;graph theory;mathematics;linear equation;computability;computational complexity theory;transitive closure;total order;algorithm;descriptive complexity theory;set theory;algebra	Logic	-6.433707430843078	18.30296997197442	144224
9c0e091cc653f9b224d5582ae5c6d79f6669ef1f	verisimpl 2: an open-source software for the verification of max-plus-linear systems	piece wise affine systems;difference bound matrices;model checking;bisimulations;transition systems;discrete event systems;max plus algebra;model abstractions	This work presents a technique to generate finite abstractions of autonomous Max-Plus-Linear (MPL) systems, a class of discrete-event systems employed to characterize the dynamics of the timing related to the synchronization of successive events. Abstractions of MPL systems are derived as finite-state transition systems. A transition system is obtained first by partitioning the state space of the MPL system into finitely many regions and then by associating a unique state of the transition system to each partitioning region. Relations among the states of the transition system are then set up based on the underlying dynamical transitions between the corresponding partitioning regions of the MPL state space. In order to establish formal equivalences, the obtained finite abstractions are proven either to simulate or to bisimulate the original MPL system. The approach enables the study of general properties of the original MPL system formalised as logical specifications, by verifying them over the finite abstraction via model checking. The article presents a new, extended and improved implementation of a software tool (available online) for the discussed formal abstraction of MPL systems, and is tested on a numerical benchmark against a previous version.	atomic sentence;benchmark (computing);binary decision diagram;finite-state machine;linear system;model checking;modulo operation;multiphoton lithography;numerical analysis;open-source software;programming tool;recursion (computer science);refinement (computing);rewriting;satisfiability modulo theories;simulation;space partitioning;state space;state transition table;theory;transition system;tree structure;verification and validation	Dieky Adzkiya;Yining Zhang;Alessandro Abate	2016	Discrete Event Dynamic Systems	10.1007/s10626-015-0218-x	model checking;discrete mathematics;computer science;theoretical computer science;mathematics;algorithm	Logic	-12.389642100064652	26.487363479098338	144259
0eb3097af4e0bc45cfdff4f69b80f5b888224c04	productive coprogramming with guarded recursion	total functional programming;guarded recursion;electronic computers computer science;corecursion;coalgebras	"""Total functional programming offers the beguiling vision that, just by virtue of the compiler accepting a program, we are guaranteed that it will always terminate. In the case of programs that are not intended to terminate, e.g., servers, we are guaranteed that programs will always be productive. Productivity means that, even if a program generates an infinite amount of data, each piece will be generated in finite time. The theoretical underpinning for productive programming with infinite output is provided by the category theoretic notion of final coalgebras. Hence, we speak of coprogramming with non-well-founded codata, as a dual to programming with well-founded data like finite lists and trees.  Systems that offer facilities for productive coprogramming, such as the proof assistants Coq and Agda, currently do so through syntactic guardedness checkers. Syntactic guardedness checkers ensure that all self-recursive calls are guarded by a use of a constructor. Such a check ensures productivity. Unfortunately, these syntactic checks are not compositional, and severely complicate coprogramming.  Guarded recursion, originally due to Nakano, is tantalising as a basis for a flexible and compositional type-based approach to coprogramming. However, as we show, by itself, guarded recursion is not suitable for coprogramming due to the fact that there is no way to make finite observations on pieces of infinite data. In this paper, we introduce the concept of clock variables that index Nakano's guarded recursion. Clock variables allow us to """"close over"""" the generation of infinite data, and to make finite observations, something that is not possible with guarded recursion alone."""	agda;applicative programming language;category theory;causality;clock rate;compiler;computation;constructor (object-oriented programming);coq (software);corecursion;intuitionistic type theory;iteration;lars bak (computer programmer);model checking;primitive recursive function;proof assistant;recursion;software documentation;terminate (software);total functional programming;tree (data structure)	Robert Atkey;Conor McBride	2013		10.1145/2500365.2500597	corecursion;computer science;theoretical computer science;mutual recursion;programming language;algorithm	Logic	-15.351964949488279	20.3922186844067	144482
1d3827fb6274f1c02cea3abb9506b78f5c46f605	increasing efficiency of symbolic model checking by accelerating dynamic variable reordering	binary decision diagrams symbol manipulation formal verification sequential circuits state space methods;symbol manipulation;time dependent;symbolic representation symbolic model checking dynamic variable reordering verification sequential circuits reactive systems protocols state spaces ordered binary decision diagrams;state space methods;sequential circuits;ordered binary decision diagram;formal verification;binary decision diagrams;model checking;state space;reactive system;symbolic model checking;state space explosion;acceleration power system modeling sequential circuits state space methods data structures boolean functions input variables microwave integrated circuits explosions sampling methods;symbolic representation	Model checking has been proven to be a powerful tool in verification of sequential circuits, reactive systems, protocols, etc. The model checking of systems with huge state spaces is possible only if there is a very efficient representation of the model. Ordered Binary Decision Diagrams (OBDDs) allow an efficient symbolic representation of the model. Our goal is to accelerate the variable reordering process but retaining good OBDD sizes. To obtain this, we adapted two methods introduced by Meinel and Slobodova called Block Restricted Sifting (BRS) and Sample Sifting to the needs of model checking.	brs/search;binary decision diagram;model checking	Christoph Meinel;Christian Stangier	1998	Design, Automation and Test in Europe Conference and Exhibition, 1999. Proceedings (Cat. No. PR00078)	10.1145/307418.307454	model checking;embedded system;formal verification;reactive system;computer science;state space;theoretical computer science;sequential logic;programming language;abstraction model checking;symbolic trajectory evaluation;algorithm	EDA	-12.638098529888985	27.347138198341288	144610
efee8e18547ccf342037767150c1002830fd98b2	the optimal approach to recursive programs	recursive programs;optimal fixedpoints;least fixedpoints;information embedding;proof techniques;fixedpoints	The classical fixedpoint approach toward recursive programs suggests choosing the “least defined fixedpoint” as the most appropriate solution to a recursive program. A new approach is described which introduces an “optimal fixedpoint,” which, in contrast to the least defined fixedpoint, embodies the maximal amount of valuable information embedded in the program. The practical implications of this approach are discussed and techniques for proving properties of optimal fixedpoints are given. The presentation is informal, with emphasis on examples.	embedded system;maximal set;recursion (computer science)	Zohar Manna;Adi Shamir	1977	Commun. ACM	10.1145/359863.359885	mathematical optimization;discrete mathematics;theoretical computer science;mathematics	PL	-13.716457270430404	23.137344690280518	144649
51084d27d701150680b9417cdf6ace8954a7aa69	using partial orders for the efficient verification of deadlock freedom and safety properties	state space exploration;concurrent programming;deadlock detection;safety properties;state space;state explosion;state space explosion;partial order	This paper presents an algorithm for detecting deadlocks in concurrent finite-state systems without incurring most of the state explosion due to the modeling of concurrency by interleaving. For systems that have a high level of concurrency our algorithm can be much more efficient than the classical exploration of the whole state space. Finally, we show that our algorithm can also be used for verifying arbitrary safety properties.	algorithm;concurrency (computer science);deadlock;forward error correction;high-level programming language;sensor;state space;verification and validation	Patrice Godefroid;Pierre Wolper	1993	Formal Methods in System Design	10.1007/BF01383879	partially ordered set;real-time computing;concurrent computing;computer science;state space;deadlock;distributed computing;programming language;deadlock prevention algorithms;algorithm	Logic	-16.857244039946554	28.322743187002736	144802
4cf0fca3815a565978a2a42f72f07fd1111dd01f	proof-producing reflection for hol - with an application to model polymorphism		We present a reflection principle of the form “If pφq is provable, then φ” implemented in the HOL4 theorem prover, assuming the existence of a large cardinal. We use the large-cardinal assumption to construct a model of HOL within HOL, and show how to ensure φ has the same meaning both inside and outside of this model. Soundness of HOL implies that if pφq is provable, then it is true in this model, and hence φ holds. We additionally show how this reflection principle can be extended, assuming an infinite hierarchy of large cardinals, to implement model polymorphism, a technique designed for verifying systems with self-replacement functionality.	automated proof checking;automated theorem proving;hol (proof assistant);provable security;self-modifying code;verification and validation	Benja Fallenstein;Ramana Kumar	2015		10.1007/978-3-319-22102-1_11		Logic	-15.049696872739549	19.557700275910314	145019
1899a1f7e622807bc40c3ea4c3bdbfab1969575d	salsa: an automatic tool to improve the numerical accuracy of programs		This article describes Salsa, an automatic tool to improve the accuracy of the floatingpoint computations done in numerical codes. Based on static analysis methods by abstract interpretation, our tool takes as input an original program, applies to it a set of transformations and then generates an optimized program which is more accurate than the initial one. The original and the transformed programs are written in the same imperative language. This article is a concise description of former work on the techniques implemented in Salsa, extended with a presentation of the main software architecture, the inputs and outputs of the tool as well as experimental results obtained by applying our tool on a set of sample programs coming from embedded systems and numerical analysis.	abstract interpretation;algorithm;code;computation;control point (mathematics);distributed computing;embedded system;imperative programming;mathematical optimization;numerical analysis;numerical method;partial template specialization;performance;run time (program lifecycle phase);salsa;series acceleration;software architecture;static program analysis;supercomputer	Nasrine Damouche;Matthieu Martel	2017			software engineering;theoretical computer science;salsa;computer science	Logic	-17.13323814766564	30.967585905250008	145048
37eb7deae9b2fdcfcd6c2a055dbc04f55b6d84a4	an efficient path-oriented bitvector encoding width computation algorithm for bit-precise verification	observability;controllability;non-uniform encoding width;efficient path-oriented bitvector;single-bit path-controlling variable;boolean algebra;huge search space;width computation algorithm;efficient algorithm;bit-blasting;nonuniform bitwidth encoding;verification model;constant value;bit-vector formula;path-oriented bitvector encoding width computation;path-intensive model;non-uniform bitwidth encoding;path selection;encoding width;bit-precise verification;formal verification;data mining;virtualization;encoding;search space;data analysis;benchmark testing;helium;scalability;acceleration;algorithm design and analysis;computational modeling;hardware;component	Bit-precise verification with variables modeled as bitvectors has recently drawn much interest. However, a huge search space usually results after bit-blasting. To accelerate the verification of bit-vector formulae, we propose an efficient algorithm to discover non-uniform encoding widths We of variables in the verification model, which may be smaller than their original modeling widths but sufficient to find a counterexample. Different from existing approaches, our algorithm is path-oriented, in that it takes advantage of the controllability and observability values in the structure of the model to guide the computation of the paths, their encoding widths and the effective adjustment of these widths in subsequent steps. For path selection, a subset of singlebit path-controlling variables is set to constant values. This can restrict the search from those paths deemed less favorable or have been checked in previous steps, thus simplifying the problem. Experiments show that our algorithm can significantly speed up the search by focusing first on those promising, easy paths for verifying those path-intensive models, with reduced, non-uniform bitwidth encoding.	algorithm;bit array;computation;speedup;verification and validation	Nannan He;Michael S. Hsiao	2009	2009 Design, Automation & Test in Europe Conference & Exhibition		boolean algebra;algorithm design;benchmark;mathematical optimization;virtualization;observability;controllability;formal verification;computer science;theoretical computer science;component;computational model;algorithm;encoding	EDA	-13.853152589344255	29.724790928400225	145110
954b479768e73bac3fe42a8261ccbba428bdf936	making abstract interpretations complete	derandomization;undirected graph connectivity;computational complexity;inf 01 informatica;abstract interpretation;space bounded computations;static program analysis;short pseudorandom walks on graphs	Completeness is an ideal, although uncommon, feature of abstract interpretations, formalizing the intuition that, relatively to the properties encoded by the underlying abstract domains, there is no loss of information accumulated in abstract computations. Thus, complete abstract interpretations can be rightly understood as optimal. We deal with both pointwise completeness, involving generic semantic operations, and (least) fixpoint completeness. Completeness and fixpoint completeness are shown to be properties that depend on the underlying abstract domains only. Our primary goal is then to solve the problem of making abstract interpretations complete by minimally extending or restricting the underlying abstract domains. Under the weak and reasonable hypothesis of dealing with continuous semantic operations, we provide constructive characterizations for the least complete extensions and the greatest complete restrictions of abstract domains. As far as fixpoint completeness is concerned, for merely monotone semantic operators, the greatest restrictions of abstract domains are constructively characterized, while it is shown that the existence of least extensions of abstract domains cannot be, in general, guaranteed, even under strong hypotheses. These methodologies, which in finite settings give rise to effective algorithms, provide advanced formal tools for manipulating and comparing abstract interpretations, useful both in static program analysis and in semantics design. A number of examples illustrating these techniques are given.	abstract interpretation;algorithm;computation;fixed point (mathematics);least fixed point;model checking;np-completeness;static program analysis;theory;turing completeness;monotone	Roberto Giacobazzi;Francesco Ranzato;Francesca Scozzari	2000	J. ACM	10.1145/333979.333989	mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;programming language;computational complexity theory;algorithm;static program analysis;algebra	PL	-9.063184495868821	18.85967743582049	145550
4c6f82f7f29fa23442d420b5b286f7af4c1f5562	information rich enough sample paths for machine identification	minimal realization;finite state machines identification discrete event systems;discrete event dynamic system;finite state machines;discrete event system;information rich enough sample path machine identification discrete event dynamic system online modeling refinement language equivalence machine reconstruction minimal valid automata algorithm;identification;discrete event systems;automata context modeling discrete event systems formal languages system testing automatic testing whales councils virtual manufacturing;machine model	Machine identification of discrete event dynamic systems is to reconstruct machine models by a finite length sample path from an unknown machine target. Previous study of online modeling refinement shows that, with increased sample path in length, the reconstructed machines approach the unknown machine in the sense of language equivalence. However, it is not guaranteed that continuingly increased sample path always results in the exact machine reconstruction. In this study, we show that when the unknown target is persistently identifiable, there always exists an information rich enough sample path that uniquely defines the identification target. With the previously reported minimal valid automata algorithm that derives the minimal realization of unknown target, the information rich enough sample path serves as an equivalent representation of the given machine.	algorithm;automata theory;dynamical system;refinement (computing);turing completeness	Sheng-Luen Chung;Chung-Lun Li	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1399863	identification;discrete mathematics;minimal realization;discrete event dynamic system;computer science;theoretical computer science;machine learning;control theory;mathematics;event-driven finite-state machine;event;finite-state machine	Embedded	-7.156623753579637	27.289848415007192	145599
58af18308ffd1a31ad176dd874ad529083f7c00f	abstraction refinement techniques for software model checking			model checking;refinement (computing)	Mohamed Nassim Seghir	2010			programming language;abstraction model checking;model checking;symbolic trajectory evaluation;software;abstraction;refinement calculus;computer science	Logic	-17.9849169344155	27.039116174415316	145712
91b9640e2c0775efaa2376895134fa1fa0fdf09f	representability and program construction within fork algebras	binary relation;representation theorem;algorithm design;program development	The representation theorem for fork algebras was always misunderstood regarding its applications in program construction. Its application was always described as “the portability of properties of the problem domain into the abstract calculus of fork algebras”. In this paper we show that the results provided by the representation theorem are by far more important. We show that not only the heuristic power coming from concrete binary relations is captured inside the abstract calculus, but also design strategies for program development can be successfully expressed. This result makes fork algebras a programming calculus by far more powerful than it was previously thought.	fork (software development);heuristic;problem domain;software portability	Marcelo F. Frias;Gabriel Baum;Armando Martin Haeberer	1998	Logic Journal of the IGPL	10.1093/jigpal/6.2.227	algorithm design;combinatorics;discrete mathematics;computer science;binary relation;mathematics;algebra	Logic	-16.11958588226426	21.00294712873085	145837
f495b30a771807b205e6b35be3d96e6a5b275259	open petri nets: non-deterministic processes and compositionality	petri net;composition operator	We introduce ranked open nets, a reactive extension of Petri nets which generalises a basic open net model introduced in a previous work by allowing for a refined notion of interface. The interface towards the external environment of a ranked open net is given by a subset of places designated as open and used for composition. Additionally, a bound on the number of connections which are allowed on an open place can be specified. We show that the non-deterministic process semantics is compositional with respect to the composition operation over ranked open nets, a result which did not hold for basic open nets.	causality;graph rewriting;non-deterministic turing machine;openness;petri net;state space;theory;unfolding (dsp implementation)	Paolo Baldan;Andrea Corradini;Hartmut Ehrig;Barbara König	2008		10.1007/978-3-540-87405-8_18	discrete mathematics;stochastic petri net;computer science;composition operator;database;distributed computing;process architecture;petri net	Logic	-9.832982390072988	21.79418257928737	145846
6c9bad690d3a72660c446c663e46e566e0e826b1	combining decision algorithms for matching in the union of disjoint equational theories	decision tree;restriction ordre;arbol decision;algorithme;combinatorial problem;probleme combinatoire;order restriction;problema combinatorio;pattern matching;algorithms;concordance forme;arbre decision	This paper addresses the problem of systematically building a matching algorithm for the union of two disjoint theories E1 _ E2 provided that matching algorithms are known in both theories E1 and E2 . In general, the blind use of combination techniques introduces unification. Two different restrictions are considered in order to reduce this unification to matching. First, we show that combining matching algorithms (with linear constant restriction) is always sufficient for solving a pure fragment of combined matching problems. Second, the investigated method is complete for the largest class of theories where unification is not needed, including regular collapse-free theories and linear theories. Syntactic conditions are given to define this class of theories in which solving the combined matching problem is performed in a modular way. ] 1996 Academic Press, Inc.	algorithm;electronic signature;extended precision;matching (graph theory);programming paradigm;purification of quantum state;theory;unification (computer science);uninterpreted function;while	Christophe Ringeissen	1996	Inf. Comput.	10.1006/inco.1996.0042	combinatorics;discrete mathematics;computer science;3-dimensional matching;decision tree;pattern matching;mathematics;programming language;algorithm;algebra	AI	-4.78033960039727	19.364463623823546	145947
f904bce0fe696095a49eb154183112fd09f78ab2	designing programs that check their work	probabilistic algorithm;program verification;interactive proofs;polynomial time;program correctness	A program correctness checker is an algorithm for checking the output of a computation. This paper defines the concept of a program checker. It designs program checkers for a few specific and carefully chosen problems in the class P of problems solvable in polynomial time. It also applies methods of modern cryptography, especially the idea of a probabilistic interactive proof, to the design of program checkers for group theoretic computations. Finally it characterizes the problems that can be checked.	algorithm;computation;correctness (computer science);cryptography;decision problem;p (complexity);program transformation;theory;time complexity	Manuel Blum;Sampath Kannan	1989		10.1145/73007.73015	program analysis;time complexity;combinatorics;discrete mathematics;computer science;theoretical computer science;randomized algorithm;programming language;algorithm	Theory	-14.635481369813819	25.09809020190828	146042
8df82f7f73abfdc9b592f16e21c9f9621947c180	synthesis and quantitative verification of tradeoff spaces for families of software systems		Designing software subject to uncertainty in a way that provides guarantees about its run-time behavior while achieving an acceptable balance between multiple extra-functional properties is still an open problem. Tools and techniques to inform engineers about poorly-understood design spaces in the presence of uncertainty are needed. To tackle this problem, we propose an approach that combines synthesis of spaces of system design alternatives from formal specifications of architectural styles with probabilistic formal verification. The main contribution of this paper is a formal framework for specification-driven synthesis and analysis of design spaces that provides formal guarantees about the correctness of system behaviors and satisfies quantitative properties (e.g., defined over system qualities) subject to uncertainty, which is factored as a first-class entity.	analysis of algorithms;best, worst and average case;computation;correctness (computer science);first-class citizen;formal specification;formal system;formal verification;interaction;liveness;markov chain;model checking;prism (surveillance program);parallel computing;parametric model;principle of abstraction;scalability;scenario analysis;software system;software verification;spaces;specification language;systems design;time complexity;worst-case scenario	Javier Cámara;David Garlan;Bradley R. Schmerl	2017		10.1007/978-3-319-65831-5_1	open problem;software system;formal specification;systems engineering;systems design;correctness;probabilistic logic;computer science;software;theoretical computer science;formal verification	SE	-11.617463889734436	28.36725573676927	146068
0c860d420528acfcc6d46af777a67793d248b597	rewriting semantics of production rule sets	text;hazard freedom;maude;production rule sets;formal semantics;quasi delay insensitive circuits;formal verification;deadlock freedom;asynchronous digital circuits;asynchronous vlsi;rewriting logic semantics	This paper is about the semantics of production rule sets, a language used to model asynchronous digital circuits. Two formal semantics are developed and proved equivalent: a set-theoretic semantics that improves upon an earlier effort of ours, and an executable semantics in rewriting logic. The set-theoretic semantics is especially suited to meta-level proofs about production rule sets, whereas the executable semantics can be used with existing tools to establish, automatically, desirable properties of individual circuits. Experiments involving several small circuits are detailed wherein the executable semantics and the rewriting logic tool Maude are used to automatically check two important properties: hazard and deadlock freedom. In doing so, we derive several useful optimizations that make automatic checking of these properties more tractable.	asynchronous circuit;circuit design;cobham's thesis;correctness (computer science);deadlock;delay insensitive circuit;digital electronics;executable;foremost;formal grammar;maude system;merge sort;model checking;production (computer science);rewrite (programming);rewriting;rule 184;rule 90;scalability;semantics (computer science);set theory;statistical model	Michael Katelman;Sean Keller;José Meseguer	2012	J. Log. Algebr. Program.	10.1016/j.jlap.2012.06.002	action semantics;formal verification;failure semantics;computer science;theoretical computer science;formal semantics;programming language;well-founded semantics;operational semantics;denotational semantics;algorithm;computational semantics	Logic	-15.644860178193456	28.367812612228022	146097
0f15516facac736a965cd1b075ea1cd3e684a189	on the antisymmetry of galois embeddings	connexion galois;embedding;galois connection;formal specification;language theory;specification;formal methods;refinement;teoria lenguaje;specification formelle;plongement antisymetrique galois;antisymmetry;especificacion formal;afinamiento;especificacion;specification languages;espace partiellement ordonne;plongement;affinement;inmersion;galois antisymmetric embedding;partially ordered space;theorie langage;refinement techniques;antisymetrie	Abstract   A Galois connection is a ‘natural’ way of relating two partially-ordered spaces (for example, semantic models of different expressive power). The most common form of Galois connection, which reflects one model being finer than the other, is that of a Galois embedding. This paper shows a published claim, ‘ two partially-ordered spaces related by a Galois embedding in each direction are isomorphic ’, to be false in general but true in the finite case. The proof we give provides an application of a ‘disjoint from’ calculus.	antisymmetry	Jochen Burghardt;Florian Kammüller;Jeff W. Sanders	2001	Inf. Process. Lett.	10.1016/S0020-0190(00)00176-9	fundamental theorem of galois theory;embedding problem;galois theory;galois module;combinatorics;galois group;discrete mathematics;antisymmetry;formal methods;normal basis;generic polynomial;galois cohomology;computer science;philosophy of language;galois geometry;pure mathematics;splitting of prime ideals in galois extensions;embedding;formal specification;mathematics;refinement;galois extension;abelian extension;programming language;specification;algorithm;algebra;resolvent;field norm	DB	-8.328515412616774	18.709492996633706	146110
191d69816898a6491a328d547f7394afbbfae2d8	string operations in query languages	query language;low complexity;safety properties;expressive power;intermediate language;query evaluation;pattern matching;static analysis;string matching;performance optimization;regular expression;first order logic	We study relational calculi with support for string operations. While SQL restricts the ability to mix string pattern-matching and relational operations, prior proposals for embedding SQL in a compositional calculus were based on adding the operation of concatenation to first-order logic. These latter proposals yield compositional query languages extending SQL, but are unfortunately computationally complete. The unbounded expressive power in turn implies strong limits on the ability to perform optimization and static analysis of properties such as query safety in these languages. In contrast, we look at compositional extensions of relational calculus that have nice expressiveness, decidability, and safety properties, while capturing string-matching queries used in SQL. We start with an extension based on the string ordering and LIKE predicates. This extension shares some of the attractive properties of relational calculus (e.g. effective syntax for safe queries, low data complexity), but lacks the full power of regular-expression pattern-matching. When we extend this basic model to include string length comparison, we get a natural string language with great expressiveness, but one which includes queries with high (albeit bounded) data complexity. We thus explore the space between these two languages. We consider two intermediate languages: the first extends our base language with functions that trim/add leading characters, and the other extends it by adding the full power of regular-expression pattern-matching. We show that both these extensions inherit many of the attractive properties of the basic model: they both have corresponding algebras expressing safe queries, and low complexity of query evaluation.	concatenation;expressive power (computer science);first-order logic;first-order predicate;intermediate representation;mathematical optimization;pattern matching;query language;query optimization;regular expression;relational calculus;sql;static program analysis;string (computer science);string operations;string searching algorithm	Michael Benedikt;Leonid Libkin;Thomas Schwentick;Luc Segoufin	2001		10.1145/375551.375578	string operations;sargable;query optimization;codd's theorem;string;computer science;query by example;theoretical computer science;pattern matching;first-order logic;database;mathematics;conjunctive query;programming language;intermediate language;expressive power;static analysis;regular expression;algorithm;query language;tuple relational calculus;string searching algorithm;spatial query	DB	-15.060601014567183	20.828244417888925	146187
7d8be42275dcbe102326a92a0089816b319b1c4d	conversion of predicate-calculus axioms, viewed as non-deterministic programs, to corresponding deterministic programs	closure;retrieval;non deterministic;theorem proving;funarc expression;deduction	"""The paper considers the problem of converting axioms in predicate calculus to deterministic programs, which are to be used as """"rules"""" by a GPS-type supervisor. It is shown that this can be done, but that the """"objects"""" must then contain procedure closures or """"FUN-ARG-expressions"""" which are later applied. Background-Retrieval of Implicit information in a semantic data base is a kind of deduction. One approach to doing such retrieval has been resolution-style theorem-proving; a later approach has been high-level programming languages such as Planner1 and QA42, where non-de-terministic programs and pattern-directed invocation of procedures are available. The use of uniform proof procedures for this purpose has been repeatedly criticized, e.g. In 3. Users of the high-level languages have also been worried because their systems are very expensive to use4,2 and because the non-determinism is difficult to control4."""	database;first-order logic;global positioning system;high- and low-level;high-level programming language;natural deduction;regular expression	Erik Sandewall	1973			discrete mathematics;computer science;closure;mathematics;automated theorem proving;algorithm	DB	-18.01721273447729	20.199206155481257	146392
1a76153af48d2de9aaee4075b32b91d5c8e92307	on a distributed computation of supervisors in modular supervisory control	closed loop system supervisory control problem modular discrete event systems coordination control;generators;generators supervisory control closed loop systems discrete event systems safety computational modeling automata;supervisory control;closed loop systems;automata;computational modeling;safety;discrete event systems;discrete event systems closed loop systems	We discuss a supervisory control problem of modular discrete-event systems that admits a distributed computation of supervisors. We provide a characterization and an algorithm to compute the supervisors. If the specification cannot be reached, we make use of a relaxation of coordination control to compute a sublanguage of the specification for which the supervisors can be computed in a distributed way.	algorithm;best, worst and average case;computable function;computation;distributed computing;linear programming relaxation;mathematical optimization;pspace;pspace-complete;polynomial;string operations;sublanguage	Jan Komenda;Tomás Masopust;Jan H. van Schuppen	2015	2015 International Conference on Complex Systems Engineering (ICCSE)	10.1109/ComplexSys.2015.7385990	real-time computing;computer science;control theory;automaton;supervisory control;computational model	Robotics	-6.702775698214011	27.385043175480604	146567
e2280d958b776246dc171c7b880a77604c42d782	scheduling of transactions based on extended scheduling timed petri nets for soc system-level test-case generation	modelizacion;distributed system;tratamiento transaccion;systeme reparti;calculateur embarque;functional verification;red petri;pervasive computing;system on a chip;informatica difusa;modelisation;scheduling algorithm;time petri net;sistema repartido;test case generation;sistema sobre pastilla;informatique diffuse;generation test;scheduling;boarded computer;court terme;scheduling problem;test generation;systeme sur puce;transaction processing;petri net;modeling;generacion prueba;calculador embarque;ordonnancement;corto plazo;reseau petri;short term;traitement transaction;reglamento	The effective scheduling of transactions has a great potential for SoC functional verification. Petri nets have proven to be a promising technique for solving scheduling problem. This paper aims at presenting a Petri-net based approach to the scheduling of transactions generated by a test-case generator. Firstly, an extended scheduling timed Petri nets (ESTPN) model is given to support transaction scheduling. Secondly, the short term of ‘scheduling of transactions problem’is formulated by means of an ESTPN which can accommodate various scheduling policies. Finally, transactions scheduling schemes and scheduling algorithm based on ESTPN are given and cases are studied.	algorithm;coherence (physics);ibm tivoli access manager;petri net;scheduling (computing);test case	Jinshan Yu;Tun Li;Yang Guo;QingPing Tan	2006		10.1007/11802167_74	fair-share scheduling;nurse scheduling problem;fixed-priority pre-emptive scheduling;embedded system;job shop scheduling;real-time computing;earliest deadline first scheduling;gang scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;deadline-monotonic scheduling;distributed computing;scheduling;least slack time scheduling;lottery scheduling;round-robin scheduling;scheduling;ubiquitous computing;multiprocessor scheduling	Embedded	-7.962889503661834	31.179271398431172	146589
ab08fdbd4b313de17d2a0f32976b22941f25be47	a scalable proof methodology for risc processor designs: a functional approach	state functions;isa specification;risc designs;clocks;interest points;information technology;reduced instruction set computing;data abstraction function;pipelined microarchitectural implementation;functional programming;specification language;process design;verification set conditions;micro architectures;computational modeling;formal verification;data structures;micro architectures formal verification functional programming state functions risc designs;pipelines;verification set conditions risc processor designs pipelined microarchitectural implementation data abstraction function isa specification;data abstraction;time use;risc processor designs;computer science;reduced instruction set computing data structures formal verification;commutation;reduced instruction set computing process design instruction sets pipelines laboratories computer science commutation clocks information technology computational modeling;instruction sets	Most proof approaches verified a Pipelined Micro- Architectural implementation against an ISA specification, and consequently, it was impossible to find a meaningful point where the implementation state and the specification state can be compared easily. An alternative solution to such problem is to verify a PMA implementation against a sequential multi-cycle implementation. Because both models are formalised in terms of clock cycles, all synchronous intermediate states represent interesting points where the comparison could be achieved easily. Furthermore, by decomposing the state, the overall proof decomposes systematically into a set of verification conditions more simple to reason about and to verify. A major advantage of this elegant choice is the ability to carry out the proof by induction within the same specification language rather than by symbolic simulation through a proof tool which remains very tedious. Also, because both models relate to the MA level, there is no need for a data abstraction function (which remains very difficult to define for most approaches), only a time abstraction function is needed to map between the times used by the two models. The potential features of the proposed proof methodology are demonstrated over the pipelined MIPS RISC processor within Haskell framework.	abstraction (software engineering);clock signal;formal specification;formal verification;functional approach;graph rewriting;haskell;industry standard architecture;mathematical induction;pseudoforest;scalability;specification language;symbolic execution;symbolic simulation;the times;phpmyadmin	Salah Merniz;Mohamed Benmohammed	2008	Fifth International Conference on Information Technology: New Generations (itng 2008)	10.1109/ITNG.2008.92	process design;reduced instruction set computing;computer architecture;parallel computing;specification language;formal verification;computer science;state function;theoretical computer science;operating system;instruction set;pipeline transport;programming language;functional programming;computational model;information technology	EDA	-15.466388744091846	29.72995106193799	146711
6c9935063f5d267e6a319b4bbbaec5fcb9bd8cf1	two-stage method for synthesizing liveness-enforcing supervisors for flexible manufacturing systems using petri nets	flexible manufacturing systems petri nets system recovery automatic control control systems electrical equipment industry industrial control production systems automata resource management;flexible manufacturing systems;mip based deadlock detection;flexible;deadlock detection;liveness enforcing supervisors;siphon deadlock prevention elementary siphon flexible manufacturing system petri net;mixed integer program;deadlock prevention;integer programming;flexible manufacturing system;siphon enumeration;mixed integer programming;elementary siphon;petri nets;petri net;siphon;manufacturing system;reachability analysis;control method;deadlock prevention liveness enforcing supervisors flexible manufacturing systems petri nets mixed integer programming siphon enumeration mip based deadlock detection;petri nets flexible manufacturing systems integer programming	This paper develops a two-stage approach to synthesizing liveness-enforcing supervisors for flexible manufacturing systems (FMS) that can be modeled by a class of Petri nets. First, we find siphons that need to be controlled using a mixed integer programming (MIP) method. This way avoids complete siphon enumeration that is more time-consuming for a sizable plant model than the MIP method. Monitors are added for only those siphons that require them. Second, we rearrange the output arcs of the monitors on condition that liveness is still preserved. The liveness is verified by an MIP-based deadlock detection method instead of much time-consuming reachability analysis. Experimental studies show that the proposed approach is more efficient than the existing ones and can result in more permissive and structurally simpler liveness-enforcing supervisors than all the known existing methods. This paper makes the application of siphon-based deadlock control methods to industrial-size FMS possible	benchmark (computing);deadlock;downtime;industrial pc;integer programming;linear programming;liveness;petri net;reachability	Zhiwu Li;Mengchu Zhou	2006	IEEE Transactions on Industrial Informatics	10.1109/TII.2006.885185	mathematical optimization;real-time computing;integer programming;computer science;engineering;distributed computing;petri net;deadlock prevention algorithms	Robotics	-6.500546986068367	29.03105965787946	146932
1bad4e5392294f2c1e95eb2aa81fe2d4ea63c1c4	towards unifying partial evaluation, deforestation, supercompilation, and gpc	partial evaluation	We study four transformation methodologies which are automatic instances of Burstall and Darlington's fold/unfold framework: partial evaluation, deforestation, supercompilation, and generalized partial computation (GPC). One can classify these and other fold/unfold based transformers by how much information they maintain during transformation. We introduce the positive supercompiler, a version of deforestation including more information propagation, to study such a classiication in detail. Via the study of positive supercompilation we are able to show that partial evaluation and deforestation have simple information propagation , positive supercompilation has more information propagation, and supercompilation and GPC have even more information propagation. The amount of information propagation is signiicant: positive su-percompilation, GPC, and supercompilation can specialize a general pattern matcher to a xed pattern so as to obtain eecient output similar to that of the Knuth-Morris-Pratt algorithm. In the case of partial evaluation and deforestation, the general matcher must be rewritten to achieve this.	computation;computer;knuth–morris–pratt algorithm;metacompilation;partial evaluation;software propagation;transformers	Morten Heine Sørensen;Robert Glück;Neil D. Jones	1994		10.1007/3-540-57880-3_32	computer science;programming language;partial evaluation	AI	-16.08420751630206	21.662095983116345	147113
0cfad0b07e75778534e1a091ed0b05f145a40e8d	software verification with validation of results - (report on sv-comp 2017)		This report describes the 2017 Competition on Software Verification (SV-COMP), the 6th edition of the annual thorough comparative evaluation of fully-automatic software verifiers. The goal is to reflect the current state of the art in software verification in terms of effectiveness and efficiency. The major achievement of the 6th edition of SV-COMP is that the verification results were validated in most categories. The verifiers have to produce verification witnesses, which contain hints that a validator can later use to reproduce the verification result. The answer of a verifier counts only if the validator confirms the verification result. SV-COMP uses two independent, publicly available witness validators. For 2017, a new category structure was introduced that now orders the verification tasks according to the property to verify on the top level, and by the type of programs (e.g., which kind of data types are used) on a second level. The categories Overflows and Termination were heavily extended, and the category SoftwareSystems now contains also verification tasks from the software system BusyBox. The competition used 8 908 verification tasks that each consisted of a C program and a property (reachability, memory safety, termination). SV-COMP 2017 had 32 participating verification systems from 12 countries.	busybox;correctness (computer science);display resolution;electronic organizer;emoticon;european joint conferences on theory and practice of software;fairness measure;memory safety;reachability;software system;software verification;systemverilog;validator	Dirk Beyer	2017		10.1007/978-3-662-54580-5_20	verification and validation;verification;software verification;software testing	Logic	-18.516762452013182	28.019017584191346	147259
f15a72459560d80d17b4c96da2fcc08bad03770a	symbolic model checking for real-time systems	concordance;automata estado finito;real time;specification;coaccion;contrainte;concordancia;punto fijo;langage garde;modelo;constraint;especificacion;point fixe;informatique theorique;temps reel;tiempo real;modele;finite automaton;automate fini;recherche modele;symbolic model checking;models;fix point;μ calcul;computer theory;real time systems;informatica teorica	"""Abstract   We describe finite-state programs over real-numbered time in a guarded-command language with real-valued clocks or, equivalently, as finite automata with real-valued clocks. Model checking answers the question which states of a real-time program satisfy a branching-time specification (given in an extension of CTL with clock variables). We develop an algorithm that computes this set of states symbolically as a fixpoint of a functional on state predicates, without constructing the state space. For this purpose, we introduce a μ-calculus on computation trees over real-numbered time. Unfortunately, many standard program properties, such as response for all nonzeno execution sequences (during which time diverges), cannot be characterized by fixpoints: we show that the expressiveness of the timed μ-calculus is incomparable to the expressiveness of timed CTL. Fortunately, this result does not impair the symbolic verification of """"implementable"""" real-time programs-those whose safety constraints are machine-closed with respect to diverging time and whose fairness constraints are restricted to finite upper bounds on clock values. All timed CTL properties of such programs are shown to be computable as finitely approximable fixpoints in a simple decidable theory."""	model checking;real-time transcription	Thomas A. Henzinger;Xavier Nicollin;Joseph Sifakis;Sergio Yovine	1994	Inf. Comput.	10.1006/inco.1994.1045	discrete mathematics;computer science;mathematics;finite-state machine;constraint;specification;algorithm;concordance	Logic	-9.752183405057425	25.346960369534113	147343
661413c94046e298f3d780ddda21647b06daddb4	merit: an interpolating model-checker	symbolic computation;safety properties;open architecture	We present the tool MERIT, a CEGAR model-checker for safety properties of counter-systems, which sits in the Lazy Abstraction with Interpolants (LAWI) framework. LAWI is parametric with respect to the interpolation technique and so is MERIT. Thanks to its open architecture, MERIT makes it possible to experiment new refinement techniques without having to re-implement the generic, technical part of the framework. In this paper, we first recall the basics of the LAWI algorithm. We then explain two heuristics in order to help termination of the CEGAR loop: the first one presents different approaches to symbolically compute interpolants. The second one focuses on how to improve the unwinding strategy. We finally report our experimental results, obtained using those heuristics, on a large amount of classical models.	algorithm;heuristic (computer science);interpolation;lazy evaluation;loop unrolling;model checking;open architecture;refinement (computing)	Nicolas Caniart	2010		10.1007/978-3-642-14295-6_16	symbolic computation;open architecture;computer science;artificial intelligence;theoretical computer science;algorithm	Logic	-16.163459615072096	25.189573236248627	147406
bff6c72f7826ad4831c0b65e6ed322165ee5504d	realizability and verification of msc graphs	bounded graph;set lw;ltl model checking;local property;msc graphs;software speciflcation;weak realizability;set l;bounded msc-graphs;formal veriflcation;graph g;safe realizability;polynomial-time solution;message sequence charts;polynomial time;model checking;temporal logic;message sequence chart	"""Scenario-based specifications such as message sequence charts (MSC) offer an intuitive and visual way to describe design requirements. MSC-graphs allow convenient expression of multiple scenarios, and can be viewed as an early model of the system that can be subjected to a variety of analyses. Problems such as LTL model checking are undecidable for MSC-graphs in general, but are known to be decidable for the class of bounded MSC-graphs. Our first set of results concerns checking realizability of bounded MSC-graphs. An MSC-graph is realizable if there is a distributed implementation that generates precisely the behaviors in the graph. There are two notions of realizability, weak and safe, depending on whether or not we require the implementation to be deadlock-free. It is known that for a finite set of MSCs, weak realizability is coNP-complete while safe realizability has a polynomial-time solution. We establish that for bounded MSC-graphs, weak realizability is, surprisingly, undecidable, while safe realizability is in EXPSPACE. Our second set of results concerns verification of MSC-graphs. While checking properties of a graph G, besides verifying all the scenarios in the set L(G) of MSCs specified by G, it is desirable to verify all the scenarios in the set Lw(G)--the closure of G, that contains the implied scenarios that any distributed implementation of G must include. For checking whether a given MSC M is a possible behavior, checking M ∈ L(G) is NP-complete, but checking M ∈ Lw(G) has a quadratic solution. For temporal logic specifications, considering the closure makes the verification problem harder: while checking LTL properties of L(G) is PSPACE-complete for bounded graphs G, checking even simple """"local"""" properties of Lw(G) is undecidable."""	computational complexity theory;deadlock;distributed computing;expspace;fifo (computing and electronics);message sequence chart;model checking;np-completeness;pspace-complete;polynomial;requirement;temporal logic;time complexity;verification and validation	Rajeev Alur;Kousha Etessami;Mihalis Yannakakis	2001	Theor. Comput. Sci.	10.1007/3-540-48224-5_65	time complexity;model checking;feasibility study;linear logic;software requirements specification;combinatorics;discrete mathematics;message passing;verification;np-complete;concurrency;temporal logic;formal verification;computer science;theoretical computer science;deadlock;mathematics;programming language;specification;complete graph;message sequence chart;algorithm	Logic	-9.900698769262233	24.60675871904366	147775
a7a07a0722fe5c36ed49d5e544815e8c7546f0fa	on-line estimation of matching complexity in first order logic	experimental analysis;on line estimation;probleme np complet;apprentissage conceptuel;logical programming;satisfiability;constraint satisfaction;combinatorial problem;satisfaction contrainte;probleme combinatoire;problema combinatorio;aprendizaje conceptual;phase transition;programmation logique;logique ordre 1;concept learning;order parameter;problema np completo;constraint satisfaction problem;satisfaccion restriccion;programacion logica;first order logic;np complete problem;logica orden 1	The expressiveness of First Order Logic (FOL) languages is severely counterbalanced by the complexity of matching formulas on a universe. Matching is an instance of the class of Constraint Satisfaction Problems (CSP), which have shown to undergo a phase transition with respect to two order parameters: constraint density and constraint tightness. This paper analyzes the problem of satisfying FOL Horn clauses in the light of these recent results. By means of an extensive experimental analysis, we show how Horn clause verification exhibits a typical phase transition with respect to the number of binary (or greater arity) predicates, and with respect to the ratio between the number of constants in the universe and the cardinality of the basic predicates extension.		Attilio Giordana;Lorenza Saitta	1999		10.1007/BFb0095092	phase transition;combinatorics;np-complete;concept learning;constraint satisfaction;computer science;artificial intelligence;first-order logic;mathematics;constraint satisfaction problem;algorithm;experimental analysis of behavior;satisfiability	EDA	-7.601518035274615	19.292496975144537	147785
cb288f3631328b3d13d1ac219951952392e10e98	verification of actl properties by bounded model checking	bounded model checking;state space;symbolic model checking	With the papers of Biere et. al. [1, 2] in 1999, SAT-based bounded model checking (BMC) for verification of LTL properties has been introduced as a complementary technique to BDD-based symbolic model checking, and a lot of successful work has been done with this approach. The idea has later also been applied to the verification of ACTL (the universal fragment of CTL) properties [6]. The efficiency of this method is based on the observation that if a system is faulty then only a fragment of its state space is sufficient for finding an error. For valid properties, the length (completeness threshold) that needed to be checked in order to certify that the system is error free is usually quite big, such that it is not practical to use this approach for checking systems that are error free with respect to given properties. An improvement is to use approximations of such a length taking the diameter of the model into consideration. However, for a reasonably large system, this length would possibly also be large enough to make the verification intractable. Our research aims at methods for avoiding this problem when checking systems that are error free. A work in this direction related to LTL properties has been carried out and presented in [8]. This paper proposes an approach to (partly) avoid the dependence on such a completeness threshold for verification of ACTL properties. The basic idea is to find an encoding such that if it is unsatisfiable then the encoded problem instance has no witness.	approximation;model checking;state space	Wenhui Zhang	2007		10.1007/978-3-540-75867-9_70	model checking;combinatorics;discrete mathematics;computer science;state space;artificial intelligence;mathematics;abstraction model checking;algorithm	Logic	-13.599604038204511	26.81070382207007	147862
b418072db7cb5df33d7fa2a413ad262dbc62542e	nivat-processing systems: decision problems related to protection and synchronization	nivat-processing systems	The paper introduces a model for processing systems which proi ides ‘environment’ to the abstract notion of process as introduced by Nicst [ 131. X basic componcllt of the model is ;I protection mechanism which is general enough to capture as particular instances known protection strategies [e.g. take. grant, create, parameter passing) [5. 8. 91. Decision problems associated with these systems are discussed for both cases: processe\ with intinite and tinite hehaviours. Solvability results a+e obtained for the safety problem: as a corollary we get the solvability result of Beauquier in the context of his processes [I 1. IJnsolf ability results arc also derived. A concept of compatibilit) is Introduced for processes acting in parallel su )jcct to s~mc \ynchroniration condition. &‘c show that the traversing from rational to algebraic s>\tt’m$ can take the c~~mpatibility problem from solvable to unsolvable.	decision problem;integrated development environment;linear algebra;protection mechanism	Sorin Istrail;Cristian Masalagiu	1983		10.1007/3-540-12727-5_15	decision problem;corollary;protection mechanism;algebraic number;discrete mathematics;compatibility (mechanics);synchronization;mathematics	Theory	-6.6490902585781075	25.749555186547116	147891
ac767e91ec4eae16399e933d940012eb27e216e4	improving encoding efficiency for bounded model checking	computational logic and formal languages;comparative analysis;temporal logic computability finite state machines formal verification;encoding efficiency;efficient algorithm;temporal logic;computability;conference output;faculty of science environment engineering and technology;080203;sat tool;satisfiability;embedded system;sat;bounded model checking;encoding bounded model checking sat;finite state machines;formal verification;linear temporal logic;sat tool encoding efficiency bounded model checking finite state machine linear temporal logic satisfiability;encoding;finite state machine;state transition;encoding embedded software software systems embedded system protocols automata logic character generation safety algorithm design and analysis	Bounded model checking (BMC) has played an important role in verification of software, embedded systems and protocols. The idea of BMC is to encode finite state machine (FSM) and linear temporal logic (LTL) verification specification into satisfiability (SAT) instances, and then to search for a counterexample via various SAT tools. Improving encoding technology of BMC can generate a SAT instance easy to solve, and therefore is essential to improve the efficiency of BMC. In this paper, we improve the encoding of BMC by combining the characteristic of FSM state transition and semantics of LTL, get a simple and efficient recursion formula which is useful to efficiently generate SAT instances. We present an efficient algorithm to encode the modal operator (safety formula) in BMC. The experiments for comparative analysis shows that this encoding algorithm is more powerful than the existing two mainstream encoding algorithms in both the scale of generated SAT instances and the solving efficiency. The methodology presented in this paper is also valuable for optimization of other modal operator encodings in BMC.	algorithm;boolean satisfiability problem;encode;embedded system;experiment;finite-state machine;intelligent platform management interface;linear temporal logic;mathematical optimization;modal operator;model checking;nusmv;qualitative comparative analysis;recursion;state transition table;turing completeness	Jinji Yang;Kaile Su;Qingliang Chen	2008	2008 2nd IFIP/IEEE International Symposium on Theoretical Aspects of Software Engineering	10.1109/TASE.2008.23	qualitative comparative analysis;linear temporal logic;temporal logic;formal verification;computer science;theoretical computer science;computability;finite-state machine;programming language;algorithm;encoding;satisfiability	Logic	-13.068311393443853	27.045038098740385	147912
8a224c030bde3e220210f0afe6e8670841ff3065	tradeoffs in metaprogramming	turing degree;computability theory;complexity theory;programming language;program generators;safety properties;program generation;expressive power;metalanguages;metaprogramming	The design of metaprogramming languages requires appreciation of the tradeoffs that exist between important language characteristics such as safety properties, expressive power, and succinctness. Unfortunately, such tradeoffs are little understood, a situation we try to correct by embarking on a study of metaprogramming language tradeoffs using tools from computability theory. Safety properties of metaprograms are in general undecidable; for example, the property that a metaprogram always halts and produces a type-correct instance is Π02-complete. Although such safety properties are undecidable, they may sometimes be captured by a restricted language, a notion we adapt from complexity theory. We give some sufficient conditions and negative results on when languages capturing properties can exist: there can be no languages capturing total correctness for metaprograms, and no 'functional' safety properties above Σ03 can be captured. We prove that translating a metaprogram from a general-purpose to a restricted metaprogramming language capturing a property is tantamount to proving that property for the metaprogram.	computability theory;computational complexity theory;correctness (computer science);general-purpose markup language;metaprogramming;undecidable problem	Todd L. Veldhuizen	2006		10.1145/1111542.1111569	metaprogramming;turing degree;computability theory;computer science;theoretical computer science;metacompiler;programming language;expressive power;algorithm	PL	-15.826405337046694	22.729030923222332	147990
687deb37c854026160e40986c442137968825c9b	from dynamic condition response structures to büchi automata	automatic verification;buchi automaton;medical services automata petri nets indexes concurrent computing algebra analytical models;set theory;buchi automaton business processes declarative workflows;declarative workflows;automatic verification buchi automata distributed dynamic condition response structures declarative process model labelled event structures finite specifications dynamic exclusion dynamic inclusion dcr structure execution;set theory finite automata;finite automata;distribution dynamics;process model;causal relation;conditioned response;event structures;business process;business processes	Recently we have presented distributed dynamic condition response structures (DCR structures) as a declarative process model conservatively generalizing labelled event structures to allow for finite specifications of repeated, possibly infinite behavior. The key ideas are to split the causality relation of event structures in two dual relations: the condition relation and the response relation, to split the conflict relation in two relations: the dynamic exclusion and dynamic inclusion, and finally to allow configurations to be multi sets of events. In the present abstract we recall the model and show how to characterise the execution of DCR structures and the acceptance condition for infinite runs by giving a map to Bu ??chi-automata. This is the first step towards automatic verification of processes specified as DCR structures.	automata theory;büchi automaton;causality;concurrency (computer science);declarative programming;graphical user interface;linear temporal logic;petri net;process modeling;scheduling (computing)	Raghava Rao Mukkamala;Thomas T. Hildebrandt	2010	2010 4th IEEE International Symposium on Theoretical Aspects of Software Engineering	10.1109/TASE.2010.22	computer science;theoretical computer science;business process;finite-state machine;programming language;algorithm	SE	-10.815604075656308	22.007863656604222	148126
185c70ecb57c95d73a6a3344e198456ea278fc68	verification and optimization of a plc control schedule	control optimo;description systeme;controleur logique programmable;system description;sistema hibrido;controlador logica programable;real time;metodo formal;methode formelle;program verification;formal method;optimal control;verificacion programa;standard model;formal verification;model checking;programmable logical controller;scheduling;commande optimale;timing optimization;state space;promela spin environment;process control;hybrid system;verification formelle;ordonamiento;descripcion sistema;verification enumerative;verification programme;non real time;ordonnancement;programmable logic controller;verification model;systeme hybride	We report on the use of model checking techniques for both the verification of a process control program and the derivation of optimal control schedules. Most of this work has been carried out as part of a case study for the EU VHS project (Verification of Hybrid Systems), in which the program for a Programmable Logic Controller (PLC) of an experimental chemical plant had to be designed and verified. The original intention of our approach was to see how much could be achieved here using the standard model checking environment of SPIN/Promela. As the symbolic calculations of real-time model checkers can be quite expensive it is interesting to try and exploit the efficiency of established non-real-time model checkers like SPIN in those cases where promising work-arounds seem to exist. In our case we handled the relevant real-time properties of the PLC controller using a time-abstraction technique; for the scheduling we implemented in Promela a so-called variable time advance procedure . To compare and interpret the results we carried out the same case study with the aid of the real-time model checker Uppaal, enhanced with facilities for cost-guided state space exploration. Both approaches proved sufficiently powerful to verify the design of the controller and/or derive (time-)optimal schedules within reasonable time and space requirements.	mathematical optimization;model checking;optimal control;programmable logic device;promela;real-time clock;requirement;spin;scheduling (computing);state space;uppaal;workaround	Ed Brinksma;Angelika Mader;Ansgar Fehnker	2002	International Journal on Software Tools for Technology Transfer	10.1007/s10009-002-0079-0	model checking;standard model;real-time computing;formal methods;optimal control;formal verification;computer science;state space;programmable logic controller;process control;programming language;scheduling;algorithm;hybrid system	SE	-9.064809397568661	28.764079783619202	148137
e9845bb60792e5ec22745b81ee60a7dd8c516599	on the completeness of verifying message passing programs under bounded asynchrony		We address the problem of verifying message passing programs, defined as a set of parallel processes communicating through unbounded FIFO buffers. We introduce a bounded analysis that explores a special type of computations, called k-synchronous. These computations can be viewed as (unbounded) sequences of interaction phases, each phase allowing at most k send actions (by different processes), followed by a sequence of receives corresponding to sends in the same phase. We give a procedure for deciding k-synchronizability of a program, i.e., whether every computation is equivalent (has the same happens-before relation) to one of its k-synchronous computations. We also show that reachability over k-synchronous computations and checking k-synchronizability are both PSPACE-complete. Furthermore, we introduce a class of programs called flow-bounded for which the problem of deciding whether there exists a k > 0 for which the program is k-synchronizable, is decidable.	asynchrony (computer programming);computation;fifo (computing and electronics);message passing;pspace-complete;reachability;verification and validation	Ahmed Bouajjani;Constantin Enea;Kailiang Ji;Shaz Qadeer	2018		10.1007/978-3-319-96142-2_23	message passing;asynchrony;completeness (statistics);theoretical computer science;computer science;fifo (computing and electronics);computation;bounded function;distributed computing;reachability	Logic	-11.45961785815161	25.620148707008493	148258
10b13fe50144569f522661c27f8cbcd47fb85280	rewrite rules and operational semantics for model checking uml statecharts	semantica operacional;rewrite rule;diagramme etat;operational semantics;verification modele;langage modelisation unifie;specification language;semantique operationnelle;formal verification;diagrama estado;model checking;rewriting systems;unified modeling language;state diagram;verification formelle;lenguaje especificacion;langage specification;systeme reecriture	Model checking of UML statecharts is the main concern of this paper. To model check it, however, its description has to be translated into the input language of the model checker SMV. For the purpose of translating UML statecharts as closely as possible into SMV, we use rewrite rules and its operational semantics.	automata theory;automaton;finite-state machine;formal verification;model checking;operational semantics;rewrite (programming);rewriting;time complexity;uml state machine;unified modeling language	Gihwon Kwon	2000		10.1007/3-540-40011-7_39	model checking;unified modeling language;state diagram;specification language;formal verification;computer science;applications of uml;database;programming language;operational semantics;algorithm	Logic	-16.86087598886502	26.982674965771686	148321
10dcdc291fc0516dd5870d405d90306a0c873c50	a complete theory of deterministic event structures	satisfiability;event structures	We present an complete algebra of a class of deterministic event structures which are labelled prime event structures where the labelling function satis es a certain distinctness condition The operators of the algebra are summation sequential composition and join Each of these gives rise to a monoid in addition a number of distributivity properties hold Summation loosely corresponds to choice and join to parallel composition with however some nonstandard aspects The space of models is a complete partial order in fact a complete lat tice in which all operators are continuous hence minimal xpoints can be de ned inductively Moreover the submodel relation can be captured within the algebra by summation x v y i x y y therefore the e ect of xpoints can be captured by an in nitary proof rule yielding a complete proof system for recursively de ned deterministic event struc tures	process calculus;proof calculus;recursion	Arend Rensink	1995		10.1007/3-540-60218-6_12	combinatorics;discrete mathematics;computer science;mathematics;event tree;algorithm;satisfiability	Logic	-9.60330434589897	20.814293159758428	148473
6107ba920e252ad829beff22d01da11f745ed01b	concurrency and local reasoning under reverse exchange	hoare logic;relational semantics	Quite a number of aspects of concurrency are reflected by the inequational exchange law (P ∗Q) ; (R ∗S) ≤ (P ;R)∗(Q ;S) between sequential composition ; and concurrent composition ∗. In particular, recent research has shown that, under a certain semantic definition, validity of this law is equivalent to that of the familiar concurrency rule for Hoare triples. Unfortunately, while the law holds in the standard model of concurrent Kleene algebra, its is not true in the relationally based setting of algebraic separation logic. However, we show that under mild conditions the reverse inequation (P ; R) ∗ (Q ; S) ≤ (P ∗ Q) ; (R ∗ S) still holds there. From this reverse exchange law we derive slightly restricted but still reasonably useful variants of the concurrency rule. Moreover, using a corresponding definition of locality, we obtain also a variant of the frame rule, where ∗ now is interpreted as separating conjunction. These results allow using the relational setting also for modular and concurrency reasoning. Finally, we interpret the results further by discussing several variations of the approach.	application domain;concurrency (computer science);forward compatibility;hoare logic;interference (communication);kleene algebra;locality of reference;process calculus;relational calculus;separation logic	H.-H. Dang;B. Möller	2014	Sci. Comput. Program.	10.1016/j.scico.2013.07.006	theoretical computer science;programming language;algorithm	PL	-14.444164393975186	19.114700789328776	148563
d907cf41f894469510187f4806634c8e95e10312	behavioural observations of cell movements with timing aspects	mobile membranes with lifetimes;behavioural equivalences	We use membrane systems to define a formalism inspired by cell biology in which mobility and timing are explicitly specified. In order to reason about the behaviours of complex biological systems, we introduce several observational equivalences over mobile membranes with lifetimes. These equivalences based on observations correspond to several combinations of mobility operations that can be performed, timing aspects of the objects involved in mobility and their explicit positions inside membranes. Various relationships between these observational equivalences are proved. Moreover, we use the ambient logic to provide a logical characterization for located observational equivalence.		Bogdan Aman;Gabriel Ciobanu	2015	Nano Comm. Netw.	10.1016/j.nancom.2015.04.004	simulation;computer science;mathematics	Networks	-10.311224177947889	21.85915326165611	148721
10c7a5f36848e9c9340ab242dd4fca36fef32dcc	modelling fusion calculus using hd-automata	modelizacion;sistema transicion;bisimulacion;computer and information science;semantics;bisimulation;automaton;semantica;semantique;transition system;modelisation;refinement method;automata;systeme transition;side effect;algebra proceso;automate;transition systems;algebre processus;coalgebra;coalgebre;methode raffinement;data och informationsvetenskap;process algebra;modeling;metodo afinamiento;algorithm design	We propose a coalgebraic model of the Fusion calculus based on HD-automata. The main advantage of the approach is that the partition refinement algorithm designed for HD-automata is easily adapted to handle Fusion calculus processes. Hence, the transition systems of Fusion calculus processes can be minimised according to the notion of observational semantics of the calculus. As a beneficial side effect, this also provides a bisimulation checker for Fusion calculus.	algorithm;automata theory;automaton;bisimulation;diagram;iteration;lambda calculus;operational semantics;partition refinement;process calculus;refinement (computing);sequent calculus;transition system;turing completeness;π-calculus	Gian Luigi Ferrari;Ugo Montanari;Emilio Tuosto;Björn Victor;Kidane Yemane	2005		10.1007/11548133_10	process calculus;typed lambda calculus;discrete mathematics;relational calculus;computer science;artificial intelligence;simply typed lambda calculus;mathematics;semantics;proof calculus;automaton;situation calculus;programming language;calculus of communicating systems;algorithm;epsilon calculus	Logic	-9.425987638764418	25.067432171442846	148726
5310f1005606de7ccbadb670c6e00236fbb6c64b	improving control of logic programs by using functional logic languages	expressive power;logic programs;functional language	This paper shows the advantages of amalgamating functional and logic programming languages. In comparison with pure functional languages, an amalgamated functional logic language has more expressive power. In comparison with pure logic languages, functional logic languages have a better control behaviour. The latter will be shown by presenting methods to translate logic programs into a functional logic language with a narrowing/rewriting semantics. The translated programs produce the same set of answers and have at least the same efficiency as the original programs. But in many cases the control behaviour of the translated programs is improved. This requires the addition of further knowledge to the programs. We discuss methods for this and show the gain in efficiency by means of several examples.	logic programming	Michael Hanus	1992		10.1007/3-540-55844-6_124	dynamic logic;natural language processing;description logic;functional logic programming;signature;ontology language;programming language;logic programming;multimodal logic;algorithm;autoepistemic logic	PL	-17.265556281887765	19.97456975246806	148753
057350a5f07555f0749664c63c9f55aeeeab7871	rers 2016: parallel and sequential benchmarks with focus on ltl verification		The 5th challenge of Rigorous Examination of Reactive Systems (RERS 2016) once again provided generated and tailored benchmarks suited for comparing the effectiveness of automatic software verifiers. RERS is the only software verification challenge that features problems with linear temporal logic (LTL) properties in larger sizes that are available in different programming languages. This paper describes the revised rules and the refined profile of the challenge, which lowers the entry hurdle for new participants. The challenge format with its three tracks, their benchmarks, and the related LTL and reachability properties are explained. Special emphasis is put on changes that were implemented in RERS — compared to former RERS challenges. The competition comprised 18 sequential and 20 parallel benchmarks. The 20 benchmarks from the new parallel track feature LTL properties and a compact representation as labeled transition systems and Promela code.	benchmark (computing);black box;control flow;data structure;graph (abstract data type);iteration;linear temporal logic;open-source software;programming language;promela;reachability;software verification;transition system;white box (software engineering)	Maren Geske;Marc Jasper;Bernhard Steffen;Falk Howar;Markus Schordan;Jaco van de Pol	2016		10.1007/978-3-319-47169-3_59	real-time computing;computer science;theoretical computer science;algorithm	SE	-18.244867054373536	27.78954817911616	148972
ac5aa3469f529d26336ed58b54024f77820baf17	the early search for tractable ways of reasoning about programs	informatica;history program verification programming theory;history;program;technology;century 20;axiom;data processing;axioma;langage;methode;program verification;satisfiability;raisonnement;algorithme;algorithm;reasoning about programs concurrent computing mathematics proposals automatic control automatic logic units flowcharts;programming theory;siecle 20;technologie;programme;informatique;language;reasoning;turing machine sequential imperative programs concurrency reasoning about programs program verification history testing;metodo;method;siglo 20;axiome;tecnologia	This paper traces the important steps in the history –up to around 1990– of research on reasoning about programs. The main focus is on sequential imperative programs but some comments are made on concurrency. Initially, researchers focussed on ways of verifying that a program satisfies its specification (or that two programs were equivalent). Over time it became clear that post facto verification is only practical for small programs and attention turned to verification methods which support the development of programs; for larger programs it is necessary to exploit a notation of compositionality. Coping with concurrent algorithms is much more challenging – this and other extensions are considered briefly. The main thesis of this paper is that the idea of reasoning about programs has been around since they were first written; the search has been to find	cobham's thesis;concurrency (computer science);concurrent algorithm;imperative programming;tracing (software);verification and validation	Cliff B. Jones	2003	IEEE Annals of the History of Computing	10.1109/MAHC.2003.1203057	method;data processing;computer science;artificial intelligence;database;linguistics;language;axiom;programming language;algorithm;satisfiability;technology	SE	-18.71280157043159	21.912586271139254	148975
62d5ada67fbdeacabfe47bef99ba7dff6fa4c338	decidability problems for actor systems	actor system;stateless actor;expressive power;dynamic creation;relevant feature;decidability result;nominal actor-based language;turing completeness;decidability problem	We introduce a nominal actor-based language and study its expressive power. We have identified the presence/absence of fields as a relevant feature: the dynamic creation of names in combination with fields gives rise to Turing completeness. On the other hand, restricting to stateless actors gives rise to systems for which properties such as termination are decidable. Such decidability result holds in actors with states when the number of actors is finite and the state is read-only.	actor model;computation;decision problem;embedded system;expressive power (computer science);file system permissions;international federation for information processing;java virtual machine;journal of circuits, systems, and computers;lecture notes in computer science;openid;petri net;qp state machine frameworks;qualitative comparative analysis;read-only memory;software system;springer (tank);stateless protocol;turing completeness;π-calculus	Frank S. de Boer;Mohammad Mahdi Jaghoori;Cosimo Laneve;Gianluigi Zavattaro	2012	Logical Methods in Computer Science	10.2168/LMCS-10(4:5)2014	actor model and process calculi	Logic	-12.868866947247618	22.58098932826335	148987
bba4938190531a9ff97b7b4f582e403fdb95276b	parallel evaluation of datalog programs by load sharing	load sharing;computer science	Abstract   We propose a method of parallelizing the evaluation of data-intensive Datalog programs. The method is distinguished by the fact that it is  pure , i.e., does not require interprocessor communication, or synchronization overhead. The method cannot be used to parallelize every Datalog program, but we syntactically characterize several classes of Datalog programs that are  sharable , i.e., programs to which the method can be applied. We also provide a characterization of a class of  nonsharable  programs, and demonstrate that sharability is a fundamental notion that is independent of the syntactic parallelization method proposed in this paper. This notion is related to bottom-up evaluation (we propose a formal characterization of this type of control-strategies) and to program classification.	datalog	Ouri Wolfson	1992	J. Log. Program.	10.1016/0743-1066(92)90008-Q	computer science;theoretical computer science;database;datalog;programming language	DB	-15.401005615648696	19.787948762325733	149010
17fa06850e4221117b10f0ab1a4e695a35821670	program debugging and validation using semantic approximations and partial specifications	informatica;debugging;puesta a punto programa;programmation logique avec contrainte;representacion conocimientos;validacion;programacion logica con restriccion;semantics;semantica;semantique;specification language;analisis programa;debogage;programa puesta a punto;validation;lenguaje especificacion;constraint logic programming;program analysis;program debugging;analyse programme;knowledge representation;abstract interpretation;representation connaissances;langage specification;programme debogage;debugging program	The technique of Abstract Interpretation [11] has allowed the development of sophisticated program analyses which are provably correct and practical. The semantic approximations produced by such analyses have been traditionally applied to optimization during program compilation. However, recently, novel and promising applications of semantic approximations have been proposed in the more general context of program validation and debugging [3, 9, 7]. We study the case of (Constraint) Logic Programs (CLP), motivated by the comparatively large body of approximation domains, inference techniques, and tools for abstract interpretation-based semantic analysis which have been developed to a powerful and mature level for this programming paradigm (see, e. and their references). These systems can approximate at compile-time a wide range of properties, from directional types to determinacy or termination, always safely, and with a significant degree of precisión. Thus, our approach is to take advantage of these advances in program analysis tools within the context of program validation and debugging, rather than using traditional proof-based methods (e.g., [1, 2, 13, 17, 28]), developing new tools and procedures, such as specific concrete [4, 15, 16] or abstract [9, 10] diagnosers and declarative debuggers, or limiting error detection to run-time checking [28]. In this talk we discuss these issues and present a framework for combined static/dynamic validation and debugging using semantic approximations [7, 26, 21] which is meant to be a part of an advanced program development environment comprising a variety of co-existing tools [14]. Program validation and detection of errors is first performed at compile-time by inferring properties of the program via abstract interpretation-based static analysis and comparing this information against (partial) speciñcations written in terms of assertions. Such assertions are linguistic constructions which allow expressing properties of programs. Classical examples of assertions are type declarations (e.g., in the context of (C)LP those used by [22, 27, 5]). However, herein we are interested in supporting a more general setting in which assertions can be of a much more general nature, stating additionally other properties, some of which cannot always be determined statically for all programs. These properties may include properties defined by means of user programs and extend beyond the predefined set which may be	abstract interpretation;altran praxis;approximation algorithm;approximation error;compile time;compiler;construction grammar;correctness (computer science);debugger;debugging;error detection and correction;indeterminacy in concurrent computation;mathematical optimization;programming paradigm;runtime error detection;semantic analysis (compilers);static program analysis	Manuel V. Hermenegildo;Germán Puebla;Francisco Bueno;Pedro López-García	2002		10.1007/3-540-45465-9_7	program analysis;constraint logic programming;specification language;computer science;theoretical computer science;database;semantics;programming language;debugging;algorithm	PL	-18.24611156288107	22.537536475268354	149022
3d499c04cdb1702b510fc2a77add11d8f1726b52	a term rewriting approach to the automated termination analysis of imperative programs	rewrite rule;automatic generation;termination analysis;presburger arithmetic;decision procedure;rewrite systems;term rewriting	An approach based on term rewriting techniques for the automated termination analysis of imperative programs operating on integers is presented. An imperative program is transformed into rewrite rules with constraints from quantifier-free Presburger arithmetic. Any computation in the imperative program corresponds to a rewrite sequence, and termination of the rewrite system thus implies termination of the imperative program. Termination of the rewrite system is analyzed using a decision procedure for Presburger arithmetic that identifies possible chains of rewrite rules, and automatically generated polynomial interpretations are used to show finiteness of such chains. An implementation of the approach has been evaluated on a large collection of imperative programs, thus demonstrating its effectiveness and practicality.	approximation;binary search algorithm;bubble sort;computation;control point (mathematics);decision problem;heapsort;imperative programming;nonlinear system;polynomial;presburger arithmetic;quantifier (logic);rewrite (programming);rewriting;stable model semantics;static program analysis;termination analysis;timeout (computing);undecidable problem	Stephan Falke;Deepak Kapur	2009		10.1007/978-3-642-02959-2_22	discrete mathematics;computer science;presburger arithmetic;termination analysis;mathematics;programming language;algorithm	Logic	-16.642873971865733	24.272861878333746	149165
22d3ae84b3ba47d0f355c400705d8c7f4531b745	a non-local method for robustness analysis of floating point programs	robustness to errors;program analysis;floating point arithmetic	Robustness is a standard correctness property which intuitively means that if the input to the program changes less than a fixed small amount then the output changes only slightly. This notion is useful in the analysis of rounding error for floating point programs because it helps to establish bounds on output errors introduced by both measurement errors and by floating point computation. Compositional methods often do not work since key constructs—like the conditional—are not robust. We propose a method for proving the robustness of a while-loop. This method is non-local in the sense that instead of breaking the analysis down to single lines of code, it checks certain global properties of its structure. We show the applicability of our method on two standard algorithms: the CORDIC computation of the cosine and Dijkstra’s shortest path algorithm.	cordic;computation;conditional (computer programming);correctness (computer science);dijkstra's algorithm;robustness (computer science);round-off error;rounding;shortest path problem;source lines of code;while loop	Ivan Gazeau;Dale Miller;Catuscia Palamidessi	2011		10.4204/EPTCS.85.5	program analysis;discrete mathematics;computer science;floating point;theoretical computer science;mathematics;programming language;algorithm	Logic	-13.082432305185383	28.08167984908977	149221
0924dee9c5493c86fd085b33e5fa98db999aa1b6	on the aggregation techniques in stochastic petri nets and stochastic process algebras	modelizacion;distributed system;agregacion;processus aleatoire;systeme reparti;stochastic petri net;aggregation;algebre;modelisation;sistema repartido;algebra;systems analysis;stochastic process algebra;random processes;agregation;analyse systeme;systeme parallele;parallel system;petri nets;modeling;reseau petri;sistema paralelo	Stochastic Petri Nets and Stochastic Process Algebras can both be used for the qualitative and quantitative analysis of parallel and distributed systems. However, sometimes the complexity of the system can make such analysis infeasible, due to the large size of the state space underlying the model. To overcome this problem diierent aggregation techniques have been proposed in both formalisms. Two of them are compared in this paper.	distributed computing;state space;stochastic petri net;stochastic process	Marina Ribaudo	1995	Comput. J.	10.1093/comjnl/38.7.600	stochastic process;systems analysis;combinatorics;discrete mathematics;systems modeling;stochastic petri net;computer science;mathematics;process architecture;petri net;algorithm	SE	-9.347768447315348	27.875445380618697	149320
677b921ace53075378e1f8d30af9f7b3d5997f4d	application of an exact transversal hypergraph in selection of sm-components	selection hypergraph;graphs;exact transversal hypergraphs;sm components;exact cover;hypergraph;petri nets;exact transversal;transversal;concurrency hypergraphs;sequentiality hypergraph	The paper deals with the application of the hypergraph theory in selection of State Machine Components (SM-Components) of Petri nets [1,2].As it is known, Petri nets are widely used for modeling of concurrency processes. However, in order to implement the concurrent automaton, an initial Petri net ought to be decomposed into sequential automata (SM-Components), which can be easily designed as an Finite-State-Machine (FSM) or Microprogrammed Controller [3]. The last step of the decomposition process of the Petri nets is selection of SM-Components. This stage is especially important because it determines the final number of sequential automata. In the article we propose a new idea of SM-Components selection. The aim of the method is reduction of the computational complexity from exponential to polynomial. Such a reduction can be done if the selection hypergraph belongs to the exact transversal hypergraphs (xt-hypergraphs) class. Since the recognition and generation of the first transversal in the xt-hypergraphs are both polynomial, the complete selection process can be performed in polynomial time. The proposed ideas are an extension of the concept presented in [1]. The proposed method has been verified experimentally. The conducted investigations have shown that for more than 85% of examined Petri nets the selection process can be done via xt-hypergraphs.	automaton;computational complexity theory;concurrency (computer science);experiment;finite-state machine;graph coloring;microcode;petri net;polynomial;time complexity	Lukasz Stefanowicz;Marian Adamski;Remigiusz Wisniewski	2013		10.1007/978-3-642-37291-9_27	exact cover;combinatorics;discrete mathematics;theoretical computer science;machine learning;transversal;mathematics;graph;petri net	Logic	-5.010024637931195	26.564856220089975	149443
0e377ba3cc474ea04ddb6fe00020688a0f86bd46	modal interface automata	conjunction operator;modal interface automaton;multiple interface;novel interface theory;interface automata;iomts-parallel composition;modal transition;modal interface automata;recent combination iomts;de alfaro;interface theory	De Alfaro and Henzinger’s Interface Automata (IA) and Nyman et al.’s recent combination IOMTS of IA and Larsen’s Modal Transition Systems (MTS) are established frameworks for specifying interfaces of system components. However, neither IA nor IOMTS consider conjunction that is needed in practice when a component shall satisfy multiple interfaces, while Larsen’s MTS-conjunction is not closed and Beneš et al.’s conjunction on disjunctive MTS does not treat internal transitions. In addition, IOMTSparallel composition exhibits a compositionality defect. This article defines conjunction (and also disjunction) on IA and disjunctive MTS and proves the operators to be ‘correct’, i.e., the greatest lower bounds (least upper bounds) wrt. IAand resp. MTS-refinement. As its main contribution, a novel interface theory called Modal Interface Automata (MIA) is introduced: MIA is a rich subset of IOMTS featuring explicit output-must-transitions while input-transitions are always allowed implicitly, is equipped with compositional parallel, conjunction and disjunction operators, and allows a simpler embedding of IA than Nyman’s. Thus, it fixes the shortcomings of related work, without restricting designers to deterministic interfaces as Raclet et al.’s modal interface theory does.	audio feedback;automata theory;automaton;computational complexity theory;deadlock;decision problem;disjunctive normal form;finite-state machine;formal system;lu decomposition;modal logic;nondeterministic algorithm;refinement (computing);software bug;temporal logic;workbench	Gerald Lüttgen;Walter Vogler	2012	Logical Methods in Computer Science	10.2168/LMCS-9(3:4)2013	computer science;artificial intelligence;pure mathematics;mathematics;programming language;algorithm	Logic	-10.148573032868843	20.39190184984534	149559
52711bf54c718f8f028ca62dc7a1f311a796c8a5	canonical finite state machines for distributed systems	distributed system;68q55;distributed test architecture;systeme reparti;temps polynomial;canonical;maquina estado finito;implementation;specification;power of test;semantics;trace;polynomial;semantica;semantique;potencia;equivalence;interfase;terme;enrejado;research paper;sistema repartido;especificacion;treillis;informatique theorique;polinomio;interface;polynomial time;arquitectura;puissance;06bxx;traza;implementacion;machine etat fini;polynome;architecture;power;finite state machine;equivalencia;68m14;68q60;computer theory;lattice;tiempo polinomial;informatica teorica	There has been much interest in testing from finite state machines (FSMs) as a result of their suitability for modelling or specifying state-based systems. Where there are multiple ports/interfaces a multi-port FSM is used and in testing a tester is placed at each port. If the testers cannot communicate with one another directly and there is no global clock then we are testing in the distributed test architecture. It is known that the use of the distributed test architecture can affect the power of testing and recent work has characterised this in terms of local s-equivalence: in the distributed test architecture we can distinguish two FSMs, such as an implementation and a specification, if and only if they are not locally s-equivalent. However, there may be many FSMs that are locally s-equivalent to a given FSM and the nature of these FSMs has not been explored. This paper examines the set of FSMs that are locally s-equivalent to a given FSM M . It shows that there is a unique smallest FSM χmin(M) and a unique largest FSM χmax(M) that are locally s-equivalent to M . Here smallest and largest refer to the set of traces defined by an FSM and thus to its semantics. We also show that for a given FSM M the set of FSMs that are locally s-equivalent to M defines a bounded lattice. Finally, we define an FSM that, amongst all FSMs locally s-equivalent to M , has fewest states. We thus give three alternative canonical FSMs that are locally s-equivalent to an FSM M : one that defines the smallest set of traces, one that defines the largest set of traces, and one with fewest states. All three provide valuable information and the first two can be produced in time that is polynomial in terms of the number of states of M . We prove that the problem of finding an s-equivalent FSM with fewest states is NP-hard in general but can be solved in polynomial time for the special case where there are two ports.	distributed computing;existential quantification;finite-state machine;maximal set;np-equivalent;np-hardness;polynomial;system under test;time complexity;tracing (software);turing completeness	Robert Mark Hierons	2010	Theor. Comput. Sci.	10.1016/j.tcs.2009.09.039	equivalence;discrete mathematics;computer science;architecture;interface;trace;power;lattice;mathematics;semantics;finite-state machine;programming language;implementation;specification;algorithm;polynomial;algebra	Theory	-5.280732639475696	22.128935945396908	149700
8d993d225e21003fc477051332b6008534d06655	a formal semantics for isorecursive and equirecursive state abstractions	isorecursive semantics;implicit dynamic frame;recursive definition;equirecursive interpretation;equirecursive semantics;static program verification;recursive data structure;equirecursive formal semantics;equirecursive state abstraction;recursive predicate;predicate instance	Most methodologies for static program verification support recursively-defined predicates in specifications, in order to reason about recursive data structures. Intuitively, a predicate instance represents the complete unrolling of its definition; this is the equirecursive interpretation. However, this semantics is unsuitable for static verification, when the recursion becomes unbounded. For this reason, most static verifiers supporting recursive definitions employ explicit folding and unfolding of recursive definitions (specified using ghost commands, or inferred). Such a semantics differentiates between, e.g., a predicate instance and its corresponding body, while providing a facility to map between the two; this is the isorecursive semantics. While this latter interpretation is usually implemented in practice, only the equirecursive semantics is typically treated in theoretical work. In this paper we provide both an isorecursive and an equirecursive formal semantics for recursive definitions in the context of Chalice, a verification methodology based on implicit dynamic frames. We extend these assertion semantics to appropriate Hoare Logics, and prove the soundness of our definitions. The development of such formalisations requires addressing several subtle issues, regarding both the possibility of infinitelyrecursive definitions and the need for the isorecursive semantics to correctly reflect the restrictions that make it readily implementable. These questions are made more challenging still in the context of implicit dynamic frames, where the use of heap-dependent expressions provides further pitfalls for a correct formal treatment.	data structure;denotational semantics;formal verification;infinite loop;recursion (computer science);recursive data type;semantics (computer science);static program analysis	Alexander J. Summers;Sophia Drossopoulou	2013		10.1007/978-3-642-39038-8_6	formal semantics;computer science;theoretical computer science;formal semantics;programming language;operational semantics;denotational semantics;algorithm	PL	-17.744952235988638	21.9417383303642	149701
795ee1d24cb1fb64e6a2b54a2a65f2a899cda278	partial model checking of modal equations: a survey	model checking;concurrent systems;state explosion	Partial model checking is a technique for verifying concurrent systems. It gradually reduces the verification problem to the final answer by removing concurrent components one-by-one, transforming and minimizing the specifications as it proceeds. This paper gives a survey of the theory behind partial model checking and the results obtained with it.	atomic sentence;audio feedback;automata theory;concurrency (computer science);first-order logic;fixed-point iteration;level of detail;modal logic;model checking;real-time clock;real-time transcription;software propagation;state space;verification and validation	Henrik Reif Andersen;Jørn Lind-Nielsen	1999	International Journal on Software Tools for Technology Transfer	10.1007/s100090050032	model checking;partial order reduction;real-time computing;computer science;programming language;abstraction model checking	Logic	-11.96256542256092	24.83302063438185	149782
18f727ef28932aa873aabb23d0ef19121b1959e2	strictness analysis and denotational abstract interpretation	langage fonctionnel;parallelisme;lenguaje programacion;semantica denotacional;programming language;etude theorique;lenguaje funcional;metalenguage;analisis programa;metalangage;interpretacion;parallelism;paralelismo;metalanguage;denotational semantics;estudio teorico;langage programmation;interpretation;program analysis;theoretical study;analyse programme;abstract interpretation;functional language;semantique denotationnelle	Abstract   A theory of abstract interpretation ( P. Cousot and R. Cousot,  in  “Conf. Record, 4th ACM Symposium on Principles of Programming Languages,” 1977 ) is developed for a typed λ-calculus. The typed λ-calculus may be viewed as the “static” part of a two-level denotational metalanguage for which abstract interpretation was developed by  F. Nielson (Ph.D. thesis, University of Edinburgh, 1984;  in  “Proceedings, STACS 1986,” Lecture Notes in Computer Science, Vol. 210, Springer-Verlag, New York/Berlin, 1986 ). The present development relaxes a condition imposed there and this sufices to make the framework applicable to strictness analysis for the λ-calculus. This shows the possibility of a general theory for the analysis of functional programs and it gives more insight into the relative precision of the various analyses. In particular it is shown that a collecting (static;  P. Cousot and R. Cousot,  in  “Conf. Record, 6th ACM Symposium on Principles of Programming Languages,” 1979 ) semantics exists, thus answering a problem left open by  G. L. Burn, C. L. Hankin and S. Abramsky ( Sci. Comput. Programming  7  (1986) , 249–278).	abstract interpretation;strictness analysis	Flemming Nielson	1988	Inf. Comput.	10.1016/0890-5401(88)90041-7	program analysis;interpretation;metalanguage;computer science;mathematics;programming language;functional programming;denotational semantics;algorithm	Logic	-18.826717667017284	20.734228535762277	149836
45759b2fee37171c1e49c7e7faf1594c9ebf5b9e	contribution à la modélisation et à la vérification de processus workflow	verification;workflow process;modelisation;reseaux de petri;model checking;processus workflow modelisation verification reseaux de petri model checking en francais;analysis;processus workflow;petri nets;workflow process modelling analysis petri nets model checking	Work ow technology, whose role is to automate business processes and to provide a support for their management, is today an active sector of research. This thesis deals with the modelling of the work ow processes and their analysis. These processes, probably constrained by shared resources or by durations of treatment, must be checked before being executed by their work ow management systems. In this direction, we were interested by the checking of the soundness property of work ow nets (WF-nets) : subclasses of Petri nets modelling the work ow processes. To begin with, by exploring the structure theory of Petri nets, we have identi ed subclasses of WF-nets for which soundness can be checked and characterized e ectively. We also extended these subclasses by taking account of the presence of shared resources and we focused on the soundness property in the presence of an arbitrary number of instances ready to be carried out. In this part, we had to automate the computation of minimal siphons in a Petri net. For that, we chose an algorithm of the literature and improved it by the research and the contraction of alternate circuits. Then, we were concerned by the modelling and the analysis of work ow processes holding temporal constraints. We initially proposed the model of TWF-net (Timed WFnet). For this model, we de ned its soundness and proposed a method to check it. Then, we released the adopted temporal constraints by the proposal of a model covering work ow processes for witch temporal constraints vary in time intervals. We formally de ned the model of ITWF-net (Interval Timed WF-net) and gave its semantics. In addition, we developed and tested a prototype of modelling and simulation of ITWF-nets. The last part of this thesis concerns the formal analysis of work ow processes with SPIN model checker. We initially translated the work ow speci cation into Promela : the model description language used by SPIN. Then, we expressed the soundness properties in Linear Temporal Logic (LTL) and used SPIN to test if each property is satis ed by the Promela model of a given WF-net. Moreover, we expressed the properties of k-soundness for WFnets modelling several instances and (k,R)-soundness for competitive work ow processes which share resources.	algorithm;business process;computation;linear algebra;linear temporal logic;model checking;petri net;promela;prototype;spin model checker;simulation	Zohra Sbaï	2010			mathematical economics;spin-½;mathematics	Logic	-9.310480240189724	26.056678436792502	149901
b42107419bb909cf1e7897d512c860a7fa58acba	kproblog: an algebraic prolog for machine learning	algebraic prolog;kernel programming;graph kernels;machine learning	We introduce kProbLog as a declarative logical language for machine learning. kProbLog is a simple algebraic extension of Prolog with facts and rules annotated by semi-ring labels. It allows to elegantly combine algebraic expressions with logic programs. We introduce the semantics of kProbLog, its inference algorithm, its implementation and provide convergence guarantees. We provide several code examples to illustrate its potential for a wide range of machine learning techniques. In particular, we show the encodings of state-of-the-art graph kernels such as Weisfeiler-Lehman graph kernels, propagation kernels and an instance of graph invariant kernels, a recent framework for graph kernels with continuous attributes. However, kProbLog is not limited to kernel methods and it can concisely express declarative formulations of tensor-based algorithms such as matrix factorization and energy-based models, and it can exploit semirings of dual numbers to perform algorithmic differentiation. Furthermore, experiments show that kProbLog is not only of theoretical interest, but can also be applied to real-world datasets. At the technical level, kProbLog extends aProbLog (an algebraic Prolog) by allowing multiple semirings to coexist in a single program and by introducing meta-functions for manipulating algebraic values.		Francesco Orsini;Paolo Frasconi;Luc De Raedt	2017	Machine Learning	10.1007/s10994-017-5668-y	graph property;algebraic extension;algebraic number;mathematics;automatic differentiation;discrete mathematics;artificial intelligence;machine learning;theoretical computer science;kernel method;algebraic graph theory;inference;prolog	ML	-16.53523308345816	18.479890651705727	149966
15d92ab0ce13762cf1329f5aeacf1b079c4e9c00	on rational trees	graph theory;grafo ordenado;teoria grafo;logical programming;theorie graphe;graphe rationnel;rational graph;first order;programmation logique;grafo racional;labelled graph;decidibilidad;graphe ordonne;decidabilite;programacion logica;grafo marcado;ordered graph;decidability;graphe marque	Rational graphs are a family of graphs defined using labelled rational transducers. Unlike automatic graphs (defined using synchronized transducers) the first order theory of these graphs is undecidable, there is even a rational graph with an undecidable first order theory. In this paper we consider the family of rational trees, that is rational graphs which are trees. We prove that first order theory is decidable for this family. We also present counter examples showing that this result cannot be significantly extended both in terms of logic and of structure.	accessibility;context-free language;context-sensitive language;first-order logic;first-order predicate;tracing (software);transducer;undecidable problem	Arnaud Carayol;Christophe Morvan	2006		10.1007/11874683_15	decidability;pathwidth;combinatorics;discrete mathematics;computer science;graph theory;lévy family of graphs;first-order logic;mathematics;rational point;chordal graph;indifference graph;algorithm;ordered graph	Logic	-4.801031116259593	21.103888383412734	149989
7c89ba60db1bd927828570efef23a82a13a42ca7	floyd--hoare logic for quantum programs	lenguaje programacion;logica cuantica;computacion informatica;programming language;logique hoare;axiomatic;semantics;logical programming;program verification;exactitude programme;semantica;semantique;journal article;hoare logic;floyd hoare logic;quantum computation;axiomatico;quantum logic;verificacion programa;exactitud programa;quantum computer;programmation logique;logica hoare;ciencias basicas y experimentales;theory;langage programmation;completeness;weakest precondition;calcul quantique;grupo a;axiomatique;verification programme;calculo cuantico;programacion logica;axiomatic semantics;languages;logique quantique;ordinateur quantique;program correctness	Floyd--Hoare logic is a foundation of axiomatic semantics of classical programs, and it provides effective proof techniques for reasoning about correctness of classical programs. To offer similar techniques for quantum program verification and to build a logical foundation of programming methodology for quantum computers, we develop a full-fledged Floyd--Hoare logic for both partial and total correctness of quantum programs. It is proved that this logic is (relatively) complete by exploiting the power of weakest preconditions and weakest liberal preconditions for quantum programs.	axiomatic semantics;computer;correctness (computer science);formal verification;hoare logic;precondition;predicate transformer semantics;quantum computing;software development process	Mingsheng Ying	2011	ACM Trans. Program. Lang. Syst.	10.1145/2049706.2049708	computer science;theoretical computer science;computational logic;semantics;programming language;axiomatic semantics;quantum computer;algorithm	PL	-19.092035705631833	21.900821413604017	150135
b8740f877e2042e882490f0ae843f42154d6fd47	effective predicate abstraction for program verification	state space methods;information systems;information security;predicate abstraction program verification;heuristic method;maximum weight heuristic method;counterexample guided abstraction refinement method;program verification;software engineering;computer security;state space methods concrete software engineering computer science educational technology laboratories information systems computer security information security computer science education;computer science education;decision procedure;state space;computer science;educational technology;counterexample guided abstraction refinement method predicate abstraction program verification maximum weight heuristic method;predicate abstraction;concrete	The paper presents a new approach to computing the abstract state and a maximum weight heuristic method for finding the shortest counter-example in verification of imperative programs. The strategy is incorporated in a verification system based on the counterexample-guided abstraction refinement method. The proposed method slashes both the size of the abstract state space and the number of invokes of a decision procedure. A number of benchmarks are employed to evaluate the effectiveness of the approach.	benchmark (computing);decision problem;formal verification;heuristic;imperative programming;predicate abstraction;refinement (computing);state space	Li Li;Ming Gu;Xiaoyu Song;Jianmin Wang	2008	2008 2nd IFIP/IEEE International Symposium on Theoretical Aspects of Software Engineering	10.1109/TASE.2008.18	educational technology;concrete;computer science;state space;information security;theoretical computer science;high-level verification;programming language;information system;algorithm;functional verification	Logic	-16.00562389593716	29.06427500689542	150145
a841667119da7a3d13986221e3e8dcd6fbcc1d45	model generation for prs-like agents	model generation;model checking;bdi agents;bdi agent architectures;agent programming	We develop a sound foundation for model checking algorithms for the class of PRS-style BDI agents, by showing how a reachability graph for any given PRS-type agent can be constructed from the agent program, thus addressing a long-standing issue in the verification of BDI agents.	algorithm;model checking;procedural reasoning system;reachability	Wayne Wobcke;Marc Chee;Krystian Ji	2005		10.1145/1082473.1082658	model checking;simulation;computer science;artificial intelligence;computer security	AI	-9.674192600525446	26.268495762577693	150198
cbc4356e9375f2e7bf494ec01b907a829431cca7	regular-language semantics for a call-by-value programming language	programming language;game semantics;regular language;first order;regular expression	Abstract   We explain how game semantics can be used to reason about term equivalence in a finitary imperative first order language with arrays. For this language, the game-semantic interpretation of types and terms is fully characterized by their sets of complete plays. Because these sets are regular over the alphabet of moves, they are representable by (extended) regular expressions. The formal apparatus of game semantics is greatly simplified but the good theoretical properties of the model are preserved. The principal advantage of this approach is that it is mathematically elementary, while fully formalized. Since language equivalence for regular languages is decidable, this method of proving term equivalence is suitable for automation.	programming language;regular language	Dan R. Ghica	2001	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80958-7	formal language;discrete mathematics;language primitive;regular language;regular grammar;computer science;game semantics;first-order logic;mathematics;low-level programming language;programming language;generalized star height problem;operational semantics;regular expression;algorithm;semantics;context-sensitive language;computational semantics	PL	-12.276783453430745	18.68477647791845	150266
5b650541f20d9db770a36ea6308df7d6aa2214d2	extrapolating (omega-)regular model checking	infinite state systems;automata	(Omega-)Regular model checking is the name of a family of techniques in which states are represented by words, sets of states by finite automata on these objects, and transitions by finite automata operating on pairs of state encodings, i.e., finite-state transducers. In this context, the problem of computing the set of reachable states of a system can be reduced to the one of computing the iterative closure of the finite-state transducer representing its transition relation. In this tutorial article, we survey an extrapolation-based technique for computing the closure of a given transducer. The approach proceeds by comparing successive elements of a sequence of approximations of the iteration, detecting an “increment” that is added to move from one approximation to the next, and extrapolating the sequence by allowing arbitrary repetitions of this increment. The technique applies to finite-word and deterministic weak Büchi automata. Finally, we discuss the implementation of these results within the T(O)RMC toolsets and present case studies that show the advantages and the limits of the approach.	algorithm;approximation;automata theory;büchi automaton;cartesian closed category;convex set;domain-specific language;experiment;extrapolation;fast fourier transform;finite-state machine;finite-state transducer;graph product;iteration;iterative method;linear algebra;machine learning;model checking;numerical analysis;open system (computing);open system (systems theory);programming paradigm;queue (abstract data type);reachability;reverse monte carlo;sensor;simulation;string (computer science);transitive closure;tree automaton;turing completeness;verification and validation	Axel Legay	2011	International Journal on Software Tools for Technology Transfer	10.1007/s10009-011-0209-7	quantum finite automata;computer science;theoretical computer science;automaton;algorithm	Logic	-7.3565197744037985	24.030801765837964	150294
012776193343e53b195d79f5c73b6a36734da344	hierarchy of persistency with respect to the length of actions disability		"""The notion of persistency, based on the rule """"no action can disable another one"""" is one of the classical notions in concurrency theory. We recall two ways of generalization of this notion: the first is """"no action can kill another one"""" and the second """"no action can kill another enabled one"""". Afterwards we present an infinite hierarchy of computations in which one action disables another one for no longer than a specified number of steps (e/l-k-persistency). We prove that if an action is disabled, and not killed, by another one, it can not be postponed indefinitely. Finally we deal with decision problems about e/l-k persistency. We show that this kind of persistency is decidable with respect to steps, markings and nets."""	computation;concurrency (computer science);decision problem	Kamila Barylska;Edward Ochmanski	2012				Theory	-7.580301639421577	23.210323783201005	150555
2a5f780a956dcf5d9e1da25c2b308e804185f8fe	verifying intuition - ilf checks dawn proofs	verification;modelizacion;automatic proving;red petri;theorie conflit;demostracion automatica;teoria conflicto;theorem proving;modelisation;demonstration automatique;demonstration theoreme;theorem prover;distributed algorithm working notation dawn;algorithme reparti;algoritmo repartido;verificacion;demostracion teorema;petri net;distributed algorithm;modeling;notation travail algorithme reparti;regle preuve;conflict theory;reseau petri	The DAWN approach allows to model and verify distributed algorithms in an intuitive way. At a first glance, a DAWN proof may appear to be informal. In this paper, we argue that DAWN proofs are formal and can be checked for correctness fully automatically by automated theorem provers. The basic technique are proof rules which generate proof obligations. For the definition of the proof rules we adopt assertions and we introduce conflict formulas for algebraic Petri nets. Experiments show that the generated proof obligations can be automatically checked by theorem provers.		Thomas Baar;Ekkart Kindler;Hagen Völzer	1999		10.1007/3-540-48745-X_24	distributed algorithm;computer-assisted proof;probabilistically checkable proof;computer science;artificial intelligence;analytic proof;mathematics;distributed computing;automated theorem proving;proof assistant;programming language;structural proof theory;proof complexity;algorithm	Crypto	-13.991769540901755	22.081306336108877	150748
395cde4bfabd3a64267bebec940a809283c4bf08	coalgebraic description of generalized binary methods	model generation;oo programming;object oriented programming;binary methods;coalgebraic semantics;type classes;general binaries	We extend the Reichel-Jacobs coalgebraic account of specification and refinement of objects and classes in Object Oriented Programming to (generalized) binary methods. These are methods that take more than one parameter of a class type. Class types include sums and (possibly infinite) products type constructors. We study and compare two solutions for modeling generalized binary methods, which use purely covariant functors. In the first solution, which applies when we already have a class implementation, we reduce the behaviour of a generalized binary method to that of a bunch of unary methods. These are obtained by freezing the types of the extra class parameters to constant types. The bisimulation behavioural equivalence induced on objects by this model amounts to the greatest congruence w.r.t method application. Alternatively, we treat binary methods as graphs instead of functions, thus turning contravariant occurrences in the functor into covariant ones.	bisimulation;congruence of squares;graph (discrete mathematics);refinement (computing);turing completeness;type constructor;unary operation	Furio Honsell;Marina Lenisa;Rekha Redamalla	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.09.022	combinatorics;discrete mathematics;computer science;mathematics;programming language;object-oriented programming;algorithm	Logic	-11.515604432570319	18.629040417020548	150777
a255ce53558070088660867963b7b39bdb7fe0f7	coalgebraic component specification and verification in rsl	non structural system;group decision makin;specification and verification;non structural;state space methods specification languages network address translation programming sun computer science computer industry software engineering computer interfaces assembly systems;group decision making;non structural system renunciation group decision makin;group decision;renunciation	Research on non-structural system group decision-making problems largely depends on the knowledge and experience of the experts for tactical analysis. The usual method of voting may result in a great loss of information in case of much renunciation, and the accuracy of voting can therefore be directly influenced.This paper proposes a new solution to non-structural system group decision-making problems by taking advantage of the characteristics of correlate and the information easy to lose to make an overall analysis of the ayes, blackballs and renunciation polls for an accurate result.	bisimulation;component-based software engineering;modeling language;raise;refinement (computing);renderman shading language;simulation;specification language;turing completeness	Sun Meng;Bernhard K. Aichernig;Zhang Naixiao	2005	Sixth International Conference on Parallel and Distributed Computing Applications and Technologies (PDCAT'05)	10.1109/PDCAT.2005.100	simulation;group decision-making;computer science;theoretical computer science;operating system;machine learning;database;distributed computing;programming language;computer security;algorithm	HPC	-16.76261970858273	29.197908850001436	150950
81635ee41e3b6dfc6cc0f1c2aebedd68b03571c2	integrity checking in deductive databases	integrity checking;deductive databases	We describe the theory and implementation of a general theorem-proving technique for checking integrity of deductive databases recently proposed by Sadri and Kowalski. The method uses an extension of the SLDNF proof procedure and achieves the effect of the simplification algorithms of Nicolas, Lloyd, Topor et al, and Decker by reasoning forwards from the update and thus focusing on the relevant parts of the database and the relevant constraints.	algorithm;deductive database;level of detail	Robert A. Kowalski;Fariba Sadri;Paul Soper	1987			computer science	DB	-17.732598692735532	18.99743435871943	151155
24e85d28a1020b9201532998159f1c76c839ae13	call-by name partial evaluation of functional logic programs	functional logic programming;partial evaluation;functional programming	Partial evaluation is a method for program specialization based on fold/unfold transformations [4, 16]. Partial evaluation of functional programs uses only static values of given data to specialize the program. In logic programming, the so-called static/dynamic distinction is hardly present, whereas considerations of determinacy and choice points are far more important for control [8]. In this paper, we formalize a two-phase specialization method for a non-strict functional logic language which makes use of (normalizing) lazy narrowing to specialize the program w.r.t. a goal. The basic algorithm (first phase) is formalized as an instance of the framework for the partial evaluation of functional logic programs of [2], using lazy narrowing. However, the results inherited by [2] mainly regard the termination of the PE method, while the (strong) soundness and completeness results must be restated for the lazy strategy. A post-processing renaming scheme (second phase) for obtaining independence is then described and illustrated on the well-known matching example. We show that our method preserves the lazy narrowing semantics and that the inclusion of simplification steps in narrowing derivations can greatly improve control during specialization.	algorithm;emoticon;functional logic programming;indeterminacy in concurrent computation;lazy evaluation;level of detail;operational semantics;partial evaluation;partial template specialization;program transformation;requirement;sld resolution;strict function;turing completeness;two-phase locking;unfolding (dsp implementation);variable elimination;video post-processing;whole earth 'lectronic link	María Alpuente;Moreno Falaschi;Pascual Julián Iranzo;Germán Vidal	1996			functional logic programming;evaluation strategy;horn clause;concurrent constraint logic programming;partial evaluation;soundness;logic programming;algorithm;prolog;mathematics	PL	-16.66098312749202	20.351176813841523	151214
0e7058327c804b4195ab431d5a0ed68c35611e70	the interprocedural coincidence theorem	fixed point;flow analysis	"""We present an interprocedural generalization of the well-known (intraprocedural) Coincidence Theorem of Kam and Ullman, which provides a suucient condition for the equivalence of the meet over all paths (MOP) solution and the maximal xed point (MFP) solution to a data ow analysis problem. This generalization covers arbitrary imperative programs with recursive procedures, global and local variables, and formal value parameters. In the absence of procedures, it reduces to the classical intraprocedural version. In particular, our stack-based approach generalizes the coincidence theorems of Barth and Sharir/Pnueli for the same setup, which do not properly deal with local variables of recursive procedures. 1 Motivation Data ow analysis is a classical method for the static analysis of programs that supports the generation of eecient object code by \optimizing"""" compilers (cf. He, MJ]). For imperative languages, it provides information about the program states that may occur at some given program points during execution. Theoretically well-founded are data ow analyses that are based on abstract interpretation (cf. CC1]). The point of this approach is to replace the \full"""" semantics by a simpler more abstract version, which is tailored to deal with a speciic problem. Usually, the abstract semantics is speciied by a local semantic functional, which gives abstract meaning to every program statement in terms of a transformation function from a lattice C into itself. The elements of C express the data ow information of interest. The (global) abstract semantics then results from one of the following two globalization strategies; the \operational"""" meet over all paths (MOP) strategy, and the \denotational"""" maximal xed point (MFP) strategy 1 in the sense of Kam and Ullman KU]. 2 The MOP-strategy directly mimics possible program executions: it \meets"""" (intersects) all information corresponding to program paths reaching the program point under consideration. This speciies the optimal result of a globalization but is in general not eeective. The MFP-strategy iteratively approximates the greatest solution of a system of equations that express consistency between preconditions and post-conditions that are given in terms of data ow information: the precondition of a statement must be implied by each of the post-conditions of the predecessors, and the post-condition must be implied by the result of transforming the precondition according to the (abstract) meaning of the statement. In general, this leads to a suboptimal but algorithmic description. 1 These are the strategies that lead to the MOP-solution and MFP-solution, respectively. 2 The operational and …"""	abstract interpretation;approximation;compiler;computation;imperative programming;interdependence;iteration;iterative method;lattice c;local variable;maximal set;object code;postcondition;precondition;quantum superposition;recursion;sethi–ullman algorithm;stack (abstract data type);stack-oriented programming language;statement (computer science);turing completeness;whole earth 'lectronic link	Jens Knoop;Bernhard Steffen	1992		10.1007/3-540-55984-1_13	discrete mathematics;computer science;data-flow analysis;mathematics;fixed point;programming language;algorithm	PL	-16.52020268585664	20.89151596116869	151227
37df835322dd97f01848db10a16666f67315eafc	towards an effective decision procedure for ltl formulas with constraints		This paper presents an ongoing work that is part of a more wide-ranging project whose final scope is to define a method to validate LTL formulas w.r.t. a program written in the timed concurrent constraint language tccp, which is a logic concurrent constraint language based on the concurrent constraint paradigm of Saraswat. Some inherent notions to tccp processes are non-determinism, dealing with partial information in states and the monotonic evolution of the information. In order to check an LTL property for a process, our approach is based on the abstract diagnosis technique. The concluding step of this technique needs to check the validity of an LTL formula (with constraints) in an effective way. In this paper, we present a decision method for the validity of temporal logic formulas (with constraints) built by our abstract diagnosis technique.	concurrent constraint logic programming;constraint (mathematics);decision problem;linear temporal logic;long division;nondeterministic algorithm;parallel computing;programming paradigm	Marco Comini;Laura Titolo;Alicia Villanueva	2013	CoRR		concurrent constraint logic programming;discrete mathematics;real-time computing;mathematics;algorithm	Logic	-13.606384631096466	22.276927223625744	151340
8707dffd477c2d806762c31971fedaddab82f31d	efficient bisimilarities from second-order reaction semantics for pi-calculus	second order	We investigate Leifer and Milner RPO approach for deriving efficient (finitely branching) LTS’s and bisimilarities for π-calculus. To this aim, we work in a category of second-order term contexts and we apply a general pruning technique, which allows to simplify the set of transitions in the LTS obtained from the original RPO approach. The resulting LTS and bisimilarity provide an alternative presentation of symbolic LTS and Sangiorgi’s open bisimilarity.	bisimulation;path ordering (term rewriting);π-calculus	Pietro Di Gianantonio;Svetlana Jaksic;Marina Lenisa	2010		10.1007/978-3-642-15375-4_25	discrete mathematics;computer science;theoretical computer science;second-order logic;algorithm	SE	-10.53663809473026	21.077802922551832	151375
7938cc052484245744379842ef526f9e07fe51f4	generating model checkers from algebraic specifications	algebraic specification;generic model;temporal logic;real time;compiler;automatic generation;model checking;macro processor;target language;source language;language;macro operation;binary decision diagram	There is a great deal of research aimed toward the development of temporal logics and model checking algorithms which can be used to verify properties of systems. In this paper, we present a methodology and supporting tools which allow researchers and practitioners to automatically generate model checking algorithms for temporal logics from algebraic specifications. These tools are extensions of algebraic compiler generation tools and are used to specify model checkers as mappings of the form {\cal M}{\cal C}: L_s\,{\to}\,L_t, where Ls is a temporal logic source language and Lt is a target language representing sets of states of a model M, such that {\cal M}{\cal C}(f\,{\in}\,L_s) = \{ s\,{\in}\,M \mid s \models f \}. The algebraic specifications for a model checker define the logic source language, the target language representing sets of states in a model, and the embedding of the source language into the target language. Since users can modify and extend existing specifications or write original specifications, new model checking algorithms for new temporal logics can be easily and quickly developeds this allows the user more time to experiment with the logic and its model checking algorithm instead of developing its implementation. Here we show how this algebraic framework can be used to specify model checking algorithms for CTL, a real-time CTL, CTLa, and a custom extension called CTLe that makes use of propositions labeling the edges as well as the nodes of a model. We also show how the target language can be changed to a language of binary decision diagrams to generate symbolic model checkers from algebraic specifications.	model checking	Teodor Rus;Eric Van Wyk;Tom Halverson	2002	Formal Methods in System Design	10.1023/A:1014742013173	model checking;compiler;temporal logic;computer science;theoretical computer science;language;programming language;binary decision diagram;algorithm	Logic	-16.10292580492922	27.60062425972249	151390
34c7c41295c3ced1dacbbe1d84ad3d692240c437	formally checking large data sets in the railways		This article presents industrial experience of validating large data sets against specification written using the B / Event-B mathematical language and the ProB model checker.	b-method;constraint programming;data validation;model checking;solver	Thierry Lecomte;Lilian Burdy;Michael Leuschel	2012	CoRR		computer science;data mining;database;programming language	SE	-18.4184849733749	27.23894636641733	151400
311b50c7d0b6212af52b6fb23920c80d72bc47f1	model checking and fault tolerance	tolerancia falta;algebraic approach;fault tolerant;abstraction;concurrent program;program verification;abstraccion;verificacion programa;programming theory;model checking;state space;fault tolerance;programa competidor;theorie programmation;fault model;verification programme;tolerance faute;programme concurrent	Here we show that the weakly simulates relation preserves certain tempora l properties. The preserved properties are those that can be expressed in a fragment of the modal mu-calculus [13], a simple but expressive temporal logic. Formulas of the diamond-free modal mu-calculus have the following abs t rac t syntax, where L ranges over sets of actions and X ranges over proposit ional variables: The propositional connectives have their usual meaning. Formula [LImb holds of a process P if every weak transit ion from P leads to a process satisfying formula r The fixed-point operators allow propert ies to be defined recursively. They are binders; free occurrences of X in r are bound in u X . r and #X.r Falsity, writ ten f f , is an abbreviat ion for # X . X . We also write [ [cq, . . . , c~n] as an abbreviat ion for [{c~1,... , an}f , and f L ] as an abbreviat ion for ~dct L]. The semantics of a formula is given as the set of processes tha t it satisfies. Let 7= (S,A,{2* I a E A}) be a transit ion system, with ~in A. Let V be	fault tolerance;free variables and bound variables;logical connective;modal μ-calculus;model checking;recursion;temporal logic	Glenn Bruns;Ian Sutherland	1997		10.1007/BFb0000462	fault tolerance;real-time computing;computer science;theoretical computer science;algorithm	AI	-12.323823991553573	23.06350338837779	151443
09d185af102f93fecd4c022728231b4c04784a96	parameterised model checking for alternating-time temporal logic		We investigate the parameterised model checking problem for specifications expressed in alternating-time temporal logic. We introduce parameterised concurrent game structures representing infinitely many games with different number of agents. We introduce a parametric variant of ATL to express properties of the system irrespectively of the number of agents present in the system. While the parameterised model checking problem is undecidable, we define a special class of systems on which we develop a sound and complete counter abstraction technique. We illustrate the methodology here devised on the prioritised version of the train-gate-controller.	atlas transformation language;alternating-time temporal logic;model checking;undecidable problem	Panagiotis Kouvaros;Alessio Lomuscio	2016		10.3233/978-1-61499-672-9-1230	alternating-time temporal logic;linear temporal logic;artificial intelligence;machine learning;model checking;interval temporal logic;computer science	AI	-12.562910965887063	23.10828427516569	151516
adce39db382d8e115884c1574022c3e894ae6532	communication in concurrent dynamic logic	concurrent dynamic logic	Communication mechanisms are introduced into the program schemes of Concurrent Dynamic Logic, on both the propositional and the first-order levels. The elfects of these mechanisms (particularly, channels, shared variables, and “message collectors”) on issues of expressiveness and decidability are investigated. In general, we find that both respects are dominated by the extent to which the capabilities of synchronization and (unbounded) counting are enabled in the communication scheme. ‘i? 1987 Academic Press. Inc.	first-order predicate;shared variables;synchronization (computer science)	David Peleg	1987	J. Comput. Syst. Sci.	10.1016/0022-0000(87)90035-3	concurrent constraint logic programming;concurrency;temporal logic of actions	Theory	-11.769933745255358	21.07260827616817	151578
ab7ab805b3a286c84797b7bf0d60096571fb4ec9	carrier arrays: an idiom-preserving extension to apl	data structure	The idiomatic APL programming style is limited by the constraints of a rectangular, homogeneous array as a data structure. Non-scalar data is difficult to represent and manipulate, and the non-scalar APL functions have no uniform extension to higher rank arrays. The carrier array is an extension to APL which addresses these limitations while preserving the economical APL style. A carrier array is a ragged array with an associated partition which allows functions to be applied to subarrays in parallel. The primitive functions are given base definitions on scalars and vectors, and they are extended to higher rank arrays by uniform application mechanisms. Carrier arrays also allow the last dimensions of an array to be treated as a single datum; the primitive functions are given extended definitions on scalars and vectors of this non-scalar data.This paper defines the carrier array and gives the accompanying changes to the definitions of the APL primitive functions. Examples of programming with carrier arrays are presented, and implementation issues are discussed.	apl;applicative programming language;array data structure;brian;circa;compiler;control flow;definition;geodetic datum;iteration;iverson award;jean;jagged array;jenkins;john d. wiley;morrow pivot ii;netware;programming style;recursion;schmidt decomposition;symposium on principles of programming languages;thomas j. watson research center;tiling array;eric	P. Geoffrey Lowney	1981		10.1145/567532.567533	data structure;computer science;theoretical computer science;programming language;algorithm	PL	-11.50533928032836	32.0034887236465	151599
62bcdc8810938cacc12da4486f7cce670c3a0445	low-complexity switching controllers for safety using symbolic models		In this paper, we consider the problem of synthesizing low-complexity safety controllers for incrementally stable switched systems. For that purpose, we establish a new approximation result for the computation of symbolic models that are approximately bisimilar to a given switched system. The main advantage over existing results is that it allows us to design quantized switching controllers for safety specifications; these can be computed offline and therefore the online execution time is greatly reduced. Then, we present a technique to reduce the memory needed to store the control law by borrowing ideas from algebraic decision diagrams for compact function representation and by exploiting the non-determinism inherent to safety controllers. We show the merits of our approach by applying it to a simple model of temperature regulation in a building.	approximation;bisimulation;computation;data structure;diagram;function representation;online and offline;optimal control;powerset construction;run time (program lifecycle phase)	Antoine Girard	2012		10.3182/20120606-3-NL-3011.00022	control engineering;real-time computing;computer science;control theory	Embedded	-8.972896018616904	29.08685499437545	151669
7b21b7ca1dd47fbb3118dc3d2414cc080a5d5c69	an abstraction-refinement methodology for reasoning about network games		Network games (NGs) are played on directed graphs and are extensively used in network design and analysis. Search problems for NGs include finding special strategy profiles such as a Nash equilibrium and a globally optimal solution. The networks modeled by NGs may be huge. In formal verification, abstraction has proven to be an extremely effective technique for reasoning about systems with big and even infinite state spaces. We describe an abstraction-refinement methodology for reasoning about NGs. Our methodology is based on an abstraction function that maps the state space of an NG to a much smaller state space. We search for a global optimum and a Nash equilibrium by reasoning on an underand an overapproximation defined on top of this smaller state space. When the approximations are too coarse to find such profiles, we refine the abstraction function. Our experimental results demonstrate the efficiency of the methodology.		Guy Avni;Shibashis Guha;Orna Kupferman	2017		10.24963/ijcai.2017/11	machine learning;artificial intelligence;computer science;abstraction	AI	-11.704991252014784	29.425047529732687	151682
389e22d39d78663da3b24e971f6f6d5dd115817a	bounded treewidth as a key to tractability of knowledge representation and reasoning	abduction;monadic datalog;representacion conocimientos;disjunctive programming;complexite calcul;efficient algorithm;programmation disjonctive;fixed parameter tractable;temps lineaire;circonscription;database;base dato;fixed parameter tractability;intelligence artificielle;logical programming;tiempo lineal;knowledge representation and reasoning;circumscription;closed world reasoning;complejidad computacion;datalog;abduccion;programmation logique;computational complexity;linear time;base de donnees;representation connaissance;artificial intelligence;treewidth;anchura arbol;inteligencia artificial;logic programs;knowledge representation;largeur arborescente;programacion logica;programacion disyuntiva;disjunctive logic programming;circonscripcion	Several forms of reasoning in AI – like abduction, closed world reasoning, circumscription, and disjunctive logic programming – are well known to be intractable. In fact, many of the relevant problems are on the second or third level of the polynomial hierarchy. In this paper, we show how the powerful notion of treewidth can be fruitfully applied to this area. In particular, we show that all these problems become tractable (actually, even solvable in linear time), if the treewidth of the involved formulae (or of the disjunctive logic programs, resp.) is bounded by some constant. Experiments with a prototype implementation prove the feasibility of this new approach, in principle, and also give us hints for necessary improvements. In many areas of computer science, bounded treewidth has been shown to be a realistic and practically relevant restriction. We thus argue that bounded treewidth is a key factor in the development of efficient algorithms also in knowledge representation and reasoning – despite the high worst case complexity of the problems of interest.	abductive reasoning;algorithm;best, worst and average case;circumscription (logic);cobham's thesis;computer science;decision problem;disjunctive normal form;knowledge representation and reasoning;logic programming;polynomial hierarchy;prototype;time complexity;treewidth;worst-case complexity	Georg Gottlob;Reinhard Pichler;Fang Wei-Kleiner	2006		10.1016/j.artint.2009.10.003	time complexity;knowledge representation and reasoning;discrete mathematics;computer science;artificial intelligence;mathematics;datalog;treewidth;computational complexity theory;circumscription;algorithm	AI	-7.701367110116194	19.542748894385678	151715
00d8d50aadefdda89f3daa42fb5a32008a06f10f	exploiting shared structure in software verification conditions		Despite many advances, today’s software model checkers and extended static checkers still do not scale well to large code b ases, when verifying properties that depend on complex interprocedural flow o f data. An obvious approach to improve performance is to exploit software stru cture. Although a tremendous amount of work has been done on exploiting struct ure at various levels of granularity, the fine-grained shared structure am ong multiple verification conditions has been largely ignored. In this paper, we f ormalize the notion of shared structure among verification conditions, propose a novel and efficient approach to exploit this sharing, and provide experimental results that this approach can significantly improve the performance of verifica tion, even on pathand context-sensitive and dataflow-intensive properties.	context-sensitive grammar;dataflow;decision problem;formal verification;invariant (computer science);level structure;model checking;scalability;software verification;struct (c programming language);verification and validation	Domagoj Babic;Alan J. Hu	2007		10.1007/978-3-540-77966-7_15	real-time computing;computer science;theoretical computer science;distributed computing;functional verification	Logic	-17.42550824614124	28.575850332116033	151873
04b11eef6d30f0c338a33f17f0f8a9f85f57a54f	counterexample-guided abstraction refinement for the analysis of graph transformation systems	developpement logiciel;dynamic change;graph theory;distributed system;topology;transformacion grafo;movilidad;teoria grafo;systeme reparti;informatique mobile;renemen t;red petri;mobility;systems engineering;workingpaper;topologie;abstraction;mobilite;refinement;abstraccion;theorie graphe;graph transformation;specification language;topologia;refinement method;graphersetzungssystem;transformation graphe;sistema repartido;desarrollo logicial;software development;ingenierie systeme;lenguaje especificacion;methode raffinement;mobile computing;petri net;metodo afinamiento;langage specification;reseau petri;counterexample	Graph transformation systems are a general specification language for systems with dynamically changing topologies, such as mobile and distributed systems. Although in the last few years several analysis and verification methods have been proposed for graph transformation systems, counterexample-guided abstraction refinement has not yet been studied in this setting. We propose a counterexample-guided abstraction refinement technique which is based on the over-approximation of graph transformation systems (gts) by Petri nets. We show that a spurious counterexample is caused by merging nodes during the approximation. We present a technique for identifying these merged nodes and splitting them using abstraction refinement, which removes the spurious run. The technique has been implemented in the Augur tool and experimental results are discussed.	approximation;approximation algorithm;automated theorem proving;data-flow analysis;dataflow;distributed computing;graph (discrete mathematics);graph rewriting;graphical user interface;mutual exclusion;petri net;pointer (computer programming);red–black tree;refinement (computing);shape analysis (digital geometry);specification language;three-valued logic;unfolding (dsp implementation)	Barbara König;Vitaly Kozyura	2006		10.1007/11691372_13	specification language;computer science;artificial intelligence;counterexample;theoretical computer science;software development;refinement;abstraction;programming language;mobile computing;petri net;algorithm	PL	-16.556074395501266	27.995752455487317	151951
1429c03d54888a799c4343f0dde870b72002a1eb	automatically proving termination where simplification orderings fail	inequality constraint;satisfiability;term rewrite system;ordered by external client;wiskunde en informatica wiin	To prove termination of term rewriting systems (TRSs), several methods have been developed to synthesize suitable well-founded orderings automatically. However, virtually all orderings that are amenable to automation are so-called simpli cation orderings. Unfortunately, there exist numerous interesting and relevant TRSs that cannot be oriented by orderings of this restricted class and therefore their termination cannot be proved automatically with the existing techniques. In this paper we present a new automatic approach which allows to apply the standard techniques for automated termination proofs to those TRSs where these techniques failed up to now. For that purpose we have developed a procedure which, given a TRS, generates a set of inequalities (constraints) automatically. If there exists a well-founded ordering satisfying these constraints, then the TRS is terminating. It turns out that for many TRSs where a direct application of standard techniques fails, these standard techniques can nevertheless synthesize a well-founded ordering satisfying the generated constraints. In this way, termination of numerous (also non-simply terminating) term rewriting systems can be proved fully automatically.	constraint (mathematics);existential quantification;newman's lemma;rewriting;termination analysis;text simplification	Thomas Arts;Jürgen Giesl	1997		10.1007/BFb0030602	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-14.738823537913973	21.600620264050153	151971
ed95a802e494bdc918e946de4b66e8cce81fe025	a linear algorithm of a deadlock avoidance for nonpreemptible resources	deadlock avoidance		algorithm;deadlock	Ewa Klupsz	1984	Inf. Process. Lett.	10.1016/0020-0190(84)90103-0	computer science;mathematics	DB	-5.603231451143689	29.768580772611397	152120
5bee73c66c924ef6187f525ffd8a8a8a0fba1aec	actions, wreath products of c-varieties and concatenation product	language class;closure;automata estado finito;language theory;varieties of recognizable languages;concatenacion;teoria lenguaje;definicion;equivalence;variete algebrique;concatenation;produit couronne;caracterisation algebrique;definition;informatique theorique;langage reconnaissable;classe langage;wreath product;lenguaje reconocible;finite automata;uct;recognizable language;finite automaton;cerradura;automate fini;algebraic variety;equivalencia;theorie langage;variedad algebraica;fermeture;computer theory;clase lenguaje;informatica teorica	The framework of C-varieties, introduced by the third author, extends the scope of Eilenberg’s variety theory to new classes of languages. In this paper, we first define C-varieties of actions, which are closely related to automata, and prove their equivalence with the original definition of C-varieties of stamps. Next, we complete the study of the wreath product initiated by Ésik and Ito by extending its definition to C-varieties in two different ways, which are proved to be equivalent. We also state an extension of the wreath product principle, a standard tool of language theory. Finally, our main result generalizes to C-varieties the algebraic characterization of the closure under product of a variety of languages. Through the work of Eilenberg [3] and Schützenberger [13], the theory of varieties of finite semigroups and monoids emerged as an essential tool in the study of the algebra underlying families of regular languages. The current literature on the subject (see [10, 2] for a comprehensive bibliography) attests to the richness of this theory and the diversity of its applications in an increasing number of research fields including automata theory and formal languages but also model theory and logic, circuit complexity, communication complexity, discrete dynamical systems, etc. However, some important families of languages arising from open problems in language theory (the generalized star height problem), logic and circuit complexity [17], do not form varieties of languages in the sense originally described by Eilenberg. To study these new varieties of languages, Straubing [18] recently introduced the notion of C-varieties. A similar notion was introduced independently by Ésik and Ito [6]. The formal definition of a C-variety of languages is quite similar to Eilenberg’s except that it only requires closure under inverse images of morphisms belonging to some natural class C. (In the important applications, this class C is typically either the class of all length-preserving morphisms, or of all length-multiplying morphisms. In contrast, the theory developed by Eilenberg requires closure under inverse images of LIAFA, Université Paris VII and CNRS, Case 7014, 2 Place Jussieu, 75251 Paris Cedex 05, France. Laura.Chaubard@liafa.jussieu.fr LIAFA, Université Paris VII and CNRS, Case 7014, 2 Place Jussieu, 75251 Paris Cedex 05, France. Jean-Eric.Pin@liafa.jussieu.fr Department of Computer Science, Boston College, Chestnut Hill, MA 02167, USA straubin@cs.bc.edu	automata theory;circuit complexity;communication complexity;computer science;concatenation;dynamical system;formal language;generalized star height problem;linear algebra;regular language;turing completeness;vii;x-machine	Laura Chaubard;Jean-Éric Pin;Howard Straubing	2006	Theor. Comput. Sci.	10.1016/j.tcs.2006.01.039	concatenation;equivalence;combinatorics;algebraic variety;definition;wreath product;philosophy of language;pure mathematics;closure;mathematics;finite-state machine;algorithm;algebra	Theory	-5.042764866524476	19.008614000424558	152132
3ffe087b8378ec3a3efca6423ade668a04281aa8	thesis: strategies and analysis techniques in functional program optimization	program analysis and transformation;functional and logic programming;rewriting narrowing strategies;term rewriting systems;program analysis;functional programming	This paper abstracts the contents of the Ph.D. dissertation which has been recently defended by the author. Functional programs are commonly modelled by term rewriting systems. The execution of such programs often gives rise to useless, dangerous, and inefficient evaluation sequences which must be avoided in order to improve their computational behaviour. The thesis presents novel methods and techniques to optimize term rewriting systems either by defining new strategies to execute programs or by analyzing and transforming programs in order to improve their computational behaviour.	computation;functional programming;lazy evaluation;mathematical optimization;program optimization;rewriting	Santiago Escobar	2004	AI Commun.			AI	-18.80388328910792	22.976251225232797	152181
f15503dc750914bcd8d15b28ed384405cc408bdc	specifying and reasoning about dynamic access-control policies	debugging;puesta a punto programa;controle acces;securite informatique;modular reasoning;program verification;automated reasoning;specification language;debogage;computer security;dynamic environment;verificacion programa;raisonnement automatique;access control policy;first order temporal logic;policy evaluation;logique ordre 1;seguridad informatica;decidibilidad;lenguaje especificacion;access control;decidabilite;verification programme;langage specification;first order logic;decidability;razonamiento automatico;logica orden 1	Access-control policies have grown from simple matrices to non-trivial specifications written in sophisticated languages. The inc reasing complexity of these policies demands correspondingly strong automated r easoning techniques for understanding and debugging them. The need for these tec hniques is even more pressing given the rich and dynamic nature of the enviro ments in which these policies evaluate. We define a framework to represent t h behavior of accesscontrol policies in a dynamic environment. We then specify s everal interesting, decidable analyses using first-order temporal logic. Our wo rk illustrates the subtle interplay between logical and state-based methods, par ticul rly in the presence of three-valued policies. We also define a notion of policy eq uivalence that is especially useful for modular reasoning.	access control;debugging;first-order logic;first-order predicate;temporal logic	Daniel J. Dougherty;Kathi Fisler;Shriram Krishnamurthi	2006		10.1007/11814771_51	decidability;specification language;computer science;artificial intelligence;access control;theoretical computer science;first-order logic;automated reasoning;programming language;debugging;algorithm	PL	-18.007184508198005	24.07665298595435	152272
724d79f796cc4bb7b5bddeecc36dc5981cdf1722	a note on the verification of automata specifications of probabilistic real-time systems	nondeterminism;probabilistic verification;timed automata;real time systems		automata theory;automaton;real-time clock;real-time computing	Arnaldo V. Moura;Guilherme A. Pinto	2002	Inf. Process. Lett.	10.1016/S0020-0190(01)00288-5	real-time computing;computer science;theoretical computer science;automata theory;timed automaton;algorithm	Logic	-10.755264821751144	26.005167885303628	152454
9576ad75d5cb92f11b65e800f52093acbbb33401	exact intersection type abstractions for safety checking of recursion schemes	higher order model checking;abstract interpretation;intersection types	Higher-order recursion schemes are a class of higher-order programs built from a collection of first-order constants using higher-order functions and general recursion. Since the trees that they define have many decidable properties, they have become widely studied in the context of the verification through higher-order model checking.  We present a new semantic framework, in which recursion schemes can be extended by interpreted constants that are modelled concretely as a continuous function over domains and abstractly as an intersection type. When the intersection type of each constant is shown to be an exact abstract interpretation of the concrete semantics of that constant then intersection type assignment will characterise safety property checking for arbitrary terms. Since we focus on finite intersection type systems, decidability of the property checking problem is an immediate corollary.	abstract interpretation;correctness (computer science);deterministic automaton;first-order predicate;higher-order function;kind (type theory);map;model checking;recursion;type system	Steven J. Ramsay	2014		10.1145/2643135.2643142	intersection;algorithm	PL	-10.578973425672242	22.581610846504816	152620
906a205dda79cd0a6c84b7001d8e90baffeb6388	correctness of monadic state: an imperative call-by-need calculus	observational equivalence;lambda calculus;intermediate language;side effect;target language;semantic gap;source language;data structure	The extension of Haskell with a built-in state monad combines mathematical elegance with operational efficiency: ?Semantically, at the source language level, constructs that act on the state are viewed as functions that pass an explicit store data structure around.?Operationally, at the implementation level, constructs that act on the state are viewed as statements whose evaluation has the side-effect of updating the implicit global store in place.There are several unproven conjectures that the two views are consistent. Recently, we have noted that the consistency of the two views is far from obvious: all it takes for the implementation to become unsound is one judiciously-placed beta-step in the optimization phase of the compiler. This discovery motivates the current paper in which we formalize and show the correctness of the implementation of monadic state. For the proof, we first design a typed call-by-need language that models the intermediate language of the compiler, together with a type-preserving compilation map. Second, we show that the compilation is semantics-preserving by proving that the compilation of every source axiom yields an observational equivalence of the target language. Because of the wide semantic gap between the source and target languages, we perform this last step using a number of intermediate languages. The imperative call-by-need lambda-calculus is of independent interest for reasoning about system-level Haskell code providing services such as memo-functions, generation of new names, etc, and is the starting point for reasoning about the space usage of Haskell programs.	correctness (computer science);imperative programming;lazy evaluation	Zena M. Ariola;Amr Sabry	1997	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80690-5	data structure;computer science;theoretical computer science;lambda calculus;mathematics;programming language;intermediate language;side effect;algorithm;semantic gap	PL	-17.60173537370224	20.11582888987787	152980
83552780ca3cc4aaa446ac8b10a3ecee53232aa1	a transformation system for concurrent processes	langage fonctionnel;programmation technique;concurrent language;goal orientation;tecnica programacion;complexite calcul;lenguaje funcional;program transformation;transformation programme;complejidad computacion;semantic information;transformacion programa;programming theory;logica algoritmica;technique programmation;computational complexity;transition systems;algorithmic logic;theorie programmation;program analysis;functional language;logique algorithmique;concurrent process	Program transformation techniques have been extensively studied in the framework of functional and logic languages, where they were applied mainly to obtain more efficient and readable programs. All these works are based on the Unfold/Fold program transformation method developed by Burstall and Darlington in the context of their recursive equational language. The use of Unfold/Fold based transformations for concurrent languages is a relevant issue that has not yet received an adequate attention. In this paper we define a transformation methodology for CCS. We give a set of general rules which are a specialization of classical program transformation rules, such as Fold and Unfold. Moreover, we define the general form of other rules, “oriented” to the goal of a transformation strategy, and we give conditions for the correctness of these rules. We prove that a strategy using the general rules and a set of goal oriented rules is sound, i.e. it transforms CCS programs into equivalent ones. We show an example of application of our method. We define a strategy to transform, if possible, a full CCS program into an equivalent program whose semantics is a finite transition system. We show that, by means of our methodology, we are able to a find finite representations for a class of CCS programs which is larger than the ones handled by the other existing methods. Our transformational approach can be seen as unifying in a common framework a set of different techniques of program analysis. A further advantage of our approach is that it is based only on syntactic transformations, thus it does not requires any semantic information.	correctness (computer science);fold (higher-order function);functional programming;human-readable medium;logic programming;partial template specialization;program analysis;program transformation;recursion;transition system	Nicoletta De Francesco;Antonella Santone	1998	Acta Informatica	10.1007/s002360050151	program analysis;computer science;artificial intelligence;goal orientation;mathematics;programming language;computational complexity theory;functional programming;algorithm	PL	-18.113090430251827	22.38617441891908	153014
8790679fdb76661f568c842b3c0a512fcab5e8aa	difunctorial semantics of object calculus	recursion operator;object oriented programming;category theory	In this paper we give a denotational model for Abadi and Cardelli’s first order object calculus FOb1+×μ (without subtyping) in the category pCpo. The key novelty of our model is its extensive use of recursively defined types, supporting self-application, to model objects. At a technical level, this entails using some sophisticated techniques such as Freyd’s algebraic compactness to guarantee the existence of the denotations of the object types. The last sections of the paper demonstrates that the canonical recursion operator inherent in our semantics is potentially useful in object-oriented programming. This is witnessed by giving a straightforward translation of algebraic datatypes into so called wrapper classes.	algebraic data type;data transfer object;denotational semantics;domain theory;inductive type;intuitionistic type theory;linear algebra;map;mathematical induction;object type (object-oriented programming);recursion;recursive data type;recursive definition;turing completeness;universal instantiation	Johan Glimming;Neil Ghani	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.09.012	discrete mathematics;computer science;mathematics;programming language;object-oriented programming;algorithm;category theory	PL	-13.513463403727973	18.7030152398903	153016
9c58ef1700acd53c596512280c7dc6196f45573e	verification of the observer property in discrete event systems	observer property;abstractions observer property discrete event system des models natural projection op verifier nondeterministic automata nonrelevant events quadratic complexity;journal article;observer property discrete event systems natural projections;observers discrete event systems;discrete event systems;natural projections;automata observers complexity theory discrete event systems context testing supervisory control	The observer property is an important condition to be satisfied by abstractions of Discrete Event System (DES) models. This technical note presents a new algorithm that tests if an abstraction of a DES obtained through natural projection has the observer property. The procedure, called OP-Verifier, can be applied to (potentially nondeterministic) automata, with no restriction on the existence of cycles of “non-relevant” events. This procedure has quadratic complexity in the number of states. The performance of the algorithm is illustrated by a set of experiments.	automata theory;deterministic automaton;experiment;nondeterministic algorithm;nondeterministic finite automaton;reachability;search algorithm;verification and validation	Patrícia Nascimento Pena;Hugo J. Bravo;Antonio Eduardo Carrilho da Cunha;Robi Malik;Stéphane Lafortune;José Eduardo Ribeiro Cury	2014	IEEE Transactions on Automatic Control	10.1109/TAC.2014.2298985	control engineering;discrete mathematics;discrete event dynamic system;discrete system;control theory;mathematics	Logic	-6.948908601313993	26.365007381176905	153130
63304481157688e13f211dad6a71fd7fae2378d6	reliability assessment for distributed systems via communication abstraction and refinement	mdps;reliability assessment;probabilistic model checking	Distributed systems like cloud-based services are ever more popular. Assessing the reliability of distributed systems is highly non-trivial. Particularly, the order of executions among distributed components adds a dimension of non-determinism, which invalidates existing reliability assessment methods based on Markov chains. Probabilistic model checking based on models like Markov decision processes is designed to deal with scenarios involving both probabilistic behavior (e.g., reliabilities of system components) and non-determinism. However, its application is currently limited by state space explosion, which makes reliability assessment of distributed system particularly difficult. In this work, we improve the probabilistic model checking through a method of abstraction and reduction, which controls the communications among system components and actively reduces the size of each component. We prove the soundness and completeness of the proposed approach. Through an implementation in a software toolkit and evaluations with several systems, we show that our approach often reduces the size of the state space by several orders of magnitude, while still producing sound and accurate assessment.	cloud computing;distributed computing;markov chain;markov decision process;model checking;nondeterministic algorithm;reduction (complexity);refinement (computing);soundness (interactive proof);state space;statistical model	Lin Gui;Jun Sun;Yang Liu;Jin Song Dong	2015		10.1145/2771783.2771794	reliability engineering;computer science;theoretical computer science;data mining;abstraction model checking	SE	-11.090808073622846	28.48561855267318	153249
92e47c3d375400b318c02f3a8c82fc43e8ced31e	verification of cyber-physical systems based on differential-algebraic temporal dynamic logic			cyber-physical system	Xiaoxiang Zhai;Bixin Li;Min Zhu;Jiakai Li;Qiaoqiao Chen;Shunhui Ji	2012			computer science;data mining;multimodal logic;interval temporal logic;algebraic number;theoretical computer science;cyber-physical system;dynamic logic (digital electronics);runtime verification;high-level verification;temporal logic of actions	EDA	-12.802394687082373	23.365915133324496	153316
45a6b0dc26843ddf19ef430fafadad312f8be854	a correct and useful incremental copying garbage collector	incremental copying garbage collection;two dimensions;labeled transition system;real time;garbage collection;garbage collector	Designing a garbage collector with real-time properties is a particularlydifficult task, involving the construction of both an incremental run-timealgorithm as well as methods enabling a priori reasoning about schedulability in two dimensions (time and memory usage in conjunction). In order to comply with such ambitious goals with any amount of formal rigor, a comprehensive understanding of the actual algorithm used is of course a fundamental requirement. In this paper we present a formal model of an incremental copying garbage collector, where each atomic increment is modeled as a transition between states of a heap process. Soundness of the algorithm is shown by proving that the garbage collecting heap process is weakly bisimilar to a non-collecting heap with infinite storage space. In addition, we show that our collector is both terminating and useful, in the sense that it actually recovers the unreachable parts of any given heap in a finite number of steps.	bisimulation;cheney's algorithm;garbage collection (computer science);mathematical model;newman's lemma;real-time clock;scheduling (computing);unreachable memory	Martin Kero;Johan Nordlander;Per Lindgren	2007		10.1145/1296907.1296924	binomial heap;manual memory management;garbage;parallel computing;real-time computing;ephemeron;computer science;operating system;garbage in, garbage out;distributed computing;data pre-processing;garbage collection;programming language;mark-compact algorithm	PL	-18.140070092475643	30.99480558617033	153368
1d72c1bdc2ea68fcd2bcb9c84f006a694b169158	a structural soundness proof for shivers's escape technique - a case for galois connections	control flow;structural soundness proof;higher-order program fragment;galois connection;escape technique;abstract interpretation;soundness proof	Shivers’s escape technique enables one to analyse the control flow of higher-order program fragments. It is widely used, but its soundness has never been proven. In this paper, we present the first soundness proof for the technique. Our proof is structured as a composition of Galois connections and thus rests on the foundations of abstract interpretation.	abstract interpretation;control flow	Jan Midtgaard;Michael D. Adams;Matthew Might	2012		10.1007/978-3-642-33125-1_24	algorithm	PL	-14.600027973663593	21.844174549706636	153515
3c12e099d7c69e6b0cb0456dcd2b49eb9d72cd32	strings over intervals	finite-state machine;interval temporal logic;temporal semantics;intervals cum description;regular language	Intervals and the events that occur in them are encoded as strings, elaborating on a conception of events as “intervals cum description.” Notions of satisfaction in interval temporal logics are formulated in terms of strings, and the possibility of computing these via finite-state machines/transducers is investigated. This opens up temporal semantics to finite-state methods, with entailments that are decidable insofar as these can be reduced to inclusions between regular languages.	finite-state machine;interval temporal logic;regular language;transducer	Tim Fernando	2011			discrete mathematics;theoretical computer science;mathematics;algorithm	NLP	-9.390086718264275	23.76821099417203	153518
55c3c9dc1220a47585255e937d651ac5ece76fc3	a compete proof rule for strong equifair termination	compete proof rule;strong equifair termination	The notion of equifairness, strengthening the familiar notion of fairness, is introduced as a scheduling policy of non-determinism and concurrency. Under this notion, it is infinitely often the case that the number of selections of each of a family of infinitely-often jointly-enabled processes is equal. A proof rule for proving strong equifair-termination is introduced, applied to examples and shown to be (semantically) complete.		Orna Grumberg;Nissim Francez;Shmuel Katz	1983		10.1007/3-540-12896-4_367	discrete mathematics;scheduling (computing);concurrency;mathematics	Logic	-11.010024209758372	20.427428981166088	153799
55c2fa0ec5c0604a3a2a31ad1152a07d74d5adfa	efficient incremental static analysis using path abstraction		ion Rashmi Mudduluru and Murali Krishna Ramanathan {mudduluru.rashmi,muralikrishna}@csa.iisc.ernet.in Indian Institute of Science, Bangalore, India Abstract. Incremental static analysis involves analyzing changes to a version of a source code along with analyzing code regions that are semantically affected by the changes. Existing analysis tools that attempt Incremental static analysis involves analyzing changes to a version of a source code along with analyzing code regions that are semantically affected by the changes. Existing analysis tools that attempt to perform incremental analysis can perform redundant computations due to poor abstraction. In this paper, we design a novel and efficient incremental analysis algorithm for reducing the overall analysis time. We use a path abstraction that encodes different paths in the program as a set of constraints. The constraints encoded as boolean formulas are input to a SAT solver and the (un)satisfiability of the formulas drives the analysis further. While a majority of boolean formulas are similar across multiple versions, the problem of finding their equivalence is graph isomorphism complete. We address a relaxed version of the problem by designing efficient memoization techniques to identify equivalence of boolean formulas to improve the performance of the static analysis engine. Our experimental results on a number of large codebases (upto 87 KLoC) show a performance gain of upto 32% when incremental analysis is used. The overhead associated with identifying equivalence of boolean formulas is less (not more than 8.4%) than the overall reduction in analysis time.	algorithm;analysis of algorithms;boolean satisfiability problem;computation;graph isomorphism;memoization;overhead (computing);scalability;software development;solver;static program analysis;turing completeness	Rashmi Mudduluru;Murali Krishna Ramanathan	2014		10.1007/978-3-642-54804-8_9	theoretical computer science;algorithm	PL	-15.98229339263971	25.559663813062873	153849
bd9c2aa1d5f55f0edd74c803a586dc40e545ae84	a formalization of powerlist algebra in acl2	verification;data parallel;acl2;data structure;powerlists	In Misra (ACM Trans Program Lang Syst 16(6):1737–1767, 1994), Misra introduced the powerlist data structure, which is well suited to express recursive, data-parallel algorithms. Moreover, Misra and other researchers have shown how powerlists can be used to prove the correctness of several algorithms. This success has encouraged some researchers to pursue automated proofs of theorems about powerlists (Kapur 1997; Kapur and Subramaniam 1995, Form Methods Syst Des 13(2):127–158, 1998). In this paper, we show how ACL2 can be used to verify theorems about powerlists. We depart from previous approaches in two significant ways. First, the powerlists we use are not the regular structures defined by Misra; that is, we do not require powerlists to be balanced trees. As we will see, this complicates some of the proofs, but on the other hand it allows us to state theorems that are otherwise beyond the language of powerlists. Second, we wish to prove the correctness of powerlist algorithms as much as possible within the logic of powerlists. Previous approaches have relied on intermediate lemmas which are unproven (indeed unstated) within the powerlist logic. However, we believe these lemmas must be formalized if the final theorems are to be used as a foundation for subsequent work, e.g., in the verification of system libraries. In our experience, some of these unproven lemmas presented the biggest obstacle to finding an automated proof. We illustrate our approach with two case studies involving Batcher sorting and prefix sums.	acl2;adder (electronics);automated theorem proving;automation;bitonic sorter;carry-lookahead adder;correctness (computer science);data structure;first-order predicate;goto;library (computing);logic programming;logical connective;misra c;parallel algorithm;parsing;prefix sum;recursion;rewriting;self-balancing binary search tree;sorting;theory;universal quantification;verification and validation;zeller's congruence	Ruben Gamboa	2009	Journal of Automated Reasoning	10.1007/s10817-009-9140-y	verification;data structure;computer science;artificial intelligence;mathematics;programming language;algorithm	PL	-18.942260780583247	19.068960002874086	154082
3b8e73080f5e3c13dd677436d4ecc481b3981c7a	comparing action descriptions based on semantic preferences	semantic preferences;qa mathematics;computational complexity;action domain descriptions;transition diagrams;68t30;tk electrical engineering electronics nuclear engineering;68t27;qa075 electronic computers computer science	The focus of this paper is on action domain descriptions whose meaning can be represented by transition diagrams. We introduce several semantic measures to compare such action descriptions, based on preferences over possible states of the world and preferences over some given conditions (observations, assertions, etc.) about the domain, as well as the probabilities of possible transitions. This preference information is used to assemble a weight which is assigned to an action description. As applications of this approach, we study updating action descriptions and identifying elaboration tolerant action descriptions, with respect to some given conditions. With a semantic approach based on preferences, not only, for some problems, we get more plausible solutions, but also, for some problems without any solutions due to too strong conditions, we can identify which conditions to relax to obtain a solution. We further study computational issues, and give a characterization of the computational complexity of computing the semantic measures.	algorithm;algorithmic efficiency;computation;computational complexity theory;diagram;regular language description for xml;transition system	Thomas Eiter;Esra Erdem;Michael Fink;Ján Senko	2006	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-007-9077-y	computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;computational complexity theory;algorithm	AI	-9.14144241911304	23.89408797600723	154103
21fa6ac504e75b7c8905e4d61c180a4832859b68	a new characterization of lambda definability	logical relation;ordered set;decision procedure	We give a new characterization of lambda deenability in Henkin models using logical relations deened over ordered sets with varying arity. The advantage of this over earlier approaches by Plotkin and Statman is its simplicity and universality. Yet, decidability of lambda deenability for hereditarily nite Henkin models remains an open problem. But if the variable set allowed in terms is also restricted to be nite then our techniques lead to a decision procedure.	decision problem;logical relations;plotkin bound;universality probability	Achim Jung;Jerzy Tiuryn	1993		10.1007/BFb0037110	algorithm	Theory	-11.070129926072715	18.43327306021013	154208
3496c94776d7fcfaed7ab98267464acf6605c59a	deriving bisimulation relations from path based equivalence checkers		Translation validation is an undecidable problem. Bisimulation relation based approaches, nevertheless, have been widely successful in translation validation of programs albeit with some drawbacks. These drawbacks include non-termination of the verification methodology and significant restrictions on the structures of programs to be checked for equivalence. We have developed a path based equivalence checker which propagates mismatched values over consecutive paths to alleviate these drawbacks. In this work, we show how a bisimulation relation between a program and its translated version can be constructed from the outputs of such a value propagation based equivalence checker. Moreover, none of the earlier methods that establish equivalence through construction of bisimulation relations has been shown to tackle code motions across loops; the present work demonstrates, for the first time, the existence of a bisimulation relation under such a situation.	bisimulation;control flow;divergence (computer science);formal equivalence checking;loop-invariant code motion;sandy bridge;software propagation;symbolic execution;terminate (software);turing completeness;undecidable problem;unified framework	Kunal Banerjee;Dipankar Sarkar;Chittaranjan A. Mandal	2016	Formal Aspects of Computing	10.1007/s00165-016-0406-y	discrete mathematics;bisimulation;mathematics;algorithm	Logic	-14.755603492401045	26.15943970540316	154267
359fb7f6cac282bb0f98e9063565c532b73eab8d	backtrack programming	backtrack programming	A widely used method of efftcient search is examined in detail. This examination provides the opportunity to formulate its scope and methods in their full generality. In addL tion to a general exposition of the basic process, some important refinemertts are indicated. Examples are given which illustrate the salient features of this searching process.	backtrack	Beatrice A. Golomb;Leonard D. Baumert	1965	J. ACM	10.1145/321296.321300	discrete mathematics;backjumping;theoretical computer science;computer science	Graphics	-19.025420849344638	22.78860699786742	154293
65565ba100ae0bec1e63bb59838683955e77ece7	interprocedural symbolic evaluation of ada programs with aliases	developpement logiciel;verification symbolique;formal specification;programacion paralela;performance estimation;symbolic verification;parallel programming;ingenieria logiciel;software engineering;analisis programa;specification formelle;especificacion formal;desarrollo logicial;software development;genie logiciel;program analysis;analyse programme;data flow;static program analysis;programmation parallele	Symbolic Evaluation is a technique aimed at determining dynamic properties of programs. We extend our intraprocedural data-ow framework introduced in 3] to support interprocedural symbolic evaluation. Our data-ow framework utilizes a novel approach based on an array algebra to handle aliases induced by procedure calls. It serves as as a basis for static program analysis (e.g. reaching deenitions-, alias analysis, worst-case performance estimations, cache analysis). Examples for reaching deenitions-as well as alias analysis are presented. In this section we introduce the basics of interprocedural symbolic evaluation as it is used throughout the paper. We abstract from intraprocedural evaluation details such as conditional or repetitive statements in order to be concise. Treatment of intraprocedural symbolic analysis of Ada programs can be found in 3]. Symbolic evaluation is a form of static program analysis in which symbolic expressions are used to denote the values of program variables and computations (cf. e.g. 5]). In addition a path condition describes the impact of the program's control ow onto the values of variables and the condition under which control ow reaches a given program point. The underlying program representation for symbolic evaluation is usually the control ow graph (CFG), a directed labelled graph. Its nodes are the program's basic blocks (a basic block is a single entry, single exit, sequence of statements), whereas its edges represent transfers of control between basic blocks. Each edge of the CFG is assigned a condition which must evaluate to true for the program's control ow to follow this edge. Entry and Exit are distinguished nodes used to denote start and terminal node.	ada;alias analysis;basic block;best, worst and average case;computation;context-free grammar;control flow graph;graph labeling;reaching definition;s-expression;static program analysis;symbolic execution	Johann Blieberger;Bernd Burgstaller;Bernhard Scholz	1999		10.1007/3-540-48753-0_12	program analysis;data flow diagram;computer science;theoretical computer science;software development;software engineering;formal specification;programming language;algorithm;static program analysis	SE	-18.80995019867197	28.608812479981143	154297
470323f4983bf20e5d046dff5e33d3db19c2dd32	computational space efficiency and minimal model generation for guarded formulae	metodo caso peor;minimal model;constraint propagation;time complexity;optimal decision;temps minimal;decision optimale;modal logic;complexite temps;decision procedure;logique ordre 1;logique modale;de scription logic;methode cas pire;minimum time;logica modal;complejidad tiempo;logique multimodale;tiempo minimo;worst case method;first order logic;logica orden 1;decision optimal	This paper describes a number of hyperresolution-based decision procedures for a subfragment of the guarded fragment. We first present a polynomial space decision procedure of optimal worst-case space and time complexity for the fragment under consideration. We then consider minimal model generation procedures which construct all and only minimal Herbrand models for guarded formulae. These procedures are based on hyperresolution, (complement) splitting and either model constraint propagation or local minimality tests. All the procedures have concrete application domains and are relevant for multi-modal and description logics that can be embedded into the considered fragment.	best, worst and average case;computation;decision problem;description logic;embedded system;guarded logic;local consistency;modal logic;pspace;polynomial;software propagation;time complexity	Lilia Georgieva;Ullrich Hustadt;Renate A. Schmidt	2001		10.1007/3-540-45653-8_6	modal logic;time complexity;optimal decision;computer science;calculus;first-order logic;mathematics;algorithm;local consistency	AI	-7.916621996537482	19.541596562359526	154325
d0d106543c08bc3eb3c10d8b7eb2bf45812d46df	liveness with invisible ranking	automatic verification;modelizacion;diagrama binaria decision;diagramme binaire decision;automatic proving;heuristic method;vivacidad;bdd techniques;safety systems;metodo heuristico;demostracion automatica;parametrizedsystems;journal article;safety properties;vivacite;modelisation;demonstration automatique;hierarchical classification;analyse syntaxique;analisis sintaxico;syntactic analysis;liveness;classification hierarchique;systeme securite;invariante;estructura producto;methode heuristique;deductive verification;modeling;clasificacion jerarquizada;structure produit;parametrized systems;invariant;product structure;binary decision diagram	The method of invisible invariants was developed originally in order to verify safety properties of parameterized systems in a fully automatic manner. The method is based on (1) a project&generalize heuristic to generate auxiliary constructs for parameterized systems and (2) a small-model theorem, implying that it is sufficient to check the validity of logical assertions of a certain syntactic form on small instantiations of a parameterized system. The approach can be generalized to any deductive proof rule that (1) requires auxiliary constructs that can be generated by project&generalize, and (2) the premises resulting when using the constructs are of the form covered by the small-model theorem. The method of invisible ranking, presented here, generalizes the approach to liveness properties of parameterized systems. Starting with a proof rule and cases where the method can be applied almost “as is,” the paper progresses to develop deductive proof rules for liveness and extend the small-model theorem to cover many intricate families of parameterized systems.	heuristic;invariant (computer science);liveness	Yi Fang;Nir Piterman;Amir Pnueli;Lenore D. Zuck	2005	International Journal on Software Tools for Technology Transfer	10.1007/s10009-005-0193-x	systems modeling;computer science;invariant;parsing;programming language;system safety;binary decision diagram;algorithm;liveness	Logic	-14.17379630422587	22.406596595281258	154355
366259c4eb05c71aa2f2ae0c3da087991ccb47e0	natural synthesis of provably-correct data-structure manipulations		This paper presents natural synthesis, which generalizes the proof-theoretic synthesis technique to support very expressive logic theories. This approach leverages the natural proof methodology and reduces an intractable, unbounded-size synthesis problem to a tractable, bounded-size synthesis problem, which is amenable to be handled by modern inductive synthesis engines. The synthesized program admits a natural proof and is a provably-correct solution to the original synthesis problem. We explore the natural synthesis approach in the domain of imperative data-structure manipulations and present a novel syntax-guided synthesizer based on natural synthesis. The input to our system is a program template together with a rich functional specification that the synthesized program must meet. Our system automatically produces a program implementation along with necessary proof artifacts, namely loop invariants and ranking functions, and guarantees the total correctness with a natural proof. Experiments show that our natural synthesizer can efficiently produce provably-correct implementations for sorted lists and binary search trees. To our knowledge, this is the first system that can automatically synthesize these programs, their functional correctness and their termination in tandem from bare-bones control flow skeletons.	cobham's thesis;control flow;correctness (computer science);data structure;formal verification;functional specification;imperative programming;inductive reasoning;insertion sort;loop invariant;natural language;natural proof;program synthesis;recursion;sketch;theory	Xiaokang Qiu;Armando Solar-Lezama	2017	PACMPL	10.1145/3133889	implementation;natural proof;control flow;binary search tree;functional specification;theoretical computer science;correctness;loop invariant;data structure;computer science	PL	-16.348038191879343	23.5196867019913	154421
4c24e608c6bc08229cd7ad4b769bef182bc577d1	on the church-rosser and coherence properties of conditional order-sorted rewrite theories	formal veriﬁcation;text;maude;order sorted conditional speciﬁcations;rewriting modulo;church rosser property;maude order sorted conditional specifications rewriting modulo formal verification church rosser property coherence;coherence	In the effort to bring rewriting-based methods into contact with practical applications both in programing and in formal verification, there is a tension between: (i) expressiveness and generality—so that a wide range of applications can be expressed easily and naturally—, and (ii) support for formal verification, which is harder to get for general and expressive specifications. This paper answers the challenge of successfully negotiating the tension between goals (i) and (ii) for a wide class of quite expressive Maude specifications, namely: (a) equational order-sorted conditional specifications (Σ, E ∪ A), corresponding to functional programs modulo axioms such as associativity and/or commutativity and/or identity; and (b) order-sorted conditional rewrite theories R = (Σ, E ∪A,R, φ), corresponding to concurrent programs modulo axioms A. For functional programs the key formal property checked is the Church-Rosser property. For concurrent declarative programs in rewriting logic, the key property checked is the coherence between rules and equations modulo the axioms A. Such properties are essential, both for executability purposes and as a basis for verifying many other properties, such as, for example, proving inductive theorems of a functional program, or correct model checking of temporal logic properties for a concurrent program. This paper develops the mathematical foundations on which the checking of these properties (or ground versions of them) is based, presents two Maude tools, the Church-Rosser Checker (CRC) and the Coherence Checker (ChC) supporting the verification of these properties, and illustrates with examples a methodology to establish such properties using the proof obligations returned by the tools.	church–rosser theorem;concurrent computing;cyclic redundancy check;emoticon;formal verification;functional programming;maude system;model checking;modulo operation;rewrite (programming);rewriting;temporal logic;theory;verification and validation	Francisco Durán;José Meseguer	2012	J. Log. Algebr. Program.	10.1016/j.jlap.2011.12.004	discrete mathematics;coherence;theoretical computer science;mathematics;programming language;algorithm	Logic	-15.023326797193695	20.383856132511344	154607
da6ffa8dd480a7b9ad6b4903c0c26f9bce362d73	first-order unification by structural recursion	first order	First-order unification algorithms (Robinson, 1965) are traditionally implemented via general recursion, with separate proofs for partial correctness and termination. The latter tends to involve counting the number of unsolved variables and showing that this total decreases each time a substitution enlarges the terms. There are many such proofs in the literature (Manna & Waldinger, 1981; Paulson, 1985; Coen, 1992; Rouyer, 1992; Jaume, 1997; Bove, 1999). This paper shows how a dependent type can relate terms to the set of variables over which they are constructed. As a consequence, first-order unification becomes a structurally recursive program, and a termination proof is no longer required. Both the program and its correctness proof have been checked using the proof assistant LEGO (Luo & Pollack, 1992; McBride, 1999).	first-order predicate;recursion;structural induction;unification (computer science)	Conor McBride	2003	J. Funct. Program.	10.1017/S0956796803004957	computer science;first-order logic;programming language;algorithm	PL	-15.043198367801558	21.607997018816874	154638
cdcc3d193b39040a682cd7643216f95659a6e53d	continuous grammars	types;continuations;bytecode;polymorphic recursion;subroutines;java	After defining appropriate metrics on strings and parse trees, the classic definition of continuity is adapted and applied to functions from strings to parse trees. Grammars that yield continuous mappings are of special interest, because they provide a sound theoretical framework for syntax error correction. Continuity justifies the approach, taken by many error correctors, to use the function output (the parse tree), and all the additional information it provides, in order to find corrections to the function input (the string). We prove that all Bounded Context grammars are continuous and that all continuous grammars are Bounded Context Parseable grammars, giving a characterization of continuous grammars in terms of possible parsing algorithms.	algorithm;error detection and correction;parse tree;parsing;scott continuity;syntax error	Martin Ruckert	1999		10.1145/292540.292568	context-sensitive grammar;tree-adjoining grammar;indexed grammar;l-attributed grammar;parsing expression grammar;computer science;theoretical computer science;subroutine;continuation;s-attributed grammar;extended affix grammar;definite clause grammar;context-free grammar;programming language;java;stochastic context-free grammar;embedded pushdown automaton;algorithm	PL	-5.809325548837651	22.685869230018024	155035
6071c71f2f06f11ed057125463850f6769b589f5	bounded model checking of time petri nets using sat solver	bounded model checking;time petri net;sat solver;symbolic model checking	To carry out performance evaluation of an asynchronous system, the system is modeled as Time Petri Net (TPN) and an iteration of Petri net simulations produces its performance index. The TPN model needs to satisfy required properties such as deadlock freeness. We proposed a symbolic representation of TPN for SAT-based bounded model checking. In the proposed encoding scheme, firing of transitions and elapsing of place delays are expressed as boolean formulas discretely. Our representation can work with relaxed ∃-step semantics which enables to perform each step by two or more transitions. We applied the encoding to example TPN models and checked the deadlock freeness using SAT solver. The results of experiments demonstrated the effectiveness of the proposed representation.	asynchronous system;boolean satisfiability problem;deadlock;experiment;institute of electronics, information and communication engineers;iteration;line code;model checking;performance evaluation;petri net;simulation;solver	Tomoyuki Yokogawa;Masafumi Kondo;Hisashi Miyazaki;Sousuke Amasaki;Yoichiro Sato;Kazutami Arimoto	2015	IEICE Electronic Express	10.1587/elex.11.20141112	model checking;discrete mathematics;stochastic petri net;computer science;theoretical computer science;boolean satisfiability problem;programming language;petri net	AI	-12.481682699062102	26.524278397170594	155056
2b56f1a0e67a365540528fb06e67887e7ef742db	stepping from graph transformation units to model transformation units	model transformation;graph transformation	Graph transformation units are rule-based entities that allow to transform source graphs into target graphs via sets of graph transformation rules according to a control condition. The graphs and rules are taken from an underlying graph transformation approach. Graph transformation units specify model transformations whenever the transformed graphs represent models. This paper is based on the observation that in general models are not always suitably represented as single graphs, but they may be specified as the composition of a variety of different formal structures such as sets, tuples, graphs, etc., which should be transformed by compositions of different types of rules and operations instead of single graph transformation rules. Consequently, in this paper, graph transformation units are generalized to model transformation units that allow to transform such kind of composed models in a rule-based and controlled way. Moreover, two compositions of model transformation units are presented.	automata theory;automaton;correctness (computer science);data structure;diagram;directed graph;entity;finite-state machine;graph (discrete mathematics);graph rewriting;graphical model;logic programming;model transformation;process calculus;set theory;stepping level;string (computer science);termination analysis;uml state machine	Hans-Jörg Kreowski;Sabine Kuske;Caroline von Totth	2010	ECEASST	10.14279/tuj.eceasst.30.403	lattice graph;null model;graph product;null graph;graph property;computer science;clique-width;forbidden graph characterization;comparability graph;voltage graph;modular decomposition;programming language;complement graph;graph operations;indifference graph;planar graph;graph rewriting	AI	-8.06856526725739	24.29563501109013	155095
9af2b471ae1457d9bdafca47b06fc179a72eb229	a new modality for almost everywhere properties in timed automata	programmation booleenne;boolean programming;systeme temps reel;distributed system;verificacion modelo;systeme reparti;logica temporal;logique arbre calcul;temporal logic;verification modele;simultaneidad informatica;automate temporise;program verification;programacion booleana;communication conference;automata contemporizado;expressive power;logica ctl;verificacion programa;modelo logico;concurrency;sistema repartido;model checking;logique ordre 1;tctl;real time system;timed automata;sistema tiempo real;classical logic;logic model;verification programme;simultaneite informatique;logique temporelle;branching temporal logic ctl;first order logic;modele logique;logica orden 1	The context of this study is timed temporal logics for timed automata. In this paper, we propose an extension of the classical logic TCTL with a new Until modality, called “Until almost everywhere”. In the extended logic, it is possible, for instance, to express that a property is true at all positions of all runs, except on a negligible set of positions. Such properties are very convenient, for example in the framework of boolean program verification, where transitions result from changing variable values. We investigate the expressive power of this modality and in particular, we prove that it cannot be expressed with classical TCTL modalities. However, we show that model-checking the extended logic remains PSPACE-complete as for TCTL.	automata theory;boolean satisfiability problem;formal verification;modality (human–computer interaction);model checking;pspace-complete;temporal logic;timed automaton	Houda Bel Mokadem;Béatrice Bérard;Patricia Bouyer;François Laroussinie	2005		10.1007/11539452_12	model checking;classical logic;real-time operating system;concurrency;temporal logic;computer science;artificial intelligence;first-order logic;mathematics;programming language;expressive power;algorithm	Logic	-9.890035994914117	25.509201939939206	155109
