id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
9e2090b9e58f0a065ec1c58ce8588c0ee7f5c70d	evaluating approaches to adaptive landscape visualization construction	biology computing;topology;biological cells data visualization visualization adaptation model construction industry topology hamming distance;scaling phenomena;two dimensional plane;construction industry;van wijks visualization model;scaling phenomena biology computing cellular biophysics data visualisation;data visualisation;multidimensional scaling data visualization genetic algorithm adaptive landscape;visualization;adaptation model;biological cells;hamming distance;visual modeling;adaptive landscape;multidimensional scaling technique;multidimensional scaling;data visualization;adaptive landscape visualization construction;genetic algorithm;multidimensional scaling technique adaptive landscape visualization construction two dimensional plane high dimensional chromosome space van wijks visualization model;high dimensional chromosome space;cellular biophysics	The construction of an adaptive landscape visualization entails the representation of the higher dimensional chromosome space onto a two-dimensional plane from which a depiction of the landscape can be created. Although it can sometimes be difficult to justify the investment of computational expense necessary to perform this representation properly, Van Wijk's visualization model is ideally suited for use as a framework in which to compare different adaptive landscape visualizations. This model is used to demonstrate that an adaptive landscape visualization that is constructed with an established multidimensional scaling technique will result in a visualization that imparts more accurate insights, and is thus more valuable, than one that does not first attempt a multidimensional scaling of the chromosome space.	analysis of algorithms;computation;image scaling;multidimensional scaling	Robert Collier;Mark Wineberg	2010	2010 International Conference on Autonomous and Intelligent Systems, AIS 2010	10.1109/AIS.2010.5547037	hamming distance;simulation;genetic algorithm;visualization;multidimensional scaling;fitness landscape;computer science;bioinformatics;theoretical computer science;data visualization;statistics	Visualization	-2.5797865266982907	-47.624200389716655	46271
dd5d10b979a706fe9657e814ae13285ff3f55f79	credit risk assessment using rough set theory and ga-based svm	banking;information loss;neural networks;genetic optimization algorithm;support vector machines;rough set theory;commercial banks credit risk assessment rough set theory support vector machine genetic optimization algorithm;risk management;set theory;risk management set theory support vector machines support vector machine classification principal component analysis genetics neural networks rough sets predictive models optimization methods;genetics;commercial banks;discriminant analysis;financial ratios;credit risk assessment;support vector machines banking genetic algorithms rough set theory;classification rules;principal component analysis;evaluation criteria;hybrid system;rough sets;support vector machine classification;predictive models;genetic algorithms;support vector machine;rough set;optimal algorithm;credit risk;neural network;optimization methods	This paper applies a. classifier, hybridizing rough set approach and improved support vector machine(SVM) using genetic optimization algorithm (GA), to the study of credit risk assessment in commercial banks. We can get reduced information table, which implies that the number of evaluation criteria, such as financial ratios and qualitative variables is reduced with no information loss through rough set approach. And then, this reduced information table is used to develop classification rules and train SVM. Especially, in order to improve the assessment accuracy, GA is applied to optimize the parameters of SVM classifier. The rationale of our hybrid system is using rules developed by rough sets for an object that matches any of the rules and SVM for one that dose not match any of them. The effectiveness of our methodology was verified by experiments comparing traditional discriminant analysis (DA) model, BP neural networks (BPN) and standard SVM with our approach.	artificial neural network;attribute-value system;business process network;design rationale;experiment;genetic algorithm;hybrid system;linear discriminant analysis;mathematical optimization;risk assessment;rough set;set theory;software release life cycle;support vector machine	Jian-guo Zhou;Tao Bai	2008	2008 The 3rd International Conference on Grid and Pervasive Computing - Workshops	10.1109/GPC.WORKSHOPS.2008.56	rough set;computer science;machine learning;pattern recognition;data mining;ranking svm;artificial neural network	AI	9.084290394537588	-40.41346380900443	46276
1efb75002fe181db1da12b5d9e1743f1429f2bc8	incremental wrapper based random forest gene subset selection for tumor discernment		High-dimensional cancer related dataset permits the researchers to timely diagnose and facilitate in effective treatment of the cancer. Biomedicine application process on the thousands of features. It is challenging to extract the precise statistics from this high-dimensional dataset. This paper presents the Incremental Wrapper based Random Forest Gene Subset Selection of Tumor discernment that mechanisms on the principle of incremental wrapper based feature subset selection with random forest classification algorithm and this algorithm also works as performance validator. Incremental wrapper based feature subset selection is a technique to pick out a finest conceivable subset of genes from the high-dimensional data with low computational cost. Random Forest will increase the overall performance as it works better in cancer related high-dimensional dataset. The efficacy of the random forest classification algorithm as performance validator will significantly improve by working on a selective discriminative subset of prognostic genes as compare to the raw data. We evaluate the proposed methodology on the six publicly available cancer related high dimensional datasets and found that the proposed methodology outperform as compare to standard random forests.	random forest	Alia Fatima;Usman Qamar;Saad Rehman;Aiman Khan Nazir	2018		10.1007/978-3-319-99133-7_13	data mining;discernment;discriminative model;validator;raw data;computer science;random forest	Theory	9.161381627584575	-49.56445620301476	46323
7637e0f971992b7e248e6e5755fb616d7400b4f1	translocation distance: algorithms and complexity	genome rearrangement;genetics;large scale;gene order;gene family;computational biology	With the development of fast sequencing techniques, large-scale DNA molecules are investigated with respect to the relative order of genes in them. Contrary to the traditional alignment approach, genome rearrangements are based on comparison of gene orders and the evolution of gene families. Genome rearrangement has become an important area in computational biology and bioinformatics. There are three basic operations,  reversal ,  translocation , and  transposition . Here we study the translocation operations. Multi-chromosomal genomes frequently evolve by  translocation  events that exchange genetic material between two chromosomes. We will discuss both signed and unsigned cases.	algorithm;complexity	Lusheng Wang	2006	Advances in Computers	10.1016/S0065-2458(06)68003-0	bioinformatics;gene family	Theory	-0.3785366369692546	-51.968244319713314	46397
57886b562f5e634733f73d4afcfc68fef35a1822	determination of vocational fields with machine learning algorithm	machine learning algorithms;probability;bayes methods;data collection;information technology;field energy application vocational field determination machine learning vocational training technical training information technology communication technology artificial intelligence naive bayes algorithm occupation selection process;training;naive bayes;prediction algorithms;data mining;classification algorithms data mining prediction algorithms training data training machine learning algorithms probability;energy applications;field energy application;artificial intelligent;training data;naive bayes algorithm;information and communication technology;educational administrative data processing;data mining machine learning vocational field selection energy applications naive bayes;machine learning;software development;classification algorithms;vocational training bayes methods data mining educational administrative data processing learning artificial intelligence;artificial intelligence;communication technology;learning artificial intelligence;vocational training;vocational field selection;occupation selection process;vocational field determination;technical training	The importance of vocational and technical training is growing day by day in parallel to the developing technology. It is inevitable to utilise opportunities presented by information and communication technologies in order to determine vocational fields in vocational and technical training in the most efficient manner. In this respect, it is possible to create a more efficient tool compared to the current methods by utilising machine learning which is an artificial intelligence model in energy applications that predicts events in the future depending on the past experiences. In the current study, a software is developed that ensures that the system learns about the successful and unsuccessful choices made in the past by applying “Naive Bayes” algorithm, which is a machine learning algorithm, to the data collected concerning the individuals who turned out to be successful or unsuccessful in the vocational technical training process in energy applications. In the software developed, it is aimed that the system recommends the most suitable vocational field for the individual by according to the data collected from the individual who is in the occupation selection process in field energy applications.	algorithm;artificial intelligence;experience;machine learning;naive bayes classifier	Halil-Ibrahim Bulbul;Özkan Ünsal	2010	2010 Ninth International Conference on Machine Learning and Applications	10.1109/ICMLA.2010.109	information and communications technology;naive bayes classifier;computer science;artificial intelligence;data science;machine learning;information technology	ML	7.8999155087127795	-38.99665969179469	46511
acb6a99999f95bfc552d0b2b1a3a7c5dfe776cad	a hierarchical model-based approach to co-clustering high-dimensional data	high dimensional dataset;frequent pattern mining;probability distribution;high dimensional data;sequential pattern mining;kernel tracing;empirical evaluation;conditional distribution;hierarchical model	We propose a hierarchical, model-based co-clustering framework for handling high-dimensional datasets. The technique views the dataset as a joint probability distribution over row and column variables. Our approach starts by clustering tuples in a dataset, where each cluster is characterized by a different probability distribution. Subsequently, the conditional distribution of attributes over tuples is exploited to discover natural co-clusters in the data. An intensive empirical evaluation highlights the effectiveness of our approach.	biclustering;cluster analysis;clustering high-dimensional data;hierarchical database model	Gianni Costa;Giuseppe Manco;Riccardo Ortale	2008		10.1145/1363686.1363891	probability distribution;sequential pattern mining;conditional probability distribution;categorical distribution;computer science;machine learning;pattern recognition;data mining;hierarchical database model;statistics;clustering high-dimensional data	ML	-1.3476298023509719	-42.320623171065876	46735
71825e6847715d4ae4520df410f5c746363b0a63	detecting similarity of transferring datasets based on features of classification rules	databases;object recognition;learning algorithm;performance indicator;statistical measurements;classification learning algorithms similarity detection classification rules classifier sets data characterizing techniques statistical measurements datasets objective rule evaluation and;data characterizing techniques;presses;datasets;data mining;accuracy;data analysis;learning artificial intelligence data analysis;classification rules;indexation;classification algorithms;classification learning algorithms;transfer learning;similarity detection;learning artificial intelligence;data mining classification algorithms machine learning algorithms conferences availability information systems information technology databases statistical analysis learning systems;decision trees;objective rule evaluation and;classifier sets	In order to transfer mined knowledge for various datasets obtained from transferring situations, it is important to detect not only availability of transferring the knowledge but also detecting their limitations of the transfer. Although most of methods to detect the limitations use performance indices of sets of classifiers such as accuracies of classifier sets, those of each classifier are also useful. Data characterizing techniques have been developed to control learning algorithm selection by using statistical measurements of a dataset. Expanding this framework, we consider a method to reuse objective rule evaluation indices of classification rules such as support, precision, and recall, to measure similarity of different datasets. In this paper, we present a method to characterize given datasets based on objective rule evaluation indices and classification learning algorithms. The experimental results show the method can detect similarity of datasets even if the datasets have totally different attribute sets. This indicates that the limitations of transferring both of classifiers and learning algorithms can be detected as the similarity among datasets by using a learning algorithm.	algorithm selection;evaluation function;machine learning;mined;precision and recall;sensor	Hidenao Abe;Shusaku Tsumoto	2009	2009 IEEE International Conference on Data Mining Workshops	10.1109/ICDMW.2009.99	statistical classification;transfer of learning;computer science;performance indicator;cognitive neuroscience of visual object recognition;machine learning;decision tree;pattern recognition;data mining;accuracy and precision;data analysis;statistics	DB	7.988767135435648	-39.13466151673898	46817
0b77adbd686e5c3ed96cc1298918305d415249e5	fast entropic profiler: an information theoretic approach for the discovery of patterns in genomes	dna;genomics;ieee transactions;local entropy;pattern discovery;models genetic;genome;pattern recognition automated;entropy;computational biology;information theory;bioinformatics	Information theory has been used for quite some time in the area of computational biology. In this paper we present a pattern discovery method, named Fast Entropic Profiler, that is based on a local entropy function that captures the importance of a region with respect to the whole genome. The local entropy function has been introduced by Vinga and Almeida in [29], here we discuss and improve the original formulation. We provide a linear time and linear space algorithm called Fast Entropic Profiler (FastEP), as opposed to the original quadratic implementation. Moreover we propose an alternative normalization that can be also efficiently implemented. We show that FastEP is suitable for large genomes and for the discovery of patterns with unbounded length. FastEP is available at http://www.dei.unipd.it/~ciompin/main/FastEP.html.	algorithm;alignment;computation;computational biology;entropic gravity;fast fourier transform;genome;information theory;memory footprint;name;spectrometry, mass, fast atom bombardment;time complexity	Matteo Comin;Morris Antonello	2014	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2013.2297924	biology;entropy;genomics;information theory;computer science;bioinformatics;data science;machine learning;data mining;genetics;dna;algorithm;statistics;genome	Comp.	-0.9537493668844392	-47.98210972194961	46870
4511228c8e4cd844236f22e8cb37047df0c493be	supervised classification of protein structures based on convex hull representation	neural networks;supervised learning;support vector machines;supervised classification;hmm;protein structure;protein structures;hidden markov models;machine learning;functional genomics;pattern recognition;convex hull representation;svms;convex hull;automatic classification;bioinformatics	One of the central problems in functional genomics is to establish the classification schemes of protein structures. In this paper the relationship of protein structures is uncovered within the framework of supervised learning. Specifically, the novel patterns based on convex hull representation are firstly extracted from a protein structure, then the classification system is constructed and machine learning methods such as neural networks, Hidden Markov Models (HMM) and Support Vector Machines (SVMs) are applied. The CATH scheme is highlighted in the classification experiments. The results indicate that the proposed supervised classification scheme is effective and efficient.	artificial neural network;cath;comparison and contrast of classification schemes in linguistics and metadata;convex hull;experiment;extraction;functional genomics;hidden markov model;machine learning;markov chain;neural network simulation;polynomial-time approximation scheme;protein, organized by structure;staphylococcal protein a;supervised learning;support vector machine	Yong Wang;Ling-Yun Wu;Luonan Chen;Xiang-Sun Zhang	2007	International journal of bioinformatics research and applications	10.1504/IJBRA.2007.013598	protein structure;computer science;machine learning;linear classifier;pattern recognition;data mining;mathematics;one-class classification;hidden markov model	ML	6.709467249011707	-46.13712492991796	46933
2b476b69f2e4bac5fdca42cdc30225ff2a77a308	efficient and exact maximum likelihood quantisation of genomic features using dynamic programming	distribution;dynamic programming;discretisation;genomique;genomics;programacion dinamica;recombination rate distribution;genomic features;maximum likelihood;dynamic programming algorithm;taux de recombinaison;retrotransposon;maximum vraisemblance;genomica;dynamic program;maximum likelihood quantisation;line 1;discrete random variables;probability distribution;elemento transportable;quantisation;programmation dynamique;element transposable;random variable;recombination rate;transposable element;distribucion;maxima verosimilitud;transposable elements;bioinformatics	An efficient and exact dynamic programming algorithm is introduced to quantise a continuous random variable into a discrete random variable that maximises the likelihood of the quantised probability distribution for the original continuous random variable. Quantisation is often useful before statistical analysis and modelling of large discrete network models from observations of multiple continuous random variables. The quantisation algorithm is applied to genomic features including the recombination rate distribution across the chromosomes and the non-coding transposable element LINE-1 in the human genome. The association pattern is studied between the recombination rate, obtained by quantisation at genomic locations around LINE-1 elements, and the length groups of LINE-1 elements, also obtained by quantisation on LINE-1 length. The exact and density-preserving quantisation approach provides an alternative superior to the inexact and distance-based univariate iterative k-means clustering algorithm for discretisation.	algorithm;chromosomes;cluster analysis;dna transposable elements;discretization;dynamic programming;iterative method;k-means clustering;long interspersed nucleotide element-1;quantization (image processing);quantization (physics);silencer elements, transcriptional;statistical cluster	Mingzhou Song;Robert M. Haralick;Stéphane Boissinot	2010	International journal of data mining and bioinformatics	10.1504/IJDMB.2010.032167	mathematical optimization;genomics;combinatorics;transposable element;dynamic programming;mathematics;genetics;statistics	ML	0.15802528369817972	-48.20796697209593	47241
7888cd940e84453f5c1e354ef8c838c1624abf2a	atherosclerotic plaque pathological analysis by unsupervised  $k$ -means clustering		This paper introduced a high-throughput pathological analysis algorithm by using of unsupervised  $K$ -means clustering principle and lab color space. The accuracy of this algorithm was verified by comparing with well-established commercially available software. For each type of pathological staining special for atherosclerotic plaque components analysis, accurate pathological analysis results could be obtained by selecting the appropriate cluster classification number (usually 3 to 5, but not limited to 3 to 5). Bland-Altman and linear regression analysis further confirmed that the self-developed algorithm correlated well with the well-established software (correlation coefficient R2 ranged from 0.72 to 0.99). Moreover, the intra- and inter- observer coefficient of variation were relatively minor, indicating very good reproducibility. So we draw a conclusion that the self-developed algorithm could reduce the human interference factors, improve the efficiency, and be suitable for a large number of analyses of atherosclerotic pathology.	algorithm;cluster analysis;color space;high-throughput computing;interference (communication);matthews correlation coefficient;throughput;whole earth 'lectronic link	Jianqin Feng;Yongtao Zhang;Guanghua Yue;Xin Liu;Haijun Su;Pengfei Zhang	2018	IEEE Access	10.1109/ACCESS.2018.2820318	correlation coefficient;distributed computing;cluster analysis;pathological;computer science;k-means clustering;coefficient of variation;linear regression;lab color space;artificial intelligence;pattern recognition	SE	9.763656918053687	-50.335567627635726	47258
03dfa892b6e50352c8413fe00f5fbd23ab96b016	finding additive biclusters with random background	software tool;bicluster;e commerce;chernoff bound;gene expression data;data mining;objective function;polynomial time algorithm;probabilistic model;machine learning;mathematical model;pattern recognition;probability model;gene expression data analysis;random numbers;computational biology	The biclustering problem has been extensively studied in many areas including e-commerce, data mining, machine learning, pattern recognition, statistics, and more recently in computational biology. Given an n m matrix A (n m), the main goal of biclustering is to identify a subset of rows (called objects) and a subset of columns (called properties) such that some objective function that specifies the quality of the found bicluster (formed by the subsets of rows and of columns of A) is optimized. The problem has been proved or conjectured to be NP-hard under various mathematical models. In this paper, we study a probabilistic model of the implanted additive bicluster problem, where each element in the n m background matrix is a random number from [0 L 1], and a k k implanted additive bicluster is obtained from an error-free additive bicluster by randomly changing each element to a number in [0 L 1] with probability . We propose an O(n2m) time voting algorithm to solve the problem. We show that for any constant Æ such that (1 Æ)(1 )2 1 L 0, when k max 8 n log n 8 log n c log(2L) , where c is a constant number, the voting algorithm can correctly find the implanted bicluster with probability at least 1 9 n2 . We also implement our algorithm as a software tool for finding novel biclusters in microarray gene expression data, called VOTE. The implementation incorporates several nontrivial ideas for estimating the size of an implanted bicluster, adjusting the threshold in voting, dealing with small biclusters, and dealing with multiple (and overlapping) implanted biclusters. Our experimental results on both simulated and real datasets show that VOTE can find biclusters with a high accuracy and speed.	additive model;algorithm;biclustering;column (database);computation;computational biology;data mining;e-commerce;experiment;loss function;machine learning;mathematical model;microarray;np-hardness;optimization problem;pattern recognition;programming tool;random number generation;randomness;statistical model;time complexity;utility functions on indivisible goods	Jing Xiao;Lusheng Wang;Xiaowen Liu;Tao Jiang	2008		10.1007/978-3-540-69068-9_25	e-commerce;statistical model;computer science;bioinformatics;theoretical computer science;machine learning;mathematical model;data mining;mathematics;chernoff bound;algorithm;statistics	ML	3.3753758485750347	-49.374195150332106	47408
cfaad82a0ec53029ba8069d5df31f851d801263e	performance comparison of algorithms for findingtranscription factor binding sites	algorithms accuracy;planted motifs;yeast s cerevisiae;performance comparison;binding site;genetics;fungi;physiological models genetics microorganisms;co regulated genes;algorithms performance comparison;transcription factor binding sites finding;synthetic data;local search;physiological models;microorganisms;algorithms accuracy transcription factor binding sites finding algorithms performance comparison synthetic data planted motifs co regulated genes yeast s cerevisiae enumerative algorithm;enumerative algorithm	                              ¢  £ ¤ ¥ § ¢ ¥ ©  a £ ©   ¢    «       § ¢ «   ̄      ¥  ̄  a    ¥ «   ¢   ¢  ¥    £    ́ ¢ ¥ § ¢ ¥ © « ¢   «    ¥ ©   £   ©  a    § ©  ¥  « ¶ · ¥        a ©   ¢    « o » 1⁄4 1⁄2 3⁄4  «  «     ¢    §  a   ¢ a    §    ́ ¢ ¥ § ¢ ¥ © « ¢   «  ¥ §  ¥  ¥       ¢ ̄  «             £  ¢  «     Á Â  ¢ a           Â  o 1⁄4 Ã 1⁄4 Ã  ¥ § Ä a ¢ © ¥ Ä Æ Ã 3⁄4  «       ©  ¥    a    ¢    §  a  ¥ § a    a «          £ ¥ ¢ È   « ¶ É         ¢ «  ¥ ¢ « §  ¥   ¥ «  ¥     ¢  §    Â ¢    a  ¥   §    ¢  « Á  « Â  a a  «  ¥    a §    «   «     £   ©  a    § ©  ¥  «           «  Ï Ð Ñ Ò Ó Ò Ô Õ Ö Õ × Ò ¶ 1⁄4        ¥    ¥ ¥   Á     ¥       ¢ ̄   a ©   ¢    ¢ «    ¥ §   ́                 ¥          Â   ¥       «  §    «   « Á     ©       ¢ «  ¥   ¢    ́ a   Ù  a  « ¢ ̄ ¢   ¢ ¥      £            § ¢ Ú    ¥   a ©   ¢    « ¶ É    Ù    ¢   ¥  «  ¥ «  ¥     ¢  §      ̄   a Á ¥   «     ¢ « ¢ ¥ © a  Á          a ©  £  ¢              «         « Â   ¥    ¢  «     a  ¥   §      § ¢ ¥ ©   ¢  «    ¢    §  a ¶	algorithm	Saurabh Sinha;Martin Tompa	2003		10.1109/BIBE.2003.1188949	biology;bioinformatics;binding site;local search;machine learning;microorganism;planted motif search;genetics;synthetic data	Visualization	-0.05481117240274412	-51.6769327163361	47472
c19f15cff9f4dff0cca11df558d9ce96b2b260de	discretization of continuous-valued attributes in decision tree generation	cut point selection continuous valued attributes decision tree generation classification model fayyad discretization generalized splitting criterion uci machine learning repository;decision tree;fayyad discretization;splitting criterion;continuous valued splitting criterion discretization decision tree;discretization;decision tree generation;cut point selection;indexes;machine learning;classification model;continuous valued;entropy data handling decision trees;entropy;continuous valued attributes;classification tree analysis;data handling;generalized splitting criterion;uci machine learning repository;decision trees;impurities;entropy machine learning impurities indexes classification tree analysis	Decision tree is one of the most popular and widely used classification models in machine learning. The discretization of continuous-valued attributes plays an important role in decision tree generation. In this paper, we improve Fayyad's discretization method which uses the average class entropy of candidate partitions to select boundaries for discretization. Our method can reduce the number of candidate boundaries further. Here we also propose a generalized splitting criterion for cut point selection and prove that the cut points are always on boundaries when using this criterion. Along with the formal proof, we present empirical results that the decision trees generated by using such criteria are similar on several datasets from the UCI Machine Learning Repository.	decision tree;discretization;eisenstein's criterion;formal proof;high availability;machine learning	Wen-Liang Li;Rui-Hua Yu;Xizhao Wang	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5581069	mathematical optimization;decision tree learning;computer science;machine learning;decision tree;pattern recognition;incremental decision tree;mathematics;id3 algorithm;discretization of continuous features	ML	3.388197700774685	-38.5100349531773	47593
537defd70c25991c6d973aad9973a06db31f426d	a neuro-fuzzy rule-based classifier using important features and top linguistic features		The efficient feature selection for predictive and accurate classification is highly desirable in many application domains. Most of the attempts to neuro-fuzzy classifier lose information to build interpretable neuro-fuzzy classification model. This paper proposes an interpretable neuro-fuzzy classification model with significant features without loss of knowledge, which is an extension of an existing interpretable neuro-fuzzy classification model. The proposed model is designed based on the consideration of feature importance that is determined by frequency of linguistic features. The rules are then made based on important features. Therefore, the knowledge acquired in network can be comprehended to logical rules using only important features. The proposed model finally performs classification task by rule-based approach. The average accuracy calculated by 10-fold cross validation finds that the proposed model can increase performance of the already proven neuro-fuzzy system for classification tasks.	fuzzy rule;neuro-fuzzy	Saroj K. Biswas;Monali Bordoloi;Heisnam Rohen Singh;Biswajit Purkayastha	2016	IJIIT	10.4018/IJIIT.2016070103	computer science;machine learning;pattern recognition;data mining	NLP	9.892615466843951	-38.29379770177976	47754
2b9bea57e972bf356a6894746e458fb292d11794	analysis of patient groups and immunization results based on subspace clustering	knowledge discovery and exploration subspace clustering subspace analysis subspace classification classification explanation;inproceedings	Biomedical experts are increasingly confronted with what is often called Big Data, an important subclass of high-dimensional data. High-dimensional data analysis can be helpful in finding relationships between records and dimensions. However, due to data complexity, experts are decreasingly capable of dealing with increasingly complex data. Mapping higher dimensional data to a smaller number of relevant dimensions is a big challenge due to the curse of dimensionality. Irrelevant, redundant, and conflicting dimensions affect the effectiveness and efficiency of analysis. Furthermore, the possible mappings from highto low-dimensional spaces are ambiguous. For example, the similarity between patients may change by considering different combinations of relevant dimensions (subspaces). We show the potential of subspace analysis for the interpretation of high-dimensional medical data. Specifically, we analyze relationships between patients, sets of patient attributes, and outcomes of a vaccination treatment by means of a subspace clustering approach. We present an analysis workflow and discuss future directions for high-dimensional (medical) data analysis and visual exploration.	big data;cluster analysis;clustering high-dimensional data;curse of dimensionality;data-intensive computing;machine learning;microsoft outlook for mac;relevance;signal-to-noise ratio	Michael Hund;Werner Sturm;Tobias Schreck;Torsten Ullrich;Daniel A. Keim;Ljiljana Majnaric;Andreas Holzinger	2015		10.1007/978-3-319-23344-4_35	data science;machine learning;data mining;mathematics	ML	5.785781157136293	-45.98232467285958	47835
e2a079c855334c4f855c1d96b513d29fe42c0dd8	in search of intelligent genes: the cartesian genetic programming computational neuron (cgpcn)	game of checkers intelligent genes cartesian genetic programming computational neuron artificial neural network;game of checkers;biology computing;evaluation function;neural nets;computational intelligence;cartesian genetic programming;biological system modeling;computational intelligence genetic programming neurons biology computing biological information theory morphology computational and artificial intelligence cells biology artificial neural networks computer networks;intelligent genes;nerve fibers;biology;genetic programming;genetics;computer networks;evolution biology;artificial neural networks;brain modeling;morphology;computational modeling;games of skill;neural nets games of skill genetic algorithms;biological information theory;genetic algorithms;neurons;cartesian genetic programming computational neuron;complex cell;cells biology;artificial neural network;computational and artificial intelligence	Biological neurons are extremely complex cells whose morphology grows and changes in response to the external environment. Yet, artificial neural networks (ANNs) have represented neurons as simple computational devices. It has been evident for a long time that ANNs have learning abilities that are insignificant compared with some of the simplest biological brains. We argue that we understand enough neuroscience to create much more sophisticated models. In this paper, we report on our attempts to do this.We identify and evolve seven programs that together represents a neuron which grows post evolution into a complete 'neurological' system. The network that occurs by running the programs has a highly dynamic morphology in which neurons grow, and die, and neurite branches together with synaptic connections form and change. We have evaluated the capability of these networks for playing the game of checkers. Our method has no board evaluation function, no explicit learning rules and no human expertise at playing checkers is used. The learning abilities of these networks are encoded at a genetic level rather than at the phenotype level of neural connections.	artificial neural network;chaos theory;computation;evaluation function;galaxy morphological classification;genetic programming;mathematical morphology;neuron;plausibility structure;synaptic package manager	Gul Muhammad Khan;Julian Francis Miller;David M. Halliday	2009	2009 IEEE Congress on Evolutionary Computation	10.1109/CEC.2009.4982997	winner-take-all;genetic programming;genetic algorithm;computer science;artificial intelligence;theoretical computer science;machine learning;evaluation function;computational model;artificial neural network	ML	-3.3957955407165703	-47.428914503922506	47953
d33c556dfa8eacacd195ad85380af4459b73f30b	cubic: identification of regulatory binding sites through data clustering	biology computing;pattern clustering;organic compounds;clustering algorithms background noise partitioning algorithms proteins informatics computer science mathematics laboratories frequency data mining;genetics;macromolecules;binding site identification data clustering transcription factor binding sites gene transcription mrna computational identification sequence similarity cluster identification algorithm noisy background computer software cubic;computer software;noise;pattern clustering genetics macromolecules organic compounds noise computer software biology computing	Transcription factor binding sites are short fragments in the upstream regions of genes, to which transcription factors bind to regulate the transcription of genes into mRNA. Computational identification of transcription factor binding sites remains an unsolved challenging problem though a great amount of effort has been put into the study of this problem. We have recently developed a novel technique for identification of binding sites from a set of upstream regions of genes, that could possibly be transcriptionally co-regulated and hence might share similar transcription factor binding sites. By utilizing two key features of such binding sites (i.e. their high sequence similarities and their relatively high frequencies compared to other sequence fragments), we have formulated this problem as a cluster identification problem. That is to identify and extract data clusters from a noisy background. While the classical data clustering problem (partitioning a data set into clusters sharing common or similar features) has been extensively studied, there is no general algorithm for solving the problem of identifying data clusters from a noisy background. In this paper, we present a novel algorithm for solving such a problem. We have proved that a cluster identification problem, under our definition, can be rigorously and efficiently solved through searching for substrings with special properties in a linear sequence. We have also developed a method for assessing the statistical significance of each identified cluster, which can be used to rule out accidental data clusters. We have implemented the cluster identification algorithm and the statistical significance analysis method as a computer software CUBIC. Extensive testing on CUBIC has been carried out. We present here a few applications of CUBIC on challenging cases of binding site identification.		Victor Olman;Dong Xu;Ying Xu	2003	Journal of bioinformatics and computational biology	10.1109/CSB.2003.1227367	macromolecule;biology;computer science;bioinformatics;noise;theoretical computer science;machine learning;data mining;genetics	Comp.	4.300309547374163	-48.47609490385366	48034
e25db834234383fb4a4f1acf3b42c72856c7b166	identifying cell populations in flow cytometry data using phenotypic signatures	sociology statistics clustering algorithms proteins data visualization data mining estimation;phenotypic signature cell populations clustering flow cytometry	Single-cell flow cytometry is a technology that measures the expression of several cellular markers simultaneously for a large number of cells. Identification of homogeneous cell populations, currently done by manual biaxial gating, is highly subjective and time consuming. To overcome the shortcomings of manual gating, automatic algorithms have been proposed. However, the performance of these methods highly depends on the shape of populations and the dimension of the data. In this paper, we have developed a time-efficient method that accurately identifies cellular populations. This is done based on a novel technique that estimates the initial number of clusters in high dimension and identifies the final clusters by merging clusters using their phenotypic signatures in low dimension. The proposed method is called SigClust. We have applied SigClust to four public datasets and compared it with five well known methods in the field. The results are promising and indicate higher performance and accuracy compared to similar approaches reported in literature.	acoustic startle gating;algorithmic trading;approximation algorithm;benchmark (computing);bone marrow;cell (microprocessor);clock gating;cluster analysis;code;diffuse large b-cell lymphoma;dimensionality reduction;estimated;experiment;extraction;f1 score;flock;flow cytometry;graft versus host disease prophylaxis/therapy;ground truth;hemopoietic stem cell transplant;leigh disease;mbm (file format);population;pure function;purity;silo (dataset);software metric;t-distributed stochastic neighbor embedding;type signature;cell type;statistical cluster	Maziyar Baran Pouyan;Mehrdad Nourani	2017	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2016.2550428	data visualization;merge (version control);cluster analysis;homogeneous;data mining;computer science;flow cytometry;bioinformatics;gating	Visualization	4.769642249317574	-50.36746532424597	48107
a1c5ee8965e5d9b3f6e8d24138a974832c3d8a1e	soft kernel spectral clustering	pattern clustering;pattern clustering fuzzy set theory image segmentation network theory graphs;kernel communities prototypes clustering algorithms training image segmentation indexes;model selection technique soft kernel spectral clustering state of the art technique hard assignment method fuzzy assignment cosine distance cluster prototypes sksc average membership strength criterion image segmentation community detection;image segmentation;fuzzy set theory;network theory graphs;sista	In this paper we propose an algorithm for soft (or fuzzy) clustering. In soft clustering each point is not assigned to a single cluster (like in hard clustering), but it can belong to every cluster with a different degree of membership. Generally speaking, this property is desirable in order to improve the interpretability of the results. Our starting point is a state-of-the art technique called kernel spectral clustering (KSC). Instead of using the hard assignment method present therein, we suggest a fuzzy assignment based on the cosine distance from the cluster prototypes. We then call the new method soft kernel spectral clustering (SKSC). We also introduce a related model selection technique, called average membership strength criterion, which solves the drawbacks of the previously proposed method (namely balanced linefit). We apply the new algorithm to synthetic and real datasets, for image segmentation and community detection on networks. We show that in many cases SKSC outperforms KSC.	algorithm;cluster analysis;cosine similarity;dynamical system;fuzzy clustering;image segmentation;kernel (operating system);mathematical optimization;mechatronics;model selection;national fund for scientific research;pixel;sap business one;smart;spectral clustering;synthetic intelligence;vhdl-ams;verilog-ams	Rocco Langone;Raghvendra Mall;Johan A. K. Suykens	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6706850	correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;segmentation-based object categorization;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;fuzzy set;image segmentation;cluster analysis;single-linkage clustering;brown clustering;clustering high-dimensional data	Vision	3.3771352578901013	-39.96952600305951	48153
9417f2aa5a20fc7e90d24b9dd4652e95c0e1edc7	a few new features for genetic algorithms	genetic variation;shared nothing multiprocessors;natural selection;asynchronous parallel computation;biological systems;genetic algorithm;rna polymerase;parallel processing	The project described in this paper attempts to apply insight gained from studying biological systems to a conventional Genetic Algorithm. Natural genes are contained on a chromosome, surrounded by long sequences of “waste” DNA. Useful genes are selected from amid the waste by the operation of RNA polymerase. Natural selection also occurs within relatively large groups; small populations tend to die out from a lack of genetic variation. We believe both of these features are relevant in the application of Genetic Algorithms to very complex problems. Individuals in our experiment are single chromosomes which contain genes of varying length. These genes are evaluated as potential solutions to Hamilton Path problems; however, the mechanism is general enough to apply to many other problem cases. Operations on these individuals include the traditional genetic manipulations: mutation, crossover, inversion, and recombination. We added to this set a shift operation, which is shown to be effective in preventing premature convergence. Our environment supports a relatively large population of individuals, and we have implemented an efficient method for selection within such large populations.	biological system;crossover (genetic algorithm);experiment;gene expression programming;genetic algorithm;hamiltonian path;mutation (genetic algorithm);population;premature convergence;waste	Marshall Graves;William Hooper	1998		10.1145/275295.275363	computer science;bioinformatics;theoretical computer science;genetic representation;distributed computing	Comp.	-0.5762132621280517	-51.73059207741313	48166
f879baafe04a410c1ce8f9a807848897a73eceb4	new approaches to compare phylogenetic search heuristics	biology computing;topology;robinson foulds distance phylogenetic search heuristics maximum parsimony heuristics evolutionary trees good scoring trees entropy based measure topological distance measure;good scoring trees;distance measure;phylogeny organisms biomedical measurements inference algorithms topology history bioinformatics computer science performance analysis convergence;evolution biological;robinson foulds;maximum parsimony heuristics;topological distance measure;genetics;evolutionary trees;phylogenetic tree;performance analysis phylogenetic trees maximum parsimony phylogenetic heuristics;performance analysis;robinson foulds distance;phylogenetic trees;tree searching;phylogenetic search heuristics;entropy based measure;maximum parsimony;tree searching biology computing evolution biological genetics topology;phylogenetic heuristics	We present new and novel insights into the behavior of two maximum parsimony heuristics for building evolutionary trees of different sizes. First, our results show that the heuristics find different classes of good-scoring trees, where the different classes of trees may have significant evolutionary implications. Secondly, we develop a new entropy-based measure to quantify the diversity among the evolutionary trees found by the heuristics. Overall, topological distance measures such as the Robinson-Foulds distance identify more diversity among a collection of trees than parsimony scores, which implies more powerful heuristics could be designed that use a combination of parsimony scores and topological distances. Thus, by understanding phylogenetic heuristic behavior, better heuristics could be designed, which ultimately leads to more accurate evolutionary trees.	algorithm;heuristic (computer science);kullback–leibler divergence;local optimum;maximum parsimony (phylogenetics);occam's razor;phylogenetic tree;phylogenetics;self-information	Seung-Jin Sul;Suzanne Matthews;Tiffani L. Williams	2008	2008 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2008.81	biology;mathematical optimization;combinatorics;phylogenetic tree;bioinformatics;tree rearrangement;mathematics	SE	1.1353783025928157	-50.08681671251174	48311
d68c3ea7871c43df1af572a1defd10040c549003	multiview centroid based fuzzy classification of large data	electronic mail;training;youtube;feature extraction;games;computer science;correlation	Modern data is increasingly complex. High dimensionality, heterogeneity and independent multiple representations are the basic properties of today's data. With increasing sources of data collection, a single object can have multiple representations, which we call views. In this paper we propose a multiview classification technique, which uses fuzzy mapping to obtain maximum similarity between an object and nearest multiview centroids. Our fuzzy mapping based approach obtains a unit L1 hyperplane as a common space for each view. To establish the efficacy of our proposed method we present experimental comparisons with number of baselines on two synthetic and two real-world data sets.	assignment (computer science);baseline (configuration management);computation;fuzzy classification;multiview video coding;synthetic intelligence	Gaurav Tyagi;Nilesh V. Patel;Ishwar K. Sethi	2016	2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2016.0114	games;computer vision;feature extraction;computer science;artificial intelligence;machine learning;data mining;correlation;statistics	Robotics	-1.2080550407286135	-42.64935723103457	48401
577a96c75cd2d881e1712113c618af74b9dfc26b	a survey on clustering in data mining	unsupervised learning;hierarchical clustering;cluster algorithm;k means;data mining;feature vector;clustering;unsupervised classification;clustered data;exploratory data analysis	Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. Unsupervised learning (clustering) deals with which have not been pre classified in any way and so do not have a class attribute associated with them. The scope of applying clustering algorithm is to discover useful but unknown classes of items. Unsupervised learning is an approach of learning where instances are automatically placed into meaningful groups based on their similarity. This paper addresses fundamental concepts of unsupervised learning while it serveys recent clustering algorithm and their complexities.	algorithm;cluster analysis;data mining;feature vector;html attribute;unsupervised learning	M. A. Dalal;N. D. Harale	2011		10.1145/1980022.1980143	unsupervised learning;correlation clustering;data stream clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;biclustering;affinity propagation;hierarchical clustering of networks;k-means clustering;clustering high-dimensional data;conceptual clustering	ML	0.21018510719762534	-41.99407185764873	48852
2ec495f1977907df90ca02a06c30f1bd548abb44	diversity in skylines		Given an integer k, a diverse skyline contains the k skyline points that best describe the tradeoffs (among different dimensions) offered by the full skyline. This paper gives an overview of the latest results on this topic. Specifically, we first describe the state-of-the-art formulation of diverse skylines. Then, we explain several algorithms for finding a diverse skyline, where the objective is to save cost by avoiding the computation of the entire skyline. In particular, we will discuss a polynomial-time algorithm in 2D space that returns the exact result, the NP-hardness of the problem in dimensionality at least 3, and an approximate solution with good quality guarantees.	approximation algorithm;computation;np-hardness;polynomial;time complexity	Yufei Tao	2009	IEEE Data Eng. Bull.		dna;data mining;homologous chromosome;protease;lymphocyte;antibody;amino acid;biochemistry;computer science;effector cell;receptor	DB	-0.6004563575853811	-50.19377349715616	48882
3b274b5e931c46819bedbe430eee99a3330c857d	random forest classification for detecting android malware	smart phones;machine learning android malware;android;android operating system;application domain internet connected smartphone devices android malware detection random forest classification root mean squared error optimal out of bag error rate random feature selection random forest algorithm android application behavior android feature dataset random forest supervised classifier machine learning ensemble learning algorithm internet of things;internet of things;machine learning;malware;data privacy;pattern classification;smart phones android operating system data privacy feature selection internet of things invasive software learning artificial intelligence pattern classification;invasive software;feature selection;learning artificial intelligence;vegetation smart phones malware androids humanoid robots vectors error analysis	Internet connected smartphone devices play a crucial role in the application domain of Internet of Things. These devices are being widely used for day-to-day activities such as remotely controlling lighting and heating at homes, paying for parking, and recently for paying for goods using saved credit card information using Near Field Communication (NFC). Android is the most popular smartphone platform today. It is also the choice of malware authors to obtain secure and private data. In this paper we exclusively apply the machine learning ensemble learning algorithm Random Forest supervised classifier on an Android feature dataset of 48919 points of 42 features each. Our goal was to measure the accuracy of Random Forest in classifying Android application behavior to classify applications as malicious or benign. Moreover, we wanted to focus on detection accuracy as the free parameters of the Random Forest algorithm such as the number of trees, depth of each tree and number of random features selected are varied. Our experimental results based on 5-fold cross validation of our dataset shows that Random Forest performs very well with an accuracy of over 99 percent in general, an optimal Out-Of-Bag (OOB) error rate [3] of 0.0002 for forests with 40 trees or more, and a root mean squared error of 0.0171 for 160 trees.	algorithm;android;application domain;ensemble learning;information privacy;internet of things;machine learning;malware;mean squared error;near field communication;out-of-bag error;random forest;smartphone	Mohammed S. Alam;Son Thanh Vuong	2013	2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing	10.1109/GreenCom-iThings-CPSCom.2013.122	embedded system;computer science;machine learning;data mining;malware;internet privacy;feature selection;world wide web;computer security;internet of things;android	Embedded	6.681981821127138	-38.08593378215161	48948
dbbcefb74cd05a6491c2dafb53f7b1d2eecd489b	the impact of refinement strategies on sequential clustering algorithms	heart iris indexes;reassignement procedures;pattern clustering data mining;ttsas refinement strategy sequential clustering algorithms data patterns spherical clusters basic sequential algorithmic scheme mbsas;reassignement procedures clustering sequential clustering algorithms sequential clustering with merge;clustering;sequential clustering algorithms;sequential clustering with merge	Sequential clustering algorithms have been characterized as fast and straightforward methods which produce, as result, a single clustering. They have the drawback of being dependent on the order in which data patterns are input to the algorithm and, generally, produce compact and spherical clusters. The focus of the work is a group of sequential algorithms which includes the Basic Sequential Algorithmic Scheme (BSAS) and two of its variations, the MBSAS and the TTSAS. The paper investigates refinement strategies which aim to improve the performance of the three sequential algorithms based on two processes: merge and reassignment. Results from experiments conducted in various data domains (from UCI and synthetic) are presented and a comparative analysis is given as evidence of the benefits of sequential clustering algorithm coupled with a refinement procedure.		Maria do Carmo Nicoletti;Eduardo Machado Real;Osvaldo Luiz De Oliveira	2013		10.1109/ISDA.2013.6920706	correlation clustering;data stream clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;affinity propagation;clustering high-dimensional data;conceptual clustering	ML	1.9061142250034488	-40.36856288722951	49068
7e87cab76024a4bdbaed7b8e7ff9c999e725acde	genetic fuzzy fusion of svm classifiers for biomedical data	fuzzy membership function;support vector machines;cancer;support vector machines support vector machine classification bioinformatics kernel fuzzy systems computer science training data bagging genetic algorithms biological system modeling;ovarian cancer dataset genetic fuzzy fusion model multiple svm classifiers biomedical data generalization ability fuzzy system svm hyperplanes genetic algorithm;multiple svm classifiers;fuzzy set theory;generalization ability;fuzzy sets;medical computing;multiple classifiers;ovarian cancer dataset;pattern classification;support vector machines cancer fuzzy set theory fuzzy systems generalisation artificial intelligence genetic algorithms medical computing pattern classification;artificial intelligence;ovarian cancer;genetic algorithm;genetic algorithms;generalisation artificial intelligence;svm hyperplanes;biomedical data;fuzzy systems;fuzzy system;genetic fuzzy fusion model;biomedical computing;fuzzy model	Combining multiple classifiers is a natural way to discover useful information and improve the performance of individual classifiers. In this paper, we propose one approach to combine multiple SVMs and improve the generalization ability of SVM classifiers. One fuzzy system is constructed based on SVM accuracies and distances of data examples to SVM hyperplanes. The output fuzzy membership functions of the fuzzy system are tuned by a genetic algorithm (GA). The established model is applied on an ovarian cancer dataset and the experiment shows the proposed genetic fuzzy model performs more stable and more reliable than individual SVMs	fuzzy control system;genetic algorithm;support vector machine	Xiujuan Chen;Robert W. Harrison;Yanqing Zhang	2005	2005 IEEE Congress on Evolutionary Computation	10.1109/CEC.2005.1554745	genetic algorithm;fuzzy classification;computer science;artificial intelligence;machine learning;pattern recognition;data mining;mathematics;fuzzy set;fuzzy control system	Robotics	9.251542132303463	-40.068146104345765	49187
3e194056ce9643037605e834e5bdc024db376d3d	principal association mining: an efficient classification approach	data mining;classification;association rule;associative classification;knowledge discovery	Classification is one of the key tasks in business intelligence, decision science, and machine learning. Associative classification has aroused significant research interest in recent years due to its superior accuracy. Traditional association rule mining algorithms often yield many redundant and sometimes conflicting class association rules. This paper presents a new, efficient associative classification approach. This new approach produces a compact classifier with a small number of association rules, yet with good classification performance. This approach is based on a novel rule quality metric, named as Principality, which measures an association rule’s classification accuracy and coverage for a specific class. Heuristic methods utilizing the Principality metric are applied to rule pruning and associative classifier construction to produce a compact classifier. This Principal Association Mining (PAM) approach is confirmed to be effective at improving classification accuracy as well as decreasing classifier size by experiments conducted on 17 datasets. 2014 Elsevier B.V. All rights reserved.	algorithm;association rule learning;big data;decision theory;experiment;heuristic;linear classifier;machine learning;missing data;statistical classification	Fuzan Chen;Yanlan Wang;Minqiang Li;Harris Wu;Jin Tian	2014	Knowl.-Based Syst.	10.1016/j.knosys.2014.06.013	association rule learning;biological classification;computer science;machine learning;linear classifier;pattern recognition;data mining;knowledge extraction;one-class classification	AI	8.176241744158487	-39.41618131070674	49338
d4236be6976b69662999746e900bef6ba4c9a4af	pathway activity transformation for multi-class classification of lung cancer datasets	lung cancer;multilayer perceptron;anova;multi class classification;pathway activity transformation;support vector machine	Pathway-based microarray analysis has been found to be a powerful tool to study disease mechanisms and to identify biological markers of complex diseases like lung cancer. From previous studies, the use of pathway activity transformed from gene expression data has been shown to be more informative in disease classification. However, current works on a pathway activity transformation method are for binary-class classification. In this study, we propose a pathway activity transformation method for multi-class data termed Analysis-of-Variance-based Feature Set (AFS). The classification results of using pathway activity derived from our proposed method show high classification power in three-fold cross-validation and robustness in across dataset validation for all four lung cancer datasets used.	gene regulatory network;multiclass classification	Worrawat Engchuan;Jonathan H. Chan	2015	Neurocomputing	10.1016/j.neucom.2014.08.096	support vector machine;analysis of variance;computer science;bioinformatics;machine learning;multiclass classification;data mining;multilayer perceptron	Vision	9.297030595513185	-50.79836763254394	49361
31893c533b449466e4ddb9ea96cf763297fe4a8a	column-wise guided data imputation		This paper investigates data imputation techniques for pre-processing of dataset with missing values. The current literature is mainly focused on the overall accuracy, evaluated estimating the missing values on the dataset at hand, however the predictions can be suboptimal when considering the model performance for each feature. To address this problem, a Column-wise Guided Data Imputation method (cGDI) is proposed. Its main novelty resides in the selection of the most suitable model from a multitude of imputation techniques for each individual feature, through a learning process on the known data. To assess the performance of the proposed technique, empirical experiments have been conducted on 13 publicly available datasets. The results show that cGDI outperforms two baselines and has always comparable or greater estimation accuracy over four state-of-the-art methods, widely applied to solve the problem at hand. Furthermore, cGDI has a straightforward implementation and any other known imputation technique can be easily added.		Alessio Petrozziello;Ivan Jordanov	2017		10.1016/j.procs.2017.05.008	missing data;machine learning;imputation (statistics);novelty;artificial intelligence;data mining;computer science	SE	7.188892006068668	-45.00297687088109	49414
a5691a50235e476f0cc1a3701ecb398e93291ea0	analyzing behavior of objective rule evaluation indices based on pearson product-moment correlation coefficient	bootstrap method;data mining;classification rules;indexation;correlation coefficient;information gain;correlation analysis	In this paper, we present an analysis of behavior of objective rule evaluation indices on classification rule sets using Pearson product-moment correlation coefficients between each index. To support data mining post-processing, which is one of important procedures in a data mining process, at least 40 indices are proposed to find out valuable knowledge. However, their behavior have never been clearly articulated. Therefore, we carried out a correlation analysis between each objective rule evaluation indices. In this analysis, we calculated average values of each index using bootstrap method on 32 classification rule sets learned with information gain ratio. Then, we found the following relationships based on the correlation coefficient values: similar pairs, discrepant pairs, and independent indices. With regarding to this result, we discuss about relative functional relationships between each group of objective indices.	coefficient	Hidenao Abe;Shusaku Tsumoto	2008		10.1007/978-3-540-68123-6_9	association;pattern recognition;data mining;kullback–leibler divergence	AI	2.282284417871107	-38.42794085522509	49688
65088aea57ef71119bbfcc2b2b320dea8484e878	a scoring criterion for rejection of clustered p-values	cross sample cancer study;sequential analysis;fdr;cnv dataset;importance sampling;multiple comparison	In dealing with the multiplicity problem of large dataset, clusters or families of hypotheses are often the units of interest. A scoring method is motivated in adopting a rejection space for p-values that are classified into spatial or labeled groups. A score thatmeasures the benefits/costs ofmaking a true/false discovery is computed and rejection space thatmaximizes the number of rejections with positive score is adopted. Renewal and boundary-crossing theories are used to compute the exceedance probability of the score. Level of strong group type I error control is validated using Monte Carlo and importance sampling methods. It is shown that the scoringmethodmaintains detection power and achieves robustness against model deviation. The scoring method is applied on a copy number variation tumor dataset and short intervals of the chromosome with biological relevance are identified. © 2016 Elsevier B.V. All rights reserved.	error detection and correction;importance sampling;monte carlo method;rejection sampling;relevance;sampling (signal processing);theory	Qingyun Cai	2018	Computational Statistics & Data Analysis	10.1016/j.csda.2016.02.003	econometrics;importance sampling;sequential analysis;data mining;mathematics;multiple comparisons problem;statistics	AI	6.430257050415074	-48.87191517452012	49831
d1119258f28216dc7abed18399fac97c89f3dd66	a multi-view relational fuzzy c-medoid vectors clustering algorithm	fuzzy c medoid vectors;collaborative clustering;relational clustering;multi view clustering;relevance weights	This paper gives a multi-view relational fuzzy c-medoid vectors clustering algorithm that is able to partition objects taking into account simultaneously several dissimilarity matrices. The aim is to obtain a collaborative role of the different dissimilarity matrices in order to obtain a final consensus fuzzy partition. These matrices could have been obtained using different sets of variables and dissimilarity functions. This algorithm is designed to give a fuzzy partition and a vector of medoids for each fuzzy cluster as well as to learn a relevance weight for each dissimilarity matrix by optimizing an objective function. These relevance weights change at each iteration of the algorithm and are different from one cluster to another. Moreover, various tools for interpreting the fuzzy partition and fuzzy clusters provided by this algorithm are also presented. Several examples illustrate the performance and usefulness of the proposed algorithm.	algorithm;cluster analysis;free viewpoint television;medoid	Francisco de A. T. de Carvalho;Filipe M. de Melo;Yves Lechevallier	2015	Neurocomputing	10.1016/j.neucom.2014.11.083	correlation clustering;defuzzification;fuzzy clustering;flame clustering;fuzzy classification;fuzzy number;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;data mining;mathematics;single-linkage clustering;fuzzy set operations	Vision	2.6988717933860906	-39.93506186638269	50141
1c65cbd1c850cbd654aa86b86443e320e5b953ef	on the group theoretical background of assigning stepwise mutations onto phylogenies	health research;uk clinical guidelines;biological patents;substitution model;maximum likelihood;europe pubmed central;nucleotides;amino acid;citation search;physiological cellular and medical topics;group theory;nucleotide sequences;computational biology bioinformatics;evolutionary trees;phylogenetic tree;uk phd theses thesis;life sciences;model;algorithms;matrix multiplication;tree reconstruction;uk research reports;medical journals;substitutions;europe pmc;biomedical research;maximum parsimony;bioinformatics	Recently one step mutation matrices were introduced to model the impact of substitutions on arbitrary branches of a phylogenetic tree on an alignment site. This concept works nicely for the four-state nucleotide alphabet and provides an efficient procedure conjectured to compute the minimal number of substitutions needed to transform one alignment site into another. The present paper delivers a proof of the validity of this algorithm. Moreover, we provide several mathematical insights into the generalization of the OSM matrix to multi-state alphabets. The construction of the OSM matrix is only possible if the matrices representing the substitution types acting on the character states and the identity matrix form a commutative group with respect to matrix multiplication. We illustrate this approach by looking at Abelian groups over twenty states and critically discuss their biological usefulness when investigating amino acids.	algorithm;alignment;alphabet;amino acids, i.v. solution additive;bio-informatics;bioinformatics;columbia sk virus;computational complexity theory;generalization (psychology);intuition;mathematics;matrix multiplication;mutation;numerous;openstreetmap;phylogenetic tree;phylogenetics;radio frequency;requirement;ski combinator calculus;self-propelled particles;stepwise regression;undo;algorithm;osmole (unit of measure)	Mareike Fischer;Steffen Klaere;Minh Anh Nguyen;Arndt von Haeseler	2011		10.1186/1748-7188-7-36	biology;phylogenetic tree;computer science;bioinformatics;mathematics;group theory;algorithm	Comp.	0.7211612548665884	-51.73714390342074	50150
f9dd9e1dd3d0389006d4cbf49a4ffc6e738a4ecd	nonparametric techniques to extract fuzzy rules for breast cancer diagnosis problem	fuzzy set;wisconsin madison data;classification;nearest neighbors algorithm;clustering;rough set;diagnosis;rule base;breast cancer	This paper addresses breast cancer diagnosis problem as a pattern classification problem. Specifically, the problem is studied using Wisconsin-Madison breast cancer data set. Fuzzy rules are generated from the input-output relationship so that the diagnosis becomes easier and transparent for both patients and physicians. For each class, at least one training pattern is chosen as the prototype, provided (a) the maximum membership of the training pattern is in the given class, and (b) among all the training patterns, the neighborhood of this training pattern has the least fuzzy-rough uncertainty in the given class. Using the fuzzy-rough uncertainty, a cluster is constructed around each prototype. Finally, these clusters are interpreted as the fuzzy rules that relate the prognostic factors and the diagnosis results. The advantages of the proposed algorithm are, (a) there is no need to know the structure of the training data, (b) the number of fuzzy rules does not increase with the increase of the number of input dimensions, and (c) small number of fuzzy rules is generated. With the three generated fuzzy rules, 96.20% classification efficiency is achieved, which is comparable to other rule generation techniques.	addresses (publication format);breast carcinoma;emoticon;fuzzy rule;fuzzy set;malignant neoplasm of breast;mammary neoplasms;need to know;patients;prototype;rule (guideline);algorithm	Manish Sarkar;Tze-Yun Leong	2001	Studies in health technology and informatics	10.3233/978-1-60750-928-8-1394	machine learning;pattern recognition;data mining;mathematics	AI	8.366385353504562	-43.673095701039436	50192
021f41fdfb187a01aa8297a2058f662dc89aec7e	carsvm: a class association rule-based classification framework and its application to gene expression data	gene expression data;data mining;association rule mining;gene expression;gene expression analysis;machine learning;association rule;associative classifiers;support vector machine;gene selection;gene expression classification	OBJECTIVE In this study, we aim at building a classification framework, namely the CARSVM model, which integrates association rule mining and support vector machine (SVM). The goal is to benefit from advantages of both, the discriminative knowledge represented by class association rules and the classification power of the SVM algorithm, to construct an efficient and accurate classifier model that improves the interpretability problem of SVM as a traditional machine learning technique and overcomes the efficiency issues of associative classification algorithms.   METHOD In our proposed framework: instead of using the original training set, a set of rule-based feature vectors, which are generated based on the discriminative ability of class association rules over the training samples, are presented to the learning component of the SVM algorithm. We show that rule-based feature vectors present a high-qualified source of discrimination knowledge that can impact substantially the prediction power of SVM and associative classification techniques. They provide users with more conveniences in terms of understandability and interpretability as well.   RESULTS We have used four datasets from UCI ML repository to evaluate the performance of the developed system in comparison with five well-known existing classification methods. Because of the importance and popularity of gene expression analysis as real world application of the classification model, we present an extension of CARSVM combined with feature selection to be applied to gene expression data. Then, we describe how this combination will provide biologists with an efficient and understandable classifier model. The reported test results and their biological interpretation demonstrate the applicability, efficiency and effectiveness of the proposed model.   CONCLUSION From the results, it can be concluded that a considerable increase in classification accuracy can be obtained when the rule-based feature vectors are integrated in the learning process of the SVM algorithm. In the context of applicability, according to the results obtained from gene expression analysis, we can conclude that the CARSVM system can be utilized in a variety of real world applications with some adjustments.	algorithm;association rule learning;cross-validation (statistics);extraction;feature selection;feature vector;gene expression;logic programming;machine learning;rule (guideline);score;silo (dataset);subgroup;support vector machine;test set;triangulation	Keivan Kianmehr;Reda Alhajj	2008	Artificial intelligence in medicine	10.1016/j.artmed.2008.05.002	gene expression;association rule learning;computer science;machine learning;linear classifier;pattern recognition;data mining;structured support vector machine;one-class classification	ML	8.809193628419518	-45.99000357952478	50213
7a9306b6662ad188eb2ca24f00137b4131b98f52	a platform for the selection of genes in dna microarraydata using evolutionary algorithms	generalization error;decision tree;estimation method;wrapper methods;dna microarrays;nearest neighbour;feature selection;dna microarray data;evolutionary algorithm;dna microarray;k fold cross validation;bioinformatics	This paper presents a flexible framework to the task of featureselection in classification of DNA microarray data. Theuser can select a number of filter methods in the preprocessingstage and choose from a wide set of classifiers (models and algorithms from WEKA [17] are available) and accuracy estimation methods. This approach implements wrapper methods, where Evolutionary Algorithms, with variable sized set based representations are used to reduce the number of attributes. Two case studies were used to validate the approach, with three distinct classifiers (1-nearest neighbour, decision trees, SVMs), a filter method based on discriminant fuzzy patterns and k-fold cross-validation to estimate the generalization error.	cross-validation (statistics);dna microarray;decision tree;discriminant;evolutionary algorithm;generalization error;monte carlo method;weka	Miguel Rocha;Rui Mendes;Paulo Maia;Daniel Glez-Peña;Florentino Fernández Riverola	2007		10.1145/1276958.1277042	dna microarray;computer science;bioinformatics;machine learning;evolutionary algorithm;pattern recognition;feature selection	ML	9.635104146637978	-45.304113621796674	50250
945c0e3a052f8278c21673c12e676cca2a1a9b04	efficient rna isoform identification and quantification from rna-seq data with network flows	software;isoforms;rna seq;graph flow;rna isoforms;models statistical;algorithms;humans;sequence analysis rna;lasso;gene expression profiling;exons	MOTIVATION Several state-of-the-art methods for isoform identification and quantification are based on [Formula: see text]-regularized regression, such as the Lasso. However, explicitly listing the-possibly exponentially-large set of candidate transcripts is intractable for genes with many exons. For this reason, existing approaches using the [Formula: see text]-penalty are either restricted to genes with few exons or only run the regression algorithm on a small set of preselected isoforms.   RESULTS We introduce a new technique called FlipFlop, which can efficiently tackle the sparse estimation problem on the full set of candidate isoforms by using network flow optimization. Our technique removes the need of a preselection step, leading to better isoform identification while keeping a low computational cost. Experiments with synthetic and real RNA-Seq data confirm that our approach is more accurate than alternative methods and one of the fastest available.   AVAILABILITY AND IMPLEMENTATION Source code is freely available as an R package from the Bioconductor Web site (http://www.bioconductor.org/), and more information is available at http://cbio.ensmp.fr/flipflop.   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	algorithm;algorithmic efficiency;bioconductor;bioinformatics;exons;fastest;flip-flop (electronics);flow network;lasso;mathematical optimization;protein isoforms;quantitation;r language;rna;source code;sparse matrix;synthetic intelligence;system identification;transcript	Elsa Bernard;Laurent Jacob;Julien Mairal;Jean-Philippe Vert	2014		10.1093/bioinformatics/btu317	biology;rna-seq;exon;computer science;bioinformatics;lasso;gene isoform;data mining;gene expression profiling;world wide web;genetics	Comp.	2.4316508599743414	-52.02726221165572	50278
8947542e590ac5c244f56aed7818c479b4e497e8	incremental semi-supervised clustering ensemble for high dimensional data clustering	linear programming clustering algorithms computer science search problems robots cancer gene expression	Traditional cluster ensemble approaches have three limitations: (1) They do not make use of prior knowledge of the datasets given by experts. (2) Most of the conventional cluster ensemble methods cannot obtain satisfactory results when handling high dimensional data. (3) All the ensemble members are considered, even the ones without positive contributions. In order to address the limitations of conventional cluster ensemble approaches, we first propose an incremental semi-supervised clustering ensemble framework (ISSCE) which makes use of the advantage of the random subspace technique, the constraint propagation approach, the proposed incremental ensemble member selection process, and the normalized cut algorithm to perform high dimensional data clustering. The random subspace technique is effective for handling high dimensional data, while the constraint propagation approach is useful for incorporating prior knowledge. The incremental ensemble member selection process is newly designed to judiciously remove redundant ensemble members based on a newly proposed local cost function and a global cost function, and the normalized cut algorithm is adopted to serve as the consensus function for providing more stable, robust, and accurate results. Then, a measure is proposed to quantify the similarity between two sets of attributes, and is used for computing the local cost function in ISSCE. Next, we analyze the time complexity of ISSCE theoretically. Finally, a set of nonparametric tests are adopted to compare multiple semisupervised clustering ensemble approaches over different datasets. The experiments on 18 real-world datasets, which include six UCI datasets and 12 cancer gene expression profiles, confirm that ISSCE works well on datasets with very high dimensionality, and outperforms the state-of-the-art semi-supervised clustering ensemble approaches.	algorithm;cluster analysis;ensemble learning;experiment;local consistency;loss function;semi-supervised learning;semiconductor industry;software propagation;time complexity	Zhiwen Yu;Peinan Luo;Si Wu;Guoqiang Han;Jane You;Hareton K. N. Leung;Hau-San Wong;Jun Zhang	2016	IEEE Transactions on Knowledge and Data Engineering	10.1109/ICDE.2016.7498386	correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;flame clustering;computer science;bioinformatics;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;brown clustering;dbscan;biclustering;affinity propagation;clustering high-dimensional data;conceptual clustering	ML	2.0669289873259498	-41.44892072530181	50605
eb7b1237e75a4e328ae75deb66cf4ba00f1cf1f4	a new gene selection technique using feature selection methodology	gene selection;dna microarray;gene expression;feature selection	The DNA Microarray technology can measure the expression levels of thousands of genes simultaneously, and produces huge volumes of gene expression data. Such gene data include complex variations among expression levels of genes in the various classes of samples, which allows for classifying and clustering the samples based on only a small subset of genes. We aim to identify those genes that demonstrate high capabilities of discrimination between the classes of samples (e.g. the normal vs disease tissue samples). We present a new technique for gene selection and extraction using various feature selection techniques. Our method is based on computing thresholds and discriminating capabilities of each gene, and classifying the data according to only those genes that have highest discriminating capabilities. The method extracts very small subsets of informative genes that can improve the classification accuracy. We applied the method on four different common gene expression datasets used mainly for this purpose. The method produces encouraging and competitive results of classification performance compared with recent similar techniques.	cluster analysis;dna microarray;feature selection;information;statistical classification	Noushin Ghaffari;Hisham Al-Mubaid	2006			distributed computing;gene expression;computer science;gene;dna microarray;cluster analysis;gene expression profiling;feature selection;bioinformatics	Comp.	7.976379199405428	-48.59120629376489	50760
707607619f8d441a41f55a65e2578a473a62d36a	optimization of fuzzy systems using group-based evolutionary algorithm	group based evolutionary algorithm gea;differential evolution de;fuzzy system fs;proceedings paper;optimization	This paper proposes a group-based evolutionary algorithm (GEA) for the fuzzy system (FS) optimization. Initially, we adopt an entropy measure method to determine the number of rules. Fuzzy rules are automatically generated from training data by entropy measure. Subsequently, the GEA is performed to optimize all the free parameters for the FS design. In the evolution process, a FS is coded as an individual. All individuals based on their performance are partitioned into a superior group and an inferior group. The superior group, which is composed of individuals with better performance, uses a global evolution operation to search potential individuals. In the inferior group, individuals with a worse performance employ the local evolution operation to search better individuals near the current best individual. Finally, the proposed FS with GEA model (FS-GEA) is applied to time series forecasting problem. Results show that the proposed FS-GEA model obtains better performance than other algorithm.	evolutionary algorithm;fuzzy control system	Jyh-Yeong Chang;Ming-Feng Han;Chin-Teng Lin	2012		10.1007/978-3-642-34487-9_36	simulation;artificial intelligence;machine learning	Robotics	10.017703878584005	-40.76923852049418	50779
85e529ff9b835b665a8b6c2a8f74ba951e1e726a	a practical outlier detection approach for mixed-attribute data	data mining;outlier detection;mixture model;mixed attribute data;bivariate beta	Outlier detection in mixed-attribute space is a challenging problem for which only few approaches have been proposed. However, such existing methods suffer from the fact that there is a lack of an automatic mechanism to formally discriminate between outliers and inliers. In fact, a common approach to outlier identification is to estimate an outlier score for each object and then provide a ranked list of points, expecting outliers to come first. A major problem of such an approach is where to stop reading the ranked list? How many points should be chosen as outliers? Other methods, instead of outlier ranking, implement various strategies that depend on user-specified thresholds to discriminate outliers from inliers. Ad hoc threshold values are often used. With such an unprincipled approach it is impossible to be objective or consistent. To alleviate these problems, we propose a principled approach based on the bivariate beta mixture model to identify outliers in mixed-attribute data. The proposed approach is able to automatically discriminate outliers from inliers and it can be applied to both mixed-type attribute and single-type (numerical or categorical) attribute data without any feature transformation. Our experimental study demonstrates the suitability of the proposed approach in comparison to mainstream methods.		Mohamed Bouguessa	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.07.018	anomaly detection;computer science;machine learning;pattern recognition;mixture model;data mining;mathematics;statistics	AI	-0.592500044556812	-41.97919665737013	50850
8848df333af9374c55f9c6c75becfb687368b3e8	feature selection stability assessment based on the jensen-shannon divergence	jensen shannon divergence;stability;robustness;feature selection;feature ranking	Feature selection and ranking techniques play an important role in the analysis of high-dimensional data. In particular, their stability becomes crucial when the feature importance is later studied in order to better understand the underlying process. The fact that a small change in the dataset may affect the outcome of the feature selection/ranking algorithm has been long overlooked in the literature. We propose an information-theoretic approach, using the Jensen-Shannon divergence to assess this stability (or robustness). Unlike other measures, this new metric is suitable for different algorithm outcomes: full ranked lists, partial sublists (top-k lists) as well as the least studied partial ranked lists. This generalized metric attempts to measure the disagreement among a whole set of lists with the same size, following a probabilistic approach and being able to give more importance to the differences that appear at the top of the list. We illustrate and compare it with popular metrics like the Spearman rank correlation and the Kuncheva's index on feature selection/ ranking outcomes artificially generated and on an spectral fat dataset with different filter-based feature selectors.	feature selection;jensen's inequality;shannon (unit)	Roberto Guzmán-Martínez;Rocío Alaíz-Rodríguez	2011		10.1007/978-3-642-23780-5_48	stability;computer science;machine learning;jensen–shannon divergence;pattern recognition;data mining;mathematics;feature selection;statistics;robustness	AI	7.48231986915211	-44.755104772269846	50963
c9653893f77f917564e78c5ebfea0b4b8dea2574	subtractive clustering of vertices for cpca based animation geometry compression	subtractive clustering;animation geometry compression;cpca;geometry compression;principal component analysis;k means algorithm	In the Clustered PCA(CPCA) algorithm for compressing the animation geometry sequences, the vertex trajectories are clustered using the K-means algorithm followed by the Principal Component Analysis(PCA) of the clusters. However, the compression performance of the method is constrained by the initial random selection of the cluster centres. This paper presents a stable method for initializing the cluster centres by the subtractive clustering technique prior to the application of the K-means algorithm. Simulation results on some test animation sequences show better performance of the CPCA with the proposed initialization compared to the CPCA with random initialization.	algorithm;cluster analysis;k-means clustering;simulation;vertex (geometry)	Sanjib Das;Prabin Kumar Bora;Anup Kumar Gogoi	2010		10.1145/1924559.1924587	computer vision;machine learning;pattern recognition;mathematics	ML	2.4629969863820547	-42.88517219279746	51407
96b45b367c5c190467b17e2ba142608d30124dda	linear fuzzy clustering with selection of variables using graded possibilistic approach	pattern clustering;possibilistic clustering;principal component analysis input variables least squares approximation data mining prototypes fuzzy sets databases data analysis data structures clustering algorithms;selection of variables;indexing terms;data mining;fuzzy set theory;variable selection;fuzzy clustering;variable selection data mining fuzzy clustering possibilistic clustering principal component analysis;close relationships;principal component analysis;linear model;graded possibilistic approach;correlation structure;correlation structure graded possibilistic approach linear fuzzy clustering knowledge discovery;numerical experiment;principal component analysis data mining fuzzy set theory pattern clustering;cluster model;linear fuzzy clustering;principal component;knowledge discovery	Linear fuzzy clustering is a useful tool for knowledge discovery in databases (KDD), and several modifications have been proposed in order to analyze real world data. This paper proposes a new approach for estimating local linear models, in which linear fuzzy clustering is performed by selecting variables that are useful for extracting correlation structure in each cluster. The new clustering model uses two types of memberships. One is the conventional membership that represents the degree of membership of each sample in each cluster. The other is the additional parameter that represents the relative responsibility of each variable for estimation of local linear models. The additional membership takes large values when the variable has close relationship with local principal components, and is calculated by using the graded possibilistic approach. Numerical experiments demonstrate that the proposed method is useful for identifying local linear model taking typicality of each variable into account.	algorithm;cluster analysis;collaborative filtering;data mining;database;experiment;fuzzy clustering;fuzzy cognitive map;fuzzy set;linear model;mathematical optimization;missing data;mixture model;model selection;mutual exclusion;numerical method;weight function	Katsuhiro Honda;Hidetomo Ichihashi;Francesco Masulli;Stefano Rovetta	2007	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2006.889946	fuzzy clustering;flame clustering;computer science;machine learning;pattern recognition;data mining;mathematics;knowledge extraction;cluster analysis;feature selection;principal component analysis	ML	2.7772335662066063	-38.60617904989456	51501
cfe3fc3c2bb329ffcd52aefa77729461ba248c41	fast calculation of pairwise mutual information for gene regulatory network reconstruction	microarray data;regulatory network;gene expression;b cell;mutual information;kernel method;microarray;gene regulatory network;information theoretic;software implementation	We present a new software implementation to more efficiently compute the mutual information for all pairs of genes from gene expression microarrays. Computation of the mutual information is a necessary first step in various information theoretic approaches for reconstructing gene regulatory networks from microarray data. When the mutual information is estimated by kernel methods, computing the pairwise mutual information is quite time-consuming. Our implementation significantly reduces the computation time. For an example data set of 336 samples consisting of normal and malignant B-cells, with 9563 genes measured per sample, the current available software for ARACNE requires 142 hours to compute the mutual information for all gene pairs, whereas our algorithm requires 1.6 hours. The increased efficiency of our algorithm improves the feasibility of applying mutual information based approaches for reconstructing large regulatory networks.	algorithm;computation (action);gene expression;gene regulatory network;kernel method;microarray;mutual information;nc (complexity);theory;time complexity	Peng Qiu;Andrew J. Gentles;Sylvia K. Plevritis	2009	Computer methods and programs in biomedicine	10.1016/j.cmpb.2008.11.003	microarray analysis techniques;kernel method;gene regulatory network;gene expression;computer science;bioinformatics;theoretical computer science;microarray;data mining;mutual information	Comp.	3.33079214218483	-51.25374606023332	51523
5c9a1ff38bbd4364c1d945da73c42a8274cbd76a	a new efficient density-based data clustering technique using cross expansion for data mining	pattern clustering;random access memory;new efficient density;data mining abstracts clustering algorithms prediction algorithms noise filtering algorithms random access memory;density based clustering data mining data clustering;prediction algorithms;data points;diagonal sampling;data mining;pattern clustering data mining;density based clustering;data clustering;filtering algorithms;data clustering technique;abstracts;cross expansion;clustering algorithms;noise filtering rate new efficient density data clustering technique cross expansion data mining diagonal sampling data points;noise;noise filtering rate	This investigation develops a new data clustering technique. It is a new density-based clustering scheme by diagonal sampling and a new method of fold and rotation for enhancing data clustering performance. The proposed algorithm's expansion without selecting data points to increase computation cost and it may considerably lower time cost The experimental results confirm that the presented approach has fairly high clustering accuracy and noise filtering rate, and is faster than numerous well-known existing density-based data clustering algorithms such as DBSCAN, IDBSCAN, KIDBSCAN and FDBSCAN approaches.	algorithm;cluster analysis;computation;dbscan;data mining;data point;sampling (signal processing)	Cheng-Fa Tsai;Po-Yi She	2014	2014 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2014.7009662	correlation clustering;determining the number of clusters in a data set;data stream clustering;subclu;k-medians clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;dbscan;optics algorithm;biclustering;affinity propagation;statistics;clustering high-dimensional data	ML	-1.678760112511097	-40.47565828462619	51690
28c2f7a40928c779a5be47edeca93c79cfb7df52	br: a new method for computing all typical testors	typical testors;pattern recognition;feature selection;exponential growth	Typical testors are very useful in Pattern Recognition, especially for Feature Selection problems. The complexity of computing all typical testors of a training matrix has an exponential growth with respect to the number of features. Several methods that speed up the calculation of the set of all typical testors have been developed, but nowadays, there are still problems where this set is impossible to find. With this aim, a new external scale algorithm  BR  is proposed. The experimental results demonstrate that this method clearly outperforms the two best algorithms reported in the literature.		Alexsey Lias-Rodríguez;Aurora Pons-Porrata	2009		10.1007/978-3-642-10268-4_50	exponential growth;computer science;theoretical computer science;machine learning;pattern recognition;mathematics;feature selection;algorithm	HPC	7.800466192421676	-42.00745577008687	51955
37f07a938389a883cc3b6187c43a9a075e01f44a	a systems biology approach to solving the puzzle of unknown genomic gene-function association using grid-ready svm committee machines	biology computing;genomics;support vector machines;ncku 成功大學 成大 圖書館 機構典藏;biological system modeling;genetics;data visualisation;data analysis;proteins;support vector machines biology computing data analysis data visualisation genetics grid computing learning artificial intelligence proteins;dissertations and theses journal referred papers conference papers nsc reserach report patent nckur ir ncku institutional repostiory 博碩士論文 期刊論文 國科會研究報告 專利 成大機構典藏;protein interaction network analysis systems biology approach unknown genomic gene function association grid ready svm committee machines high throughput data computational intelligence machine learning visualization intelligence protein protein interactions protein function annotation machine learning intelligence grid computing large scale data analysis;informatics;learning artificial intelligence;grid computing;genomics bioinformatics genetics proteins biological system modeling informatics;bioinformatics	Genomic researchers face the common challenge of deriving the functions of genes and proteins from high-throughput data. Experimental validation of protein function is costly and time-consuming. With the increased effectiveness of computational intelligence approaches, researchers aim to target the problem with in silico prediction of protein interactions and functions. We propose a systems biology approach that consists of machine-learning and visualization intelligence and aims to predict protein-protein interactions and enhance protein function annotation. Our machine-learning intelligence, SVM committee machines, is compatible with grid computing and large-scale data analysis. In this paper, we not only elucidate the computational power of protein interactions prediction, but also aim to emphasize the interpretation of protein function annotation through protein interaction network analysis.	computation;computational intelligence;gene ontology term enrichment;gene expression profiling;grid computing;high-throughput computing;interaction network;machine learning;systems biology;throughput	Tsung-Lu Michael Lee;Jung-Hsien Chiang	2012	IEEE Computational Intelligence Magazine	10.1109/MCI.2012.2215126	support vector machine;genomics;computer science;bioinformatics;artificial intelligence;data science;machine learning;data mining;data analysis;informatics;grid computing	Comp.	5.319050967590994	-49.79392149521608	51997
867dd987b9439868b2239604907406b3486953fd	correlation as a heuristic for accurate and comprehensible ant colony optimization based classifiers	ant colony optimisation;data mining ant colony optimization based classification rule discovery algorithm heuristic function dataset attributes correlation classification algorithms;data mining;data mining ant colony optimization classification algorithms;pattern classification;classification algorithms training accuracy probabilistic logic ant colony optimization heuristic algorithms image color analysis;pattern classification ant colony optimisation data mining	The primary objective of this research is to propose and investigate a novel ant colony optimization-based classification rule discovery algorithm and its variants. The main feature of this algorithm is a new heuristic function based on the correlation between attributes of a dataset. Several aspects and parameters of the proposed algorithm are investigated by experimentation on a number of benchmark datasets. We study the performance of our proposed approach and compare it with several state-of-the art commonly used classification algorithms. Experimental results indicate that the proposed approach builds more accurate models than the compared algorithms. The high accuracy supplemented by the comprehensibility of the discovered rule sets is the main advantage of this method.	ant colony optimization algorithms;apriori algorithm;association rule learning;benchmark (computing);experiment;heuristic (computer science);mathematical optimization;phase-shift oscillator	Abdul Rauf Baig;Waseem Shahzad;Salabat Khan	2013	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2012.2231868	ant colony optimization algorithms;parallel metaheuristic;computer science;machine learning;pattern recognition;data mining;metaheuristic	Vision	9.659391048442888	-43.42470742746671	52281
0a2f918961b9a9775d5523d0ea2972c0366a0c06	accelerating calculations of rna secondary structure partition functions using gpus	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;physiological cellular and medical topics;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	RNA performs many diverse functions in the cell in addition to its role as a messenger of genetic information. These functions depend on its ability to fold to a unique three-dimensional structure determined by the sequence. The conformation of RNA is in part determined by its secondary structure, or the particular set of contacts between pairs of complementary bases. Prediction of the secondary structure of RNA from its sequence is therefore of great interest, but can be computationally expensive. In this work we accelerate computations of base-pair probababilities using parallel graphics processing units (GPUs). Calculation of the probabilities of base pairs in RNA secondary structures using nearest-neighbor standard free energy change parameters has been implemented using CUDA to run on hardware with multiprocessor GPUs. A modified set of recursions was introduced, which reduces memory usage by about 25%. GPUs are fastest in single precision, and for some hardware, restricted to single precision. This may introduce significant roundoff error. However, deviations in base-pair probabilities calculated using single precision were found to be negligible compared to those resulting from shifting the nearest-neighbor parameters by a random amount of magnitude similar to their experimental uncertainties. For large sequences running on our particular hardware, the GPU implementation reduces execution time by a factor of close to 60 compared with an optimized serial implementation, and by a factor of 116 compared with the original code. Using GPUs can greatly accelerate computation of RNA secondary structure partition functions, allowing calculation of base-pair probabilities for large sequences in a reasonable amount of time, with a negligible compromise in accuracy due to working in single precision. The source code is integrated into the RNAstructure software package and available for download at http://rna.urmc.rochester.edu .	analysis of algorithms;base pairing;cuda;computation (action);computer graphics;download;fastest;fold (higher-order function);graphics processing unit;inborn errors of metabolism;memory disorders;multiprocessing;probability;rna;recursion;round-off error;run time (program lifecycle phase);single-precision floating-point format;source code;free energy	Harry A. Stern;David H. Mathews	2013		10.1186/1748-7188-8-29	biology;medical research;computer science;bioinformatics;data science;algorithm	Comp.	-2.098081244784425	-51.90949762741345	52291
9ff3c2641b97add087673843c179292671f35ed2	classification of clinical gene-sample-time microarray expression data via tensor decomposition methods	three-dimensional microarray data;tensor decomposition method;recent advance;microarray technology;tensor decomposition;gst datasets;gene expression;expression level;real gst dataset;clinical gene-sample-time microarray expression;microarray data;gst data	With the recent advances in microarray technology, the expression levels of genes with respect to samples can be monitored over a series of time points. Such three-dimensional microarray data, termed gene-sample-time (GST) microarray data, are gene expression matrices measured as a time-series. They have not yet received considerable attention, and analysis methods need to be devised specifically to tackle the complexity of GST datasets. We propose methods that are based on tensor decomposition for the sample classification. We use tensor decomposition in order to extract discriminative features as well as multilinearly reducing high dimensionality. We then classify the test samples in the reduced spaces. We have tested and compared our approaches on a real GST dataset. We show that our methods are at least comparable in prediction accuracy to recent methods devised for GST data. Most importantly, our methods run much faster than current approaches.	microarray	Yifeng Li;Alioune Ngom	2010		10.1007/978-3-642-21946-7_22	bioinformatics	Vision	7.711358195273861	-50.151514469557284	52324
2b5aa2a8745dffd9c083a97ced7b9b47d7097a07	on efficient meta-filtering of big data	dna;histograms;measurement;data mining;information filters;algorithm design and analysis	There is an explosion in the number of new approaches being developed for extracting information from biological data, creating a need for intelligent ways to utilize so many methods. We take a perspective based on viewing methods as filters which reject undesired data and which may have complementary and redundant performance. We consider approaches for efficiently combining such filters. We provide quantitative strategies for choosing which filters to use and the best order to apply them, based on viewing each filter as a coordinate transformation on performance metrics. The approach is demonstrated using a range of methods to filter sequence data for homology detection, where we show the advantages of more sophisticated strategies in terms of achieving competitive speed with significantly-improved rejection of undesired data.	big data;choose (action);deny (action);filter (software);homologous gene;homology (biology);rejection sampling	Keith Dillon;Yu-Ping Wang	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7591350	algorithm design;computer science;theoretical computer science;machine learning;data mining;histogram;mathematics;dna;measurement;statistics	Robotics	6.225814017695577	-48.85364455654254	52618
13c03f664881195a63cd54a49789d8c2d5328460	searching protein 3-d structures for optimal structure alignment using intelligent algorithms and data structures	similarity metric;inverse document frequency;vector model;time complexity;information retrieval;frequency inverse document frequency term weighing schema;structural classification;vectors bioinformatics information retrieval pattern classification proteins proteomics tree data structures;protein 3 d structures;optimal structure alignment;frequency inverse document frequency term weighing schema protein 3 d structures optimal structure alignment intelligent algorithms data structures bioinformatics information retrieval algorithms vector model protein similarity structural classification cosine similarity measure;tree data structures;term frequency;structural bioinformatics;suffix tree;algorithms artificial intelligence computational biology data mining databases protein humans protein structure tertiary proteins reproducibility of results structural homology protein;proteins;vectors;intelligent algorithms;indexing;data structures;proteins amino acids indexing information retrieval clustering algorithms data structures;pattern classification;protein similarity;clustering algorithms;amino acids;cosine similarity measure;structural classification of proteins;structural classification of proteins scop;suffix trees;proteomics;similarity measure;algorithms and data structure;protein data bank;information retrieval algorithms;information retrieval ir;vector model information retrieval ir proteins structural classification of proteins scop similarity measure suffix trees;bioinformatics;structure alignment	In this paper, we present a novel algorithm for measuring protein similarity based on their 3-D structure (protein tertiary structure). The algorithm used a suffix tree for discovering common parts of main chains of all proteins appearing in the current research collaboratory for structural bioinformatics protein data bank (PDB). By identifying these common parts, we build a vector model and use some classical information retrieval (IR) algorithms based on the vector model to measure the similarity between proteins - all to all protein similarity. For the calculation of protein similarity, we use term frequency inverse document frequency (tf × idf) term weighing schema and cosine similarity measure. The goal of this paper is to introduce new protein similarity metric based on suffix trees and IR methods. Whole current PDB database was used to demonstrate very good time complexity of the algorithm as well as high precision. We have chosen the structural classification of proteins (SCOP) database for verification of the precision of our algorithm because it is maintained primarily by humans. The next success of this paper would be the ability to determine SCOP categories of proteins not included in the latest version of the SCOP database (v. 1.75) with nearly 100% precision.	algorithm;categories;cosine similarity;data structure;databases;information retrieval;physical information;scop;similarity measure;structural bioinformatics;suffix tree;tf–idf;time complexity;trees (plant);verification of theories;worldwide protein data bank;tertiary	Tomás Novosad;Václav Snásel;Ajith Abraham;Jack Y. Yang	2010	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2010.2079939	computer science;bioinformatics;machine learning;pattern recognition;data mining;proteomics;protein structure database;tf–idf	Comp.	-3.154513947068307	-51.993069609926955	52661
8a16bcf5b8c0d68dac045a2a5e1196c090eec4e1	ensemble-index: a new approach to indexing large databases	discrete wavelet transform;high dimensionality;fourier transform;singular value decomposition;time series data mining;index structure;time series;indexing and retrieval;data mining;dimensionality reduction;indexation;discrete fourier transform;query by content;dimensional reduction;similarity search;polynomial approximation	The problem of similarity search (query-by-content) has attracted much research interest. It is a difficult problem because of the inherently high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier Transform (DFT), the Discrete Wavelet Transform (DWT) and Piecewise Polynomial Approximation. In this work, we introduce a novel framework for using ensembles of two or more representations for more efficient indexing. The basic idea is that instead of committing to a single representation for an entire dataset, different representations are chosen for indexing different parts of the database. The representations are chosen based upon a local view of the database. For example, sections of the data that can achieve a high fidelity representation with wavelets are indexed as wavelets, but highly spectral sections of the data are indexed using the Fourier transform. At query time, it is necessary to search several small heterogeneous indices, rather than one large homogeneous index. As we will theoretically and empirically demonstrate this results in much faster query response times.	approximation;database;dimensionality reduction;discrete fourier transform;discrete wavelet transform;polynomial;similarity search;singular value decomposition	Eamonn J. Keogh;Selina Chu;Michael J. Pazzani	2001		10.1145/502512.502531	fourier transform;discrete mathematics;computer science;machine learning;discrete fourier transform;time series;pattern recognition;data mining;mathematics;discrete wavelet transform;singular value decomposition;statistics;dimensionality reduction	ML	-4.083266349733378	-41.61658020817227	52728
7817d7f27e8b0abd4ac4d9b60dccf124cec82a7a	hybrid ensemble approach for classification	unsupervised learning;cluster algorithm;learning algorithm;classifier ensemble;neural networks;knowledge extraction;data fusion;ensembles;performance analysis;hybrid system;classifiers;medical data classification;classification accuracy;data classification;breast cancer;hybrid systems;neural network	This paper presents a novel hybrid ensemble approach for classification in medical databases. The proposed approach is formulated to cluster extracted features from medical databases into soft clusters using unsupervised learning strategies and fuse the decisions using parallel data fusion techniques. The idea is to observe associations in the features and fuse the decisions made by learning algorithms to find the strong clusters which can make impact on overall classification accuracy. The novel techniques such as parallel neural-based strong clusters fusion and parallel neural network based data fusion are proposed that allow integration of various clustering algorithms for hybrid ensemble approach. The proposed approach has been implemented and evaluated on the benchmark databases such as Digital Database for Screening Mammograms, Wisconsin Breast Cancer, and Pima Indian Diabetics. A comparative performance analysis of the proposed approach with other existing approaches for knowledge extraction and classification is presented. The experimental results demonstrate the effectiveness of the proposed approach in terms of improved classification accuracy on benchmark medical databases.	algorithm;artificial neural network;benchmark (computing);cluster analysis;computer cluster;data mining;data visualization;database;feature selection;k-means clustering;machine learning;memory-level parallelism;organizing (structure);performance;problem domain;qualitative comparative analysis;radial basis function;self-organization;self-organizing map;unsupervised learning	Brijesh Verma;Syed Zahid Hassan	2009	Applied Intelligence	10.1007/s10489-009-0194-7	computer science;machine learning;pattern recognition;data mining;artificial neural network;hybrid system	AI	6.179457167790417	-41.10193131352917	52758
e49047419f28db97b8b59b3f5e61ce8fb4b35e57	rmcl-esa: a novel method to detect co-regulatory functional modules in cancer		Considering the increasingly large scale of gene expression data, common module identification algorithms exist many problems, such as large search space and long running time. A novel co-regulatory modules identification algorithm RMCL-ESA (Regularized Markov Cluster u0026 Explosion Search Algorithm) based on improved Markov cluster and explosion search strategy has been proposed. Improved Markov cluster is adapted to preprocess gene expression profiles through three subprocedure: expansion, inflation, prune, which filter redundant genes and save computational cost. Then, two-stage explosion search strategy has been explored for identifying co-regulatory modules. Comparing with existing methods on breast cancer and ovary cancer datasets from TCGA, CRMs (Co-regulatory Functional Modules) of RMCL-ESA include more significant biological function GO-terms and regulation pathways with high enrichment score.	esa;phased array	Jiawei Luo;Ying Yin	2018		10.1007/978-3-319-95933-7_93	artificial intelligence;machine learning;computer science;cancer;subprocedure;markov chain;search algorithm	NLP	8.846615801950575	-50.90596226909141	52808
fad0aee0aa0846c71c8383d11eacf4e7255b282c	an agglomerate chameleon algorithm based on the tissue-like p system		The Chameleon algorithm plays an important role in data mining and data analysis. Membrane computing, as a new kind of parallel biological computing model, can reduce the time complexity and improve the computational efficiency. In this study, an agglomerate Chameleon algorithm is proposed which generates the sub-clusters by the K-medoids algorithm method. Then, the agglomerate Chameleon algorithm based on the Tissue-like P system is constructed with all the rules being created. The time complexity of the proposed algorithm is decreased from (O(K*(n-K)^{2}*C_n^K)_{}^{}) to (O(n*C_n^K)_{}^{}) through the parallelism of the P system. Experimental results show that the proposed algorithm has low error rate and is appropriate for big cluster analysis. The proposed algorithm in this study is a new attempt in applications of membrane system and it provides a novel perspective of cluster analysis.	algorithm;p system	Yuzhen Zhao;Xiyu Liu;Wenping Wang	2015		10.1007/978-3-662-49014-3_63	mathematical optimization;word error rate;time complexity;computer science;cluster analysis;p system;algorithm;membrane computing	EDA	-2.0333536160348524	-40.86815615205413	53020
f8c167c6c1049986c49933fdcea44e1472a1a6e7	the discrete dynamics of developmental systems	organisms;biology computing;genomics;evolutionary computation;probability density function;basin of attraction;data mining;network analysis;discrete dynamic networks;trajectory;organisms genomics bioinformatics dna cells biology buildings information resources evolution biology information science data mining;developmental systems;developmental systems discrete dynamic networks;dynamic networks;dynamic properties;bioinformatics	Operation of developmental systems is in many ways similar to that of discrete dynamic networks. Applying such network analysis to developmental system enables investigation of the dynamic properties of development at different levels. In this work the basins of attraction of a developmental system is explored in order to gain information about the details from the interwoven nature of the development of structure and behaviour. The investigation show how such method of analysis can offer insight about the workings of developmental systems.		Gunnar Tufte	2009	2009 IEEE Congress on Evolutionary Computation	10.1109/CEC.2009.4983215	organism;probability density function;genomics;network analysis;computer science;bioinformatics;artificial intelligence;trajectory;theoretical computer science;evolutionary computation	Robotics	-4.341806366542461	-49.209177559593684	53092
a131067a5dcf8705aa913ad47b1c34c6142444dc	unification of some least squares clustering methods	k means;clustering method;least square;local search;neural network	We give a unifying description of several least squares clustering methods. We consider the K-means/Lloyd’s algorithm, various local search based methods and clustering techniques based on neural networks. The relations between the various algorithms are explained. Also some computational results of these algorithms are given. Mathematics Subject Classifications (2000): 62H30, 68T05-20, 90C59, 91C20, 92B20.	algorithm;artificial neural network;cluster analysis;han unification;k-means clustering;least squares;local search (optimization)	Huub M. M. ten Eikelder;A. A. van Erk	2004	J. Math. Model. Algorithms	10.1023/B:JMMA.0000036581.87064.ed	correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;local search;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering;least squares;biclustering;artificial neural network;k-means clustering;clustering high-dimensional data	ML	3.2138727858386558	-41.29894455460583	53239
139bac89ad38df9e6d6b12b22dd8c0ca285ab91d	gene-finding as an attribute selection task	dna;biology computing;attribute selection;efficient algorithm;gene finding;data mining;attribute selection scheme;genetics;dna data mining bioinformatics attribute selection scheme pattern classification genetics microarray technology;cancer data mining databases genetics bioinformatics machine learning dna filters diseases malignant tumors;pattern classification;microarray technology;data consistency;pattern classification biology computing data mining dna genetics;bioinformatics	"""For data miners, bioinformatics pose a most demanding challenge than only creating efficient algorithms. They should work with databases that are more """"horizontal"""" than """"vertical"""", as the data consist of a few samples of a large (sometimes huge) number of attributes in the case of micro-arrays. More important is the fact that there is a priori biological knowledge saying that only a few genes are normally linked to each characteristic exhibited by the individual. It allows one to use Attribute Selection to determine which attributes are more likely to induce the observable characteristic. In this paper a study on many configurations of attribute selection schemes is made on two typical bioinformatics datasets. The results show that sequential subset generation guarantees better results and reiterates the use of the wrapper approach to achieve better classification, despite its running time being larger than the filter approach."""	algorithm;bioinformatics;data mining;database;gene prediction;observable;time complexity;wrapper library	Helyane Bronoski Borges;Júlio C. Nievola	2007	6th IEEE/ACIS International Conference on Computer and Information Science (ICIS 2007)	10.1109/ICIS.2007.104	computer science;bioinformatics;pattern recognition;data mining	DB	8.267908489521838	-47.96949532204726	53275
7ecdf7d2431f621ae3678b866296238eacb690fa	a comparative study on parallel lda algorithms in mapreduce framework		Although several parallel latent Dirichlet allocation (LDA) algorithms have been implemented to extract topic features from large-scale text data sets, very few studies compare their performance in real-world industrial applications. In this paper, we build a novel multi-channel MapReduce framework to compare fairly three representative parallel LDA algorithms such as parallel variational Bayes (PVB), parallel Gibbs sampling (PGS) and parallel belief propagation (PBP). Experimental results confirm that PGS yields the best application performance in search engine and online advertising system of Tencent, one of the biggest Internet companies in China, while PBP has the highest topic modeling accuracy. Moreover, PGS is more scalable in MapReduce framework than PVB and PBP because of its low memory usage and efficient sampling technique.		Yang Gao;Zhenlong Sun;Yi Wang;Xiaosheng Liu;Jianfeng Yan;Jia Zeng	2015		10.1007/978-3-319-18032-8_53	machine learning;pattern recognition;data mining	ML	-3.473227795885289	-40.31776344567044	53375
62d458264a50fe7a49a491f6ef8c660e3cc25150	atom environment kernels on molecules		The measurement of molecular similarity is an essential part of various machine learning tasks in chemical informatics. Graph kernels provide good similarity measures between molecules. Conventional graph kernels are based on counting common subgraphs of specific types in the molecular graphs. This approach has two primary limitations: (i) only exact subgraph matching is considered in the counting operation, and (ii) most of the subgraphs will be less relevant to a given task. In order to address the above-mentioned limitations, we propose a new graph kernel as an extension of the subtree kernel initially proposed by Ramon and Gärtner (2003). The proposed kernel tolerates an inexact match between subgraphs by allowing matching between atoms with similar local environments. In addition, the proposed kernel provides a method to assign an importance weight to each subgraph according to the relevance to the task, which is predetermined by a statistical test. These extensions are evaluated for classification and regression tasks of predicting a wide range of pharmaceutical properties from molecular structures, with promising results.		Hiroshi Yamashita;Tomoyuki Higuchi;Ryo Yoshida	2014	Journal of chemical information and modeling	10.1021/ci400403w	atomic physics	ML	1.950573210954916	-48.4603245795333	53421
a50ecb9ed4f376eac69c456cf0d31089b435ec28	k-means properties on six clustering benchmark datasets	clustering algorithms;clustering quality;k-means;benchmark	This paper has two contributions. First, we introduce a clustering basic benchmark. Second, we study the performance of k-means using this benchmark. Specifically, we measure how the performance depends on four factors: (1) overlap of clusters, (2) number of clusters, (3) dimensionality, and (4) unbalance of cluster sizes. The results show that overlap is critical, and that k-means starts to work effectively when the overlap reaches 4% level.	algorithm;angular defect;benchmark (computing);cluster analysis;iteration;k-means clustering;loss function;optimization problem;sampling (signal processing);speaker recognition	Pasi Fränti;Sami Sieranoja	2018	Applied Intelligence	10.1007/s10489-018-1238-7	artificial intelligence;computer science;pattern recognition;machine learning;cluster (physics);cluster analysis;k-means clustering;curse of dimensionality	ML	1.190430919790431	-41.65080076762222	53486
f3d1d242e5f98f63b1959feb289cb1ab6044b4f4	combining clustering techniques and formal concept analysis to characterize interestingness measures	k means;knowledge extraction;interestingness measure;data analysis methods;association rule;clustering method;quality measures;formal concept analysis	"""Formal Concept Analysis """"FCA"""" is a data analysis method which enables to discover hidden knowledge existing in data. A kind of hidden knowledge extracted from data is association rules. Different quality measures were reported in the literature to extract only relevant association rules. Given a dataset, the choice of a good quality measure remains a challenging task for a user. Given a quality measures evaluation matrix according to semantic properties, this paper describes how FCA can highlight quality measures with similar behavior in order to help the user during his choice. The aim of this article is the discovery of Interestingness Measures """"IM"""" clusters, able to validate those found due to the hierarchical and partitioning clustering methods (AHC and kmeans). Then, based on the theoretical study of sixty one interestingness measures according to nineteen properties, proposed in a recent study, FCA describes several groups of measures."""	association rule learning;cluster analysis;computer cluster;flickr;formal concept analysis;k-means clustering;scalability	Dhouha Grissa;Sylvie Guillaume;Engelbert Mephu Nguifo	2010	CoRR		association rule learning;computer science;formal concept analysis;data science;pattern recognition;data mining;mathematics;knowledge extraction;data analysis;k-means clustering	AI	0.6177216424296021	-41.76880129339801	53504
aacd401b23aa4759f9720a96b8511dcd22c19594	pessimistic multi-granulation rough set-based classification for heart valve disease diagnosis	rough set theory;heart valves;heart disease diagnosis;cardiovascular disease;pmgrs;feature selection;heart valve data;data classification;pessimistic multi granulation rough sets	The primary contribution of this study relies on proposing a new method, which can detect heart diseases in respective heart valve data. In this work, supervised quick reduct feature selection algorithm is applied for selecting important features from heart valve data. The classification method is applied only for relevant features selected using supervised quick reduct from heart valve data. In this paper, a new classification approach based on pessimistic multi-granulation rough sets (PMGRS) is applied for heart valve disease diagnosis. In multi-granulation rough sets, set approximations are well-defined by multiple equivalence relations on the universe, leading to an effective model for classification. This is confirmed by experimental evaluation, which shows excellent classification performance and also demonstrates that the proposed approach is superior to other benchmark classification algorithms including naive Bayes, multi-layer perceptron (MLP), and J48 and decision table classifiers.	rough set	Ahmad Taher Azar;S. Senthil Kumar;H. Hannah Inbarani;Aboul Ella Hassanien	2016	IJMIC	10.1504/IJMIC.2016.077744	rough set;computer science;machine learning;pattern recognition;data mining;mathematics;feature selection	ML	9.752943652479516	-38.823547337793194	53520
2dfa55e6711e690984b77c9b563dc39d035e8d5f	an exact solution for finding minimum recombinant haplotype configurations on pedigrees with missing data by integer linear programming	pedigree analysis;integer linear programming;branch and bound algorithm;haplotyping;recombination;missing data imputation	We study the problem of reconstructing haplotype configurations from genotypes on pedigree data with missing alleles under the Mendelian law of inheritance and the minimum recombination principle, which is important for the construction of haplotype maps and genetic linkage/association analysis. Our previous results show that the problem of finding a minimum-recombinant haplotype configuration (MRHC) is in general NP-hard. The existing algorithms for MRHC either are heuristic in nature and cannot guarantee optimality, or only work under some restrictions (on e.g. the size and structure of the input pedigree, the number of marker loci, the number of recombinants in the pedigree, etc.). In addition, most of them cannot handle data with missing alleles and, for those that do consider missing data, they usually do not perform well in terms of minimizing the number of recombinants when a significant fraction of alleles are missing. In this paper, we develop an effective integer linear programming (ILP) formulation of the MRHC problem with missing data and a branch-and-bound strategy that utilizes a partial order relationship (and some other special relationships) among variables to decide the branching order. The partial order relationship is discovered in the preprocessing of constraints by considering unique properties in our ILP formulation. A directed graph is built based on the variables and their partial order relationship. By identifying and collapsing the strongly connected components in the graph, we may greatly reduce the size of an ILP instance. Non-trivial (lower and upper) bounds on the optimal number of recombinants are introduced at each branching node to effectively prune the search tree. When multiple solutions exist, a best haplotype configuration is selected based on a maximum likelihood approach. Our results on simulated data show that the algorithm could recover haplotypes with 50 loci from a pedigree of size 29 in seconds on a standard PC. Its accuracy is more than 99.8% for data with no missing alleles and 98.3% for data with 20% missing alleles in terms of correctly recovered phase information at each marker locus. As an application of our algorithm to real data, we present some test results on reconstructing haplotypes from a genome-scale SNP data set consisting of 12 pedigrees that have 0.8% to 14.5% missing alleles.	algorithm;branch and bound;directed graph;heuristic;integer programming;international hapmap project;locus;linear programming;linkage (software);map;missing data;preprocessor;recombinant dna;search tree;strongly connected component	Jing Li;Tao Jiang	2004		10.1145/974614.974618	biology;mathematical optimization;integer programming;haplotype;bioinformatics;mathematics;genetics;branch and bound;recombination;statistics	Comp.	1.1296241699777758	-51.29466788791823	53797
88f2752aa929a54209f0c54b44af9477da58811e	intelligent hierarchical structure of classifiers to assess static security of power system		In this paper, a new method is presented, to assess the security of the power system. In this method, an intelligent hierarchical structure for classifiers is used, which requires fewer calculation efforts in comparison with direct methods. Therefore, it is suitable for real time applications. Also, the correlations among different scenarios of the power system are considered. Therefore, the results are more realistic. The proposed method is implemented on IEEE 39-Bus New England and IEEE 300-Bus networks and the results show the superiority of the proposed method over other ones, to assess the system static security.		M. Gholami;Gevork B. Gharehpetian;Mokhtar Mohammadi	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-151566	real-time computing;simulation;computer science;machine learning;data mining	HPC	7.1572832896559575	-38.907454798518444	54249
22b94d7d99c3110e75ae66aef38d8517026ebd68	mining statistically significant co-location and segregation patterns	co location;systems;segregation;information technology and systems;spatial data;statistically significant pattern;spatial interaction;data mining;database management;spatial databases information technology and systems database management database applications data mining systems;statistical testing bayes methods data mining pattern classification statistical distributions;data distribution feature interaction statistically significant colocation pattern mining statistically significant segregation pattern mining user specified thresholds random distribution interaction patterns statistical test feature null distribution computational cost naive approach;spatial databases;database applications;computational modeling indexes runtime atmospheric measurements particle measurements data mining data models	In spatial domains, interaction between features gives rise to two types of interaction patterns: co-location and segregation patterns. Existing approaches to finding co-location patterns have several shortcomings: (1) They depend on user specified thresholds for prevalence measures; (2) they do not take spatial auto-correlation into account; and (3) they may report co-locations even if the features are randomly distributed. Segregation patterns have yet to receive much attention. In this paper, we propose a method for finding both types of interaction patterns, based on a statistical test. We introduce a new definition of co-location and segregation pattern, we propose a model for the null distribution of features so spatial auto-correlation is taken into account, and we design an algorithm for finding both co-location and segregation patterns. We also develop two strategies to reduce the computational cost compared to a naïve approach based on simulations of the data distribution, and we propose an approach to reduce the runtime of our algorithm even further by using an approximation of the neighborhood of features. We evaluate our method empirically using synthetic and real data sets and demonstrate its advantages over a state-of-the-art co-location mining algorithm.	algorithm;algorithmic efficiency;analysis of algorithms;approximation;autocorrelation;computation;computational complexity theory;ecology;pattern language;randomness;sampling (signal processing);simulation;space-filling curve;spatial analysis;speedup;synthetic data;synthetic intelligence	Sajib Barua;Jörg Sander	2014	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2013.88	computer science;data science;pattern recognition;data mining;database;system;spatial analysis	ML	-2.7372953222284067	-43.32019637312208	54361
f00839fe752a5c527a6bc3e92296f2234608d5f2	possibilistic clustering with seeds		Clustering methods assign objects to clusters using only as prior information the characteristics of the objects. However, clustering algorithms performance can be improved when background knowledge is available. Such background knowledge can be incorporated in a clustering method as label constraints which results in a semi-supervised clustering algorithm. We propose to extend two possibilistic clustering algorithms to make use of available a priori information. The goal is twofold: to improve the accuracy of the clustering result by leading the method towards a desired solution and to detect outliers by taking advantage of the generated possibilistic partition. The proposed methods are called semi-supervised repulsive possibilistic c-means (SRPCM) and semi-supervised possibilistic fuzzy c-means (SPFCM). They correspond to possibilistic clustering algorithms that introduce label constraints. Experimental results show that the proposed algorithms using label constraints improve (1) the clustering result and (2) the outliers detection.	active learning (machine learning);algorithm;cluster analysis;experiment;fuzzy clustering;fuzzy cognitive map;k-means clustering;mathematical model;partition type;seeds (cellular automaton);semi-supervised learning;semiconductor industry	Violaine Antoine;Jose A. Guerrero;Ling-zhi Zeng;Gerardo Romero-Galván	2018	2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2018.8491655	machine learning;outlier;partition (number theory);fuzzy logic;artificial intelligence;computer science;cluster analysis;linear programming	Robotics	2.3664500371090407	-41.94005052285695	54445
c599c5cbd0e9de52f74e7d8dcf6d38eaa546c3d4	a novel many-objective clustering algorithm in mobile ad hoc networks	mobile ad hoc networks;clustering;objective tree;sub-objectives;weight factors	In mobile ad hoc networks, clustering refers to the process of identifying the set of clusterheads that optimize one or more network objectives. To optimize each objective, the nodes of the network should be evaluated and compared in terms of one or more corresponding attributes. In many-objective problems, as the number of favorable network objectives increases, the number of assessment attributes can increase significantly. Based on such attributes, two clusterhead selection approaches have been proposed: weight-based and dominance-based methods. In the weight-based methods, the large number of attributes in the weight equation reduces the accuracy of the weight factors. In dominance-based methods, the large number of attributes in the comparison process enlarges the Pareto set and reduces the convergence speed. In this paper, we propose an approach that decomposes the main objectives into intermediate sub-objectives in a hierarchical manner. Common sub-objectives can then be estimated based on the measurable node attributes. We combine these sub-objectives, rather than the raw attributes, in the weight equation. By exploiting this approach, we reduce five different objectives to just two sub-objectives for use in our proposed clustering algorithm. The results indicate that the proposed clustering algorithm is considerably more efficient than the well-known weighted clustering algorithm and its fast version, in terms of network objectives.	algorithm;cluster analysis	Reza Assareh;Masoud Sabaei;Ahmad Khademzadeh;Midia Reshadi	2017	Wireless Personal Communications	10.1007/s11277-017-4653-x	computer science;correlation clustering;mobile ad hoc network;hierarchical clustering of networks;pareto principle;cluster analysis;machine learning;measure (mathematics);canopy clustering algorithm;artificial intelligence;convergence (routing)	Mobile	5.152976859368557	-41.41294394202169	54459
49cf2aba6eea20ab86ecdb29482b432bbce14e35	read mapping on de bruijn graph	assembly;de bruijn graph;genomics;hamiltonian path;ngs;np-complete;read mapping;sequence graph;path	Mapping reads on references is a central task in numerous genomic studies. Since references are mainly extracted from assembly graphs, it is of high interest to map efficiently on such structures. In this work we formally define this problem and provide a first theoretical and practical study toward this direction when the graph is a de Bruijn Graph. We show that the problem is NP-Complete and we propose simple and efficient heuristics with a prototype implementation called BGREAT that handle real world instances of the problem with moderate resources. It achieves to map millions reads per CPU hour on a complex human de Bruijn graph. BGREAT availability: github.com/Malfoy/BGREAT	central processing unit;de bruijn graph;heuristic (computer science);karp's 21 np-complete problems;prototype	Antoine Limasset;Pierre Peterlongo	2015	CoRR			Comp.	0.0657459313936107	-50.7562780913462	54520
829edc8c2ca3d1365ce5c21f203531235a446979	a genetic algorithm for motif finding based on statistical significance		Understanding of transcriptional regulation through the dis- covery of transcription factor binding sites (TFBS) is a fundamental problem in molecular biology research. Here we propose a new com- putational method for motif discovery by mixing a genetic algorithm structure with several statistical coefficients. The algorithm was tested with 56 data sets from four different species. The motifs obtained were compared to the known motifs for each one of the data sets, and the accuracy in this prediction compared to 14 other methods both at nu- cleotide and site level. The results, though did not stand out in detection of false positives, showed a remarkable performance in most of the cases in sensitivity and in overall performance at site level, generally outper- forming the other methods in these statistics, and suggesting that the algorithm can be a useful tool to successfully predict motifs in different kinds of sets of DNA sequences.	genetic algorithm;motif	Josep Basha Gutierrez;Martin C. Frith;Kenta Nakai	2015		10.1007/978-3-319-16483-0_43	biology;bioinformatics;machine learning;data mining	EDA	7.275941507181911	-49.1718813821029	54605
4a14bf7a2d56b1e7fadd7f6cc97e2dbdf9466efc	an efficient fuzzy kohonen clustering network algorithm	unsupervised learning;clustering analysis;fuzzy classification;pattern clustering;learning rate;clustering algorithms iterative algorithms convergence fuzzy systems partitioning algorithms parallel processing educational institutions information science knowledge engineering mobile communication;convergence;fuzzy classification fuzzy kohonen clustering network learning algorithm clustering analysis iterative procedure fuzzy convergence operator unsupervised learning;convergence of numerical methods;fkcn fcm kcn;fkcn;iterative procedure;convergence rate;fuzzy set theory;mathematical operators;iterative methods;error analysis;cluster analysis;heuristic algorithms;networked learning;classification algorithms;network algorithm;pattern classification;fuzzy convergence operator;clustering algorithms;error rate;self organization;iris;fcm;unsupervised learning convergence of numerical methods fuzzy set theory iterative methods mathematical operators pattern classification pattern clustering;kcn;algorithm design and analysis;fuzzy kohonen clustering network learning algorithm	Fuzzy Kohonen clustering networks (FKCN) are well known for clustering analysis (unsupervised learning and self-organizing). This classification of FKCN algorithm is a set of iterative procedures that suffer some major problems, for example its constringency rate is not too fast for a large amount of datasets. To overcome these defects, an efficient fuzzy Kohonen network algorithm is proposed in this paper, which can significantly reduce the computation time required to partition a dataset into desired clusters. By introducing the threshold values and fuzzy convergence operators in the network learning procedure to adjust the learning rates dynamically, the network convergence rate is greatly improved and the error rates of dataset cluster are significantly decreased. Experimental results show the new algorithm is on average three times faster than the original FKCN algorithm. We also demonstrate that the quality of the improved FKCN is better than the original FKCN algorithm.	cluster analysis;computation;dijkstra's algorithm;iterative method;network convergence;organizing (structure);rate of convergence;self-organization;self-organizing map;teuvo kohonen;time complexity;unsupervised learning	Yanqing Yang;Zhenhong Jia;Chun Chang;Xizhong Qin;Tao Li;Hao Wang;Junkai Zhao	2008	2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2008.91	unsupervised learning;computer science;machine learning;pattern recognition;data mining;cluster analysis	ML	4.116479887038723	-38.99435631732856	54620
5c4bbd1a0549a4cec5177ee8939851e35f8f8fe9	data clustering based on hybrid k-harmonic means and modifier imperialist competitive algorithm	modified imperialist competitive algorithm;k means;data clustering;k harmonic means	Clustering is a process for partitioning datasets. Clustering is one of the most commonly used techniques in data mining and is very useful for optimum solution. K-means is one of the simplest and the most popular methods that is based on square error criterion. This algorithm depends on initial states and is easily trapped and converges to local optima. Some recent researches show that K-means algorithm has been successfully applied to combinatorial optimization problems for clustering. K-harmonic means clustering solves the problem of initialization using a built-in boosting function, but it is suffering from running into local optima. In this article, we purpose a novel method that is based on combining two algorithms; K-harmonic means and modifier imperialist competitive algorithm. It is named ICAKHM. To carry out this experiment, four real datasets have been employed whose results indicate that ICAKHM. Four real datasets are employed to measure the proposed method include Iris, Wine, Glass and Contraceptive Method Choice with small, medium and large dimensions. The experimented results show that the new method (ICAKHM) carries out better results than the efficiency of KHM, PSOKHM, GSOKHM and ICAKM methods.	benchmark (computing);boosting (machine learning);canonical account;cluster analysis;combinatorial optimization;data mining;data point;genetic algorithm;hybrid algorithm;imperialist competitive algorithm;independent computing architecture;k-means clustering;local optimum;mathematical optimization;modifier key;scheduling (computing);velocity (software development)	Marjan Abdeyazdan	2013	The Journal of Supercomputing	10.1007/s11227-013-1053-1	correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;computer science;artificial intelligence;canopy clustering algorithm;machine learning;cure data clustering algorithm;cluster analysis;imperialist competitive algorithm;algorithm;k-means clustering;clustering high-dimensional data	ML	4.036695202997428	-41.59884673329824	54893
5df4feb1d50f1988cbf2c773d4a710de5a031cb4	on weighting clustering	minimisation;optimisation sous contrainte;unsupervised learning;constrained optimization;cluster algorithm;iterative unsupervised learning;pattern clustering;fuzzy c mean;fuzzy k hbox rm means;fuzzy k means;iterative algorithms;boosting algorithms;complete log likelihoods;bregman divergences;algorithme k moyenne;algorithms artificial intelligence cluster analysis computer simulation information storage and retrieval models statistical pattern recognition automated;analisis forma;k means;bregman divergence;logique floue;supervised classification;k harmonic means clustering algorithm;khbox rm means;harmonic mean;logica difusa;apprentissage non supervise;minimization methods;constrained minimization;indexing terms;divergence bregman;fuzzy set theory;fuzzy sets;fuzzy logic;harmonic means clustering clustering bregman divergences k hbox rm means fuzzy k hbox rm means expectation maximization;optimizacion con restriccion;harmonic means clustering;boosting;supervised classification methods;expectation maximization;clustering;displays;pattern classification;unsupervised learning expectation maximisation algorithm fuzzy set theory minimisation pattern classification pattern clustering;algorithme em;clustering algorithms;algoritmo k media;k means clustering algorithm;k means algorithm;data reweighting;algoritmo em;pattern analysis;data reweighting weighting clustering iterative unsupervised learning instance points supervised classification methods boosting algorithms constrained minimization bregman divergence weight modifications complete log likelihoods k means clustering algorithm fuzzy c means clustering algorithm expectation maximization clustering algorithm k harmonic means clustering algorithm;expectation maximization clustering algorithm;weight modifications;clustering algorithms boosting iterative algorithms unsupervised learning minimization methods displays design methodology algorithm design and analysis tail taylor series;classification moyenne harmonique	Recent papers and patents in iterative unsupervised learning have emphasized a new trend in clustering. It basically consists of penalizing solutions via weights on the instance points, somehow making clustering move toward the hardest points to cluster. The motivations come principally from an analogy with powerful supervised classification methods known as boosting algorithms. However, interest in this analogy has so far been mainly borne out from experimental studies only. This paper is, to the best of our knowledge, the first attempt at its formalization. More precisely, we handle clustering as a constrained minimization of a Bregman divergence. Weight modifications rely on the local variations of the expected complete log-likelihoods. Theoretical results show benefits resembling those of boosting algorithms and bring modified (weighted) versions of clustering algorithms such as k-means, fuzzy c-means, expectation maximization (EM), and k-harmonic means. Experiments are provided for all these algorithms, with a readily available code. They display the advantages that subtle data reweighting may bring to clustering	boosting (machine learning);bregman divergence;cluster analysis;computation;expectation–maximization algorithm;experiment;fuzzy clustering;generic drugs;iterative method;k-means clustering;legal patent;motivation;paper;performance;solutions;supervised learning;unsupervised learning;variational principle;version;weight;benefit;statistical cluster	Richard Nock;Frank Nielsen	2006	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2006.168	unsupervised learning;correlation clustering;constrained clustering;mathematical optimization;constrained optimization;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;mathematics;cluster analysis;biclustering;statistics;k-means clustering;clustering high-dimensional data;conceptual clustering	ML	3.6320669371480605	-39.92275432614207	54939
c158d51c5aa1f40084f4f5b27ff8e029979ee2b0	adaptive fuzzy c-means clustering algorithm for interval data type based on interval-dividing technique	interval dividing;adaptive fuzzy c-means clustering algorithm;interval data type;comprehensive evaluation	Clustering for symbolic data type is a necessary process in many scientific disciplines, and the fuzzy c-means clustering for interval data type (IFCM) is one of the most popular algorithms. This paper presents an adaptive fuzzy c-means clustering algorithm for interval-valued data based on interval-dividing technique. This method gives a fuzzy partition and a prototype for each fuzzy cluster by optimizing an objective function. And the adaptive distance between the pattern and its cluster center varies with each algorithm iteration and may be either different from one cluster to another or the same for all clusters. The novel part of this approach is that it takes into account every point in both intervals when computing the distance between the cluster and its representative. Experiments are conducted on synthetic data sets and a real data set. To compare the comprehensive performance of the proposed method with other four existing methods, the corrected rand index, the value of objective function and iterations are introduced as the evaluation criterion. Clustering results demonstrate that the algorithm proposed in this paper has remarkable advantages.	algorithm;call of duty: black ops;cluster analysis;computer cluster;experiment;iteration;loss function;optimization problem;prototype;rand index;synthetic data	Chaozheng Bao;Hong Peng;Di He;Junning Wang	2017	Pattern Analysis and Applications	10.1007/s10044-017-0663-2	artificial intelligence;adaptive neuro fuzzy inference system;correlation clustering;cluster analysis;rand index;mathematics;pattern recognition;k-medians clustering;algorithm;canopy clustering algorithm;cure data clustering algorithm;fuzzy classification	DB	2.4156953727390427	-39.638928999333245	54989
cf75caffbe8926e681d04f8bdf53a1edefac03d8	nonlinear dimensionality reduction of gene expression data for visualization and clustering analysis of cancer tissue samples	clustering analysis;cluster algorithm;comparative analysis;expression pattern;expression profile;euclidean distance;gene expression data;small samples;feature space;gene expression;cancer tissue;visualization;cluster analysis;nonlinear connection;nonlinear dimensionality reduction;principal component analysis;feature selection;computer analysis;gene function;high dimension;environmental factor	Gene expression data are the representation of nonlinear interactions among genes and environmental factors. Computing analysis of these data is expected to gain knowledge of gene functions and disease mechanisms. Clustering is a classical exploratory technique of discovering similar expression patterns and function modules. However, gene expression data are usually of high dimensions and relatively small samples, which results in the main difficulty for the application of clustering algorithms. Principal component analysis (PCA) is usually used to reduce the data dimensions for further clustering analysis. While PCA estimates the similarity between expression profiles based on the Euclidean distance, which cannot reveal the nonlinear connections between genes. This paper uses nonlinear dimensionality reduction (NDR) as a preprocessing strategy for feature selection and visualization, and then applies clustering algorithms to the reduced feature spaces. In order to estimate the effectiveness of NDR for capturing biologically relevant structures, the comparative analysis between NDR and PCA is exploited to five real cancer expression datasets. Results show that NDR can perform better than PCA in visualization and clustering analysis of complex gene expression data.		Jinlong Shi;Zhigang Luo	2010	Computers in biology and medicine	10.1016/j.compbiomed.2010.06.007	correlation clustering;fuzzy clustering;computer science;bioinformatics;machine learning;data mining;mathematics;cluster analysis;feature selection;clustering high-dimensional data	ML	6.837203244254016	-48.48504727012049	55059
de1dfff5388efb297f951976b02ae6f525247619	detection of encrypted data based on support vector data description	kernel;nist;support vector machines;support vector data description detection of encrypted data nist sp800 22 standard;training;support vector data description;nist sp800 22 standard;cryptography;feature extraction;support vector machines cryptography nonparametric statistics;detection of encrypted data;svdd technique support vector data description data encryption encrypted data detection svdd algorithm nonparametric approach;data models;cryptography support vector machines training kernel feature extraction data models nist	Data encryption has been widely used. It is important to detect encrypted data. We present a method for detection of encrypted data based on the Support Vector Data Description (SVDD) algorithm. The SVDD is a single class, non-parametric approach for modeling the support of a distribution. We apply the SVDD techniques for detection of encrypted data. Experimental results show that the SVDD can be adopted as an effective tool for detection of encrypted data.	algorithm;categorization;effective method;encryption;feature selection;randomness tests	Juan Meng;Yuhuan Zhou;Zhisong Pan	2013	2013 International Conference on Advanced Cloud and Big Data	10.1109/CBD.2013.17	computer science;theoretical computer science;pattern recognition;data mining	SE	7.5193110486927495	-39.339219268041695	55265
0cf3f249d5bea851a5031aa565b25cb1fafa73d9	dictionary learning: analysis of spatial gene expression data and local identifiability theory	local identifiability;gene networks;stability;sparse decomposition;berkeley bin yu wu;nonnegative matrix factorization;siqi;statistics;dictionary learning;bioinformatics dictionary learning analysis of spatial gene expression data and local identifiability theory university of california;bioinformatics	Spatial gene expression data enable the detection of local covariability and are extremely useful for identifying local gene interactions during normal development. The abundance of spatial expression data in recent years has led to the modeling and analysis of regulatory networks. The inherent complexity of such data makes it a challenge to extract biological information. In the first part of the thesis, we developed staNMF, a method that combines a dictionary learning algorithm called nonnegative matrix factorization (NMF), with a new stability-driven criterion to select the number of dictionary atoms. When applied to a set of {\em Drosophila} early embryonic spatial gene expression images, one of the largest datasets of its kind, staNMF identified a dictionary with 21 atoms, which we call {\em principal patterns} (PP). Providing a compact yet biologically interpretable representation of {\em Drosophila} expression patterns, PP are comparable to a fate map generated experimentally by laser ablation and show exceptional promise as a data-driven alternative to manual annotations. Our analysis mapped genes to cell-fate programs and assigned putative biological roles to uncharacterized genes. Furthermore, we used the PP to generate local transcription factor (TF) regulatory networks. Spatially local correlation networks (SLCN) were constructed for six PP that span along the embryonic anterior-posterior axis. Using a two-tail 5\% cut-off on correlation, we reproduced 10 of the 11 links in the well-studied gap gene network. The performance of PP with the {\em Drosophila} data suggests that staNMF provides informative decompositions and constitutes a useful computational lens through which to extract biological insight from complex and often noisy gene expression data.The biological interpretability of the NMF-derived dictionary motivated us to understand why dictionary learning works analytically. In particular, if the observed data are generated from a ground truth dictionary, under what conditions can dictionary learning recovers the true dictionary? In the second part of the thesis, we studied the local correctness, or {\em local identifiability}, of a particular dictionary learning formulation with the $l_1$-norm objective function. Suppose we observe $N$ data points $\x_i\in \mathbb R^K$ for $i=1,...,N$, where $\x_i$'s are $i.i.d.$ random linear combinations of the $K$ columns from a square and invertible dictionary $\D_0 \in \mathbb R^{K\times K}$. We assumed that the random linear coefficients are generated from either the $s$-sparse Gaussian model or the Bernoulli-Gaussian model. For the population case, we established a sufficient and almost necessary condition for $\D_0$ to be locally identifiable, i.e., a local minimum of the expected $l_1$-norm objective function. Our condition covers both sparse and dense cases of the random linear coefficients and significantly improves the sufficient condition in Gribonval and Schnass (2010). Moreover, we demonstrated that for a complete $\mu$-coherent reference dictionary, i.e., a dictionary with absolute pairwise column inner-product at most $\coh\in[0,1)$, local identifiability holds even when the random linear coefficient vector has up to $O(\mu^{-2})$ nonzeros on average. Finally, it was shown that our local identifiability results translate to the finite sample case with high probability provided $N = O(K\log K)$.	dictionary;machine learning	Siqi Wu	2016			k-svd;computer science;machine learning;data mining;statistics	ML	5.570034036708838	-50.759810810696024	55312
f1abb0963eea600d9953ae2bfeb9a60387ba25b2	computational intelligence in medicine and biology	computational intelligence	Applying Computational Intelligence (CI) approaches, such as fuzzy computing, neural computing, learning/classification machines, advanced clustering methods, intelligent adaptive systems, genetic and evolutionary computing, is getting increasingly popular in medical and biological applications when there is need to support the development of intelligent behavior in complex and changing environments. In order to explore the volume and the evolution of CI-related journal publications in the medical domain and the popularity of the various CI methodologies, we conducted a small scale investigation using Medline, Scirus and BibFinder for the period 2001–2006. The purpose was not to exhaustively search the domain but to establish a prime guide for the interested reader, which may help to forecast the future developments of CI technologies in medicine and biology.	adaptive system;artificial neural network;cluster analysis;computational intelligence;evolutionary computation;fuzzy logic;medline;scirus	George D. Magoulas;Georgios Dounias	2007	Applied Intelligence	10.1007/s10489-007-0067-x	computational biology;artificial architecture;computer science;artificial intelligence	AI	4.505372064458956	-46.70542059997184	55362
2bfa1b7b4d2766d99deec24cd37cb090fcec9197	stream data clustering based on grid density and attraction	dynamic change;cluster algorithm;density based algorithms;data stream;real time;stream data;time window;real time data;data mining;data clustering;clustering;k means algorithm;high speed	Clustering real-time stream data is an important and challenging problem. Existing algorithms such as CluStream are based on the k-means algorithm. These clustering algorithms have difficulties finding clusters of arbitrary shapes and handling outliers. Further, they require the knowledge of k and user-specified time window. To address these issues, this article proposes D-Stream, a framework for clustering stream data using a density-based approach.  Our algorithm uses an online component that maps each input data record into a grid and an offline component that computes the grid density and clusters the grids based on the density. The algorithm adopts a density decaying technique to capture the dynamic changes of a data stream and a attraction-based mechanism to accurately generate cluster boundaries.  Exploiting the intricate relationships among the decay factor, attraction, data density, and cluster structure, our algorithm can efficiently and effectively generate and adjust the clusters in real time. Further, a theoretically sound technique is developed to detect and remove sporadic grids mapped by outliers in order to dramatically improve the space and time efficiency of the system. The technique makes high-speed data stream clustering feasible without degrading the clustering quality. The experimental results show that our algorithm has superior quality and efficiency, can find clusters of arbitrary shapes, and can accurately recognize the evolving behaviors of real-time data streams.	algorithm;areal density (computer storage);cluster analysis;data stream clustering;geodesic grid;k-means clustering;online and offline;real-time clock;real-time data;row (database)	Li Tu;Yixin Chen	2009	TKDD	10.1145/1552303.1552305	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;theoretical computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;cluster analysis;dbscan;affinity propagation;clustering high-dimensional data	ML	-1.7806644399206453	-38.50311159865728	55698
be073c7c9c13f90a86d63a9196a926b63a6a7e22	fusion of dimensionality reduction methods: a case study in microarray classification	classifier fusion;nearest neighbor searches;biology computing;k nearest neighbor classifier;dimensionality reduction methods;technology;training;computer and information science;colon;data mining;principal component analysis fusion power generation nearest neighbor searches testing high performance computing robustness informatics data analysis medical treatment euclidean distance;teknik;microarray classification;accuracy;dimensionality reduction;principal component analysis;microarray gene expression data sets;high dimensional data;pattern classification;tumors;sensor fusion biology computing pattern classification;k nearest neighbor;microarrays nearest neighbor classification dimensionality reduction feature fusion classifier fusion;feature fusion;microarray gene expression data sets dimensionality reduction methods microarray classification k nearest neighbor classifier feature fusion classifier fusion;sensor fusion;data och informationsvetenskap;dimensional reduction;nearest neighbor classification;microarrays	Dimensionality reduction has been demonstrated to improve the performance of the k-nearest neighbor (kNN) classifier for high-dimensional data sets, such as microarrays. However, the effectiveness of different dimensionality reduction methods varies, and it has been shown that no single method constantly outperforms the others. In contrast to using a single method, two approaches to fusing the result of applying dimensionality reduction methods are investigated: feature fusion and classifier fusion. It is shown that by fusing the output of multiple dimensionality reduction techniques, either by fusing the reduced features or by fusing the output of the resulting classifiers, both higher accuracy and higher robustness towards the choice of number of dimensions is obtained.	dimensionality reduction;k-nearest neighbors algorithm;microarray	Sampath Deegalla;Henrik Boström	2009	2009 12th International Conference on Information Fusion		computer science;machine learning;pattern recognition;data mining;dimensionality reduction	Robotics	9.177197604108267	-48.55588129059843	55731
48ee7caa3590c3dec44322fe3d30aa321107b6b8	sqlem: fast clustering in sql using the em algorithm	task performance;database;large data sets;data mining;archive;scalable;very large database;data analysis;internet;data mining application;high dimensional data;number of clusters;astronomy;em algorithm	Clustering is one of the most important tasks performed in Data Mining applications. This paper presents an efficient SQL implementation of the EM algorithm to perform clustering in very large databases. Our version can effectively handle high dimensional data, a high number of clusters and more importantly, a very large number of data records. We present three strategies to implement EM in SQL: horizontal, vertical and a hybrid one. We expect this work to be useful for data mining programmers and users who want to cluster large data sets inside a relational DBMS.	cluster analysis;computer cluster;data mining;expectation–maximization algorithm;programmer;relational database management system;sql	Carlos Ordonez;Paul Cereghini	2000		10.1145/342009.335468	data stream clustering;scalability;the internet;expectation–maximization algorithm;computer science;data science;database model;cure data clustering algorithm;data mining;database;data stream mining;cluster analysis;data analysis;clustering high-dimensional data;very large database	ML	-2.1002946671328058	-39.30007615331551	55905
f82f123c05bc91c334e544bc24921bec6f1bfd35	clustering approaches for dealing with multiple dna microarray datasets	gene expression data;integration analysis;particle swarm optimization;supervised clustering;consensus clustering;formal concept analysis;partitioning algorithms	This paper centres on clustering approaches that deal with multiple DNA microarray datasets. Four clustering algorithms for deriving a clustering solution from multiple gene expression matrices studying the same biological phenomenon are considered: two unsupervised cluster techniques based on information integration, a hybrid consensus clustering method combining Particle Swarm Optimization and k-means that can be referred to supervised clustering, and a supervised consensus clustering algorithm enhanced by Formal Concept Analysis (FCA), which initially produces a list of different clustering solutions, one per onsensus clustering ormal Concept Analysis	algorithm;cluster analysis;consensus clustering;dna microarray;formal concept analysis;k-means clustering;particle swarm optimization	Veselka Boeva	2014	J. Comput. Science	10.1016/j.jocs.2013.05.003	correlation clustering;mathematical optimization;fuzzy clustering;flame clustering;computer science;bioinformatics;formal concept analysis;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;particle swarm optimization;biclustering;affinity propagation;clustering high-dimensional data;conceptual clustering	AI	3.0269249644033085	-42.38182246506216	56068
14ffd9ab8f6efae12c8564dff569cab717369d30	particle swarm and bayesian networks applied to attribute selection for protein functional classification	particle swarm;bayesian network;protein function;attribute selection;naive bayes;qa 76 software;data mining;computer programming;naive bayes classifier;prediction accuracy;bioinformatics;bayesian networks	The Discrete Particle Swarm (DPSO) algorithm is an optimizationmethod that belongs to the fertile paradigm of Swarm Intelligence. The DPSO was designed for the task of attribute selection and it deals with discrete variables in a straightforward manner. This work extends the DPSO algorithm in two ways. First, we enable the DPSO to select attributes for a Bayesian network algorithm, which is a much more sophisticated algorithm than the Naive Bayes classifier previously used by this algorithm. Second, we apply the DPSO to a challenging protein functional classification data set, involving a large number of classes to be predicted. The performance of the DPSO is compared to the performance of a Binary PSO on the task of selecting attributes in this challenging data set. The criteria used for comparison are: (1) maximizing predictive accuracy; and (2) finding the smallest subset of attributes.	algorithm;bayesian network;naive bayes classifier;particle swarm optimization;programming paradigm;statistical classification;swarm intelligence	Elon Santos Correa;Alex Alves Freitas;Colin G. Johnson	2007		10.1145/1274000.1274081	naive bayes classifier;computer science;machine learning;pattern recognition;bayesian network;data mining;feature selection	ML	9.015459221866223	-46.62230663863589	56179
e1b57d673305287667abf1871a63b9d2392a84cc	intrusion detection algorithms based on correlation information entropy and binary particle swarm optimization		In current intrusion detection, redundant features often lead to the degradation of detection accuracy. Aiming at this problem, an intrusion detection algorithm based on correlation information entropy and binary particle swarm optimization algorithm was proposed. Correlation information entropy was used to sort features. This can filter irrelevant features. So the feature dimension was reduced. Then some better subsets that were gotten from feature sorting were used as the part initial population. In this way, the following particle swarm optimization algorithm would have a good starting point. The test results showed that the better classification performance was obtained according to the selected optimal feature subset, and the testing time of the system was reduced effectively.	algorithm;elegant degradation;entropy (information theory);genetic algorithm;intrusion detection system;mathematical optimization;overhead (computing);particle swarm optimization;relevance;sorting	Yanfei Wang;Peiyu Liu;Min Ren;Xiao-Xue Chen	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393229	entropy (information theory);support vector machine;feature extraction;feature dimension;intrusion detection system;population;computer science;algorithm;sorting;particle swarm optimization	Robotics	9.98577194819685	-42.636235129754624	56230
ba010e232dc3b22eb25860857cb0eb6805091454	detecting pattern-based outliers	high density;regular spacing;complete spatial randomness;outlier detection;clustering;low density	Outlier detection targets those exceptional data that deviate from the general pattern. Besides high density clustering, there is another pattern called low density regularity. Thus, there are two types of outliers w.r.t. them. We propose two techniques: one to identify the two patterns and the other to detect the corresponding outliers.	sensor	Tianming Hu;Sam Yuan Sung	2003	Pattern Recognition Letters	10.1016/S0167-8655(03)00165-X	anomaly detection;computer science;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;complete spatial randomness;statistics	Vision	-0.635852211694912	-41.38588776225949	56337
0a0aba7dc60812847102009573343958e8609c24	dual clustering: integrating data clustering over optimization and constraint domains	optimisation sous contrainte;extraction information;constrained optimization;cluster algorithm;complejidad espacio;pattern clustering;optimisation;interlaced clustering classification algorithm dual clustering constraint domain spatial data clustering nongeometric attribute optimization domain;analyse amas;classification algorithm;algorithm complexity;spatial reasoning;partition donnee;constraint optimization;iterative algorithms;analisis datos;information extraction;time complexity;complejidad algoritmo;data partition;scattering;indexing terms;satisfiability;data mining;pattern clustering data mining spatial reasoning visual databases optimisation computational complexity;constraint satisfaction;dual clustering index terms data mining data clustering;similitude;spatial database;constraint domain;optimizacion con restriccion;objective function;data clustering;satisfaction contrainte;data analysis;complexite temps;cluster analysis;complexite algorithme;fouille donnee;computational complexity;base donnee spatiale;optimization domain;classification algorithms;similarity;space complexity;pattern recognition;constraint optimization clustering algorithms iterative algorithms scattering pattern analysis partitioning algorithms classification algorithms algorithm design and analysis data mining pattern recognition;clustering algorithms;analyse donnee;spatial data structures;analisis cluster;base dato especial;pattern analysis;interlaced clustering classification algorithm;index terms data mining;satisfaccion restriccion;similitud;complexite espace;dual clustering;spatial data clustering;complejidad tiempo;spatial clustering;similarity measure;busca dato;extraccion informacion;algorithm design and analysis;particion dato;partitioning algorithms;nongeometric attribute;visual databases;structure donnee spatiale	Spatial clustering has attracted a lot of research attention due to its various applications. In most conventional clustering problems, the similarity measurement mainly takes the geometric attributes into consideration. However, in many real applications, the nongeometric attributes are what users are concerned about. In the conventional spatial clustering, the input data set is partitioned into several compact regions and data points which are similar to one another in their nongeometric attributes may be scattered over different regions, thus making the corresponding objective difficult to achieve. To remedy this, we propose and explore in this paper a new clustering problem on two domains, called dual clustering, where one domain refers to the optimization domain and the other refers to the constraint domain. Attributes on the optimization domain are those involved in the optimization of the objective function, while those on the constraint domain specify the application dependent constraints. Our goal is to optimize the objective function in the optimization domain while satisfying the constraint specified in the constraint domain. We devise an efficient and effective algorithm, named Interlaced Clustering-Classification, abbreviated as ICC, to solve this problem. The proposed ICC algorithm combines the information in both domains and iteratively performs a clustering algorithm on the optimization domain and also a classification algorithm on the constraint domain to reach the target clustering effectively. The time and space complexities of the ICC algorithm are formally analyzed. Several experiments are conducted to provide the insights into the dual clustering problem and the proposed algorithm.	algorithm;authorization;cluster analysis;computer cluster;dspace;data point;duality (optimization);emoticon;experiment;ieee xplore;interlaced video;loss function;mathematical optimization;optimization problem	Cheng-Ru Lin;Ken-Hao Liu;Ming-Syan Chen	2005	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2005.75	correlation clustering;constrained clustering;constrained optimization;data stream clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;brown clustering;dbscan;algorithm;clustering high-dimensional data	DB	0.19635167095861908	-39.66609924150852	56355
5ee75717dee57fc1ee695dbed6c06b2f451b499d	distributed gaussian mixture model summarization using the mapreduce framework	distributed density based clustering;gaussian mixture model;distributed cluster summarization;mapreduce	With an accelerating rate of data generation, sophisticated techniques are essential to meet scalability requirements. One of the promising avenues for handling large datasets is distributed storage and processing. Further, data summarization is a useful concept for managing large datasets, wherein a subset of the data can be used to provide an approximate yet useful representation. Consolidation of these tools can allow a distributed implementation of data summarization. In this paper, we achieve this by proposing and implementing a distributed Gaussian Mixture Model Summarization using the MapReduce framework MR-SGMM. In MR-SGMM, we partition input data, cluster the data within each partition with a density-based clustering algorithm called DBSCAN, and for all clusters we discover SGMM core points and their features. We test the implementation with synthetic and real datasets to demonstrate its validity and efficiency. This paves the way for a scalable implementation of Summarization using Gaussian Mixture Model SGMM.		Arina Esmaeilpour;Elnaz Bigdeli;Fatemeh Cheraghchi;Bijan Raahemi;Behrouz Homayoun Far	2016		10.1007/978-3-319-34111-8_39	computer science;data science;automatic summarization;machine learning;mixture model;data mining;database	ML	-3.0273329795654607	-39.34690457508438	56377
a04eeafbc238214298ff1df320d3af1ccb45c3ef	p2p traffic identification using support vector machine and cuckoo search algorithm combined with particle swarm optimization algorithm		As peer-to-peer (P2P) technology booms lots of problems arise such as rampant piracy, congestion, low quality etc. Thus, accurate identification of P2P traffic makes great sense for efficient network management. As one of the optimal classifiers, support vector machine (SVM) has been successfully used in P2P traffic identification. However, the performance of SVM is largely dependent on its parameters and the traditional tuning methods are inefficient. In the paper, a novel hybrid method to optimize parameters of SVM based on cuckoo search algorithm combined with particle swarm optimization algorithm is proposed. The first stage of the proposed approach is to tune the best parameters for SVM with training data. Subsequently, the SVM configured with the best parameters is employed to identify P2P traffic. In the end, we demonstrate the effectiveness of our approach on-campus traffic traces. Experimental results indicate that the proposed method outperforms SVM based on genetic algorithm, particle swarm optimization algorithm and cuckoo search algorithm.	cuckoo search;particle swarm optimization;search algorithm;support vector machine	Zhiwei Ye;Mingwei Wang;Chunzhi Wang;Hui Xu	2014		10.1007/978-3-662-46826-5_10	mathematical optimization;multi-swarm optimization;derivative-free optimization;machine learning;metaheuristic	AI	6.1884132232941385	-38.606185259899455	56805
396a159dcd40f00d57e80bac413e82ca560e1df2	bioinspired applications in artificial and natural computation, third international work-conference on the interplay between natural and artificial computation, iwinac 2009, santiago de compostela, spain, june 22-26, 2009, proceedings, part ii	algorithm analysis;natural computing;computer vision;artificial intelligent;problem complexity;pattern recognition;computational biology	Measurements over the Aquiles Tendon through Ecographic Images Processing.- A New Approach in Metal Artifact Reduction for CT 3D Reconstruction.- Genetic Approaches for the Automatic Division of Topological Active Volumes.- Object Discrimination by Infrared Image Processing.- Validation of Fuzzy Connectedness Segmentation for Jaw Tissues.- Breast Cancer Classification Applying Artificial Metaplasticity.- Ontology Based Approach to the Detection of Domestics Problems for Independent Senior People.- A Wireless Sensor Network for Assisted Living at Home of Elderly People.- An Ambient Assisted Living System for Telemedicine with Detection of Symptoms.- Applying Context-Aware Computing in Dependent Environments.- A Smart Solution for Elders in Ambient Assisted Living.- Convergence of Emergent Technologies for the Digital Home.- Results of an Adaboost Approach on Alzheimer's Disease Detection on MRI.- Analysis of Brain SPECT Images for the Diagnosis of Alzheimer Disease Using First and Second Order Moments.- Neurobiological Significance of Automatic Segmentation: Application to the Early Diagnosis of Alzheimer's Disease.- Support Vector Machines and Neural Networks for the Alzheimer's Disease Diagnosis Using PCA.- Functional Brain Image Classification Techniques for Early Alzheimer Disease Diagnosis.- Quality Checking of Medical Guidelines Using Interval Temporal Logics: A Case-Study.- Classification of SPECT Images Using Clustering Techniques Revisited.- Detection of Microcalcifications Using Coordinate Logic Filters and Artificial Neural Networks.- Rule Evolving System for Knee Lesion Prognosis from Medical Isokinetic Curves.- Denoising of Radiotherapy Portal Images Using Wavelets.- A Block-Based Human Model for Visual Surveillance.- Image Equilibrium: A Global Image Property for Human-Centered Image Analysis.- Vision-Based Text Segmentation System for Generic Display Units.- Blind Navigation along a Sinuous Path by Means of the See ColOr Interface.- Using Reconfigurable Supercomputers and C-to-Hardware Synthesis for CNN Emulation.- Access Control to Security Areas Based on Facial Classification.- Comparing Feature Point Tracking with Dense Flow Tracking for Facial Expression Recognition.- A Memory-Based Particle Filter for Visual Tracking through Occlusions.- Classification of Welding Defects in Radiographic Images Using an ANN with Modified Performance Function.- Texture Classification of the Entire Brodatz Database through an Orientational-Invariant Neural Architecture.- Eye-Hand Coordination for Reaching in Dorsal Stream Area V6A: Computational Lessons.- Toward an Integrated Visuomotor Representation of the Peripersonal Space.- Evidence for Peak-Shaped Gaze Fields in Area V6A: Implications for Sensorimotor Transformations in Reaching Tasks.- Segmenting Humans from Mobile Thermal Infrared Imagery.- My Sparring Partner Is a Humanoid Robot.- Brain-Robot Interface for Controlling a Remote Robot Arm.- Learning to Coordinate Multi-robot Competitive Systems by Stimuli Adaptation.- A Behavior Based Architecture with Auction-Based Task Assignment for Multi-robot Industrial Applications.- On the Control of a Multi-robot System for the Manipulation of an Elastic Hose.- An Improved Evolutionary Approach for Egomotion Estimation with a 3D TOF Camera.- A Frame for an Urban Traffic Control Architecture.- Partial Center of Area Method Used for Reactive Autonomous Robot Navigation.- Mathematical Foundations of the Center of Area Method for Robot Navigation.- Determining Sound Source Orientation from Source Directivity and Multi-microphone Recordings.- A Braitenberg Lizard: Continuous Phonotaxis with a Lizard Ear Model.- A New Metric for Supervised dFasArt Based on Size-Dependent Scatter Matrices That Enhances Maneuver Prediction in Road Vehicles.- A Strategy for Evolutionary Spanning Tree Construction within Constrained Graphs with Application to Electrical Networks.- An Evolutionary Approach for Correcting Random Amplified Polymorphism DNA Images.- A Method to Minimize Distributed PSO Algorithm Execution Time in Grid Computer Environment.- Assessment of a Speaker Recognition System Based on an Auditory Model and Neural Nets.- CIE-9-MC Code Classification with knn and SVM.- Time Estimation in Injection Molding Production for Automotive Industry Based on SVR and RBF.- Performance of High School Students in Learning Math: A Neural Network Approach.	computation;natural computing		2009		10.1007/978-3-642-02267-8	computer vision;computer science;artificial intelligence;machine learning	Robotics	-0.7641211392860405	-45.48153956640176	56861
2853e3942d239f05a2fd02c712d04d18aa89e66d	optimal evolutionary reconstructions with a parsimony graph	conceptual modeling;uml;web applications;evolutionary trees;linear time;web services;data structure;heuristic algorithm	The current approach to constructing evolutionary trees uses base sequences, such as DNA, and parsimony to determine the best tree. Most parsimony algorithms typically use heuristic algorithms, which try to search in an efficient way for one optimal tree, although there are an exponential number of labellings for the tree and more than one of these may be optimal. This paper presents an algorithm for building a unique data structure, in linear time, termed a parsimony graph that contains all parsimonious reconstructions of the sequences in the internal nodes of the tree. Using this parsimony graph, we can extract information, such as average parsimony distance on each edge, to help identify closely correlated DNA regions.	algorithm;computational phylogenetics;data structure;graph labeling;heuristic;maximum parsimony (phylogenetics);occam's razor;phylogenetic tree;time complexity	Treena J. Larrew	2006		10.1145/1185448.1185554	mathematical optimization;combinatorics;machine learning;tree rearrangement;mathematics;maximum parsimony	Theory	1.0000213801845272	-50.86057495165013	56885
8108b064a540ffbf42c832d2898e8a62c1d85bc7	towards conflict resolution in collaborative clustering	groupware;pattern clustering;collaborative work;iterative algorithms;data partitioning conflict resolution collaborative clustering;collaborative clustering;collaboration;collaboration collaborative work clustering algorithms clustering methods partitioning algorithms iterative algorithms genetic algorithms algorithm design and analysis navigation;genetics;data partitioning;iterative methods;indexes;navigation;clustering algorithms;genetic algorithms;optimization;clustering methods;conflict resolution;pattern clustering groupware;algorithm design and analysis;partitioning algorithms	In recent years, a lot of work has focused on the use of multiple clusterings for partitioning data. These approaches are supported by the existence of a huge number of clustering algorithms. Thus, different methods have been proposed to create alternative clustering results from the same data. However, the different clustering results are usually generated without sharing information and the user is often asked to select the final result. To cope with these issues, a new paradigm named collaborative clustering has been proposed recently. In collaborative clustering, different clustering methods work together (i.e. collaborate) to reach an agreement on the clustering of a common dataset. At the end of the collaboration, the results are expected to be strongly similar. In this paper, we address the problem of the collaboration of different clustering methods and we compare four collaboration strategies. Our experiments compare the different strategies on synthetic and real-life datasets and provide insight into the advantages and the drawbacks of each strategy.	cluster analysis;experiment;genetic algorithm;iterative method;knowledge integration;portal;programming paradigm;real life;synthetic intelligence;time complexity	Germain Forestier;Cédric Wemmert;Pierre Gançarski	2010	2010 5th IEEE International Conference Intelligent Systems	10.1109/IS.2010.5548343	constrained clustering;fuzzy clustering;computer science;data science;machine learning;consensus clustering;data mining;cluster analysis;brown clustering;clustering high-dimensional data;conceptual clustering	DB	1.999036283097563	-42.83655414225938	56929
9dea7e2993050870fcee110f2bf4b660e8765aa7	rough clustering utilizing the principle of indifference	overlapping clusters;rough k means;laplace s principle of indifference	Clustering is one of the most widely used method in data mining with applications in virtually any domain. Its main objective is to group similar objects into the same cluster, while dissimilar objects should belong to different clusters. In particular k-means clustering, as member of the partitioning clustering family, has obtained great popularity. The classic (hard) k-means assigns an object unambiguously to one and only one cluster. To address uncertainty soft clustering has been introduced using concepts like fuzziness, possibility or roughness. A decade ago Lingras and West introduced a k-means approach based on the interval interpretation of rough sets theory. In the past years their rough k-means has gained increasing attention. In our paper, we propose a refined rough k-means algorithm that utilizes Laplace’s principle of indifference to calculate the means. As we will discuss this provides a sounder justification for the impacts of the objects in the approximations in comparison to established rough k-means algorithms. Furthermore, the weighting in the mean function is based on individual objects rather than on aggregated sub-means. In experiments, we compare the refined algorithm to related approaches. 2014 Elsevier Inc. All rights reserved.	algorithm;approximation;cluster analysis;data mining;experiment;k-means clustering;rough set	Georg Peters	2014	Inf. Sci.	10.1016/j.ins.2014.02.073	correlation clustering;mathematical optimization;fuzzy clustering;flame clustering;artificial intelligence;mathematics;cluster analysis;algorithm;statistics	AI	2.2455879579607734	-38.585508066644586	56967
1d8c51b81851a4f6bffba1b407f8f3d2ea0a69ac	genetic programming for biomarker detection in mass spectrometry data	biomarker detection data set;gp method;biomarker detection;important feature;genetic programming;high-level feature;ms data;small number;naive bayes;mass spectrometry data;classification performance	"""Classification of mass spectrometry (MS) data is an essential step for biomarker detection which can help in diagnosis and prognosis of diseases. However, due to the high dimensionality and the small sample size, classification of MS data is very challenging. The process of biomarker detection can be referred to as feature selection and classification in terms of machine learning. Genetic programming (GP) has been widely used for classification and feature selection, but it has not been effectively applied to biomarker detection in the MS data. In this study we develop a GP based approach to feature selection, feature extraction and classification of mass spectrometry data for biomarker detection. In this approach, we firstly use GP to reduce the """"redundant"""" features by selecting a small number of important features and constructing high-level features, then we use GP to classify the data based on selected features and constructed features. This approach is examined and compared with three well known machine learning methods namely decision trees, naive Bayes and support vector machines on two biomarker detection data sets. The results show that the proposed GP method can effectively select a small number of important features from thousands of original features for these problems, the constructed high-level features can further improve the classification performance, and the GP method outperforms the three existing methods, namely naive Bayes, SVMs and J48, on these problems."""	genetic programming	Soha Ahmed;Mengjie Zhang;Lifeng Peng	2012		10.1007/978-3-642-35101-3_23	computer science;machine learning;pattern recognition;data mining	Theory	9.63121287928772	-46.75893331750457	57099
430464f149b84948361c3e904d55b9ba3715d6a0	combining pso and fcm for dynamic fuzzy clustering problems		This paper proposes a dynamic data clustering algorithm, called PSOFC, in which Particle Swarm Optimization (PSO) is combined with the fuzzy c-means (FCM) clustering method to find the number of clusters and cluster centers concurrently. Fuzzy c-means can be applied to data clustering problems but the number of clusters must be given in advance. This paper tries to overcome this shortcoming. In the evolu- tionary process of PSOFC, a discrete PSO is used to search for the best number of clusters. With a specified number of cluster, each particle employs FCM to refine cluster centers for data clustering. Thus PSOFC can automatically determine the best number of clusters during the data clustering process. Six datasets were used to evaluate the proposed algo- rithm. Experimental results demonstrated that PSOFC is an effective algorithm for solving dynamic fuzzy clustering problems.	fuzzy clustering;fuzzy cognitive map;particle swarm optimization	Yucheng Kao;Ming-Hsien Chen;Kai-Ming Hsieh	2014		10.1007/978-3-319-12970-9_1	correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;clustering high-dimensional data	Theory	3.426777116932087	-41.56753140510326	57156
1bb19364a8994ed45855d3d4b34c20661ceedc62	a granular computing approach to symbolic value partitioning	granular computing;symbolic value partition;attribute taxonomy tree;information entropy	Symbolic value partitioning is a knowledge reduction technique in the field of data min- ing. In this paper, we propose a granular computing approach for the partitioning task that includes granule construction and granule selection algorithms. The granule construction algorithm takes advantage of local information associated with each attribute. A binary attribute value taxonomy tree is built to merge these attribute values in a bottom-up manner using information-loss heuristics. The use of a balancing technique enables us to control different nodes in the same level to have approximately the same size. The granule selection algorithm uses global information about all of the attributes in the decision system. Hence, nodes across the taxonomy forest of all attributes are selected and expanded using information-gain heuristics. We present a series of experimental results that demonstrate the effectiveness of the proposed approach in terms of reducing the data size and improving the resulting classification accuracy.	granular computing	Liu-Ying Wen;Fan Min	2015	Fundam. Inform.	10.3233/FI-2015-1297	attribute domain;granular computing;computer science;artificial intelligence;theoretical computer science;machine learning;data mining;algorithm;entropy	HPC	-4.157185017595079	-38.386298592925186	57164
0ffa0155667fb6f6966d63bbb6489124e33e6885	statistical properties of transactional databases	multi agent system;cooperation;data mining;statistical properties;agent;statistical analysis;data mining algorithm;performance prediction;communication act	Most of the complexity of common data mining tasks is due to the unknown amount of information contained in the data being mined. The more patterns and corelations are contained in such data, the more resources are needed to extract them. This is confirmed by the fact that in general there is not a single best algorithm for a given data mining task on any possible kind of input dataset. Rather, in order to achieve good performances, strategies and optimizations have to be adopted according to the dataset specific characteristics. For example one typical distinction in transactional databases is between sparse and dense datasets. In this paper we consider Frequent Set Counting as a case study for data mining algorithms. We propose a statistical analysis of the properties of transactional datasets that allows for a characterization of the dataset complexity. We show how such characterization can be used in many fields, from performance prediction to optimization.	algorithm;data mining;database;global variable;mathematical optimization;mined;performance prediction;sparse matrix	Paolo Palmerini;Salvatore Orlando;Raffaele Perego	2004		10.1145/967900.968009	computer science;data science;machine learning;multi-agent system;data mining;database;world wide web;cooperation	ML	-1.5451625858922011	-43.82856268540473	57405
52add4ee987569e99ad1e067d6d1cb1c1d3e639c	asymptotic analysis of simd optimized dft based exon prediction algorithm for dna sequences	asymptotic analysis;dna sequence		algorithm;simd	Syed Shams-ul-Haq	2010			exon;asymptotic analysis;dna sequencing;simd;mathematics;bioinformatics	Comp.	-2.9137841438878667	-51.28231336238964	57442
2456b899c6f057ed45e1fec446d66c7cf7410509	parallel out-of-core algorithm for genome-scale enumeration of metabolic systemic pathways	organisms;genomics;mathematics;high dimensionality;metabolic network;search space;genomics bioinformatics biochemistry organisms laboratories us department of energy algorithm design and analysis computer science mathematics biotechnology;data representation;convex cone;parallel implementation;computer science;convex analysis;metabolic pathway;us department of energy;biotechnology;algorithm design and analysis;biochemistry;steady state;bioinformatics;out of core algorithms	Systemic pathways-oriented approaches to analysis of metabolic networks are effective for small networks but are computationally infeasible for genome scale networks. Current computational approaches to this analysis are based on the mathematical principles of convex analysis. The enumeration of a complete set of “ systemically independent” metabolic pathways is at the core of these approaches and it is computationally the most demanding component. An efficient parallel outof-core algorithm for generating a complete set of systemically independent metabolic pathways, termed “ extreme pathways” , is presented. These pathways represent the edges of a high-dimensional convex cone and can be used to derive any admissible steady-state flux distribution (or phenotype) for a specified metabolic genotype. The algorithm can be used for computing “ elementary flux modes” that are different but closely related to extreme pathways. The algorithm combines a bitmap data representation, search space reduction, and out-of-core implementation to improve CPU-time and memory requirements by several orders of magnitude. Augmented with a parallel implementation, it provides extremely scalable performance. No previous parallel and/or out-of-core algorithms for the enumeration of systemically defined metabolic pathways are known.	bitmap;central processing unit;computational complexity theory;convex analysis;convex cone;data (computing);flux qubit;out-of-core algorithm;requirement;scalability;steady state	Nagiza F. Samatova;Al Geist;George Ostrouchov;Anatoli V. Melechko	2002		10.1109/IPDPS.2002.1016588	convex analysis;organism;algorithm design;metabolic pathway;mathematical optimization;genomics;convex cone;computer science;bioinformatics;theoretical computer science;distributed computing;external data representation;steady state;algorithm;metabolic network;statistics	HPC	-2.3522766452604458	-51.11632528001815	57521
aef165539657034d0135f8d16d2522806b472e1e	a methodology for selecting the most suitable cluster validation internal indices	validation criteria;linear regression;cluster evaluation	Validation of clustering results is an important issue in the context of machine learning research and it is essential for the success of clustering applications. Choosing the appropriate validation index for evaluating the results of a particular clustering algorithm remains a challenge. The quality of partitions generated by different clustering algorithms can be evaluated using different indices based on external or internal criteria. In this paper, we have proposed a methodology for selecting the most suitable cluster validation internal index, relating external and internal criteria through a regression model applied on the results of partitioning clustering algorithm.	algorithm;cluster analysis;machine learning	Caroline Tomasini;Leonardo R. Emmendorfer;Eduardo Nunes Borges;Karina Scurupa Machado	2016		10.1145/2851613.2851885	correlation clustering;fuzzy clustering;computer science;linear regression;machine learning;data mining;cluster analysis;statistics	DB	1.4610719790573643	-39.71672205132289	57604
41ef14421df4c5f0817209a2ec63ebc84385fbc3	restricted dcj-indel model: sorting linear genomes with dcj and indels	genomics;translocation genetic;indel mutation;computational biology bioinformatics;models genetic;genome;chromosomes;algorithms;combinatorial libraries;dna breaks double stranded;computer appl in life sciences;mutation;microarrays;bioinformatics	The double-cut-and-join (DCJ) is a model that is able to efficiently sort a genome into another, generalizing the typical mutations (inversions, fusions, fissions, translocations) to which genomes are subject, but allowing the existence of circular chromosomes at the intermediate steps. In the general model many circular chromosomes can coexist in some intermediate step. However, when the compared genomes are linear, it is more plausible to use the so-called restricted DCJ model, in which we proceed the reincorporation of a circular chromosome immediately after its creation. These two consecutive DCJ operations, which create and reincorporate a circular chromosome, mimic a transposition or a block-interchange. When the compared genomes have the same content, it is known that the genomic distance for the restricted DCJ model is the same as the distance for the general model. If the genomes have unequal contents, in addition to DCJ it is necessary to consider indels, which are insertions and deletions of DNA segments. Linear time algorithms were proposed to compute the distance and to find a sorting scenario in a general, unrestricted DCJ-indel model that considers DCJ and indels. In the present work we consider the restricted DCJ-indel model for sorting linear genomes with unequal contents. We allow DCJ operations and indels with the following constraint: if a circular chromosome is created by a DCJ, it has to be reincorporated in the next step (no other DCJ or indel can be applied between the creation and the reincorporation of a circular chromosome). We then develop a sorting algorithm and give a tight upper bound for the restricted DCJ-indel distance. We have given a tight upper bound for the restricted DCJ-indel distance. The question whether this bound can be reduced so that both the general and the restricted DCJ-indel distances are equal remains open.	chromosomal translocation;chromosome inversion;chromosomes;clinical act of insertion;coexist (image);genome;hl7publishingsubsection <operations>;indel mutation;inversion (discrete mathematics);sorting algorithm;time complexity;contents - htmllinktype	Poly H. da Silva;Raphael Machado;Simone Dantas;Marília D. V. Braga	2012		10.1186/1471-2105-13-S19-S14	mutation;biology;genomics;dna microarray;bioinformatics;chromosome;genetics;genome	Theory	-0.24057369755276703	-51.76902688285158	58165
cc00587f14f043f6a73921678f5b0a71f7ab29ed	a fast parallel algorithm for finding the longest common sequence of multiple biosequences	sequence comparison;parallel algorithm;sequence homology;amino acid;growth and development;computational biology bioinformatics;conserved sequence;longest common subsequence;time factors;algorithms;sequence analysis;sequence alignment;combinatorial libraries;dna sequence;computer appl in life sciences;computing methodologies;microarrays;bioinformatics	Searching for the longest common sequence (LCS) of multiple biosequences is one of the most fundamental tasks in bioinformatics. In this paper, we present a parallel algorithm named FAST_LCS to speedup the computation for finding LCS. A fast parallel algorithm for LCS is presented. The algorithm first constructs a novel successor table to obtain all the identical pairs and their levels. It then obtains the LCS by tracing back from the identical character pairs at the last level. Effective pruning techniques are developed to significantly reduce the computational complexity. Experimental results on gene sequences in the tigr database show that our algorithm is optimal and much more efficient than other leading LCS algorithms. We have developed one of the fastest parallel LCS algorithms on an MPP parallel computing model. For two sequences X and Y with lengths n and m, respectively, the memory required is max{4*(n+1)+4*(m+1), L}, where L is the number of identical character pairs. The time complexity is O(L) for sequential execution, and O(|LCS(X, Y)|) for parallel execution, where |LCS(X, Y)| is the length of the LCS of X and Y. For n sequences X1, X2, ..., X n , the time complexity is O(L) for sequential execution, and O(|LCS(X1, X2, ..., X n )|) for parallel execution. Experimental results support our analysis by showing significant improvement of the proposed method over other leading LCS algorithms.	bioinformatics;computation (action);computational complexity theory;fastest;goodyear mpp;name;parallel algorithm;parallel computing;speedup;time complexity	Yixin Chen;Andrew Wan;Wei Liu	2006	BMC Bioinformatics	10.1186/1471-2105-7-S4-S4	biology;dna sequencing;amino acid;dna microarray;computer science;bioinformatics;theoretical computer science;sequence analysis;sequence alignment;longest common subsequence problem;parallel algorithm;conserved sequence;algorithm	HPC	-1.961782872646267	-52.03067831564882	58670
dfbcd6f16dc31fb1429aad515971561677fc96b1	finding local communities in protein networks	cluster algorithm;protein network;computational biology bioinformatics;graph partitioning;proteins;local community;random walk;protein protein interaction;algorithms;combinatorial libraries;computational biology;computer appl in life sciences;article;biological process;databases protein;microarrays;bioinformatics	Protein-protein interactions (PPIs) play fundamental roles in nearly all biological processes, and provide major insights into the inner workings of cells. A vast amount of PPI data for various organisms is available from BioGRID and other sources. The identification of communities in PPI networks is of great interest because they often reveal previously unknown functional ties between proteins. A large number of global clustering algorithms have been applied to protein networks, where the entire network is partitioned into clusters. Here we take a different approach by looking for local communities in PPI networks. We develop a tool, named Local Protein Community Finder, which quickly finds a community close to a queried protein in any network available from BioGRID or specified by the user. Our tool uses two new local clustering algorithms Nibble and PageRank-Nibble, which look for a good cluster among the most popular destinations of a short random walk from the queried vertex. The quality of a cluster is determined by proportion of outgoing edges, known as conductance, which is a relative measure particularly useful in undersampled networks. We show that the two local clustering algorithms find communities that not only form excellent clusters, but are also likely to be biologically relevant functional components. We compare the performance of Nibble and PageRank-Nibble to other popular and effective graph partitioning algorithms, and show that they find better clusters in the graph. Moreover, Nibble and PageRank-Nibble find communities that are more functionally coherent. The Local Protein Community Finder, accessible at http://xialab.bu.edu/resources/lpcf , allows the user to quickly find a high-quality community close to a queried protein in any network available from BioGRID or specified by the user. We show that the communities found by our tool form good clusters and are functionally coherent, making our application useful for biologists who wish to investigate functional modules that a particular protein is a part of.	anatomy, regional;apache axis;artificial neural network;awards;biogrid;biological processes;carder.su;cluster analysis;clustering coefficient;coherence (physics);community;computation;computational science;conductance (graph);degeneracy (graph theory);emoticon;experiment;graph - visual representation;graph partition;ibm notes;informatics (discipline);interaction;kinetic void;local algorithm;manuscripts;name;nephrogenic systemic fibrosis;network topology;nibble;optic axis of a crystal;pagerank;pixel density;proton pump inhibitors;relevance;sampling (signal processing);sampling - surgical action;semantic network;staphylococcal protein a;trichosanthis;undersampling;vertex;vision;citation;statistical cluster;wu zhu yu extract	Konstantin Voevodski;Shang-Hua Teng;Yu Xia	2009		10.1186/1471-2105-10-297	protein–protein interaction;biology;dna microarray;bioinformatics;graph partition;data science;machine learning;biological process;genetics;random walk	ML	0.7117711820629967	-49.40300515534007	59003
0c817d1b67face506be74425df8d9e62bb027058	an information retrieval perspective on visualization of gene expression data with ontological annotation	ontological annotation;dimensionality reduction methods;information retrieval;ontologies artificial intelligence genetic algorithms genetics information retrieval systems medical information systems;visualization dimensionality reduction gene ontology information retrieval structured annotation;information retrieval perspective;gene expression data;ontologies artificial intelligence;genetics;gene expression;data analysis;visualization;dimensionality reduction;vectors;information retrieval data visualization gene expression ontologies optimization methods vectors bioinformatics data analysis information analysis computer science;medical information systems;image color analysis;high dimensional data;data visualization;structured annotation;information retrieval systems;genetic algorithms;ontologies;computer science;dimensional reduction;information analysis;gene ontology information retrieval perspective gene expression ontological annotation high dimensional data data visualization dimensionality reduction methods;noise;optimization methods;bioinformatics;gene ontology	High-dimensional data are often visualized by dimensionality reduction methods whose goals are not directly related to visualization. We use a recent formalization of visualization as information retrieval and apply that formalism to data with structured annotations: we analyze gene expression data with annotations from the Gene Ontology (GO). We show that using the GO information in visualization yields better retrieval with respect to known ontological relationships and allows discovery of data properties not explained by the ontology.	dimensionality reduction;gene ontology;information retrieval;semantics (computer science)	Jaakko Peltonen;Helena Aidos;Nils Gehlenborg;Alvis Brazma;Samuel Kaski	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495665	information visualization;computer science;bioinformatics;data mining;data analysis;information retrieval;data visualization;statistics	Visualization	4.949515356329162	-48.78657129067262	59488
36dda951a8f95f48458d06d7200eb038c033c258	k-ccm: a center-based algorithm for clustering categorical data with missing values		This paper focuses on solving the problem of clustering for categorical data with missing values. Specifically, we design a new framework that can impute missing values and assign objects into appropriate clusters. For the imputation step, we use a decision tree-based method to fill in missing values. For the clustering step, we use a kernel density estimation approach to define cluster centers and an information theoretic-based dissimilarity measure to quantify the differences between objects. Then, we propose a center-based algorithm for clustering categorical data with missing values, namely k-CCM. An experimental evaluation was performed on real-life datasets with missing values to compare the performance of the proposed algorithm with other popular clustering algorithms in terms of clustering quality. Generally, the experimental result shows that the proposed algorithm has a comparative performance when compared to other algorithms for all datasets.	algorithm;categorical variable;missing data	Duy-Tai Dinh;Van-Nam Huynh	2018		10.1007/978-3-030-00202-2_22	imputation (statistics);machine learning;artificial intelligence;cluster analysis;missing data;categorical variable;decision tree;kernel density estimation;algorithm;computer science	ML	1.5633403879861305	-40.78303919478955	59725
4a7266aa81d94a8d76559dc76c44dd9f4c2d9f86	pggp: prototype generation via genetic programming	68t20;1nn classification;genetic programming;pattern classification;68t10;prototype generation	Prototype generation (PG) methods aim to find a subset of instances taken from a large training data set, in such a way that classification performance (commonly, using a 1NN classifier) when using prototypes is equal or better than that obtained when using the original training set. Several PG methods have been proposed so far, most of them consider a small subset of training instances as initial prototypes and modify them trying to maximize the classification performance on the whole training set. Although some of these methods have obtained acceptable results, training instances may be under-exploited, because most of the times they are only used to guide the search process. This paper introduces a PG method based on genetic programming in which many training samples are combined through arithmetic operators to build highly effective prototypes. The genetic program aims to generate prototypes that maximize an estimate of the generalization performance of an 1NN classifier. Experimental results are reported on benchmark data to assess PG methods. Several aspects of the genetic program are evaluated and compared to many alternative PG methods. The empirical assessment shows the effectiveness of the proposed approach outperforming most of the state of the art PG techniques when using both small and large data sets. Better results were obtained for data sets with numeric attributes only, although the performance of the proposed technique on mixed data was very competitive as well. © 2015 Elsevier B.V. All rights reserved.	genetic programming;prototype;test set;the times	Hugo Jair Escalante;Mario Graff;Alicia Morales-Reyes	2016	Appl. Soft Comput.	10.1016/j.asoc.2015.12.015	genetic programming;computer science;artificial intelligence;machine learning;data mining;algorithm	AI	10.010972798635041	-41.93385052092462	59726
0a896f8e40356ff899dff24b3c1d0e5e594ba6cb	cognitive consistency routing algorithm of capsule-network		Artificial Neural Networks (ANNs) are computational models inspired by the central nervous system (especially the brain) of animals and are used to estimate or generate unknown approximation functions that rely on a large amount of inputs. The Capsule Neural Network [1] is a novel structure of Convolutional Neural Networks(CNN) which simulates the visual processing system of human brain. In this paper, we introduce a psychological theory which is called Cognitive Consistency to optimize the routing algorithm of Capsnet to make it more close to the working pattern of human brain. Our experiments show that progress had been made compared with the baseline.	algorithm;approximation;artificial neural network;baseline (configuration management);computation;computational model;convolutional neural network;experiment;routing	Huayu Li	2018	CoRR		computer science;convolutional neural network;visual processing;artificial intelligence;machine learning;deep learning;artificial neural network;computational model;algorithm;cognition;human brain;psychological theory	AI	1.2247832767801305	-45.64847011576443	59728
e7e7c58ba4c36573d02a884fff9e83623de4053e	interval attributes description based fcm clustering algorithm for noisy data	fuzzy c means algorithm;cluster algorithm;pattern clustering data handling fuzzy set theory;pattern clustering;noisy data;interval attributes description;fuzzy set theory;possibilistic c means;clustering algorithms phase change materials noise generators prototypes partitioning algorithms data mining computer science pattern recognition fuzzy control;superposition cluster center interval attributes description noisy data fuzzy c means algorithm possibilistic c means;data handling;superposition cluster center	In allusion to the disadvantages that fuzzy c-means algorithm is sensitivity to noise and possibilistic c-means is easy to generate superposition cluster center, interval attributes description based FCM clustering algorithm is proposed in this paper. Firstly, an interval attributes description model of noisy data is presented. Then a clustering algorithm of interval attributes data based on two ends of interval number implemented by FCM. Finally, the simulations of practical data set are made by the algorithm of this paper and PCM. The results validated the feasibility and efficiencies of the algorithm proposed in this paper.	algorithm;cluster analysis;fuzzy cognitive map;signal-to-noise ratio	Shixiong Xia;Yue'e Li;Yong Zhou	2008		10.1109/WKDD.2008.96	correlation clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;group method of data handling;pattern recognition;cure data clustering algorithm;data mining;mathematics;fuzzy set;cluster analysis	ML	2.551415893281786	-39.15548096538413	59863
d0f8b1a5778cbc9982e15a71b59f681ff6445a6a	fast parallel construction of correlation similarity matrices for gene co-expression networks on multicore clusters		Abstract Gene co-expression networks are gaining attention in the present days as useful representations of biologically interesting interactions among genes. The most computationally demanding step to generate these networks is the construction of the correlation similarity matrix, as all pairwise combinations must be analyzed and complexity increases quadratically with the number of genes. In this paper we present MPICorMat, a hybrid MPI/OpenMP parallel approach to construct similarity matrices based on Pearson’s correlation. It is based on a previous tool (RMTGeneNet) that has been used on several biological studies and proved accurate. Our tool obtains the same results as RMTGeneNet but significantly reduces runtime on multicore clusters. For instance, MPICorMat generates the correlation matrix of a dataset with 61,170 genes and 160 samples in less than one minute using 16 nodes with two Intel Xeon Sandy-Bridge processors each (256 total cores), while the original tool needed almost 4.5 hours. The tool is also compared to another available approach to construct correlation matrices on multicore clusters, showing better scalability and performance. MPICorMat is an open-source software and it is publicly available at https://sourceforge.net/projects/mpicormat/ .	gene co-expression network;multi-core processor	Jorge Gonz&#x00E1;lez-Dom&#x00ED;nguez;María J. Martín	2017		10.1016/j.procs.2017.05.023	xeon;artificial intelligence;machine learning;parallel computing;covariance matrix;theoretical computer science;scalability;computer science;multi-core processor;matrix (mathematics);pairwise comparison;correlation;supercomputer	Theory	-2.169210924305069	-50.952153395461025	60009
5de0b965825cfc176f830942b534205169710eb9	the distance between randomly constructed genomes		The study of genome rearrangements has developed a sophisticated technology for inferring a minimizing sequence of operations necessary to transform one genome into another, where the genomes are represented by signed permutations on 1, · · · , n and the operations are modeled on the biological processes of inversion, reciprocal translocation, chromosome fusion and fission, transposition of chromosomal segments, excision and reintegration of circular chromosomal segments, among others. Once these inferences are made, however, there is a need for some way to statistically validate both the inferences and the assumptions of the evolutionary model. Our approach has been to see to what extent there is an signal remaining in the comparative structure of the two genomes, or whether evolution has largely scrambled the order of each one with respect to the other, in terms of the evolutionary model assumed. This has led to the study of completely scrambled, i.e., randomized, genomes as a null baseline for the detection of a evolutionary signal. Insofar as a pair of genomes retain some evidence of evolutionary relationship, this should be detectible by contrast to randomized genomes. In previous papers, we have worked out the statistical properties of random genomes consisting of one or more circular chromosomes, 1 and those of two random genomes containing the same number c of linear chromosomes.. The latter paper concentrated on showing that the number of circular chromosomes inevitably associated with random linear chromosomes is very small with realistic numbers of chromosomes. It only included a rough estimation of the statistical properties of the linear chromosomes. The present paper introduces a new way of representing the comparison of linear genomes, requiring only a single source/sink vertex in the breakpoint graph of the two genomes, instead of the numerous “chromosomal caps” used in other treatments. This facilitates a more rigorous treatment of the case of linear chromosomes, including the more realistic situation where the number of linear chromosomes may be different (χ1 and χ2)	baseline (configuration management);frequency capping;inversion (discrete mathematics);java caps;models of dna evolution;randomized algorithm;randomness;singlet fission	Wei Xu	2007			combinatorics;discrete mathematics;bioinformatics;mathematics	Theory	0.8241126574563309	-51.864545921539104	60490
fcc6652772c07a1e479fbeedf2cea3d89f6badcd	genetic algorithms for clustering and fuzzy clustering	fuzzy clustering;genetic algorithm	Clustering has been an area of intensive research for several decades because of its multifaceted applications in innumerable domains. Clustering can be either Boolean, where a single data point belongs to exactly one cluster, or fuzzy, where a single data point can have nonzero belongingness to more than one cluster. Traditionally, optimization of some well-defined objective function has been the standard approach in both clustering and fuzzy clustering. Hence, researchers have investigated the utility of evolutionary computing and related techniques in this regard. The different approaches differ in their choice of the objective function and/or the optimization strategy used. In particular, clustering using genetic algorithms (GAs) has attracted attention of researchers, and has been studied extensively. This paper presents a short review of some of different approaches of GA-based clustering methods. Two techniques, one with fixed number of clusters and another with a variable number of fuzzy clusters, are described along with some experimental results on numerical as well as image data sets. © 2011 John Wiley u0026 Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 524–531 DOI: 10.1002/widm.47	cluster analysis;fuzzy clustering;genetic algorithm	Sanghamitra Bandyopadhyay	2011	Wiley Interdiscip. Rev. Data Min. Knowl. Discov.	10.1002/widm.47	correlation clustering;constrained clustering;determining the number of clusters in a data set;fuzzy clustering;flame clustering;computer science;bioinformatics;artificial intelligence;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;brown clustering;dbscan;biclustering;affinity propagation;statistics;clustering high-dimensional data;conceptual clustering	ML	4.0337759226839545	-42.419502895760594	60542
7165ea5745a97dd9e51a2711362060c04cab0d88	a thresholded fuzzy c-means algorithm for semi-fuzzy clustering	fuzzy c means algorithm;multidimensional data;fuzzy clustering;clustering method;pattern recognition	In this paper, the problem of achieving 'semi-fuzzy ' or 'soft' clustering of multidimensional data is discussed.A technique base d on thresholding the results of the fuzzy c-means algorithm is introduced.The propo sed approach is analysed and contrasted with the soft clustering method (see S. Z. Selim and M. A. Ismail, Pattern Recognition 17, 559-568) showing the merits of the new method.Separation of clusters in the semi-fuzzy clustering context is in troduced and the use of the proposed technique to measure the degree of separation is ex plained.	algorithm;cluster analysis;fuzzy clustering;pattern recognition;sed;semiconductor industry;six degrees of separation;thresholding (image processing)	Mohamed S. Kamel;Shokri Z. Selim	1991	Pattern Recognition	10.1016/0031-3203(91)90002-M	correlation clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	Vision	2.1140754402884037	-40.50737869011691	60690
9fb33c890061cd67ce2f001ebf5d33c3d758a30d	bi-tour ant colony optimization for diagonal clustering	ant colony optimization;relational database system;clustered data;ant colony optimization algorithm	Clustering data represented by two-dimensional matrix can be achieved by re-arranging the permutation of rows and columns  into diagonal block structure. The non-overlapping fragments are then determined by identifying the blocks formed in the revised  matrix. In this paper, we propose using Bitour Ant Colony Optimization (BTACO) for diagonal clustering, which is a variant  from the Ant Colony Optimization algorithm. In the computational study, we show that BTACO can re-arrange the matrix into  diagonal block structure well for vertical partitioning in the relational database system.  	ant colony optimization algorithms	Kwan-Ho Woo;Chun Hung Cheng	2005		10.1007/3-540-32391-0_96	ant colony optimization algorithms;artificial bee colony algorithm	Theory	2.918337943963518	-41.61586138056949	60941
7a40f6271a1dad2c9e35a824023db2e0afced42f	frequent itemset mining in big data with effective single scan algorithms		This paper considers frequent itemsets mining in transactional databases. It introduces a new accurate single scan approach for frequent itemset mining (SSFIM), a heuristic as an alternative approach (EA-SSFIM), as well as a parallel implementation on Hadoop clusters (MR-SSFIM). EA-SSFIM and MR-SSFIM target sparse and big databases, respectively. The proposed approach (in all its variants) requires only one scan to extract the candidate itemsets, and it has the advantage to generate a fixed number of candidate itemsets independently from the value of the minimum support. This accelerates the scan process compared with existing approaches while dealing with sparse and big databases. Numerical results show that SSFIM outperforms the state-of-the-art FIM approaches while dealing with medium and large databases. Moreover, EA-SSFIM provides similar performance as SSFIM while considerably reducing the runtime for large databases. The results also reveal the superiority of MR-SSFIM compared with the existing HPC-based solutions for FIM using sparse and big databases.		Youcef Djenouri;Djamel Djenouri;Jerry Chun-Wei Lin;Asma Belhadi	2018	IEEE Access	10.1109/ACCESS.2018.2880275	big data;data mining;cluster analysis;distributed computing;computer science;heuristic	ML	-4.421131709269967	-39.32238144938171	60953
006a4e796121d3911c2222049a4430117411afc0	hierarchical document clustering using frequent itemsets	document clustering;hierarchical clustering;high dimensionality;association rule mining;frequent itemset	"""Most state-of-the art document clustering methods are modifications of traditional clustering algorithms that were originally designed for data tuples in relational or transactional database. However, they become impractical in real-world document clustering which requires special handling for high dimensionality, high volume, and ease of browsing. Furthermore, incorrect estimation of the number of clusters often yields poor clustering accuracy. In this thesis, we propose to use the notion of frequent itemsets, which comes from association rule mining, for document clustering. The intuition of our clustering criterion is that there exist some common words, called frequent itemsets, for each cluster. We use such words to cluster documents and a hierarchical topic tree is then constructed from the clusters. Since we are using frequent itemsets as a preliminary step, the dimension of each document is therefore, drastically reduced, which in turn increases efficiency and scalability. To my parents and A h a """"When we see persons of worth, we should think of equaling them; when we see persons of a contrary character, we should turn inwards and examine ourselves."""" CONFUCIUS"""	algorithm;association rule learning;cluster analysis;database transaction;existential quantification;organizing (structure);outline (list);requirement;scalability	Benjamin C. M. Fung;Ke Wang;Martin Ester	2003		10.1137/1.9781611972733.6	association rule learning;document clustering;computer science;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;brown clustering;biclustering;clustering high-dimensional data	ML	-0.713192654815665	-41.76847492476542	61024
139b2e0418891f3be0f68d1352a52d041a0e31c7	a game theoretical approach to the classification problem in gene expression data analysis	interaction index;cooperative game;cooperative game theory;shapley value;gene expression analysis;indexation;classification problem;gene expression data analysis;gene expression pattern;high power	Microarray technology allows for the evaluation of the level of expression of thousands of genes in a sample of cells under a given condition. In this paper, we introduce a methodology based on cooperative Game Theory for the selection of groups of genes with high power in classifying samples, according to gene expression patterns. The connection between microarray games and classification games is discussed and the use of the Shapley value to measure the power of genes for classification is motivated on particular instances and compared to the interaction index.	game theory	Vito Fragnelli;Stefano Moretti	2008	Computers & Mathematics with Applications	10.1016/j.camwa.2006.12.088	gene expression;bioinformatics;machine learning;data mining;mathematics;shapley value	ML	7.586448332704089	-48.53718109786093	61378
351173eb4f7feeb378e9ded8acd5cb65100434ac	clustering binary data streams with k-means	sufficient statistic;k means;time series;data mining;data streams;incremental learning;discretize;symbolic;k means algorithm;binary data;clustered data;sparse matrices	Clustering data streams is an interesting Data Mining problem. This article presents three variants of the K-means algorithm to cluster binary data streams. The variants include On-line K-means, Scalable K-means, and Incremental K-means, a proposed variant introduced that finds higher quality solutions in less time. Higher quality of solutions are obtained with a mean-based initialization and incremental learning. The speedup is achieved through a simplified set of sufficient statistics and operations with sparse matrices. A summary table of clusters is maintained on-line. The K-means variants are compared with respect to quality of results and speed. The proposed algorithms can be used to monitor transactions.	algorithm;binary data;cluster analysis;data mining;k-means clustering;online and offline;quality of results;scalability;sparse matrix;speedup	Carlos Ordonez	2003		10.1145/882082.882087	computer science;machine learning;pattern recognition;data mining;database;data stream mining;k-means clustering	ML	-2.633737435795613	-39.05795692838689	61435
d9ef1a14264df09d364f64a4465f5480f9cb18ca	biomarker discovery based on bbha and adaboostm1 on microarray data for cancer classification	cancer;colon;particle swarm optimization;classification algorithms;tumors;decision trees;algorithm design and analysis	In this paper, a new approach based on Binary Black Hole Algorithm (BBHA) and Adaptive Boosting version Ml (AdaboostM1) is proposed for finding genes that can classify the group of cancers correctly. In this approach, BBHA is used to perform gene selection and AdaboostM1 with 10-fold cross validation is adopted as the classifier. Also, to find the relation between the biomarkers for biological point of view, decision tree algorithm (C4.5) is utilized. The proposed approach is tested on three benchmark microarrays. The experimental results show that our proposed method can select the most informative gene subsets by reducing the dimension of the data set and improve classification accuracy as compared to several recent studies.	benchmark (computing);biological markers;black hole;c4.5 algorithm;cross reactions;decision tree;entity name part qualifier - adopted;information;list of algorithms;malignant neoplasms;microarray	Elnaz Pashaei;Mustafa Ozen;Nizamettin Aydin	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7591380	statistical classification;algorithm design;computer science;bioinformatics;machine learning;decision tree;data mining;particle swarm optimization;cancer	EDA	8.403128599763834	-47.200022170171366	61686
65a3b1f4b129f3bb81ec8013ddf9368882c7925b	an autonomous star identification algorithm based on one-dimensional vector pattern for star sensors	star sensor;biological patents;biomedical journals;star identification;text mining;europe pubmed central;citation search;citation networks;star pattern;research articles;abstracts;open access;life sciences;clinical guidelines;one dimensional vector pattern;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	In order to enhance the robustness and accelerate the recognition speed of star identification, an autonomous star identification algorithm for star sensors is proposed based on the one-dimensional vector pattern (one_DVP). In the proposed algorithm, the space geometry information of the observed stars is used to form the one-dimensional vector pattern of the observed star. The one-dimensional vector pattern of the same observed star remains unchanged when the stellar image rotates, so the problem of star identification is simplified as the comparison of the two feature vectors. The one-dimensional vector pattern is adopted to build the feature vector of the star pattern, which makes it possible to identify the observed stars robustly. The characteristics of the feature vector and the proposed search strategy for the matching pattern make it possible to achieve the recognition result as quickly as possible. The simulation results demonstrate that the proposed algorithm can effectively accelerate the star identification. Moreover, the recognition accuracy and robustness by the proposed algorithm are better than those by the pyramid algorithm, the modified grid algorithm, and the LPT algorithm. The theoretical analysis and experimental results show that the proposed algorithm outperforms the other three star identification algorithms.	abra gene;algorithm;arabic numeral 0;autonomous robot;computational complexity theory;entity name part qualifier - adopted;feature vector;gucy2c protein, human;matching;parallel port;pattern language;self-replicating machine;simulation;stellar (payment network);sensor (device)	Liyan Luo;Lu-ping Xu;Hua Zhang	2015		10.3390/s150716412	text mining;medical research;computer science;bioinformatics;data science;data mining	AI	4.098588562524586	-46.31798517911022	61817
fdd36e71bdfe4179bee7545f69da9e410d7ed8fb	data mining using concepts of independence, unimodality and homophily		With widespread use of information technologies, more and more complex data is generated and collected every day. Such complex data is various in structure, size, type and format, e.g. time series, texts, images, videos and graphs. Complex data is often high-dimensional and heterogeneous, which makes separation of wheat (knowledge) from chaff (noise) more difficult. Clustering is a main mode of knowledge discovery from complex data, which groups objects in such a way that intra-group objects are more similar than inter-group objects. Traditional clustering methods such as k-means, Expectation-Maximization clustering (EM), DBSCAN and spectral clustering are either deceived by the curse of dimensionality or spoiled by heterogenous information. So, how to effectively explore complex data? In some cases, people may only have some partial information about complex data. For example, in social networks, not every user provides his/her profile information such as personal interests. Can we leverage limited user information and friendship network wisely to infer likely labels of unlabeled users so that advertisers can do accurate advertising? This is problem of learning from labeled and unlabeled data, which is literarily attributed to semi-supervised classification. rnrnTo gain insights into these problems, this thesis focuses on developing clustering and semi-supervised classification methods that are driven by concepts of independence, unimodality and homophily. The proposed methods leverage techniques from diverse areas, such as statistics, information theory, graph theory, signal processing, optimization and machine learning. Specifically, this thesis develops four methods, i.e. FUSE, ISAAC, UNCut, and wvGN. FUSE and ISAAC are clustering techniques to discover statistically independent patterns from high-dimensional numerical data. UNCut is a clustering technique to discover unimodal clusters in attributed graphs in which not all attributes are relevant to graph structure. wvGN is a semi-supervised classification technique using theory of homophily to infer labels of unlabeled vertices in graphs. We have verified our clustering and semi-supervised classification methods on various synthetic and real-world data sets. The results are superior to those of state-of-the-art.		Wei Ye	2018			knowledge extraction;graph theory;homophily;spectral clustering;cluster analysis;curse of dimensionality;dbscan;artificial intelligence;complex data type;computer science;pattern recognition	ML	-2.246112441632459	-41.93824802656099	61840
b75ba5be2b2cfa4aada535ab140155837889d0b9	parallel evolution using multi-chromosome cartesian genetic programming	cartesian genetic programming;image classification;parallel evolution;region of interest;parallelisation;evolutionary algorithm;mammography;digital circuits;multiple chromosomes;chromosome evolution;x rays	Parallel and distributed methods for evolutionary algorithms have concentrated on maintaining multiple populations of genotypes, where each genotype in a population encodes a potential solution to the problem. In this paper, we investigate the parallelisation of the genotype itself into a collection of independent chromosomes which can be evaluated in parallel. We call this multi-chromosomal evolution (MCE). We test this approach using Cartesian Genetic Programming and apply MCE to a series of digital circuit design problems to compare the efficacy of MCE with a conventional single chromosome approach (SCE). MCE can be readily used for many digital circuits because they have multiple outputs. In MCE, an independent chromosome is assigned to each output. When we compare MCE with SCE we find that MCE allows us to evolve solutions much faster. In addition, in some cases we were able to evolve solutions with MCE that we unable to with SCE. In a case-study, we investigate how MCE can be applied to to a single objective problem in the domain of image classification, namely, the classification of breast X-rays for cancer. To apply MCE to this problem, we identify regions of interest (RoI) from the mammograms, divide the RoI into a collection of sub-images and use a chromosome to classify each sub-image. This problem allows us to evaluate various evolutionary mutation operators which can pairwise swap chromosomes either randomly or topographically or reuse chromosomes in place of other chromosomes.	arithmetic logic unit;benchmark (computing);cartesian closed category;computer vision;computer-aided design;crossover (genetic algorithm);database;digital electronics;evolutionary algorithm;genetic programming;integrated circuit design;linuxmce;mathematical optimization;paging;parallel computing;population;preprocessor;randomness;region of interest;scalable cluster environment;speedup;tinymce	James Alfred Walker;Katharina Völk;Stephen L. Smith;Julian Francis Miller	2009	Genetic Programming and Evolvable Machines	10.1007/s10710-009-9093-2	contextual image classification;computer science;bioinformatics;artificial intelligence;parallel evolution;machine learning;evolutionary algorithm;digital electronics;algorithm;region of interest	ML	2.7594025977206913	-46.706040794725126	61847
b0ef54696bf27e48c0cfb142c1604673cc891c75	standard and fuzzy rough entropy clustering algorithms in image segmentation	granular computing;cluster algorithm;image segmentation;image processing;fuzzy rough entropy measure;rough entropy measure;image clustering;rough sets;k means algorithm;entropy measure;cluster model;rough set;group presentation	Clustering or data grouping presents fundamental initial procedure in image processing. This paper addresses the problem of combining the concept of rough sets and entropy measure in the area of image segmentation. In the present study, comprehensive investigation into rough set entropy based thresholding image segmentation techniques has been performed. Segmentation presents the low-level image transformation routine concerned with image partitioning into distinct disjoint and homogenous regions with thresholding algorithms most often applied in practical solutions when there is pressing need for simplicity and robustness. Simultaneous combining entropy based thresholding with rough sets results in rough entropy thresholding algorithm. In the present paper, new algorithmic schemes Standard  RECA (Rough Entropy Clustering Algorithm) and Fuzzy  RECA in the area of rough entropy based partitioning routines have been proposed. Rough entropy clustering incorporates the notion of rough entropy into clustering model taking advantage of dealing with some degree of uncertainty in analyzed data. Both Standard and Fuzzy  RECA algorithmic schemes performed usually equally robustly compared to standard  k -means algorithm. At the same time, in many runs yielding slightly better performance making possible future implementation in clustering applications.	image segmentation	Dariusz Malyszko;Jaroslaw Stepaniuk	2008		10.1007/978-3-540-88425-5_42	machine learning;pattern recognition;data mining;mathematics	Vision	2.544886857123332	-40.116639105981115	62016
1583d337ae271f6b26e0d3c96d0b7a698fe8765a	protein family classification using sparse markov transducers	protein family;amino acid;protein family classification;probabilistic model;machine learning;probability distribution;probabilistic suffix tree;probabilistic suffix trees;data structure	We present a method for classifying proteins into families based on short subsequences of amino acids using a new probabilistic model called sparse Markov transducers (SMT). We classify a protein by estimating probability distributions over subsequences of amino acids from the protein. Sparse Markov transducers, similar to probabilistic suffix trees, estimate a probability distribution conditioned on an input sequence. SMTs generalize probabilistic suffix trees by allowing for wild-cards in the conditioning sequences. Since substitutions of amino acids are common in protein families, incorporating wild-cards into the model significantly improves classification performance. We present two models for building protein family classifiers using SMTs. As protein databases become larger, data driven learning algorithms for probabilistic models such as SMTs will require vast amounts of memory. We therefore describe and use efficient data structures to improve the memory usage of SMTs. We evaluate SMTs by building protein family classifiers using the Pfam and SCOP databases and compare our results to previously published results and state-of-the-art protein homology detection methods. SMTs outperform previous probabilistic suffix tree methods and under certain conditions perform comparably to state-of-the-art protein homology methods.		Eleazar Eskin;William Stafford Noble;Yoram Singer	2003	Journal of computational biology : a journal of computational molecular cell biology	10.1089/106652703321825964	probability distribution;statistical model;amino acid;data structure;computer science;bioinformatics;machine learning;pattern recognition;mathematics;protein family;statistics	Comp.	3.4473180458561536	-50.819854323745645	62038
1d2f55dd7b43fdb330872a6d22146aec7175ace5	maximum margin clustering on evolutionary data	maximum margin clustering;conference paper;evolutionary data	Evolutionary data, such as topic changing blogs and evolving trading behaviors in capital market, is widely seen in business and social applications. The time factor and intrinsic change embedded in evolutionary data greatly challenge evolutionary clustering. To incorporate the time factor, existing methods mainly regard the evolutionary clustering problem as a linear combination of snapshot cost and temporal cost, and reflect the time factor through the temporal cost. It still faces accuracy and scalability challenge though promising results gotten. This paper proposes a novel evolutionary clustering approach, evolutionary maximum margin clustering (e-MMC), to cluster large-scale evolutionary data from the maximum margin perspective. e-MMC incorporates two frameworks: Data Integration from the data changing perspective and Model Integration corresponding to model adjustment to tackle the time factor and change, with an adaptive label allocation mechanism. Three e-MMC clustering algorithms are proposed based on the two frameworks. Extensive experiments are performed on synthetic data, UCI data and real-world blog data, which confirm that e-MMC outperforms the state-of-the-art clustering algorithms in terms of accuracy, computational cost and scalability. It shows that e-MMC is particularly suitable for clustering large-scale evolving data.	algorithmic efficiency;blog;cluster analysis;embedded system;evolutionary algorithm;experiment;memory management controller;scalability;snapshot (computer storage);synthetic data	Xuhui Fan;Lin Zhu;Longbing Cao;Xia Cui;Yew-Soon Ong	2012		10.1145/2396761.2396842	correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;flame clustering;computer science;artificial intelligence;data science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;brown clustering;clustering high-dimensional data;conceptual clustering	ML	-1.2789504270383882	-39.485537004156726	62238
60d2046f8a1adad4934a94fbb2df7da6850a9cca	an automatic clustering algorithm inspired by membrane computing	membrane systems;automatic clustering;tissue like membrane systems;membrane computing;membrane clustering algorithm	We develop a membrane clustering algorithm to deal with automatic clustering problem.We design a tissue-like membrane system with fully connected structure.We develop an improved velocity-position model. Membrane computing is a class of distributed parallel computing models. Inspired from the structure and inherent mechanism of membrane computing, a membrane clustering algorithm is proposed to deal with automatic clustering problem, in which a tissue-like membrane system with fully connected structure is designed as its computing framework. Moreover, based on its special structure and inherent mechanism, an improved velocity-position model is developed as evolution rules. Under the control of evolution-communication mechanism, the tissue-like membrane system cannot only find the most appropriate number of clusters but else determine a good clustering partitioning for a data set. Six benchmark data sets are used to evaluate the proposed membrane clustering algorithm. Experiment results show that the proposed algorithm is superior or competitive to three state-and-the-art automatic clustering algorithms recently reported in the literature.	algorithm;cluster analysis;membrane computing	Hong Peng;Jun Wang;Peng Shi;Agustin Riscos-Núñez;Mario J. Pérez-Jiménez	2015	Pattern Recognition Letters	10.1016/j.patrec.2015.08.008	correlation clustering;computer science;bioinformatics;membrane computing;canopy clustering algorithm;machine learning;data mining;affinity propagation	Vision	4.32290679044205	-42.284338138257546	62614
b252cc034ec245c60a772a07725986200f5c2548	parallel algorithm for the hp protein folding problem	biology computing;proteins amino acids computational modeling clustering algorithms protein engineering model checking prediction algorithms;molecular configurations;program verification;algorithm correctness parallel algorithm hp protein folding problem cell function metabolism 3d native structure 3d protein structure predictïon amino acid sequence free energy minimization structural biology long proteins optimal minimum free energy computational resources simplified protein model hydrophobic polar protein model protein conformation distributed processing nodes server cluster memory constraint model checking parallel execution small proteins exponential complexity;model checking bioinformatics parallel algorithms performance analysis;proteins;model checking;computational complexity;molecular biophysics;performance analysis;proteins biology computing computational complexity free energy molecular biophysics molecular configurations parallel algorithms program verification;free energy;bioinformatics;parallel algorithms	Proteins play a key role in cells' function and metabolism. Their functions are directly related with the three-dimensional (3D) native structure. Different algorithms have been proposed to predict the 3D protein structure from the amino acids sequence by minimizing its free energy, nonetheless, this problem still a great challenge in structural biology. The space of possible conformations becomes very large for long proteins, making it difficult to search for the optimal minimum free energy with the available computational resources. Due to the complexity involved in the protein folding problem the researchers have developed some simplified protein models as the hydrophobic-polar (HP). In this work we propose a parallel algorithm to predict protein conformations in HP model by dividing tasks among processing nodes distributed in a server cluster. With this approach we circumvent the memory constraints observed by other approaches. We discuss the correctness of the algorithm based on the verification results achieved by model checking. The performance of our algorithm was evaluated but although benefits of parallel execution are noticeable for small proteins, we demonstrated that our algorithm has an exponential complexity.	3d computer graphics;beowulf cluster;brute-force search;c++;computational resource;computer cluster;correctness (computer science);distributed algorithm;energy level;experiment;heuristic (computer science);inter-process communication;mathematical optimization;message passing interface;model checking;nondeterministic algorithm;parallel algorithm;parallel computing;promela;protein structure prediction;server (computing);time complexity;whole earth 'lectronic link	Matheus M. dos Santos;Mauricio G. Goulart;Giovana J. Gelatti;Karina S. Machado;Adriano Velasque Werhli;Odorico Machado Mendizabal	2013	2013 2nd Workshop-School on Theoretical Computer Science	10.1109/WEIT.2013.18	computer science;bioinformatics;theoretical computer science;algorithm	Comp.	-2.259139284152855	-49.721288001967515	62786
b7fe9d2975b933dceae87d970bd37cb233f144cc	robust differential expression analysis by learning discriminant boundary in multi-dimensional space of statistical attributes	computational biology bioinformatics;algorithms;computer appl in life sciences;microarrays;bioinformatics	Performing statistical tests is an important step in analyzing genome-wide datasets for detecting genomic features differentially expressed between conditions. Each type of statistical test has its own advantages in characterizing certain aspects of differences between population means and often assumes a relatively simple data distribution (e.g., Gaussian, Poisson, negative binomial, etc.), which may not be well met by the datasets of interest. Making insufficient distributional assumptions can lead to inferior results when dealing with complex differential expression patterns. We propose to capture differential expression information more comprehensively by integrating multiple test statistics, each of which has relatively limited capacity to summarize the observed differential expression information. This work addresses a general application scenario, in which users want to detect as many as DEFs while requiring the false discovery rate (FDR) to be lower than a cut-off. We treat each test statistic as a basic attribute, and model the detection of differentially expressed genomic features as learning a discriminant boundary in a multi-dimensional space of basic attributes. We mathematically formulated our goal as a constrained optimization problem aiming to maximize discoveries satisfying a user-defined FDR. An effective algorithm, Discriminant-Cut, has been developed to solve an instantiation of this problem. Extensive comparisons of Discriminant-Cut with 13 existing methods were carried out to demonstrate its robustness and effectiveness. We have developed a novel machine learning methodology for robust differential expression analysis, which can be a new avenue to significantly advance research on large-scale differential expression analysis.	addresses (publication format);algorithm;chamaecyparis lawsoniana;constrained optimization;constraint (mathematics);false discovery rate;linear discriminant analysis;machine learning;mathematical optimization;normal statistical distribution;numerous;optimization problem;population;sensor;statistic (data);statistical test;universal instantiation	Yuanzhe Bei;Pengyu Hong	2016		10.1186/s12859-016-1386-x	biology;dna microarray;computer science;bioinformatics;machine learning	ML	5.549652751314062	-52.0616756524277	62898
a3471221b9bc9ffc001974d71440611a4b095eca	a collaborative filtering-based approach to biomedical knowledge discovery		Motivation The increase in publication rates makes it challenging for an individual researcher to stay abreast of all relevant research in order to find novel research hypotheses. Literature-based discovery methods make use of knowledge graphs built using text mining and can infer future associations between biomedical concepts that will likely occur in new publications. These predictions are a valuable resource for researchers to explore a research topic. Current methods for prediction are based on the local structure of the knowledge graph. A method that uses global knowledge from across the knowledge graph needs to be developed in order to make knowledge discovery a frequently used tool by researchers.   Results We propose an approach based on the singular value decomposition (SVD) that is able to combine data from across the knowledge graph through a reduced representation. Using cooccurrence data extracted from published literature, we show that SVD performs better than the leading methods for scoring discoveries. We also show the diminishing predictive power of knowledge discovery as we compare our predictions with real associations that appear further into the future. Finally, we examine the strengths and weaknesses of the SVD approach against another well-performing system using several predicted associations.   Availability and implementation All code and results files for this analysis can be accessed at https://github.com/jakelever/knowledgediscovery.   Contact sjones@bcgsc.ca.   Supplementary information Supplementary data are available at Bioinformatics online.	bioinformatics;bioinformatics;collaborative filtering;computation;computational resource;extraction;geographic information systems;graph - visual representation;inference;knowledge graph;mental association;molecular biology;scientific publication;score;singular value decomposition;text mining;weakness	Jake Lever;Sitanshu Gakkhar;Michael Gottlieb;Tahereh Rashnavadi;Santina Lin;Celia Siu;M Alex Smith;Martin R. Jones;Martin Krzywinski;Steven J. M. Jones	2018	Bioinformatics	10.1093/bioinformatics/btx613	collaborative filtering;computer science;data science;knowledge extraction;data mining	Comp.	7.527726241225453	-50.43423666947913	63077
876ce7515c34ec24e523b7a25a78374a017ecfdd	automatic graph building approach for spectral clustering	kernel function;spectral clustering;graph analysis	Spectral clustering techniques have shown their capability to identify the data relationships using graph analysis, achieving better accuracy than traditional algorithms as k-means. Here, we propose a methodology to build automatically a graph representation over the input data for spectral clustering based approaches by taking into account the local and global sample structure. Regarding this, both the Euclidean and the geodesic distances are used to identify the main relationships between a given point and neighboring samples around it. Then, given the information about the local data structure, we estimate an affinity matrix by means of Gaussian kernel. Synthetic and real-world datasets are tested. Attained results show how our approach outperforms, in most of the cases, benchmark methods.	spectral clustering	Andrés Eduardo Castro-Ospina;Andrés Marino Álvarez-Meza;Germán Castellanos-Domínguez	2013		10.1007/978-3-642-41822-8_24	kernel;correlation clustering;combinatorics;data stream clustering;power graph analysis;fuzzy clustering;flame clustering;computer science;machine learning;pattern recognition;mathematics;cluster analysis;spectral clustering;statistics	AI	1.250887551489172	-41.907160171944966	63090
004a6931b35a5ce8eb0d4c5c028b7c71640d320c	uncovering bivariate interactions in high dimensional data using random forests with data augmentation	random forests;bivariate interactions;high dimensional data	Random Forests (RF) is an ensemble technology for classification and regression which has become widely accepted in the bioinformatics community in the last few years. Its predictive strength, along with some of the utilities, rich in information, provided by the output, has made RF an efficient data mining tool for discovering patterns in high dimensional data. In this paper we propose a search strategy that explores a subset of the input space in an exhaustive way using RF as the search engine. Our procedure begins by taking the variables previously rejected by a sequential search procedure and uses the out of bag error rate of the ensemble, obtained when trained over an augmented data set, as criterion to capture difficult to uncover bivariate patterns associated with an outcome variable. We will show the performance of the procedure in some synthetic scenarios and will give an application to a real microarray experiment in order to illustrate how it works for gene expression data.	bivariate data;interaction;random forest	Jorge M. Arevalillo;Hilario Navarro	2011	Fundam. Inform.	10.3233/FI-2011-602	random forest;computer science;artificial intelligence;data science;machine learning;data mining;statistics;clustering high-dimensional data	DB	8.427945091317838	-45.96507951329131	63237
d538a55205d29b798839c8b05bc1a563a449b1cc	improving random projection with genetic algorithms: student research abstract		Within the field of machine learning, Random Projection (RP) is one of the simplest methods available to perform dimensionality reduction. This method relies on a classical result known as the Johnson-Lindenstrauss lemma, which states that a small set of points in a high dimensional feature space can be mapped into a space of much lower dimension in such a way that pairwise distances between the points are nearly preserved. Later on, it was proved that this mapping could be performed with a projection matrix whose elements are drawn from an extremely simple distribution [1], simplifying the projection computation to aggregate evaluation. However, due to the randomness introduced in the method during the construction of the projection matrix, the algorithm behaves non-deterministically. To illustrate this, Figure 1 shows the distribution of the stress measure over 200 runs of Random Projection on 500 samples from two typical machine learning datasets.	aggregate data;computation;deterministic algorithm;dimensionality reduction;feature vector;genetic algorithm;machine learning;rp (complexity);random projection;randomness	Daniel López Sánchez	2017		10.1145/3019612.3019942	mathematical optimization;projection (set theory);dykstra's projection algorithm;dimensionality reduction;projection (mathematics);feature vector;projection (linear algebra);machine learning;artificial intelligence;computer science;random projection;projection (relational algebra)	Theory	7.902048745051071	-42.91553116784215	63406
8b57468747857665904b899ff6469f21a107fb29	a image texture and bp neural network basec malicious files detection technique for cloud storage systems		In a complicated cloud storage environment in which users upload a large number of files everyday, in order to better solve the challenge of inefficient malicious detection and weak adaptability of multi-platform detection in the traditional way, we propose a malicious file detection method which is based on image texture analysis and BP neural network algorithm. By combining the technology of image analysis and the malicious file detection, the malicious file is converted into grayscale image, the GLCM (Ground Launched Cruise Missile) and the GIST (Generalized Search Trees) algorithms are used to extract the texture features, and the BP neural network algorithm is then used for learning and training. In this paper, we propose and implement a malicious file detection system by means of image texture extraction. Through the experimental analysis on a large number of virus samples from the well-known VirusShare project, the experimental results show that our proposed approach has the characteristics of fast speed, high adaptability and high accuracy.	algorithm;artificial neural network;cloud storage;gist;grayscale;image analysis;image texture;malware;upload;whole earth 'lectronic link	Guanchao Wen;Yupeng Hu;Chen Jiang;Na Cao;Zheng Qin	2017	2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)	10.1109/INFCOMW.2017.8116414	gist;adaptability;feature extraction;malware;cloud storage;computer vision;cloud computing;image texture;computer security;upload;computer science;artificial intelligence	Vision	6.520568241960783	-38.088116335187145	63474
d088fe22f8760d211b74a807aebd648f12a4c377	clustering with attribute-level constraints	attribute level;pattern clustering;set theory constraint handling constraint satisfaction problems pattern clustering;set theory;constrained clustering;satisfiability;constraint satisfaction problems;k medoids algorithm attribute level constraint clustering background knowledge constraint satisfaction instance level constraints;background knowledge;constraint handling;attribute level constrained clustering;runtime clustering algorithms electronic mail data mining equations animals	In many clustering applications the incorporation of background knowledge in the form of constraints is desirable. In this paper, we introduce a new constraint type and the corresponding clustering problem: attribute constrained clustering. The goal is to induce clusters of binary instances that satisfy constraints on the attribute level. These constraints specify whether instances may or may not be grouped to a cluster, depending on specific attribute values. We show how the well-established instance-level constraints, must-link and cannot-link, can be adapted to the attribute level. A variant of the k-Medoids algorithm taking into account attribute level constraints is evaluated on synthetic and real-world data. Experimental results show that such constraints may provide better clustering results at lower specification costs if constraints can be expressed on the attribute level.	algorithm;cluster analysis;constrained clustering;k-medoids;medoid;np-hardness;parameter (computer programming);synthetic intelligence;whole earth 'lectronic link	Jana Schmidt;Elisabeth Maria Brändle;Stefan Kramer	2011	2011 IEEE 11th International Conference on Data Mining	10.1109/ICDM.2011.36	correlation clustering;constrained clustering;data stream clustering;attribute domain;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;constraint satisfaction dual problem;mathematics;cluster analysis;constraint;brown clustering;constraint satisfaction problem;set theory;satisfiability;clustering high-dimensional data	DB	3.903164198114001	-40.49845679481466	63635
86ae093c379171a5741f6cb5ec06009dd2c986a0	novel hybrid feature selection algorithms for diagnosing erythemato-squamous diseases	generalized f score gf;support vector machines;erythemato squamous diseases;sequential forward floating search;sequential forward search sfs;sequential forward search;sequential backward floating search;two stage hybrid algorithms;generalized f score;support vector machines svm;feature selection;sequential forward floating search sffs;article;sequential backward floating search sbfs	This paper proposes hybrid feature selection algorithms to build the efficient diagnostic models based on a new accuracy criterion, generalized F-score (GF) and SVM. The hybrid algorithms adopt Sequential Forward Search (SFS), and Sequential Forward Floating Search (SFFS), and Sequential Backward Floating Search (SBFS), respectively, with SVM to accomplish hybrid feature selection with the new accuracy criterion to guide the procedure. We call them as modified GFSFS, GFSFFS and GFSBFS, respectively. These hybrid methods combine the advantages of filters and wrappers to select the optimal feature subset from the original feature set to build the efficient classifiers. To get the best and statistically meaningful classifiers, we not only conduct 10-fold cross validation experiments on training subset, but also on the whole erythemato-squamous diseases datasets. Experimental results show that our proposed hybrid methods construct efficient diagnosis classifiers with high average accuracy when compared with traditional algorithms.	algorithm;feature selection	Juanying Xie;Jinhu Lei;Weixin Xie;Xinbo Gao;Yong Shi;Xiaohui Liu	2012		10.1007/978-3-642-29361-0_21	computer science;machine learning;pattern recognition;data mining	ECom	10.013699617916963	-43.45193397224436	63707
1e47b52a7e9ba39267047fbc814144feb1d2ead0	improving classification accuracy using gene ontology information		Classification problems, e.g., gene function prediction problem, are very important in bioinformatics. Previous work mainly focuses on the improvement of classification techniques used. With the emergence of Gene Ontology (GO), extra knowledge about the gene products can be extracted from GO. Such kind of knowledge reveals the relationship of the gene products and is helpful for solving the classification problems. In this paper, we propose a new method to integrate the knowledge from GO into classifiers. The results from the experiments demonstrate the efficacy of our new method.	bioinformatics;emergence;experiment;gene ontology;gene prediction;naive bayes classifier;semantic similarity;statistical classification	Ying Shen;Lin Zhang	2013		10.1007/978-3-642-39678-6_29	computer science;bioinformatics;data science;data mining	AI	8.108253576047732	-49.15187615679439	63775
f1b447a681cf91c20a85a6ede64938a8b6623e0e	investigating the effect of training-testing data stratification on the performance of soft computing techniques: an experimental study	soft computing;data set division;permeability;porosity;stratification proportion	AbstractCross-validation of soft computing techniques needs to be done efficiently to avoid overfitting and underfitting. This is more important in petroleum reservoir characterisation applications where the often-limited training and testing data subsets represent Wells with known and unknown target properties, respectively. Existing data stratification strategies have been haphazardly chosen without any experimental basis. In this study, the optimal training–testing stratification proportions have been rigorously investigated using the prediction of porosity and permeability of petroleum reservoirs as an experimental case. The comparative performances of seven traditional and advanced machine learning techniques were considered. The overall results suggested a recommendable optimum training stratification that could serve as a good reference for researchers in similar applications.	experiment;soft computing;stratified sampling	Anifowose Fatai;Amar Khoukhi;Abdul Azeez Abdul Raheem	2017	J. Exp. Theor. Artif. Intell.	10.1080/0952813X.2016.1198936	computer science;data mining;soft computing;porosity;permeability	DB	10.009654699529197	-38.81731676522488	63810
b1bee261a94fcf0b49c30ac22d103a5ddc57ba20	case based reasoning with bayesian model averaging: an improved method for survival analysis on microarray data	microarray data;prediction method;case base reasoning;curse of dimensionality;data mining;classification;limit set;gene expression;bayesian model averaging;survival analysis;feature selection;gene selection;bioinformatics	Microarray technology enables the simultaneous measurement of thousands of gene expressions, while often providing a limited set of samples. These datasets require data mining methods for classification, prediction, and clustering to be tailored to the peculiarity of this domain, marked by the so called ‘curse of dimensionality'. One main characteristic of these specialized algorithms is their intensive use of feature selection for improving their performance. One promising method for feature selection is Bayesian Model Averaging (BMA) to find an optimal subset of genes. This article presents BMA applied to gene selection for classification on two cancer gene expression datasets and for survival analysis on two cancer gene expression datasets, and explains how case based reasoning (CBR) can benefit from this model to provide, in a hybrid BMA-CBR classification or survival prediction method, an improved performance and more expansible model.	case-based reasoning;ensemble learning;microarray	Isabelle Bichindaritz;Amalia Annest	2010		10.1007/978-3-642-14274-1_26	gene-centered view of evolution;limit set;microarray analysis techniques;gene expression;curse of dimensionality;biological classification;computer science;bioinformatics;pattern recognition;data mining;survival analysis;feature selection	ML	8.18502173658967	-48.72275187732679	63960
3cdb46ff26aecce1dd537089e4633f3e651dfa7a	fragmentization of distance measure for pattern generation by a self-organizing map	feature map distance measure fragmentization pattern generation self organizing map som data set topology preserving projection intermediate pattern generation computer simulations asymmetrical patterns symmetrical patterns;topology pattern classification self organising feature maps	A self-organizing map (SOM) can be seen as an analytical tool to discover some underlying rules in the given data set. Based on such distinctive nature called topology-preserving projection, a new method for generating intermediate patterns was proposed. According to the results of preceding studies, most developed patterns are not morphing but dissolve. Then, in order to overcome this problem, a fragmentized distance measure is introduced in this paper. As a result of computer simulations, it is confirmed that some asymmetrical patterns are developed even though only symmetrical ones are used for training. This fact reminds us that the distance measure is quite essential, because a feature map is developed through training based on the distance measure.	3d projection;computer simulation;emoticon;morphing;organizing (structure);self-organization;self-organizing map	Hiroshi Wakuya;Eishi Takahama;Hideaki Itoh;Hisao Fukumoto;Tatsuya Furukawa	2012	The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems	10.1109/SCIS-ISIS.2012.6505275	computer vision;computer science;machine learning;pattern recognition;mathematics	Robotics	5.500457133801493	-40.21109767365112	64028
e280d0cd31d6a1cf2aedcc362e427c26a9091346	a general multi-relational classification approach using feature generation and selection	high dimensionality;search space;feature generation;inductive logic programming;data mining;multi relational classification;feature selection	Multi-relational classification is an important data mining task, since much real world data is organized in multiple relations. The major challenges come from, firstly, the large high dimensional search spaces due to many attributes in multiple relations and, secondly, the high computational cost in feature selection and classifier construction due to the high complexity in the structure of multiple relations. The existing approaches mainly use the inductive logic programming (ILP) techniques to derive hypotheses or extract features for classification. However, those methods often are slow and sometimes cannot provide enough information to build effective classifiers. In this paper, we develop a general approach for accurate and fast multi-relational classification using feature generation and selection. Moreover, we propose a novel similarity-based feature selection method for multi-relational classification. An extensive performance study on several benchmark data sets indicates that our approach is accurate, fast and highly scalable.	algorithmic efficiency;benchmark (computing);computation;feature selection;inductive logic programming;inductive reasoning;relational data mining;scalability	Miao Zou;Tengjiao Wang;Hongyan Li;Dongqing Yang	2010		10.1007/978-3-642-17313-4_3	searching the conformational space for docking;computer science;machine learning;linear classifier;pattern recognition;data mining;feature selection;feature	ML	8.107701520755766	-39.95543459669528	64145
03a0862a97c5055a112129f34e1c22feadfcb495	combining semi-supervision and hubness to enhance high-dimensional data clustering			cluster analysis;semiconductor industry	Mateus C. de Lima;Maria Camila Nardini Barioni;Humberto Luiz Razente	2017	JIDM		clustering high-dimensional data;cluster analysis;artificial intelligence;computer science;pattern recognition	ML	2.5042480358433767	-41.109898788251215	64932
95d3375c7034605f9f514ac6baf7d6d2b57a1c64	ant colony inspired clustering based on the distribution function of the similarity of attributes		The paper presents results of research on the clustering problem on the basis of swarm intelligence using a new algorithm based on the normalized cumulative distribution function of attributes. In this approach, we assume that the analysis of likelihood of the occurrence of particular types of attributes and their values allows us to measure the similarity of the objects within a given category and the dissimilarity of the objects between categories. Therefore, on the basis of the complex data set of attributes of any type, we can successfully raise a lot of interesting information about these attributes without necessity of considering their real meaning. Our research shows that the algorithm inspired by the mechanisms observed in nature may return better results due to the modification of the neighborhood based on the similarity coefficient.	ant colony	Arkadiusz Lewicki;Krzysztof Pancerz;Ryszard Tadeusiewicz	2013		10.1007/978-3-642-34300-1_14	ant colony;cumulative distribution function;swarm intelligence;ant colony optimization algorithms;normalization (statistics);cluster analysis;artificial intelligence;distribution function;mathematics;complex data type;pattern recognition	ML	2.0301936894167802	-38.25929358408203	64937
7e3df8aaf14475d5c1f11fb253758d50be57a501	using bayesian posterior probability in confidence of attributes for feature selection	bayesian posterior probability;uncertainty;rough set theory;confidence;redundant attributes;quickreduct;machine learning;rough sets;feature selection;entropy	Rough set theory is an efficient reduction technique to deal with vagueness and uncertainty. Many studies have been accomplished for the feature selection while they have been carried out to trade off the sophisticated process of feature selection algorithm against the robustness and accuracy of reducts. In this paper, a new Bayesian posterior probability-based QuickReduct (BPPQR) measure is introduced to determine the optimal attributes with the accurate strength of the association among the indiscernible subsets. Therefore, a new rough entropy-based QuickReduct algorithm which focuses on the reduction of redundant attributes is proposed in order to extract the optimal reduct and the core. The performance of the system is evaluated in MATLAB on several benchmark datasets with resides in UCI machine learning repository. The proposed heuristic approach can cope with the drawbacks of the conventional one, and the satisfying performances have been carried out in the process of feature selection in decision systems.	benchmark (computing);big data;confidentiality;extended boot record;feature selection;heuristic;holographic principle;matlab;machine learning;performance;qr code;rough set;selection algorithm;set theory;vagueness	Inkyoo Park;Jongjin Park;Gyoo-Seok Choi	2015	IJSN	10.1504/IJSN.2015.070418	rough set;computer science;machine learning;pattern recognition;data mining;feature selection	AI	7.7509240791377865	-39.48333602660501	65062
83671c7357140b34c8cd43942bd5c4b86ec2d8a4	classification with missing data using multi-layered artificial immune systems	logistic discriminant models;support vector machines;unsupervised multilayered artificial immune system;risk analysis;insurance data processing;insurance risk classification multi layered artificial immune system missing dat;multi layered artificial immune system;k nearest neighbour model;high dimensional data;missing dat;insurance risk classification;pattern classification;high dimensional data missing data classification unsupervised multilayered artificial immune system insurance risk classification problem k nearest neighbour model support vector machines logistic discriminant models;insurance risk classification problem;artificial immune systems;missing data classification;risk analysis artificial immune systems insurance data processing pattern classification	The nature of missing data problems forces us to build models that maintain high accuracies and steadiness. The models developed to achieve this are usually complex and computationally expensive. In this paper, we propose an unsupervised multi-layered artificial immune system for an insurance classification problem that is characterised as highly dimensional and contains escalating missing data. The system is compared with the k-nearest neighbour, support vector machines and logistic discriminant models. Overall, the results show that whilst k-nearest neighbour achieves the highest accuracy, the multi-layered artificial immune system is steady and maintains high performance compared to other models, regardless of how the missing data is distributed in a dataset.	analysis of algorithms;artificial immune system;discriminant;k-nearest neighbors algorithm;missing data;support vector machine;unsupervised learning;while	Mlungisi Duma;Bhekisipho Twala;Tshilidzi Marwala;Fulufhelo Vincent Nelwamondo	2012	2012 IEEE Congress on Evolutionary Computation	10.1109/CEC.2012.6256420	support vector machine;risk analysis;computer science;machine learning;pattern recognition;data mining;clustering high-dimensional data	HPC	8.978047465404345	-38.24680267880958	65066
affea71c8c2625e077cae0818b5864fd36bf33bf	regulatory network analysis acceleration with reconfigurable hardware	t cell differentiation;hardware design languages;software;field programmable gate array;regulatory network;computer model;biological system modeling;biological system modeling field programmable gate arrays hardware design languages hardware computational modeling emulation software;emulation;algorithms cell differentiation cells cultured computer simulation feedback physiological gene expression regulation humans models biological signal transduction t lymphocytes;fpga implementation;computational modeling;hardware design;field programmable gate arrays;reconfigurable hardware;parallel simulation;hardware	In medical research it is of great importance to be able to quickly obtain answers to inquiries about system response to different stimuli. Modeling the dynamics of biological regulatory networks is a promising approach to achieve this goal, but existing modeling approaches suffer from complexity issues and become inefficient with large networks. In order to improve the efficiency, we propose the implementation of models of regulatory networks in hardware, which allows for highly parallel simulation of these networks. We find that our FPGA implementation of an example model of peripheral naïve T cell differentiation provides five orders of magnitude speedup when compared to software simulation.	biological network;cell (microprocessor);cell differentiation process;complexity;computer simulation;emulator;field-programmable gate array;naivety;peripheral;reconfigurable computing;speedup;orders - hl7publishingdomain	Natasa Miskov-Zivanov;Andrew Bresticker;Deepa Krishnaswamy;Sreesan Venkatakrishnan;Prashant Kashinkunti;Diana Marculescu;James R. Faeder	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6089916	computer simulation;real-time computing;computer science;theoretical computer science;field-programmable gate array;computer engineering	EDA	-4.386487159451703	-51.01994800836858	65236
8522fcfd99665ab1df6cffd10500fb52dda0c05b	a filter-based evolutionary approach for selecting features in high-dimensional micro-array data	classification algorithm;high dimensionality;search space;data mining;data analysis;genetic algorithm;feature selection;evolutionary algorithm	Evolutionary algorithms have received much attention in extracting knowledge on high-dimensional micro-array data, being crucial to their success a suitable definition of the search space of the potential solutions. In this paper, we present an evolutionary approach for selecting informative genes (features) to predict and diagnose cancer. We propose a procedure that combines results of filter methods, which are commonly used in the field of data mining, to reduce the search space where a genetic algorithm looks for solutions (i.e. gene subsets) with better classification performance, being the quality (fitness) of each solution evaluated by a classification method. The methodology is quite general because any classification algorithm could be incorporated as well a variety of filter methods. Extensive experiments on a public micro-array dataset are presented using four popular filter methods and SVM.	data mining;evolutionary algorithm;experiment;genetic algorithm;information;iterative and incremental development;microarray;support vector machine	Laura Maria Cannas;Nicoletta Dessì;Barbara Pes	2010		10.1007/978-3-642-16327-2_36	cultural algorithm;machine learning;pattern recognition;data mining;mathematics	ML	9.91137248835191	-43.54732505079118	65311
51940e401fb82aa4afa6d4b1a0c81a115a78d27b	accurately clustering moving objects with adaptive history filtering	moving object;data clustering algorithm;cluster algorithm;pattern clustering;moving objects clusters detection;machine learning spatio temporal data mining;history;distance measure;spatiotemporal data sets;finite impulse response filter;information filtering;data clustering algorithm adaptive history filtering moving objects clusters detection moving objects clusters tracking spatiotemporal data sets;spatio temporal data mining;data mining;data clustering;accuracy;adaptive filters;spatio temporal data;filtering algorithms;machine learning;adaptive history filtering;pattern clustering data mining information filtering learning artificial intelligence;clustering algorithms;history adaptive filters clustering algorithms monitoring radiofrequency identification computer science object detection law enforcement tracking space technology;learning artificial intelligence;moving objects clusters tracking	This paper addresses the problem of detecting and tracking clusters of moving objects in spatio-temporal data sets. Spatio-temporal data sets contain data objects that move in space over time. Traditional data clustering algorithms work well on static data sets that contain well separated clusters. Traditional techniques breakdown when they are applied to spatio-temporal data sets. They are not capable of tracking clusters when the moving objects intersect the space occupied by objects from another cluster. This work aims to improve the accuracy of traditional data clustering algorithms on spatio-temporal data sets. Many clustering algorithms create clusters based on the distance between the objects. We extend this distance measure to be a function of the position history of the objects. We show through a series of experiments that the use of the history based distance measures greatly improves the accuracy of existing data clustering algorithms on spatio-temporal data sets. In random data sets we achieve up to a 90% improvement in cluster accuracy. To evaluate the clustering algorithms we created 100 spatio-temporal data sets. We also defined a set of metrics that are used to evaluate the performance of the clustering algorithms on the spatio-temporal data sets.	algorithm;cluster analysis;experiment;randomness;sensor;spatiotemporal database	James Rosswog;Kanad Ghose	2009	2009 24th International Symposium on Computer and Information Sciences	10.1109/ISCIS.2009.5291901	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;clustering high-dimensional data	DB	-0.8374081502298414	-38.4178017151266	65370
543aaf30120c0137b529fb551c181c01c7b69691	cost-sensitive spam detection using parameters optimization and feature selection		E-mail spam is no more garbage but risk since it recently includes virus attachments and spyware agents which make the recipients’ system ruined, therefore, there is an emerging need for spam detection. Many spam detection techniques based on machine learning techniques have been proposed. As the amount of spam has been increased tremendously using bulk mailing tools, spam detection techniques should counteract with it. To cope with this, parameters optimization and feature selection have been used to reduce processing overheads while guaranteeing high detection rates. However, previous approaches have not taken into account feature variable importance and optimal number of features. Moreover, to the best of our knowledge, there is no approach which uses both parameters optimization and feature selection together for spam detection. In this paper, we propose a spam detection model enabling both parameters optimization and optimal feature selection; we optimize two parameters of detection models using Random Forests (RF) so as to maximize the detection rates. We provide the variable importance of each feature so that it is easy to eliminate the irrelevant features. Furthermore, we decide an optimal number of selected features using two methods; (i) only one parameters optimization during overall feature selection and (ii) parameters optimization in every feature elimination phase. Finally, we evaluate our spam detection model with cost-sensitive measures to avoid misclassification of legitimate messages, since the cost of classifying a legitimate message as a spam far outweighs the cost of classifying a spam as a legitimate message. We perform experiments on Spambase dataset and show the feasibility of our approaches.	anti-spam techniques;attachments;experiment;feature selection;machine learning;mathematical optimization;numerical analysis;radio frequency;random forest;relevance;sensor;spamming;spyware	Sang Min Lee;Dong Seong Kim;Jong Sou Park	2011	J. UCS	10.3217/jucs-017-06-0944	machine learning;data mining;world wide web	Security	6.155280440312568	-38.57452127589626	65581
27c9ecd7b574594403a9760469056e605eaf0b2c	monotony of surprise and large-scale quest for unusual words	combinatorics on words;analysis of algorithm;probabilistic model;pattern discovery;annotated suffix trees;large scale;statistical analysis;molecular biology;over and under represented words;combinatoric on words;data structure;design and analysis of algorithms;statistical analysis of sequences	The problem of characterizing and detecting recurrent sequence patterns such as substrings or motifs and related associations or rules is variously pursued in order to compress data, unveil structure, infer succinct descriptions, extract and classify features, etc. In Molecular Biology, exceptionally frequent or rare words in bio-sequences have been implicated in various facets of biological function and structure. The discovery, particularly on a massive scale, of such patterns poses interesting methodological and algorithmic problems, and often exposes scenarios in which tables and synopses grow faster and bigger than the raw sequences they are meant to encapsulate. In previous study, the ability to succinctly compute, store, and display unusual substrings has been linked to a subtle interplay between the combinatorics of the subwords of a word and local monotonicities of some scores used to measure the departure from expectation. In this paper, we carry out an extensive analysis of such monotonicities for a broader variety of scores. This supports the construction of data structures and algorithms capable of performing global detection of unusual substrings in time and space linear in the subject sequences, under various probabilistic models.	british informatics olympiad;data table;data structure;description;function (biology);greater;inference;mental association;rule (guideline);sensor;sequence motif;structure of articular surface of bone;substring;algorithm	Alberto Apostolico;Mary Ellen Bock;Stefano Lonardi	2002	Journal of computational biology : a journal of computational molecular cell biology	10.1145/565196.565200	statistical model;data structure;computer science;bioinformatics;machine learning;data mining;mathematics;algorithm;statistics;combinatorics on words	Comp.	-3.4055594909640488	-50.39049291197547	65700
b65aade9fdbb4f0cc6672fec92d0dd974a58e5db	feature weight estimation for gene selection: a local hyperlinear learning approach	support vector machines;bayes theorem;databases genetic;computational biology bioinformatics;discriminant analysis;cluster analysis;algorithms;humans;neoplasms;combinatorial libraries;computational biology;computer appl in life sciences;gene expression profiling;oligonucleotide array sequence analysis;microarrays;bioinformatics	Modeling high-dimensional data involving thousands of variables is particularly important for gene expression profiling experiments, nevertheless,it remains a challenging task. One of the challenges is to implement an effective method for selecting a small set of relevant genes, buried in high-dimensional irrelevant noises. RELIEF is a popular and widely used approach for feature selection owing to its low computational cost and high accuracy. However, RELIEF based methods suffer from instability, especially in the presence of noisy and/or high-dimensional outliers. We propose an innovative feature weighting algorithm, called LHR, to select informative genes from highly noisy data. LHR is based on RELIEF for feature weighting using classical margin maximization. The key idea of LHR is to estimate the feature weights through local approximation rather than global measurement, which is typically used in existing methods. The weights obtained by our method are very robust in terms of degradation of noisy features, even those with vast dimensions. To demonstrate the performance of our method, extensive experiments involving classification tests have been carried out on both synthetic and real microarray benchmark datasets by combining the proposed technique with standard classifiers, including the support vector machine (SVM), k-nearest neighbor (KNN), hyperplane k-nearest neighbor (HKNN), linear discriminant analysis (LDA) and naive Bayes (NB). Experiments on both synthetic and real-world datasets demonstrate the superior performance of the proposed feature selection method combined with supervised learning in three aspects: 1) high classification accuracy, 2) excellent robustness to noise and 3) good stability using to various classification algorithms.	algorithmic efficiency;anterior descending branch of left coronary artery;approximation;benchmark (computing);computation;dhrystone;dimensions;effective method;elegant degradation;expectation–maximization algorithm;experiment;feature selection;gene expression profiling;gene expression programming;human body weight;information;instability;k-nearest neighbors algorithm;linear discriminant analysis;microarray;naive bayes classifier;relevance;signal-to-noise ratio;single linkage cluster analysis;supervised learning;support vector machine;synthetic intelligence	Hongmin Cai;Peiying Ruan;Michael K. Ng;Tatsuya Akutsu	2013		10.1186/1471-2105-15-70	support vector machine;dna microarray;computer science;bioinformatics;machine learning;data mining;gene expression profiling;cluster analysis;linear discriminant analysis;bayes' theorem;genetics	ML	9.254254502356	-47.89323553379026	65894
b9061be39b4b1f1c05c31375d14b74800d44a831	molecular phylogenetic trees are inferred by using minimum model-based complexity	molecular phylogenetics		molecular phylogenetics;phylogenesis	Fengrong Ren;Hiroshi Tanaka;Toshitsugu Okayama;Takashi Gojobori	1997			computational phylogenetics;molecular phylogenetics;phylogenetic network	Robotics	0.8628681049963538	-50.978979709552945	66180
0ca8c4a42e86526e9ae752d9c63545d78c1422bf	an efficient branch-and-bound algorithm for the assignment of protein backbone nmr peaks	nmr;assignment problem;computer program;peak assignment;search space;protein sequence;branch and bound algorithm;bipartite matching;biological nmr;search space reduction efficient branch and bound algorithm protein backbone nmr peak assignment nmr resonance assignment nmr protein structure target protein sequence inter residue spatial relationships intra residue spatial relationships atoms backbone resonance peak assignment constrained weighted bipartite matching problem np hard problem greedy filtering algorithm;computational method;chemical shift;spine nuclear magnetic resonance protein sequence bioinformatics amino acids informatics laboratories computer science filtering algorithms chemical technology;protein structure;proteins;algorithm theory;computational complexity;search problems proteins tree searching biological nmr molecular biophysics computational complexity algorithm theory;molecular biophysics;combinatorial technique;greedy algorithm;search problems;spatial relationships;tree searching;branch and bound	NMR resonance assignment is one of the key steps in solving an NMR protein structure. The assignment process links resonance peaks to individual residues of the target protein sequence, providing the prerequisite for establishing intra- and inter-residue spatial relationships between atoms. The assignment process is tedious and time-consuming, which could take many weeks. Though there exist a number of computer programs to assist the assignment process, many NMR labs are still doing the assignments manually to ensure quality. This paper presents a new computational method based on our recent work towards automating the assignment process, particularly the process of backbone resonance peak assignment. We formulate the assignment problem as a constrained weighted bipartite matching problem. While the problem, in the most general situation, is NP-hard, we present an efficient solution based on a branch-and-bound algorithm with effective bounding techniques and a greedy filtering algorithm for reducing the search space. Our experimental results on 70 instances of (pseudo) real NMR data derived from 14 proteins demonstrate that the new solution runs much faster than a recently introduced (exhaustive) two-layer algorithm and recovers more correct peak assignments than the two-layer algorithm.	amino acid sequence;assignment problem;branch and bound;computer program;existential quantification;greedy algorithm;internet backbone;magnetic resonance imaging;matching (graph theory);np-hardness;pseudo brand of pseudoephedrine;vertebral column	Guohui Lin;Dong Xu;Zhi-Zhong Chen;Tao Jiang;Jianjun Wen;Ying Xu	2002	Proceedings. IEEE Computer Society Bioinformatics Conference	10.1109/CSB.2002.1039339	mathematical optimization;combinatorics;linear bottleneck assignment problem;computer science;bioinformatics;generalized assignment problem;mathematics;weapon target assignment problem;branch and bound;molecular biophysics	Comp.	0.8862483464946245	-49.363929919202704	66244
a7e6a402c6b27309612f9762b19966e363af4b6f	using cvi for understanding class topology in unsupervised scenarios		Cluster validation in Clustering is an open problem. The most exploited possibility is the validation through cluster validity indexes (CVIs). However, there are many indexes available, and they perform inconsistently scoring different partitions over a given dataset. The aim of the study carried out is the analysis of seventeen CVIs to get a common understanding of its nature, and proposing an efficient strategy for validating a given clustering. A deep understanding of what CVIs are measuring has been achieved by rewriting all of them under a common notation. This exercise revealed that indexes measure different structural properties of the clusters. A Principal Component Analysis (PCA) confirmed this conceptual classification. Our methodology proposes to perform a multivariate joint analysis of the indexes to learn about the cluster topology instead of using them for simple ranking in a competitive way.		Beatriz Sevilla Villanueva;Karina Gibert;Miquel Sànchez-Marrè	2016		10.1007/978-3-319-44636-3_13	artificial intelligence;machine learning;pattern recognition	Metrics	1.2279152196989227	-43.588871129354004	66545
992988544b1014bdc5a3eb4e6e01b597492377ad	an empirical comparison of label prediction algorithms on automatically inferred networks	technology and engineering	The task of predicting the label of a network node, based on the labels of the remaining nodes, is an area of growing interest in machine learning, as various types of data are naturally represented as nodes in a graph. As an increasing number of methods and approaches are proposed to solve this task, the problem of comparing their performance becomes of key importance. In this paper we present an extensive experimental comparison of 15 different methods, on 15 different labelled-networks, as well as releasing all datasets and source code. In addition, we release a further set of networks that were not used in this study (as not all benchmarked methods could manage very large datasets). Besides the release of data, protocols and algorithms, the key contribution of this study is that in each of the 225 combinations we tested, the best performance—both in accuracy and running time—was achieved by the same algorithm: Online Majority Vote. This is also one of the simplest methods to implement.	algorithm;machine learning;time complexity	Omar Ali;Giovanni Zappella;Tijl De Bie;Nello Cristianini	2012			computer science;bioinformatics;data science;data mining	ML	-2.1658630259806073	-46.369875952528766	66567
d5d4f66146ac7667c8b8a7a6c243236f36da4a0c	guiding supervised learning by bio-ontologies in medical data analysis		Ontologies are popular way of representing knowledge and semantics of data in medical and health fields. Surprisingly, few machine learning methods allow for encoding semantics of data and even fewer allow for using ontologies to guide learning process. This paper discusses the use of data semantics and ontologies in health and medical applications of supervised learning, and particularly describes how the Unified Medical Language System (UMLS) is used within AQ21 rule learning software. Presented concepts are illustrated using two applications based on distinctly different types of data and methodological issues.	ontology (information science);supervised learning	Janusz Wojtusiak;Hua Min;Eman Elashkar;Hedyeh Mobahi	2016		10.1007/978-3-319-92928-6_1	supervised learning;open biomedical ontologies;natural language processing;data type;software;ontology (information science);semantics;artificial intelligence;machine learning;unified medical language system;computer science	ML	5.458307934532086	-45.46925625274286	66645
422402e6429a72c1cd7e16fc849f8796aa55f34a	minimum description length and clustering with exemplars	cluster algorithm;minimization;pattern clustering;optimisation;complexity theory;trees mathematics optimisation pattern clustering stochastic processes;prior knowledge;distance based clustering;trees mathematics;density based clustering;objective function;indexes;synthetic data clustering performance minimum description length density based clustering distance based clustering clustering performance objective functions stochastic complexity similarity based clustering combinatorial optimization minimum arborescence tree algorithm;computational modeling;stochastic processes;minimum arborescence tree algorithm;minimum description length;number of clusters;clustering algorithms;stochastic complexity;synthetic data;combinatorial optimization;clustering algorithms systems engineering and theory tree graphs learning clustering methods partitioning algorithms computer science stochastic systems artificial intelligence bioinformatics;encoding;similarity based clustering;information theoretic;synthetic data clustering performance;clustering performance objective functions	We propose an information-theoretic clustering framework for density-based clustering and similarity or distance-based clustering with objective functions of clustering performance derived from stochastic complexity and minimum description length (MDL) arguments. Under this framework, the number of clusters and parameters can be determined in a principled way without prior knowledge from users. We show that similarity-based clustering can be viewed as combinatorial optimization on graphs. We propose two clustering algorithms, one of which relies on a minimum arborescence tree algorithm which returns optimal clustering under the proposed MDL objective function for similarity-based clustering. We demonstrate clustering performance on synthetic data.	algorithm;cluster analysis;combinatorial optimization;information theory;kolmogorov complexity;list of algorithms;loss function;mdl (programming language);mathematical optimization;minimum description length;optimization problem;self-similarity;synthetic data	Po-Hsiang Lai;Joseph A. O'Sullivan;Robert Pless	2009	2009 IEEE International Symposium on Information Theory	10.1109/ISIT.2009.5205937	complete-linkage clustering;database index;correlation clustering;constrained clustering;mathematical optimization;determining the number of clusters in a data set;data stream clustering;minimum description length;k-medians clustering;fuzzy clustering;combinatorial optimization;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;computational model;dbscan;biclustering;affinity propagation;encoding;statistics;synthetic data;clustering high-dimensional data;conceptual clustering	ML	3.676542567785938	-40.48349195085028	66686
3d765c7918b04e1a727f64bfe3117171917f91da	a biclustering algorithm based on a bicluster enumeration tree: application to dna microarray data	health research;uk clinical guidelines;biological patents;evaluation function;europe pubmed central;citation search;data mining and knowledge discovery;computational biology bioinformatics;search trees;uk phd theses thesis;tree structure;life sciences;algorithms;dna microarray data;uk research reports;medical journals;computer appl in life sciences;europe pmc;biomedical research;bioinformatics	In a number of domains, like in DNA microarray data analysis, we need to cluster simultaneously rows (genes) and columns (conditions) of a data matrix to identify groups of rows coherent with groups of columns. This kind of clustering is called biclustering. Biclustering algorithms are extensively used in DNA microarray data analysis. More effective biclustering algorithms are highly desirable and needed. We introduce BiMine, a new enumeration algorithm for biclustering of DNA microarray data. The proposed algorithm is based on three original features. First, BiMine relies on a new evaluation function called Average Spearman's rho (ASR). Second, BiMine uses a new tree structure, called Bicluster Enumeration Tree (BET), to represent the different biclusters discovered during the enumeration process. Third, to avoid the combinatorial explosion of the search tree, BiMine introduces a parametric rule that allows the enumeration process to cut tree branches that cannot lead to good biclusters. The performance of the proposed algorithm is assessed using both synthetic and real DNA microarray data. The experimental results show that BiMine competes well with several other biclustering methods. Moreover, we test the biological significance using a gene annotation web-tool to show that our proposed method is able to produce biologically relevant biclusters. The software is available upon request from the authors to academic users.	biclustering;citrus aurantium;cluster analysis;coherence (physics);column (database);dna microarray format;dhrystone;evaluation function;gene annotation;pollard's rho algorithm;search tree;tree structure;statistical cluster	Wassim Ayadi;Mourad Elloumi;Jin-Kao Hao	2009		10.1186/1756-0381-2-9	computer science;bioinformatics;data science;evaluation function;data mining;tree structure;biclustering;clustering high-dimensional data	ML	3.6803027674490614	-49.952531765510024	66814
d3aea2b31860e744e446df3d257b05ed3a3191b0	pruning multivariate decision trees by hyperplane merging	decision tree	Several techniques for induction of multivariate decision trees have been published in the last couple of years. Internal nodes of such trees typically contain binary tests questioning to what side of a hyperplane the example lies. Most of these algorithms use cut-off pruning mechanisms similar to those of traditional decision trees. Nearly unexplored remains the large domain of substitutional pruning methods, where a new decision test (derived from previous decision tests) replaces a subtree. This paper presents an approach to multivariate-tree pruning based on merging the decision hyperplanes, and demonstrates its performance on artificial and benchmark data.	decision tree	Miroslav Kubat;Doris Flotzinger	1995		10.1007/3-540-59286-5_58	decision tree learning;alternating decision tree;incremental decision tree;decision stump;pruning	ML	8.424111788627231	-45.00831947877458	66854
95c122c27494a3aed3dbab462c5190502f6c626e	clustering microarray data based on density and shared nearest neighbor measure	p-tree;gene expression profiles;co-expressed genes;microarray data;clustering;data clustering;data management;data structure;nearest neighbor;data mining;cluster analysis	Microarray technology is being used to study several genes at a time generating huge amounts of data. Managing and extracting useful information from such data is a big challenge for the bioinformatics community. Clustering is an important data mining technique that has been proved to be useful for the analysis and understanding of such data. Clustering is used to automatically identify similar groups of objects based on a given measure. In microarray data, the genes that exhibit similar expression profiles are co-expressed and will be grouped into a single cluster. In this paper, we propose a new clustering algorithm based on density and shared nearest neighbor measure to identify clusters of genes exhibiting similar expression profiles. In our algorithm, we used an efficient bitwise vertical data structure called P-tree to decompose the microarray data into separate bit vectors. Pearson’s correlation coefficient is used as the similarity measure to identify the core points of the clusters by calculating the density of the genes. Also in our algorithm, there is no need to specify the number of clusters ahead. The clusters in the data set are identified automatically based on the core genes. We experimentally show that our algorithm is fast and scalable when applied on Iyer’s microarray data set for cluster analysis.	algorithm;bioinformatics;bit array;bitwise operation;cluster analysis;data mining;data structure;experiment;microarray;phylogenetic tree;scalability;similarity measure	Ranapratap Syamala;Taufik Abidin;William Perrizo	2006			computer science;clustering high-dimensional data;nearest-neighbor chain algorithm;cluster analysis;correlation clustering;similarity measure;k-nearest neighbors algorithm;artificial intelligence;k-medians clustering;pattern recognition;cure data clustering algorithm	ML	-1.9548977123649574	-40.739426403853194	67173
5b8370dcb393b166b46a918d08a28d1bb895c664	a system for motif search in biological networks		In this work, we investigate an important problem from the biology field, which consists in searching for specific patterns, named motifs, in networks which represent certain biological interactions, such as metabolic networks, regulatory networks or Protein-Protein Interaction (PPI) networks. Two linear integer models are proposed, one of them using the concept of representatives. We present computational experiments made with instances generated from PPI networks with approximately 8.000 proteins and 29.000 interactions among them. As experimentally verified, the two proposed models were able to solve all instances in a very satisfactory amount of computational time.		Alexandre da Silva Freire;Karla Roberta Lima;D. Rojas	2018		10.1145/3229345.3229378	biology (field);motif (music);theoretical computer science;biological network;integer programming;integer;computer science	Comp.	-0.14855102525587197	-50.48726382476775	67267
800d80b428a4e15f09c4a0e8bd77d1b578fa67b5	incorporating pca and fuzzy-art techniques into achieve organism classification based on codon usage consideration	integrated approach;hidden information;fuzzy adaptive resonance theory fuzzy art;amino acid;cluster analysis;codon usage;principle component analysis;dna sequence;high dimension;principle component analysis pca;adaptive resonance theory	To recognize the DNA sequence and mine the hidden information to achieve the classification of organisms are viewed as a difficult work to biologists. As we know, the amino acids are the basic elements to construct DNA. Hence, if the codon usage of amino acids can be analyzed well, the useful information about classification of organisms may be obtained. However, if we choose too many amino acids to perform the clustering analysis, the high dimensions also lead the clustering analysis to be a complicated structure. Hence, in this study, we will incorporate the principle component analysis and fuzzy-ART clustering techniques into constructing an integrated approach. The useful information about organisms classification based on the codon usage can be mined by using the proposed approach. Finally, we also employ a case including 18 bacteria to demonstrate the rationality and feasibility of our proposed approach.	amino acids;cluster analysis;codon (nucleotide sequence);mined;principal component analysis;rationality;statistical cluster	Kun-Lin Hsieh;I-Ching Yang	2008	Computers in biology and medicine	10.1016/j.compbiomed.2008.05.007	dna sequencing;codon usage bias;amino acid;computer science;bioinformatics;adaptive resonance theory;machine learning;data mining;mathematics;cluster analysis;principal component analysis	Comp.	6.590422551868306	-48.38394939600654	67316
2ae847acffe05d826bec9a1e8a20e2dde8fa68b1	density-preserving projections for large-scale local anomaly detection	anomaly detection;dimensionality reduction	Outlier or anomaly detection is a fundamental data mining task with the aim to identify data points, events, transactions which deviate from the norm. The identification of outliers in data can provide insights about the underlying data generating process. In general, outliers can be of two kinds: global and local. Global outliers are distinct with respect to the whole data set, while local outliers are distinct with respect to data points in their local neighbourhood. While several approaches have been proposed to scale up the process of global outlier discovery in large databases, this has not been the case for local outliers. We tackle this problem by optimising the use of local outlier factor (LOF) for large and high-dimensional data. We propose projection-indexed nearest-neighbours (PINN), a novel technique that exploits extended nearest-neighbour sets in a reduced-dimensional space to create an accurate approximation for k-nearest-neighbour distances, which is used as the core density measurement within LOF. The reduced dimensionality allows for efficient sub-quadratic indexing in the number of items in the data set, where previously only quadratic performance was possible. A detailed theoretical analysis of random projection (RP) and PINN shows that we are able to preserve the density of the intrinsic manifold of the data set after projection. Experimental results show that PINN outperforms the standard projection methods RP and PCA when measuring LOF for many high-dimensional real-world data sets of up to 300,000 elements and 102,600 dimensions. A further investigation into the use of high-dimensionality-specific indexing such as spatial approximate sample hierarchy (SASH) shows that our novel technique holds benefits over even these types of highly efficient indexing. We cement the practical applications of our novel technique with insights into what it means to find local outliers in real data including image and text data, and include potential applications for this knowledge.	anomaly detection;approximation algorithm;data mining;data point;database;local outlier factor;principal component analysis;rp (complexity);random projection;text corpus	Timothy de Vries;Sanjay Chawla;Michael E. Houle	2011	Knowledge and Information Systems	10.1007/s10115-011-0430-4	anomaly detection;computer science;machine learning;data mining;mathematics;statistics;dimensionality reduction	ML	-1.2104958463435092	-42.15427264378628	67837
abd70928563197cdb1edf497d1b4c0d2fc7e5d25	land cover classification based on adaptive interval-valued type-2 fuzzy clustering analysis	remote sensing image;type 2 fuzzy sets;land cover classification;spot5;adaptive fuzzy clustering	The classic methods, such as FCM, often fail to carry out accurate modeling for the high-level fuzzy uncertainty, and then cause the classification error that should not be ignored in the application. Fortunately, the type-2 fuzzy set is a tool to handle this type of uncertainty. An adaptive interval-valued type-2 fuzzy C-Means clustering algorithm A-IT2FCM is proposed, including:1 a proper modeling method for interval-valued type-2 fuzzy set;2 an effective type reduction approach by adaptively searching the equivalent type-1 fuzzy sets for the type-2. Three different type-2 fuzzy clustering algorithms are used: the algorithm based on Karnik-Mendel type reduction, a method based on simple type reduction, and A-IT2FCM presented in this article. The experimental data are two data windows of SPOT5 image from Zhuhai and Beijing, China. Results show that, A-IT2FCM outperforms the other algorithms compared. Especially when obvious density difference exists between objects in the data, A-IT2FCM can achieve more accurate class boundaries and higher classification accuracy.	cluster analysis;fuzzy clustering	Hui He;Xianchuan Yu;Dan Hu	2015		10.1007/978-3-319-25159-2_65	defuzzification;fuzzy clustering;adaptive neuro fuzzy inference system;flame clustering;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;fuzzy associative matrix;fuzzy set operations	ML	2.7698806896280983	-38.632943359370884	67863
6aaa5b1e47bf28465a2dc67ce9a77a02d53350d9	a fast synchronization clustering algorithm		This paper presents a Fast Synchronization Clustering algorithm (FSynC), which is an improved version of SynC algorithm. In order to decrease the time complexity of the original SynC algorithm, we combine grid cell partitioning method and Red-Black tree to construct the near neighbor point set of every point. By simulated experiments of some artificial data sets and several real data sets, we observe that FSynC algorithm can often get less time than SynC algorithm for many kinds of data sets. At last, it gives some research expectations to popularize this	algorithm;chen–ho encoding;cluster analysis;entity–relationship model;experiment;information systems;information and computer science;machine learning;minimum description length;red–black tree;sync (unix);synchronization (computer science);time complexity	Xinquan Chen	2014	CoRR		real-time computing;ramer–douglas–peucker algorithm;computer science;theoretical computer science;canopy clustering algorithm;machine learning;fsa-red algorithm	ML	-1.8213416732581786	-39.06367400645823	68040
cd9eaea335b6728fffe7fa653e2d7b46f859c107	the structure of granular network based on granular computing and its application in data reduction	pattern discovery algorithms;human single nucleotide polymorphism;human chromosome;probabilistic method;pattern discovery algorithm;snp prediction study;human single nucleotide polymorphisms;machine learning;probabilistic pattern discovery method;snp prediction;pattern similarity;deterministic method;coding snps;single nucleotide polymorphism;learning artificial intelligence;dna sequence;random forests;data mining;random forest;dna sequences	The granulating is the segmentation from the whole into the parts. The elementary granulation and the granulation are defined in this paper, and with construction of the granular network, the relevant knowledge of graph theory is applied to granular network and its computing. In the paper it is shown using the example that it is feasible and effective that granular network is applied in describing data reduction in information system. The method has the characteristic of simple and visual form and so on. Compared with other analysis methods, its time complexity is decreased to O(n).	granular computing;graph theory;information system;time complexity	Shaobo Deng;Min Li;Sujie Guan;Lian Chen	2007	2007 IEEE International Conference on Granular Computing (GRC 2007)	10.1109/GrC.2007.51	random forest;dna sequencing;computer science;bioinformatics;machine learning;data mining	Robotics	4.909107220357945	-48.14117932841411	68301
21818c7c4a649ed04b9c55570f546f85b19b3b2a	a framework for initialising a dynamic clustering algorithm: art2-a	pattern clustering adaptive resonance theory big data medical information systems;prototypes;subspace constraints;vectors;big data;heuristic algorithms;prototypes heuristic algorithms vectors clustering algorithms breast cancer subspace constraints big data;clustering algorithms;benchmark datasets dynamic clustering algorithm art2 a adaptive resonance theory family dynamic online clustering big health data signal processing field adapted separation and concordance framework seco real world heath related datasets;breast cancer	Algorithms in the Adaptive Resonance Theory (ART) family adapt to structural changes in data as new information presents, making it an exciting candidate for dynamic online clustering of big health data. Its use however has largely been restricted to the signal processing field. In this paper we introduce an refinement of the ART2-A method within an adapted separation and concordance (SeCo) framework which has been shown to identify stable and reproducible solutions from repeated initialisations that also provides evidence for an appropriate number of initial clusters that best calibrates the algorithm with the data presented. The results show stable, reproducible solutions for a mix of real-world heath related datasets and well known benchmark datasets, selecting solutions which better represent the underlying structure of the data than using a single measure of separation. The scalability of the method and it's facility for dynamic online clustering makes it suitable for finding structure in big data.	adaptive resonance theory;algorithm;benchmark (computing);big data;cluster analysis;concordance (publishing);refinement (computing);scalability;signal processing	Simon J. Chambers;Ian H. Jarman;Paulo J. G. Lisboa	2014	2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)	10.1109/CIDM.2014.7008678	correlation clustering;constrained clustering;data stream clustering;big data;fuzzy clustering;computer science;data science;canopy clustering algorithm;breast cancer;machine learning;cure data clustering algorithm;data mining;prototype;cluster analysis;statistics;clustering high-dimensional data	ML	2.0746736859081403	-44.66055064009767	68354
6ba7be6312a4733c3ed758312e3e743c2a5181f8	revisiting visual attention identification based on eye tracking data analytics	clustering;random walk;affinity propagation;eye tracking;visual attention	Visual attention identification is crucial to human visual perception analysis and relevant applications. In this paper, we propose a comprehensive visual attention identification algorithm consisting of clustering and center identification. In the clustering process, a spatial-temporal affinity propagation method for accurate fixation clustering is proposed. For the identified clusters, the random walk based method is utilized to extract the center of each cluster, which presents the essential part of an area of interest (AOI). The proposed approach addresses the problem of fixation overlapping in eye movement analytics. Compared with the state-of-the-arts methods, the proposed method shows superior performance for different eye tracking experiments.	affinity propagation;algorithm;cluster analysis;experiment;eye tracking;processor affinity;software propagation	Yingxue Zhang;Zhenzhong Chen	2016	2016 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2016.7805537	computer vision;simulation;eye tracking;computer science;machine learning;cluster analysis;random walk;affinity propagation;statistics	ML	-0.5083565761780936	-44.64692287860105	68522
831d6c4a12c41412ba17c1b22880ebbdbfe52c92	mr-dis: democratic instance selection for big data by mapreduce		Instance selection is a popular preprocessing task in knowledge discovery and data mining. Its purpose is to reduce the size of data sets maintaining their predictive capabilities. The usual emerging problem at this point is that these methods quite often suffer of high computational complexity, which becomes highly inconvenient for processing huge data sets. In this paper, a parallel implementation for the instance selection algorithm Democratic Instance Selection (DIS) is presented. The main advantages of the DIS algorithm turn out to be its computational complexity, linear in the number of instances, as well as its internal structure, intuitively parallelizable. The purpose of this paper is threefold: firstly, the design of the DIS algorithm by following the MapReduce model; secondly, its implementation in the popular big data framework Spark; and finally, its empirical comparison over large-scale data sets. The results show that the processing time is reduced in a linear manner as the number of Spark executors increases, what makes it suitable for big data applications. In addition, the algorithm is publicly accessible to the scientific community.	apache spark;big data;bitbucket;computational complexity theory;data mining;limbo;mapreduce;preprocessor;scalability;selection algorithm;windows firewall	Álvar Arnaiz-González;Alejandro González-Rogel;José-Francisco Díez-Pastor;Carlos López Nozal	2017	Progress in Artificial Intelligence	10.1007/s13748-017-0117-5	knowledge extraction;computer science;data mining;parallelizable manifold;artificial intelligence;machine learning;big data;computational complexity theory;spark (mathematics);data set;preprocessor;instance selection	ML	-3.3010018454504495	-39.220535982260216	68862
d181d849e66c37a564c2631e3d2b274a84dc547f	fuzzy clustering using linguistic-valued exponent		TÓM TẮT Bài báo này được thực hiện nhằm mục đích nghiên cứu tìm hiểu thuật toán phân cụm FCM và các ý tưởng cải tiến đã có; tiến hành phân tích và phát hiện những đặc điểm phù hợp trong thuật toán FCM có thể áp dụng được đại số gia tử một lý thuyết sử dụng đại số trong việc biểu diễn giá trị của các biến ngôn ngữ. Từ đó, đề xuất một hướng cải tiến mới, đó là sử dụng lý thuyết đại số gia tử vào trọng số mũ của thuật toán FCM và sau cùng là xây dựng cài đặt một thuật toán phân cụm mờ sử dụng đại số gia tử để có thể áp dụng giải quyết bài toán phân cụm trong các ứng dụng thực tế.	cluster analysis;fuzzy clustering;fuzzy cognitive map	Hung Thai Le;Khang Ding Tran;Hung Van Le	2015	CoRR		correlation clustering;theoretical computer science;machine learning;artificial intelligence;fuzzy clustering;cluster analysis;mathematics;exponent	NLP	2.7568714266626233	-41.07016530128064	68924
c053a6b2887dc0f4c05c6ad3f898806e9941a0b0	sensitivity versus accuracy in ensemble models of artificial neural networks from multi-objective evolutionary algorithms		This paper proposes a framework to obtain ensembles of classifiers from a Multi-objective Evolutionary Algorithm (MOEA), improving the restrictions imposed by two non-cooperative performance measures for multiclass problems: (1) the Correct Classification Rate or Accuracy (CCR) and, (2) the Minimum Sensitivity (MS) of all classes, i.e., the lowest percentage of examples correctly predicted as belonging to each class with respect to the total number of examples in the corresponding class. The proposed framework is based on collecting Pareto fronts of Artificial Neural Networks models for multiclass problems by the Memetic Pareto Evolutionary NSGA2 (MPENSGA2) algorithm, and it builds a new Pareto front (ensemble) from stored fronts. The ensemble built significantly improves the closeness to the optimum solutions and the diversity of the Pareto front. For verifying it, the performance of the new front obtained has been measured with the habitual use of weighting methodologies, such as Majority Voting, Simple Averaging and Winner Takes All. In addition to CCR and MS measures, three trade-off measures have been used to obtain the goodness of a Pareto front as a whole: Hyperarea, Laumanns’s Hyperarea (LAUMANNS) and Zitzler’s Spread (M3). The proposed framework can be adapted for any MOEA that aims to improve the compaction and diversity of its Pareto front, and whose fitness functions impose severe restrictions for multiclass problems.	artificial neural network;centrality;data compaction;ensemble forecasting;ensembles of classifiers;evolutionary algorithm;fitness function;level of measurement;moea framework;memetics;multiclass classification;pareto efficiency;verification and validation;weapon target assignment problem	Juan Carlos Fernández;Manuel Cruz-Ramírez;César Hervás-Martínez	2016	Neural Computing and Applications	10.1007/s00521-016-2781-y	mathematical optimization;artificial intelligence;machine learning;mathematics;statistics	AI	9.190398122438705	-41.765627208632225	68939
e34c2df90a92b8b5e9591cd3642420ab6e624959	a new hybrid approach for mining breast cancer pattern using discrete particle swarm optimization and statistical method	cause of death;statistical method;data mining;journal article;hybrid approach;particle swarm optimizer;computational complexity;classification rules;data mining algorithm;discrete particle swarm optimization;breast cancer	Breast cancer is one of the leading causes of death among the women in many parts of the world. In 2007, approximately 178,480 women in the United States have been found to have invasive breast cancer. In this paper, we have developed an efficient hybrid data mining approach to separate from a population of patients who have and who do not have breast cancer. The proposed data mining approach has consists of two phases. In first phase, the statistical method will be used to pre-process the data which can eliminate the insignificant features. It can reduce the computational complexity and speed up the data mining process. In second phase, we proposed a new data mining methodology which based on the fundamental concept of the standard particle swarm optimization (PSO) namely discrete PSO. This phase aimed at creating a novel PSO in which each particle was coded in positive integer numbers and has a feasible system structure. Based on the obtained results, our proposed DPSO can improve the accuracy to 98.71%, sensitivity to 100% and specificity to 98.21%. When compared with the previous research, the proposed hybrid approach shows the improvement in both accuracy and robustness. According to the high quality of our research results, the proposed DPSO data mining algorithm can be used as the reference for making decision in hospital and provide the reference for the researchers.	mathematical optimization;particle swarm optimization	Wei-Chang Yeh;Wei-Wen Chang;Yuk Ying Chung	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.10.004	multi-swarm optimization;artificial intelligence;cause of death;breast cancer;machine learning;data mining;computational complexity theory	Vision	5.164836622247175	-44.60021266081664	69115
436a65342618dc19d77250f7605b6c5fa67389b3	distributedfba.jl: high-level, high-performance flux balance analysis in julia	parallel computing;flux variability analysis;metabolic networks;computational modeling	MOTIVATION Flux balance analysis, and its variants, are widely used methods for predicting steady-state reaction rates in biochemical reaction networks. The exploration of high dimensional networks with such methods is currently hampered by software performance limitations.   RESULTS DistributedFBA.jl is a high-level, high-performance, open-source implementation of flux balance analysis in Julia. It is tailored to solve multiple flux balance analyses on a subset or all the reactions of large and huge-scale networks, on any number of threads or nodes.   AVAILABILITY The code is freely available on github.com/opencobra/COBRA.jl. The documentation can be found at opencobra.github.io/COBRA.jl.   CONTACT ronan.mt.fleming@gmail.com.	biochemical reaction;documentation;flux balance analysis;high- and low-level;julia;open-source software;software performance testing;steady state;subgroup	Laurent Heirendt;Ines Thiele;Ronan M. T. Fleming	2017		10.1093/bioinformatics/btw838	simulation;computer science;bioinformatics;theoretical computer science;distributed computing;computational model	HPC	-4.526352586958802	-51.96054953291745	69144
da8b94b7c4971aea5d84326fa27943fb6803aa0d	quick hierarchical biclustering on microarray gene expression data	graph theory;biology computing;pattern clustering;microarray gene expression;hierarchical graph quick hierarchical biclustering algorithm microarray gene expression data mining bioinformatics research inter bicluster relationship;gene expression data;data mining;quick hierarchical biclustering algorithm;genetics;inter bicluster relationship;pattern clustering biology computing data mining genetics graph theory;hierarchical graph;bioinformatics research;gene expression fluctuations fungi computer science bioinformatics shape partitioning algorithms dna time measurement clustering algorithms	Mining biclusters that exhibit both consistent trends and trends with similar degrees of fluctuations is vital to bioinformatics research. However; existing biclustering methods are not very efficient and effective at mining such biclusters. Moreover, few inter-bicluster relationships are delivered to biologists. In this paper, we introduce a quick hierarchical biclustering algorithm (QHB) to efficiently mine biclusters with both consistent trends and trends with similar degrees of fluctuations. Our QHB produces not only biclusters but also a hierarchical graph of inter-bicluster relationships. We experimented with the Yeast dataset and compared QHB against an existing biclustering scheme, DBF Our results show that QHB identifies biclusters with better quality. In addition, QHB shows the relationships among biclusters. Moreover compared with DBF, QHB is much more efficient and offers users a progressive way of bicluster exploration	algorithm;biclustering;bioinformatics;microarray;quantum fluctuation;dbase	Liping Ji;Kenneth Wei-Liang Mock;Kian-Lee Tan	2006	Sixth IEEE Symposium on BioInformatics and BioEngineering (BIBE'06)	10.1109/BIBE.2006.253323	computer science;bioinformatics;graph theory;data science;data mining;genetics;biclustering	Security	4.0784369296528915	-48.46086798718407	69496
2807eb1d7596c5414a8378c1a9c09c05fec58b33	sparse laplacian component analysis for internet traffic anomalies detection		We consider the problem of anomaly detection in network traffic. It is a challenging problem because of high-dimensional and noisy nature of network traffic. A popularly used technique is subspace analysis . In particular, subspace analysis aims to separate the high-dimensional space of traffic signals into disjoint subspaces corresponding to normal and anomalous network conditions. Principal component analysis (PCA) and its improvements have been applied for this analysis. In this work, we take a different approach to determine the subspaces, and propose to capture the essence of the data using the eigenvectors of graph Laplacian, which we refer as Laplacian components (LCs). Our main contribution is to propose a regression framework to compute LCs followed by its application in anomaly detection. This framework provides much flexibility in incorporating different properties into the LCs, notably LCs with sparse loadings, which we exploit in detail. In other words, our contribution is a new framework to compute the graph Fourier transform (GFT). The proposed framework enables sparse loadings and potentially other properties to be incorporated into the analysis components of GFT to suit different tasks. Furthermore, different from previous work that uses a sample graph to preserve local structure, we advocate modeling with a dual-input feature graph that encodes the correlation of the time series data and prior information. Therefore, the proposed model can readily incorporate the “physics” of some applications as prior information to improve the analysis. We perform experiments on volume anomaly detection using three real data sets. We demonstrate that the proposed model can correctly uncover the essential low-dimensional principal subspace containing the normal Internet traffic and achieve outstanding detection performance.	anomaly detection;experiment;exploit (computer security);laplacian matrix;network traffic control;principal component analysis;sparse matrix;time series	Manas Khatua;Seyed Hamid Safavi;Ngai-Man Cheung	2018	IEEE Transactions on Signal and Information Processing over Networks	10.1109/TSIPN.2018.2818950	linear subspace;time series;fourier transform;eigenvalues and eigenvectors;anomaly detection;laplacian matrix;principal component analysis;subspace topology;artificial intelligence;pattern recognition	ML	-1.6154276082040933	-42.974511268474714	69606
96553b4d2f6d69d283807200012b7080f3593a28	algorithms and statistical methods for exact motif discovery		The motif discovery problem consists of uncovering exceptional patterns (called motifs) in sets of sequences. It arises in molecular biology when searching for yet unknown functional sites in DNA sequences. In this thesis, we develop a motif discovery algorithm that (1) is exact, that means it returns a motif with optimal score, (2) can use the statistical significance with respect to complex background models as a scoring function, (3) takes into account the effects of self-overlaps of motif instances, and (4) is efficient enough to be useful in large-scale applications. To this end, several algorithms and statistical methods are developed. First, the concepts of deterministic arithmetic automata (DAAs) and probabilistic arithmetic automata (PAAs) are introduced. We prove that they allow calculating the distributions of values resulting from deterministic computations on random texts generated by arbitrary finite-memory text models. This technique is applied three times: first, to compute the distribution of the number of occurrences of a pattern in a random string, second, to compute the distribution of the number of character accesses made by windowbased pattern matching algorithms, and, third, to compute the distribution of clump sizes, where a clump is a maximal set of overlapping motif occurrences. All of these applications are interesting theoretical topics in themselves and, in all three cases, our results go beyond those known previously. In order to compute the distribution of the number of occurrences of a motif in a random text, a deterministic finite automaton (DFA) accepting the motif’s instances is needed to subsequently construct a PAA. We therefore address the problem of efficiently constructing minimal DFAs for motif types common in computational biology. We introduce simple non-deterministic finite automata (NFAs) and prove that these NFAs are transformed into minimal DFAs by the classical subset construction. We show that they can be built from (sets of) generalized strings and from consensus strings with a Hamming neighborhood, allowing the direct construction of minimal DFAs for these pattern types. As a contribution to the field of motif statistics, we derive a formula for the expected clump size of motifs. It is remarkably simple and does not involve laborious operations like matrix inversions. This formula plays an important role in developing bounds for the expected clump size of partially known motifs. Such bounds are needed to obtain bounds for the p-value of a partially known motif. Using these, we are finally able to devise a branch-and-bound algorithm for motif discovery that extracts provably optimal motifs with respect to their p-values in compound Poisson approximation. Markovian text models of arbitrary order can be used as a background model (or null model). The algorithm is further generalized to jointly handle a motif and its reverse complement. An Open Source implementation is publicly available as part of the MoSDi software	algorithm;amplifier;approximation;branch and bound;computation;computational biology;dfa minimization;deterministic finite automaton;finite-state machine;kolmogorov complexity;maximal set;moore neighborhood;nondeterministic finite automaton;null model;pattern matching;powerset construction;presburger arithmetic;sequence motif;text mining;window function	Tobias Marschall	2011			bioinformatics;machine learning;data mining;mathematics	Theory	-0.581274282895577	-51.393424527853334	69610
1096f01b383a7895f8817d89e4d8f834e7c5b4a1	lpmerge: an r package for merging genetic maps by linear programming		UNLABELLED Consensus genetic maps constructed from multiple populations are an important resource for both basic and applied research, including genome-wide association analysis, genome sequence assembly and studies of evolution. The LPmerge software uses linear programming to efficiently minimize the mean absolute error between the consensus map and the linkage maps from each population. This minimization is performed subject to linear inequality constraints that ensure the ordering of the markers in the linkage maps is preserved. When marker order is inconsistent between linkage maps, a minimum set of ordinal constraints is deleted to resolve the conflicts.   AVAILABILITY AND IMPLEMENTATION LPmerge is on CRAN at http://cran.r-project.org/web/packages/LPmerge.	approximation error;biologic preservation;conflict (psychology);genetic algorithm;linear inequality;linear programming;linkage (software);map;ordinal position;ordinal data;population;r language;sequence assembly;social inequality;genetic linkage	Jeffrey B. Endelman;Christophe Plomion	2014	Bioinformatics	10.1093/bioinformatics/btu091	mathematical optimization;bioinformatics;data mining;mathematics	Comp.	1.4330613410027693	-51.158759577602	69688
95c9f866289c626577a05b05167b78d866c02759	shadowed c-means: integrating fuzzy and rough clustering	three valued logic;ombre;fuzzy set;estudio comparativo;rough set theory;conjunto difuso;outlier;ensemble flou;cluster validity index;fuzzy sets;shadowed sets;algorithme;etude comparative;observacion aberrante;algorithm;fuzzy clustering;sombra;c means algorithm;shadow;theorie ensemble approximatif;model uncertainty;indexation;signal classification;comparative study;number of clusters;classification signal;observation aberrante;rough sets;cluster validity;classification automatique;rough set;automatic classification;clasificacion automatica;algoritmo	A new method of partitive clustering is developed in the framework of shadowed sets. The core and exclusion regions of the generated shadowed partitions result in a reduction in computations as compared to conventional fuzzy clustering. Unlike rough clustering, here the choice of threshold parameter is fully automated. The number of clusters is optimized in terms of various validity indices. It is observed that shadowed clustering can efficiently handle overlapping among clusters as well as model uncertainty in class boundaries. The algorithm is robust in the presence of outliers. A comparative study is made with related partitive approaches. Experimental results on synthetic as well as real data sets demonstrate the superiority of the proposed approach.	cluster analysis;rough set	Sushmita Mitra;Witold Pedrycz;Bishal Barman	2010	Pattern Recognition	10.1016/j.patcog.2009.09.029	correlation clustering;constrained clustering;data stream clustering;rough set;k-medians clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;statistics	Vision	1.918428117165715	-39.90062802011929	69785
99e2b52a2beeb825a433857e763d20e16be44e49	null qq plots: a simple graphical alternative to significance testing for the comparison of classifiers	comparative classification study null qq plots graphical alternative significance testing machine learning algorithms statistical significance tests ten benchmark data sets;data models testing machine learning accuracy sonar single photon emission computed tomography machine learning algorithms;statistical testing learning artificial intelligence pattern classification;pattern classification;statistical testing;learning artificial intelligence	The evaluation of machine learning algorithms is commonly based on statistical significance tests. However, the suitability of such tests is often questionable. We propose null QQ plots as a simple yet powerful graphical alternative to significance testing. Using ten benchmark data sets, we demonstrate that these plots concisely summarize the essential results from a comparative classification study, while they are easy to produce and interpret.	algorithm;benchmark (computing);characterization test;graphical user interface;machine learning;tencent qq	Daniel P. Berrar	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		statistical hypothesis testing;computer science;machine learning;data mining;statistics	SE	6.603406996393165	-44.67780108355246	69816
3e8f42b9c27184d1cba37cd2a71a944a5cbd1549	finding representative points in multivariate data using pca		The idea of representation has been used in various fields of study from data analysis to political science. In this paper, we define representativeness and describe a method to isolate data points that can represent the entire data set. Also, we show how the minimum set of representative data points can be generated. We use data from GLOBE (a project to study the effects on Land Change based on a set of parameters that include temperature, forest cover, human population, atmospheric parameters and many other variables) to test & validate the algorithm. Principal Component Analysis (PCA) is used to reduce the dimensions of the multivariate data set, so that the representative points can be generated efficiently and its Representativeness has been compared against Random Sampling of points from the data set.	algorithm;data point;principal component analysis;sampling (signal processing)	Ashwinkumar Ganesan;Tim Oates;Matthew D. Schmill	2016	CoRR		multidimensional analysis;computer science;data mining;statistics	ML	6.647868371969535	-41.76643335718931	69986
9b7739b7c88534de803a38b10df02cb652bf1beb	qdist-quartet distance between evolutionary trees	bioinformatique;evolutionary trees;bioinformatica;bioinformatics	SUMMARY QDist is a program for computing the quartet distance between two unrooted trees, i.e. the number of quartet topology differences between the trees, where a quartet topology is the topological subtree induced by four species. The program is based on an algorithm with running time O(n log2 n), which makes it practical to compare large trees. Available under GNU license.   AVAILABILITY http://www.birc.dk/Software/QDist	algorithm;anatomy, regional;binary logarithm;gnu;phylogenetic tree;quartet distance;time complexity;tree (data structure);trees (plant)	Thomas Mailund;Christian N. S. Pedersen	2004	Bioinformatics	10.1093/bioinformatics/bth097	biology;phylogenetic tree;computer science;bioinformatics;theoretical computer science;weight-balanced tree;algorithm	Comp.	0.22348283875979374	-50.37391049963987	70604
084a8a8ecf75948242e4101a2dd979f6429cd34b	the knockoff filter for fdr control in group-sparse and multitask regression		We propose the group knockoff filter, a method for false discovery rate control in a linear regression setting where the features are grouped, and we would like to select a set of relevant groups which have a nonzero effect on the response. By considering the set of true and false discoveries at the group level, this method gains power relative to sparse regression methods. We also apply our method to the multitask regression problem where multiple response variables share similar sparsity patterns across the set of possible features. We discussed various methods to handle partial missingness in some outcomes and potential high dimension setting. Empirically, the group knockoff filter successfully controls false discoveries at the group level in both settings, with substantially more discoveries made by leveraging the group structure.	computer multitasking;counterfeit consumer goods;missing data;sparse matrix	Ran Dai;Rina Barber	2016			econometrics;machine learning;mathematics;statistics	ML	7.375951553574673	-50.94250500986695	70627
165cf5e471b32122ba3a38709873cecf9b1b9a58	sda: software-defined accelerator for large-scale dnn systems	neural networks;accelerators;internet;mobile communication;algorithm design and analysis;biological neural networks;software defined networks	This article consists of a collection of slides from the author's conference presentation on the special features, system design and architectures, processing capabilities, and targeted markets for Baidu's family of software defined accelerator products (SDA) for large scale deep neural network (DNN) systems.	artificial neural network;deep learning;systems design	Jian Ouyang;Shiding Lin;Wei Qi;Yong Wang;Bo Yu;Song Jiang	2014	2014 IEEE Hot Chips 26 Symposium (HCS)	10.1109/HOTCHIPS.2014.7478821	speech recognition;computer science;artificial intelligence;world wide web	EDA	-2.441232590176473	-45.21302825941521	70643
b9303fdb72406b896e38ae9ccbf3864b6955ea26	on the design of populational clustering		The application of clustering algorithms for partitioning the population in evolutionary computation is discussed. Specific aspects which characterize this task lead to opportunities which can be explored by the clustering algorithm. A supervised clustering algorithm is described, which illustrates the exploration of those	cluster analysis;data validation;evolutionary algorithm;evolutionary computation;k-means clustering;supervised learning	Leonardo R. Emmendorfer	2010			cluster analysis;artificial intelligence;pattern recognition;computer science	ML	3.4224072609030745	-42.26487077774528	70698
5e31ed5e609d681374fc1c4bde5588a1d854ffa7	evolutionary multi-objective fault diagnosis of power transformers		This paper introduces a two step algorithm for fault diagnosis of power transformers (2-ADOPT) using a binary version of the multi-objective particle swarm optimization (MOPSO) algorithm. Feature subset selection and ensemble classifier selection are implemented to improve the diagnosing accuracy for dissolved gas analysis (DGA) of power transformers. First, the proposed method selects the most effective features in a multi objective framework and the optimum number of features, simultaneously, which are used as inputs to train classifiers in the next step. The input features are composed of DGA performed on the oil of power transformers along with the various ratios of these gases. In the second step, the most accurate and diverse classifiers are selected to create a classifier ensemble. Finally, the outputs of selected classifiers are combined using the Dempster-Shafer combination rule in order to determine the actual faults of power transformers. In addition, the obtained results of the proposed method are compared to three other scenarios: 1) multi-objective ensemble classifier selection without any feature selection step which takes all the features to train classifiers and then applies MOPSO algorithm to find the best ensemble of classifiers, 2) a well-known classifier ensemble technique called random forests, and 3) another powerful decision tree ensemble which is called oblique random forests. The comparison results were favourable to the proposed method and showed the high reliability of this method for power transformers fault classification.	decision tree;domain generation algorithm;ensemble learning;feature selection;mathematical optimization;oblique projection;particle swarm optimization;random forest;transformers	Abdolrahman Peimankar;Stephen John Weddell;Thahirah Jalal;Andrew Craig Lapthorn	2017	Swarm and Evolutionary Computation	10.1016/j.swevo.2017.03.005	random subspace method;feature selection;decision tree;multi-objective optimization;transformer;cascading classifiers;random forest;machine learning;particle swarm optimization;artificial intelligence;computer science;pattern recognition	AI	9.954342480745435	-41.0278233595048	70850
476e63c1f38d1ce8b19baa73552d5560c197ec17	extracting understandable 3d object groups with multiple similarity metrics		Some of the main difficulties involved in the clustering problem are the interpretation of the clusters and the choice of the number of clusters. The imposition of a complete clustering, in which all the objects must be classified might lead to incoherent and not convincing groups. In this paper we present an approach which alleviates this problem by proposing incomplete but reliable clustering strategies. The method is based on two pillars: using a set of different metrics which are evaluated through a clustering confidence measure and achieving a hard/soft clustering consensus. This method is particularly ad- dressed to 3D shape grouping in which the objects are represented through geometric features defined over mesh models. Our approach has been tested using eight metrics defined on geometrical descriptors in a collection of free- shape objects. The results show that in all cases the algorithm yields coherent and meaningful groups for several numbers of clusters. The clustering strategy here proposed might be useful for future developments in the unsupervised grouping field.		Antonio Adán;Miguel Adán	2012		10.1007/978-3-642-33275-3_22	correlation clustering;constrained clustering;fuzzy clustering;flame clustering;theoretical computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering;statistics	HCI	1.23228378766419	-42.73948181064722	70914
dbe94ec07caad366d1eb22263608a834134e1aaa	a semi-supervised approach to estimate the number of clusters per class	medical diagnosis problems semisupervised learning approach cluster per class estimation unlabeled data semisupervised clustering algorithm multiple cluster per class k means algorithm mcck pair wise constraints class labels one cluster per class assumption structure learning disease subtype identification;pattern clustering;clustering algorithms prototypes partitioning algorithms algorithm design and analysis breast cancer ionosphere;constrained clustering;semi supervised learning;semi supervised learning constrained clustering;pattern clustering diseases learning artificial intelligence medical diagnostic computing;diseases;learning artificial intelligence;medical diagnostic computing	The disparity between the available amount of unlabeled and labeled data in several applications made semi-supervised learning become an active research topic. Most studies on semi-supervised clustering assume that the number of classes is equal to the number of clusters. This paper introduces a semi-supervised clustering algorithm, named Multiple Clusters per Class k-means (MCCK), which estimates the number of clusters per class via pair wise constraints generated from class labels. Experiments with eight datasets indicate that the algorithm outperforms three traditional algorithms for semi-supervised clustering, especially when the one-cluster-per-class assumption does not hold. Finally, the learned structure can offer a valuable description of the data in several applications. For instance, it can aid the identification of subtypes of diseases in medical diagnosis problems.	algorithm;binocular disparity;cluster analysis;constrained clustering;experiment;k-means clustering;mixture model;semi-supervised learning;semiconductor industry;supervised learning;usability	Davidson M. Sestaro;Thiago F. Covoes;Eduardo R. Hruschka	2012	2012 Brazilian Symposium on Neural Networks	10.1109/SBRN.2012.31	semi-supervised learning;correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;brown clustering;affinity propagation;k-means clustering;clustering high-dimensional data;conceptual clustering	ML	8.110032048040113	-40.61723542488301	70970
8edf68c4b3c9b1650562eb373a6284b33e154ded	a regression-based k nearest neighbor algorithm for gene function prediction from heterogeneous data	similarity metric;escherichia coli;probability;heterogeneous data;data type;feature space;probabilistic inference;computational biology bioinformatics;models genetic;cluster analysis;functional analysis;functional genomics;genome bacterial;gene expression regulation;feature weighting;reproducibility of results;escherichia coli proteins;prediction accuracy;artificial intelligence;algorithms;k nearest neighbor;regression analysis;pattern recognition automated;support vector machine;neural networks computer;combinatorial libraries;computational biology;gene function;genes bacterial;computer appl in life sciences;similarity measure;computer simulation;oligonucleotide array sequence analysis;sequence analysis protein;genome sequence;microarrays;bioinformatics;heterogeneous data sources	As a variety of functional genomic and proteomic techniques become available, there is an increasing need for functional analysis methodologies that integrate heterogeneous data sources. In this paper, we address this issue by proposing a general framework for gene function prediction based on the k-nearest-neighbor (KNN) algorithm. The choice of KNN is motivated by its simplicity, flexibility to incorporate different data types and adaptability to irregular feature spaces. A weakness of traditional KNN methods, especially when handling heterogeneous data, is that performance is subject to the often ad hoc choice of similarity metric. To address this weakness, we apply regression methods to infer a similarity metric as a weighted combination of a set of base similarity measures, which helps to locate the neighbors that are most likely to be in the same class as the target gene. We also suggest a novel voting scheme to generate confidence scores that estimate the accuracy of predictions. The method gracefully extends to multi-way classification problems. We apply this technique to gene function prediction according to three well-known Escherichia coli classification schemes suggested by biologists, using information derived from microarray and genome sequencing data. We demonstrate that our algorithm dramatically outperforms the naive KNN methods and is competitive with support vector machine (SVM) algorithms for integrating heterogenous data. We also show that by combining different data sources, prediction accuracy can improve significantly. Our extension of KNN with automatic feature weighting, multi-class prediction, and probabilistic inference, enhance prediction accuracy significantly while remaining efficient, intuitive and flexible. This general framework can also be applied to similar classification problems involving heterogeneous datasets.	gene prediction;genetic heterogeneity;graceful exit;handling (psychology);hoc (programming language);inference;k-nearest neighbors algorithm;microarray;numerous;proteomics;single linkage cluster analysis;support vector machine;whole genome sequencing;gene function	Zizhen Yao;Walter L. Ruzzo	2006	BMC Bioinformatics	10.1186/1471-2105-7-S1-S11	computer simulation;functional genomics;functional analysis;support vector machine;whole genome sequencing;regulation of gene expression;dna microarray;feature vector;data type;computer science;bioinformatics;machine learning;probability;data mining;cluster analysis;escherichia coli;k-nearest neighbors algorithm;regression analysis	ML	7.538578296072623	-49.11473254915286	71522
7b5983bf0a0cf5cdc7a9eb561dc9306ac58beb87	improving the accuracy of classifiers for the prediction of translation initiation sites in genomic sequences	modelizacion;generic algorithm;translation initiation site;classification;modelisation;genome;prediction accuracy;feature selection;genoma;modeling;clasificacion;genome sequence	The prediction of the Translation Initiation Site (TIS) in a genomic sequence is an important issue in biological research. Although several methods have been proposed to deal with this problem, there is a great potential for the improvement of the accuracy of these methods. Due to various reasons, including noise in the data as well as biological reasons, TIS prediction is still an open problem and definitely not a trivial task. In this paper we follow a three-step approach in order to increase TIS prediction accuracy. In the first step, we use a feature generation algorithm we developed. In the second step, all the candidate features, including some new ones generated by our algorithm, are ranked according to their impact to the accuracy of the prediction. Finally, in the third step, a classification model is built using a number of the top ranked features. We experiment with various feature sets, feature selection methods and classification algorithms, compare with alternative methods, draw important conclusions and propose improved models with respect to prediction accuracy.	algorithm;feature selection	George Tzanis;Christos Berberidis;Anastasia Alexandridou;Ioannis P. Vlahavas	2005		10.1007/11573036_40	whole genome sequencing;systems modeling;genetic algorithm;biological classification;computer science;bioinformatics;machine learning;pattern recognition;data mining;feature selection;genome	ML	8.269557412749023	-49.22570795330058	71614
ca65db71a58357f9e8a153ca209f10ccbf15f01d	a refined and heuristic algorithm for ld tagsnps selection	tagsnps selection;biology computing;genomics;genome wide association study;greedy algorithms;greedy algorithms algorithm design and analysis bioinformatics partitioning algorithms genomics couplings educational institutions;greedy algorithms biology computing genomics;tagsnps selection snps ld;snps;hapmap project ld tagsnp selection algorithm single nucleotide polymorphisms genome wide association studies heuristic algorithm htag;heuristic algorithm;ld;single nucleotide polymorphism	Single Nucleotide Polymorphisms (SNPs) play an important role in Genome-wide Association Studies. To reduce genotyping costs, several LD tagSNPs selection algorithms have been proposed. In this paper, the advantages and disadvantages of current LD tagSNPs selection algorithms are analyzed. And a refined and heuristic algorithm HTag for LD tagSNPs selection is developed: (1) The tagSNPs selection procedure of Xu et al. is modified to improve selection performance. (2) A strategy to optimize the selection result is proposed. Using data downloaded from the Hap Map Project, the performance of these methods is evaluated and our algorithm shows improvements in tagging efficiency.	heuristic (computer science);international hapmap project;selection algorithm	Hailin Chen;Zuping Zhang;Jing Xia	2011	2011IEEE 10th International Conference on Trust, Security and Privacy in Computing and Communications	10.1109/TrustCom.2011.234	biology;bioinformatics;machine learning;genetics	SE	3.3239539765853765	-48.19097984293957	71733
db7dc701202a63f03f5bbc971c08cda2df892f0e	phylogenetic network analysis as a parsimony optimization problem	phylogeny;computational biology bioinformatics;proteins;algorithms;influenza a virus h1n1 subtype;neural networks computer;computational biology;computer appl in life sciences;microarrays;bioinformatics	Many problems in comparative biology are, or are thought to be, best expressed as phylogenetic “networks” as opposed to trees. In trees, vertices may have only a single parent (ancestor), while networks allow for multiple parent vertices. There are two main interpretive types of networks, “softwired” and “hardwired.” The parsimony cost of hardwired networks is based on all changes over all edges, hence must be greater than or equal to the best tree cost contained (“displayed”) by the network. This is in contrast to softwired, where each character follows the lowest parsimony cost tree displayed by the network, resulting in costs which are less than or equal to the best display tree. Neither situation is ideal since hard-wired networks are not generally biologically attractive (since individual heritable characters can have more than one parent) and softwired networks can be trivially optimized (containing the best tree for each character). Furthermore, given the alternate cost scenarios of trees and these two flavors of networks, hypothesis testing among these explanatory scenarios is impossible. A network cost adjustment (penalty) is proposed to allow phylogenetic trees and soft-wired phylogenetic networks to compete equally on a parsimony optimality basis. This cost is demonstrated for several real and simulated datasets. In each case, the favored graph representation (tree or network) matched expectation or simulation scenario. The softwired network cost regime proposed here presents a quantitative criterion for an optimality-based search procedure where trees and networks can participate in hypothesis testing simultaneously.	contain (action);graph (abstract data type);mathematical optimization;maximum parsimony (phylogenetics);occam's razor;optimization problem;personality character;phylogenesis;phylogenetic network;phylogenetic tree;phylogenetics;simulation;trees (plant);vertex (geometry);explanation	Ward C. Wheeler	2015		10.1186/s12859-015-0675-0	biology;dna microarray;computer science;bioinformatics;artificial intelligence;tree rearrangement;genetics;algorithm;phylogenetics	Comp.	1.8130375223923878	-52.0325447940468	71880
538f9c653663c77948bf7ac6b734ca959fa30eae	a comparative study of filter-based feature ranking techniques	software metrics;software;feature selection techniques;measurement;support vector machines;selected works;filter based feature ranking techniques;niobium;software systems;software metrics learning artificial intelligence pattern classification;performance metric;radio frequency;machine learning;training data set;pattern classification;analysis of variance;software algorithms;bepress;feature selection;learning artificial intelligence;classification accuracy;performance metrics;feature selection techniques filter based feature ranking techniques machine learning training data set classification models performance metrics;classification models;measurement radio frequency support vector machines niobium software analysis of variance software algorithms;imbalanced data sets	One factor that affects the success of machine learning is the presence of irrelevant or redundant information in the training data set. Filter-based feature ranking techniques (rankers) rank the features according to their relevance to the target attribute and we choose the most relevant features to build classification models subsequently. In order to evaluate the effectiveness of different feature ranking techniques, a commonly used method is to assess the classification performance of models built with the respective selected feature subsets in terms of a given performance metric (e.g., classification accuracy or misclassification rate). Since a given performance metric usually can capture only one specific aspect of the classification performance, it may be unable to evaluate the classification performance from different perspectives. Also, there is no general consensus among researchers and practitioners regarding which performance metrics should be used for evaluating classification performance. In this study, we investigated six filter-based feature ranking techniques and built classification models using five different classifiers. The models were evaluated using eight different performance metrics. All experiments were conducted on four imbalanced data sets from a telecommunications software system. The experimental results demonstrate that the choice of a performance metric may significantly influence the classification evaluation conclusion. For example, one ranker may outperform another when using a given performance metric, but for a different performance metric the results may be reversed. In this study, we have found five distinct patterns when utilizing eight performance metrics to order six feature selection techniques.	experiment;feature selection;location-based service;machine learning;relevance;software performance testing;software system;test set	Huanjing Wang;Taghi M. Khoshgoftaar;Kehan Gao	2010	2010 IEEE International Conference on Information Reuse & Integration	10.1109/IRI.2010.5558966	support vector machine;niobium;analysis of variance;computer science;machine learning;pattern recognition;data mining;feature selection;radio frequency;software metric;measurement;software system	SE	7.111835673584287	-40.57448448725904	72088
c49120ca2670f8ce827f6bc7961c0daa01734353	weighted fuzzy-possibilistic c-means over large data sets	large data set;c means;weighted cluster;weighted fuzzy possibilitic c means;fuzzy c means;fuzzy possibilitic	Up to now, several algorithms for clustering large data sets have been presented. Most clustering approaches for data sets are the crisp ones, which cannot be well suitable to the fuzzy case. In this paper, the authors explore a single pass approach to fuzzy possibilistic clustering over large data set. The basic idea of the proposed approach (weighted fuzzy-possibilistic c-means, WFPCM) is to use a modified possibilistic c-means (PCM) algorithm to cluster the weighted data points and centroids with one data segment as a unit. Experimental results on both synthetic and real data sets show that WFPCM can save significant memory usage when comparing with the fuzzy c-means (FCM) algorithm and the possibilistic c-means (PCM) algorithm. Furthermore, the proposed algorithm is of an excellent immunity to noise and can avoid splitting or merging the exact clusters into some inaccurate clusters, and ensures the integrity and purity of the natural classes. DOI: 10.4018/jdwm.2012100104 International Journal of Data Warehousing and Mining, 8(4), 82-107, October-December 2012 83 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. samples by a certain rule such as chisquare or divergence hypothesis (Hathaway et al., 2006). The incremental approaches (Bradley et al., 1998; Farnstrom et al., 2000; Gupta et al., 2004; Karkkainen et al., 2007; Luhr et al., 2009; Nguyen-Hoang et al., 2009; Ning et al., 2009; O’Callaghan et al., 2002; Ramakrishnan et al., 1996; Siddiqui et al., 2009; Wan et al., 2010, 2011) generally maintain past knowledge from the previous runs of a clustering algorithm to produce or improve the future clustering model. Nevertheless, as Hore et al. (2007) pointed out, many existing algorithms for large and very large data sets are used for the crisp case, rarely for the fuzzy case. This is because fuzzy cluster needs to perform repeatedly the clustering iterations until the optimal solution or the acceptable approximate optimal solution is gained, and scan repeatedly the data set. This may greatly conflicts with the requirement of processing algorithm for large data set. Kwok, Smith, Lozano, and Taniar (2002) clustered insurance data set with an parallel fuzzy c-means (PFCM) clustering method. Hore, Hall, and Goldgof (2007) presented a single pass fuzzy c-means algorithm (SP) for clustering large data set, since FCM has innate sensitive dependence on noises, while in large data set, noises usually are unavoidable, and thus PFCM and SP have considerable trouble in noisy environments. The possibilistic clustering algorithm was presented by Krishnapuram and Keller (1993, 1996). In this clustering structure, each cluster is disentangled from the others and the membership values are represented as the typicality of the point to the class prototypes. Actually, the possibilistic clustering algorithm leads to higher noise immunity with respect to classical algorithms derived from Bezdek’s fuzzy c-means (FCM) (Bezdek, 1981), but it is sensitive to the initialization (Xie et al., 2008). In this paper, we propose a weighted fuzzypossibilistic c-means (WFPCM) algorithm for large or very large data sets. WFPCM can produce an excellent clustering result in a single pass through the data sets with finite memory allocated. The rest of this paper is organized as follows. Section “Fuzzy c-means (FCM) algorithm” surveys FCM algorithm. Section “Possibilistic c-means (PCM) algorithm” does the same for PCM. In section “Weighted fuzzypossibilistic c-means (WFPCM) algorithm,” we present the new algorithm. The experimental results on both synthetic and real data sets are reported in section “Empirical results and evaluation.” Finally, we make our conclusions in section “Conclusions.” FUZZY C-MEANS (FCM) ALGORITHM The task of fuzzy c-means (FCM) algorithm (Bezdek, 1981) is to minimize the objective function J U v u d m ij m ij j n	approximation algorithm;cluster analysis;data point;data segment;david w. bradley;fuzzy clustering;fuzzy cognitive map;iteration;loss function;optimization problem;pure function;synthetic intelligence;word lists by frequency;x image extension	Renxia Wan;Yuelin Gao;Caixia Li	2012	IJDWM	10.4018/jdwm.2012100104	defuzzification;fuzzy clustering;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;machine learning;data mining;fuzzy set;fuzzy set operations;algorithm	AI	2.8893645853327445	-38.76679266037141	72161
36708bbc473ec0b07752782de8a4d00f903eec3d	gene functional classification from heterogeneous data	learning algorithm;information retrieval;prior information;kernel function;heterogeneous data;prosite;pattern matching;bit parallelism;feature selection;support vector machine;dna microarray;computational biology;gene function;genome sequence	In our attempts to understand cellular function at the molecular level, we must be able to synthesize information from disparate types of genomic data. We consider the problem of inferring gene functional classifications from a heterogeneous data set consisting of DNA microarray expression measurements and phylogenetic profiles from whole-genome sequence comparisons. We demonstrate the application of the support vector machine (SVM) learning algorithm to this functional inference task. Our results suggest the importance of exploiting prior information about the heterogeneity of the data. In particular, we propose an SVM kernel function that is explicitly heterogeneous. We also show how to use knowledge about heterogeneity to aid in feature selection.	algorithm;dna microarray;feature selection;phylogenetics;support vector machine	Paul Pavlidis;Jason Weston;Jinsong Cai;William Noble Grundy	2001		10.1145/369133.369228	kernel;support vector machine;whole genome sequencing;dna microarray;prosite;computer science;bioinformatics;machine learning;pattern matching;pattern recognition;feature selection	ML	9.015236016178187	-51.98140825611966	72335
c0a811694785668fc89c3abfb76524cb89c526e5	an efficient ant colony algorithm for dna motif finding		Finding motifs in gene sequences is one of the most important prob- lems of bioinformatics and belongs to NP-hard type. This paper proposes a new ant colony optimization algorithm based on consensus approach, in which a relax tech- nique is applied to find the location of the motif. The efficiency of the algorithm is evaluated by comparing it with the state-of-the-art algorithms.	algorithm;ant colony optimization algorithms;motif	Hoang X. Huan;Duong T. A. Tuyet;Doan T. T. Ha;Nguyen T. Hung	2014		10.1007/978-3-319-11680-8_47	ant colony optimization algorithms;bioinformatics;artificial intelligence;machine learning	Logic	-0.038007232711572136	-49.90205508140394	72503
605b4d870a84b3dc5ba457aed23e3a05f4054149	compassionately conservative balanced cuts for image segmentation		The Normalized Cut (NCut) objective function, widely used in data clustering and image segmentation, quantifies the cost of graph partitioning in a way that biases clusters or segments that are balanced towards having lower values than unbalanced partitionings. However, this bias is so strong that it avoids any singleton partitions, even when vertices are very weakly connected to the rest of the graph. Motivated by the BÃ¼hler-Hein family of balanced cut costs, we propose the family of Compassionately Conservative Balanced (CCB) Cut costs, which are indexed by a parameter that can be used to strike a compromise between the desire to avoid too many singleton partitions and the notion that all partitions should be balanced. We show that CCB-Cut minimization can be relaxed into an orthogonally constrained lt-minimization problem that coincides with the problem of computing Piecewise Flat Embeddings (PFE) for one particular index value, and we present an algorithm for solving the relaxed problem by iteratively minimizing a sequence of reweighted Rayleigh quotients (IRRQ). Using images from the BSDS500 database, we show that image segmentation based on CCB-Cut minimization provides better accuracy with respect to ground truth and greater variability in region size than NCut-based image segmentation.		Nathan D. Cahill;Tyler L. Hayes;Renee T. Meinhold;John F. Hamilton	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00181	piecewise;artificial intelligence;normalization (statistics);cluster analysis;pattern recognition;image segmentation;graph partition;computer science;linear programming;singleton;compromise	Vision	4.342070220635329	-39.986972841490314	72512
3d7fbfa73555c33e097f2be51a7ab499b7f7b8c2	inducing nnc-trees quickly	decision tree;decision trees prototypes databases neural networks nearest neighbor searches classification tree analysis computational efficiency neck cybernetics machine learning algorithms;decision rules;pattern classification;pattern classification decision trees;computational cost nnc tree decision tree nearest neighbor classifier non terminal node decision rules;computational cost;non terminal node;nearest neighbor classifier;decision trees;nnc tree;decision rule	An NNC-tree is a decision tree (DT) with each non-terminal node containing a nearest neighbor classifier (NNC). Compared with the axis-parallel decision trees (APDTs), NNC-trees are more comprehensible for large problems, because the decision rules corresponding to the trees are simpler. Currently, the author has proposed an algorithm for inducing NNC-trees based on the R4-rule. However, compared with C4.5, which is a popular program for inducing APDTs, the computation of our algorithm is relatively expensive. This paper proposes two methods for reducing the computational cost. The efficiency of the proposed methods is verified through experiments on three public databases.	algorithmic efficiency;apache axis;c4.5 algorithm;computation;database;decision tree;experiment;nearest neighbour algorithm	Qiangfu Zhao	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.385295	ball tree;influence diagram;decision tree model;decision tree learning;computer science;machine learning;decision tree;pattern recognition;alternating decision tree;incremental decision tree;data mining;decision rule;cover tree;id3 algorithm;decision stump	Robotics	9.659915625760984	-38.89415520973781	72549
4f67d12086eec6cabef4f85edec917d16fe990ba	l-sme: a system for mining loosely structured motifs	genome-wide application;standard exact search;discovery method;intuitive graphical interface;motifs structure;earlier motif discovery system;boxes swap;detailed description;complex motif;genome-wide search	"""Singling out the regions that are over-represented in suitably selected sets of DNA sequences provides us with insights on the biological functions played by the corresponding macromolecules. These regions are called motifs in the literature. A motif templatê p is a tuple l A pattern instance p forˆp is a string b l 1 X (d 1)b l 2 X (d 2). .. b l r −1 X (d r −1)b lr b l i is a string with length in the range [min _l i , max _l i ] X (d j) is a sequence of """" don't care """" symbols with length in the range [min _d j , max _d j ] A pattern instance p occurs in a DNA sequence s if there is a substring s of s if s matches p The motif discovery problem over a set of DNA sequences is to find all the instances forˆp that occur in at least Q of them L-SME is a tool for motif discovery supporting various innovative functionalities, under various different perspectives L-SME allows the user to specify any kind of model template L-SME deals with other relevant variabilities in pattern matching, in particular, it supports both Hamming and Levenshtein distance box skips: a user-definable number of boxes is not matched at all box swaps: a user-definable number of inversions between adjacent boxes Fassetti & Greco & Terracina L-SME: a system for mining loosely structured motifs"""	don't-care term;levenshtein distance;maxima and minima;pattern matching;sequence motif;string (computer science);substring;transformation between distributions in time–frequency analysis;window function	Fabio Fassetti;Gianluigi Greco;Giorgio Terracina	2011		10.1007/978-3-642-23808-6_42	computer science;bioinformatics;data mining;world wide web	Theory	-1.4639283868531487	-51.670420303970594	72599
76e41f802c6a32a03d16d9bf0983c6d5521118f5	a hybrid approach to speed-up the k-means clustering method		k-means clustering method is an iterative partition-based method which for finite data-sets converges to a solution in a finite time. The running time of this method grows linearly with respect to the size of the data-set. Many variants have been proposed to speed-up the conventional k-means clustering method. In this paper, we propose a prototype-based hybrid approach to speed-up the k-means clustering method. The proposed method, first partitions the data-set into small clusters (grouplets), which are of varying sizes. Each grouplet is represented by a prototype. Later, the set of prototypes is partitioned into k clusters using the modified k-means method. The modified k-means clustering method is similar to the conventional k-means method but it avoids empty clusters (the clusters to which no pattern is assigned) in the iterative process. In each cluster of prototypes, each prototype is replaced by its corresponding set of patterns (which formed the grouplet) to derive a partition of the data-set. Since this partition of the data-set can deviate from the partition obtained using the conventional k-means method over the entire data-set, a correcting step is proposed. Both theoretically and experimentally, the conventional k-means method and the proposed hybrid method (augmented with the correcting step) are shown to yield the same result (provided, the initial k seed points are same). But, the proposed method is much faster than the conventional one. Experimentally, the proposed method is compared with the conventional method and the other recent methods that are proposed to speed-up the k-means method.	cluster analysis;computation;experiment;iteration;iterative method;k-means clustering;newton's method;prototype;time complexity;viz: the computer game	T. Hitendra Sarma;P. Viswanath;B. Eswara Reddy	2013	Int. J. Machine Learning & Cybernetics	10.1007/s13042-012-0079-7	mathematical optimization;discrete mathematics;mathematics;algorithm	AI	4.110646380959865	-41.335041642755385	73036
257d8efa9fcd5e4f3b9c8d4f6f7dedf4a067dd9c	privacy-preserving data mining: a feature set partitioning approach	k anonymity;set partitions;data mining;feature set partitioning;multiobjective optimization;genetic algorithm;genetic algorithms;classification accuracy;privacy;privacy preserving data mining	In privacy-preserving data mining (PPDM), a widely used method for achieving data mining goals while preserving privacy is based on k-anonymity. This method, which protects subject-specific sensitive data by anonymizing it before it is released for data mining, demands that every tuple in the released table should be indistinguishable from no fewer than k subjects. The most common approach for achieving compliance with k-anonymity is to replace certain values with less specific but semantically consistent values. In this paper we propose a different approach for achieving k-anonymity by partitioning the original dataset into several projections such that each one of them adheres to k-anonymity. Moreover, any attempt to rejoin the projections, results in a table that still complies with k-anonymity. A classifier is trained on each projection and subsequently, an unlabelled instance is classified by combining the classifications of all classifiers. Guided by classification accuracy and k-anonymity constraints, the proposed data mining privacy by decomposition (DMPD) algorithm uses a genetic algorithm to search for optimal feature set partitioning. Ten separate datasets were evaluated with DMPD in order to compare its classification performance with other k-anonymity-based methods. The results suggest that DMPD performs better than existing k-anonymity-based algorithms and there is no necessity for applying domain dependent knowledge. Using multiobjective optimization methods, we also examine the tradeoff between the two conflicting objectives in PPDM: privacy and predictive performance. 2010 Elsevier Inc. All rights reserved.	data mining;genetic algorithm;mathematical optimization;multi-objective optimization;privacy;statistical classification	Nissim Matatov;Lior Rokach;Oded Maimon	2010	Inf. Sci.	10.1016/j.ins.2010.03.011	genetic algorithm;computer science;artificial intelligence;machine learning;pattern recognition;data mining	ML	9.243340107022949	-43.13297974231521	73130
2d0e301cb423bceca7bfa6a424dacebf0b5a7c4e	a new sequential covering strategy for inducing classification rules with ant colony algorithms	q335 artificial intelligence;ant colony optimisation;data mining;sequential covering ant colony optimization classification data mining rule induction;state of the art rule induction classification algorithms sequential covering strategy classification rules ant colony optimization algorithms rule interaction aco classification algorithms pheromone values candidate rule list quality;training prediction algorithms predictive models classification algorithms ant colony optimization accuracy computational modeling;pattern classification;pattern classification ant colony optimisation data mining	Ant colony optimization (ACO) algorithms have been successfully applied to discover a list of classification rules. In general, these algorithms follow a sequential covering strategy, where a single rule is discovered at each iteration of the algorithm in order to build a list of rules. The sequential covering strategy has the drawback of not coping with the problem of rule interaction, i.e., the outcome of a rule affects the rules that can be discovered subsequently since the search space is modified due to the removal of examples covered by previous rules. This paper proposes a new sequential covering strategy for ACO classification algorithms to mitigate the problem of rule interaction, where the order of the rules is implicitly encoded as pheromone values and the search is guided by the quality of a candidate list of rules. Our experiments using 18 publicly available data sets show that the predictive accuracy obtained by a new ACO classification algorithm implementing the proposed sequential covering strategy is statistically significantly higher than the predictive accuracy of state-of-the-art rule induction classification algorithms.	algorithm;ant colony optimization algorithms;binary file;documentation;download;experiment;heuristic;iteration;mathematical optimization;rule induction;sorting;sourceforge	Fernando E. B. Otero;Alex Alves Freitas;Colin G. Johnson	2013	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2012.2185846	computer science;machine learning;pattern recognition;data mining	ML	9.170572215495973	-43.042517700899154	73273
2cf5b6359b26fa708e10baab902d17bf4904c981	a novel clustering technique based on improved noising method	cluster algorithm;analyse amas;partition method;image processing;time complexity;procesamiento imagen;classification;traitement image;complexite temps;cluster analysis;methode partition;sum of squares;pattern recognition;analyse non convexe;analisis cluster;metodo particion;reconnaissance forme;non convex analysis;reconocimiento patron;complejidad tiempo;clasificacion;analisis no convexo	In this article, the clustering problem under the criterion of minimum sum of squares clustering is considered. It is known that this problem is a nonconvex program which possesses many locally optimal values, resulting that its solution often falls into these traps. To explore the proper result, a novel clustering technique based on improved noising method called INMC is developed, in which one-step DHB algorithm as the local improvement operation is integrated into the algorithm framework to fine-tune the clustering solution obtained in the process of iterations. Moreover, a new method for creating the neighboring solution of the noising method called mergence and partition operation is designed and analyzed in detail. Compared with two noising method based clustering algorithms recently reported, the proposed algorithm greatly improves the performance without the increase of the time complexity, which is extensively demonstrated for experimental data sets.		Yongguo Liu;Wei Zhang;Dong Zheng;Kefei Chen	2005		10.1007/11578079_9	time complexity;correlation clustering;combinatorics;data stream clustering;image processing;biological classification;computer science;artificial intelligence;canopy clustering algorithm;machine learning;cure data clustering algorithm;mathematics;explained sum of squares;cluster analysis;algorithm	EDA	2.6654592426672985	-41.568649097797596	73374
c27933821ced6f5f9a201667e6e844af2beea41a	increasing coverage to improve detection of network and host anomalies	anomaly detection;intrusion detection;rule learning;hybrid approach;rule weighting;rule pruning;rule replacement	For intrusion detection, the LERAD algorithm learns a succinct set of comprehensible rules for detecting anomalies, which could be novel attacks. LERAD validates the learned rules on a separate held-out validation set and removes rules that cause false alarms. However, removing rules with possible high coverage can lead to missed detections. We propose three techniques for increasing coverage—Weighting, Replacement and Hybrid. Weighting retains previously pruned rules and associate weights to them. Replacement, on the other hand, substitutes pruned rules with other candidate rules to ensure high coverage. We also present a Hybrid approach that selects between the two techniques based on training data coverage. Empirical results from seven data sets indicate that, for LERAD, increasing coverage by Weighting, Replacement and Hybrid detects more attacks than Pruning with minimal computational overhead.	algorithm;intrusion detection system;overhead (computing);sensor	Gaurav Tandon;Philip K. Chan	2009	Machine Learning	10.1007/s10994-009-5145-3	intrusion detection system;anomaly detection;computer science;machine learning;pattern recognition;data mining	AI	6.825815309965813	-39.60496424148026	73638
1378da04ed1059e19898820cb27992348fe0c665	local optimization for global alignment of protein interaction networks	traveling salesman problem;evolutionary history;evolutionary dynamics;np hard problem;network structure;protein interaction network	We propose a novel algorithm, PISwap, for computing global pairwise alignments of protein interaction networks, based on a local optimization heuristic that has previously demonstrated its effectiveness for a variety of other NP-hard problems, such as the Traveling Salesman Problem. Our algorithm begins with a sequence-based network alignment and then iteratively adjusts the alignment by incorporating network structure information. It has a worst-case pseudo-polynomial running-time bound and is very efficient in practice. It is shown to produce improved alignments in several well-studied cases. In addition, the flexible nature of this algorithm makes it suitable for different applications of network alignments. Finally, this algorithm can yield interesting insights into the evolutionary history of the compared species.	algorithm;alignment;best, worst and average case;computation (action);heuristic;local search (optimization);mathematical optimization;np-hardness;polynomial;pseudo brand of pseudoephedrine;pseudo-polynomial time;travelling salesman problem;protein protein interaction	Leonid Chindelevitch;Chung-Shou Liao;Bonnie Berger	2010	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		biology;2-opt;mathematical optimization;cross-entropy method;computer science;bioinformatics;machine learning;np-hard;mathematics;evolutionary dynamics;travelling salesman problem;genetics;3-opt;bottleneck traveling salesman problem	Comp.	-0.09148791839279696	-50.18856885928299	73719
97b3ea9f103a53b148c55de025b0c65eb470e66a	ayesian automatic relevance determination algorithms for classifying gene expression data	gene expression data;gene expression;cdna microarray;automatic relevance determination;feature selection	Motivation: We investigate two new Bayesian classification algorithms incorporating feature selection. These algorithms are applied to the classification of gene expression data derived from cDNA microarrays. Results: We demonstrate the effectiveness of the algorithms on three gene expression datasets for cancer, showing they compare well with alternative kernel-based techniques. By automatically incorporating feature selection, accurate classifiers can be constructed utilizing very few features and with minimal hand-tuning. We argue that the feature selection is meaningful and some of the highlighted genes appear to be medically important.	algorithm;relevance	Ilya Shmulevich;Edward R. Dougherty;Wei Zhang	2002	Bioinformatics	10.1093/bioinformatics/18.10.1332	biology;gene expression;computer science;bioinformatics;pattern recognition;data mining;feature selection;genetics	Comp.	7.4713705272785775	-49.70894146649652	74188
a74580aa0eb35ed3d8920b3b02376456c094f926	sep/cop: an efficient method to find the best partition in hierarchical clustering based on a new cluster validity index	hierarchical clustering;partition method;algoritmo busqueda;algorithme recherche;search algorithm;cluster validity index;validite;hierarchical classification;methode partition;validity;validez;indexation;signal classification;classification hierarchique;classification signal;metodo particion;cluster validity;classification automatique;automatic classification;clasificacion automatica;clasificacion jerarquizada;post processing	Hierarchical clustering algorithms provide a set of nested partitions called a cluster hierarchy. Since the hierarchy is usually too complex it is reduced to a single partition by using cluster validity indices. We show that the classical method is often not useful and we propose SEP, a new method that efficiently searches in an extended partition set. Furthermore, we propose a new cluster validity index, COP, since many of the commonly used indices cannot be used with SEP. Experiments performed with 80 synthetic and 7 real datasets confirm that SEP/COP is superior to the method currently used and furthermore, it is less sensitive to noise.	cluster analysis;hierarchical clustering;symantec endpoint protection	Ibai Gurrutxaga;Iñaki Albisua;Olatz Arbelaitz;José Ignacio Martín;Javier Muguerza;Jesús M. Pérez;Iñigo Perona	2010	Pattern Recognition	10.1016/j.patcog.2010.04.021	computer science;data mining;mathematics;hierarchical clustering;video post-processing;algorithm;validity;statistics;search algorithm	Vision	1.7213549741310608	-39.92284805080663	74214
ebb99ec68f6d6c27ea52eafd2069eb0b947d0558	a hybrid text classification model based on rough sets and genetic algorithms	kernel;high dimensionality;support vector machines;classification speed;rough set theory;documents automatic categorization;text analysis;data mining;genetics;text classification;feature vector;accuracy;machine learning techniques;svm classification accuracy;biological cells;machine learning;hybrid text classification;rough sets theory;classification algorithms;pattern classification;text analysis data mining pattern classification rough set theory support vector machines;rough sets;support vector machine classification;genetic algorithm;genetic algorithms;feature selection;rough sets theory hybrid text classification genetic algorithms documents automatic categorization data mining knowledge discovery machine learning techniques support vector machines feature selection svm classification accuracy classification speed;support vector machine;document classification;classification accuracy;rough set;parameter optimization;genetic algorithms document classification support vector machine rough sets;support vector machines biological cells accuracy classification algorithms kernel support vector machine classification genetics;knowledge discovery	Automatic categorization of documents into pre-defined taxonomies is a crucial step in data mining and knowledge discovery. Standard machine learning techniques like support vector machines(SVM) and related large margin methods have been successfully applied for this task. Unfortunately, the high dimensionality of input feature vectors impacts on the classification speed. The kernel parameters setting for SVM in a training process impacts on the classification accuracy. Feature selection is another factor that impacts classification accuracy. The objective of this work is to reduce the dimension of feature vectors, optimizing the parameters to improve the SVM classification accuracy and speed. In order to improve classification speed we spent rough sets theory to reduce the feature vector space. We present a genetic algorithm approach for feature selection and parameters optimization to improve classification accuracy. Experimental results indicate our method is more effective than traditional SVM methods and other traditional methods.	categorization;data mining and knowledge discovery;document classification;feature selection;feature vector;genetic algorithm;machine learning;mathematical optimization;rough set;set theory;support vector machine	Xiaoyue Wang;Zhen Hua;Rujiang Bai	2008	2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing	10.1109/SNPD.2008.142	rough set;computer science;machine learning;linear classifier;pattern recognition;data mining;feature selection;k-nearest neighbors algorithm	AI	8.61830825384816	-39.46154096747871	74426
5a2b039fd2940c3bc793c8b9e1e2921a0ee68344	data pre-processing by genetic algorithms for bankruptcy prediction	support vector machine classifier data preprocessing genetic algorithms data mining techniques feature selection dimensionality reduction data reduction public bankruptcy prediction dataset;data reduction bankruptcy prediction data mining data pre processing genetic algorithms feature selection;support vector machines;financial management;data mining;support vector machines data mining financial management genetic algorithms pattern classification;support vector machines data mining genetic algorithms accuracy classification algorithms training machine learning;prediction accuracy;pattern classification;genetic algorithm;genetic algorithms;feature selection;data reduction;bankruptcy prediction;support vector machine;experience base;dimensional reduction;data pre processing genetic algorithms	Bankruptcy prediction has been approached by data mining techniques. However, since data pre-processing including feature selection or dimensionality reduction and data reduction is a very important stage for successful data mining, very few consider performing both tasks to examine the impact of data pre-processing on prediction performance. This paper applies genetic algorithms, which have been widely used for the data pre-processing tasks, for feature selection and data reduction over a public bankruptcy prediction dataset. In particular, the experiments based on different priorities of performing feature selection and data reduction are conducted. The results show that performing data reduction only can allow the support vector machine (SVM) classifier to provide the highest rate of prediction accuracy. However, executing both feature selection and data reduction with different priorities performs the same. They not only largely reduce the dataset size, but also keep the similar performance as SVM without data pre-processing.	data mining;data pre-processing;dimensionality reduction;experiment;feature selection;genetic algorithm;preprocessor;support vector machine	Chih-Fong Tsai;Jui-Sheng Chou	2011	2011 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2011.6118222	support vector machine;genetic algorithm;computer science;machine learning;pattern recognition;data mining;feature selection	DB	8.466997360690524	-39.03057869762598	74454
7faea4849df6ed57126e5c77979c2b7f886586b7	a new separation measure for improving the effectiveness of validity indices	cluster algorithm;fuzzy c mean;separation measure;data distribution;large scale;machine learning;clustering;indexation;validity index	Many validity indices have been proposed for quantitatively assessing the performance of clustering algorithms. One limitation of existing indices is their lack of generalizability, due to their dependence on the specific algorithms and structures of the data space. To handle large-scale datasets with arbitrary structures, this research study proposes a new cluster separation measure for improving the effectiveness of existing validity indices. This is achieved by partitioning the original data space into a grid-based structure which allows the introduction of a new measurement for assessing the true data distribution between any two clusters instead of the distance between the two cluster prototypes. To validate the effectiveness of the proposed separation measure, we adopt two commonly used validity indices, the Davies–Bouldin’s function (DB) and Tibshirani’s Gap statistic (GS). These indices are denoted as R-DB-1 and R-GS-1 for clusters with sphere-shaped structures and R-DB-2 and R-GS-2 for irregular-shaped structures. This integration enables the indices to evaluate both partitional algorithms and hierarchical algorithms. Partitional algorithms including C-Means (CM), Fuzzy C-Means (FCM), and hierarchical algorithms, including DBSCAN and CLIQUE, are used to test the performance of the new indices. Two synthetic datasets with spherical structures and four synthetic datasets with irregular shapes are first compared. Five real datasets from the UCI machine learning repository are then used to further test the measure’s performance. The experimental results provide evidence that the new indices outperform the original indices. 2009 Elsevier Inc. All rights reserved.	algorithm;benchmark (computing);cluster analysis;complexity;dbscan;data structure;database index;dataspaces;davies–bouldin index;dhrystone;fuzzy cognitive map;machine learning;roland gs;separation kernel;synthetic intelligence	Shihong Yue;Jeen-Shing Wang;Teresa Wu;Huaxiang Wang	2010	Inf. Sci.	10.1016/j.ins.2009.11.005	computer science;machine learning;data mining;mathematics;cluster analysis;statistics	DB	0.9284768101128437	-41.6516639755752	74617
640f162b262888db3b873e0fa5cbd389cfcaf724	application of attribute weighting method based on clustering centers to discrimination of linearly non-separable medical datasets	subtractive clustering based attribute weighting;classification;liver disorders;fuzzy c means clustering based attribute weighting;k means clustering based attribute weighting;mammography;2 d spiral dataset	In this paper, attribute weighting method based on the cluster centers with aim of increasing the discrimination between classes has been proposed and applied to nonlinear separable datasets including two medical datasets (mammographic mass dataset and bupa liver disorders dataset) and 2-D spiral dataset. The goals of this method are to gather the data points near to cluster center all together to transform from nonlinear separable datasets to linear separable dataset. As clustering algorithm, k-means clustering, fuzzy c-means clustering, and subtractive clustering have been used. The proposed attribute weighting methods are k-means clustering based attribute weighting (KMCBAW), fuzzy c-means clustering based attribute weighting (FCMCBAW), and subtractive clustering based attribute weighting (SCBAW) and used prior to classifier algorithms including C4.5 decision tree and adaptive neuro-fuzzy inference system (ANFIS). To evaluate the proposed method, the recall, precision value, true negative rate (TNR), G-mean1, G-mean2, f-measure, and classification accuracy have been used. The results have shown that the best attribute weighting method was the subtractive clustering based attribute weighting with respect to classification performance in the classification of three used datasets.	adaptive neuro fuzzy inference system;attribute grammar;benchmark (computing);c4.5 algorithm;cns disorder;class;cluster analysis;data transformation;data point;data pre-processing;decision making;decision tree;f1 score;inference engine;k-means clustering;liver diseases;machine learning;name;neuro-fuzzy;nonlinear system;performance;preprocessor;sensitivity and specificity;silo (dataset);statistical cluster	Kemal Polat	2011	Journal of Medical Systems	10.1007/s10916-011-9741-y	correlation clustering;k-medians clustering;fuzzy clustering;biological classification;flame clustering;machine learning;pattern recognition;data mining;mathematics;cluster analysis;single-linkage clustering	ML	8.327108420080954	-43.87330903564292	74694
b524fb7cdd225c0871675e482bba4f8852e49707	grouping genetic algorithm for data clustering	heuristic;data clustering;grouping genetic algorithm	Clustering can be visualized as a grouping problem as it consists of identifying finite set of groups in a dataset. Grouping genetic algorithms are specially designed to handle grouping problems. As the clustering criteria such as minimizing the with-in cluster distance is high-dimensional, non-linear and multi-modal, many standard algorithms available in the literature for clustering tend to converge to a locally optimal solution and/or have slow convergence. Even genetic guided clustering algorithms which are capable of identifying better quality solutions in general are also not totally immune to these shortcomings because of their ad hoc approach towards clustering invalidity and context insensitivity. To remove these shortcomings we have proposed a hybrid steady-state grouping genetic algorithm. Computational results show the effectiveness of our approach.	cluster analysis;genetic algorithm;xslt/muenchian grouping	Santhosh Peddi;Alok Singh	2011		10.1007/978-3-642-27172-4_28	correlation clustering;constrained clustering;data stream clustering;heuristic;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;clustering high-dimensional data	AI	3.560312715452764	-42.3136774218265	75418
e4fbff044d502e3f0d09b3aad038ed527f186637	textual data compression in computational biology: algorithmic techniques	huffman coding;data compression theory and practice;hidden markov models;kolmogorov complexity;pattern discovery in bioinformatics;reverse engineering of biological networks;minimum description length principle;algorithms;alignment free sequence comparison;sequence alignment;entropy;lempel ziv compressors;bioinformatics	In a recent review [R. Giancarlo, D. Scaturro, F. Utro, Textual data compression in computational biology: a synopsis, Bioinformatics 25 (2009) 1575–1586] the first systematic organization and presentation of the impact of textual data compression for the analysis of biological data has been given. Its main focus was on a systematic presentation of the key areas of bioinformatics and computational biology where compression has been used together with a technical presentation of how well-known notions from information theory have been adapted to successfully work on biological data. Rather surprisingly, the use of data compression is pervasive in computational biology. Starting from that one, the focus of this companion review is on the computational methods involved in the use of data compression in computational biology. Indeed, although one would expect  ad hoc  adaptation of compression techniques to work on biological data, unifying and homogeneous algorithmic approaches are emerging. Moreover, given that experiments based on parallel sequencing are the future for biological research, data compression techniques are among a handful of candidates that seem able, successfully, to deal with the deluge of sequence data they produce; although, until now, only in terms of storage and indexing, with the analysis still being a challenge. Therefore, the two reviews, complementing each other, are perceived to be a useful starting point for computer scientists to get acquainted with many of the computational challenges coming from computational biology in which core ideas of the information sciences are already having a substantial impact.		Raffaele Giancarlo;Davide Scaturro;Filippo Utro	2012	Computer Science Review	10.1016/j.cosrev.2011.11.001	entropy;combinatorics;computer science;bioinformatics;artificial intelligence;theoretical computer science;machine learning;sequence alignment;data mining;mathematics;algorithm;hidden markov model;huffman coding	Theory	-3.466661025518176	-50.42535184169963	75667
823e3638e8e04f8a381a0f091f52ce099cc85c50	sparse genomic structural variant detection: exploiting parent-child relatedness for signal recovery	computational genomics sparse signal recovery convex optimization next generation sequencing data structural variants;computational genomics;structural variants;sparse signal recovery;convex optimization;signal reconstruction dna genomics optimisation signal detection;next generation sequencing data;genomics bioinformatics dna sequential analysis signal reconstruction conferences;signal reconstruction sparse genomic structural variant detection parent child relatedness signal recovery genomic dna sv sparsity promoting l 1 penalty novel optimization method	Structural variants (SVs) - rearrangements of an individuals' genome - are an important source of heterogeneity in human and other mammalian species. Typically, SVs are identified by comparing fragments of DNA from a test genome to a known reference genome, but errors in both the sequencing and the noisy mapping process contribute to high false positive rates. When multiple related individuals are studied, their relatedness offers a constraint to improve the signal of true SVs. We develop a computational method to predict SVs given genomic DNA from a child and both parents. We demonstrate that enforcing relatedness between individuals and constraining our solution with a sparsity-promoting ℓ1 penalty (since SV instances should be rare) results in improved performance. We present results on both simulated genomes as well as two-sequenced parent-child trios from the 1000 Genomes Project.	cone (formal languages);detection theory;sparse matrix;systemverilog	Mario Banuelos;Rubi Almanza;Lasith Adhikari;Roummel F. Marcia;Suzanne Sindi	2016	2016 IEEE Statistical Signal Processing Workshop (SSP)	10.1109/SSP.2016.7551828	biology;bioinformatics;machine learning;genetics	Comp.	7.078084531749925	-51.26444407819055	75682
975c01117f1cde58ee768fb5ff3a7cbbef97aa4a	protein side-chain packing problem: a maximum edge-weight clique algorithmic approach	protein design;reduction;clique algorithms;protein side chain packing	"""""""Protein Side-chain Packing"""" has an ever-increasing application in the field of bio-informatics, dating from the early methods of homology modeling to protein design and to the protein docking. However, this problem is computationally known to be NP-hard. In this regard, we have developed a novel approach to solve this problem using the notion of a maximum edge-weight clique. Our approach is based on efficient reduction of protein side-chain packing problem to a graph and then solving the reduced graph to find the maximum clique by applying an efficient clique finding algorithm developed by our co-authors. Since our approach is based on deterministic algorithms in contrast to the various existing algorithms based on heuristic approaches, our algorithm guarantees of finding an optimal solution. We have tested this approach to predict the side-chain conformations of a set of proteins and have compared the results with other existing methods. We have found that our results are favorably comparable or better than the results produced by the existing methods. As our test set contains a protein of 494 residues, we have obtained considerable improvement in terms of size of the proteins and in terms of the efficiency and the accuracy of prediction."""	algorithm;bioinformatics;british informatics olympiad;clique (graph theory);docking (molecular);graph - visual representation;heuristic;homology (biology);homology modeling;informatics (discipline);macromolecular docking;np-hardness;set packing;staphylococcal protein a;test set	Dukka Bahadur;Tatsuya Akutsu;Etsuji Tomita;Tomokazu Seki	2004	Journal of bioinformatics and computational biology	10.1142/S0219720005000904	mathematical optimization;combinatorics;set packing;reduction;bioinformatics;mathematics;protein design	Comp.	0.17221371898123258	-49.97571024612277	75822
c1835d9bfd0f02502a21d409c65c998530f02168	an improving pruning technique with restart for the kohonen self-organizing feature map	unsupervised learning;reduced architecture pruning technique kohonen self organizing feature map penalty term clustering measure delayed pruning activation restarting phase;pattern clustering;signal processing algorithms clustering algorithms unsupervised learning computer industry automation phase measurement delay speech recognition data analysis signal mapping;neural net architecture;neural net architecture self organising feature maps pattern classification pattern clustering unsupervised learning;self organising feature maps;self organized feature map;pattern classification	Presents a pruning technique developed for the one-dimensional Kohonen self-organizing feature map (SOM) to be applied in clustering and classification problems. Its innovative aspect is the combined proposition of a penalty term, a clustering measure, a delayed pruning activation and a restarting phase. The proposed algorithm (PSOM) always guides to a reduced architecture capable of representing the data set. We compare the PSOM with the original SOM applying them to three different classification problems. The results show that the PSOM is able to present superior performance in all cases.	organizing (structure);self-organization;self-organizing map	Leandro Nunes de Castro;Fernando José Von Zuben	1999		10.1109/IJCNN.1999.832674	unsupervised learning;computer science;machine learning;pattern recognition;data mining	Robotics	4.4051311150727415	-38.9660437415296	75842
11bc055a04034f20f46adfd945a0d576a33ed0b3	chaos of protein folding	biology computing;topological dynamics;neural nets;amino acid sequence;artificial intelligent;proteins;computational complexity;structure prediction;proteins chaos encoding amino acids lattices three dimensional displays tin;artificial intelligence;protein folding;genetic algorithm;genetic algorithms;biology protein folding chaos np complete problem artificial intelligence tools neural network genetic algorithm 3d shape prediction amino acids sequence topological dynamic 2d hydrophobic hydrophilic square lattice model structure prediction;proteins artificial intelligence biology computing computational complexity genetic algorithms neural nets;lattice model;np complete problem;neural network	As protein folding is a NP-complete problem, artificial intelligence tools like neural networks and genetic algorithms are used to attempt to predict the 3D shape of an amino acids sequence. Underlying these attempts, it is supposed that this folding process is predictable. However, to the best of our knowledge, this important assumption has been neither proven, nor studied. In this paper the topological dynamic of protein folding is evaluated. It is mathematically established that protein folding in 2D hydrophobic-hydrophilic (HP) square lattice model is chaotic as defined by Devaney. Consequences for both structure prediction and biology are then outlined.	3d modeling;artificial intelligence;artificial neural network;chaos theory;computation;computational intelligence;genetic algorithm;image resolution;mixing (mathematics);mutation (genetic algorithm);np-completeness;stochastic process;topological entropy	Jacques M. Bahi;Nathalie Côté;Christophe Guyeux	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033463	genetic algorithm;computer science;bioinformatics;artificial intelligence;machine learning;mathematics;artificial neural network	Robotics	-3.337751409574586	-48.29661801744904	75947
8ae23511456887c0389708b0ef5d4df0571ffe55	personalized modeling based gene selection for microarray data analysis	gene expression data;colon cancer;microarray data analysis;cancer diagnosis;comparative study;clinical decision support system;gene selection;central nervous system	This paper presents a novel gene selection method based on personalized modeling. Identifying a compact set of genes from gene expression data is a critical step in bioinformatics research. Personalized modeling is a recently introduced technique for constructing clinical decision support systems. In this paper we have provided a comparative study using the proposed Personalized Modeling based Gene Selection method (PMGS) on two benchmark microarray datasets (Colon cancer and Central Nervous System cancer data). The experimental results show that our method is able to identify a small number of informative genes which can lead to reproducible and acceptable predictive performance without expensive computational cost. These genes are of importance for specific groups of people for cancer diagnosis and prognosis.	microarray	Yingjie Hu;Qun Song;Nikola K. Kasabov	2008		10.1007/978-3-642-02490-0_148	gene-centered view of evolution;microarray analysis techniques;clinical decision support system;computer science;bioinformatics;central nervous system;comparative research;data mining;microarray databases	ML	8.524313130117745	-50.32014282238347	76052
ea1840452a2aa6fe4476bc0bf9ec45d6aa93270c	fuzzy linear discriminant analysis-guided maximum entropy fuzzy clustering algorithm	optimal transformation matrix;maximum entropy fuzzy clustering algorithm;fuzzy scatter matrix;fuzzy linear discriminant analysis	Linear Discriminant Analysis (LDA) is a classical statistical approach for supervised feature extraction and dimensionality reduction, hard c-means (HCM) is a classical unsupervised learning algorithm for clustering. Based on the analysis of the relationship between LDA and HCM, Linear Discriminant Analysis-guided adaptive subspace hard c-means clustering algorithm (LDA-HCM) had been proposed. LDA-HCM combines LDA and HCM into a coherent framework and can adaptively reduce the dimension of data while performing data clustering simultaneously. Seeing that LDA-HCM is still a hard clustering algorithm, we consider the fuzzy extension version of LDA-HCM in this paper. To this end, firstly, we propose a new optimization criterion of Fuzzy Linear Discriminant Analysis (FLDA) by extending the value of membership function in classical LDA from binary 0 or 1 into closed interval [0, 1]. In the meantime, we present an efficient algorithm for the proposed FLDA. Secondly, we show the close relationship between FLDA and Maximum Entropy Fuzzy Clustering Algorithm (MEFCA): they both are maximizing fuzzy between-class scatter and minimizing within-class scatter simultaneously. Finally, based on the above analysis, combining FLDA and MEFCA into a joint framework, we propose fuzzy Linear Discriminant Analysis-guided maximum entropy fuzzy clustering algorithm (FLDA-MEFCA). LDA-MEFCA is a natural and effective fuzzy extension of LDA-HCM. Due to the introduction of soft decision strategy, FLDA-MEFCA can yield fuzzy partition of data set and is more flexible than LDA-HCM. We also give the convergence proof of FLDA-MEFCA. Extensive experiments on a collection of benchmark data sets are presented to show the effectiveness of the proposed algorithm.	algorithm;cluster analysis;fuzzy clustering;linear discriminant analysis	Xiaobin Zhi;Jiulun Fan;Feng Zhao	2013	Pattern Recognition	10.1016/j.patcog.2012.12.007	mathematical optimization;defuzzification;fuzzy clustering;flame clustering;fuzzy classification;fuzzy number;canopy clustering algorithm;machine learning;pattern recognition;mathematics;fuzzy associative matrix;cluster analysis	Vision	2.916087289473542	-39.79874694722326	76075
12659a0342167bf98eda58c8bc28f1bc4aa8296e	a new contiguity-constrained agglomerative hierarchical clustering algorithm for image segmentation	image segmentation;aggregation index;agglomerative hierarchical classification;contiguity constrained clustering methods;agglomerative hierarchical clustering;clustering method;classification image;medical image segmentation	This paper introduces a new constrained hierarchical agglomerative algorithm with an aggregation index which uses neighbouring relations present in the data. Experiments show the behaviour of the proposed contiguity-constrained agglomerative hierarchical algorithm in the case of medical image segmentation.	algorithm;cluster analysis;hierarchical clustering;image segmentation	Eduardo R. Concepción Morales;Yosu Yurramendi Mendizabal	2009		10.1007/978-3-642-14264-2_27	canopy clustering algorithm;machine learning;segmentation-based object categorization;pattern recognition;data mining;single-linkage clustering;scale-space segmentation;brown clustering;hierarchical clustering of networks	ML	2.4460647409610456	-40.99058870450062	76307
70799337e9e7b069b02ea2506cfd237f6e7b98d6	decision tree induction for dynamic, high-dimensional data using p-trees	data mining;classification;information systems;p-trees.;decision tree induction;information system;data structure;high dimensional data;decision tree	Decision Tree Induction is a powerful classification tool that is much used in practice and works well for static data with dozens of attributes. We adapt the decision tree concept to a setting where data changes rapidly and hundreds or thousands of attributes may be relevant. Decision tree branches are evaluated as needed, based on the most recent data, focusing entirely on the data that needs to be classified. Our algorithm is based on the P-tree data structure that allows fast evaluation of counts of data points, and results in scaling that is better than linear in the data set size.	bioinformatics;c4.5 algorithm;data point;data structure;decision tree;image scaling;lazy evaluation;pstree;statistical classification;tree (data structure)	Anne M. Denton;William Perrizo	2004			clustering high-dimensional data;computer science;incremental decision tree;decision tree;decision tree learning;data structure;machine learning;influence diagram;alternating decision tree;pattern recognition;artificial intelligence;id3 algorithm	ML	8.43818604408377	-45.38595772793257	76490
8d1952459d0d83c44a197de9824060db9bd69d42	grid distance-based improving accuracy clustering algorithm	cluster algorithm;grid distance clustering grid algorithm accuracy;pattern clustering;complexity theory;approximation algorithms;grid distance;grid based clustering algorithm;grid;algorithm;mathematical operation;accuracy;distance measurement;clustering;clustering algorithms;clustering algorithms distance measurement approximation algorithms algorithm design and analysis signal processing algorithms complexity theory accuracy;logic distance;signal processing algorithms;algorithm design and analysis;mathematical operation grid based clustering algorithm logic distance	In order to improve the quality and efficiency of the grid-based clustering algorithm, the paper presents a new improving precision clustering algorithm based on the distance between grids. On the one hand, the algorithm deals with datasets by calculating the logic distance between grids, which makes up the shortcoming of some algorithm that need much mathematical operation to support. On the other hand, it presents a new technique to deal with boundary points of clusters. The experimental result shows that the algorithm is efficient with the accuracy to discover the boundary points and its run time.	algorithm;cluster analysis;run time (program lifecycle phase)	Chunjiang Pang;Weixiang Cheng;Weihua Niu	2008	2008 International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2008.49	correlation clustering;mathematical optimization;data stream clustering;computer science;theoretical computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;fsa-red algorithm;cluster analysis;k-medoids;approximation algorithm;algorithm	Robotics	-0.01942715837378422	-40.13379572334181	76531
5280f5d00f73777d9751bd4060dda59073f9df0f	fault diagnosis method based on supervised particle swarm optimization classification algorithm			algorithm;mathematical optimization;particle swarm optimization	Bo Zheng;Hong-Zhong Huang;Wei Guo;Yan-Feng Li;Jinhua Mi	2018	Intell. Data Anal.	10.3233/IDA-163392	artificial intelligence;machine learning;pattern recognition;computer science;particle swarm optimization	AI	9.481983900752304	-40.543659331617995	76828
cf9559ec479fbc88b53bc501ac637d1d323ea8d6	parallelized training of deep nn: comparison of current concepts and frameworks		Horizontal scalability is a major facilitator of recent advances in deep learning. Common deep learning frameworks offer different approaches for scaling the training process. We operationalize the execution of distributed training using Kubernetes and helm templates. This way we lay ground for a systematic comparison of deep learning frameworks. For two of them, TensorFlow and MXNet we examine their properties with regard to throughput, scalability and practical ease of use.		Sebastian J&#228;ger;Hans-Peter Zorn;Stefan Igel;Christian Zirpins	2018		10.1145/3286490.3286561	throughput;software engineering;deep learning;scalability;usability;operationalization;artificial intelligence;facilitator;computer science	Security	-1.6913815460388422	-45.415528118799664	77604
2316c617ba7bc94204a206aec83870bb34f7c7ec	exploring the genetic patterns of complex diseases via the integrative genome-wide approach	genomics;convex optimization;gwaws;low rank and sparse;biclustering;snps;diseases;gwass;sparse matrices;noise;data models;bioinformatics	Genome-wide association studies (GWASs), which assay more than a million single nucleotide polymorphisms (SNPs) in thousands of individuals, have been widely used to identify genetic risk variants for complex diseases. However, most of the variants that have been identified contribute relatively small increments of risk and only explain a small portion of the genetic variation in complex diseases. This is the so-called missing heritability problem. Evidence has indicated that many complex diseases are genetically related, meaning these diseases share common genetic risk variants. Therefore, exploring the genetic correlations across multiple related studies could be a promising strategy for removing spurious associations and identifying underlying genetic risk variants, and thereby uncovering the mystery of missing heritability in complex diseases. We present a general and robust method to identify genetic patterns from multiple large-scale genomic datasets. We treat the summary statistics as a matrix and demonstrate that genetic patterns will form a low-rank matrix plus a sparse component. Hence, we formulate the problem as a matrix recovering problem, where we aim to discover risk variants shared by multiple diseases/traits and those for each individual disease/trait. We propose a convex formulation for matrix recovery and an efficient algorithm to solve the problem. We demonstrate the advantages of our method using both synthesized datasets and real datasets. The experimental results show that our method can successfully reconstruct both the shared and the individual genetic patterns from summary statistics and achieve comparable performances compared with alternative methods under a wide range of scenarios. The MATLAB code is available at:http://www.comp.hkbu.edu.hk/~xwan/iga.zip.	annotation;architecture as topic;categories;causal filter;convex optimization;eaf2 gene;electronic supplementary materials;f1 score;genetic polymorphism;genetic algorithm;genetic programming;genome-wide association study;hereditary diseases;immune system diseases;matlab;mathematical optimization;mental association;nucleotides;optimization problem;performance;simulation;single nucleotide polymorphism;sparse matrix;trait;whole genome sequencing	Ben Teng;Can Yang;Jiming Liu;Zhipeng Cai;Xiang Wan	2016	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2015.2459692	single-nucleotide polymorphism;biology;data modeling;genomics;convex optimization;sparse matrix;computer science;bioinformatics;noise;machine learning;data mining;mathematics;genetics;biclustering	ML	7.067745473640811	-51.38965899378442	77852
892bdc86908fb75589e488434c2a05d7e59450cf	an efficient algorithm to identify dna motifs		We consider the problem of identifying motifs that abstracts the task of finding short conserved sites in genomic DNA. The planted (l, d)-motif problem, PMP, is the mathematical abstraction of this problem, which consists of finding a substring of length l that occurs in each si in a set of input sequences S = {s1, s2, . . . ,st} with at most d substitutions. Our propose algorithm combines the voting algorithm and pattern matching algorithm to find exact motifs. The combined algorithm is achieved by running the voting algorithm on t′ sequences, t′ < t. After that we use the pattern matching on the output of the voting algorithm and the reminder sequences, t − t′. Two values of t′ are calculated. The first value of t′ makes the running time of our proposed algorithm less than the running time of voting algorithm. The second value of t′ makes the running time of our proposed algorithm is minimal. We show that our proposed algorithm is faster than the voting algorithm by testing both algorithms on simulated data from (9, d ≤ 2) to (19, d ≤ 7). Finally, we test the performance of the combined algorithm on realistic biological data.	algorithm;dna computing	Mostafa M. Abbas;Hazem M. Bahig	2013	Mathematics in Computer Science	10.1007/s11786-013-0165-6	suurballe's algorithm;commentz-walter algorithm;ramer–douglas–peucker algorithm;weighted majority algorithm;theoretical computer science;machine learning;mathematics;fsa-red algorithm;dinic's algorithm;freivalds' algorithm;nondeterministic algorithm;shortest path faster algorithm;algorithm;difference-map algorithm;output-sensitive algorithm;population-based incremental learning	Theory	-0.5765480034528557	-51.515275063473126	77891
54bc04e8ecf56d707cf957d2fed16c70757f911f	drug/nondrug classification with consensual self-organising map and self-organising global ranking algorithms	neural networks;nondrug compounds;classification;classifier design;drug design;self organising maps;self organising global ranking;ranking algorithm;classifier evaluation;self organising map;bioinformatics;consensual som;consensual sogr	In this paper, a special consensual approach is discussed for separating the druglike compounds from the non-druglike compounds. It involves a group decision to produce a consensus of multiple classification results obtained with a single classification algorithm. The individual results are obtained with either the Self Organising Global Ranking (SOGR) or Self Organising Map (SOM). The main difference between the proposed algorithm and SOM is the neighbourhood concept. The constructed consensual model has a preprocessing unit which consists of transformation of input patterns by random matrices and median filtering to generate independent errors for a single type of classifier, and a postprocessing unit for consensus. The confirmed drugs were classified with a consensual accuracy of 90.63% while nondrugs resulted in 80.44% accuracy. The SOGR results were better than the SOM algorithm results.		Ayca C. Pehlivanli;Okan K. Ersoy;Turgay Ibrikci	2008	International journal of computational biology and drug design	10.1504/IJCBDD.2008.022212	biological classification;computer science;bioinformatics;artificial intelligence;machine learning;data mining;artificial neural network;drug design	Vision	7.697489792806608	-48.056553412829516	78065
0e97915f567880bc2d3c85a0dfac89a2c34c1661	a comparison of two approaches to discretization: multiple scanning and c4.5		In a Multiple Scanning discretization technique the entire attribute set is scanned many times. During every scan, the best cut- point is selected for all attributes. The main objective of this paper is to compare the quality of two setups: the Multiple Scanning discretization technique combined with the C4.5 classification system and the inter- nal discretization technique of C4.5. Our results show that the Multiple Scanning discretization technique is significantly better than the inter- nal discretization used in C4.5 in terms of an error rate computed by ten-fold cross validation (two-tailed test, 5 % level of significance). Addi- tionally, the Multiple Scanning discretization technique is significantly better than a variant of discretization based on conditional entropy intro- duced by Fayyad and Irani called Dominant Attribute. At the same time, decision trees generated from data discretized by Multiple Scanning are significantly simpler from decision trees generated directly by C4.5 from the same data sets.	c4.5 algorithm;discretization	Jerzy W. Grzymala-Busse;Teresa Mroczek	2015		10.1007/978-3-319-19941-2_5	discretization error;mathematical optimization;discrete mathematics;machine learning;discretization;mathematics;discretization of continuous features	ML	7.879618698921427	-41.377554286550605	78300
c031e9c53aeb32c1c770b5381e573368f7f9fcd3	an efficient privacy-preserving classification method with condensed information		Privacy-preserving is a challenging problem in real-world data classification. Among the existing classification methods, the support vector machine (SVM) is a popular approach which has a high generalization ability. However, when datasets are privacy and complexity, the processing capacity of SVM is not satisfactory. In this paper, we propose a new method CI-SVM to achieve efficient privacy-preserving of the SVM. On the premise of ensuring the accuracy of classification, we condense the original dataset by a new method, which transforms the privacy information to condensed information with little additional calculation. The condensed information carries the class characteristics of the original information and doesn’t expose the detailed original data. The time-consuming of classification is greatly reduced because of the fewer samples as condensed information. Our experiment results on datasets show that the proposed CI-SVM algorithm has obvious advantages in classification efficiency.		Xinning Li;Zhiping Zhou	2017		10.1007/978-3-319-71598-8_49	pattern recognition;data classification;artificial intelligence;support vector machine;computer science;premise	NLP	9.460625237493709	-38.89955514453074	78434
51dcfd96488958437a8c91e4c2771d9289aabf44	an adaptive ant-based clustering algorithm with improved environment perception	unsupervised learning;cluster algorithm;evaluation function;pattern clustering;optimisation;swarm intelligence;clustering algorithms partitioning algorithms data mining image processing machine learning algorithms cybernetics usa councils petroleum machine learning bioinformatics;optimization adaptive ant colony swarm intelligence clustering;probability density function;data collection;ant colony;unsupervised learning optimisation pattern clustering;data mining;data clustering;machine learning;adaptive ant based clustering algorithm;clustering;environment perception;mathematical model;pattern recognition;clustering algorithms;optimization;clustered data;algorithm design and analysis;adaptive ant colony;adaptive ant based clustering algorithm environment perception data clustering data mining machine learning bioinformatics pattern recognition;partitioning algorithms;bioinformatics	Data clustering plays an important role in many disciplines, including data mining, machine learning, bioinformatics, pattern recognition, and other fields. When there is a need to learn the inherent grouping structure of data in an unsupervised manner, ant-based clustering stand out as the most widely used group of swarm-based clustering algorithms. Under this perspective, this paper presents a new Adaptive Ant-based Clustering Algorithm (AACA) for clustering data sets. The algorithm takes into account the properties of aggregation pheromone and perception of the environment together with other modifications to the standard parameters that improves its convergence. The performance of AACA is studied and compared to other methods using various patterns and data sets. It is also compared to standard clustering using a set of analytical evaluation functions and a range of synthetic and real data collection. Experimental results have shown that the proposed modifications improve the performance of ant-colony clustering algorithm in term of quality and run time.	algorithm;bioinformatics;cluster analysis;data mining;evaluation function;machine learning;machine perception;pattern recognition;run time (program lifecycle phase);swarm;synthetic intelligence;unsupervised learning	Idris El-Feghi;Mohamed Errateeb;Majid Ahmadi;M. A. Ahmed	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5346291	unsupervised learning;correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;fuzzy clustering;flame clustering;swarm intelligence;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;brown clustering;dbscan;biclustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	Robotics	3.3450067641635712	-41.97819439550198	78531
93be902a510bd6f47a57811f2ebd4247724ff050	generating fuzzy rule base classifier for highly imbalanced datasets using a hybrid of evolutionary algorithms and subtractive clustering	differential evolution;subtractive clustering;multi gene genetic programming;fuzzy inference system	In this paper, a design methodology is proposed for generating a fuzzy rule-based classifier for highly imbalanced datasets (only binary classification problems). The classifier is based on sugeno-type fuzzy inference system (FIS) and is generated using subtractive clustering, differential evolution (DE) and multi-gene genetic programming (MGGP) to obtain fuzzy rules. Subtractive clustering and DE are utilized for producing antecedents of rules and MGGP is employed for generating the functions in the consequence parts of rules. Feature selection is utilized as an important pre-processing step for dimension reduction. Performance of the proposed method is compared with some fuzzy rule-based classification approaches taken from the literature. The experiments are performed over 22 highly imbalanced datasets from KEEL dataset repository; the classification results are evaluated using AUC as a performance measure. Some statistical non-parametric tests are used to compare classification performance of different methods in different datasets. The obtained results reveal that the proposed classifier outperforms other methods in terms of AUC values.	cluster analysis;evolutionary algorithm;fuzzy rule;rule-based system	Mahdi Mahdizadeh;M. Eftekhari	2014	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-141261	differential evolution;fuzzy clustering;fuzzy classification;computer science;machine learning;pattern recognition;data mining;mathematics	AI	9.308963425399934	-41.51180679910305	78644
17db796cd06aafefdc7ee1e4f8369efc663918db	an efficient greedy k-means algorithm for global gene trajectory clustering	k means;gene expression data;gene expression analysis;clustering method;k means algorithm;global optimization;greedy elimination method;k means clustering;biological process	Optimal clustering of co-regulated genes is critical for reliable inference of the underlying biological processes in gene expression analysis, for which the K-means algorithm have been widely employed for its efficiency. However, given that the solution space is large and multimodal, which is typical of gene expression data, K-means is prone to produce inconsistent and sub-optimal cluster solutions that may be unreliable and misleading for biological interpretation. This paper applies a novel global clustering method called the greedy elimination method (GEM) to alleviate these problems. GEM is simple to implement, yet very effective in improving the global optimality of the solutions. Experiments over two sets of gene expression data show that the GEM scores significantly lower clustering errors than the standard K-means and the greedy incremental method.	cluster analysis;greedy algorithm;k-means clustering	Zeke S. H. Chan;Lesley Collins;Nikola K. Kasabov	2006	Expert Syst. Appl.	10.1016/j.eswa.2005.09.049	correlation clustering;constrained clustering;greedy randomized adaptive search procedure;mathematical optimization;greedy algorithm;computer science;bioinformatics;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;global optimization;k-means clustering;clustering high-dimensional data	ML	3.7112359614182586	-42.95301221663614	78717
6b735667ac7e4fd3578c245a211bdd8dee21a3fa	exploring new search algorithms and hardware for phylogenetics: raxml meets the ibm cell	max imum likelihood;phylogeny reconstruction;maximum likelihood;search algorithm;cell broadband engine;phylogenetic tree;ibm cell;control flow;floating point;dna sequence;phylogenetic inference;raxml	Phylogeny reconstruction is considered to be among the grand challenges in Bioinformatics due to the immense computational requirements. RAxML is currently among the fastest and most accurate programs for phylogenetic tree inference under the Maximum Likelihood (ML) criterion. Initially, we introduce new tree search heuristics that accelerate RAxML by factor 2.43 while returning equally good trees. The performance of the new search algorithm has been assessed on 18 real-world datasets comprising 148 up to 4,843 DNA sequences. We then present the implementation, optimization, and evaluation of RAxML on the IBM Cell Broadband Engine. We address problems and provide solutions pertaining to the optimization of floating point code, control flow, communication, and scheduling of multi-level parallelism on the Cell.	bioinformatics;cell (microprocessor);computational phylogenetics;control flow;fastest;grand challenges;heuristic (computer science);list of phylogenetics software;mathematical optimization;multi-core processor;openmp;over-the-top content;parallel computing;phylogenetic tree;recursion;requirement;run time (program lifecycle phase);scheduling (computing);search algorithm	Alexandros Stamatakis;Filip Blagojevic;Dimitrios S. Nikolopoulos;Christos D. Antonopoulos	2007	VLSI Signal Processing	10.1007/s11265-007-0067-4	dna sequencing;parallel computing;phylogenetic tree;computer science;bioinformatics;floating point;theoretical computer science;operating system;maximum likelihood;programming language;control flow;algorithm;statistics;search algorithm	HPC	-1.8788831657897778	-50.73743351195963	79375
2db5e9e559331f18d829f4d2983fc01448f15cfe	ti-dbscan: clustering with dbscan by means of the triangle inequality	triangle inequality;data mining;high dimensional data;grouped data;neighborhood search;clustered data;spatial access method	Grouping data into meaningful clusters is an important data mining task. DBSCAN is recognized as a high quality density-based algorithm for clustering data. It enables both the determination of clusters of any shape and the identification of noise in data. The most time-consuming operation in DBSCAN is the calculation of a neighborhood for each data point. In order to speed up this operation in DBSCAN, the neighborhood calculation is expected to be supported by spatial access methods. DBSCAN, nevertheless, is not efficient in the case of high dimensional data. In this paper, we propose a new efficient TI-DBSCAN algorithm and its variant TI-DBSCAN-REF that apply the same clustering methodology as DBSCAN. Unlike DBSCAN, TI-DBSCAN and TI-DBSCAN-REF do not use spatial indices; instead they use the triangle inequality property to quickly reduce the neighborhood search space. The experimental results prove that the new algorithms are up to three orders of magnitude faster than DBSCAN, and efficiently cluster both low and high dimensional data.	algorithm;cluster analysis;dbscan;data mining;data point;display resolution;geforce 900 series;social inequality;ti-nspire series	Marzena Kryszkiewicz;Piotr Lasek	2010		10.1007/978-3-642-13529-3_8	subclu;pattern recognition;data mining;database;mathematics;cluster analysis;dbscan;optics algorithm;clustering high-dimensional data	DB	-1.99977358686899	-40.2212526029856	79422
0c8ebbe1479b0374947069eda41e39e847de109a	k-nearest neighbors optimization-based outlier removal	k nearest neighbors;quantitative structure activity relationship;outlier detection;distance based method;optimization;outlier removal	Datasets of molecular compounds often contain outliers, that is, compounds which are different from the rest of the dataset. Outliers, while often interesting may affect data interpretation, model generation, and decisions making, and therefore, should be removed from the dataset prior to modeling efforts. Here, we describe a new method for the iterative identification and removal of outliers based on a k-nearest neighbors optimization algorithm. We demonstrate for three different datasets that the removal of outliers using the new algorithm provides filtered datasets which are better than those provided by four alternative outlier removal procedures as well as by random compound removal in two important aspects: (1) they better maintain the diversity of the parent datasets; (2) they give rise to quantitative structure activity relationship (QSAR) models with much better prediction statistics. The new algorithm is, therefore, suitable for the pretreatment of datasets prior to QSAR modeling.	iterative method;k-nearest neighbors algorithm;mathematical optimization;outlier;quantitative structure-activity relationship;quantitative structure–activity relationship;silo (dataset)	Abraham Yosipof;Hanoch Senderowitz	2015	Journal of computational chemistry	10.1002/jcc.23803	anomaly detection;quantitative structure–activity relationship;k-nearest neighbors algorithm	Vision	9.547478485211155	-51.457244027656785	79717
35ae2c0c0c79f7765bc9de89c99d5025dbfeafb4	analysis of low-correlated spatial gene expression patterns: a clustering approach in the mouse brain data hosted in the allen brain atlas		The Allen Brain Atlas (ABA) provides a similar gene expression dataset by genome-scale mapping of the C57BL/6J mouse brain. In this study, the authors describe a method to extract the spatial information of gene expression patterns across a set of 1047 genes. The genes were chosen from among the 4104 genes having the lowest Pearson correlation coefficient used to compare the expression patterns across voxels in a single hemisphere for available coronal and sagittal volumes. The set of genes analysed in this study is the one discarded in the article by Bohland et al., which was considered to be of a lower consistency, not a reliable dataset. Following a normalisation task with a global and local approach, voxels were clustered using hierarchical and partitioning clustering techniques. Cluster analysis and a validation method based on entropy and purity were performed. They analyse the resulting clusters of the mouse brain for different number of groups and compared them with a classically-defined anatomical reference atlas. The high degree of correspondence between clusters and anatomical regions highlights how gene expression patterns with a low Pearson correlation coefficient between sagittal and coronal sections can accurately identify different neuroanatomical regions.	allen brain atlas;cluster analysis	Paolo Rosati;Carmen Alina Lupascu;Domenico Tegolo	2018	IET Computer Vision	10.1049/iet-cvi.2018.5217	coronal plane;brain atlas;voxel;sagittal plane;spatial analysis;cluster analysis;pearson product-moment correlation coefficient;artificial intelligence;pattern recognition;mathematics;atlas (anatomy)	Vision	2.639824873125611	-48.350216710854816	79926
95d2efd40157dffd96451c79cbff63f96b9dc3af	adding hidden nodes to gene networks (extended abstract)	maximum likelihood;compression.;bayesian networks;gene network;network expansion;minimum description length;em	  Bayesian networks are widely used for modelling gene networks. We investigate the problem of expanding a given Bayesian network by adding a hidden node – a node on which no experimental data are given. Finding a good expansion  (a new hidden node and its neighborhood) can point to regions where the model is not rich enough, and help locate new, unknown  variables that are important for understanding the network. We study the computational complexity of this expansion, show  it is hard, and describe an EM based heuristic algorithm for solving it. The algorithm was applied to synthetic datasets and  to yeast gene expression datasets, and produces good, encouraging results.    	gene regulatory network	Benny Chor;Tamir Tuller	2004		10.1007/978-3-540-30219-3_11		Theory	1.1752352180496608	-50.6068500738652	80161
ef0f38f04fb1a7874ffaecd71442946f2244503b	sreveal: scalable extensions of reveal towards regulatory network inference	dna;microarray data;belief networks;experimental method;regulatory network;boolean functions;prior knowledge;spectrum;regulatory networks;gene expression;decision support systems intelligent systems;probabilistic boolean network;time lags;computational complexity;dynamic bayesian network;transcription factor;information theory belief networks bioinformatics boolean functions computational complexity dna;time lag;time lags information theory regulatory networks inference;mutual information;transcription factor list sreveal scalable extensions reveal gene regulatory networks inference dynamic bayesian networks probabilistic boolean networks gene expression information theoretic approaches larger regulatory networks high throughput microarray data aracne clr mutual information;high throughput;gene regulatory network;information theoretic;inference;information theory;bioinformatics	Most of the popular approaches towards gene regulatory networks inference e.g., Dynamic Bayesian Networks, Probabilistic Boolean Networks etc. are computationally complex and can only be used to infer small networks. While high-throughput experimental methods to monitor gene expression provide data for thousands of genes, these methods cannot fully utilize the entire spectrum of generated data. With the advent of information theoretic approaches in the last decade, the inference of larger regulatory networks from high throughput microarray data has become possible. Not all information theoretic approaches are scalable though; only methods that infer networks considering pair-wise interactions between genes such as, relevance networks, ARACNE and CLR to name a few, can be scaled upto genome-level inference. ARACNE and CLR attempt to improve the inference accuracy by pruning false edges, and do not bring in newer true edges. REVEAL is another information theoretic approach, which considers mutual information between multiple genes. As it goes beyond pair wise interactions, this approach was not scalable and could only infer small networks. In this paper, we propose two algorithms to improve the scalability of REVEAL by utilizing a transcription factor list (that can be predicted from the gene sequences) as prior knowledge and implementing time lags to further reduce the potential transcription factors that may regulate a gene. Our proposed S-REVEAL algorithms can infer larger networks with higher accuracy than the popular CLR algorithm.	algorithm;boolean network;dynamic bayesian network;gene regulatory network;high-throughput computing;information theory;interaction;microarray;mutual information;relevance;scalability;throughput;transcription (software)	Vijender Chaitankar;Preetam Ghosh;Mohamed O. Elasri;Edward J. Perkins	2011	2011 11th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2011.6121850	high-throughput screening;spectrum;microarray analysis techniques;gene regulatory network;gene expression;information theory;computer science;bioinformatics;machine learning;data mining;boolean function;mutual information;computational complexity theory;dna;dynamic bayesian network;statistics;transcription factor	Comp.	4.527348879371964	-49.7843500321838	80226
789f056bdb073d8786370380023230974b3aebb2	semiparametric rma background-correction for oligonucleotide arrays	dna;genetic engineering;microarray data;microarray quality control project;microarray quality control project semiparametric rma background correction oligonucleotide arrays microarray technology gene expression level monitoring noise removal robust multichip average procedure rma procedure unjustified parametric assumption rma model fitness maqc;oligonucleotide arrays;maqc;molecular biophysics biological techniques dna genetic engineering;robust multichip average procedure;gene expression level monitoring;rma model fitness;unjustified parametric assumption;semiparametric rma background correction;molecular biophysics;rma procedure;microarray technology;biological techniques;probes monitoring background noise sequences bioinformatics reservoirs noise robustness quality control parametric statistics gaussian noise;quality control;noise removal	Microarray technology has provided an opportunity to simultaneously monitor the expression levels of a large number of genes in response to intentional perturbations. A necessary step towards successful use of microarray technology is background correction which aims to remove noise. One of the most popular algorithms for background correction is the robust multichip average (RMA) procedure which relies on an unjustified parametric assumption. In this paper we first check the fitness of the RMA model using a graphical approach and then propose a new background correction method based on a semiparametric RMA model (semi-RMA). Evaluation of the proposed approach based on spike-in data and MAQC (microarray quality control project) data shows our semi-RMA model provides a better fit to microarray data than other approaches.	algorithm;graphical user interface;microarray;revolution in military affairs;semiconductor industry;semiparametric model	Ionut Bebu;Françoise Seillier-Moiseiwitsch;Hongfang Liu	2007	2007 IEEE 7th International Symposium on BioInformatics and BioEngineering	10.1109/BIBE.2007.4375756	genetic engineering;biology;microarray analysis techniques;quality control;bioinformatics;data mining;genetics;dna;statistics;molecular biophysics	Embedded	3.3665289624587147	-51.850655262474504	80552
11e636ee71fcad7fa1f142a7b30c4a878250f37d	an analysis of nk and generalized nk landscapes		Simulated landscapes have been used for several decades to evaluate search strategies whose goal is to find the landscape location with maximum fitness. Applications of simulated landscapes include modeling the capacity of enzymes to catalyze reactions or ligands to bind to proteins, and the clinical effectiveness of medical treatments. Understanding properties of landscapes is important for understanding search difficulty. This paper presents a novel and straightforward way of describing NK landscapes and studying their properties. We prove that NK landscapes can be represented by linear interaction models and derive the statistical properties of the model coefficients, thereby providing insight into how the NK algorithm parses importance to main effects and interactions. An important insight derived from the linear model representation is that the rank of the linear model defined by the NK algorithm is correlated with the number of local optima, a strong determinant of landscape complexity and search difficulty. We show that the maximal rank for an NK landscape is obtained through epistatic interactions that form balanced incomplete block designs. Finally, by representing NK models in matrix form, we are able to provide an analytic expression representing the expected number of local optima on the landscape.	algorithm;coefficient;fractal landscape;interaction;linear model;local optimum;maximal set;simulated annealing	Jeffrey S. Buzas;Jeffrey H. Dinitz	2012	CoRR		mathematical optimization;nk model	ML	0.3503356993423508	-49.17990225220586	80567
e1000b1e8c281c0f9b8a3b1a1b22e1842a876c0e	communication-efficient exact clustering of distributed streaming data		A widely used approach to clustering a single data stream is the two-phased approach in which the online phase creates and maintains micro-clusters while the off-line phase generates the macro-clustering from the micro-clusters. We use this approach to propose a distributed framework for clustering streaming data. Every remote-site process generates and maintains micro-clusters that represent cluster information summary from its local data stream. Remote sites send the local microclusterings to the coordinator, or the coordinator invokes the remote methods in order to get the local micro-clusterings from the remote sites. Having received all the local micro-clusterings from the remote sites, the coordinator generates the global clustering by the macro-clustering method. Our theoretical and empirical results show that the global clustering generated by our distributed framework is similar to the clustering generated by the underlying centralized algorithm on the same data set. By using the local micro-clustering approach, our framework achieves high scalability, and communication-efficiency.	algorithm;centralized computing;cluster analysis;online and offline;scalability;stream (computing);streaming media	Dang-Hoan Tran;Kai-Uwe Sattler	2013		10.1007/978-3-642-39640-3_31	correlation clustering;data stream clustering;cluster analysis	ML	-3.0284880692302907	-38.96533725767619	80591
18e74a84b1603e5f943bd03a5962bff8220ce21a	www 2011 invited tutorial overview: latent variable models on the internet	latent variables;relational data;document analysis;data integrity;latent variable;cluster of workstations;latent dirichlet allocation;sampling;latent variable model;data analysis;user profile;graphical models;recommender system;mixture model;clustering;graphical model;topic models	Graphical models are an effective tool for analyzing structured and relational data. In particular, they allow us to arrive at insights that are implicit, i.e. latent in the data. Dealing with such data on the internet poses a range of challenges. Firstly, the sheer size renders many well-known inference algorithms infeasible. Secondly, the problems arising on the internet do not always fit well into the known categories for latent variable inference such as Latent Dirichlet Allocation or clustering.  In this tutorial we address a number of aspects. Firstly, we present a variety of applications ranging from general purpose document analysis, ideology detection, clustering of sequential data, and dynamic user profiling to recommender systems and data integration. Secondly we give an overview over a number of popular models such as mixture models, topic models, nonparametric variants of temporal dependence, and an integrated analysis and clustering approach, all of which can be used to solve a range of data analysis problems at hand. Thirdly, we present a range of sampling based algorithms for large scale distributed inference using multicore systems and clusters of workstations.	algorithm;cluster analysis;computer cluster;graphical model;internet;latent dirichlet allocation;latent variable;mixture model;multi-core processor;recommender system;rendering (computer graphics);sampling (signal processing);www;workstation	Amr Ahmed;Alexander J. Smola	2011		10.1145/1963192.1963311	latent variable;computer science;data science;machine learning;data mining;database;graphical model;recommender system	ML	-2.262428088362648	-39.757512172406685	80815
fd1ef1d13682d8151f4fcc0c77070b103d2ee602	chinese historic image threshold using adaptive k-means cluster and bradley's			david w. bradley;k-means clustering	Zhi-Kai Huang;Yong-Li Ma;Li Lu;Fan-Xing Rao;Ling-Ying Hou	2016		10.1007/978-3-319-42297-8_17	speech recognition;algorithm	Vision	3.0636131806364624	-44.45415775508444	80909
b336a952d9a845e4fc3aed3c2ec03efd4d3d6973	simulated annealing and optimal protocols	cancer;simulated annealing;optimization;vaccine	Many human tumors cannot easily be avoided. In most cases a prophylactic vaccination prevents the tumor growth. In particular the Triplex vaccine prevented mammary carcinoma formation using a Chronic schedule, but it is not known if this schedule is minimal. A computational model named SimTriplex was able to reproduce in silico the in vivo experiments. The combination of SimTriplex and Genetic Algorithms (GA), produced a minimal vaccination schedule capable to guarantee in silico survival. This process required the use of a High Performance Computing (HPC) infrastructure for several days. In this paper we show another optimization procedure based on simulated annealing approach used to get a faster algorithm response.	computation;computational model;experiment;genetic algorithm;heuristic;mathematical optimization;programming paradigm;schedule (computer science);simulated annealing;software release life cycle;video-in video-out	Marzio Pennisi;Roberto Catanuto;Francesco Pappalardo;Santo Motta;Emilio Mastriani;Alessandro Cincotti	2009	Journal of Circuits, Systems, and Computers	10.1142/S0218126609005770	simulation;simulated annealing;computer science;bioinformatics;engineering;cancer	HPC	1.9368420705498157	-47.07463679723874	81062
53ebdfec4180d25080a001551f94335273ea719f	robustness and evolvability of recombination in linear genetic programming	genetic programming;accessibility;neutrality;recombination;robustness;evolvability	The effect of neutrality on evolutionary search is known to be crucially dependent on the distribution of genotypes over phenotypes. Quantitatively characterizing robustness and evolvability in genotype and phenotype spaces greatly helps to understand the influence of neutrality on Genetic Programming. Most existing robustness and evolvability studies focus on mutations with a lack of investigation of recombinational operations. Here, we extend a previously proposed quantitative approach of measuring mutational robustness and evolvability in Linear GP. By considering a simple LGP system that has a compact representation and enumerable genotype and phenotype spaces, we quantitatively characterize the robustness and evolvability of recombination at the phenotypic level. In this simple yet representative LGP system, we show that recombinational properties are correlated with mutational properties. Utilizing a population evolution experiment, we demonstrate that recombination significantly accelerates the evolutionary search process and particularly promotes robust phenotypes that innovative phenotypic explorations.	linear genetic programming	Ting Hu;Wolfgang Banzhaf;Jason H. Moore	2013		10.1007/978-3-642-37207-0_9	genetic programming;evolutionary capacitance;robustness;computer science;bioinformatics;accessibility;genetics;evolvability;recombination;robustness	Web+IR	-4.268678037311163	-47.52439142425341	81074
848188668f6c8a9505f168ca09d6732bef04c7df	an automatic gene ontology software tool for bicluster and cluster comparisons	stress;bicluster method;biology computing;cluster algorithm;software tool;mathematics computing;gene regulatory networks;gene regulatory networks automatic gene ontology software bicluster method cluster method open source matlab software tool microarray datasets;biology;data mining;ontologies artificial intelligence;genetics;gene expression;public domain software;clustering method;clustering algorithms;ontologies;software tools;automatic gene ontology software;cluster method;gene regulatory network;dimensional reduction;open source matlab software tool;software tools biology computing cellular biophysics genetics mathematics computing ontologies artificial intelligence public domain software;cellular biophysics;microarray datasets;open source;ontologies software tools clustering algorithms bioinformatics open source software genomics software algorithms genetics gene expression biological information theory;bioinformatics;gene ontology	We propose an Automatic Gene Ontology (AGO) software as a flexible, open-source Matlab software tool that allows the user to easily compare the results of the bicluster and cluster methods. This software provides several methods to differentiate and compare the results of candidate algorithms. The results reveal that bicluster/cluster algorithms could be considered as integrated modules to recover the interesting patterns in the microarray datasets. The further application of AGO could to solve the dimensionality reduction of the gene regulatory networks. Availability: AGO and help file is available at http://home.k-space.org/FADL/Downloads/AGO_prgram.zip	algorithm;biclustering;dimensionality reduction;gene ontology;gene regulatory network;matlab;microarray;open-source software;programming tool	Fadhl M. Al-Akwaa;Yasser M. Kadah	2009	2009 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology	10.1109/CIBCB.2009.4925723	biology;gene regulatory network;computer science;bioinformatics;theoretical computer science;data mining;genetics	Comp.	5.036697201142927	-49.3870189856232	81471
e34b29cb1f51e0c6a64850c67555ad7080253e17	an effective algorithm based on density clustering framework	clustering analysis data analysis data mining machine learning information retrieval density based clustering framework dcf neighborhood density estimation model;clustering algorithms reverse k nearest neighbors neighborhood density estimation data mining minimum spanning tree;shape;estimation;data analysis data mining pattern clustering;heuristic algorithms;classification algorithms;clustering algorithms;clustering algorithms partitioning algorithms algorithm design and analysis classification algorithms shape estimation heuristic algorithms;algorithm design and analysis;partitioning algorithms	Clustering analysis has the very broad applications on data analysis, such as data mining, machine learning, and information retrieval. In practice, most of clustering algorithms suffer from the effects of noises, different densities and shapes, cluster overlaps, etc. To solve the problems, in this paper, we propose a simple but effective density-based clustering framework (DCF) and implement a clustering algorithm based on DCF. In DCF, a raw data set is partitioned into core points and non-core points by a neighborhood density estimation model, and then the core points are clustered first, because they usually represent the center or dense region of the cluster structure. Finally, DCF classifies the non-core points into initial clusters in sequence. In experiments, we compare our algorithm with Dp and DBSCAN algorithms on synthetic and real-world data sets. The experimental results show that the performance of the proposed clustering algorithm is comparable with DBSCAN and Dp algorithms.	algorithm;cluster analysis;dbscan;data mining;design rule for camera file system;experiment;information retrieval;machine learning;synthetic intelligence	Jianyun Lu;Qingsheng Zhu	2017	IEEE Access	10.1109/ACCESS.2017.2688477	statistical classification;correlation clustering;constrained clustering;algorithm design;determining the number of clusters in a data set;estimation;data stream clustering;subclu;k-medians clustering;fuzzy clustering;flame clustering;shape;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;brown clustering;dbscan;optics algorithm;biclustering;affinity propagation;statistics;k-means clustering;clustering high-dimensional data	ML	0.40363624428794886	-40.4913516371204	81559
e80a31d3cc2f1e0c7e71b98b0abc4f724dfda40a	clustering in a multi-agent data mining environment	cluster algorithm;liverpool;agent based;multi agent data mining;average distance;agent based clustering;data mining;repository;university	A Multi-Agent based approach to clustering using a generic Multi-Agent Data Mining (MADM) framework is described. The process use a collection of agents, running several different clustering algorithms, to determine a “best” cluster configuration. The issue of determining the most appropriate configuration is a challenging one, and is addressed in this paper by considering two metrics, total Within Group Average Distance (WGAD) to determine cluster cohesion, and total Between Group Distance (BGD) to determine separation. The proposed process is implemented using the MASminer MADM framework which is also introduced in this paper. Both the clustering technique and MASminer are evaluated. Comparison of the two “best fit” measures indicates that WGAD can be argued to be the most appropriate metric.	agent-based model;algorithm;cluster analysis;curve fitting;data mining;experiment;fold (higher-order function);multi-agent system	Santhana Chaimontree;Katie Atkinson;Frans Coenen	2010		10.1007/978-3-642-15420-1_9	correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;computer science;data science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;affinity propagation;clustering high-dimensional data	ML	3.084124348140356	-42.571131666888654	81603
78dd56b27c6fdd52212c01f4d7bea6e27932d269	enhancement of initial equivalency for protein structure alignment based on encoded local structures	biology computing;optimisation;molecular configurations;vectors biology computing iterative methods molecular biophysics molecular configurations optimisation proteins statistical analysis;structure alignment initial equivalency secondary structure structural alphabet;fr tm align protein structure alignment algorithms encoded local structures iterative optimization process near optimal alignments surrounding solution space vector based alignment algorithm encoded local structural alphabets twilight zone statistical analysis match index mirage align algorithm residue based algorithm tm align;iterative methods;proteins;statistical analysis;vectors;secondary structure;molecular biophysics;amino acids;initial equivalency;optimization;structural alphabet;protein engineering;algorithm design and analysis;proteins algorithm design and analysis protein engineering optimization vectors amino acids statistical analysis;structure alignment	Most alignment algorithms find an initial equivalent residue pair followed by an iterative optimization process to explore better near-optimal alignments in the surrounding solution space of the initial alignment. It plays a decisive role in determining the alignment quality since a poor initial alignment may make the final alignment trapped in an undesirable local optimum even with an iterative optimization. We proposed a vector-based alignment algorithm with a new initial alignment approach accounting for local structure features called MIRAGE-align. The new idea is to enhance the quality of the initial alignment based on encoded local structural alphabets to identify the protein structure pair whose sequence identity falls in or below twilight zone. The statistical analysis of alignment quality based on match index and computation time demonstrated that MIRAGE-align algorithm outperformed four previously published algorithms, i.e., the residue-based algorithm, the vector-based algorithm, TM-align, and Fr-TM-align. MIRAGE-align yields a better estimate of initial solution to enhance the quality of initial alignment and enable the employment of a noniterative optimization process to achieve a better alignment.	accidental falls;algorithm;align (company);alphabet;appendix;behavior;computation (action);download;executable;feasible region;heuristic;heuristics;iterative method;local optimum;mirage syndrome;mathematical optimization;michael j. fischer;microsoft windows;mirage porcelain;protein, organized by function;protein, organized by structure;randomness;scientific publication;scop;sequence alignment;test set;time complexity	Kenneth Hung;Jui-Chih Wang;Cheng-Wei Chen;Cheng-Long Chuang;Kun-Nan Tsai;Chung-Ming Chen	2012	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2012.2204892	algorithm design;structural alignment;computer science;bioinformatics;theoretical computer science;machine learning;mathematics;protein engineering;iterative method;protein secondary structure;molecular biophysics	Comp.	-1.2263774393268176	-50.377870771032214	81725
d0660929b284e7828e8db33f5803e9ff07573b73	breast cancer classification by artificial immune algorithm based validity interval cells selection.		We present in this work an Artificial Immune System (AIS) algorithm for breast cancer classification and diagnosis. The main contribution is to select memory cells according to their belonging to a validity interval based on average similarity of training cells. The behaviour of these created memory cells preserves the diversity of original cancer learning class. All these operations allow to generate a set of memory cells with a global representativeness of the database which enables breast cancer classification and recognition. Promising results have been obtained on both Wisconsin Diagnosis Breast Cancer Database (WDBC) and (DDSM) Digital Database for Screening Mammography.	algorithm;artificial immune system;database;memory cell (binary);mutation (genetic algorithm);vehicle information and communication system	Rima Daoudi;Khalifa Djemal	2016		10.5220/0006057202090216	machine learning;pattern recognition;statistics	Comp.	8.97444844500871	-41.78446576622964	81841
d3e673feae9c8c53c5deae1b9c1cce40b9f53658	fast scalable selection algorithms for large scale data	binmedian method fast scalable selection algorithms large scale data mapreduce framework computing cluster environment hadoop sorting method pig method;sorting algorithm design and analysis clustering algorithms partitioning algorithms libraries computer science educational institutions;median finding;selection algorithms;distributed processing;public domain software;big data;median finding hadoop map reduce selection algorithms;public domain software big data distributed processing;map reduce;hadoop	Selection finding, and its most common form median finding, are used as a measure of central tendency for problems in biology, databases, and graphics. These problems often require selection finding as a subcomponent where it can be called many times, and as such speed is important. The Map/Reduce framework has been shown to be an important tool for creating scalable applications. There are a number of valid implementations of the selection algorithms inside of a Map/Reduce framework, certain of which are compared in this paper. However, as the volume of data increases, subtle theoretical algorithmic implementation differences can lead to significant differences in practical application. Therefore, an efficient and scalable selection finding method has the potential to provide general benefit to a number of applications. This paper compares algorithms that have been redesigned or created for the Map/Reduce framework for the purpose of selection finding, or, finding the k-th ranked element in an unordered set. This paper takes the concepts used from two existing selection algorithms and translates them into a novel method using the Map/Reduce framework with two variations. Each approach uses a different methodology to reduce the total amount of workload needed for a selection. All the algorithms are compared together for scalability and efficiency in a computing cluster environment with up to 256 processing cores. The results show that the methods proposed in this paper outperform several common alternatives in identifying medians with Hadoop, including using sorting, Pig, and BinMedian methods. Our implementations are also available upon request.	apache hadoop;computer cluster;database;graphics;mapreduce;scalability;selection algorithm;sorting;unordered associative containers (c++)	Lee Parnell Thompson;Weijia Xu;Daniel P. Miranker	2013	2013 IEEE International Conference on Big Data	10.1109/BigData.2013.6691602	computer science;theoretical computer science;data mining;database	DB	-2.6221577565062395	-39.666341115116865	82076
331ea8e355d15a7df3accdd8923f5ee574cc88d0	bayesian inference on hidden knowledge in high-throughput molecular biology data	knowledge network;learning process;hidden information;bayesian inference;convergence rate;information overload;molecular biology;genetic network;prediction accuracy;missing data;synthetic data;high throughput;bayesian model	Along with the information overload brought about by the Internet in communication, economics, and sociology, high-throughput biology techniques produce vast amount of data, which are usually represented in form of matrices and considered as knowledge networks. A spectral based approach has been proved useful in extracting hidden information within such networks to estimate missing data. In this paper, we propose the use of a simple nonparametric Bayesian model to fully automate this approach and better utilize the available data at each stage of the learning process. Although the algorithm is developed with a general purpose in mind, within the scope of this paper, we evaluate its performance by applying on three different examples from the field of proteomics and genetic networks. The comparison with other general or data-specific methods has shown favor to ours. Systematic tests on synthetic data are also performed, showing the approach's robustness in handling large percentage of missing data both in term of prediction accuracy and convergence rate. Finally, we describe a procedure to explore the nature of different types of noise containing within investigated systems.		Viet Anh Nguyen;Zdena Koukolíková-Nicola;Franco Bagnoli;Pietro Liò	2008		10.1007/978-3-540-89197-0_77	computer science;artificial intelligence;data science;machine learning;data mining;bayesian inference;statistics	Comp.	5.718700192839555	-49.795215469337315	82437
26d31c8daeb05519b4085021a5ac1d71310697b8	consistency and rates for clustering with dbscan		We propose a simple and efficient modification of the popular DBSCAN clustering algorithm. This modification is able to detect the most interesting vertical threshold level in an automated, data-driven way. We establish both consistency and optimal learning rates for this modification.	algorithm;cluster analysis;dbscan	Bharath K. Sriperumbudur;Ingo Steinwart	2012			subclu;pattern recognition;data mining;mathematics;cluster analysis;dbscan;optics algorithm;statistics	ML	1.235481679188891	-40.61306021420021	82673
2fab9cb5c2ab524425f7d38a08317700057a28ee	a new pairwise kernel for biological network inference with support vector machines	indirect inference;distance metric learning;saccharomyces cerevisiae;metabolic networks and pathways;systems biology;protein sequence;gene regulation;signal transduction;supervised classification;saccharomyces cerevisiae proteins;heterogeneous data;convex optimization;computational biology bioinformatics;gene expression;protein protein interaction;models statistical;algorithms;support vector machine;combinatorial libraries;computer appl in life sciences;gene expression profiling;biological network;microarrays;bioinformatics	Much recent work in bioinformatics has focused on the inference of various types of biological networks, representing gene regulation, metabolic processes, protein-protein interactions, etc. A common setting involves inferring network edges in a supervised fashion from a set of high-confidence edges, possibly characterized by multiple, heterogeneous data sets (protein sequence, gene expression, etc.). Here, we distinguish between two modes of inference in this setting: direct inference based upon similarities between nodes joined by an edge, and indirect inference based upon similarities between one pair of nodes and another pair of nodes. We propose a supervised approach for the direct case by translating it into a distance metric learning problem. A relaxation of the resulting convex optimization problem leads to the support vector machine (SVM) algorithm with a particular kernel for pairs, which we call the metric learning pairwise kernel. This new kernel for pairs can easily be used by most SVM implementations to solve problems of supervised classification and inference of pairwise relationships from heterogeneous data. We demonstrate, using several real biological networks and genomic datasets, that this approach often improves upon the state-of-the-art SVM for indirect inference with another pairwise kernel, and that the combination of both kernels always improves upon each individual kernel. The metric learning pairwise kernel is a new formulation to infer pairwise relationships with SVM, which provides state-of-the-art results for the inference of several biological networks from heterogeneous genomic data.	algorithm;bioinformatics;biological network inference;convex optimization;gene expression regulation;genetic heterogeneity;kernel (operating system);linear programming relaxation;mathematical optimization;metabolism;optimization problem;supervised learning;support vector machine;protein protein interaction	Jean-Philippe Vert;Jian Qiu;William Stafford Noble	2007	BMC Bioinformatics	10.1186/1471-2105-8-S10-S8	protein–protein interaction;biology;support vector machine;biological network;convex optimization;regulation of gene expression;gene expression;dna microarray;bioinformatics;machine learning;protein sequencing;data mining;gene expression profiling;systems biology;signal transduction	ML	7.184016902644744	-51.11958169200798	82742
31444822bcc871c434aa0487e6c1c88c5d4b4d80	improving software quality classification with random projection	performance measure;focusing;software metrics;random projection classification software metrics;software quality management;fault high software modules software quality classification random projection software product software engineering software metrics software quality management naive bayes ripper feature extraction;information science;software maintenance;bayes methods;naive bayes;software fault tolerance;classification;software engineering;software quality bayes methods software fault tolerance software metrics;feature extraction;principal component analysis;software metric;software tools;software quality software metrics software maintenance software tools feature extraction benchmark testing focusing principal component analysis information science software engineering;random projection;software product;ripper;software quality;benchmark testing;fault high software modules;software quality classification	Improving the quality of software products is one of the principal objectives of software engineering. Software metrics are the key tool in software quality management. In this paper we propose to use naive Bayes and RIPPER for software quality classification and use random projection to improve the performance of classifiers. Feature extraction via random projection has attracted considerable attention in recent years. The approach has interesting theoretical underpinnings and offers computational advantages. Results on benchmark dataset MIS, using Accuracy and Recall as performance measures, indicate that random projection can improve the classification performance of all four learners we investigate: Naive Bayes, RIPPER, MLP and IB1. With the help of random projection, naive Bayes and RIPPER are better than MLP and IB1 in finding fault-high software modules, which can be sought as potentially highly faulty modules where most of our testing and maintenance effort should be focused	benchmark (computing);feature extraction;memory-level parallelism;naive bayes classifier;random projection;software engineering;software metric;software quality management	Xin Jin;Rongfang Bie	2006	2006 5th IEEE International Conference on Cognitive Informatics	10.1109/COGINF.2006.365690	reliability engineering;computer science;data mining;database	SE	7.0769939067513645	-40.533097060231675	82792
7c6494f983460a8b002447a4e1007e5507e0ba58	mining diversified association rules in big datasets: a cluster/gpu/genetic approach		Abstract Association rule mining is a popular data mining task, which has important in many domains. Because the task of association rule mining is very time consuming, evolutionary and swarm based algorithms have been designed to find approximate solutions. However, these approaches still have long execution times, especially when applied on dense and big databases, or when low minsup and minconf threshold values are used. Moreover, these approaches suffer from the lack of diversity in the rules presented to the user. To address these drawbacks of previous algorithms, this paper proposes an efficient parallel algorithm named CGPUGA. It is a genetic algorithm that runs on clusters of GPUs to efficiently discover diversified association rules. It benefits from cluster computing to generate rules. Then, to evaluate rules, which is the most time consuming task, the designed algorithm relies on the massively parallel GPU threads. Furthermore, to deal with the issue of rule quality, the search space of rules is partitioned into several regions assigned to different workers, and rules found by each workers are the merged to ensure diversification. The designed approach has been empirically compared with state-of-the-art algorithms using small, medium, large and big datasets. Results reveal that CGPUGA is 600 times faster than the sequential version of the algorithm for big datasets. Moreover, it outperforms state-of-the-art high performance computing based association rule mining algorithms for real big datasets such as Pokec, Webdocs and Wikilinks. In terms of rule quality, results show that the designed CGPUGA algorithm provides rules of higher quality compared to the state-of-the-art NIGGAR, MSP-MPSO and MPGA algorithms for diversified association rule mining.	association rule learning;big data;graphics processing unit	Youcef Djenouri;Asma Belhadi;Philippe Fournier-Viger;Hamido Fujita	2018	Inf. Sci.	10.1016/j.ins.2018.05.031	massively parallel;machine learning;artificial intelligence;genetic algorithm;swarm behaviour;mathematics;association rule learning;thread (computing);parallel algorithm;supercomputer;computer cluster	DB	-3.1137691137992114	-39.67162896927854	82916
bdf3870ece9b2a751edb234b143b49a2366008dd	evolution of incremental complex behavior on cellular machines		Complex multi-cellular organisms are the result of evolution over billions of years. Their ability to reproduce and survive through adaptation to selection pressure did not happen suddenly; it required gradual genome evolution that eventually led to an increased emergent complexity. In this paper we investigate the emergence of complexity in cellular machines, using two different evolutionary strategies. The first approach is a conventional genetic algorithm, where the target is the maximum complexity. This is compared to an incremental approach, where complexity is gradually evolved. We show that an incremental methodology could be better suited to help evolution to discover complex emergent behaviors. We also propose the usage of a genome parameter to detect the behavioral regime. The parameter may indicate if the evolving genomes are likely to be able to achieve more complex behaviors, giving information on the evolvability of the system. The experimental model used herein is based on 2-dimensional cellular automata. We show that the incremental approach is promising when evolution targets an increase of complexity.	artificial life;automata theory;cellular automaton;emergence;evolution;evolutionary algorithm;experiment;feasible region;genetic algorithm;genetic distance;information processing	Stefano Nichele;Gunnar Tufte	2013		10.7551/978-0-262-31709-2-ch011	biology;simulation;bioinformatics;artificial intelligence;machine learning	ML	-4.4908832505730905	-47.6309201390424	83021
d2558df92f11c4447ae42e37896a837eac80e52e	knowledge discovery from medical data: an empirical study with xcs	empirical study;learning classifier system;data mining;breast cancer;knowledge discovery	In this chapter we describe the use of a modern learning classifier system to a data mining task. In particular, in collaboration with a medical specialist, we apply XCS to a primary breast cancer data set. Our results indicate more effective knowledge discovery than with C4.5.		Faten Kharbat;Mohammed Odeh;Larry Bull	2008		10.1007/978-3-540-78979-6_5	computer science;data science;machine learning;data mining;k-optimal pattern discovery	ML	5.761866324430749	-45.63722613284044	83033
c70a0f5e40a1cc66eaf857f1d0604553605b5ed2	filling the gaps in solar big data: interpolation of solar filament event instances	interpolation;shape;big data;time series analysis;observatories;sun;clustering algorithms	This paper introduces a new interpolation method that fills the gap in missing solar filament big data that are captured every day from numerous ground-based and space-based observatories. It proposes a new algorithm that takes two filament event instances and interpolates between them given a cadence. The method combines K-means clustering algorithm, time series shape representation, and linear interpolation to generate the missing filament polygons. This is the first research of this kind that attempts to address the problem of solar big data interpolation. We evaluate the proposed method using area, shape, and distance accuracy criteria. Finally, we outline future improvements and opportunities for solar data interpolation.	algorithm;big data;cluster analysis;data mining;dynamic time warping;electron hole;feature vector;jaccard index;k-means clustering;linear interpolation;sigmoid function;time series	Soukaina Filali Boubrahimi;Berkay Aydin;Dustin Kempton;Sushant S. Mahajan;Rafal A. Angryk	2016	2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)	10.1109/BDCloud-SocialCom-SustainCom.2016.25	mathematical optimization;big data;interpolation;shape;computer science;data science;time series;data mining;cluster analysis	Visualization	-0.713080890512132	-39.90913924305024	83046
44b12ad462a60a58cf15a697436b265c15223536	quantum isorank: efficient alignment of multiple ppi networks		Comparative analyses of protein-protein interaction networks play important roles in the understanding of biological processes. However, the growing enormity of available data on the networks becomes a computational challenge for the conventional alignment algorithms. Quantum algorithms generally provide greater efficiency over their classical counterparts in solving various problems. One of such algorithms is the quantum phase estimation algorithm which generates the principal eigenvector of a stochastic matrix with probability one. Using the quantum phase estimation algorithm, we introduce a quantum computing approach for the alignment of protein-protein interaction networks by following the classical algorithm IsoRank which uses the principal eigenvector of the stochastic matrix representing the Kronecker product of the normalized adjacency matrices of networks for the pairwise alignment. We also present a greedy quantum measurement scheme to efficiently procure the alignment from the output state of the phase estimation algorithm where the eigenvector is encoded as the amplitudes of this state. The complexity of the quantum approach outperforms the classical running time.	adjacency matrix;greedy algorithm;measurement in quantum mechanics;pixel density;procurement;quantum algorithm;quantum computing;quantum phase estimation algorithm;stochastic matrix;time complexity	Anmer Daskin	2015	CoRR		mathematical optimization;combinatorics;theoretical computer science;machine learning;mathematics;quantum algorithm;quantum phase estimation algorithm;quantum sort	ML	-0.6590532618878494	-48.668979438181616	83068
56a6e9e6d32995e792cc7cf78edf497742e79a9b	merge method for shape-based clustering in time series microarray analysis	merge methods;time series;microarray analysis;clustering	A challenging task in time-course microarray data analysis is to combine the information provided by multiple time series in order to cluster genes meaningfully. This paper proposes a novel merge method to accomplish this goal obtaining clusters with highly correlated genes. The main idea of the proposed method is to generate a clustering, starting from clusterings created from different time series individually, that takes into account the number of times each clustering assemble two genes into the same group. Computational experiments are performed for real-world time series microarray with the purpose of finding co-expressed genes related to the production and growth of a certain bacteria. The results obtained by the introduced merge method are compared with clusterings generated by time series individually and averaged as well as interpreted	cluster analysis;computation;experiment;microarray;time series	Irene Barbero;Camelia Chira;Javier Sedano;Carlos Prieto;José Ramón Villar;Emilio Corchado	2012		10.1007/978-3-642-32639-4_99	microarray analysis techniques;computer science;bioinformatics;machine learning;time series;data mining;cluster analysis;statistics	ML	4.139188811479476	-47.137056930515655	83148
b17aeb473ebd056f1138a7a6fdbb3a538c680f5e	exploring the number of groups in robust model-based clustering	trimming;strength of group assignments;info eu repo semantics preprint;heterogeneous clusters;number of groups;estadistica	Two key questions in Clustering problems are how to determine the number of groups properly and measure the strength of group-assignments. These questions are specially involved when the presence of certain fraction of outlying data is also expected. Any answer to these two key questions should depend on the assumed probabilisticmodel, the allowed group scatters and what we understand by noise. With this in mind, some exploratory “trimming-based” tools are presented in this work together with their justifications. The monitoring of optimal values reached when solving a robust clustering criteria and the use of some “discriminant” factors are the basis for these exploratory tools.	cluster analysis;discriminant;mind	Luis Angel García-Escudero;Alfonso Gordaliza;Carlos Matrán;Agustín Mayo-Iscar	2011	Statistics and Computing	10.1007/s11222-010-9194-z	trimming;data mining;mathematics;algorithm;statistics	Theory	-0.5877482104813557	-42.813280604194254	83183
717b9843a325314529a046711042e72c6f51afdd	on improving the prediction accuracy of a decision tree using genetic algorithm		Decision trees are one of the most popular classifiers used in a wide range of real-world problems. Thus, it is very important to achieve higher prediction accuracy for decision trees. Most of the well-known decision tree induction algorithms used in practice are based on greedy approaches and hence do not consider conditional dependencies among the attributes. As a result, they may generate suboptimal solutions. In literature, often genetic programming-based (a complex variant of genetic algorithm) decision tree induction algorithms have been proposed to eliminate some of the problems of greedy approaches. However, none of the algorithms proposed so far can effectively address conditional dependencies among the attributes. In this paper, we propose a new, easy-to-implement genetic algorithm-based decision tree induction technique which is more likely to ascertain conditional dependencies among the attributes. An elaborate experimentation is conducted on thirty well known data sets from the UCI Machine Learning Repository in order to validate the effectiveness of the proposed technique.		Md. Nasim Adnan;Md Zahidul Islam;Md. Mostofa Akbar	2018		10.1007/978-3-030-05090-0_7	data mining;artificial intelligence;machine learning;genetic algorithm;knowledge extraction;genetic programming;decision tree;complex variant;computer science;data set	ML	9.400985097268038	-42.95630869177759	83213
de67cdba41a4d1def3d9f981d6b6d979ac6764a1	triangular kernel nearest neighbor based clustering for pattern extraction in spatio-temporal database	spatio temporal database;nearest neighbor searches;gaussian kernel function;pattern clustering;kernel;traffic accident;gaussian kernel nearest neighbor based clustering;visual databases decision making feature extraction gaussian processes pattern clustering temporal databases;gaussian processes;spatio temporal database clustering kernel nearest neighbor triangular kernel function gaussian kernel function;spatio temporal databases;k means;kernel function;spatio temporal database clustering;triangular kernel nearest neighbor based clustering;indexes;spatio temporal database clustering triangular kernel nearest neighbor based clustering pattern extraction decision making gaussian kernel nearest neighbor based clustering;clustering;feature extraction;nearest neighbor;number of clusters;gaussian kernel;vehicle crash testing;clustering algorithms;temporal databases;kernel nearest neighbor searches clustering algorithms indexes algorithm design and analysis vehicle crash testing;kernel nearest neighbor;triangular kernel function;pattern extraction;algorithm design and analysis;visual databases	To date, various fields of applications have utilized spatio-temporal databases not only to store data, but to support decision making. For example, in traffic accident analysis; it is required to have knowledge on the pattern of accidents resulting in death. Thus, in such analysis, clustering technique is desired to implement pattern extraction. This paper presents clustering of spatio-temporal database using kernel nearest neighbor approach. It is chosen due to its ability to determine the number of clusters automatically. There are various types of kernel functions exist in the literatures, but the issue of concern is how to determine an appropriate kernel function for this application. In this study, two commonly used kernel functions, namely Gaussian and triangular, are investigated. From various experiments conducted, both functions produce reasonable clusters, but the triangular kernel nearest neighbor based clustering (TKNN) provides better performance with smaller number of iteration compared to Gaussian kernel nearest neighbor based clustering (ILGC) and K-means. Thus, TKNN is good option in clustering spatio-temporal database.	accident analysis;cluster analysis;experiment;gaussian (software);iteration;k-means clustering;kernel (operating system);pattern recognition;spatiotemporal database;systems design;temporal database;traffic analysis	Aina Musdholifah;Siti Zaiton Mohd Hashim	2010	2010 10th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2010.5687288	nearest-neighbor chain algorithm;correlation clustering;kernel method;string kernel;kernel embedding of distributions;radial basis function kernel;computer science;machine learning;pattern recognition;data mining;mathematics;cluster analysis;tree kernel;single-linkage clustering;variable kernel density estimation;polynomial kernel;kernel smoother	AI	1.0003520238218397	-38.9924114830623	83489
d3ab5704195b61c0999378badba1c1128c385961	on combining graph-partitioning with non-parametric clustering for image segmentation	image segmentation;k means;graph partitioning;k means algorithm	The goal of this communication is to suggest an alternative implementation of the k-way Ncut approach for image segmentation. We believe that our implementation alleviates a problem associated with the Ncut algorithm for some types of images: its tendency to partition regions that are nearly uniform with respect to the segmentation parameter. Previous implementations have used the k-means algorithm to cluster the data in the eigenspace of the affinity matrix. In the k-means based implementations, the number of clusters is estimated by minimizing a function that represents the quality of the results produced by each possible value of k. Our proposed approach uses the clustering algorithm of Koontz and Fukunaga in which k is automatically selected as clusters are formed (in a single iteration). We show comparison results obtained with the two different approaches to non-parametric clustering. The Ncut generated oversegmentations are further suppressed by a grouping stage—also Ncut based—in our implementation. The affinity matrix for the grouping stage uses similarity based on the mean values of the segments. 2004 Elsevier Inc. All rights reserved. * Corresponding author. 1-765-494-0880. E-mail addresses: aleix@ecn.purdue.edu (A.M. Mart ınez), mitrapiy@ecn.purdue.edu (P. Mittrapiyanuruk), kak@ecn.purdue.edu (A.C. Kak). 1077-3142/$ see front matter 2004 Elsevier Inc. All rights reserved. doi:10.1016/j.cviu.2004.01.003 A.M. Mart ınez et al. / Computer Vision and Image Understanding 95 (2004) 72–85 73	algorithm;cluster analysis;computer vision;data mart;graph partition;image segmentation;iteration;k-means clustering;processor affinity	Aleix M. Martínez;Pradit Mittrapiyanuruk;Avinash C. Kak	2004	Computer Vision and Image Understanding	10.1016/j.cviu.2004.01.003	mathematical optimization;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;k-means clustering	AI	1.8972224635734616	-43.045038923545356	83579
d8991f6c26724c415a1b4a03a0f3b2007693a132	efficient mining of contrast patterns on large scale imbalanced real-life data	journal article	Contrast pattern mining has been studied intensively for its strong discriminative capability. However, the state-of-the-art methods rarely consider the class imbalanced problem, which has been proved to be a big challenge in mining large scale data. This paper introduces a novel pattern, i.e. converging pattern, which refers to the itemsets whose supports contrast sharply from the minority class to the majority one. A novel algorithm, ConvergMiner, which adopts T*-tree and branch bound pruning strategies to mine converging patterns efficiently, is proposed. Substantial experiments in online banking fraud detection show that the ConvergMiner greatly outperforms the existing cost-sensitive classification methods in terms of predicative accuracy. In particular, the efficiency improves with the increase of data imbalance.		Jinjiu Li;Can Wang;Wei Wei;Mu Li;Chunming Liu	2013		10.1007/978-3-642-37453-1_6	computer science;data science;data mining;information retrieval	ML	7.887620900606075	-40.11588402791287	83590
e4626c2edee897ec4698d76097cb4283ec836600	an adaptive sweep-circle spatial clustering algorithm based on gestalt		An adaptive spatial clustering (ASC) algorithm is proposed in this present study, which employs sweep-circle techniques and a dynamic threshold setting based on the Gestalt theory to detect spatial clusters. The proposed algorithm can automatically discover clusters in one pass, rather than through the modification of the initial model (for example, a minimal spanning tree, Delaunay triangulation or Voronoi diagram). It can quickly identify arbitrarily-shaped clusters while adapting efficiently to non-homogeneous density characteristics of spatial data, without the need of prior knowledge or parameters. The proposed algorithm is also ideal for use in data streaming technology with dynamic characteristics flowing in the form of spatial clustering in large data sets.	algorithm;cluster analysis;delaunay triangulation;file spanning;gestalt psychology;minimum spanning tree;ti advanced scientific computer;voronoi diagram	Qingming Zhan;Shuguang Deng;Zhihua Zheng	2017	ISPRS Int. J. Geo-Information	10.3390/ijgi6090272	correlation clustering;fuzzy clustering;data stream clustering;cluster analysis;delaunay triangulation;canopy clustering algorithm;k-medians clustering;machine learning;computer science;artificial intelligence;cure data clustering algorithm;pattern recognition	ML	0.026981546465849958	-40.34723141780084	83595
83cd164f2df27f57e1e059c2a227e8912e8616ab	asymptotics of hierarchical clustering for growing dimension	hierarchical clustering;62h30;linkage function;clustering behavior	Modern day science presentsmany challenges to data analysts. Advances in data collection provide very large (number of observations and number of dimensions) data sets. In many areas of data analysis an informative task is to find natural separations of data into homogeneous groups, i.e. clusters. In this paper we study the asymptotic behavior of hierarchical clustering in situations where both sample size and dimension grow to infinity.We derive explicit signal vs noise boundaries between different types of clustering behaviors. We also show that the clustering behavior within the boundaries is the same across a wide spectrum of asymptotic settings. © 2013 Elsevier Inc. All rights reserved.	cluster analysis;hierarchical clustering;information	Petro Borysov;Jan Hannig;J. S. Marron	2014	J. Multivariate Analysis	10.1016/j.jmva.2013.11.010	correlation clustering;constrained clustering;determining the number of clusters in a data set;k-medians clustering;fuzzy clustering;flame clustering;consensus clustering;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;biclustering;statistics;hierarchical clustering of networks;clustering high-dimensional data	ML	-0.5981528498262518	-42.66227858044781	83871
0c6f8fb086f5f8b3c69eb6a2b31518ff6e1c4a49	shuffling biological sequences	amino acid	Abstract   This paper considers the following  sequence shuffling  problem: Given a biological sequence (either DNA or protein)  s , generate a random instance among all the permutations of  s  that exhibit the  same  frequencies of  k -lets (e.g. dinucleotides, doublets of amino acids, triplets, etc.). Since certain biases in the usage of  k -lets are fundamental to biological sequences, effective generation of such sequences is essential for the evaluation of the results of many sequence analysis tools. This paper introduces two sequence shuffling algorithms: A simple swapping-based algorithm is shown to generate a near-random instance and appears to work well, although its efficiency is unproven; a generation algorithm based on Euler tours is proven to produce a precisely uniform instance, and hence solve the sequence shuffling problem, in time not much more than linear in the sequence length.		Denise B. Kandel;Yossi Matias;Ron Unger;Peter Winkler	1996	Discrete Applied Mathematics	10.1016/S0166-218X(97)81456-4	amino acid;bioinformatics;mathematics;algorithm	Theory	-0.4317529736829882	-51.46383535668011	84192
f91d57badfd813ac7cff8359c20dec3f84fbaeea	robust local outlier detection	global outlier robust local outlier detection big data mining statistical parameters chebyshevs inequality density peak reachability;robustness standards data mining big data chebyshev approximation electronic mail statistical analysis;electronic mail;standards;data mining;clustering based;outlier detection;robust;clustering based outlier detection robust data mining;statistical analysis;big data;statistical analysis big data data mining;robustness;chebyshev approximation	With the rapid expansion of data scale, big data mining and analysis has attracted increasing attention. Outlier detection as an important task of data mining is widely used in many applications. However, conventional outlier detection methods have difficulty handling large-scale datasets. In addition, most of them typically can only identify global outliers and are over sensitive to parameters variation. In this paper, we propose a robust method for robust local outlier detection with statistical parameters, which incorporates the clustering based ideas in dealing with big data. Firstly, This method find some density peaks of dataset by 3s standard. Secondly each remaining data object in the dataset is assigned to the same cluster as its nearest neighbor of higher density. Finally, we use Chebyshevs inequality and density peak reachability to identify local outliers of each group. The experimental results demonstrate the efficiency and accuracy of the proposed method in identifying both global and local outliers, Moreover, the method also proved more robust analysis than typical outlier detection methods, such as LOF and DBSCAN.	algorithm;anomaly detection;big data;cluster analysis;dbscan;data mining;decibel;distributed computing;local outlier factor;reachability;robustness (computer science);social inequality;synthetic intelligence	Haizhou Du;Shengjie Zhao;Daqiang Zhang	2015	2015 IEEE International Conference on Data Mining Workshop (ICDMW)	10.1109/ICDMW.2015.114	big data;computer science;data science;data mining;statistics;robustness;approximation theory	DB	-0.8829016442269209	-40.40232110180477	84228
da4ca9febd1e91e1b75051a4af3bcf3f329b443e	a chi-square-based decision for real-time malware detection using pe-file features		The real-time detection of malware remains an open issue, since most of the existing approaches for malware categorization focus on improving the accuracy rather than the detection time. Therefore, finding a proper balance between these two characteristics is very important, especially for such sensitive systems. In this paper, we present a fast portable executable (PE) malware detection system, which is based on the analysis of the set of Application Programming Interfaces (APIs) called by a program and some technical PE features (TPFs). We used an efficient feature selection method, which first selects the most relevant APIs and TPFs using the chi-square (KHI2) measure, and then the Phi (φ) coefficient was used to classify the features in different subsets, based on their relevance. We evaluated our method using different classifiers trained on different combinations of feature subsets. We obtained very satisfying results with more than 98% accuracy. Our system is adequate for real-time detection since it is able to categorize a file (Malware or Benign) in 0.09 seconds.	algorithm;application programming interface;chi;categorization;coefficient;executable;feature selection;malware;real-time clock;real-time transcription;relevance	Mohamed Belaoued;Smaine Mazouzi	2016	JIPS	10.3745/JIPS.03.0058	real-time computing;theoretical computer science;computer science;malware;chi-square test	SE	6.415478256267426	-39.37056544780908	84452
892ce38ef0f9c0390f9c8106642fa5c932672809	evolutionary data measures: understanding the difficulty of text classification tasks		Classification tasks are usually analysed and improved through new model architectures or hyperparameter optimisation but the underlying properties of datasets are discovered on an ad-hoc basis as errors occur. However, understanding the properties of the data is crucial in perfecting models. In this paper we analyse exactly which characteristics of a dataset best determine how difficult that dataset is for the task of text classification. We then propose an intuitive measure of difficulty for text classification datasets which is simple and fast to calculate. We show that this measure generalises to unseen data by comparing it to stateof-the-art datasets and results. This measure can be used to analyse the precise source of errors in a dataset and allows fast estimation of how difficult a dataset is to learn. We searched for this measure by training 12 classical and neural network based models on 78 real-world datasets, then use a genetic algorithm to discover the best measure of difficulty. Our difficulty-calculating code1 and datasets2 are publicly available.		Edward Collins;Sadi Poletto;Bingbing Zhang	2018				ML	7.45985411514782	-42.900810008717535	84494
fe2da4b35854f0acefb4fa3caf7aa72da43356e7	cluster identification using maximum configuration entropy	machine learning;data mining	 Clustering is an important task in data mining and machine learning. In this paper, a normalized graph sampling algorithm for clustering that improves the solution of clustering via the incorporation of a priori constraint in a stochastic graph sampling procedure is adopted. The important question of how many clusters exists in the dataset and when to terminate the clustering algorithm is solved via computing the ensemble average change in entropy. Experimental results show the feasibility of the suggested approach.		C. H. Li	2005		10.1007/11498186_15	trimethylsilyl;fluorine;hydrocarbon;medicinal chemistry;acyl group;sulfonic acid;aryl;artificial intelligence;halogen;chemistry;sulfur;pattern recognition	NLP	6.601785819687056	-48.59199459625339	84554
8cde1c98261c6767dd39f679c29355adc3031035	incremental procedures for partitioning highly intermixed multi-class datasets into hyper-spherical and hyper-ellipsoidal clusters	move to front;data model;outlier detection;data clustering;discriminant analysis;geometrical approximation;mini max partition;data models;knowledge discovery	Two procedures for partitioning large collections of highly intermixed datasets of different classes into a number of hyper-spherical or hyper-ellipsoidal clusters are presented. The incremental procedures are to generate a minimum numbers of hyper-spherical or hyper-ellipsoidal clusters with each cluster containing a maximum number of data points of the same class. The procedures extend the move-to-front algorithms originally designed for construction of minimum sized enclosing balls or ellipsoids for dataset of a single class. The resulting clusters of the dataset can be used for data modeling, outlier detection, discrimination analysis, and knowledge discovery.	algorithm;anomaly detection;cluster analysis;data modeling;data point;database;embedded system;entity;estimation theory;move-to-front transform;norm (social)	Qinglu Kong;Qiuming Zhu	2007	Data Knowl. Eng.	10.1016/j.datak.2007.03.006	data modeling;anomaly detection;data model;computer science;machine learning;pattern recognition;data mining;database;knowledge extraction;cluster analysis;linear discriminant analysis	ML	0.6508629255703544	-38.942428747154224	84709
038dab5e29c40f0833dbaef68c626edf6839146e	on the sample complexity of subspace clustering with missing data	em type algorithm sample complexity missing data large complex data analysis super polynomial partially observed vectors poly logarithmic factor subspace clustering algorithm;pattern clustering computational complexity data analysis expectation maximisation algorithm;signal processing algorithms clustering algorithms conferences signal processing vectors computational modeling monitoring;subspace clustering matrix completion	Subspace clustering is a useful tool for analyzing large complex data, but in many relevant applications missing data are common. Existing theoretical analysis of this problem shows that subspace clustering from incomplete data is possible, but that analysis requires the number of samples (i.e., partially observed vectors) to be super-polynomial in the dimension d. Such huge sample sizes are unnecessary when no data are missing and uncommon in applications. There are two main contributions in this paper. First, it is shown that if subspaces have rank at most r and the number of partially observed vectors greater than dr+1 (times a poly-logarithmic factor), then with high probability the true subspaces are the only subspaces that agree with the observed data. We may conclude that subspace clustering may be possible without impractically large sample sizes and that we can certify the output of any subspace clustering algorithm by checking its fit to the observed data. The second main contribution is a novel EM-type algorithm for subspace clustering with missing data. We demonstrate and compare it to several other algorithms. Experiments with simulated and real data show that such algorithms work well in practice.	algorithm;cluster analysis;clustering high-dimensional data;experiment;missing data;polynomial;sample complexity;with high probability	Daniel L. Pimentel-Alarcón;Robert D. Nowak;Laura Balzano	2014	2014 IEEE Workshop on Statistical Signal Processing (SSP)	10.1109/SSP.2014.6884630	correlation clustering;data stream clustering;subclu;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;mathematics;cluster analysis;brown clustering;dbscan;biclustering;affinity propagation;statistics;clustering high-dimensional data	ML	-1.8423318059425782	-40.141513655722626	84980
91bbcc6be8732805f78d46a8c7ea7210e11f2d33	document clustering via adaptive subspace iteration	document clustering;cluster algorithm;subspace identification;information retrieval;adaptive subspace identification;factor analysis;number of clusters;data reduction;alternating optimization	Document clustering has long been an important problem in information retrieval. In this paper, we present a new clustering algorithm ASI1 , which uses explicitly modeling of the subspace structure associated with each cluster. ASI simultaneously performs data reduction and subspace identification via an iterative alternating optimization procedure. Motivated from the optimization procedure, we then provide a novel method to determine the number of clusters. We also discuss the connections of ASI with various existential clustering approaches. Finally, extensive experimental results on real data sets show the effectiveness of ASI algorithm.	algorithm;cluster analysis;information retrieval;iteration;mathematical optimization	Tao Li;Sheng Ma;Mitsunori Ogihara	2004		10.1145/1008992.1009031	correlation clustering;data stream clustering;data reduction;document clustering;k-medians clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;factor analysis;information retrieval	Web+IR	-0.11968726325450867	-41.501950924352194	85096
88443690c0c01febc2764c64add745e3faf868cf	automatic clustering of eye gaze data for machine learning	cybernetics;optics;gaze tracking;classification algorithms;clustering algorithms;algorithm design and analysis;conferences	Eye gaze patterns or scanpaths of subjects looking at art while answering questions related to the art have been used to decode those tasks with the use of certain classifiers and machine learning techniques. Some of these techniques require the artwork to be divided into several Areas or Regions of Interest. In this paper, two ways of clustering the static visual stimuli - k-means and the density based clustering algorithm called OPTICS - were used for this purpose. These algorithms were used to cluster the gaze points before classification. The classification success rates were then compared. While it was observed that both k-means and OPTICS gave better success rates than manual clustering, which is itself higher than chance level, OPTICS consistently gave higher success rates than k-means given the right parameter settings. OPTICS also formed clusters that look more intuitive and consistent with the heat map readings than k-means, which formed clusters that look unintuitive and less consistent with the heat map.	cluster analysis;heat map;k-means clustering;machine learning;optics algorithm;region of interest	Khushnood Z. Naqshbandi;Tamás D. Gedeon;Umran Azziz Abdulla	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844411	algorithm design;computer vision;cybernetics;computer science;artificial intelligence;machine learning;mathematics;cluster analysis;statistics	ML	3.5199876950579845	-41.01051549415792	85352
3468c25f3d38493c271525c357d8fc7fcb22c937	ontology matching based on multi-aspect consensus clustering of communities		With the increase in the number of existing ontologies, ontology integration becomes a challenging task. A fundamental step in ontology integration is ontology matching, which is the process of finding correspondences between elements of different ontologies. For large-scale ontology matching, some authors developed a divide-and-conquer strategy, which partitions ontologies, clusters similar partitions and restricts the matching process to ontology elements of similar partitions. Works related to this strategy considered only a single ontology aspect for clustering. In this paper, we proposed a solution for ontology matching based on Bayesian Cluster Ensembles (BCE) of multiple aspects of ontology partitions. We partition ontologies applying Community Detection techniques. We believe that BCE of multiple aspects of ontology partitions can provide an ontology clustering that is more precise than the clustering of a single aspect. This can result in a more precise matching.	bayesian network;cluster analysis;consensus clustering;ontology (information science);ontology alignment;web ontology language	André Ippolito;Jorge Rady de Almeida Júnior	2016		10.5220/0005893103210326	consensus clustering;pattern recognition;data mining;ontology-based data integration;information retrieval	AI	-1.5701303573338017	-43.95493841496107	85445
4b2f2b667cbf6d72b52a34eeac26482bcca9d96c	rough sets-based identification of heart valve diseases using heart sounds	rought sets;heart sounds;classification;feature reduction;machine learning;identification	Recently, heart sound signals have been used in the detection of the heart valve status and the identification of the heart valve disease. Heart sound data sets represents real life data that contains continuous and a large number of features that could be hardly classified by most of classification techniques. Feature reduction techniques should be applied prior applying data classifier to increase the classification accuracy results. This paper introduces the ability of rough set methodology to successfully classify heart sound diseases without the need applying feature selection. The capabilities of rough set in discrimination, feature reduction classification have proved their superior in classification of objects with very excellent accuracy results. The experimental results obtained, show that the overall classification accuracy offered by the employed rough set approach is high compared with other machine learning techniques including Support Vector Machine (SVM), Hidden Naive Bayesian network (HNB), Bayesian network (BN), Naive Bayesian tree (NBT), Decision tree (DT), Sequential minimal optimization (SMO).	rough set	Mostafa A. Salama;Aboul Ella Hassanien;Jan Platos;Aly A. Fahmy;Václav Snásel	2012		10.1007/978-3-642-28942-2_60	identification;biological classification;computer science;machine learning;pattern recognition;data mining	HCI	9.681239726144725	-38.78044248605508	85502
541988afbfc93933aa3ead40f401f3bfdd5826ed	a hopfield neural network based algorithm for rna secondary structure prediction	computers;ribonucleic acids;prediction method;sensitivity and specificity;parallel algorithm;ribonucleic acid;rna secondary structure;prediction algorithms;hopfield neural nets;hopfield neural networks rna biology computing computer networks concurrent computing equations sequences parallel algorithms thermodynamics prediction methods;classical prediction method hopfield neural network based algorithm rna secondary structure prediction parallel algorithm ribonucleic acids;hopfield neural network;parallel algorithms hopfield neural nets macromolecules;hopfield neural network based algorithm;hopfield neural networks;rna;heuristic algorithms;secondary structure;classical prediction method;macromolecules;rna secondary structure prediction;algorithm design and analysis;base pair;parallel algorithms;maximum independent set	In this paper a Hopfield neural network (HNN) based parallel algorithm is presented for predicting the secondary structure of ribonucleic acids (RNA). The HNN here is used to find the near-maximum independent set of an adjacent graph made of RNA base pairs and then compute the stable secondary structure of RNA. We modified the motion equation proposed in paper to reflect more biological essence of RNA secondary structure in which the ther mo dynamic parameters of base pair is used in our algorithm to control the variation rate of inhibitory and encouragement terms in the equation. Comparisons with the algorithm presented in paper and other two classical prediction methods (Zuker 's and Nussinov 's) show that our method is more sensitive and specific. In addition, our algorithm can be very efficient and be applied to sequences up to several thousands of base long with more degree of parallelism	artificial neural network;complexity;degree of parallelism;earth bulge;hopfield network;independent set (graph theory);langton's loops;parallel algorithm;parallel computing;programming paradigm	Qi Liu;Xiuzi Ye;Yin Zhang	2006	First International Multi-Symposiums on Computer and Computational Sciences (IMSCCS'06)	10.1109/IMSCCS.2006.9	computer science;artificial intelligence;theoretical computer science;machine learning	ML	-2.269478299609457	-49.53752848530094	85911
20dd073137ddcfcec9dd94ed3c4d0eb8df9484bf	an experimental study on automatically labeling hierarchical clusters using statistical features	hierarchical clustering;document hierarchy;cluster labeling		experiment	Pucktada Treeratpituk;James P. Callan	2006		10.1145/1148170.1148328	computer science;machine learning;pattern recognition;data mining;hierarchical clustering;single-linkage clustering;dendrogram;hierarchical clustering of networks	HCI	2.1083897258627706	-41.37058444923299	85915
22fcc64418bfecaceb9ba6048ab00947ba62b085	a comparison of cluster algorithms as applied to unsupervised surveys		Abstract: When considering answering important questions with data, unsupervised data offers extensive insight opportunity and unique challenges. This study considers student survey data with a specific goal of clustering students into like groups with underlying concept of identifying different poverty levels. Fuzzy logic is considered during the data cleaning and organising phase helping to create a logical dependent variable for analysis comparison. Using multiple data reduction techniques, the survey was reduced and cleaned. Finally, multiple clustering techniques (k-means, k-modes and hierarchical clustering) are applied and compared. Though each method has strengths, the goal was to identify which was most viable when applied to survey data and specifically when trying to identify the most impoverished students.	algorithm;cluster analysis;fuzzy logic;hierarchical clustering;k-means clustering;organizing (structure);plasma cleaning;unsupervised learning	Kathleen Campbell Garwood;Arpit Arun Dhobale	2018	CoRR		data mining;artificial intelligence;fuzzy logic;machine learning;variables;cluster analysis;data reduction;survey data collection;computer science;hierarchical clustering	ML	0.27678215892048985	-42.13428703218059	86245
5df8ebd4a12623429bfcb8b2e3f0ebeb4824b748	efficient event correlation over distributed systems		Event correlation is a cornerstone for process discovery over event logs crossing multiple data sources. The computed correlation rules and process instances will greatly help us to unleash the power of process mining. However, exploring all possible event correlations over a log could be time consuming, especially when the log is large. State-of-the-art methods based on MapReduce designed to handle this challenge have offered significant performance improvements over standalone implementations. However, all existing techniques are still based on a conventional generating-and-pruning scheme. Therefore, event partitioning across multiple machines is often inefficient. In this paper, following the principle of filtering-and-verification, we propose a new algorithm, called RF-GraP, which provides a more efficient correlation over distributed systems. We present the detailed implementation of our approach and conduct a quantitative evaluation using the Spark platform. Experimental results demonstrate that the proposed method is indeed efficient. Compared to the state-of-the-art, we are able to achieve significant performance speedups with obviously less network communication.	algorithm;distributed computing;event correlation;event partitioning;mapreduce;radio frequency	Long Cheng;Boudewijn F. van Dongen;Wil M. P. van der Aalst	2017	2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)		business process discovery;data-intensive computing;big data;spark (mathematics);algorithm design;process mining;event correlation;distributed computing;computer science;event partitioning	EDA	-3.811202291763625	-39.56651137508475	86496
e8917a962d3f99c4d6bd42e0898fa19a3f8affba	simultaneous application of clustering and correspondence analysis	databases;eigenvalues and eigenfunctions;pattern clustering;optimisation;data mining;eigenvalues;fuzzy set theory;clustering algorithms algorithm design and analysis entropy data mining industrial engineering educational institutions eigenvalues and eigenfunctions lagrangian functions marine vehicles databases;objective function;iterative methods;fuzzy clustering;picard iteration;marine vehicles;eigenvalues and eigenfunctions pattern clustering fuzzy set theory optimisation iterative methods;correspondence analysis;adaptive method;fuzzy cluster membership;clustering algorithms;entropy;fuzzy c means clustering;eigenvalues correspondence analysis fuzzy c means clustering algorithm objective function maximization fuzzy cluster membership objective function picard iteration adaptive method;algorithm design and analysis;lagrangian functions;industrial engineering;objective function maximization;fuzzy c means clustering algorithm	An algorithm which simultaneously applies the fuzzy c-means clustering algorithm and the correspondence analysis is developed. Maximization of an objective function yields membership of fuzzy clusters and assigns values to categories and individuals in the correspondence analysis. A regularization term is introduced into the objective function. The algorithms are Picard iteration through necessary conditions of the optimality for the objective function. An adaptive method using eigenvalues is introduced.	cluster analysis;correspondence analysis	Asuka Yamakawa;Katushiko Honda;Hidetomo Ichihashi;Tetsuya Miyoshi	1999		10.1109/IJCNN.1999.830865	fixed-point iteration;algorithm design;entropy;mathematical optimization;discrete mathematics;fuzzy clustering;eigenvalues and eigenvectors;computer science;machine learning;mathematics;iterative method;fuzzy set;cluster analysis;correspondence analysis;multiple correspondence analysis	ML	3.2425994762663177	-39.72188515505118	86703
f779518475c76958c1af2b1a7bebbdd4ec304314	evolutionary algorithms for de novo drug design - a survey	soft computing;multi objective optimization;computer aided drug design;genetic algorithm;evolutionary algorithm;de novo drug design	In de novo drug design multiple pharmaceutically important parameters need to be optimized. Various tools using evolutionary algorithm, a soft computing technique for multi-objective optimization to find novel molecules for drug development are surveyed. De novo drug design supplies novel molecules for drug development.In de novo drug design multiple parameters are optimized.Evolutionary algorithms are used for multi-objective optimization.Evolutionary algorithms used in de novo drug design tools are analyzed here. The process of drug design and discovery demands several man years and huge investment. Computer-aided drug design (CADD) technique is an aid to speed up the drug discovery process. De novo drug design, a CADD technique to identify drug-like novel chemical structures from a huge chemical search space, helps to find new drugs by the optimization of multiple pharmaceutically relevant parameters required for a successful drug. As the search space is very large in the case of de novo drug design, evolutionary algorithm (EA), a soft computing technique can be used to find an optimal solution, which in this case is a novel drug. In this paper, various EA techniques used in de novo drug design tools are surveyed and analyzed in detail, with particular emphasis on the computational aspects.	de novo transcriptome assembly;evolutionary algorithm	R. Vasundhara Devi;S. Siva Sathya;Mohane Selvaraj Coumar	2015	Appl. Soft Comput.	10.1016/j.asoc.2014.09.042	genetic algorithm;computer science;bioinformatics;artificial intelligence;multi-objective optimization;machine learning;evolutionary algorithm;soft computing	EDA	2.34525552060687	-47.09140611771372	86721
7fbcd5db0b891553953bd834b3d76f21ac6fbbe8	attribute-based decision graphs: a framework for multiclass data classification	interdisciplinar;graph based classification;attribute based decision graphs;data graph construction;missing attribute values;multiclass classification	Graph-based algorithms have been successfully applied in machine learning and data mining tasks. A simple but, widely used, approach to build graphs from vector-based data is to consider each data instance as a vertex and connecting pairs of it using a similarity measure. Although this abstraction presents some advantages, such as arbitrary shape representation of the original data, it is still tied to some drawbacks, for example, it is dependent on the choice of a pre-defined distance metric and is biased by the local information among data instances. Aiming at exploring alternative ways to build graphs from data, this paper proposes an algorithm for constructing a new type of graph, called Attribute-based Decision Graph-AbDG. Given a vector-based data set, an AbDG is built by partitioning each data attribute range into disjoint intervals and representing each interval as a vertex. The edges are then established between vertices from different attributes according to a pre-defined pattern. Classification is performed through a matching process among the attribute values of the new instance and AbDG. Moreover, AbDG provides an inner mechanism to handle missing attribute values, which contributes for expanding its applicability. Results of classification tasks have shown that AbDG is a competitive approach when compared to well-known multiclass algorithms. The main contribution of the proposed framework is the combination of the advantages of attribute-based and graph-based techniques to perform robust pattern matching data classification, while permitting the analysis the input data considering only a subset of its attributes.	algorithm;data mining;decision tree;gene distance metric;graph - visual representation;machine learning;new type;pattern matching;similarity measure;subgroup;vertex (graph theory)	João Roberto Bertini;Maria do Carmo Nicoletti;Liang Zhao	2017	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2016.09.008	variable and attribute;attribute domain;computer science;machine learning;multiclass classification;pattern recognition;data mining;mathematics;complement graph;indifference graph;statistics;synthetic data	DB	6.42936015164363	-42.22974222910735	86787
d43f1538f2e5bd963658fbf2f73525aa9e5f6043	efficient mean‐shift clustering using gaussian kd‐tree	mean shift;kd tree;i 4 computing methodologies image processing and computer vision applications	Mean shift is a popular approach for data clustering, however, the high computational complexity of the mean shift procedure limits its practical applications in high dimensional and large data set clustering. In this paper, we propose an efficient method that allows mean shift clustering performed on large data set containing tens of millions of points at interactive rate. The key in our method is a new scheme for approximating mean shift procedure using a greatly reduced feature space. This reduced feature space is adaptive clustering of the original data set, and is generated by applying adaptive KD-tree in a high-dimensional affinity space. The proposed method significantly reduces the computational cost while obtaining almost the same clustering results as the standard mean shift procedure. We present several kinds of data clustering applications to illustrate the efficiency of the proposed method, including image and video segmentation, static geometry model and time-varying sequences	adaptive filter;algorithm;blackwell (series);cluster analysis;compiler;computation;computational complexity theory;eurographics;feature vector;image resolution;mean shift;processor affinity;upsampling	Chunxia Xiao;Meng Liu	2010	Comput. Graph. Forum	10.1111/j.1467-8659.2010.01793.x	correlation clustering;data stream clustering;mean-shift;k-medians clustering;fuzzy clustering;flame clustering;computer science;theoretical computer science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;k-d tree;data mining;cluster analysis;programming language;dbscan;biclustering;k-means clustering;clustering high-dimensional data	ML	-2.4741072184626933	-40.612097125333065	87129
5f98ab703e713df2583d84c2e489a5712f908fd7	variable selection for fisher linear discriminant analysis using the modified sequential backward selection algorithm for the microarray data	microarray data;weighted mahalanobis distance;fisher linear discriminant analysis;modified sbs;variable selection	One of the major challenges is small sample size as compared to large features number for microarray data. Variable selection is an important step for improving diagnostics of cancer or the classification according to the phenotypes via gene expression data. In this study, we propose a modified sequential backward selection (SBS) algorithm to deal with the case where the covariance matrix is singular. Then we propose a variable selection algorithm based on the weighted Mahalanobis distance and modified SBS methods. Furthermore, based on the proposed variable selection algorithm, a Fisher linear discriminant method is proposed to improve the accuracy of tumor classification through simultaneously taking into account genes’ joint discriminatory power. To validate the efficiency, we apply the proposed discriminant method to two different DNA microarray data sets for experiment investigation. The empirical results show that our method for tumor classification can obtain better classification effectiveness than Markov random field method and independent variable group analysis I methods, which demonstrates that the proposed variable selection method can obtain more correct and informative gene subset if taking into account the joint discriminatory power of genes for tumor classification.	feature selection;linear discriminant analysis;microarray;selection algorithm	Hongyi Peng;Chunfu Jiang;Xiang Fang;Jinshan Liu	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.03.141	microarray analysis techniques;econometrics;pattern recognition;mathematics;linear discriminant analysis;feature selection;statistics	ML	7.846273216449283	-49.49885885424829	87155
5b3dc9013e4e55160ce3da2e15ca92987d98ab92	on the gene team mining problem	set theory bioinformatics cellular biophysics data mining genetics;data mining biological cells genomics bioinformatics computational biology clustering algorithms databases;set theory;data mining;genetics;apriori algorithm gene team mining problem chromosome representation genes set permutation representation δ set δ team gene team finding problem chromosome set apriori technique data mining apriori property pseudosupport concept;cellular biophysics;bioinformatics	Let Σ be a set of n genes. A chromosome G can be represented as a permutation of Σ. A subset D of Σ is a δ-set of G if two consecutive genes in G∩D has distance at most δ. For a set G of m chromosomes, a set D is a δ-team of G if D is a δ-set of every chromosome of G. Given a gene set Σ, a chromosome set G, and an integer δ, the gene team finding problem is to find all possible δ-teams of G. Given a gene set Σ, a chromosome set G of m chromosomes, an integer k ≤ m, and an integer δ, the gene team mining problem is to find all possible δ-teams for any possible chromosome set G' such that G' ⊆ G and |G'| ≥ k. In this paper, we study the gene team mining problem. It is known that the Apriori technique is used wildly in data mining. However, the gene team mining problem has no Apriori property that all nonempty subsets of a δ-team (δ-set) must also be a δ-team (δ-set). Thus, many techniques used in data mining cannot be applied for this gene team mining problem. In this paper, we propose a concept of pseudo-support. By using this concept, an Apriori-like algorithm can be obtained to solve the gene team mining problem.	apriori algorithm;data mining;geforce 7 series;simulation	Hao-Sen Chen;Guanling Lee;Sheng-Lung Peng	2012	2012 9th International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2012.6234337	computer science;bioinformatics;machine learning;data mining;mathematics;chromosome;set theory	ML	3.763570319832208	-47.780885687680154	87219
ac2143e34a10d5cc82744c9d30555270ae97f270	an algorithm of apriori based on medical big data and cloud computing	medical data;data mining;apriori algorithm;hadoop;cloud computing	With the enormous development in the field of medical industry, the value of medical data is highlighted increasingly. The concept of medical big data has become the study target of experts and scholars at the same time. This paper researches the association rules algorithm in existing medical data mining technology, and improves the Apriori algorithm by introducing of interest degree threshold. Based on the Hadoop platform and cloud computing technology, this paper proposes a new association rule algorithm of medical data mining, combined with the MapReduce, interest measure, confidence coefficient, and support degree. At the end of this paper, a simulation experiment is carried out based on Hadoop platform, which proves the superiority of the improved algorithm.	apache hadoop;apriori algorithm;association rule learning;big data;cloud computing;coefficient;data mining;mapreduce;simulation	Xiaoyan Cui;Shimeng Yang;Darning Wang	2016	2016 4th International Conference on Cloud Computing and Intelligence Systems (CCIS)	10.1109/CCIS.2016.7790284	cloud computing;computer science;data science;operating system;data mining;fsa-red algorithm;apriori algorithm;world wide web	DB	-3.224236620443264	-38.227955319620214	87291
89d339d8da952b6d860e6ce3017fe0ff4e60f4c3	an algorithm for mining weighted dense maximal 1-complete regions	high dimensionality;search algorithm;subspace clustering	We propose a new search algorithm for a special type of subspace clusters, called maximal 1-complete regions, from high dimensional binary valued datasets. Our algorithm is suitable for dense datasets, where the number of maximal 1-complete regions is much larger than the number of objects in the datasets. Unlike other algorithms that find clusters only in relatively dense subspaces, our algorithm finds clusters in all subspaces. We introduce the concept of weighted density in order to find interesting clusters in relatively sparse subspaces. Experimental results show that our algorithm is very efficient, and uses much less memory than other algorithms.	bmc bioinformatics;cluster analysis;clustering high-dimensional data;data mining;experiment;han unification;maximal set;sigkdd;search algorithm;sparse matrix;time complexity	Haiyun Bian;Raj Bhatnagar	2008		10.1007/978-3-540-78488-3_2	mathematical optimization;combinatorics;machine learning;mathematics	ML	-1.8040499869562685	-41.16244848618544	87422
47604fb4fd752d66ac1ecd345187ef37e50d7a38	multiple sequence alignment using simulated annealing	simulated annealing;multiple sequence alignment	Multiple sequence alignment is a useful technique for studying molecular evolution and analyzing structure-sequence relationships. Dynamic programming of multiple sequence alignment has been widely used to find an optimal alignment. However, dynamic programming does not allow for certain types of gap costs, and it limits the number of sequences that can be aligned due to its high computational complexity. The focus of this paper is to use simulated annealing as the basis for developing an efficient multiple sequence alignment algorithm. An algorithm called Multiple Sequence Alignment using Simulated Annealing (MSASA) has been developed. The computational complexity of MSASA is significantly reduced by replacing the high-temperature phase of the annealing process by a fast heuristic algorithm. This heuristic algorithm facilitates in minimizing the solution set of the low-temperature phase of the annealing process. Compared to the dynamic programming approach, MSASA can (i) use natural gap costs which can generate better solution, (ii) align more sequences and (iii) take less computation time.	algorithm;align (company);computation;computational complexity theory;dynamic programming;evolution, molecular;heuristic (computer science);multiple sequence alignment;simulated annealing;time complexity	Jin Wook Kim;Sakti Pramanik;Moon-Jung Chung	1994	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/10.4.419	biology;simulated annealing;multiple sequence alignment;computer science;bioinformatics	AI	-0.6888475450651217	-50.45925785521187	87463
7ea68245ea6641406aef5c6bcae6ab8646cc832a	from cluster-based outlier detection to time series discord discovery		Anomalous patterns or discords are just the kind of outliers in time series. In this paper, we present a new approach for time series discord discovery which is based on cluster-based outlier detection. In this approach, first, subsequence candidates are extracted from the time series using a segmentation method, then these candidates are transformed into the same length and are input for an appropriate clustering algorithm, and finally, we identify discords by using a measure suggested in the cluster-based outlier detection method given by He et al. 2003. The experimental results show that our approach is much more efficient than the HOTSAX algorithm in detecting time series discords while the anomalous patterns discovered by the two methods perfectly match with each other.	anomaly detection;time series	Nguyen Huy Kha;Duong Tuan Anh	2015		10.1007/978-3-319-25660-3_2	anomaly detection;artificial intelligence;mathematics;outlier;cluster analysis;pattern recognition	ML	-0.16989882347025462	-38.82908916260674	87474
b95baffd574dc3594e81e3adc66ac328a4e55fa8	using multivariate methods to infer knowledge from genomic data	microarray data;virulence factor;protein function;phylogenetic profiles;statistical genomics;multivariate statistical analysis	Since the introduction of genome sequencing techniques several methods for genomic data preprocessing and analysis have been published and applied to answer different biological questions. Rarely, multivariate methods have been used to extract knowledge about protein roles. Two of the most informative types of data are gene expression data (microarrays) and phylogenetic profiles indicating presence of genes in other organisms and therefore providing information about their co-evolution. Here we show that these two types of data, analyzed by means of principal component analysis and non parametric discriminant analysis, provide useful information about protein function and their participation in virulence processes.	data pre-processing;gene expression;inference;information;linear discriminant analysis;microarray;phylogenetics;preprocessor;principal component analysis;scientific publication;whole genome sequencing	Liliana López Kleine;Nicolás Molano;Luis Ospina	2013	International journal of bioinformatics research and applications	10.1504/IJBRA.2013.053607	biology;microarray analysis techniques;computer science;bioinformatics;data mining;genetics	Comp.	6.100468528737819	-50.30554816178235	87514
d05b236a11844d9d92b1970f71d8c1c03fbb23fc	gas turbine fault diagnosis using random forests	gas turbine;random forest;fault diagnosis	In the present paper, Random Forests are used in a critical and at the same time non trivial problem concerning the diagnosis of Gas Turbine blading faults, portraying promising results. Random forests-based fault diagnosis is treated as a Pattern Recognition problem, based on measurements and feature selection. Two different types of inserting randomness to the trees are studied, based on different theoretical assumptions. The classifier is compared against other Machine Learning algorithms such as Neural Networks, Classification and Regression Trees, Naive Bayes and K-Nearest Neighbor. The performance of the prediction model reaches a level of 97% in terms of precision and recall, improving the existing state-of-the-art levels achieved by Neural Networks by a factor of 1.5%-2%.	feature selection;k-nearest neighbors algorithm;machine learning;naive bayes classifier;neural networks;neural network software;pattern recognition;precision and recall;random forest;randomness	Manolis Maragoudakis;Euripidis Loukis;Panayotis-Prodromos Pantelides	2008		10.3233/978-1-58603-891-5-769	random forest;simulation;computer science;machine learning;data mining	AI	8.774191068396776	-40.37256117877592	87933
d007f71381d887ec23b1a291044cf60b9f06eb73	finding representatives in a large dataset of spectral reflectances	ivrg	We propose a new method to construct representative spectra from a large database of spectral reflectances. The key is the optimisation of a Support Vector type functional. The representatives are constructed such that they sit at positions of high density in the set of spectra. At the same time they are constructed to be as orthogonal as possible. The representatives are expressible as a linear combination of data samples with positive coefficients. Therefore, they are positive and physically realisable. We show the differences of these representatives to representatives found with well-known methods like principal component analysis and k–means clustering.	cluster analysis;coefficient;mathematical optimization;principal component analysis	Silvio Borer;Sabine Süsstrunk	2004			computer science;data mining;database;information retrieval	ML	0.5974900299697536	-42.63041259301595	88049
36f441b3ebabc3bbf636bdb399d2c95a8a2d38f8	hardware accelerator for the multifractal analysis of dna sequences		The multifractal analysis has allowed to quantify the genetic variability and non-linear stability along the human genome sequence. It has some implications in explaining several genetic diseases given by some chromosome abnormalities, among other genetic particularities. The multifractal analysis of a genome is carried out by dividing the complete DNA sequence in smaller fragments and calculating the generalized dimension spectrum of each fragment using the chaos game representation and the box-counting method. This is a time consuming process because it involves the processing of large data sets using floating-point representation. In order to reduce the computation time, we designed an application-specific processor, here called multifractal processor, which is based on our proposed hardware-oriented algorithm for calculating efficiently the generalized dimension spectrum of DNA sequences. The multifractal processor was implemented on a low-cost SoC-FPGA and was verified by processing a complete human genome. The execution time and numeric results of the Multifractal processor were compared with the results obtained from the software implementation executed in a 20-core workstation, achieving a speed up of 2.6x and an average error of 0.0003 percent.	algorithm;chaos game;chromosome aberrations;computation;congenital abnormality;execution;field-programmable gate array;float;hardware acceleration;hereditary diseases;meddra system organ class;multifractal system;nonlinear system;numbers;run time (program lifecycle phase);small;spatial variability;speedup;system on a chip;thinking outside the box;time complexity;workstation	Jorge E. Duarte-S&#x00E1;nchez;Jaime Velasco-Medina;Pedro A. Moreno	2018	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2017.2731339	machine learning;artificial intelligence;algorithm design;software;multifractal system;computation;speedup;computer science;algorithm;hardware acceleration;chaos game;fractal	Comp.	2.582940591260419	-46.827885637217506	88107
37ba2ddfc21b5bb2a42d25c42e044c7cd0548646	studying the conditions for learning dynamic bayesian networks to discover genetic regulatory networks	bayesian network;donnee experimentale;pulga de dna;algoritmo busqueda;structure search;genetic network modeling;puce a dna;algorithme recherche;search algorithm;dato experimental;genetic regulatory network;search strategy;algoritmo genetico;reseau regulateur genetique;reseau bayes;red bayes;dynamic bayesian network;genetic network;functional genomics;genome;strategie recherche;dna chip;bayes network;algorithme genetique;genetic algorithm;genoma;estrategia investigacion;bayesian networks	Learning regulatory interactions between genes from microarray measurements presents one of the major challenges in functional genomics. This article studies the suitability of learning dynamic Bayesian networks under realistic experimental settings. Through extensive artificial-data experiments, it is investigated how the performance of discovering the true interactions depends on varying data conditions. These experiments show that the performance most strongly deteriorates when the connectivity of the original network increases, and more than a proportional increase in the number of samples is needed to compensate for this. Furthermore, it was found that a lower performance is achieved when the original network size becomes larger, but this decrease can be greatly reduced with increased computational effort. Finally, it is shown that the performance of the search algorithm benefits more from a larger number of restarts rather than from the use of more sophisticated search strategies.	computation;dynamic bayesian network;experiment;functional genomics;gene regulatory network;interaction;microarray;search algorithm;tree traversal	Rogier J. P. van Berlo;Eugene P. van Someren;Marcel J. T. Reinders	2003	Simulation	10.1177/0037549703040942	computer science;bioinformatics;artificial intelligence;machine learning;bayesian network	ML	0.15898515909570418	-46.85950791637109	88179
5dbd30b93a6db55b184bb1a687ebc2bc0efcaac5	a local network neighbourhood artificial immune system for data clustering	pattern clustering;artificial immune system;natural immune system local network neighbourhood artificial immune system data clustering network topology artificial lymphocyte neighbourhood;proof of concept;data clustering;artificial immune systems immune system automatic logic units network topology training data protection computational intelligence computer science africa euclidean distance;network topology;dynamic environment;immune system;network theory;pattern clustering artificial immune systems network topology;clustered data;artificial immune systems	The artificial immune system (AIS) is inspired by the functioning of the natural immune system. There are different theories with regards to the organisational behaviour of the natural immune system. One of these theories is the network theory. In this paper a novel network based AIS model is proposed. The proposed Local Network Neighbourhood Artificial Immune System (LNNAIS) is inspired by the network topology of lymphocytes to learn the antigen structure from one another. LNNAIS has a different interpretation of the network theory compared to existing network based AIS models. LNNAIS uses a concept of an artificial lymphocyte (ALC) neighbourhood to determine the network links between the ALCs. The purpose of this paper is to provide a proof of concept that an artificial lymphocyte (ALC) neighbourhood can cluster data in a dynamic environment. LNNAIS only requires one pass through the training data of antigen patterns for clustering.	artificial immune system;cluster analysis;ieee congress on evolutionary computation;network theory;network topology;organizational behavior;sigmoid function;statistical model	A. J. Graaff;Andries Petrus Engelbrecht	2007	2007 IEEE Congress on Evolutionary Computation	10.1109/CEC.2007.4424480	computer science;artificial intelligence;machine learning;data mining;artificial immune system	AI	3.1811760560417772	-42.653845273112424	88224
b6d19ea73f84de291d926ed3825b66c5eeb6b449	knowledge discovery in gene expression data via evolutionary algorithms	biology computing;classification algorithm;support vector machines;cancer;micro array data;prediction algorithms;pattern classification biology computing data mining genetic algorithms;gene expression data;data mining;feature space;feature selection algorithms knowledge discovery gene expression data evolutionary algorithms microarray data classification predictor classifier initial gene filtering feature spaces genetic algorithm;genetics;accuracy;k nearest neighbor micro array data feature selection genetic algorithms support vector machines;classification algorithms;pattern classification;genetic algorithm;k nearest neighbor;genetic algorithms;feature selection;support vector machine;evolutionary algorithm;classification accuracy;data classification;genetic algorithms accuracy support vector machines prediction algorithms classification algorithms cancer genetics	Methods currently used for micro-array data classification aim to select a minimum subset of features, namely a predictor, that is necessary to construct a classifier of best accuracy. Although effective, they lack in facing the primary goal of domain experts that are interested in detecting different groups of biologically relevant markers. In this paper, we present and test a framework which aims to provide different subsets of relevant genes. It considers initial gene filtering to define a set of feature spaces each of ones is further refined by taking advantage from a genetic algorithm. Experiments show that the overall process results in a certain number of predictors with high classification accuracy. Compared to state-of-art feature selection algorithms, the proposed framework consistently generates better feature subsets and keeps improving the quality of selected subsets in terms of accuracy and size.	evolutionary algorithm;experiment;feature selection;genetic algorithm;kerrison predictor;sensor;subject-matter expert	Laura Maria Cannas;Nicoletta Dessì;Barbara Pes	2011	2011 22nd International Workshop on Database and Expert Systems Applications	10.1109/DEXA.2011.48	support vector machine;genetic algorithm;computer science;machine learning;evolutionary algorithm;pattern recognition;data mining;feature selection	Vision	8.21873653840802	-47.56512933823337	88482
c8c2fa85cda115865b210fea76cc58afec0bb364	the deep coalescence consensus tree problem is pareto on clusters	optimal solution;efficient algorithm;population size;satisfiability;gene trees;simulation experiment;incomplete lineage sorting;biological process	Phylogenetic methods must account for the biological processes that create incongruence between gene trees and the species phylogeny. Deep coalescence, or incomplete lineage sorting creates discord among gene trees at the early stages of species divergence or in cases when the time between speciation events was short and the ancestral population sizes were large. The deep coalescence problem takes a collection of gene trees and seeks the species tree that implies the fewest deep coalescence events, or the smallest deep coalescence reconciliation cost. Although this approach can to be useful for phylogenetics, the consensus properties of this problem are largely uncharacterized, and the accuracy of heuristics is untested. We prove that the deep coalescence consensus tree problem satisfies the highly desirable Pareto property for clusters (clades). That is, in all instances, each cluster that is present in all of the input gene trees, called a consensus cluster, will also be found in every optimal solution. We introduce an efficient algorithm that, given a candidate species tree that does not display the consensus clusters, will modify the candidate tree so that it includes all of the clusters and has a lower (more optimal) deep coalescence cost. Simulation experiments demonstrate the efficacy of this algorithm, but they also indicate that even with large trees, most solutions returned by the recent efficient heuristic display the consensus clusters.	coalescing (computer science);pareto efficiency	Harris T. Lin;John Gordon Burleigh;Oliver Eulenstein	2011		10.1007/978-3-642-21260-4_19	biology;mathematical optimization;population size;bioinformatics;coalescent theory;biological process;genetics;satisfiability	NLP	1.2601802403565256	-51.96777341864637	88595
1642f0c0b2834f4d043710f7a6f1259940ddf7d8	mutiple classifier system based android malware detection	smart phones android operating system computer crime feature selection invasive software pattern classification;android malware detection;hackers;smart phones;computer crime;fpr;fpr mutiple classifier system android malware detection smartphone hackers android system android malicious application problem multiple classifier system feature selection puma tpr;android operating system;base classifier android malicious application multiple classifier system feature selection;android system;accuracy;mutiple classifier system;accuracy abstracts feature extraction;abstracts;feature extraction;tpr;pattern classification;base classifier;android malicious application problem;invasive software;feature selection;android malicious application;smartphone;multiple classifier system;puma	Smartphone becomes more popular recently. Much important information is stored and processed in the Smartphones. It attracts the attention of hackers. Malware is one of the most common security issues in Smartphone, especially for the Android system due to its compatibility. In this paper, we focus on the Android malicious application problem. As malwares with different purposes have different properties, only using a single classifier to discriminate the benign and malicious application may not be good enough. A detection method using Multiple Classifier System is proposed. Each base classifier is responsible for one type of malware. Android applications are classified as malwares when any one of base classifier decides they are malwares. The feature selection has been applied for each base classifier separately to increase the performance. The experimental results show that our proposed method outperforms than existing method PUMA, which classifies all types of malwares by a single classifier, in term of accuracy, TPR and FPR.	android;feature selection;film-type patterned retarder;learning classifier system;malware;principle of good enough;smartphone	Wen Liu	2013	2013 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2013.6890444	hacker;feature extraction;computer science;machine learning;accuracy and precision;internet privacy;feature selection;world wide web;computer security	SE	6.654441424571961	-38.05525564748155	88852
e18348bdcf0c920296492d76238efb652fc356a2	on the effectiveness of fuzzy clustering as a data discretization technique for large-scale classification of solar images	cluster algorithm;image recognition;pattern clustering;discrete data;texture feature extraction;texture feature extraction fuzzy clustering data discretization technique large scale classification solar image recognition;discretization;large scale classification;data discretization technique;texture features;data mining;classification;fuzzy set theory;image texture;accuracy;indexes;fuzzy clustering;large scale;large scale systems sun pixel image recognition data mining feature extraction clustering algorithms observatories information retrieval image retrieval;image recognition discretization fuzzy clustering classification;astronomy computing;feature extraction;solar image recognition;classification algorithms;sun;clustering algorithms;pattern clustering astronomy computing feature extraction fuzzy set theory image recognition image texture	This paper presents experimental results on the utilization of fuzzy clustering as a discretization technique for purpose of solar images recognition. By extracting texture features from our solar images, and consequently applying fuzzy clustering techniques on these features, we were able to determine what clustering algorithm and what algorithm's initialization parameters produced the best data discretization. Based on these results we discretized some of our texture features and ran them on two different classifiers comparing how well the classifiers performed on our original data versus the discretized data. Our experimental results demonstrate that discretization of our data via fuzzy clustering carries significant potential since on our classifiers produced similar results on the original and the discretized data, and the reduction of storage space achieved through cluster-based discretization has been very significant.	algorithm;cluster analysis;discretization;experiment;fuzzy clustering;product binning;service data objects	Juan M. Banda;Rafal A. Angryk	2009	2009 IEEE International Conference on Fuzzy Systems	10.1109/FUZZY.2009.5277273	statistical classification;image texture;database index;data stream clustering;fuzzy clustering;feature extraction;biological classification;computer science;machine learning;pattern recognition;discretization;data mining;mathematics;accuracy and precision;fuzzy set;cluster analysis;discretization of continuous features	Robotics	1.0546310794660454	-39.11547728521723	88912
b5975a2e420211105e2a628752e5c4a25f6b6d24	quantwiz: a scalable parallel software package for label-free protein quantification	distributed memory;biology computing;distributed memory systems;parallel algorithm;label free;high performance liquid chromatography;abundance ratio comparison;scalable parallel software package;mass spectrometry;lc ms based label free protein quantification;quantification;scaling up;scalability quantification mass spectrometry label free abundance ratio comparison parallel;life science;liquid chromatography;proteins;program testing;programming scalability supercomputers;quantwiz;scalability testing quantwiz scalable parallel software package proteomics life science mass spectrometry liquid chromatography ms method lc ms based label free protein quantification distributed memory parallel algorithm;life sciences;parallel;software package;scalability testing;scalability;mass spectroscopy;proteomics;programming;chromatography;supercomputers;software packages;ms method;parallel algorithms;software packages biology computing chromatography distributed memory systems mass spectroscopy parallel algorithms program testing proteins proteomics	In the context of the prosperous development of Proteomics in life science, protein quantification, especially these based on Mass Spectrometry (short for MS) method, becomes an essential part of research. In our previous work, we developed a new software package called QuantWiz for high performance Liquid Chromatography (short for LC)-MS-based label-free protein quantification. We solved those problems of portability, applicability and longtime running existed in other software for protein quantification based on MS method. In this paper, we first compared the LC-MS-based label-free protein quantification accuracy of QuantWiz with the well-known Census software package. Then we designed and implemented a distributed memory version parallel algorithm for QuantWiz. Finally, we performed scalability testing of our new parallel algorithm and showed that our new parallel algorithm can scale up to 512 processes on Dawning 5000A.	dawning information industry;distributed memory;parallel algorithm;proteomics;scalability testing;whole earth 'lectronic link	Jing Wang;Yunquan Zhang;Xianyi Zhang;Xiangzheng Sun;Quanhu Sheng	2010	2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)	10.1109/BICTA.2010.5645126	computer science;bioinformatics;analytical chemistry;theoretical computer science	HPC	-2.7711964676214063	-50.11147782775018	89059
1b2240d7a8a2ae73d8cd132e56d5781f31a5d829	gene subset selection for cancer classification using statsitical and rough set approach	microarray cancer data;rough set theory;cancer classification;gene selection;discernibility matrix	Microarray technique is very useful for measuring expression levels of thousands or more of genes simultaneously. One of challenges in classification of cancer using high-dimensional gene expression data is to select minimal number of relevant genes which can maximize classification accuracy. Because of the distinct characteristics inherent to specific cancerous gene expression profiles, selecting the most informative cancer-related genes from high volume microarray gene expression data is an important and challenging bioinformatics research topic. In the paper, first some important genes are identified based on their rank computed statistically and then rough set theory is applied on reduced gene set for selecting genes with high class-discrimination capability. The method constructs relative discernibility matrix to find out the core genes which are essentially required to distinguish the normal and tumor samples and iteratively adds high ranked noncore genes one at a time to core genes for maximizing classification accuracy. The method is applied on some well known cancerous datasets to show the goodness of the method.	rough set	Asit Kumar Das;Soumen Kumar Pati	2012		10.1007/978-3-642-35380-2_35	gene-centered view of evolution;rough set;computer science;bioinformatics;machine learning;pattern recognition;data mining;mathematics	Vision	7.9283721646293435	-48.48943333134288	89167
2196c16b383c8591168ee647ae6500a45d01c8bf	microbiome data representation by joint nonnegative matrix factorization with laplacian regularization	ieee transactions;data representation human microbiome nonnegative matrix factorization multi view clustering data integration;phylogeny;data representation;multi view clustering;laplace equations;laplace equations linear programming phylogeny communities computational biology bioinformatics ieee transactions;human microbiome;nonnegative matrix factorization;linear programming;communities;computational biology;data integration;bioinformatics	Microbiome datasets are often comprised of different representations or views which provide complementary information to understand microbial communities, such as metabolic pathways, taxonomic assignments, and gene families. Data integration methods including approaches based on nonnegative matrix factorization NMF combine multi-view data to create a comprehensive view of a given microbiome study by integrating multi-view information. In this paper, we proposed a novel variant of NMF which called Laplacian regularized joint non-negative matrix factorization LJ-NMF for integrating functional and phylogenetic profiles from HMP. We compare the performance of this method to other variants of NMF. The experimental results indicate that the proposed method offers an efficient framework for microbiome data analysis.	community;gene family;host media processing;lightweight java;matrix regularization;microbiome;non-negative matrix factorization;numerous;phylogenetics	Xingpeng Jiang;Xiaohua Hu;Weiwei Xu	2017	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2015.2440261	computational biology;human microbiome;computer science;bioinformatics;linear programming;theoretical computer science;data integration;machine learning;mathematics;external data representation;non-negative matrix factorization	ML	7.211506144887075	-51.14179197253455	89198
f11456aa84e692b262d82817ee379924d5b1b4ca	developments on a multi-objective metaheuristic (momh) algorithm for finding interesting sets of classification rules	cluster algorithm;rule induction;classification rules	In this paper, we experiment with a combination of innovative approaches to rule induction to encourage the production of interesting sets of classification rules. These include multi-objective metaheuristics to induce the rules; measures of rule dissimilarity to encourage the production of dissimilar rules; and rule clustering algorithms to evaluate the results obtained. Our previous implementation of NSGA-II for rule induction produces a set of cc-optimal rules (coverage-confidence optimal rules). Among the set of rules produced there may be rules that are very similar. We explore the concept of rule similarity and experiment with a number of modifications of the crowding distance to increasing the diversity of the partial classification rules produced by the multi-objective algorithm.	algorithm;cluster analysis;crowding;experiment;metaheuristic;multi-objective optimization;pareto efficiency;rule induction	Beatriz de la Iglesia;Alan P. Reynolds;Victor J. Rayward-Smith	2005		10.1007/978-3-540-31880-4_57	machine learning;pattern recognition;data mining;mathematics	ML	4.63085261178605	-42.070297489762076	89282
dde613bddfab292b9fee72bfb2011de18a625bb6	fuzzy co-clustering with automated variable weighting	unsupervised learning;minimization;convergence;feature ponderation;prototypes;co clustering;linear programming;mathematical model;clustering algorithms;partitioning algorithms;clustering algorithms partitioning algorithms linear programming mathematical model convergence minimization prototypes	We propose two fuzzy co-clustering algorithms based on the double Kmeans algorithm. Fuzzy approaches are known to require more computation time than hard ones but the fuzziness principle allows a description of uncertainties that often appears in real world applications. The first algorithm proposed, fuzzy double Kmeans (FDK) is a fuzzy version of double Kmeans (DK). The second algorithm, weighted fuzzy double Kmeans (W-FDK), is an extension of FDK with automated variable weighting allowing co-clustering and feature selection simultaneously. We illustrate our contribution using Monte Carlo simulations on datasets with different parameters and real datasets commonly used in the co-clustering context.	algorithm;biclustering;cluster analysis;computation;feature selection;iteration;k-means clustering;monte carlo method;simulation;time complexity	Charlotte Laclau;Francisco de A. T. de Carvalho;Mohamed Nadif	2015	2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2015.7337802	unsupervised learning;mathematical optimization;convergence;computer science;linear programming;machine learning;pattern recognition;mathematical model;mathematics;prototype;cluster analysis	Robotics	3.063712320980762	-39.858922603864954	89810
1017d83252746aa732f937b787c2f316456ce523	evolution leads to a diversity of motion-detection neuronal circuits		A central goal of evolutionary biology is to explain the origins and distribution of diversity across life. Beyond species or genetic diversity, we also observe diversity in the circuits (genetic or otherwise) underlying complex functional traits. However, while the theory behind the origins and maintenance of genetic and species diversity has been studied for decades, theory concerning the origin of diverse functional circuits is still in its infancy. It is not known how many different circuit structures can implement any given function, which evolutionary factors lead to different circuits, and whether the evolution of a particular circuit was due to adaptive or non-adaptive processes. Here, we use digital experimental evolution to study the diversity of neural circuits that encode motion detection in digital (artificial) brains. We find that evolution leads to an enormous diversity of potential neural architectures encoding motion detection circuits, even for circuits encoding the exact same function. Evolved circuits vary in both redundancy and complexity (as previously found in genetic circuits) suggesting that similar evolutionary principles underlie circuit formation using any substrate. We also show that a simple (designed) motion detection circuit that is optimally-adapted gains in complexity when evolved further, and that selection for mutational robustness led this gain in complexity.	encode;evolution;genetic algorithm	Ali Tehrani-Saleh;Thomas LaBar;Christoph Adami	2018	CoRR		encode;genetics;genetic diversity;motion detection;encoding (memory);biology;electronic circuit;biological neural network;species diversity;experimental evolution;evolutionary biology	ML	-4.361809534606461	-47.44138369098319	89836
684190272bf525edb2628da750c1096f8f1ff95b	handling numeric attributes with ant colony based classifier for medical decision making	ant miner;ant colony optimization;rule learning;classification;numeric attributes;medical data mining	In data mining many datasets are described with both discrete and numeric attributes. Most Ant Colony Optimization based classifiers can only deal with discrete attributes and need a pre-processing discretization step in case of numeric attributes. We propose an adaptation of AntMiner+ for rule mining which intrinsically handles numeric attributes. We describe the new approach and compare it to the existing algorithms. The proposed method achieves comparable results with existing methods on UCI datasets, but has advantages on datasets with strong interactions between numeric attributes. We analyse the effect of parameters on the classification accuracy and propose sensible defaults. We describe application of the new method on a real world medical domain which achieves comparable results with the existing method.	algorithm;ant colony;apache ant (another neat tool);dspace;discretization;exclusive or;fuzzy classification;graph (abstract data type);machine learning;medical decision making;nant;run time (program lifecycle phase);sampling (signal processing);syncope (medicine)	Matej Piculin;Marko Robnik-Sikonja	2014	Expert Syst. Appl.	10.1016/j.eswa.2014.06.017	ant colony optimization algorithms;biological classification;computer science;data science;machine learning;data mining	ML	8.832734529796557	-42.205130785492386	90421
2ce82c5b43ea99a713399f9eb5ec5291f8833ff8	lrsslmda: laplacian regularized sparse subspace learning for mirna-disease association prediction		Predicting novel microRNA (miRNA)-disease associations is clinically significant due to miRNAs' potential roles of diagnostic biomarkers and therapeutic targets for various human diseases. Previous studies have demonstrated the viability of utilizing different types of biological data to computationally infer new disease-related miRNAs. Yet researchers face the challenge of how to effectively integrate diverse datasets and make reliable predictions. In this study, we presented a computational model named Laplacian Regularized Sparse Subspace Learning for MiRNA-Disease Association prediction (LRSSLMDA), which projected miRNAs/diseases' statistical feature profile and graph theoretical feature profile to a common subspace. It used Laplacian regularization to preserve the local structures of the training data and a L1-norm constraint to select important miRNA/disease features for prediction. The strength of dimensionality reduction enabled the model to be easily extended to much higher dimensional datasets than those exploited in this study. Experimental results showed that LRSSLMDA outperformed ten previous models: the AUC of 0.9178 in global leave-one-out cross validation (LOOCV) and the AUC of 0.8418 in local LOOCV indicated the model's superior prediction accuracy; and the average AUC of 0.9181+/-0.0004 in 5-fold cross validation justified its accuracy and stability. In addition, three types of case studies further demonstrated its predictive power. Potential miRNAs related to Colon Neoplasms, Lymphoma, Kidney Neoplasms, Esophageal Neoplasms and Breast Neoplasms were predicted by LRSSLMDA. Respectively, 98%, 88%, 96%, 98% and 98% out of the top 50 predictions were validated by experimental evidences. Therefore, we conclude that LRSSLMDA would be a valuable computational tool for miRNA-disease association prediction.	area under curve;biological markers;colon classification;colonic neoplasms;computational model;cross reactions;cross-validation (statistics);dimensionality reduction;esophageal neoplasms;graph - visual representation;inference;kidney neoplasm;laplacian matrix;lymphoma;mammary neoplasms;mental association;micrornas;name;projections and predictions;sparse;taxicab geometry;triangulation	Xing Chen;Li Huang	2017		10.1371/journal.pcbi.1005912	biology;cross-validation;bioinformatics;disease association;biological data;dimensionality reduction;regularization (mathematics);subspace topology;pattern recognition;colon neoplasm;artificial intelligence;laplace operator	AI	8.466580345699205	-52.05967939481588	90455
923683d94b1e24a62a9cbf4f6b0ec194e182f82e	pattern recognition using artificial immune system		In this thesis, the uses of Artificial Immune Systems (AIS) in Machine learning is studded. the thesis focus on some of immune inspired algorithms such as clonal selection algorithm and artificial immune network. The effect of changing the algorithm parameter on its performance is studded. Then a new immune inspired algorithm for unsupervised classification is proposed. The new algorithm is based on clonal selection principle and named Unsupervised Clonal Selection Classification (UCSC). The new proposed algorithm is almost parameter free. The algorithm parameters are data driven and it adjusts itself to make the classification as fast as possible. The performance of UCSC is evaluated. The experiments show that the proposed UCSC algorithm has a good performance and more reliable.	artificial immune system;pattern recognition	Mohammad Tarek Al-Muallim	2017	CoRR		immune system;artificial immune system;clonal selection algorithm;clonal selection;artificial intelligence;pattern recognition;computer science	Robotics	9.331471904771309	-40.74854248255722	90965
e65d07661be37640151a40e8a69955a0e3b5e058	on effectiveness of pre-processing by clustering in prediction of c.e. technological data with anns		Civil Engineering technological data are naturally clustered in a specific way. A black-box model of relation between concrete composition and concrete properties can be constructed using a suitable artificial neural network like Fuzzy ARTMAP that was implemented for the presented experiments. After training the system allows valuable prediction of technological data. It was expected that pretreatment of data by their clustering should enable improved prediction on testing examples. The clustering was realized in two different ways: with k — means algorithm and with GCA approach. The improvement of the precision of predictions was found rather limited, but the final efficiency was better, as more records have been positively recognized. The approach seems to be even more promising in case of data of particular internal structure and application of advanced procedures of clustering.		Janusz Kasperkiewicz;Dariusz Alterman	2003			machine learning;pattern recognition;data mining	ML	9.907228587725932	-38.8391688207893	90968
0a09abb913ee38a1a9ef480a55c9421489419980	an implementation of a distributed stochastic gradient descent for recommender systems based on map-reduce	collaborative filter ing;matrix factorization;recommender system;gradient descent;matrix decomposition;lead;movielens datasets distributed stochastic gradient descent recommender systems mapreduce load balancing dsgd matrix factorization technique hadoop cluster;matrix decomposition sparse matrices recommender systems optimization scalability clustering algorithms lead;parallelization recommender system collaborative filtering matrix factorization gradient descent;parallelization;clustering algorithms;optimization;scalability;stochastic processes data handling gradient methods matrix decomposition parallel processing pattern clustering recommender systems;recommender systems;sparse matrices	This work presents an implementation of a Distributed Stochastic Gradient Descent (DSGD) for Recommender Systems based on Hadoop/MapReduce. Recommender Systems aim at presenting first the information in which users may be more interested. To do this, they analyse a great volume of data that represent the users preferences (e.g. ratings). Thus, this stirs up the need of load-balancing. DSGD is a Matrix Factorization technique that has demonstrated high accuracy and scalability. In this work we expose this algorithm and modify it to improve its accuracy and adaptability to a hadoop cluster. The experimentation phase uses Movie-Lens datasets. Comparisons with other algorithms are given. Results show the good performance of the implementation.	algorithm;apache hadoop;load balancing (computing);mapreduce;recommender system;scalability;stochastic gradient descent	Manuel Pozo;Raja Chiky	2015	2015 International Workshop on Computational Intelligence for Multimedia Understanding (IWCIM)	10.1109/IWCIM.2015.7347074	computer science;theoretical computer science;machine learning;distributed computing	HPC	-3.1419060876057445	-39.89366739019302	91065
81f405ee94747d0eaf338a33ea820cb1aa50f262	fuzzy k -nearest neighbor method for protein secondary structure prediction and its parallel implementation	parallel algorithm;nearest neighbor method;feature vector;protein secondary structure prediction;pattern classification;k nearest neighbor;sequence alignment;parallel implementation	Fuzzy k-nearest neighbor method is a generalization of nearest neighbor method, the simplest algorithm for pattern classification. One of the important areas for application of the pattern classification is the protein secondary structure prediction, an important topic in the field of bioinformatics. In this work, we develop a parallel algorithm for protein secondary structure prediction, based on the fuzzy k-nearest neighbor method, that uses evolutionary profile obtained from PSI-BLAST (Position Specific Iterative Basic Local Sequence Alignment Tool) as the feature vectors.		Seung-Yeon Kim;Jaehyun Sim;Julian Lee	2006		10.1007/11816102_48	nearest-neighbor chain algorithm;large margin nearest neighbor;nearest neighbour algorithm;ball tree;nearest neighbor graph;best bin first;feature vector;computer science;machine learning;pattern recognition;sequence alignment;data mining;parallel algorithm;cover tree;nearest neighbor search;fixed-radius near neighbors;k-nearest neighbors algorithm	Comp.	4.006161237961581	-42.952744024491714	91476
0b16a779a0dfbed7430e43c4fff69c91a81a9dcf	low-rank matrix factorization and co-clustering algorithms for analyzing large data sets	matrix factorization;isoperimetric co clustering;co clustering	With the ever increasing data, there is a greater need for analyzing and extracting useful and meaningful information out of it. The amount of research being conducted in extracting this information is commendable. From clustering to bi and multi clustering, there are a lot of different algorithms proposed to analyze and discover the hidden patterns in data, in every which way possible. On the other hand, the size of the data sets is increasing with each passing day and hence it is becoming increasingly difficult to try and analyze all this data and find clusters in them without the algorithms being computationally prohibitive. In this study, we have tried to study both the domains and understand the development of the algorithms and how they are being used. We have compared the different algorithms to try and get a better idea of which algorithm is more suited for a particular situation.	algorithm;approximation algorithm;biclustering;cluster analysis;data mining;graph partition;isoperimetric inequality;monte carlo method;sigkdd;sampling (signal processing);sparse matrix;x image extension	Archana Donavalli;Manjeet Rege;Xumin Liu;Kourosh Jafari-Khouzani	2010		10.1007/978-3-642-27872-3_41	correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;computer science;theoretical computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;cluster analysis;matrix decomposition;biclustering	ML	-2.133016954055984	-39.881129345460934	91484
0395e70c0907aacc2e1dc1a39181c6123a4d7a9f	contemporary qsar classifiers compared		We present a comparative assessment of several state-of-the-art machine learning tools for mining drug data, including support vector machines (SVMs) and the ensemble decision tree methods boosting, bagging, and random forest, using eight data sets and two sets of descriptors. We demonstrate, by rigorous multiple comparison statistical tests, that these techniques can provide consistent improvements in predictive performance over single decision trees. However, within these methods, there is no clearly best-performing algorithm. This motivates a more in-depth investigation into the properties of random forests. We identify a set of parameters for the random forest that provide optimal performance across all the studied data sets. Additionally, the tree ensemble structure of the forest may provide an interpretable model, a considerable advantage over SVMs. We test this possibility and compare it with standard decision tree models.	algorithm;decision trees;decision tree;forests;machine learning;quantitative structure-activity relationship;quantitative structure–activity relationship;random forest;statistical test;support vector machine;the forest;trees (plant)	Craig L. Bruce;James L. Melville;Stephen D. Pickett;Jonathan D. Hirst	2007	Journal of chemical information and modeling	10.1021/ci600332j	random forest;decision tree learning;machine learning;pattern recognition;incremental decision tree;data mining;mathematics;ensemble learning	ML	9.062879875580855	-44.9716805446135	91658
af192129d456531416740691fa5f11475e013e54	evolutionary method for the assembly of rigid protein fragments	energy functions;protein structure;evolutionary strategy;genetic algorithm;optimization	Genetic algorithms constitute a powerful optimization method that has already been used in the study of the protein folding problem. However, they often suffer from a lack of convergence in a reasonably short time for complex fitness functions. Here, we propose an evolutionary strategy that can reproducibly find structures close to the minimum of a potential function for a simplified protein model in an efficient way. The model reduces the number of degrees of freedom of the system by treating the protein structure as composed of rigid fragments. The search incorporates a double encoding procedure and a merging operation from subpopulations that evolve independently of one another, both contributing to the good performance of the full algorithm. We have tested it with protein structures of different degrees of complexity, and present our conclusions related to its possible application as an efficient tool for the analysis of folding potentials.	complexity;contribution;convergence (action);double encoding;fitness function;genetic algorithm;mathematical optimization;muscle rigidity;protein structure prediction	David de Sancho;Lidia Prieto;Ana M. Rubio;Antonio Rey	2005	Journal of computational chemistry	10.1002/jcc.20150	protein structure;mathematical optimization;genetic algorithm;evolution strategy	Comp.	-1.2338802415561987	-49.98846681494141	91986
d7b2be714eacd522131d54d4aebbcfa85e1df481	high performance phylogenetic inference	statistical approach;biology computing;parallel programming biology computing genetics tree searching statistical analysis distributed object management;gene mutation;search method;large data sets;parallel programming;search strategy;phylogeny systematics data analysis biological information theory genetic mutations humans biogeography dna sequences iterative algorithms;genetics;statistical analysis;nona high performance phylogenetic inference phylogenetic analysis biological research programs gene genealogy gene mutation generational relationships human epidemiology viral transmission biogeography dna sequences very large data sets phylogenetic systematics search strategies topologies parsimony ratchet iterative tree search methods statistical approach tree island sampling parsimonious trees parallel ratchet masterworker based master process dogma system worker tasks wrapper code;distributed object management;tree searching;dna sequence;high performance;phylogenetic inference;phylogenetic analysis	Phylogenetic analysis is an integral part of many biological research programs. In essence, it is the study of gene genealogy. It is the study of gene mutation and the generational relationships. Phylogenetic analysis is being used in many diverse areas such as human epidemiology [2], viral transmission [ 13. biogeography [ 31, md systematics [4]. Researchers are now commonly generating many DNA sequences from many individuals, thus creating very large data sets. However, our ability to analyze the data has not kept pace with data generation, and phylogenetics has now reached a crossroads where we cannot effectively analyze the data we generate. The chief challenge of phylogenetic systematics in the next century will be to develop algorithms and search strategies to effectively analyze large data sets. The crux of the computational problem is that the actual landscape of possible topologies can be extraordinarily difficult to evaluate with large data sets.	algorithm;cladistics;computational problem;floor and ceiling functions;phylogenetic tree;phylogenetics	Mark J. Clement;Quinn Snell;Glenn Judd;Michael Whiting	1999		10.1109/HPDC.1999.805315	dna sequencing;bioinformatics;theoretical computer science	ML	-3.8338556733300937	-49.75138762173848	92090
00fa505f1b2d8812c0e4748df59b5868231d85f9	factors affecting boosting ensemble performance on dna microarray data	dna;boosting iterations;biology computing;model specification;boosting ensemble classifiers;high dimensionality;ensemble accuracy boosting ensemble performance dna microarray data boosting techniques high dimensionality classification performance boosting algorithms boosting ensemble classifiers boosting iterations diversity measures model type model accuracy;classification performance;boosting algorithms;colon;diversity measures;model accuracy;model type;boosting ensemble performance;computational fluid dynamics;iterative methods;data analysis;computational fluid dynamics colon;ensemble accuracy;pattern classification biology computing data analysis dna iterative methods;boosting techniques;pattern classification;dna microarray data	Boosting techniques have been applied to DNA microarray data because their high dimensionality has made them difficult to analyze. However, classification performance varies between boosting algorithms. We have investigated factors affecting error in boosting ensemble classifiers on DNA Microarray data: number of training samples, number of boosting iterations, complexity of base learners and diversity of models. Specifically we have applied diversity measures to investigate the relationships between model type, model accuracy, diversity and ensemble accuracy.	algorithm;boosting (machine learning);dna microarray;iteration	Geoffrey R. Guile;Wenjia Wang	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596600	computational fluid dynamics;computer science;machine learning;pattern recognition;data mining;iterative method;data analysis;specification;dna;statistics	ML	8.6112244807296	-48.65630377766807	92120
df9bcb9fbe495cb0af395ee161fa5a539d46cd4b	an incremental mixed data clustering method using a new distance measure	distance measure;clustering evaluation measures;mixed data clustering;incremental learning;som	Clustering is one of the most applied unsupervised machine learning tasks. Although there exist several clustering algorithms for numeric data, more sophisticated clustering algorithms to address mixed data (numeric and categorical data) more efficiently are still required. Other important issues to be considered in clustering are incremental learning and generating a sufficient number of clusters without specifying the number of clusters a priori. In this paper, we introduce a mixed data clustering method which is incremental and generates a sufficient number of clusters automatically. The proposed method is based on the Adjusted SelfOrganizing Incremental Neural Network (ASOINN) algorithm exploiting a new distance measure and new update rules for handling mixed data. The proposed clustering method is compared with the ASOINN and three other clustering algorithms comprehensively. The results of comparative experiments on various data sets using several clustering evaluation measures show the effectiveness of the proposed mixed data clustering method.	algorithm;artificial neural network;categorical variable;cluster analysis;existential quantification;experiment;intrusion detection system;level of measurement;machine learning;online and offline;supervised learning;unsupervised learning	Fakhroddin Noorbehbahani;Sayyed Rasoul Mousavi;Abdolreza Mirzaei	2015	Soft Comput.	10.1007/s00500-014-1296-7	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;affinity propagation;clustering high-dimensional data;conceptual clustering	ML	2.3901702785209444	-41.51202156548379	92183
841f0a551929a86f96de6478c64929420ea1227b	fuzzy v-measure - an evaluation method for cluster analyses of ambiguous data		This paper discusses an extension of the V-measure (Rosenberg and Hirschberg, 2007), an entropy-based cluster evaluation metric. While the original work focused on evaluating hard clusterings, we introduce the Fuzzy V-measure which can be used on data that is inherently ambiguous. We perform multiple analyses varying the sizes and ambiguity rates and show that while entropy-based measures in general tend to suffer when ambiguity increases, a measure with desirable properties can be derived from these in a straightforward manner.	cluster analysis;entropy (information theory);hirschberg's algorithm	Jason Utt;Sylvia Springorum;Maximilian Köper;Sabine Schulte im Walde	2014			data mining	NLP	1.977490196835529	-38.34205282766976	92315
ccdaf53af1dab2d5f2ce5e600d42659f96e4d25a	on developing a driver identification methodology using in-vehicle data recorders	automobiles cameras face iris recognition data collection;machine learning identification methods in vehicle data recorders classification stacked generalization supervised learning	Recently, cutting edge technologies to facilitate data collection have emerged on a large scale. One of the most prominent is the in-vehicle data recorder (IVDR). There are multiple ways to assign the IVDR’s data to the different drivers who share the same vehicle. Irrespective of the level of sophistication, all of these technologies still suffer considerable limitations in their accuracy. The purpose of this paper is to propose a methodology, which can identify the driver of a given trip using historical trip-based data. To do so, an advanced machine learning pipeline is proposed. The main goal is to take advantage of highly available data—such as driver-labeled floating car data collected by a IVDR—to build a pattern-based algorithm able to identify the trip’s driver category when its true identity is unknown. This stepwise process includes feature generation/selection, multiple heterogeneous explanatory models, and an ensemble approach (i.e., stacked generalization) to reduce their generalization error. Our goal is to provide an inexpensive alternative to existing driver identification technologies, which can serve as their complement and/or validation purposes. Experiments conducted over a real-world case study from Israel uncover the potential of this idea: it obtained an accuracy of ~88% and Cohen’s Kappa agreement score of ~74%.	1-wire;apriori algorithm;complement (complexity);computer vision;data logger;ensemble learning;experiment;facial recognition system;feature engineering;feature selection;feature vector;futures studies;generalization error;ivdr;machine learning;stacking;stepwise regression;trustworthy computing;unsupervised learning	Luís Moreira-Matias;Haneen Farah	2017	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2016.2639361	data collection;simulation;computer vision;generalization error;data logger;floating car data;data mining;machine learning;artificial intelligence;computer science	ML	10.004291494863322	-38.823225557413075	92619
82f756d557d58565463858741421a046a45b1081	an evolutionary subspace clustering algorithm for high-dimensional data	large data sets;high dimensional data;subspace clustering;synthetic data;evolutionary algorithm	We present an algorithm for generating subspace clusterings of large data sets with many attributes. An evolutionary algorithm is used to form groups of relevant attributes. Those groups are replaced by their centroids, making it possible to cluster the objects in a much lower dimensional space. Preliminary experiments with scalable synthetic data sets suggest that the algorithm generates competitive clusterings while scaling quite well.	cluster analysis;clustering high-dimensional data;evolutionary algorithm;experiment;image scaling;scalability;synthetic data	Seyednaser Nourashrafeddin;Dirk V. Arnold;Evangelos E. Milios	2012		10.1145/2330784.2331011	computer science;canopy clustering algorithm;machine learning;evolutionary algorithm;pattern recognition;data mining;mathematics;synthetic data;clustering high-dimensional data	ML	0.9191540378862697	-41.26745052788607	92803
5af6b89bdfe950efc8cdeb0f96171611a4ae2e9f	practical path-based methods for clustering arbitrary shaped data sets	clustering algorithms time complexity euclidean distance partitioning algorithms clustering methods indexes noise;box clustering path based method arbitrary shaped data set clustering path based clustering arbitrary shaped cluster time complexity path distance calculation modified floyd algorithm large scale data set path based algorithm;pattern clustering computational complexity	Path-based clustering is a well-known method for extracting arbitrary shaped clusters. However, its high time complexity limits some possible applications. In this paper, we propose two new algorithms to speed up the original path-based method. A basic method focuses on the path-distance calculation. A modified Floyd algorithm is applied to reduce the time complexity from Θ(n2m + n3 log n) to Θ(n3 + nk). An improved method emphasizes large scale data sets. A preprocess is used to reduce the number of data points to the path-based algorithm. Moreover, this algorithm can automatic determine the number of clusters by a box clustering. The new approaches are applied to a variety of test data sets with arbitrary shapes and the experimental results show that our method is efficient in dealing with the given problems.	central processing unit;cluster analysis;data point;distance matrix;euclidean distance;floyd–warshall algorithm;k-means clustering;preprocessor;test data;time complexity	Cong Liu;Aimin Zhou;Qiannan Du;Guixu Zhang	2013	2013 Ninth International Conference on Natural Computation (ICNC)	10.1109/ICNC.2013.6818115	correlation clustering;constrained clustering;determining the number of clusters in a data set;combinatorics;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;theoretical computer science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;affinity propagation;clustering high-dimensional data	EDA	0.20437663924015687	-40.389485676810715	92858
0fbb25aaf3cafd609731cc201f194a3c5bb5bcb0	subspace clustering on static datasets and dynamic data streams using bio­-inspired algorithms. (subspace clustering sur jeux de données statiques et sur streams dynamiques à l'aide d'algorithmes bio-inspirés)		Recent technical advances have facilitated the massive acquisition of data described by a large number of measurable properties (high dimensional datasets). New technologies have also enabled the continuous acquisition of data over time, providing users with possibly infinite data streams. The analysis of both high dimensional and streaming data by means of traditional clustering algorithm turn out to be troublesome. In the context of high dimensional data, common similarity measures used by clustering techniques tend to be less meaningful, leading to a degradation of the clustering quality. Moreover, in the case of data streams, the large volume of data does not allow to run several passes on the dataset. In order to overcome these problems, a variety of new approaches has been proposed in the literature. An important task that has been investigated in the context of high dimensional data is subspace clustering. This data mining task is recognized as more general and complicated than standard clustering, since it aims to detect groups of similar objects called clusters, and at the same time to find the subspaces where these similarities appear. Furthermore, subspace clustering approaches as well as traditional clustering ones have recently been extended to deal with data streams by updating clustering models in an incremental way. The different algorithms that have been proposed in the literature, rely on very different algorithmic foundations. Among these approaches, evolutionary algorithms have been under-explored, even if these techniques have proven to be valuable addressing other NP-hard problems. The aim of this thesis was to take advantage of new knowledge from evolutionary biology in order to conceive evolutionary subspace clustering algorithms for static datasets and dynamic data streams. Chameleoclust, the first algorithm developed in this work, takes advantage of the large degree of freedom provided by bio-like features such as a variable genome length, the existence of functional and non-functional elements and mutation operators including chromosomal rearrangements. KymeroClust, our second algorithm, is a k-medians based approach that relies on the duplication and the divergence of genes, a cornerstone evolutionary mechanism. SubMorphoStream, the last one, tackles the subspace clustering task over dynamic data streams. It relies on two important mechanisms that favor fast adaptation of bacteria to changing environments, namely gene amplification and foreign genetic material uptake. All these algorithms were compared to the main state-of-the-art techniques, obtaining competitive results. Results suggest that these algorithms are useful complementary tools in the analyst toolbox. In addition, two applications called EvoWave and EvoMove have been developed to assess the capacity of these algorithms to address real world problems. EvoWave is an application that handles the analysis of Wi-Fi signals to detect different contexts. EvoMove, the second one, is a musical companion that produces sounds based on the clustering of dancer moves captured using motion sensors.	british informatics olympiad;cluster analysis;clustering high-dimensional data;data mining;dynamic data;elegant degradation;evolutionary algorithm;gene expression programming;k-medians clustering;np-hardness;sensor;stream (computing)	Sergio Peignier	2017				DB	-0.08406850586982949	-47.69250359351967	92929
3a1fa961ec5571ff45c9cc6793c75e409bd54109	evolutionary algorithm based on overlapped gene expression	genetic engineering;modelizacion;inmunidad;immune algorithm;fonction polynomiale;immunite;bioinformatique;probabilistic approach;algoritmo genetico;gene expression;modelisation;expression genique;immunity;enfoque probabilista;approche probabiliste;ingenieria genetica;genie genetique;success rate;algorithme genetique;algorithme evolutionniste;genetic algorithm;algoritmo evolucionista;bioinformatica;evolutionary algorithm;funcion polinomial;polynomial function;modeling;expresion genetica;bioinformatics	Inspired by the overlap gene expression in biological study, this paper proposes a novel evolutionary algorithm-EAOGE i.e. Evolutionary Algorithm based on Overlapped Gene Expression. Different from existing works, EAOGE suggests a new expression structure of genes with probabilities of overlapped expression for some segments. The main contributions are: (1) Proposing a novel model and an algorithm of gene expression while borrowing some ideas from artificial immunity algorithm; (2) Analyzing the expressing space and encode characteristic of the new model; (3) The extensive experiments in function finding shows that new model is 2.8~9.7 times faster than usual GEP method, and in higher-degree polynomial function finding, the success rate of EAOGE is over 10 times than usual GEP.	encode;evolutionary algorithm;experiment;gene expression programming;polynomial	Jing Peng;Changjie Tang;Jing Zhang;Chang-an Yuan	2005		10.1007/11539902_23	genetic engineering;gene expression;systems modeling;genetic algorithm;computer science;bioinformatics;artificial intelligence;evolutionary algorithm;algorithm	Comp.	3.7797404319549357	-46.69657571547285	92932
94b0b5157182e6d6ca5ecc6a70f7e401f341aca8	clustering gene expression data using a graph-theoretic approach: an application of minimum spanning trees	graph theory;cluster algorithm;expression pattern;saccharomyces cerevisiae;gene expression data;data clustering;multi dimensional;efficient implementation;minimum spanning tree;global optimization;biological process	MOTIVATION Gene expression data clustering provides a powerful tool for studying functional relationships of genes in a biological process. Identifying correlated expression patterns of genes represents the basic challenge in this clustering problem.   RESULTS This paper describes a new framework for representing a set of multi-dimensional gene expression data as a Minimum Spanning Tree (MST), a concept from the graph theory. A key property of this representation is that each cluster of the expression data corresponds to one subtree of the MST, which rigorously converts a multi-dimensional clustering problem to a tree partitioning problem. We have demonstrated that though the inter-data relationship is greatly simplified in the MST representation, no essential information is lost for the purpose of clustering. Two key advantages in representing a set of multi-dimensional data as an MST are: (1) the simple structure of a tree facilitates efficient implementations of rigorous clustering algorithms, which otherwise are highly computationally challenging; and (2) as an MST-based clustering does not depend on detailed geometric shape of a cluster, it can overcome many of the problems faced by classical clustering algorithms. Based on the MST representation, we have developed a number of rigorous and efficient clustering algorithms, including two with guaranteed global optimality. We have implemented these algorithms as a computer software EXpression data Clustering Analysis and VisualizATiOn Resource (EXCAVATOR). To demonstrate its effectiveness, we have tested it on three data sets, i.e. expression data from yeast Saccharomyces cerevisiae, expression data in response of human fibroblasts to serum, and Arabidopsis expression data in response to chitin elicitation. The test results are highly encouraging.   AVAILABILITY EXCAVATOR is available on request from the authors.	biological processes;chitin;cluster analysis;computer software;file spanning;gene expression;graph - visual representation;graph theory;minimum spanning tree;numerous;partition problem;saccharomyces cerevisiae;tree (data structure);trees (plant);algorithm;statistical cluster	Ying Xu;Victor Olman;Dong Xu	2002	Bioinformatics	10.1093/bioinformatics/18.4.536	correlation clustering;constrained clustering;determining the number of clusters in a data set;fuzzy clustering;flame clustering;computer science;bioinformatics;graph theory;canopy clustering algorithm;minimum spanning tree;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;brown clustering;biological process;biclustering;global optimization;clustering high-dimensional data	Comp.	3.204763166040791	-50.33633940339241	92991
62141d460f51aaaacebc2142f802b14c34641520	a comparative evaluation of proximity measures for spectral clustering		A cluster analysis algorithm is considered successful when the data is clustered into meaningful groups so that the objects in the same group are similar, and the objects residing in two different groups are different from one another. One such cluster analysis algorithm, the spectral clustering algorithm, has been deployed across numerous domains ranging from image processing to clustering protein sequences with a wide range of data types. The input, in this case, is a similarity matrix, constructed from the pair-wise similarity between the data objects. The pair-wise similarity between the objects is calculated by employing a proximity (similarity, dissimilarity or distance) measure. It follows that the success of a spectral clustering algorithm therefore heavily depends on the selection of the proximity measure. While, the majority of prior research on the spectral clustering algorithm emphasizes the algorithm-specific issues, little research has been performed on the evaluation of the performance of the proximity measures. To this end, we perform a comparative and exploratory analysis on several existing proximity measures to evaluate their suitability for the spectral clustering algorithm. Our results indicate that the commonly used Euclidean distance measure may not always be a good choice especially in domains where the data is highly imbalanced and the correct clustering of the boundary objects are crucial. Furthermore, for numeric data, measures based on the relative distances often yield better results than measures based on the absolute distances, specifically when aiming to cluster boundary objects. When considering mixed data, the measure for numeric data has the highest impact on the final outcome and, again, the use of the Euclidian measure may be inappropriate.	algorithm;cluster analysis;computer cluster;euclidean distance;image processing;level of measurement;missing data;peptide sequence;similarity measure;sparse matrix;spectral clustering;usability	Nadia Farhanaz Azam;Herna L. Viktor	2011			machine learning;computer science;artificial intelligence;spectral clustering	ML	0.8468022590686967	-41.99249022158285	93317
c628e9e6f1c099ea715b44f4d0233a04d56726a3	cudagrn: parallel speedup of inferring large gene regulatory networks from expression data using random forest	gpu;random forests;compute unified device architecture cuda;gene regulatory network	Reverse engineering of the Gene Regulatory Networks (GRNs) from high-throughput gene expression data is one of the most pressing challenges of computational biology. In this paper a method for parallelization of the Gene Regulatory Network inference algorithm, GENIE3, based on GPU by exploiting the compute unified device architecture (CUDA) programming model is designed and implemented. GENIE3 solves regulatory network prediction by developing tree based ensemble of Random forests. Our proposed method significantly improves the computational efficiency of GENIE3 by constructing the forest on the GPU in parallel. Our experiments on real and synthetic datasets show that, CUDA implementation outperforms sequential implementation by achieving a speed-up of 15 times (real data) and 14 to 18 times (synthetic data) respectively.	algorithm;cuda;computational biology;dhrystone;experiment;gene regulatory network;graphics processing unit;high-throughput computing;parallel computing;programming model;random forest;reverse engineering;speedup;synthetic data;the forest;throughput	Seyed Ziaeddin Alborzi;D. A. K. Maduranga;Rui Fan;Jagath C. Rajapakse;Jie Zheng	2014		10.1007/978-3-319-09192-1_8	random forest;biology;gene regulatory network;parallel computing;computer science;bioinformatics;theoretical computer science;machine learning;genetics	Comp.	-1.9773658909596066	-50.998995981723674	93809
fad8d1eede0e948a004440fc7e7199de9b61c64b	application of artificial bee colony for intrusion detection systems	artificial bee colony;swarm intelligence;intrusion detection;machine learning	The demand for better intrusion detection systems, especially anomaly intrusion detection, increases daily, as new attacks arise and Internet speeds increase. The criterion for a good intrusion detection system is to detect emerging attacks with high accuracy at line rates. Existing systems suffer from high false positives and negatives, and are unable to handle increasing traffic rates. This paper applies artificial bee colony for anomaly-based intrusion detection systems. In addition, it uses two feature selection techniques to reduce the amount of data used for detection and classification. KDD Cup 99 dataset was used to evaluate the proposed algorithm. Experimental results show that artificial bee colony achieves average accuracy rate of 97.5% for known attacks and 93.2% overall for known and unknown attacks. The new algorithm outperforms all methods reported in the literature. Copyright © 2012 John Wiley & Sons, Ltd.		Monther Aldwairi;Yaser M. Khamayseh;Mohammad Al-Masri	2015	Security and Communication Networks	10.1002/sec.588	anomaly-based intrusion detection system;intrusion detection system;swarm intelligence;computer science;artificial intelligence;machine learning;computer security	Security	6.332151505905799	-38.11677786827952	93810
61048235350f050a6b200548cc467b68c4fe9b2d	a variational bayesian approach to robust identification of switched arx models	biological patents;biomedical journals;text mining;europe pubmed central;uncertainty;bayes methods;citation search;citation networks;research articles;abstracts;open access;variational bayesian vb number of local models robustness switched auto regressive exogenous sarx;integrated circuit modeling;life sciences;clinical guidelines;robustness;full text;switches;rest apis;gaussian distribution;switches robustness bayes methods uncertainty data models gaussian distribution integrated circuit modeling;orcids;europe pmc;biomedical research;data models;bioinformatics;literature search	A variational Bayesian approach to robust identification of switched auto-regressive exogenous models is developed in this paper. By formulating the problem of interest under a full Bayesian identification framework, the number of local-models can be determined automatically, while accounting for the uncertainty of parameter estimates in the overall identification procedure. A set of significance coefficients is used to assign proper importance weights to local-models. By maximizing the marginal likelihood of the identification data, insignificant local-models will be suppressed and the optimal number of local-models can be determined. Considering the fact that the identification data may be contaminated with outliers, t distributions with adjustable tails are utilized to model the contaminating noise so that the proposed identification algorithm is robust. The effectiveness of the proposed Bayesian approach is demonstrated through a simulated example as well as a detailed industrial application.	algorithm;appendix;arx;calculus of variations;cluster analysis;coefficient;data point;embedding;estimated;functional derivative;hp 48 series;ik gene;inference;inverse kinematics;marginal model;mathematical optimization;normal statistical distribution;optimization problem;population parameter;robustness (computer science);significance arithmetic;tail;tails;variational principle;weight;drk protein, drosophila;late nucleophagy;retrovirus vector ln;statistical cluster;t-distribution	Yaojie Lu;Biao Huang;Shima Khatibisepehr	2016	IEEE Transactions on Cybernetics	10.1109/TCYB.2015.2499771	normal distribution;data modeling;econometrics;mathematical optimization;text mining;uncertainty;network switch;computer science;artificial intelligence;machine learning;data mining;statistics;robustness	ML	8.675035768948204	-51.434474610828254	93892
0304ab373ebe4eede75498af76cdd089c55ca914	sorted kernel matrices as cluster validity indexes	sorting.;kernel matrix;— affinity matrix;fuzzy c-means fcm;reordering;clustering;data analysis;distance metric;kernel machine;indexation	Two basic issues for data analysis and kernel-machines design are approached in this paper: determining the number of partitions of a clustering task and the parameters of kernels. A distance metric is presented to determine the similarity between kernels and FCM proximity matrices. It is shown that this measure is maximized, as a function of kernel and FCM parameters, when there is coherence with embedded structural information. We show that the alignment function can be maximized according FCM and kernel parameters. The results presented shed some light on the general problem of setting up the number of partitions in a clustering task and in the proper setting of kernel parameters according to structural information. Keywords— Affinity matrix, Clustering, Fuzzy C-Means (FCM), Kernel matrix, Reordering, Sorting.	cluster analysis;embedded system;fuzzy cognitive map;kernel (operating system);kernel method;sorting	Francisco Queiroz;Antonio Braga;Witold Pedrycz	2009			kernel (statistics);polynomial kernel;kernel principal component analysis;kernel method;cluster analysis;kernel embedding of distributions;artificial intelligence;pattern recognition;variable kernel density estimation;radial basis function kernel;mathematics	ML	1.0623701813956905	-41.98069379669416	94017
b87f1f6240ca0dbd7be96936c98e77e43bdc50f4	partitioned optimization algorithms for multiple sequence alignment	dynamic programming;biology computing;sequences;optimisation;ant colony optimization;protein family;iterative algorithms;sequence similarity;nucleotides;protein sequence;amino acid sequence;biology computing molecular biophysics optimisation sequences proteins;biology;partitioning algorithms sequences iterative algorithms computer science bioinformatics ant colony optimization dynamic programming hidden markov models biology assembly;assembly;large scale;local structure;hidden markov models;proteins;phylogenetic tree;ant colony algorithm;molecular biology;molecular biophysics;genetic algorithm;sequence alignment;multiple sequence alignment;ant colony optimization multiple sequence alignment molecular biology bioinformatics partitioning approach;computer science;partitioning approach;optimal algorithm;evolutionary theory;nucleic acid;partitioning algorithms;bioinformatics	Multiple sequence alignment is an important and difficult problem in molecular biology and bioinformatics. In this paper, we propose a partitioning approach that significantly improves the solution time and quality by utilizing the locality structure of the problem. The algorithm solves the multiple sequence alignment in three stages. First, an automated and suboptimal partitioning strategy is used to divide the set of sequences into several subsections. Then a multiple sequence alignment algorithm based on ant colony optimization is used to align the sequences of each subsection. Finally, the alignment of original sequences can be obtained by assembling the result of each subsection. The ant colony algorithm is highly optimized in order to avoid local optimal traps and converge to global optimal efficiently. Experimental results show that the algorithm can significantly reduce the running time and improve the solution quality on large-scale multiple sequence alignment benchmarks.	algorithm;align (company);ant colony optimization algorithms;bioinformatics;converge;locality of reference;mathematical optimization;multiple sequence alignment;time complexity	Yixin Chen;Yi Pan;Juan Chen;Wei Liu;Ling Chen	2006	20th International Conference on Advanced Information Networking and Applications - Volume 1 (AINA'06)	10.1109/AINA.2006.260	ant colony optimization algorithms;computer science;bioinformatics;theoretical computer science;machine learning;needleman–wunsch algorithm;molecular biophysics	Comp.	-1.1025568541685522	-50.67978547365286	94062
8f0eda51a4df81ed750e2cb085c151d7bb77f3d2	probabilistic-fuzzy clustering algorithm	fuzzy c means algorithm;cluster algorithm;pattern clustering;fuzzy c mean;statistical analysis gaussian distribution pattern clustering;probabilistic distance structure;k means;objective function;fuzzy clustering;statistical analysis;probabilistic fuzzy clustering algorithm;statistics;clustering algorithms gaussian distribution covariance matrix euclidean distance data mining shape fuzzy sets design engineering measurement standards constraint optimization;probabilistic distance structure probabilistic fuzzy clustering algorithm fuzzy c means algorithm gaussian distribution;gaussian distributions;gaussian distribution	Clustering algorithms divide up a data set into classes/clusters, where similar data objects are assigned to the same cluster. When the boundary between clusters is ill defined, which yields situations where the same data object belongs to more than one class. The notion of fuzzy clustering becomes relevant. In this course, each datum belongs to a given class with some membership grade, between 0 and 1. The most prominent fuzzy clustering algorithm is the fuzzy c-means introduced by Bezdek, a fuzzification of k-means or ISODATA algorithm. On the other hand, several research issues have been risen regarding both the objective function to be minimized and the optimization constraints, which help to identify proper cluster shape. This paper addresses the issue where the data objects consist of Gaussian distributions. The approach advocated in this case is to use a probabilistic distance structure based on BHATTACHARYYA distance in standard fuzzy c-means algorithm. This leads to a modified FCM, with a probabilistic distance structure. The performances of the proposed algorithm are evaluated through some academic examples and the superiority of the modified FCM was clearly laid bare	algorithm;cluster analysis;fuzzy clustering;fuzzy cognitive map;fuzzy set;geodetic datum;k-means clustering;loss function;mathematical optimization;optimization problem;performance	Samia Nefti-Meziani;Mourad Oussalah	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1401288	normal distribution;complete-linkage clustering;correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;k-medoids;statistics;k-means clustering;clustering high-dimensional data	Robotics	3.291922105317612	-39.866095582066144	94248
bafec77eb7d1036cedfdabe4efdb9927606ad364	an adaptive cluster-target covariance based principal component analysis for interval-valued data	fuzzy classification;pattern clustering;human based subjective decision adaptive cluster target covariance principal component analysis interval valued data fuzzy classification structure higher dimensional space alignment criterion cluster similarity weighted covariance matrix;covariance analysis;measurement;adaptive cluster target covariance;higher dimensional space;cluster similarity;interval valued data;joints;fuzzy set theory;expected value;weighted covariance matrix;equations principal component analysis mathematical model covariance matrix clustering methods joints measurement;principal component analysis;fuzzy classification structure;number of clusters;mathematical model;clustering methods;data consistency;human based subjective decision;alignment criterion;principal component analysis covariance analysis fuzzy set theory pattern clustering;covariance matrix	We propose a new principal component analysis (PCA) for interval-valued data by using a covariance involving a fuzzy classification structure based on dissimilarity in higher dimensional space in which objects exist. The covariance for interval-valued data is obtained adaptively by evaluating the validity of the fuzzy classification structure based on the selection of an appropriate number of clusters. In order to select an appropriate number of clusters, we propose an alignment criterion to evaluate the obtained classification structure and prove the concentration of the criterion around the expected value with respect to variation of similarity among clusters. The merit of this PCA is to consider not only the projection of objects to a lower dimensional space, but also the dissimilarity of objects in a higher dimensional space by using a weighted covariance matrix. The weight is estimated as the degree of contribution for the fuzzy classification structure based on dissimilarity of objects in the higher dimensional space. A numerical example of interval-valued data consisting of human based subjective decisions shows a better performance when compared with a result of an ordinary PCA.	cluster analysis;fuzzy classification;fuzzy clustering;numerical analysis;principal component analysis	Mika Sato-Ilic	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584542	covariance matrix;analysis of covariance;fuzzy classification;machine learning;pattern recognition;mathematical model;mathematics;fuzzy set;data consistency;expected value;measurement;statistics;principal component analysis	Robotics	2.5473854613689286	-38.60857639499798	94255
c82a1fc0bfe50f72646600e77544def2e46ba996	a method of two-stage clustering with constraints using agglomerative hierarchical algorithm and one-pass k-means++			algorithm;k-means++	Yusuke Tamura;Nobuhiro Obara;Sadaaki Miyamoto	2013		10.1007/978-3-319-02821-7_3		ML	2.5892386432172487	-41.214371588288564	94343
61a33bd35046bf02b9e0a1ea55b3cfa716c14ec1	computational modeling and visualization in the biological sciences		Discoveries in computational molecular cell biology and bioinformatics promise to provide new therapeutic interventions to disease. With the rapid growth of sequence and structural information for thousands of proteins, and hundreds of cell types computational processingare a restricting factor in obtaining quantitative understanding of molecular-cellular function. Processing and analysis is necessary both for input data (often from imaging) and simulation results. To make biological conclusions, this data must be input to and combined with results from computational analysis and simulations. Furthermore, as parallelism is increasingly prevalent, utilizing the available processing power is essential to development of scalable solutions needed for realistic scientific inquiry. However, complex image processing and even simulations performed on large clusters, multi-core CPU, GPU-type parallelization means that nave cache unaware algorithms may not efficiently utilize available hardware. Future gains thus require improvements to a core suite of algorithms underpinning the data processing, simulation, optimization and visualization needed for scientific discovery. In this talk, I shall highlight current progress on these algorithms as well as provide several challenges for the scientific community.	computation;computational model	Chandrajit L. Bajaj	2012		10.1007/978-3-642-32129-0_1	computational biology;biological data visualization	HPC	-4.139001393128029	-51.13917371997143	94367
9226b34f617b24b1031e584714f0a8665f151949	a non-negative matrix factorization method for detecting modules in heterogeneous omics multi-modal data		MOTIVATION Recent advances in high-throughput omics technologies have enabled biomedical researchers to collect large-scale genomic data. As a consequence, there has been growing interest in developing methods to integrate such data to obtain deeper insights regarding the underlying biological system. A key challenge for integrative studies is the heterogeneity present in the different omics data sources, which makes it difficult to discern the coordinated signal of interest from source-specific noise or extraneous effects.   RESULTS We introduce a novel method of multi-modal data analysis that is designed for heterogeneous data based on non-negative matrix factorization. We provide an algorithm for jointly decomposing the data matrices involved that also includes a sparsity option for high-dimensional settings. The performance of the proposed method is evaluated on synthetic data and on real DNA methylation, gene expression and miRNA expression data from ovarian cancer samples obtained from The Cancer Genome Atlas. The results show the presence of common modules across patient samples linked to cancer-related pathways, as well as previously established ovarian cancer subtypes.   AVAILABILITY AND IMPLEMENTATION The source code repository is publicly available at https://github.com/yangzi4/iNMF.   CONTACT gmichail@umich.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	accessibility;algorithm;antivirus software;binocular disparity;bioinformatics;biological markers;biological network;biological system;cell cycle control;cell survival;contribution;copy number polymorphism;dna copy number variations;dna repair;data collection;data sources;database validation plan;electronic signature;extended rotated sidebent;file spanning;gene expression;genetic heterogeneity;high-throughput computing;interaction;lithium;malignant neoplasm of ovary;micrornas;modal logic;mutation;neoplasms;non-negative matrix factorization;non-negative matrix factorization;numerous;omics;patients;regulation;repository (version control);sensor;snapshot (computer storage);source code;sparse matrix;subtype (attribute);synthetic data;throughput;algorithm;bacterium u01;funding grant;ovarian neoplasm	Zi Yang;George Michailidis	2016	Bioinformatics	10.1093/bioinformatics/btv544	bioinformatics;theoretical computer science;data mining	Comp.	7.027933475809749	-51.92661211741396	94465
04b1447cf73d64a7c12e6c0bd6040ea47260a810	improving categorical data clustering algorithm by weighting uncommon attribute value matches	cluster algorithm;categorical data	This paper presents an improved Squeezer algorithm for categorical data clustering by giving greater weight to uncommon attribute value matches in similarity computations. Experimental results on real life datasets show that, the modified algorithm is superior to the original Squeezer algorithm and other clustering algorithm with respect to clustering accuracy.	categorical variable;cluster analysis;computation;heuristic;pollard's rho algorithm for logarithms;real life;rock (processor)	Zengyou He;Xiaofei Xu;Shengchun Deng	2006	Comput. Sci. Inf. Syst.	10.2298/CSIS0601023H	correlation clustering;constrained clustering;data stream clustering;categorical variable;k-medians clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;fsa-red algorithm;cluster analysis;single-linkage clustering	DB	1.5263358149681578	-40.7776900742666	94573
2b910054bcb1210538aec70cefa0aa787e62dec7	exploring the uniform effect of fcm clustering: a data distribution perspective	uniform effect;data distribution;coefficient of variation cv;clustering;fuzzy c means fcm	"""Fuzzy c-means (FCM) is a well-known and widely used fuzzy clustering method. Though there have been considerable studies that focused on the improvement of FCM algorithm or its applications, it is still necessary to understand the effect of data distributions on the performance of FCM. In this paper, we present an organized study of FCM clustering from the perspective of data distribution. We first analyze the structure of the objective function of FCM and find that FCM has the same uniform effect as K-means. Namely, FCM also tends to produce clusters of relatively uniform sizes. The coefficient of variation (CV) is introduced to measure the variation of cluster sizes in a given data set. Then based on the change of CV values between the original """"true"""" cluster sizes and the cluster sizes partitioned by FCM clustering, a necessary but not sufficient criterion for the validation of FCM clustering is proposed from the data distribution perspective. Finally, our experiments on six synthetic data sets and ten real-world data sets further demonstrate the uniform effect of FCM. It tends to reduce the variation in cluster sizes when the CV value of the original data distribution is larger than 0.88, and increase the variation when the variation of original """"true"""" cluster sizes is low."""	cluster analysis;fuzzy cognitive map	Kaile Zhou;Shanlin Yang	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.01.001	computer science;machine learning;pattern recognition;data mining;cluster analysis;statistics	NLP	0.8652544682498617	-40.91560711768285	94985
d40b8eb6a74c6fec78762bdd7f2195f8a1c7b7f4	metric-space search in bioinformatics	metric space;sequence similarity;iron;indexing method;statistical properties;user experience;biological data;similarity search	Of the many problems in biological data retrieval, the problem of biological sequence retrieval has the highest profile. The standard solution to this problem, BLAST, has ascended, like google, to become synonymous with search. Also like Google, BLAST leverages statistical properties as heuristics to create a good user experience. Ironically, many early biological sequence similarity efforts explicitly sought to model evolutionary distance as a metric-distance. Recent interest in metric-index methods has rekindled these early directions. A review of these efforts provides an opportunity to characterize the challenges and opportunities in similarity search of biological data.	ascendency;blast;bioinformatics;data retrieval;heuristic (computer science);sequence alignment;similarity search;user experience	Daniel P. Miranke	2010	SIGSPATIAL Special	10.1145/1862413.1862422	user experience design;biological data;metric space;computer science;data science;data mining;iron;information retrieval	Comp.	-3.6094757929540746	-50.38888306878043	95639
194b5530c07826b9bf2e69d654115631a43f5efd	reverse nearest neighbors search in ad hoc subspaces	databases;nearest neighbor searches;q measurement;distributed database;conference_paper;neural networks;query processing;reverse nearest neighbor;lungs;resource management;euclidean distance;spatial database;indexation;spatial databases;nearest neighbor;recurrent neural networks nearest neighbor searches databases neural networks multidimensional systems computer science lungs q measurement resource management euclidean distance;distributed databases;recurrent neural networks;missing values;experimental evaluation;computer science;synthetic data;article;multidimensional systems	Given an object q, modeled by a multidimensional point, a reverse nearest neighbors (RNN) query returns the set of objects in the database that have q as their nearest neighbor. In this paper, we study an interesting generalization of the RNN query, where not all dimensions are considered, but only an ad hoc subset thereof. The rationale is that 1) the dimensionality might be too high for the result of a regular RNN query to be useful, 2) missing values may implicitly define a meaningful subspace for RNN retrieval, and 3) analysts may be interested in the query results only for a set of (ad hoc) problem dimensions (i.e., object attributes). We consider a suitable storage scheme and develop appropriate algorithms for projected RNN queries, without relying on multidimensional indexes. Given the significant cost difference between random and sequential data accesses, our algorithms are based on applying sequential accesses only on the projected atomic values of the data at each dimension, to progressively derive a set of RNN candidates. Whether these candidates are actual RNN results is then validated via an optimized refinement step. In addition, we study variants of the projected RNN problem, including RkNN search, bichromatic RNN, and RNN retrieval for the case where sequential accesses are not possible. Our methods are experimentally evaluated with real and synthetic data	algorithm;design rationale;experiment;hoc (programming language);missing data;random neural network;refinement (computing);synthetic data	Man Lung Yiu;Nikos Mamoulis	2006	22nd International Conference on Data Engineering (ICDE'06)	10.1109/ICDE.2006.129	multidimensional systems;computer science;recurrent neural network;machine learning;pattern recognition;data mining;euclidean distance;database;k-nearest neighbors algorithm;distributed database;spatial database;synthetic data	DB	-2.1894775513761116	-42.70237711834306	95765
548e52016f5a9ce28a0060bd22ea511a1d4c7b11	set matching measures for external cluster validity	cluster validation;measurement;data mining;indexes;external validity index;clustering;correction for chance;normalization;clustering algorithms;mutual information;comparing clusterings;entropy;adjustment for chance;partitioning algorithms	Comparing two clustering results of a data set is a challenging task in cluster analysis. Many external validity measures have been proposed in the literature. A good measure should be invariant to the changes of data size, cluster size, and number of clusters. We give an overview of existing set matching indexes and analyze their properties. Set matching measures are based on matching clusters from two clusterings. We analyze the measures in three parts: 1) cluster similarity, 2) matching, and 3) overall measurement. Correction for chance is also investigated and we prove that normalized mutual information and variation of information are intrinsically corrected. We propose a new scheme of experiments based on synthetic data for evaluation of an external validity index. Accordingly, popular external indexes are evaluated and compared when applied to clusterings of different data size, cluster size, and number of clusters. The experiments show that set matching measures are clearly better than the other tested. Based on the analytical comparisons, we introduce a new index called Pair Sets Index (PSI).	cluster analysis;experiment;external validity;mutual information;national vulnerability database;pure function;purity (quantum mechanics);requirement;similarity measure;synthetic data;theory;variation of information	Mohammad Rezaei;Pasi Fränti	2016	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2016.2551240	computer science;machine learning;pattern recognition;data mining;mathematics;cluster analysis;statistics	DB	1.6216983633440187	-39.0770799765047	95832
6fcc4120aa1899f03752a2b2686a7e335581dc33	large-scale inference of conjunctive bayesian networks		UNLABELLED The continuous time conjunctive Bayesian network (CT-CBN) is a graphical model for analyzing the waiting time process of the accumulation of genetic changes (mutations). CT-CBN models have been successfully used in several biological applications such as HIV drug resistance development and genetic progression of cancer. However, current approaches for parameter estimation and network structure learning of CBNs can only deal with a small number of mutations (<20). Here, we address this limitation by presenting an efficient and accurate approximate inference algorithm using a Monte Carlo expectation-maximization algorithm based on importance sampling. The new method can now be used for a large number of mutations, up to one thousand, an increase by two orders of magnitude. In simulation studies, we present the accuracy as well as the running time efficiency of the new inference method and compare it with a MLE method, expectation-maximization, and discrete time CBN model, i.e. a first-order approximation of the CT-CBN model. We also study the application of the new model on HIV drug resistance datasets for the combination therapy with zidovudine plus lamivudine (AZT + 3TC) as well as under no treatment, both extracted from the Swiss HIV Cohort Study database.   AVAILABILITY AND IMPLEMENTATION The proposed method is implemented as an R package available at https://github.com/cbg-ethz/MC-CBN CONTACT: niko.beerenwinkel@bsse.ethz.ch   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	approximation algorithm;bayesian network;bioinformatics;ct scan;cannabinol;color gradient;estimation theory;expectation–maximization algorithm;graphical model;importance sampling;inference;lamivudine;monte carlo method;mutation;one thousand;order of approximation;population parameter;r language;sampling (signal processing);simulation;switzerland;time complexity;tree accumulation;zidovudine	Hesam Montazeri;Jack Kuipers;Roger D. Kouyos;Jürg Böni;Sabine Yerly;Thomas Klimkait;Vincent Aubert;Huldrych F. Günthard;Niko Beerenwinkel;The Swiss Hiv Cohort Study	2016	Bioinformatics	10.1093/bioinformatics/btw459	frequentist inference;bayesian statistics	Comp.	2.64132682655972	-51.741702631577226	96071
04ed760e0a8e557b15048092c10710257598667f	a study of fuzzy clustering to archetypal analysis		This paper presents a comparative study between a method for fuzzy clustering which retrieves pure individual types from data, the fuzzy clustering with proportional membership (FCPM), and an archetypal analysis algorithm based on Furthest-Sum approach (FS-AA). A simulation study comprising 82 data sets is conducted with a proper data generator, FCPM-DG, whose goal is twofold: first, to analyse the ability of archetypal clustering algorithm to recover Archetypes from data of distinct dimensionality; second, to analyse robustness of FCPM and FS-AA algorithms to outliers. The effectiveness of these algorithms are yet compared on clustering 12 diverse benchmark data sets from machine learning. The evaluation conducted with five primer unsupervised validation indices shows the good quality of the clustering solutions.	archetypal analysis;fuzzy clustering	Gonçalo Sousa Mendes;Susana Nascimento	2018		10.1007/978-3-030-03496-2_28	robustness (computer science);artificial intelligence;computer science;fuzzy clustering;cluster analysis;outlier;curse of dimensionality;pattern recognition;data set	Logic	0.695168550178462	-42.16321377657373	96119
a3179f7292d4d6d7cabb1fd4983860b8fb228a0a	nugget discovery with a multi-objective cultural algorithm		Partial classification popularly known as nugget discovery comes under descriptive knowledge discovery. It involves mining rules for a target class of interest. Classification “If-Then” rules are the most sought out by decision makers since they are the most comprehensible form of knowledge mined by data mining techniques. The rules have certain properties namely the rule metrics which are used to evaluate them. Mining rules with user specified properties can be considered as a multi-objective optimization problem since the rules have to satisfy more than one property to be used by the user. Cultural algorithm (CA) with its knowledge sources have been used in solving many optimization problems. However research gap exists in using cultural algorithm for multi-objective optimization of rules. In the current study a multi-objective cultural algorithm is proposed for partial classification. Results of experiments on benchmark data sets reveal good performance.	benchmark (computing);cultural algorithm;data mining;experiment;mathematical optimization;mined;multi-objective optimization;optimization problem	Sujatha Srinivasan;Sivakumar Ramakrishnan	2012	CoRR	10.5121/cseij.2012.2302	artificial intelligence;data science;machine learning;data mining	ML	7.827521171218005	-43.91349135154935	96426
d2d81a61e2bb9ef37921799de3d53be99b8ce959	an effective data classification algorithm based on the decision table grid	pattern classification decision tables;decision table data mining knn classification grid;data mining;classification;grid;knn;classification algorithms rough sets geophysics computing partitioning algorithms information systems distributed computing grid computing home computing information science data engineering;pattern classification;k nearest neighbor;decision tables;high dimension;data classification;decision table;homegeneous data data classification algorithm decision table grid k nearest neighbor classification technique	In order to overcome the disadvantages that the traditional k-nearest neighbor classification technique makes inadquate use of distribution characteristics of homegeneous data, and that it is of slow speed and low efficiency, an effective data classification algorithm based on the decision table grid is presented. The main process is to construct the corresponding decision table after discretizing training samples, to map the training samples to the corresponding grid based on the decision condition, and to map the samples to be classified to the corresponding grid, then to judge the classification of the samples by the given principle. The algorithm can quickly classify the samples to be classified and can improve the precision as well. Experiments show that it has good effect, being more suitable for high dimension data classification and capable of dealing with the training samples with many classes.	decision table;experiment;k-nearest neighbors algorithm	Hongjie Liu;Boqin Feng;Jianjie Wei	2008	Seventh IEEE/ACIS International Conference on Computer and Information Science (icis 2008)	10.1109/ICIS.2008.101	computer science;machine learning;pattern recognition;data mining;one-class classification	Robotics	4.05288612524397	-38.06455994414822	96434
53a1d192830bc5469b0e57eab8d90151b928dbe5	identification of tandem repeats over large-alphabet inputs	tandem repeats;sequence analysis;large alphabet;primitive;maximal;bioinformatics	Makes use of simple list structure instead of suffix trees.Algorithm finds all the primitive maximal tandem repeats of all sizes.Theoretically the algorithm is linear in space.In practice the algorithm is linear in time and independent of the size of alphabet.Application in stringology and bioinformatics. A primitive tandem repeat α is a substring in string S if it can be expressed as two or more contiguous copies of β, where the base β cannot be expressed in terms of yet a shorter substring. Substring α is maximal if there is no copy of β to either its left or right. Tandem repeats (or arrays) are known to play an important role in biology, e.g. determining parentage. We present a deterministic algorithm that finds all the exact primitive maximal tandem arrays that occur in S, where at iteration k it discovers all the repeats with base length k. Our algorithm uses a simple list structure for all its operations. In theory, the algorithm scales well with the size of the alphabet. For strings over the alphabet Σ it has a complexity of O ( | S | + | Σ | ) in space, and O ( B | S | - B 2 / 2 ) in time, where B is the length of the longest primitive base of a tandem repeat in S. Experimental results on real biological sequences, randomly generated sequences using large sized alphabets, and Fibonacci strings, show that in practice the algorithm has indeed a linear complexity, both in space and time.		Aqil M. Azmi	2016	Inf. Sci.	10.1016/j.ins.2016.01.050	combinatorics;computer science;bioinformatics;sequence analysis;mathematics;algorithm;tandem repeat	DB	-0.7631960335599213	-51.55346267985721	96629
277236e3281cdc32289e9c34a28c7238f3a8e191	feature reduced weighted fuzzy binarization for histogram comparison of promoter sequences		Effective biological sequence analysis methods are in great demand. This is due to the increasing amount of sequence data being generated from the improved sequencing techniques. In this study, we select statistically significant features/motifs from the Position Specific Motif Matrices of promoters. Later, we reconstruct these matrices using the chosen motifs. The reconstructed matrices are then binarized using triangular fuzzy membership values. Then the binarized matrix is assigned weights to obtain the texture features. Histogram is plotted to visualize the distribution of texture values of each promoter and later histogram difference is computed across pairs of promoters. This histogram difference is a measure of underlying dissimilarity in the promoters being compared. A dissimilarity matrix is constructed using the histogram difference values of all the promoter pairs. From the experiments, the combination of feature reduction and fuzzy binarization seems to be useful in promoter differentiation.		K. Kouser;Lalitha Rangarajan	2016		10.1007/978-981-10-4859-3_16	histogram matching;pattern recognition	Vision	2.7637265691327473	-48.35673079079488	96732
ff9982bde9bfb3976a13654dfc53dcea36760588	d-fuzzstream: a dispersion-based fuzzy data stream clustering		Fuzzy clustering algorithms have recently been investigated as appropriate techniques to extract knowledge from Data Streams due to their unsupervised nature and flexibility to deal with changes in the distribution of data. While most fuzzy clustering algorithms for Data Streams are based on chunks, the FuzzStream algorithm, proposed before by the authors of this paper, pioneered a fuzzy extension of a different approach known as the Online-Offline Framework (OOF). The extended framework, named Fuzzy Online-Offline Framework (FOOF), includes two steps known as fuzzy abstraction and fuzzy clustering. The fuzzy abstraction step continuously summarizes data in a set of cluster features called Fuzzy Micro Cluster (FMiC). Then, these FMiCs are later clustered in the fuzzy clustering step to generate the data partition. Although FuzzStream has shown to be more robust than other OOF-based algorithms, the fuzzy abstraction process in the algorithm overly reduces the data summarization, almost producing one FMiC for each example, also suffering from high overlapping FMiCs. Furthermore, the algorithm has a long processing time due to its need to calculate membership matrices for every example. In this paper we propose the d-FuzzStream algorithm, an adaptation of FuzzStream using the concepts of fuzzy dispersion and fuzzy similarity in order to improve the data summarization while minimizing the complexity of the algorithm. Experiments showed that the proposed algorithm generates FMiCs with higher representativeness and lower execution time than its original version, still producing similar clustering results.	algorithm;cluster analysis;data stream clustering;fuzzy clustering;online and offline;run time (program lifecycle phase);unsupervised learning	Veronica Ralls;Priscilla de Abreu Lopes;Heloisa A. Camargo	2018	2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2018.8491534	fuzzy logic;automatic summarization;representativeness heuristic;machine learning;artificial intelligence;fuzzy clustering;data stream mining;data stream clustering;computer science;cluster analysis;abstraction	DB	-1.300825299443669	-38.35971274896752	96751
a4dfa70d63cec4b0a0c2d5822d81dc9d1bbec173	a nonparametric bayesian method of translating machine learning scores to probabilities in clinical decision support	bayesian;calibration;machine learning;nonparametric;statistics	Probabilistic assessments of clinical care are essential for quality care. Yet, machine learning, which supports this care process has been limited to categorical results. To maximize its usefulness, it is important to find novel approaches that calibrate the ML output with a likelihood scale. Current state-of-the-art calibration methods are generally accurate and applicable to many ML models, but improved granularity and accuracy of such methods would increase the information available for clinical decision making. This novel non-parametric Bayesian approach is demonstrated on a variety of data sets, including simulated classifier outputs, biomedical data sets from the University of California, Irvine (UCI) Machine Learning Repository, and a clinical data set built to determine suicide risk from the language of emergency department patients. The method is first demonstrated on support-vector machine (SVM) models, which generally produce well-behaved, well understood scores. The method produces calibrations that are comparable to the state-of-the-art Bayesian Binning in Quantiles (BBQ) method when the SVM models are able to effectively separate cases and controls. However, as the SVM models’ ability to discriminate classes decreases, our approach yields more granular and dynamic calibrated probabilities comparing to the BBQ method. Improvements in granularity and range are even more dramatic when the discrimination between the classes is artificially degraded by replacing the SVM model with an ad hoc k-means classifier. The method allows both clinicians and patients to have a more nuanced view of the output of an ML model, allowing better decision making. The method is demonstrated on simulated data, various biomedical data sets and a clinical data set, to which diverse ML methods are applied. Trivially extending the method to (non-ML) clinical scores is also discussed.	calibration;class;clinical decision support system;decision making;decision support systems, clinical;evaluation procedure;hoc (programming language);horner's method;k-means clustering;machine learning;machine translation;patients;probability;product binning;quality of health care;support vector machine	Brian Connolly;K. Bretonnel Cohen;Daniel Santel;Ulya Bayram;John Pestian	2017		10.1186/s12859-017-1736-3	data mining;clinical decision support system;bioinformatics;categorical variable;probabilistic logic;suicide risk;data set;nonparametric statistics;machine learning;artificial intelligence;computer science;bayesian probability	ML	7.816949856002562	-43.75907211787351	96881
965a2801b809213daf966eb215fd832a05664315	ppcas: implementation of a probabilistic pairwise model for consistency-based multiple alignment in apache spark		Large-scale data processing techniques, currently known as Big-Data, are used to manage the huge amount of data that are generated by sequencers. Although these techniques have significant advantages, few biological applications have adopted them. In the Bioinformatic scientific area, Multiple Sequence Alignment (MSA) tools are widely applied for evolution and phylogenetic analysis, homology and domain structure prediction. Highly-rated MSA tools, such as MAFFT, ProbCons and T-Coffee (TC), use the probabilistic consistency as a prior step to the progressive alignment stage in order to improve the final accuracy. In this paper, a novel approach named PPCAS (Probabilistic Pairwise model for Consistency-based multiple alignment in Apache Spark) is presented. PPCAS is based on the MapReduce processing paradigm in order to enable large datasets to be processed with the aim of improving the performance and scalability of the original algorithm.	apache spark	Jordi Lladós;Fernando Guirado;Fernando Cores	2017		10.1007/978-3-319-65482-9_45	computer science;phylogenetic tree;multiple sequence alignment;scalability;spark (mathematics);probabilistic logic;pairwise comparison;machine learning;data processing;artificial intelligence	NLP	-1.7261135609084364	-50.72257766459114	96954
08ca1e159cea98d37e8ac89903263fdde15bb4e8	kgem: an em-based algorithm for local reconstruction of viral quasispecies	genomics;sociology error correction frequency estimation sensitivity computer science educational institutions;microorganisms expectation maximisation algorithm genomics;viral quasispecies next generation sequencing local reconstruction expectation maximization error correction;microorganisms;expectation maximization method kgem em based algorithm viral quasispecies local reconstruction sequencing errors viral population natural heterogeneity;expectation maximisation algorithm	The main challenge in local viral quasispecies reconstruction is to eliminate sequencing errors while preserving the natural heterogeneity of the viral population. This paper presents a new approach to error correction via an expectation maximization (EM) method.	error detection and correction;expectation–maximization algorithm	Alexander Artyomenko;Nicholas Mancuso;Alex Zelikovsky;Pavel Skums;Ion I. Mandoiu	2013	2013 IEEE 3rd International Conference on Computational Advances in Bio and medical Sciences (ICCABS)	10.1109/ICCABS.2013.6629226	biology;mathematical optimization;genomics;bioinformatics;machine learning;microorganism	Vision	2.45527588931602	-51.56008256613929	97047
3c99cc77e2631594a842f58d56cf18a88e43aa57	learning to recall		From the infinite set of routes that you could drive to work, you have probably found a way that gets you there in a reasonable time, dealing with traffic conditions and running minimal risks. Humans are very good at learning such efficient sequences based on very little feedback, but it is unclear how the brain learns to solve such tasks. At CWI, in collaboration with the Netherlands Institute for Neuroscience (NIN), we have developed a biologically realistic neural model that, like animals, can be trained to recall relevant past events and then to perform optimal action sequences, just by rewarding it for correct sequences of actions. The model explains neural activations found in the brains of animals trained on similar tasks.	humans	Jaldert O. Rombouts;Pieter R. Roelfsema;Sander M. Bohte	2012	ERCIM News		information retrieval;data mining;recall;computer science	ML	-0.03208171183978876	-45.70063609392956	97079
96d8ec3c365466fc4f5506f9606fafe9a625e590	performance comparison of five exact graph matching algorithms on biological databases	benchmarking;graph matching;graph methods in bioinformatics	Graphs are a powerful data structure that can be applied to several problems in bioinformatics. Graph matching, in its diverse forms, is an important operation on graphs, involved when there is the need to compare two graphs or to find substructures into larger structures. Many graph matching algorithms exist, and their relative efficiency depends on the kinds of graphs they are applied to. In this paper we will consider some popular and freely available matching algorithms, and will experimentally compare them on graphs derived from bioinformatics applications, in order to help the researchers in this field to choose the right tool for the problem at hand.	algorithm;bioinformatics;data structure;experiment;graph database;graph operations;least absolute deviations;list of biological databases;matching (graph theory);performance evaluation;rs-232;subgraph isomorphism problem	Vincenzo Carletti;Pasquale Foggia;Mario Vento	2013		10.1007/978-3-642-41190-8_44	combinatorics;clique-width;3-dimensional matching;theoretical computer science;machine learning;comparability graph;mathematics;modular decomposition;graph operations;matching;benchmarking	Comp.	1.1646248632653051	-48.574826282772264	97775
ccc9e6935ad5d3ad567b073db121f6e5a06c5546	network-regularized sparse logistic regression models for clinical risk prediction and biomarker discovery	logistics computational modeling training testing biological system modeling cancer mathematical model;feature selection sparse logistic regression network regularized penalty survival risk prediction	Molecular profiling data e.g., gene expression has been used for clinical risk prediction and biomarker discovery. However, it is necessary to integrate other prior knowledge like biological pathways or gene interaction networks to improve the predictive ability and biological interpretability of biomarkers. Here, we first introduce a general regularized Logistic Regression LR framework with regularized term $\lambda \Vert \boldsymbol {w}\Vert _1 + \eta \boldsymbol {w}^T\boldsymbol {M}\boldsymbol {w}$, which can reduce to different penalties, including Lasso, elastic net, and network-regularized terms with different $\boldsymbol {M}$. This framework can be easily solved in a unified manner by a cyclic coordinate descent algorithm which can avoid inverse matrix operation and accelerate the computing speed. However, if those estimated $\boldsymbol {w}_i$ and $\boldsymbol {w}_j$ have opposite signs, then the traditional network-regularized penalty may not perform well. To address it, we introduce a novel network-regularized sparse LR model with a new penalty $\lambda \Vert \boldsymbol {w}\Vert _1 + \eta |\boldsymbol {w}|^T\boldsymbol {M}|\boldsymbol {w}|$ to consider the difference between the absolute values of the coefficients. We develop two efficient algorithms to solve it. Finally, we test our methods and compare them with the related ones using simulated and real data to show their efficiency.	approximation algorithm;arabic numeral 0;biological markers;carcinoma of lung;coefficient;coordinate descent;elastic net regularization;ephrin type-b receptor 1, human;gene expression;gene ontology term enrichment;genes, vif;glioblastoma;gradient descent;interaction network;kegg;lr parser;lasso;mesa;molecular profiling;multinomial logistic regression;newton;newton's method;patients;sparse matrix;staphylococcal protein a;statistical classification;subtype (attribute);synthetic data;unified framework	Wenwen Min;Juan Liu;Shihua Zhang	2018	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2016.2640303	machine learning;artificial intelligence;lasso (statistics);coordinate descent;elastic net regularization;matrix (mathematics);logistic regression;computer science;inverse;absolute value	ML	8.512135361714687	-51.910877770704	97847
b7aee992aeb00f5449183e971430679426e36d03	intrusion detection method using neural networks based on the reduction of characteristics	neural networks;anomaly detection;intrusion detection;spectrum;data model;artificial intelligent;principal component analysis;selection criteria;intrusion detection system;artificial neural network;neural network	The application of techniques based on Artificial Intelligence for intrusion detection systems (IDS), mostly, artificial neural networks (ANN), is becoming a mainstream as well as an extremely effective approach to address some of the current problems in this area. Nevertheless, the selection criteria of the features to be used as inputs for the ANNs remains a problematic issue, which can be put, in a nutshell, as follows: The wider the detection spectrum of selected features is, the lower the performance efficiency of the process becomes and vice versa. This paper proposes sort of a compromise between both ends of the scale: a model based on Principal Component Analysis (PCA) as the chosen algorithm for reducing characteristics in order to maintain the efficiency without hindering the capacity of detection. PCA uses a data model to diminish the size of ANN's input vectors, ensuring a minimum loss of information, and consequently reducing the complexity of the neural classifier as well as maintaining stability in training times. A test scenario for validation purposes was developed, using based-on-ANN IDS. The results obtained based on the tests have demonstrated the validity of the proposal.	artificial neural network;intrusion detection system	Iren Lorenzo-Fonseca;Francisco Maciá Pérez;Francisco José Mora-Gimeno;Rogelio Lau-Fernández;Juan Antonio Gil-Martínez-Abarca;Diego Marcos-Jorquera	2009		10.1007/978-3-642-02478-8_162	anomaly-based intrusion detection system;intrusion detection system;computer science;artificial intelligence;machine learning;data mining;time delay neural network;artificial neural network	ML	9.464479204334916	-38.36456434645796	97945
a674ba5df744d963c13eee74d9eaefd43502518a	a weighted common structure based clustering technique for xml documents	xml clustering;document clustering;frequent pattern;xml mining;data exchange;data mining;semistructured data;xml document;sequential pattern mining	XML has recently become very popular as a means of representing semistructured data and as a standard for data exchange over the Web, because of its varied applicability in numerous applications. Therefore, XML documents constitute an important data mining domain. In this paper, we propose a new method of XML document clustering by a global criterion function, considering the weight of common structures. Our approach initially extracts representative structures of frequent patterns from schemaless XML documents using a sequential pattern mining algorithm. Then, we perform clustering of an XML document by the weight of common structures, without a measure of pairwise similarity, assuming that an XML document is a transaction and frequent structures extracted from documents are items of the transaction. We conducted experiments to compare our method with previous methods. The experimental results show the effectiveness of our approach.		Jeong Hee Hwang;Keun Ho Ryu	2010	Journal of Systems and Software	10.1016/j.jss.2010.02.004	well-formed document;xml catalog;data exchange;sequential pattern mining;xml validation;binary xml;simple api for xml;xml;document clustering;xml schema;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;information retrieval;efficient xml interchange	SE	-3.914603791554551	-43.63067270592939	97978
4c8a4b5490aee08fdbe995cbe74f00833d141e5e	aggregate distance based clustering using fibonacci series-fibclus	empirical analysis;fibonacci series and golden ratio;golden ratio;k means;data type;evaluation metric;fibonacci number;expectation maximization;clustering;similarity evaluation;numeric darasets;clustering numeric;categorical and mix datasets	This paper proposes an innovative instance similarity based evaluation metric that reduces the search map for clustering to be performed. An aggregate global score is calculated for each instance using the novel idea of Fibonacci series. The use of Fibonacci numbers is able to separate the instances effectively and, in hence, the intra-cluster similarity is increased and the intercluster similarity is decreased during clustering. The proposed FIBCLUS algorithm is able to handle datasets with numerical, categorical and a mix of both types of attributes. Results obtained with FIBCLUS are compared with the results of existing algorithms such as k-means, x-means expected maximization and hierarchical algorithms that are widely used to cluster numeric, categorical and mix data types. Empirical analysis shows that FIBCLUS is able to produce better clustering solutions in terms of entropy, purity and F-score in comparison to the above described existing algorithms.	aggregate data;aggregate function;cluster analysis;expectation–maximization algorithm;k-means clustering;numerical analysis;online and offline;overhead (computing);purity (quantum mechanics)	Rakesh Rawat;Richi Nayak;Yuefeng Li;Slah Alsaleh	2011		10.1007/978-3-642-20291-9_6	correlation clustering;fibonacci number;k-medians clustering;expectation–maximization algorithm;fuzzy clustering;data type;computer science;machine learning;data mining;cluster analysis;single-linkage clustering;golden ratio;statistics;k-means clustering	AI	1.644765582432833	-40.54424392162564	98036
7e6d52537004c895ef47ab0e1d9204b2003ef667	a novel aco-ga hybrid algorithm for feature selection in protein function prediction	high dimensionality;ant colony optimization;protein sequence;enzyme;feature selection fs;feature vector;hierarchical classification;protein function prediction;computational complexity;functional genomics;prediction accuracy;genetic algorithm;feature selection;ant colony optimization aco;hybrid algorithm;genetic algorithm ga;bioinformatics	Protein function prediction is an important problem in functional genomics. Typically, protein sequences are represented by feature vectors. A major problem of protein datasets that increase the complexity of classification models is their large number of features. Feature selection (FS) techniques are used to deal with this high dimensional space of features. In this paper, we propose a novel feature selection algorithm that combines genetic algorithms (GA) and ant colony optimization (ACO) for faster and better search capability. The hybrid algorithm makes use of advantages of both ACO and GA methods. Proposed algorithm is easily implemented and because of use of a simple classifier in that, its computational complexity is very low. The performance of proposed algorithm is compared to the performance of two prominent population-based algorithms, ACO and genetic algorithms. Experimentation is carried out using two challenging biological datasets, involving the hierarchical functional classification of GPCRs and enzymes. The criteria used for comparison are maximizing predictive accuracy, and finding the smallest subset of features. The results of experiments indicate the superiority of proposed algorithm.	feature selection;hybrid algorithm;protein function prediction;software release life cycle	Shahla Nemati;Mohammad Ehsan Basiri;Nasser Ghasem-Aghaee;Mehdi Hosseinzadeh Aghdam	2009	Expert Syst. Appl.	10.1016/j.eswa.2009.04.023	functional genomics;enzyme;ant colony optimization algorithms;genetic algorithm;feature vector;hybrid algorithm;computer science;bioinformatics;machine learning;protein sequencing;pattern recognition;protein function prediction;computational complexity theory;feature selection	AI	8.815783360924776	-46.852186361585304	98116
971c800751eeb569832839e515972296981cc62b	ranking evaluation functions to improve genetic feature selection in content-based image retrieval of mammograms	image retrieval content based retrieval algorithm design and analysis information retrieval genetic algorithms biomedical imaging medical diagnostic imaging breast cancer breast tissue performance analysis;evaluation function;decision tree;information retrieval system;cancer;breast tissue density analysis;training;naive bayes;biological organs;tumours;breast cancer diagnosis;data mining;genetics;tumours biological organs cancer diagnostic radiography feature extraction genetic algorithms image retrieval mammography medical image processing;association rule mining;accuracy;biological cells;feature extraction;medical image processing;evaluation criteria;genetic feature selection;nearest neighbor;genetic algorithm;genetic algorithms;feature selection;process improvement;support vector machine;query answering;mammography;ranking evaluation function;content based image retrieval;fitness function mammograms content based image retrieval ranking evaluation function genetic feature selection genetic algorithm breast cancer diagnosis breast tissue density analysis;mammograms;diagnostic radiography;fitness function;gallium;image retrieval	The ranking problem is a crucial task in the information retrieval systems. In this paper, we take advantage of single valued ranking evaluation functions in order to develop a new method of genetic feature selection tailored to improve the accuracy of content-based image retrieval systems. We propose to boost the feature selection ability of the genetic algorithms (GA) by employing an evaluation criteria (fitness function) that relies on order-based ranking evaluation functions. The evaluation criteria are provided by the GA and has been successfully employed as a measure to evaluate the efficacy of content-based image retrieval process, improving up to 22% the precision of the query answers. Experiments on three medical datasets containing breast cancer diagnosis and breast tissue density analysis showed that fitness functions based on ranking evaluation functions occupy an essential role on the algorithms' performance, obtaining results significatively better than other fitness function designs. The experiments also showed that the proposed method obtains results superior than feature selection based on the traditional decision-tree C4.5, naive bayes, support vector machine, 1-nearest neighbor and association rule mining.	association rule learning;c4.5 algorithm;content-based image retrieval;decision tree;emoticon;evaluation function;experiment;feature selection;feature vector;fitness function;formal concept analysis;genetic algorithm;information retrieval;naive bayes classifier;software release life cycle;support vector machine	Sérgio Francisco da Silva;Agma J. M. Traina;Marcela Xavier Ribeiro;João Batista Neto;Caetano Traina	2009	2009 22nd IEEE International Symposium on Computer-Based Medical Systems	10.1109/CBMS.2009.5255397	ranking;genetic algorithm;image retrieval;computer science;machine learning;pattern recognition;data mining;feature selection;ranking svm;information retrieval	Vision	8.173489682645847	-44.84805371042501	98201
38139a1925f92500af057115ab51090901801d72	analyzing high-dimensional data by subspace validity	visual databases statistical testing feature extraction image segmentation;image segmentation;real data sets high dimensional data analysis arbitrary shaped projected clusters noise levels subspace validity statistical tests;statistical test;feature extraction;high dimensional data;data analysis space technology testing noise level topology humans clustering algorithms computer science information analysis automation;statistical testing;inproceedings;visual databases	We are proposing a novel method that makes it possible to analyze high dimensional data with arbitrary shaped projected clusters and high noise levels. At the core of our method lies the idea of subspace validity. We map the data in a way that allows us to test the quality of subspaces using statistical tests. Experimental results, both on synthetic and real data sets, demonstrate the potential of our method.	computer cluster;data mining;norm (social);synthetic intelligence	Amihood Amir;Reuven Kashi;Nathan S. Netanyahu;Daniel A. Keim;Markus Wawryniuk	2003		10.1109/ICDM.2003.1250955	statistical hypothesis testing;computer science;machine learning;pattern recognition;data mining;statistics	ML	1.4715140293827729	-40.47450805025046	98232
22c96de5f63370f6b2a9a0b29a1e4a173574988f	artificial data generation for one-class classification - a case study of dimensionality reduction for text and biological data	one class classification;biological data;dimensional reduction	Artificial negatives have been employed in a variety of contexts in machine learning to overcome data availability problems. In this paper we explore the use of artificial negatives for dimension reduction in one-class classification, that is classification problems where only positive examples are available for training. We present four different strategies for generating artificial negatives and show that two of these strategies are very effective for discovering discriminating projections on the data, i.e., low dimension projections for discriminating between positive and real negative examples. The paper concludes with an assessment of the selection bias of this approach to dimension reduction for one-class classification.	artificial intelligence;dimensionality reduction;machine learning;one-class classification;selection bias	Santiago D. Villalba;Padraig Cunningham	2009			biological data;computer science;machine learning;pattern recognition;data mining;one-class classification	AI	8.764001102333436	-45.628386874648896	98250
c87b8c5794e812e90c66cc271544a590e2c40326	an extended validity index for identifying community structure in networks	complex network;k means;dissimilarity index;prior knowledge;simulated annealing;community structure;indexation;euclidean space;validity index	To find the best partition of a large and complex network into a small number of communities has been addressed in many different ways. In this paper, a new validity index for network partition is proposed, which is motivated by the construction of Xie-Beni index in Euclidean space. The simulated annealing strategy is used to minimize this extended validity index, associating with a dissimilarity-index-based k-means iterative procedure, under the framework of a random walker Markovian dynamics on the network. The proposed algorithm(SAEVI) can efficiently and automatically identify the community structure of the network and determine an appropriate number of communities without any prior knowledge about the community structure during the cooling process. The computational results on several artificial and real-world networks confirm the capability of the algorithm.		Jian Liu	2010		10.1007/978-3-642-13318-3_33	index of dissimilarity;simulated annealing;computer science;euclidean space;machine learning;pattern recognition;data mining;mathematics;community structure;complex network;k-means clustering	ML	1.5168401034517989	-42.12171505822336	98443
41464a8745c343936a904e68a9999f3e08515a8e	the impact of basic matrix dimension on the performance of algorithms for computing typical testors		Within Testor Theory, typical testors are irreducible subsets of attributes preserving the object discernibility ability of the original set of attributes. Computing all typical testors from a dataset has exponential complexity regarding its number of attributes, however there are other properties of a dataset that have some influence on the performance of different algorithms. Previous studies have determined that a significant runtime reduction can be obtained from selecting the appropriate algorithm for a given dataset. In this work, we present an experimental study evaluating the effect of basic matrix dimensionality on the performance of the algorithms for typical testor computation. Our experiments are carried out over synthetic and real–world datasets. Finally, some guidelines obtained from the experiments, for helping to select the best algorithm for a given dataset, are summarised.		Vladimir Rodriguez;José Fco. Martínez-Trinidad;Jesús Ariel Carrasco-Ochoa;Manuel Lazo-Cortés	2018		10.1007/978-3-319-92198-3_5	computer science;machine learning;artificial intelligence;reduct;computation;curse of dimensionality;algorithm;matrix (mathematics);exponential function	HPC	7.732868095705094	-41.96466413068449	98511
587f2896d9c65a7a5494bb5ebd56ee67dc720aa7	cancer informatics by prototype networks in mass spectrometry	confidence estimation;cancer informatics;mass spectrometry;clinical proteomics;prototype classifiers	OBJECTIVE Mass spectrometry has become a standard technique to analyze clinical samples in cancer research. The obtained spectrometric measurements reveal a lot of information of the clinical sample at the peptide and protein level. The spectra are high dimensional and, due to the small number of samples a sparse coverage of the population is very common. In clinical research the calculation and evaluation of classification models is important. For classical statistics this is achieved by hypothesis testing with respect to a chosen level of confidence. In clinical proteomics the application of statistical tests is limited due to the small number of samples and the high dimensionality of the data. Typically soft methods from the field of machine learning are used to generate such models. However for these methods no or only few additional information about the safety of the model decision is available. In this contribution the spectral data are processed as functional data and conformal classifier models are generated. The obtained models allow the detection of potential biomarker candidates and provide confidence measures for the classification decision.   METHODS First, wavelet-based techniques for the efficient processing and encoding of mass spectrometric measurements from clinical samples are presented. A prototype-based classifier is extended by a functional metric and combined with the concept of conformal prediction to classify the clinical proteomic spectra and to evaluate the results.   RESULTS Clinical proteomic data of a colorectal cancer and a lung cancer study are used to test the performance of the proposed algorithm. The prototype classifiers are evaluated with respect to prediction accuracy and the confidence of the classification decisions. The adapted metric parameters are analyzed and interpreted to find potential biomarker candidates.   CONCLUSIONS The proposed algorithm can be used to analyze functional data as obtained from clinical mass spectrometry, to find discriminating mass positions and to judge the confidence of the obtained classifications, providing robust and interpretable classification models.	algorithm;biological markers;carcinoma of lung;classification;consistency model;embedded system;embedding;function representation;informatics (discipline);machine learning;mass effect trilogy;non-small cell lung carcinoma;overfitting;preparation;preprocessor;proteomics;prototype;sparse matrix;spectrometry;statistical test;wavelet	Frank-Michael Schleif;Thomas Villmann;Markus Kostrzewa;Barbara Hammer;Alexander Gammerman	2009	Artificial intelligence in medicine	10.1016/j.artmed.2008.07.018	mass spectrometry;computer science;bioinformatics;machine learning;data mining;statistics	ML	9.014391518116108	-50.09340696749963	98735
2d26d6dbd10d8a98f29babe24e1e5a04ce544e5d	constructing (almost) phylogenetic trees from developmental sequences data	genetique;arbre phylogenetique;analyse amas;consensus;analisis datos;genetica;data mining;classification;arbol filogenetico;genetics;data analysis;cluster analysis;phylogenetic tree;fouille donnee;consenso;decouverte connaissance;descubrimiento conocimiento;analyse donnee;analisis cluster;busca dato;clasificacion;knowledge discovery	In this paper we present a new way of constructing almost phylogenetic trees. Almost since we reconstruct the tree, but without the timestamps. Rather than basing the tree on genetic sequence data ours is based on developmental sequence data. Using frequent episode discovery and clustering we reconstruct the consensus tree from the literature almost completely.	phylogenesis	Ronnie Bathoorn;Arno Siebes	2004		10.1007/978-3-540-30116-5_46	phylogenetic tree;consensus;biological classification;computer science;bioinformatics;data mining;knowledge extraction;cluster analysis;data analysis;algorithm	Theory	0.9952575356416397	-46.418442111839994	98885
f2c1f1bad80daa45d9b1796c6403b1bb757d53d0	a hybrid som-svm method for analyzing zebra fish gene expression	microarray technology;cellular biophysics;genetics;certain complex biological problem;pattern classification;gene expression;analysis process;hybrid som-svm method;biology computing;molecular biophysics;zebra fish gene expression;support vector machine;molecular biology research;global gene expression analysis;zebra fish gene;self-organising feature maps;expression data;classification performance;analyzing zebra fish;dna;support vector machines;data filtering tool;self-organizing maps;machine learning;molecular biology	Microarray technology can be employed to quantitatively measure the expression of thousands of genes in a single experiment. It has become one of the main tools for global gene expression analysis in molecular biology research in recent years. The large amount of expression data generated by this technology makes the study of certain complex biological problems possible, and machine learning methods are expected to play a crucial role in the analysis process. We present our results from integrating a self-organizing maps (SOM) and a support vector machine (SVM) for the analysis of the various functions of zebra fish genes based on their expression. We discuss how SOM can be used as a data-filtering tool to improve the classification performance of the SVM on this data set.	dna microarray;machine learning;organizing (structure);self-organization;self-organizing map;support vector machine	Wu Wei;Liu Xin;Xu Min;Peng Jinrong;Rudy Setiono	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334191	support vector machine;computer science;bioinformatics;machine learning;data mining;molecular biophysics	HPC	6.857014654660914	-48.37405549048261	98944
e13fbb37cedf5ff9faca8a1d4ae7c8c7982c6dca	$f$-information measures for efficient selection of discriminative genes from microarray data	microarray data;support vector machines bayes methods bioinformatics cancer feature extraction genetics information theory molecular biophysics;diagnostic test;microarray gene expression data;performance evaluation;support vector machines;cancer;bayes methods;distributed computing;colon;testing;gene expression data;classification;genetics;gene selection problem;colon cancer;leukemia;gene expression;naive bayes classifier;gene expression profiling models genetic oligonucleotide array sequence analysis;accuracy;microarray analysis;models genetic;k nearest neighbor rule;mutual information gene expression accuracy breast cancer colon performance evaluation testing information theory genetic communication distributed computing;feature extraction;evaluation criteria;mutual information classification feature selection gene selection microarray analysis;f information measure;molecular biophysics;prediction accuracy;mutual information;k nearest neighbor;gene class relevance;colon cancer f information measure microarray gene expression data information theory gene selection problem gene gene redundancy gene class relevance naive bayes classifier k nearest neighbor rule support vector machine mutual information breast cancer leukemia;feature selection;gene gene redundancy;support vector machine;gene selection;breast cancer;gene expression profiling;information theory;oligonucleotide array sequence analysis;bioinformatics;genetic communication	Among the great amount of genes presented in microarray gene expression data, only a small fraction is effective for performing a certain diagnostic test. In this regard, mutual information has been shown to be successful for selecting a set of relevant and nonredundant genes from microarray data. However, information theory offers many more measures such as the f-information measures that may be suitable for selection of genes from microarray gene expression data. This paper presents different f-information measures as the evaluation criteria for gene selection problem. To compute the gene-gene redundancy (respectively, gene-class relevance), these information measures calculate the divergence of the joint distribution of two genes' expression values (respectively, the expression values of a gene and the class labels of samples) from the joint distribution when two genes (respectively, the gene and class label) are considered to be completely independent. The performance of different f-information measures is compared with that of the mutual information based on the predictive accuracy of naive Bayes classifier, K -nearest neighbor rule, and support vector machine. An important finding is that some f-information measures are shown to be effective for selecting relevant and nonredundant genes from microarray data. The effectiveness of different f-information measures, along with a comparison with mutual information, is demonstrated on breast cancer, leukemia, and colon cancer datasets. While some f -information measures provide 100% prediction accuracy for all three microarray datasets, mutual information attains this accuracy only for breast cancer dataset, and 98.6% and 93.6% for leukemia and colon cancer datasets, respectively.	colon carcinoma;colon classification;diagnostic tests;gene expression;genes, vif;genetic selection;information theory;interaction information;k-nearest neighbors algorithm;mammary neoplasms;microarray;mutual information;naive bayes classifier;relevance;rule 90;selection algorithm;silo (dataset);support vector machine;support vector machine;leukemia;multiplicity	Pradipta Maji	2009	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2008.2004502	biology;support vector machine;microarray analysis techniques;information theory;computer science;bioinformatics;machine learning;pattern recognition;data mining;feature selection;molecular biophysics	ML	7.6670704833902805	-48.92909676328322	98953
01e1195fa475fe5d428602d71cc4fb50b6e89a11	clustering algorithm for network constraint trajectories	moving object;cluster algorithm;road traffic;moving object database;spatial database;spatial data mining;clustering method;similarity measure	Spatial data mining is an active topic in spatial databases. This paper proposes a new clustering method for moving object trajectories databases. It applies specifically to trajectories that only lie on a predefined network. The proposed algorithm (NETSCAN) is inspired from the wellknown density based algorithms. However, it takes advantage of the network constraint to estimate the object density. Indeed, NETSCAN first computes dense paths in the network based on the moving object count, then, it clusters the sub-trajectories which are similar to the dense paths. The user can adjust the clustering result by setting a density threshold for the dense paths, and a similarity threshold within the clusters. This paper describes the proposed method. An implementation is reported, along with experimental results that show the effectiveness of our approach and the flexibility allowed by the user parameters.	algorithm;cluster analysis;data mining;online analytical processing;sensor;spatial database;stochastic matrix	Ahmed Kharrat;Iulian Sandu Popa;Karine Zeitouni;Sami Faïz	2008		10.1007/978-3-540-68566-1_36	correlation clustering;data stream clustering;object-based spatial database;k-medians clustering;fuzzy clustering;canopy clustering algorithm;pattern recognition;cure data clustering algorithm;data mining;database;cluster analysis	DB	-0.10949725029117063	-40.16938787659746	99150
df1974eaa489c57fa418d0090b03ea4c0a8fc179	syfsel: generating synthetic fuzzy sets made simple		Empirical tests can help determine if methods developed for fuzzy sets work correctly. However, finding a large enough data set with suitable properties to conduct thorough tests can be challenging. This paper presents a new library named SyFSeL (Synthetic Fuzzy Set Library) which automatically generates synthetic fuzzy sets with specified characteristics and fuzzy set type. SyFSeL generates as many sets as desired, with adjustable parameters to enable users to emulate real data. Generated fuzzy sets are exported so users can import them into their own fuzzy systems software. SyFSeL can also create graphical plots of the generated sets, examples of which are shown in this paper. The library is cross-platform and open-source under the GNU General Public License, and users are free to develop upon and adapt the code. However, SyFSeL has been designed so that no understanding of the code is required to use it.		Josie McCulloch	2018	2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2018.8491549	license;machine learning;fuzzy logic;software;artificial intelligence;fuzzy set;fuzzy control system;python (programming language);computer science	Robotics	5.98080453513276	-44.516746794181934	99330
59d7bf3064b9abf0b5e2e99c395b2a6fde11bd1b	classification of heterogeneous microarray data by maximum entropy kernel	microarray data;sensitivity and specificity;oral cavity;heterogeneous data;computational biology bioinformatics;reproducibility of results;prediction accuracy;algorithms;entropy;missing values;support vector machine;combinatorial libraries;computer appl in life sciences;information storage and retrieval;maximum entropy;gene expression profiling;oligonucleotide array sequence analysis;databases protein;microarrays;bioinformatics	There is a large amount of microarray data accumulating in public databases, providing various data waiting to be analyzed jointly. Powerful kernel-based methods are commonly used in microarray analyses with support vector machines (SVMs) to approach a wide range of classification problems. However, the standard vectorial data kernel family (linear, RBF, etc.) that takes vectorial data as input, often fails in prediction if the data come from different platforms or laboratories, due to the low gene overlaps or consistencies between the different datasets. We introduce a new type of kernel called maximum entropy (ME) kernel, which has no pre-defined function but is generated by kernel entropy maximization with sample distance matrices as constraints, into the field of SVM classification of microarray data. We assessed the performance of the ME kernel with three different data: heterogeneous kidney carcinoma, noise-introduced leukemia, and heterogeneous oral cavity carcinoma metastasis data. The results clearly show that the ME kernel is very robust for heterogeneous data containing missing values and high-noise, and gives higher prediction accuracies than the standard kernels, namely, linear, polynomial and RBF. The results demonstrate its utility in effectively analyzing promiscuous microarray data of rare specimens, e.g., minor diseases or species, that present difficulty in compiling homogeneous data in a single laboratory.	cavity quantum electrodynamics;compiler;dental caries;entropy maximization;genetic heterogeneity;kernel (operating system);laboratory;leukemia, b-cell;microarray;missing data;new type;oral cavity carcinoma;polynomial;principle of maximum entropy;published database;radial basis function;renal carcinoma;secondary carcinoma;specimen;support vector machine;leukemia	Wataru Fujibuchi;Tsuyoshi Kato	2007	BMC Bioinformatics	10.1186/1471-2105-8-267	biology;support vector machine;microarray analysis techniques;entropy;gene chip analysis;dna microarray;radial basis function kernel;computer science;bioinformatics;principle of maximum entropy;data science;data mining;gene expression profiling	ML	8.225681605676526	-50.83161293474146	99525
cb83268b18e2ffa6fcc518d3a874006f0d9d4b51	a robust neural network classifier to model the compressive strength of high performance concrete using feature subset selection	data mining;classifiers;concrete and compressive strength prediction;knowledge discovery	"""High performance concrete (HPC) is a mixture of cement, fine aggregate, coarse aggregate, water and other ingredients. Modeling the compressive strength of HPC (or concrete strength) is a difficult task in building materials because it is influenced by the proportions of various ingredients within the HPC. Researchers have tried to confront this difficulty by modeling the strength of concrete using artificial neural networks (ANN). The influence of ingredients on concrete strength still remains unknown due to the """"black box"""" nature of ANN. This paper investigates the influence of ingredients on concrete strength modeling using feature selection. A robust ANN-model using the knowledge from feature selection has been developed to predict the strength effectively. Experimental results have shown that the proposed model is efficient with respect to runtime and prediction."""	aggregate data;artificial neural network;black box;feature selection;mathematical model	M. Venu;R. Uday Kiran;R. Kiranmai	2012		10.1145/2459118.2459119	computer science;data mining;knowledge extraction	ML	9.893181034244671	-38.691487894739325	100161
87b1f41d5ec0143035bb713e65a71f5192c7fb5b	quality control for crowdsourced hierarchical classification	labeling crowdsourcing probabilistic logic pharmaceuticals reliability electronic mail quality control;pharmaceuticals;crowdsoucring;reliability;electronic mail;quality control method hierarchical relationship multiclass classification hierarchical structure worker abilities item response theory worker response model crowdsourced hierarchical classification tasks label aggregation method accuracy;hierarchical classification;hierarchical classification crowdsoucring quality control;probabilistic logic;quality control;crowdsourcing;quality control pattern classification;labeling	Repeated labeling is a widely adopted quality control method in crowdsourcing. This method is based on selecting one reliable label from multiple labels collected by workers because a single label from only one worker has a wide variance of accuracy. Hierarchical classification, where each class has a hierarchical relationship, is a typical task in crowdsourcing. However, direct applications of existing methods designed for multi-class classification have the disadvantage of discriminating among a large number of classes. In this paper, we propose a label aggregation method for hierarchical classification tasks. Our method takes the hierarchical structure into account to handle a large number of classes and estimate worker abilities more precisely. Our method is inspired by the steps model based on item response theory, which models responses of examinees to sequentially dependent questions. We considered hierarchical classification to be a question consisting of a sequence of subquestions and built a worker response model for hierarchical classification. We conducted experiments using real crowdsourced hierarchical classification tasks and demonstrated the benefit of incorporating a hierarchical structure to improve the label aggregation accuracy.	crowdsourcing;experiment;hierarchical database model;item response theory;multiclass classification	Naoki Otani;Yukino Baba;Hisashi Kashima	2015	2015 IEEE International Conference on Data Mining	10.1109/ICDM.2015.83	quality control;labeling theory;computer science;data science;machine learning;data mining;reliability;probabilistic logic;crowdsourcing;statistics	Vision	5.664709746177583	-39.52561289314894	100462
3b1c3ecab088161383d0a24b5abe50130071118b	fuzzy equivalence relation clustering with transitive closure, transitive opening and the optimal transitive approximation	fuzzy equivalence relation;numerical experiment fuzzy equivalence relation clustering transitive closure transitive opening transitive lower approximation suboptimal transitive approximation fuzzy similarity relation agglomerative hierarchical clustering fuzzy c means clustering;transive closure;pattern clustering equivalence classes fuzzy set theory;approximation algorithms;the optimal transitive approximation;会议论文;transitive opening;fuzzy c means clustering clustering fuzzy equivalence relation transive closure transitive opening the optimal transitive approximation;clustering;classification algorithms;clustering algorithms;approximation methods;approximation methods clustering algorithms approximation algorithms clustering methods classification algorithms educational institutions partitioning algorithms;fuzzy c means clustering;clustering methods;partitioning algorithms	In this paper, the fuzzy equivalence relation clustering is studied. Firstly, an algorithm for obtaining a transitive lower approximation or a suboptimal transitive approximation of a fuzzy similarity relation is given. Secondly, the relationship between fuzzy equivalence relation clustering and the agglomerative hierarchical clustering is investigated. Thirdly, a clustering method combining fuzzy equivalence relation clustering and fuzzy c-means clustering is proposed. Finally, numerical experiments are carried out to verify the algorithm.	algorithm;approximation;cluster analysis;experiment;fuzzy clustering;fuzzy cognitive map;fuzzy set;hierarchical clustering;numerical analysis;transitive closure;turing completeness	Yanli Jiang;Guannan Deng	2013	2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2013.6816241	transitive reduction;statistical classification;correlation clustering;combinatorics;discrete mathematics;fuzzy clustering;flame clustering;computer science;machine learning;preorder;mathematics;cluster analysis;transitive relation;equivalence relation;fuzzy set operations;approximation algorithm	Robotics	2.4346700534225025	-39.31100371850742	100511
6d733eb9ab4dd196563e83e0fe1725dda76cb3a1	efficient rna structure comparison algorithms	rna substructure;multiple rna structure comparison;suffix array algorithm	Recently proposed relative addressing-based ([Formula: see text]) RNA secondary structure representation has important features by which an RNA structure database can be stored into a suffix array. A fast substructure search algorithm has been proposed based on binary search on this suffix array. Using this substructure search algorithm, we present a fast algorithm that finds the largest common substructure of given multiple RNA structures in [Formula: see text] format. The multiple RNA structure comparison problem is NP-hard in its general formulation. We introduced a new problem for comparing multiple RNA structures. This problem has more strict similarity definition and objective, and we propose an algorithm that solves this problem efficiently. We also develop another comparison algorithm that iteratively calls this algorithm to locate nonoverlapping large common substructures in compared RNAs. With the new resulting tools, we improved the RNASSAC website (linked from http://faculty.tamuc.edu/aarslan ). This website now also includes two drawing tools: one specialized for preparing RNA substructures that can be used as input by the search tool, and another one for automatically drawing the entire RNA structure from a given structure sequence.		Abdullah N. Arslan;Jithendar Anandan;Eric Fry;Keith Monschke;Nitin Ganneboina;Jason Bowerman	2017	Journal of bioinformatics and computational biology	10.1142/S0219720017400091	substructure;artificial intelligence;binary search algorithm;machine learning;rna;compressed suffix array;nucleic acid structure;bioinformatics;algorithm;nucleic acid secondary structure;search algorithm;mathematics;suffix array	Comp.	-1.0154015954703193	-50.84071334159385	100584
7ee67bc04b23f861dcaab0ee1be7adb65d524c06	a multidimensional genetic programming approach for identifying epsistatic gene interactions		We propose a novel methodology for binary and multiclass classification that uses genetic programming to construct features for a nearest centroid classifier. The method, coined M4GP, improves upon earlier approaches in this vein (M2GP and M3GP) by simplifying the program encoding, using advanced selection methods, and archiving solutions during the run. In our recent paper, we test this stategy against traditional GP formulations of the classification problem, showing that this framework outperforms boolean and floating point encodings. In comparison to several machine learning techniques, M4GP achieves the best overall ranking on benchmark problems. We then compare our algorithm against state-ofthe-art machine learning approaches to the task of disease classification using simulated genetics datasets with up to 5000 features. The results suggest that our proposed approach performs on par with the best results in literature with less computation time, while producing simpler models.	algorithm;archive;benchmark (computing);bitwise operation;computation;genetic programming;interaction;machine learning;multiclass classification;nearest centroid classifier;time complexity	William La Cava;Sara Silva;Kourosh Danai;Lee Spector;Leonardo Vanneschi;Jason H. Moore	2018		10.1145/3205651.3208217	artificial intelligence;boolean algebra;computer science;machine learning;nearest centroid classifier;encoding (memory);genetic programming;ranking;binary number;floating point;multiclass classification	AI	9.65394628178055	-44.09876993810136	100601
80726f2786a81f26ba404dc423b234c9eedab9bb	a modified possibilistic fuzzy c-means clustering algorithm	multiscale structure possibilistic clustering fuzzy clustering mean shift;possibilistic clustering;mean shift;fuzzy clustering;clustering algorithms partitioning algorithms algorithm design and analysis phase change materials bandwidth complexity theory sensitivity;multiscale structure;mpfcm modified possibilistic fuzzy c means clustering algorithm possibilistic partition data set mean shift clustering algorithm msc cluster number;possibility theory fuzzy set theory pattern clustering	Possibilistic clustering algorithm can give the fuzzy and possibilistic partition of the data set. This paper analyzes the mean shift clustering algorithm (MSC) and the possibilistic fuzzy c-means clustering algorithm (PFCM) in detail, base on which a modified possibilistic fuzzy c-means clustering algorithm (MPFCM) is proposed. The analysis shows that PFCM has the initialization sensitivity problems, while MSC can determine the cluster number in different scales and it is independent to the initializations. MPFCM not only inherits the merit of both the PFCM and MSC, but also avoids the problems from them. The experimental results show the relatively better performance of the proposed algorithm on computation and initialization.	algorithm;cluster analysis;computation;mean shift	Fuheng Qu;Yating Hu;Yaohong Xue;Yong Yang	2013	2013 Ninth International Conference on Natural Computation (ICNC)	10.1109/ICNC.2013.6818096	correlation clustering;mathematical optimization;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;dbscan;clustering high-dimensional data	Robotics	2.9543120297077112	-39.40474989224138	100676
c02824471c08cc02726500c58888ab516223e0dc	normalized information distance is not semicomputable	data mining;article letter to editor;data analysis;kolmogorov complexity;computational complexity;pattern recognition;similarity measure	Normalized information distance (NID) uses the theoretica l notion of Kolmogorov complexity, which for practical purposes is approximated by the length of the c ompressed version of the file involved, using a real-world compression program. This practical applicat ion is called ‘normalized compression distance’ and it is trivially computable. It is a parameter-free simil arity measure based on compression, and is used in pattern recognition, data mining, phylogeny, clusterin g, and classification. The complexity properties of its theoretical precursor, the NID, have been open. We show t at the NID is neither upper semicomputable nor lower semicomputable. Index Terms — Normalized information distance, Kolmogorov complexity , semicomputability.	approximation algorithm;computable function;data compression;data mining;kolmogorov complexity;network interface device;pattern recognition;phylogenetics;statistical classification	Sebastiaan Terwijn;Leen Torenvliet;Paul M. B. Vitányi	2010	CoRR		computer science;machine learning;pattern recognition;normalized compression distance;data mining;mathematics;data analysis;computational complexity theory;statistics	ML	-0.9689277693359074	-47.571125780635334	100726
2dc6b4a1fc5d9454e734fbf73001f872d76c6883	binary fish school search applied to feature selection: application to icu readmissions	marine animals educational institutions frequency selective surfaces encoding vectors convergence benchmark testing;search problems feature selection hospitals;intensive care unit readmission problem binary fish school search novel feature selection approach icu readmissions premature convergence population based optimization algorithm binary encoding scheme bfss fuzzy modeling wrapper approach	This paper proposes a novel feature selection approach formulated based on the Fish School Search (FSS) optimization algorithm, intended to cope with premature convergence. In order to use this population based optimization algorithm in feature selection problems, we propose the use of a binary encoding scheme for the internal mechanisms of the fish school search, emerging the binary fish school search (BFSS). The suggested algorithm was combined with fuzzy modeling in a wrapper approach for Feature Selection (FS) and tested over three benchmark databases. This hybrid proposal was applied to an ICU (Intensive Care Unit) readmission problem. The purpose of this application was to predict the readmission of ICU patients within 24 to 72 hours after being discharged. We assessed the experimental results in terms of performance measures and the number of features selected by each used FS algorithms. We observed that our proposal can correctly select the discriminating input features.	algorithm;benchmark (computing);binary file;database;feature selection;fish school search;flying-spot scanner;international components for unicode;line code;mathematical optimization;premature convergence	Joao A. G. Sargo;Susana M. Vieira;João Miguel da Costa Sousa;Carmelo J. A. Bastos Filho	2014	2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2014.6891802	machine learning;data mining	Robotics	9.653870215694855	-43.061364429002936	100854
5b7906faa2102bceb4e78cf97964f527f72d17ed	fuzzy clustering of macroarray data	dna array;gene expression;fuzzy clustering;bacterial genome;gene function	The complete sequence of bacterial genomes provides new perspectives for the study of gene expression and gene function. DNA array experiments allow measuring the expression levels for all genes of an organism in a single hybridization experiment. Computational analysis of the macroarray data is used extensively to extract groups of similarly expressed genes. The aim is to organize DNA array data so that the underlying structures can be recognized and explored. The gene groups identified as clusters are searched for genes known to be involved in similar biological processes, implying that genes of unknown functions may be involved in the same processes. Commonly used computational techniques include hierarchical clustering, K-means clustering and self-organizing maps. These share many features, particularly the distance metric, which measures the relationship between samples or genes in the data space formed by the expression values. The output from different clustering algorithms usually depends more on the type of distance metric used than on any other factor. The central limitation of most of the commonly used algorithms is that they are unable to identify genes whose expression is similar to multiple, distinct gene groups, thereby masking the relationships between genes that are coregulated with different groups of genes in response to different conditions. For example, the K-means clustering partitions genes into a defined set of discrete clusters, attempting to maximize the expression similarity of the genes in each cluster assigning each gene to only one cluster, obscuring the relationship between the conditionally coregulated genes. The recently implemented heuristic variant of Fuzzy C-Means (FCM) clustering [4] shows the advantage		Olga Georgieva;Frank Klawonn;Elisabeth Härtig	2004		10.1007/3-540-31182-3_8	molecular biology;bioinformatics;genetics	Comp.	3.731299146165729	-49.72027841452398	100892
3a7cc35fd38862f3a3b2959a57bb63ece127d126	mean-entropy discretized features are effective for classifying high-dimensional bio-medical data	high dimensionality;gene expression data;feature selection;proteomic profiling data	This paperstudiesanempiricalfeatureselectionheuristicsfor classifying high-dimensional bio-medicaldata.A feature’ s discriminatingpower can be measuredby its entropy value.Basedon this idea,we do not considerthose featuresthatareignoredby theentropy idea.Sucha selectioncanusuallyreduce thedimensionalityof thedataby 90–95%. Thenwe ranktheremainingfeatures, andselectfeatureswhoseentropy is smallerthantheaverageof all theremaining features’entropies.This roundof selectioncanusuallyfurtherreducetwo thirds of thefeatures.So,wecanachieve areductionfrom tensof thousandsof features to only hundredsof importantfeatures.Furthermore,we alsoobserve that learning algorithms,including our new tree-committeeclassifier , generallyimprove their accuracy afterthefeatureselection.This heuristicsappearsto bemoresystematic than the prevailing useof specificnumbers of top-ranked featuresfor classification.	adobe creative suite;algorithm;british informatics olympiad;canonical account;discretization;feature selection;machine learning	Jinyan Li;Huiqing Liu;Limsoon Wong	2003			computer science;machine learning;pattern recognition;data mining;feature selection	ML	6.998726826415006	-47.974425995002065	100938
238023d007bb70c749c82140e81c3d06cf3af7e1	clustering validity checking methods: part ii	unsupervised learning;clustering validation;pattern discovery;pattern recognition;cluster validity	Clustering results validation is an important topic in the context of pattern recognition. We review approaches and systems in this context. In the first part of this paper we presented clustering validity checking approaches based on internal and external criteria. In the second, current part, we present a review of clustering validity approaches based on relative criteria. Also we discuss the results of an experimental study based on widely known validity indices. Finally the paper illustrates the issues that are under-addressed by the recent approaches and proposes the research directions in the field.	cluster analysis;experiment;pattern recognition	Maria Halkidi;Yannis Batistakis;Michalis Vazirgiannis	2002	SIGMOD Record	10.1145/601858.601862	unsupervised learning;computer science;data science;consensus clustering;pattern recognition;data mining;cluster analysis;conceptual clustering	DB	1.385342875357167	-39.56744768271494	100953
65af4b86f57d7c21485ed2edc8806338a9e51d25	tackling the dream challenge for gene regulatory networks reverse engineering	challenging task;grn reverse engineering;biological network;gene expression data;gene regulatory network;gene regulatory networks;naive bayes classifier;dream challenge;dream conference;hardest task;computational technique	The construction and the understanding of Gene Regulatory Networks (GRNs) are among the hardest tasks faced by systems biology. The inference of a GRN from gene expression data (the GRN reverse engineering), is a challenging task that requires the exploitation of diverse mathematical and computational techniques. The DREAM conference proposes several challenges about the inference of biological networks and/or the prediction of how they are influenced by perturbations. This paper describes a method for GRN reverse engineering that the authors submitted to the 2010 DREAM challenge. The methodology is based on a combination of well known statistical methods into a Naive Bayes classifier. Despite its simplicity the approach fared fairly well when compared to other proposals on real networks.	biological network;gene regulatory network;naive bayes classifier;perturbation theory;reverse engineering;systems biology	Alessia Visconti;Roberto Esposito;Francesca Cordero	2011		10.1007/978-3-642-23954-0_34	computer science;bioinformatics;artificial intelligence;data mining	Comp.	6.2702752967228745	-47.34353108275491	100991
f6009af6be3abfac80282a884f9bb7a7c85187b5	gpgpu implementation of nearest neighbor search with product quantization	nearest neighbor searches;gpu;quantization signal;cuda;autotuning;indexes quantization signal vectors computational complexity table lookup graphics processing units nearest neighbor searches;indexes;storage management graphics processing units multiprocessing systems search problems;multicore;vectors;computational complexity;graphics processing units;image search;autotuning gpgpu implementation general purpose computing on gpu nearest neighbor search parallelization product quantization high precision search memory consumption parallel systems multicore processors parameter selection optimistic search pseudomatrix transposition;table lookup;cuda multithreading image search autotuning gpu multicore;multithreading	A nearest neighbor search with product quantization is a prominent method that achieves a high-precision search with less memory consumption than an exhaustive way. However, in order to accomplish a large size search with a large reference data, the search method have to be accelerated by using parallel systems such as multicore processors and GPGPU (General Purpose computing on GPU) systems. The distance calculation between a query and a reference data is an independent operation that is easily parallelized, but the reduction computation of distances after that is not completely parallel, so this leads to performace degradation. Therefore, in order to maximize a speedup, the adequate parameter selection is required in terms of parallelism. In this paper, the baseline of parallelization of the nearest neighbor search with product quantization is described, and the validity of our approach (Optimistic Search), which utilizes a small number of candidates of nearest neighbors, is discussed with experiments. We also show the effectiveness of pseudo matrix transposition for the sake of the efficient search. In addition, the method for autotuning is proposed and its effectiveness is empirically confirmed.	auto-tune;baseline (configuration management);central processing unit;computation;elegant degradation;experiment;geforce 500 series;general-purpose computing on graphics processing units;graphics processing unit;least squares;lynnfield (microprocessor);multi-core processor;nearest neighbor search;parallel computing;reflow soldering;speedup	Akiyoshi Wakatani;Akio Murakami	2014	2014 IEEE International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2014.42	multi-core processor;beam search;database index;parallel computing;best bin first;multithreading;computer science;theoretical computer science;operating system;machine learning;distributed computing;nearest neighbor search;fixed-radius near neighbors;computational complexity theory	HPC	-3.562502234039759	-41.349871851835715	100992
72b8be842222e075a4fa6d4fa5e02177725f053a	an effective, practical and low computational cost framework for the integration of heterogeneous data to predict functional associations between proteins by means of artificial neural networks	systems biology;multilayer perceptrons;data distribution;functional linkage network;data integration	Nowadays, the uncovering of new functional relationships between proteins is one of the major goals of biological studies. For this task, the integration of evidences from heterogeneous data sources by means of machine learning methodologies has been demonstrated to be an effective way of providing a complete genome-wide functional network and more accurate inferences of new functional associations. This work presents a new framework to be used in Artificial Neural Networks (ANNs) for the task of predicting functional relationships between proteins through the integration of evidences from heterogeneous data sources. The developing of such new methodology is motivated by the problems that arise when applying ANNs to this kind of problems, namely, the computational cost of ANN optimization process due to the nature of data (large number of instances and high dimensionality). The method selects smaller representative/non-random subsets from the original data set selected for ANN optimization process, resulting in a reduction of the number of data to be trained and, consequently, the computational cost. Moreover, the fact that the subsets are not only smaller, but also representative from the original one, (i) prevents the repetition of the optimization process several times with different random subsets of data, which is commonly used to get a reliable and fair evaluation of ANN's prediction accuracy, and (ii) benefits the learning procedure in the sense of a reduction of the overfitting problem, improving, this way, the prediction ability.	algorithmic efficiency;artificial neural network;neural networks	J. P. Florido;Héctor Pomares;Ignacio Rojas;Alberto Guillén;Francisco M. Ortuño Guzman;José M. Urquiza	2013	Neurocomputing	10.1016/j.neucom.2012.11.040	computer science;artificial intelligence;data integration;machine learning;data mining;systems biology	ML	9.452130252775618	-49.2463766925782	101001
09e46239f4317e3b679712e458791adff216fc5a	integration of clustering and multidimensional scaling to determine phylogenetic trees as spherical phylograms visualized in 3 dimensions	interpolation;microbial communities;phylogeny;phylogeny visualization clustering algorithms equations mathematical model interpolation three dimensional displays;environmental genomics;sequence alignment multidimensional scaling phylogenetic analysis genetic sequence data fungal communities ordination clustering techniques sequence data analysis bacterial communities spherical phylogram sp phylogenetic tree visualization dacidr tree display methods interpolative joining ij 3d space branch lengths mantel tests classification accuracy;visualization;pattern clustering biology computing data visualisation evolution biological genetics interpolation microorganisms pattern classification;phylogenetic tree;three dimensional displays;environmental genomics phylogenetic tree multidimensional scaling microbial communities;multidimensional scaling;mathematical model;clustering algorithms	Phylogenetic analysis is commonly used to analyze genetic sequence data from fungal communities, while ordination and clustering techniques commonly are used to analyze sequence data from bacterial communities. However, few studies have attempted to link these two independent approaches. In this paper, we propose a method, which we call spherical phylogram (SP), to display the phylogenetic tree within the clustering and visualization result from a pipeline called DACIDR. In comparison with traditional tree display methods, the correlations between the tree and the clustering can be observed directly. In addition, we propose an algorithm called interpolative joining (IJ) to construct and visualize the SP in 3D space. In the experiments, we used the sum of branch lengths to quantify the general fit between the clustering and the phylogenetic tree in SP and Mantel tests to determine how well the same grouping of sequences was preserved between the clustering and the SP. Our results show that DACIDR has a classification accuracy that is similar to a phylogenetic tree generated using a multiple sequence alignment, while having much lower computational cost.	algorithm;algorithmic efficiency;cluster analysis;computation;experiment;ijustine;multidimensional scaling;multiple sequence alignment;phylogenetic tree;phylogenetics	Yang Ruan;Geoffrey L. House;Saliya Ekanayake;Ursel Schutte;James D. Bever;Haixu Tang;Geoffrey C. Fox	2014	2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing	10.1109/CCGrid.2014.126	correlation clustering;phylogenetic tree;visualization;multidimensional scaling;interpolation;computer science;bioinformatics;computational phylogenetics;theoretical computer science;mathematical model;data mining;cluster analysis;single-linkage clustering;phylogenetic network;phylogenetics	Comp.	3.037691377590146	-48.80762066872312	102503
07f433f487c233666e4ee7d8852868b9fee5107b	multiclass microarray data classification using ga/ann method	artificial neural network;genetic algorithm;microarray data;cell line	This work aims to explore the use of gene expression data in discriminating heterogeneous cancers. We introduce hybrid learning methodology that integrates genetic algorithms (GA) and artificial neural networks (ANN) to find optimal subsets of genes for tissue/cancer classification. This method was tested on two published microarray datasets: (1) NCI60 cancer cell lines and (2) the GCM dataset. Experimental results on classifying both datasets show that our GA/ANN method not only outperformed many reported prediction approaches, but also reduced the number of predictive genes needed in classification analysis.	microarray;monte carlo method;software release life cycle	Tsun-Chen Lin;Ru-Sheng Liu;Ya-Ting Chao;Shu-Yuan Chen	2006		10.1007/11801603_129	genetic engineering;microarray analysis techniques;gene expression;genetic algorithm;dna microarray;biological classification;computer science;bioinformatics;artificial intelligence;machine learning;test method;cell culture;artificial neural network;algorithm	ML	8.340218790916577	-48.61485858564454	102582
cf6b208ec861eb16eab7bb82d9c63aa464b0015a	multi-test decision trees for gene expression data analysis	univariate tests;classification;gene expression;decision trees	This paper introduces a new type of decision trees which are more suitable for gene expression data. The main motivation for this work was to improve the performance of decision trees under a possibly small increase in their complexity. Our approach is thus based on univariate tests, and the main contribution of this paper is the application of several univariate tests in each non-terminal node of the tree. In this way, obtained trees are still relatively easy to analyze and understand, but they become more powerful in modelling high dimensional microarray data. Experimental validation was performed on publicly available gene expression datasets. The proposed method displayed competitive accuracy compared to the commonly applied decision tree methods.	decision tree	Marcin Czajkowski;Marek Grzes;Marek Kretowski	2011		10.1007/978-3-642-25261-7_12	gene expression;decision tree learning;biological classification;computer science;bioinformatics;machine learning;decision tree;incremental decision tree;data mining	Theory	8.624915148271679	-45.884832937111625	102624
89af146345a7e9aab723583a6494e4d4665c0896	two-stage gene selection for support vector machine classification of microarray data	microarray data;support vector machines;significance analysis of microarrays;baseline method;microarray data classification;significance analysis;svm;support vector machine;two stage linear regression;gene selection;sam;microarrays	This paper proposes a new stable gene selection method for support vector machines (SVM) classification of microarray data, aiming to improve the classification accuracy. A two-stage algorithm is used to select genes, leading to the construction of a compact multivariate linear regression model, which contains only genes less than the number of experiments as well as a weight vector for each gene index. An SVM then learns the microarray data based on this linear regression model. The experimental results, from two well-known microarray datasets, show that SVMs with two-stage gene selection maintains a consistently high accuracy with a small number of genes. It is also shown that the proposed method outperforms the two other typical gene selection methods – baseline method and significance analysis of microarrays in terms of accuracy.	microarray;support vector machine	Xiao-Lei Xia;Kang Li;George W. Irwin	2009	IJMIC	10.1504/IJMIC.2009.029029	support vector machine;gene chip analysis;computer science;bioinformatics;machine learning;pattern recognition;data mining;structured support vector machine	ML	8.217157552398234	-48.98734320194833	102678
2661466f259e9ea6b850f2a82870111188fbd91d	spatiotemporal indexing techniques for efficiently mining spatiotemporal co-occurrence patterns	spatiotemporal phenomena indexing data mining trajectory atmospheric measurements particle measurements;chebyshev polynomial indexing spatiotemporal co occurrence pattern mining specifically designated spatiotemporal indexing techniques spatiotemporal datasets polygon based representations data access spatiotemporal indexing structures scalable and efficient trajectory index seti;spatiotemporal phenomena data mining database indexing	In this paper, we investigate using specifically-designated spatiotemporal indexing techniques for mining cooccurrence patterns from spatiotemporal datasets with evolving polygon-based representations. Previously, suggested techniques for spatiotemporal pattern mining algorithms did not take spatiotemporal indexing techniques into account. We present a new framework for mining spatiotemporal co-occurrence patterns that can use various indexing techniques for efficiently accessing data. Two well-studied spatiotemporal indexing structures, Scalable and Efficient Trajectory Index (SETI) and Chebyshev Polynomial Indexing are currently implemented and available in our framework.	algorithm;chebyshev polynomials;data mining;polynomial;search for extraterrestrial intelligence;spatiotemporal pattern	Berkay Aydin;Dustin Kempton;Vijay Akkineni;Shaktidhar Reddy Gopavaram;Karthik Ganesan Pillai;Rafal A. Angryk	2014	2014 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2014.7004398	geography;data science;data mining;information retrieval;spatiotemporal database	DB	-4.0042908410491895	-41.934041327668176	103046
a1e17ffef2503f217693bdb14c0814e217b04f50	semi-supervised distributed clustering for bioinformatics - comparison study		Clustering analysis is a widely used technique in bioinformatics and biochemistry for variety of applications such as detection of new cell types, evaluation of drug response, etc. Since different applications and cells may require different clustering algorithms combining multiple clustering results into a consensus clustering using distributed clustering is a popular and efficient method to improve the quality of clustering analysis. Currently existing solutions are commonly based on supervised techniques which do not require any a priori knowledge. However in certain cases, a priori information on particular labelings may be available a priori. In these cases it is expected that performance improvement can be achieved by utilizing this prior information. To this purpose in this paper, we propose two semi-supervised distributed clustering algorithms and evaluate their performance for different base clusterings.	algorithm;bioinformatics;cluster analysis;consensus clustering;semiconductor industry;supervised learning	Huaying Li;Aleksandar Jeremic	2017		10.5220/0006253502590264	consensus clustering;data mining;cluster analysis	HPC	5.982107484015986	-49.46058055747572	103071
8923a5882ecf37a0ec19f2b37937da79e2825225	maximum likelihood combination of multiple clusterings	distance function;maximum likelihood;markov random field;centroid clustering;cluster analysis;maximum likelihood estimate;metric distance function;consensus clustering	A promising direction for more robust clustering is to derive multiple candidate clusterings over a common set of objects and then combine them into a consolidated one, which is expected to be better than any candidate. Given a candidate clustering set, we show that with a particular pairwise potential used in Markov random fields, the maximum likelihood estimation is the one closest to the set in terms of a metric distance between clusterings. To minimize such a distance, we present two combining methods based on the new similarity determined by the whole candidate set. We evaluate them on both artificial and real datasets, with candidate clusterings either from full space or subspace. Experiments show that they not only lead to a closer distance to the candidate set, but also achieve a smaller or comparable distance to the true clustering. 2006 Elsevier B.V. All rights reserved.	cluster analysis;experiment;markov chain;markov random field	Tianming Hu;Ying Yu;Jinzhi Xiong;Sam Yuan Sung	2006	Pattern Recognition Letters	10.1016/j.patrec.2006.02.013	k-medians clustering;computer science;machine learning;consensus clustering;pattern recognition;mathematics;hierarchical clustering;maximum likelihood;statistics	Vision	1.2886005197156545	-41.83567455121609	103584
f07415599c6202016534ab88f19af74201fcfac8	simone: statistical inference for modular networks	high dimensionality;reseau;gene expression data;red;partial correlation;inferencia;statistical inference;gaussian graphical model;gene regulatory network;inference;network	SUMMARY The R package SIMoNe (Statistical Inference for MOdular NEtworks) enables inference of gene-regulatory networks based on partial correlation coefficients from microarray experiments. Modelling gene expression data with a Gaussian graphical model (hereafter GGM), the algorithm estimates non-zero entries of the concentration matrix, in a sparse and possibly high-dimensional setting. Its originality lies in the fact that it searches for a latent modular structure to drive the inference procedure through adaptive penalization of the concentration matrix.   AVAILABILITY Under the GNU General Public Licence at http://cran.r-project.org/web/packages/simone/	algorithm;arabic numeral 0;coefficient;congenital glucose-galactose malabsorption;estimated;experiment;gnu;gene expression profiling;gene regulatory network;generic group model;graphical model;inference;microarray;normal statistical distribution;penalty method;sparse matrix	Julien Chiquet;Alexander Smith;Gilles Grasseau;Catherine Matias;Christophe Ambroise	2009	Bioinformatics	10.1093/bioinformatics/btn637	biology;gene regulatory network;statistical inference;computer science;bioinformatics;partial correlation;machine learning;data mining;statistics	ML	4.2950226319821665	-51.60341169408982	103637
66701ab161e5139aff636491a2a41ede9b4995af	mar: maximum attribute relative of soft set for clustering attribute selection	attribute relative;clustering attributes;complexity;data mining;soft set theory	Clustering, which is a set of categorical data into a homogenous class, is a fundamental operation in data mining. One of the techniques of data clustering was performed by introducing a clustering attribute. A number of algorithms have been proposed to address the problem of clustering attribute selection. However, the performance of these algorithms is still an issue due to high computational complexity. This paper proposes a new algorithm called Maximum Attribute Relative (MAR) for clustering attribute selection. It is based on a soft set theory by introducing the concept of the attribute relative in information systems. Based on the experiment on fourteen UCI datasets and a supplier dataset, the proposed algorithm achieved a lower computational time than the three rough set-based algorithms, i.e. TR, MMR, and MDA up to 62%, 64%, and 40% respectively and compared to a soft set-based algorithm, i.e. NSS up to 33%. Furthermore, MAR has a good scalability, i.e. the executing time of the algorithm tends to increase linearly as the number of instances and attributes are increased respectively.	cluster analysis	Rabiei Mamat;Tutut Herawan;Mustafa Mat Deris	2013	Knowl.-Based Syst.	10.1016/j.knosys.2013.05.009	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;complexity;attribute domain;k-medians clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;affinity propagation	DB	5.65646370313131	-41.38550194550781	103693
33dbc9055e85167620510d93aaa8b5049d83ec2d	hypothesis testing for topological data analysis	persistence diagram;permutation test;null hypothesis test;topological data analysis;62g10;62g09;55n35	Persistence homology is a vital tool for topological data analysis. Previous work has developed some statistical estimators for characteristics of collections of persistence diagrams. However, tools that provide statistical inference for scenarios in which the observations are persistence diagrams are not developed. We propose the use of randomization-style null hypothesis significance tests (NHST) for these situations. We demonstrate this method to analyze a range of simulated and experimental data.	diagram;homology (biology);persistence (computer science);topological data analysis	Andrew Robinson;Katharine Turner	2013	CoRR		econometrics;mathematics;statistics	Theory	5.487176424800892	-51.73987879205025	104126
62a078edeb73643daf559bed980fa678a210d026	a novel method of searching the microarray data for the best gene subsets by using a genetic algorithm	microarray data;parallelisme;cancerology;microprocessor;pulga de dna;tumor maligno;search space;puce a dna;hombre;algoritmo genetico;classification;resolucion problema;parallelism;paralelismo;biomimetique;cancerologie;human;dna chip;algorithme genetique;genetic algorithm;tumeur maligne;microprocesseur;cancerologia;cancer classification;gene selection;microprocesador;clasificacion;problem solving;resolution probleme;malignant tumor;homme;biomimetics	Searching for a small subset of genes out of the thousands of genes in Microarray is a crucial problem for accurate cancer classification. In this paper, a novel gene selection method based on genetic algorithms (GAs) is proposed. In order to reduce the search space of GAs, a novel pre-selection procedure is also introduced. To evaluate the performance of the presented method, experiments on five open datasets are conducted, and the results show that it performs rather well.	genetic algorithm;microarray	Bin Ni;Juan Liu	2004		10.1007/978-3-540-30217-9_116	biomimetics;gene-centered view of evolution;microarray analysis techniques;genetic algorithm;dna microarray;biological classification;computer science;bioinformatics;artificial intelligence;algorithm	Robotics	3.424805597973435	-46.53470181055373	104163
eea75189c640e9df01b9b28701146ff2640dc96c	information-optimal genome assembly via sparse read-overlap graphs		MOTIVATION In the context of third-generation long-read sequencing technologies, read-overlap-based approaches are expected to play a central role in the assembly step. A fundamental challenge in assembling from a read-overlap graph is that the true sequence corresponds to a Hamiltonian path on the graph, and, under most formulations, the assembly problem becomes NP-hard, restricting practical approaches to heuristics. In this work, we avoid this seemingly fundamental barrier by first setting the computational complexity issue aside, and seeking an algorithm that targets information limits In particular, we consider a basic feasibility question: when does the set of reads contain enough information to allow unambiguous reconstruction of the true sequence?   RESULTS Based on insights from this information feasibility question, we present an algorithm-the Not-So-Greedy algorithm-to construct a sparse read-overlap graph. Unlike most other assembly algorithms, Not-So-Greedy comes with a performance guarantee: whenever information feasibility conditions are satisfied, the algorithm reduces the assembly problem to an Eulerian path problem on the resulting graph, and can thus be solved in linear time. In practice, this theoretical guarantee translates into assemblies of higher quality. Evaluations on both simulated reads from real genomes and a PacBio Escherichia coli K12 dataset demonstrate that Not-So-Greedy compares favorably with standard string graph approaches in terms of accuracy of the resulting read-overlap graph and contig N50.   AVAILABILITY Available at github.com/samhykim/nsg   CONTACT courtade@eecs.berkeley.edu or dntse@stanford.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	bioinformatics;computational complexity theory;eulerian path;genome;graph - visual representation;greedy algorithm;hamiltonian path;heuristics;np-hardness;reading (activity);silo (dataset);sparse matrix;string graph;time complexity	Ilan Shomorony;Samuel H. Kim;Thomas A. Courtade;David Tse	2016	Bioinformatics	10.1093/bioinformatics/btw450	computational biology;sequence assembly;biology;pattern recognition;artificial intelligence;graph	Comp.	1.0108699873668763	-50.4604761568685	104366
8591602d2e1c40596daa536ac2b17dfd0611ef5c	tobae: a density-based agglomerative clustering algorithm	automatic;filtering;computacion informatica;water puddles;terrain;clustering;ciencias basicas y experimentales;matematicas;agglomerative;grupo a;noise removal;density threshold;density distribution;non parametric	This paper presents a novel density based agglomerative clustering algorithm named TOBAE which is a parameter-less algorithm and automatically filters noise. It finds the appropriate number of clusters while giving a competitive running time. TOBAE works by tracking the cumulative density distribution of the data points on a grid and only requires the original data set as input. The clustering problem is solved by automatically finding the optimal density threshold for the clusters. It is applicable to any N-dimensional data set which makes it highly relevant for real world scenarios. The algorithm outperforms state of the art clustering algorithms by the additional feature of automatic noise filtration around clusters. The concept behind the algorithm is explained using the analogy of puddles ('tobae'), which the algorithm is inspired from. This paper provides a detailed algorithm for TOBAE along with the complexity analysis for both time and space. We show experimental results against known data sets and show how TOBAE competes with the best algorithms in the field while providing its own set of advantages.	algorithm;cluster analysis	Shehzad Khalid;Shahid Razzaq	2015	J. Classification	10.1007/s00357-015-9166-2	filter;nonparametric statistics;correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;terrain;k-medians clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;brown clustering;automatic transmission;dbscan;affinity propagation;statistics	ML	0.2984347172631561	-40.63956424062476	104394
44d964865e760f0c12cdd863b03df34a4c11a95b	a modified bee colony optimization (mbco) and its hybridization with k-means for an application to data clustering		Abstract Among the nature inspired heuristic or meta-heuristic optimization algorithms, Bee Colony Optimization (BCO) algorithms are widely used to solve clustering problem. In this paper, a modified BCO (MBCO) approach is proposed for data clustering. In the proposed MBCO, the forgiveness characteristics of bees and giving a fair chance to both trustworthy and untrustworthy bees are being taken care of. A probability based selection ( Pbselection ) approach is introduced in the proposed MBCO for assigning unallocated data points in every iteration. The result shows that, the proposed method gives faster convergence as compared to the existing well known algorithms. In order to improve the performance of MBCO further and to obtain global optimal and diverse solution, the proposed MBCO is hybridized with k -means algorithm. In average, the hybridized MKCLUST and KMCLUST give same or better result than the proposed MBCO. To validate the proposed algorithms, seven standard data sets are considered. From classification error percentages calculation, it is observed that the proposed algorithms perform better compared to some existing algorithms. The simulation results infer that the proposed algorithms can be efficiently used for data clustering.	bees algorithm;cluster analysis;k-means clustering	Pranesh Das;Dushmanta Kumar Das;Shouvik Dey	2018	Appl. Soft Comput.	10.1016/j.asoc.2018.05.045	trustworthiness;mathematical optimization;artificial intelligence;machine learning;k-means clustering;cluster analysis;data point;mathematics;heuristic;data set;convergence (routing)	ML	3.8914475508090303	-41.904216803751694	104516
507fe06be09fc3e31c5c9102cd4d9fa81820e610	adaptive structure concept factorization for multiview clustering		Most existing multiview clustering methods require that graph matrices in different views are computed beforehand and that each graph is obtained independently. However, this requirement ignores the correlation between multiple views. In this letter, we tackle the problem of multiview clustering by jointly optimizing the graph matrix to make full use of the data correlation between views. With the interview correlation, a concept factorization–based multiview clustering method is developed for data integration, and the adaptive method correlates the affinity weights of all views. This method differs from nonnegative matrix factorization–based clustering methods in that it can be applicable to data sets containing negative values. Experiments are conducted to demonstrate the effectiveness of the proposed method in comparison with state-of-the-art approaches in terms of accuracy, normalized mutual information, and purity.	affinity analysis;algorithm;cluster analysis;data structure;experiment;graph - visual representation;intuition;iteration;manifold regularization;mutual information;non-negative matrix factorization;purity;rendering (computer graphics);time complexity;statistical cluster	Kun Zhan;Jinhui Shi;Jing Wang;Haibo Wang;Yuange Xie	2018	Neural Computation	10.1162/neco_a_01055	artificial intelligence;mathematics;machine learning;data integration;theoretical computer science;normalization (statistics);mutual information;cluster analysis;data set;factorization;matrix (mathematics);non-negative matrix factorization	AI	0.707735698182901	-43.236421690960725	104726
82f50ae41fdca126574ce4804c6cfc39fec41367	rule generation and rule selection techniques for cost-sensitive associative classification	association rule;decision tree;information value	Classification aims to assign a data object to its appropriate class, what is traditionally performed through a small dataset mode l such as decision tree. Associative classification is a novel strategy for per forming this task where the model is composed of a particular set of association rule s, in which the consequent of each rule (i.e., its right-hand-side) is restric ted to the classification class attribute. Rule generation and rule selection are two m ajor issues in associative classification. Rule generation aims to find a set o f association rules that better describe the entire dataset, while rule selectio n aims to select, for a particular case, the best rule among all rules generated. Ru le generation and rule selection techniques dramatically affect the effecti veness of the classifier. In this paper we propose new techniques for rule generation and r ule selection. In our proposed technique, rules are generated based on the con cept of maximal frequent class itemsets (increasing the size of the rule pat t rn), and then selected based on their informative value and on the cost that an error imply (possibly reducing misclassifications). We validate our techniques usi ng two important real world problems: spam detection and protein homology detecti on. Further, we compare our techniques against other existing ones, rangin g from well known näıve-Bayes to domain-specific classifiers. Experimental res ults how that our techniques are able to achieve a significant improvement of 3 0% in the effectiveness of the classification.	algorithm;anti-spam techniques;association rule learning;automata theory;data mining;decision tree;html attribute;homology (biology);information;maximal set;naive bayes classifier;naruto shippuden: clash of ninja revolution 3	Adriano Veloso;Wagner Meira	2005			data mining;associative property;decision tree;association rule learning;machine learning;domain specificity;ranging;computer science;artificial intelligence;pattern recognition	ML	8.968611583850512	-46.26984877084381	104770
cbe913b435b9978aec2dc4a5a955d0fb0ef4e8cd	principal metabolic flux mode analysis		Motivation In the analysis of metabolism, two distinct and complementary approaches are frequently used: Principal component analysis (PCA) and stoichiometric flux analysis. PCA is able to capture the main modes of variability in a set of experiments and does not make many prior assumptions about the data, but does not inherently take into account the flux mode structure of metabolism. Stoichiometric flux analysis methods, such as Flux Balance Analysis (FBA) and Elementary Mode Analysis, on the other hand, are able to capture the metabolic flux modes, however, they are primarily designed for the analysis of single samples at a time, and not best suited for exploratory analysis on a large sets of samples.   Results We propose a new methodology for the analysis of metabolism, called Principal Metabolic Flux Mode Analysis (PMFA), which marries the PCA and stoichiometric flux analysis approaches in an elegant regularized optimization framework. In short, the method incorporates a variance maximization objective form PCA coupled with a stoichiometric regularizer, which penalizes projections that are far from any flux modes of the network. For interpretability, we also introduce a sparse variant of PMFA that favours flux modes that contain a small number of reactions. Our experiments demonstrate the versatility and capabilities of our methodology. The proposed method can be applied to genome-scale metabolic network in efficient way as PMFA does not enumerate elementary modes. In addition, the method is more robust on out-of-steady steady-state experimental data than competing flux mode analysis approaches.   Availability and implementation Matlab software for PMFA and SPMFA and dataset used for experiments are available in https://github.com/aalto-ics-kepaco/PMFA.   Supplementary information Supplementary data are available at Bioinformatics online.	bioinformatics;enumerated type;expectation–maximization algorithm;experiment;flux balance analysis;geographic information systems;matlab;mathematical optimization;metabolic process, cellular;metabolic network modelling;principal component analysis;projections and predictions;sample variance;silo (dataset);sparse matrix;spatial variability;steady state	Sahely Bhadra;Peter Blomberg;Sandra Castillo;Juho Rousu	2018		10.1093/bioinformatics/bty049	data mining;flux;steady state;computer science;flux balance analysis;principal component analysis;bioinformatics;interpretability;small number;maximization	Comp.	5.853055439549556	-51.46086788504393	105230
74662fa5182a030fa025bf4276f123c2880f2557	locally optimal search method for identifying genes from microarray data	dna;microarray data;high dimensionality;cancer;search method;gene expression data;tree searching dna genetics cancer medical computing pattern classification feature extraction;feature space;genetics;medical computing;colon cancer;gene expression;search methods cancer dna partitioning algorithms spatial databases testing laboratories gene expression colon sequences;feature extraction;feature subset selection;pattern classification;learning problems;dna microarray data;cancer classification;class separability locally optimal search method gene identification microarray data gene expression data cancer classification dna microarray data feature subset selection algorithm partitional branch and bound algorithm very high dimensional feature space colon cancer database leukemia database forward selection algorithms individual ranking methods criterion function;tree searching;branch and bound	The gene expression data obtained from microarrays have shown to be useful in cancer classification. DNA microarray data have extremely high dimensionality compared to the small number of available samples. An important step in microarray studies is to remove genes irrelevant to the learning problem and to select a small number of genes expressed in biological samples under specific conditions. In this paper, we propose a novel feature subset selection algorithm, partitional branch and bound (PBB) algorithm. This new algorithm is very efficient for selecting sets of genes in very high dimensional feature space. Two databases are considered: the colon cancer database and the leukemia database. Our experimental results show that the proposed algorithm yields a better subset of features than both forward selection algorithms and individual ranking methods in terms of a criterion function measuring class separability.	branch and bound;colon classification;dna microarray;database;feature selection;feature vector;linear separability;loss function;parallel building blocks;relevance;selection algorithm;stepwise regression	Xue-wen Chen;Huaixin Chen	2004	ICARCV 2004 8th Control, Automation, Robotics and Vision Conference, 2004.	10.1109/ICARCV.2004.1468986	microarray analysis techniques;gene expression;feature vector;feature extraction;computer science;bioinformatics;pattern recognition;data mining;mathematics;branch and bound;dna;cancer	ML	8.222543395089676	-47.53312270748656	105636
046e9d03502c7f65e02774b0d9585f26c41a836c	a random matrix-based approach for dependency detection from data streams	spectrum;data mining;eigenvalues;correlation matrix;random matrix;random matrices;covariance matrix	This paper describes a novel approach to detect correlation from data streams in the context of MobiMine, an experimental mobile data mining system. It presents a brief description of the MobiMine and identifies the problem of detecting dependencies among stocks from incrementally observed financial data streams. This is a non-trivial problem since the stock-market data is inherently noisy and small incremental volumes of data makes the estimation process more vulnerable to noise. This paper presents EDS, a technique to estimate the correlation matrix from data streams by exploiting some properties of the distribution of eigenvalues for random matrices. It separates the “information” from the “noise” by comparing the eigen-spectrum generated from the observed data with that of random matrices. The comparison immediately leads to a decomposition of the covariance matrix into two matrices: one capturing the “noise” and the other capturing useful “information.” The paper also presents experimental results using Nasdaq 100 stock data.	converge;data mining;data stream mining;dijkstra's algorithm;eigen (c++ library);extended data services;numerical analysis;power iteration;randomness;sensor	Hillol Kargupta;Krishnamoorthy Sivakumar;Samiran Ghosh	2002			multivariate t-distribution;random field;data stream mining;covariance mapping;multivariate random variable;covariance matrix;estimation of covariance matrices;covariance;artificial intelligence;mathematics;pattern recognition	ML	-1.8841317848148813	-38.254141452786484	105779
4f8b3fbb955278b86c72e7c6547a6045e0f42321	a heuristic clustering algorithm using union of overlapping pattern-cells	cluster algorithm;electrical engineering	Relative geometric arrangements of the sample points, with reference to the structure of the imbedding space, produce clusters. Hence, if each sample point is imagined to acquire a volume of a small M-cube (called pattern-cell), depending on the ranges of its (M) features and number (N) of samples; then overlapping pattern-cells would indicate naturally closer sample-points. A chain or blob of such overlapping cells would mean a cluster and separate clusters would not share a common pattern-cell between them. The conditions and an analytic method to find such an overlap are developed. A simple, intuitive, nonparametric clustering procedure, based on such overlapping pattern-cells is presented. It may be classified as an agglomerative, hierarchical, linkage-type clustering procedure. The algorithm is fast, requires low storage and can identify irregular clusters. Two extensions of the algorithm, to separate overlapping clusters and to estimate the nature of pattern distributions in the sample space, are also indicated.	algorithm;cluster analysis;heuristic	C. S. Warnekar;G. Krishna	1979	Pattern Recognition	10.1016/0031-3203(79)90054-2	complete-linkage clustering;mathematical optimization;combinatorics;computer science;machine learning;mathematics;single-linkage clustering	Vision	0.13847750674873338	-43.53919763892596	106050
d66de293cb1c73e84bf94910f3d4d25d70c36c4d	genetic algorithms and particle swarm optimization for exploratory projection pursuit	62 07;68t20;projection pursuit;exploratory projection pursuit;large data sets;exploratory analysis;68 04;particle swarm optimizer;clustering;particle swarm optimization;indexation;62 09;62h99;genetic algorithm;high dimension	Exploratory Projection Pursuit (EPP) methods have been developed thirty years ago in the context of exploratory analysis of large data sets. These methods consist in looking for low-dimensional projections that reveal some interesting structure existing in the data set but not visible in high dimension. Each projection is associated with a real valued index which optima correspond to valuable projections. Several EPP indices have been proposed in the statistics literature but the main problem lies in their optimization. In the present paper, we propose to apply Genetic Algorithms (GA) and recent Particle Swarm Optimization (PSO) algorithm to the optimization of several projection pursuit indices. We explain how the EPP methods can be implemented in order to become an efficient and powerful tool for the statistician. We illustrate our proposal on several simulated and real data sets.	cma-es;computation;cooley–tukey fft algorithm;discrete cosine transform;discriminant;evolutionary algorithm;exploratory testing;fastest;genetic algorithm;global optimization;ieee 1284;inverted index;iteration;iterative method;local optimum;loss function;mathematical optimization;minimum bounding box;nat friedman;optimization problem;optimizing compiler;parallel computing;particle swarm optimization;phase-shift oscillator;software release life cycle;the times;tyler oakley	Alain Berro;Souad Larabi Marie-Sainte;Anne Ruiz-Gazen	2010	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-010-9211-0	projection pursuit;mathematical optimization;multi-swarm optimization;genetic algorithm;computer science;machine learning;data mining;mathematics;cluster analysis;particle swarm optimization	AI	4.303360592930784	-42.79225845245026	106062
5c459384832304586f35417beabf50e9528a543d	a data-driven clustering approach for fault diagnosis		Clustering is an important approach in fault diagnosis. The dominant sets algorithm is a graph-based clustering algorithm, which defines the dominant set as a concept of a cluster. In this paper, we make an in-depth investigation of the dominant sets algorithm. As a result, we find that this algorithm is dependent on the similarity parameter in constructing the pairwise similarity matrix, and has the tendency to generate spherical clusters only. Based on the merits and drawbacks of this algorithm, we apply the histogram equalization transformation to the similarity matrices for the purpose of removing the influence of similarity parameters, and then use a density-based cluster expansion process to improve the clustering results. In experimental validation of the proposed algorithm, we use two criterions to evaluate the clustering results in order to arrive at convincing conclusions. Data clustering experiments on ten data sets and fault detection experiments on the Tennessee Eastman process demonstrate the effectiveness of the proposed algorithm.	algorithm;cluster analysis;computer cluster;experiment;fault detection and isolation;histogram equalization;similarity measure	Jian Hou;Bing Xiao	2017	IEEE Access	10.1109/ACCESS.2017.2771365	cluster expansion;computer science;distributed computing;cluster analysis;algorithm design;fault detection and isolation;matrix (mathematics);data set;pairwise comparison;artificial intelligence;histogram equalization;pattern recognition	ML	0.8064908233163202	-41.26227863195299	106102
10c26e163f5b13fbc14313f5a050479fb81a397b	irrelevant gene elimination for partial least squares based dimension reduction by using feature probes	microarray data;metodo estadistico;donnee experimentale;minimos cuadrados parciales;high dimensionality;dimension reduction;partial least square;indicacion;selection;gen;dato experimental;bioinformatique;statistical method;gene expression data;data mining;partial least squares;microreseau;reduction dimension;gene expression;expression genique;microarray analysis;computational science and engineering;partiel;methode statistique;filter;partial;microarreglo;moindre carre partiel;gene;reduccion dimension;filtre;feature selection;microarray;classifier performance;parcial;dna microarray data;bioinformatica;indication;seleccion;dna microarray;classification accuracy;gene selection;irrelevant gene elimination;pls;filtro;expresion genetica;bioinformatics	It is hard to analyse gene expression data which has only a few observations but with thousands of measured genes. Partial Least Squares based Dimension Reduction (PLSDR) is superior for handling such high dimensional problems, but irrelevant features will introduce errors into the dimension reduction process. Here, feature selection is applied to filter the data and an algorithm named PLSDRg is described by integrating PLSDR with gene elimination, which is performed by the indication of t-statistic scores on standardised probes. Experimental results on six microarray data sets show that PLSDRg is effective and reliable to improve generalisation performance of classifiers.	algorithm;dimensionality reduction;excretory function;feature selection;gene expression;handling (psychology);microarray;name;partial least squares regression;relevance	Xue-Qiang Zeng;Guozheng Li;Gengfeng Wu;Jack Y. Yang;Mary Yang	2009	International journal of data mining and bioinformatics	10.1504/IJDMB.2009.023886	biology;microarray analysis techniques;computer science;bioinformatics;data mining;mathematics;feature selection;statistics	ML	6.185319941982214	-46.95014093926924	106148
f279384371e76b8a78c555dcaceb145a3f939b5c	gene selection and classification of human lymphoma from microarray data	microarray data;receiver operator characteristic;linear discriminate analysis;machine learning;feature selection;multi layer perceptron;support vector machine;dna microarray;classification accuracy;gene selection	"""Experiments in DNA microarray provide information of thousands of genes, and bioinformatics researchers have analyzed them with various machine learning techniques to diagnose diseases. Recently support vector machines (SVM) have been demonstrated as an effective tool in analyzing microarray data. Previous work involving SVM used every gene in the microarray to classify normal and malignant lymphoid tissue [21]. This paper shows that, using gene selection techniques that selected only 10% of the genes in 'Lymphochip"""" (a DNA microarray developed at Stanford University School of Medicine), a classification accuracy of about 98% is achieved which is a comparable performance to using every gene. This paper thus demonstrates the usefulness of feature selection techniques in conjunction with SVM to improve its performance in analyzing Lymphochip microarray data. The improved performance was evident in terms of better accuracy, ROC (receiver operating characteristics) analysis and faster training. Using the subsets of Lymphochip, this paper then compared the performance of SVM against two other well-known classifiers: multi-layer perceptron (MLP) and linear discriminant analysis (LDA). Experimental results show that SVM outperforms the other two classifiers."""	bioinformatics;dna microarray;feature selection;linear discriminant analysis;machine learning;memory-level parallelism;multilayer perceptron;support vector machine	Joarder Kamruzzaman;Suryani Lim;Iqbal Gondal;Rezaul K. Begg	2005	TENCON 2005 - 2005 IEEE Region 10 Conference	10.1007/11573067_38	computer science;bioinformatics;machine learning;pattern recognition	Comp.	9.16186794910598	-48.4802168421065	106687
4359027bf19231105df32d32ffdc24036a52e672	partitions selection strategy for set of clustering solutions	model selection;multi objective genetic algorithm;clustering;cluster model	Clustering is a difficult task: there is no single cluster definition and the data can have more than one underlying structure. Pareto-based multi-objective genetic algorithms (e.g., MOCK-Multi-Objective Clustering with automatic K-determination and MOCLE-Multi-Objective Clustering Ensemble) were proposed to tackle these problems. However, the output of such algorithms can often contains a high number of partitions, becoming difficult for an expert to manually analyze all of them. In order to deal with this problem, we present two selection strategies, which are based on the corrected Rand, to choose a subset of solutions. To test them, they are applied to the set of solutions produced by MOCK and MOCLE in the context of several datasets. The study was also extended to select a reduced set of partitions from the initial population of MOCLE. These analysis show that both versions of selection strategy proposed are very effective. They can significantly reduce the number of solutions and, at the same time, keep the quality and the diversity of the partitions in the original set of solutions.	cluster analysis;genetic algorithm;pareto efficiency;rand index;selection algorithm	Katti Faceli;Tiemi C. Sakata;Marcílio Carlos Pereira de Souto;André Carlos Ponce de Leon Ferreira de Carvalho	2010	Neurocomputing	10.1016/j.neucom.2010.03.028	correlation clustering;constrained clustering;mathematical optimization;k-medians clustering;fuzzy clustering;computer science;machine learning;data mining;mathematics;cluster analysis;single-linkage clustering;model selection;clustering high-dimensional data	EDA	3.942729160531452	-42.01043687827451	106715
dbf82092016e88aed220bab1f41d9826a306eec6	exploring deep markov models in genomic data compression using sequence pre-analysis	markov processes biology computing data analysis data compression genomics;bioinformatics genomics dna context data compression context modeling data models;textual data deep markov models genomic data compression algorithm data sequence pre analysis low complexity regions hash tables repetitive genomic sequences;finite context models genomic data compression hash tables	The pressure to find efficient genomic compression algorithms is being felt worldwide, as proved by several prizes and competitions. In this paper, we propose a compression algorithm that relies on a pre-analysis of the data before compression, with the aim of identifying regions of low complexity. This strategy enables us to use deeper context models, supported by hash-tables, without requiring huge amounts of memory. As an example, context depths as large as 32 are attainable for alphabets of four symbols, as is the case of genomic sequences. These deeper context models show very high compression capabilities in very repetitive genomic sequences, yielding improvements over previous algorithms. Furthermore, this method is universal, in the sense that it can be used in any type of textual data (such as quality-scores).	algorithm;data compression;markov chain;markov model;text corpus	Diogo Pratas;Armando J. Pinho	2014	2014 22nd European Signal Processing Conference (EUSIPCO)		computer science;bioinformatics;theoretical computer science;data mining	ML	-3.369544268190123	-50.79470557943138	106760
552f294d0b00fe2fbf3f21db23e417ef51e6f977	novel clustering-based approach for local outlier detection	density peak reachability clustering based approach big data mining big data analysis global outlier identification parameters variation robust local outlier detection statistical parameters clustering based ideas chebyshev inequality;big data standards data mining robustness chebyshev approximation electronic mail statistical analysis;statistical analysis big data data analysis data mining pattern clustering;clustering based outlier detection big data data mining	With the rapid expansion of data scale, big data mining and analysis have attracted increasing attention. Outlier detection as an important task of data mining is widely used in many applications. However, conventional outlier detection methods have difficulty handling large-scale datasets. In addition, most of them typically can only identify global outliers and are over sensitive to parameters variation. In this paper, we propose a novel method for robust local outlier detection with statistical parameters, which incorporates the clustering-based ideas in dealing with big data. Firstly, this method finds some density peaks of dataset by 3σ standard. Secondly, each remaining data object in the dataset is assigned to the same cluster as its nearest neighbor of higher density. Finally, we use Chebyshev's inequality and density peak reachability to identify local outliers of each group. The experimental results demonstrate the efficiency and accuracy of the proposed method in identifying both global and local outliers. Moreover, the method is also proved to be more stability analysis than typical outlier detection methods, such as LOF (Local Outlier Factor) and DBSCAN (Density-Based Spatial Clustering of Applications with Noise).	algorithm;anomaly detection;apache hadoop;benchmark (computing);big data;cluster analysis;dbscan;data mining;decibel;device driver;dhrystone;global positioning system;local outlier factor;reachability;social inequality;whole earth 'lectronic link	Haizhou Du;Shengjie Zhao;Daqiang Zhang;Jinsong Wu	2016	2016 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)	10.1109/INFCOMW.2016.7562187	correlation clustering;data stream clustering;k-medians clustering;computer science;data science;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;statistics;clustering high-dimensional data	ML	-0.8191881073869202	-40.34429395572438	106841
6c450f0b41fb37073e1b9febd4a87cbb08dd0ab8	a fast multiple attractor cellular automata with modified clonal classifier for splicing site prediction in human genome	multiple attractor cellular automata maca;cellular automata ca;clonal classifier cc;splicing site	1. Dept of CSE, JNTU Hyderabad, profkiransree@gmail.com 2. Dept of CSE, ANU, Guntur, drirameshbabu@gmail.com 3. Dept of CSE, University College of Engineering, JNTUK. Abstract Bioinformatics encompass storing, analyzing and interpreting the biological data. Most of the challenges for Machine Learning methods like Cellular Automata is to furnish the functional information with the corresponding biological sequences. In eukaryotes DNA is divided into introns and exons. The introns will be removed to make the coding region by a process called splicing. By indentifying a splice site we can easily specify the DNA sequence category (Donor/Accepter/Neither).Splicing sites play an important role in understanding the genes. A class of CA which can handle fuzzy logic is employed with modified clonal algorithm is proposed to identify the splicing site. This classifier is tested with Irvine Primate Splice Junction Database. It is compared with NNspIICE, GENIO, HSPL and SPIICE VIEW. The reported accuracy and efficiency of prediction is quite promising.	algorithm;bioinformatics;cellular automaton;donor-acceptor scheme;fuzzy logic;machine learning;splice (system call)	Pokkuluri Kiran Sree;Inampudi Ramesh Babu;N. S. S. S. N. Usha Devi	2014	CoRR		bioinformatics;machine learning	ML	4.927226085748711	-46.04099784803981	107167
e6fa87fee24c6152937e18b26e42fec5c2911305	a weighted local least squares imputation method for missing value estimation in microarray gene expression data	metodo cuadrado menor;microarray data;methode moindre carre;estimacion;pulga de dna;dato;peso;least squares method;vector angle;analisis datos;puce a dna;data;gene expression data;vecteur;gene expression;expression genique;data analysis;weight;estimation;donnee;weighted local least square imputation;poids;least square;missing value imputation;wllsi;dna chip;analyse donnee;angulo;vector;gene similarities;missing values;gene expression data analysis;angle;article;breast cancer;expresion genetica;bioinformatics	Many clustering techniques and classification methods for analysing microarray data require a complete dataset. However, very often gene expression datasets contain missing values due to various reasons. In this paper, we first propose to use vector angle as a measurement for the similarity between genes. We then propose the Weighted Local Least Square Imputation (WLLSI) method for missing values estimation. Numerical results on both synthetic data and real microarray data indicate that WLLSI method is more robust. The imputation methods are then applied to a breast cancer dataset and interesting results are obtained.		Wai-Ki Ching;Limin Li;Nam-Kiu Tsing;Ching-Wan Tai;Tuen-Wai Ng;Alice S. Wong;Kwai-Wa Cheng	2010	International journal of data mining and bioinformatics	10.1504/IJDMB.2010.033524	bioinformatics;data mining;imputation;least squares;statistics	Comp.	5.703476329093204	-46.76200473927367	107435
34dc216cae0cafe62c5c62f6fe3b9f3d0571a7d5	repeated sequences in linear genetic programming genomes	discipulus;evolutionary computation;hierarchical building blocks;linear genetic programming;building block;repeats finder;repeated sequences;chaotic time series;genetic programming;genetics;growth of genomes;artificial intelligent;microsatellites;ssr tracts;code reuse;machine learning;gpengine;repetitive elements;genetic algorithms;information entropy;unequal crossover;duplication tandemly repeated genes;artificial evolution	Biological chromosomes are replete with repetitive sequences, microsatellites, SSR tracts, ALU, and so on, in their DNA base sequences. We started looking for similar phenomena in evolutionary computation. First studies find copious repeated sequences, which can be hierarchically decomposed into shorter sequences, in programs evolved using both homologous and two-point crossover but not with headless chicken crossover or other mutations. In bloated programs the small number of effective or expressed instructions appear in both repeated and nonrepeated code. Hinting that building-blocks or code reuse may evolve in unplanned ways. Mackey–Glass chaotic time series prediction and eukaryotic protein localization (both previously used as artificial intelligence machine learning benchmarks) demonstrate the evolution of Shannon information (entropy) and lead to models capable of lossy Kolmogorov compression. Our findings with diverse benchmarks and genetic programming (GP) systems suggest this emergent phenomenon may be widespread in genetic systems.	arithmetic logic unit;artificial intelligence;benchmark (computing);code reuse;crossover (genetic algorithm);emergence;entropy (information theory);evolutionary computation;genetic algorithm;homology (biology);kolmogorov complexity;linear genetic programming;lossy compression;machine learning;shannon (unit);time series	William B. Langdon;Wolfgang Banzhaf	2005	Complex Systems		genetic programming;genetic algorithm;computer science;bioinformatics;artificial intelligence;machine learning;microsatellite;evolutionary computation;entropy	AI	-4.09069431109311	-47.481575671164606	107578
7298809320866ecda1605d454f1801f859e70189	k-nearest uphill clustering in the protein structure space		The protein structure classification problem, which is to assign a protein structure to a cluster of similar proteins, is one of the most fundamental problems in the construction and application of the protein structure space. Early manually curated protein structure classifications (e.g., SCOP and CATH) are very successful, but recently suffer the slow updating problem because of the increased throughput of newly solved protein structures. Thus, fully automatic methods to cluster proteins in the protein structure space have been designed and developed. In this study, we observed that the SCOP superfamilies are highly consistent with clustering trees representing hierarchical clustering procedures, but the tree cutting is very challenging and becomes the bottleneck of clustering accuracy. To overcome this challenge, we proposed a novel density-based K-nearest uphill clustering method that effectively eliminates noisy pairwise protein structure similarities and identifies density peaks as cluster centers. Specifically, the density peaks are identified based on K-nearest uphills (i.e., proteins with higher densities) and K-nearest neighbors. To our knowledge, this is the first attempt to apply and develop density-based clustering methods in the protein structure space. Our results show that our density-based clustering method outperforms the state-of-the-art clustering methods previously applied to the problem. Moreover, we observed that computational methods and human experts could produce highly similar clusters at high precision values, while computational methods also suggest to split some large superfamilies into smaller clusters.	cluster analysis;k-nearest neighbors algorithm	Xuefeng Cui;Xin Gao	2017	Neurocomputing	10.1016/j.neucom.2016.04.065	correlation clustering;fuzzy clustering;flame clustering;bioinformatics;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;dbscan;clustering high-dimensional data	Theory	4.489504961063213	-48.88958481612667	108269
b482d2c9da3e03d75cb8605bb299ef94bef0b9f8	constrained clustering by a novel graph-based distance transformation	databases;unsupervised learning;graph theory;cluster algorithm;shortest path;pattern clustering;copgb k means algorithm graph based distance transformation shortest path computation constraint graph clustering algorithm;clustering algorithm;copgb k means algorithm;accuracy;distance measurement;shortest path computation;pattern clustering graph theory;clustering algorithms;k means algorithm;constraint graph;distance transform;clustering algorithms databases scattering unsupervised learning computer science algorithm design and analysis large scale systems humans supervised learning;algorithm design and analysis;partitioning algorithms;graph based distance transformation	In this work we present a novel method to model instance-level constraints within a clustering algorithm. Thereby, both similarity and dissimilarity constraints can be used coevally. The proposed extension is based on a distance transformation by shortest path computations in a constraint graph. With a new technique cannot-links are consistently supported and the dissimilarity is extended to their neighbourhoods. We quantitatively compare the results achieved by our COPGB-K-Means algorithm with the state-of-the-art algorithms on standard databases and show that qualitatively good results and a fast realisation are not mutually exclusive.	algorithm;cluster analysis;computation;constrained clustering;constraint graph;database;k-means clustering;shortest path problem	Kai Rothaus;Xiaoyi Jiang	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761106	unsupervised learning;constrained clustering;combinatorics;computer science;graph theory;machine learning;pattern recognition;mathematics;cluster analysis	Vision	1.3294713374068057	-42.1533546595789	109622
0f17ec5c1115499516ab87b2939d10b7455323ee	pathway-based human disease clustering tool using self-organizing maps		Understanding how different diseases are related to one another based on their shared pathways could provide new insights into disease etiology and classification. The exploration of disease-disease associations by using a system-level biological data is made possible as the data is now publicly available via databases such as in the database maintained by Kyoto Encyclopedia of Genes and Genomes (KEGG). By being able to cluster and visualize relationships with respect to shared entities on the pathways of human diseases, researchers would be fully able to use the available pathway databases for the said scientific purposes. Thus, there is a need for an algorithm that is able to effectively visualize the topology of a multi-dimensional data. Self-Organizing Map (SOM) is a type of artificial neural network that employs unsupervised learning capable of discovering patterns in datasets by reducing multi-dimensional data to a low-dimensional representation. SOM can be used as a pre-processing step for cluster analysis via k-means clustering and hierarchical clustering. PathSOM is a software that uses self-organizing maps to create different visualizations of the underlying relationships of human diseases utilizing the available pathway data from KEGG.	algorithm;artificial neural network;cluster analysis;database;disease ontology;entity;gene regulatory network;hierarchical clustering;k-means clustering;kegg;organizing (structure);preprocessor;self-organization;self-organizing map;statistical classification;unsupervised learning	James-Andrew R. Sarmiento;Angelyn Lao;Geoffrey A. Solano	2017	2017 8th International Conference on Information, Intelligence, Systems & Applications (IISA)	10.1109/IISA.2017.8316389	cluster analysis;machine learning;biological data;unsupervised learning;artificial neural network;self-organizing map;kegg;k-means clustering;hierarchical clustering;computer science;artificial intelligence	ML	6.636745612043062	-49.40251917441911	109643
84980d1db558782bb373ce4afff954f683314bc6	extending data reliability measure to a filter approach for soft subspace clustering	cluster algorithm;pattern clustering;reliability;dk atira pure researchoutput researchoutputtypes contributiontojournal article;wrapper and filter;pattern clustering data analysis filtering theory iterative methods;k means;gene expression data;gene expression;iterative methods;clustering algorithms reliability gene expression clustering methods algorithm design and analysis data analysis;data analysis;gene expression analysis;clustering method;data reliability;subspace clustering;clustering algorithms;attribute weight;experimental evaluation;clustering methods;soft subspace clustering;algorithm design;baseline models data reliability measure filter approach data analysis tasks data attributes crisp subspace approaches optimal cluster specific dimension weights soft subspace clustering methods k means iterative disclosed cluster centers local weights published gene expression data sets;algorithm design and analysis;wrapper and filter attribute weight data reliability gene expression analysis soft subspace clustering;filtering theory	The measure of data reliability has recently proven useful for a number of data analysis tasks. This paper extends the underlying metric to a new problem of soft subspace clustering. The concept of subspace clustering has been increasingly recognized as an effective alternative to conventional algorithms (which search for clusters without differentiating the significance of different data attributes). While a large number of crisp subspace approaches have been proposed, only a handful of soft counterparts are developed with the common goal of acquiring the optimal cluster-specific dimension weights. Most soft subspace clustering methods work based on the exploitation of k-means and greatly rely on the iteratively disclosed cluster centers for the determination of local weights. Unlike such wrapper techniques, this paper presents a filter approach which is efficient and generally applicable to different types of clustering. Systematical experimental evaluations have been carried out over a collection of published gene expression data sets. The results demonstrate that the reliability-based methods generally enhance their corresponding baseline models and outperform several well-known subspace clustering algorithms.	algorithm;baseline (configuration management);cluster analysis;clustering high-dimensional data;data redundancy;evaluation;gene expression;k-means clustering;replication (computing);scientific publication;weight;statistical cluster	Tossapon Boongoen;Changjing Shang;Natthakan Iam-on;Qiang Shen	2011	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2011.2160341	correlation clustering;constrained clustering;algorithm design;data stream clustering;gene expression;fuzzy clustering;computer science;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;statistics;clustering high-dimensional data	ML	0.3251880360129528	-41.34020911231829	109874
5cd9a38095536af7a76950c50066ed2d88edf4a2	hybrid approach for intrusion detection using fuzzy association rules		Rapid development of internet and network technologies has led to considerable increase in number of attacks. Intrusion detection system is one of the important ways to achieve high security in computer networks. However, it have curse of dimensionality which tends to increase time complexity and decrease resource utilization. To improve the ability of detecting anomaly intrusions, a combined algorithm is proposed based on Weighted Fuzzy C-Mean Clustering Algorithm (WFCM) and Fuzzy logic. Decision making is performed in two stages. In the first stage, WFCM algorithm is applied to reduce the input data space. The reduced dataset is then fed to Fuzzy Logic scheme to build the fuzzy sets, membership function and the rules that decide whether an instance represents an anomaly or not.		Samira Douzi;Ibtissam Benchaji;Bouabid El Ouahidi	2018	2018 2nd Cyber Security in Networking Conference (CSNet)	10.1109/CSNET.2018.8602882	fuzzy logic;time complexity;data mining;fuzzy set;cluster analysis;anomaly detection;association rule learning;intrusion detection system;computer science;membership function	Metrics	8.774671507602726	-38.55186233424399	109928
e6752ab19e7471936595ce563c6960a4de1f4fc4	macs: multi-attribute co-clusters with high correlation information	pattern mining;real world application;high dimensional data;mutual information	In many real-world applications that analyze correlations between two groups of diverse entities, each group of entities can be characterized by multiple attributes. As such, there is a need to co-cluster  multiple  attributes' values into pairs of highly correlated clusters. We denote this co-clustering problem as the  multi-attribute co-clustering  problem. In this paper, we introduce a generalization of the mutual information between two attributes into mutual information between two attribute sets. The generalized formula enables us to use  correlation information  to discover  multi-attribute co-clusters (MACs)  . We develop a novel algorithm MACminer to mine MACs with high correlation information from datasets. We demonstrate the mining efficiency of MACminer in datasets with multiple attributes, and show that MACs with high correlation information have higher classification and predictive power, as compared to MACs generated by alternative high-dimensional data clustering and pattern mining techniques.		Kelvin Sim;Vivekanand Gopalkrishnan;Hon Nian Chua;See-Kiong Ng	2009		10.1007/978-3-642-04174-7_26	computer science;data science;machine learning;data mining;mathematics;mutual information;statistics;clustering high-dimensional data	HCI	-1.1827023464791089	-42.41169984742529	109940
4114facdae650f41368d28d688df9fb3aa5dc509	bayesian network decomposition for modeling breast cancer detection	breast cancer detection;bayesian network;part of book or chapter of book	The automated differentiation between benign and malignant abnormalities is a difficult problem in the breast cancer domain. While previous studies consider a single Bayesian network approach, in this paper we propose a novel perspective based on Bayesian network decomposition. We consider three methods that allow for different (levels of) network topological or structural decomposition. Through examples, we demonstrate some advantages of Bayesian network decomposition for the problem at hand: (i) natural and more intuitive representation of breast abnormalities and their features (ii) compact representation and efficient manipulation of large conditional probability tables, and (iii) a possible improvement in the knowledge acquisition and representation processes.	bayesian network	Marina Velikova;Nivea de Carvalho Ferreira;Peter J. F. Lucas	2007		10.1007/978-3-540-73599-1_47	variable-order bayesian network;computer science;artificial intelligence;machine learning;bayesian network;data mining;dynamic bayesian network	Vision	4.132291021367619	-44.98029919229336	109997
3eeb6ec9ca4aec2cab3e372275ec3d6715456bb0	a high performance profile-biomarker diagnosis for mass spectral profiles	simulation and modeling;carcinoma hepatocellular;support vector machines;systems biology;physiological cellular and medical topics;computational biology bioinformatics;discriminant analysis;spectrometry mass matrix assisted laser desorption ionization;liver neoplasms;principal component analysis;reproducibility of results;artificial intelligence;algorithms;humans;proteomics;computational biology;gene expression profiling;fibrosis;bioinformatics	Although mass spectrometry based proteomics demonstrates an exciting promise in complex diseases diagnosis, it remains an important research field rather than an applicable clinical routine for its diagnostic accuracy and data reproducibility. Relatively less investigation has been done yet in attaining high-performance proteomic pattern classification compared with the amount of endeavours in enhancing data reproducibility. In this study, we present a novel machine learning approach to achieve a clinical level disease diagnosis for mass spectral data. We propose multi-resolution independent component analysis, a novel feature selection algorithm to tackle the large dimensionality of mass spectra, by following our local and global feature selection framework. We also develop high-performance classifiers by embedding multi-resolution independent component analysis in linear discriminant analysis and support vector machines. Our multi-resolution independent component based support vector machines not only achieve clinical level classification accuracy, but also overcome the weakness in traditional peak-selection based biomarker discovery. In addition to rigorous theoretical analysis, we demonstrate our method’s superiority by comparing it with nine state-of-the-art classification and regression algorithms on six heterogeneous mass spectral profiles. Our work not only suggests an alternative direction from machine learning to accelerate mass spectral proteomic technologies into a clinical routine by treating an input profile as a ‘profile-biomarker’, but also has positive impacts on large scale ‘omics' data mining. Related source codes and data sets can be found at: https://sites.google.com/site/heyaumbioinformatics/home/proteomics	biological markers;code;data mining;embedding;feature selection;genetic heterogeneity;independent component analysis;linear discriminant analysis;machine learning;numerous;omics;proteomics;selection algorithm;spectrometry;support vector machine	Henry Han	2011		10.1186/1752-0509-5-S2-S5	biology;support vector machine;computer science;bioinformatics;gene expression profiling;proteomics;systems biology;principal component analysis	ML	8.670055649545587	-49.43421045626384	110002
3b829e20eeefcae4d1b62f2dd68c9ae00c5bcf3e	optimizing multi-dimensional terahertz imaging analysis for colon cancer diagnosis	decision tree;neural networks;support vector machines;journal article;colon cancer;terahertz;optimization;article;r medicine	Terahertz reflection imaging (at frequencies 0.1–10 THz/10 Hz) is non-ionizing and has potential as a medical imaging technique; however, there is currently no consensus on the optimum imaging parameters to use and the procedure for data analysis. This may be holding back the progress of the technique. This article describes the use of various intelligent analysis methods to choose relevant imaging parameters and optimize the processing of terahertz data in the diagnosis of ex vivo colon cancer samples. Decision trees were used to find important parameters, and neural networks and support vector machines were used to classify the terahertz data as indicating normal or abnormal samples. This work reanalyzes the data described in Reid et al. (2011) (Physics in Medicine and Biology, 56, 4333–4353), and improves on their reported diagnostic accuracy, finding sensitivities of 90–100% and specificities of 86–90%. This optimization of the analysis of terahertz data allows certain recommendations to be suggested concerning terahertz reflection imaging of colon cancer samples. 2012 Elsevier Ltd. All rights reserved.		Leila Eadie;Caroline B. Reid;Anthony J. Fitzgerald;Vincent P. Wallace	2013	Expert Syst. Appl.	10.1016/j.eswa.2012.10.019	support vector machine;computer science;bioinformatics;terahertz radiation;artificial intelligence;machine learning;decision tree;artificial neural network	Graphics	9.138601362518676	-49.4679257763031	110069
5bf26b68c126bfdaabab9f0ca84e826fdaf65a62	local vs global interactions in clustering algorithms: advances over k-means	global information;underlying constraint;artificial data set;topology-preserving manifold;performance function;local vs;global interaction;clustering algorithm;different manner;standard k-means algorithm;different random restarts;k means	We discuss one of the shortcomings of the standard K-means algorithm - its tendency to converge to a local rather than a global optimum. This is often accommodated by means of different random restarts of the algorithm, however in this paper, we attack the problem by amending the performance function of the algorithm in such a way as to incorporate global information into the performance function. We do this in three different manners and show on artificial data sets that the resulting algorithms are less initialisation-dependent than the standard K-means algorithm. We also show how to create a family of topology-preserving manifolds using these algorithms and an underlying constraint on the positioning of the prototypes.	algorithm;cluster analysis;interaction;k-means clustering	Wesam Barbakh;Colin Fyfe	2008	KES Journal		computer science;artificial intelligence;machine learning;data mining;algorithm;k-means clustering	ML	3.1041073019639733	-41.454202168438194	110285
906053a9c5738d5f5c19a2865f4fbdd757aca0a5	a robust clustering procedure for fuzzy data	cluster algorithm;tea evaluation;performance evaluation;lr;fuzzy data;fuzzy number;blood pressure;outlier;data type;clustering method;robustness;robust clustering algorithm;similarity measure;human perception;lr type fuzzy number	In this paper we propose a robust clustering method for handling LR-type fuzzy numbers. The proposed method based on similarity measures is not necessary to specify the cluster number and initials. Several numerical examples demonstrate the effectiveness of the proposed robust clustering method, especially robust to outliers, different cluster shapes and initial guess. We then apply this algorithm to three real data sets. These are Taiwanese tea, student data and patient blood pressure data sets. Because tea evaluation comes under an expert subjective judgment for Taiwanese tea, the quality levels are ambiguity and imprecision inherent to human perception. Thus, LR-type fuzzy numbers are used to describe these quality levels. The proposed robust clustering method successfully establishes a performance evaluation system to help consumers better understand and choose Taiwanese tea. Similarly, LR-type fuzzy numbers are also used to describe data types for student and patient blood pressure data. The proposed method actually presents good clustering results for these real data sets. © 2010 Elsevier Ltd. All rights reserved.	algorithm;cluster analysis;fuzzy logic;fuzzy number;lr parser;numerical analysis;performance evaluation	Wen-Liang Hung;Miin-Shen Yang;E. Stanley Lee	2010	Computers & Mathematics with Applications	10.1016/j.camwa.2010.04.042	correlation clustering;constrained clustering;outlier;defuzzification;k-medians clustering;fuzzy clustering;data type;flame clustering;artificial intelligence;fuzzy number;blood pressure;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;perception;robustness;clustering high-dimensional data	AI	2.6046935893795213	-38.34615331197171	110772
03742c15c16de3d61e1a97cce531ecb76aeb4a55	genetic threading	genetic threading;structure pair;protein folding;genetic algorithms;dimensional conformation;algorithm performance;linear sequence;protein threading;native conformation;threading procedure;efficient search method;indirect approach;genetic algorithm paradigm;structure determination	The biological function of proteins is dependent, to a large extent, on their native three dimensional conformation. Thus, it is important to know the structure of as many proteins as possible. Since experimental methods for structure determination are very tedious, there is a significant effort to calculate the structure of a protein from its linear sequence. Direct methods of calculating structure from sequence are not available yet. Thus, an indirect approach to predict the conformation of protein, called threading, is discussed. In this approach, known structures are used as constraints, to restrict the search for the native conformation. Threading requires finding good alignments between a sequence and a structure, which is a major computational challenge and a practical bottleneck in applying threading procedures. The Genetic Algorithm paradigm, an efficient search method that is based on evolutionary ideas, is used to perform sequence to structure alignments. A proper representation is discussed in which genetic operators can be effectively implemented. The algorithm performance is tested for a set of six sequence/structure pairs. The effects of changing operators and parameters are explored and analyzed.	constraint (mathematics);function (biology);genetic algorithm;genetic operator;programming paradigm;thread (computing);threading (protein sequence)	Jacqueline Yadgari;Amihood Amir;Ron Unger	2001	Constraints	10.1023/A:1011489723652	crystallography;bioinformatics;machine learning;mathematics	Comp.	-1.2889988511141335	-50.386432768208856	110855
3aa28a4873f2a91f4f862c0affbd5ea5329a04f5	top-k similarity search over gaussian distributions based on kl-divergence	kullback leibler divergence;gaussian distributions;top k similarity search	The problem of similarity search is a crucial task in many real-world applications such as multimedia databases, data mining, and bioinformatics. In this work, we investigate the similarity search on uncertain data modeled in Gaussian distributions. By employing Kullback-Leibler divergence (KL-divergence) to measure the dissimilarity between two Gaussian distributions, our goal is to search a database for the top-k Gaussian distributions similar to a given query Gaussian distribution. Especially, we consider non-correlated Gaussian distributions, where there are no correlations between dimensions and their covariance matrices are diagonal. To support query processing, we propose two types of novel approaches utilizing the notions of rank aggregation and skyline queries. The efficiency and effectiveness of our approaches are demonstrated through a comprehensive experimental performance study.	bioinformatics;data mining;database;kullback–leibler divergence;similarity search;uncertain data	Tingting Dong;Yoshiharu Ishikawa;Chuan Xiao	2016	JIP	10.2197/ipsjjip.24.152	pattern recognition;kullback–leibler divergence;statistics	DB	-4.234923883524073	-42.450430995703904	111129
9dc57b1a995cfa4cd282309a87f36d8af9c5445b	g-optics: fast ordering density-based cluster objects using graphics processing units		Clustering is the process of forming groups or clusters of similar objects in the dataset and has been used as an important tool for many data mining applications including the web-based ones. While density-based clustering algorithms are widely adopted, their clustering result is highly sensitive to parameter values. The OPTICS algorithm presents a solution to this problem; it produces an ordering of objects that is equivalent to the clustering results for a wide range of thresholds ϵ. In this paper, we propose an algorithm named G-OPTICS to significantly improve the performance of OPTICS using a graphics processing unit (GPU). The experimental results using real and synthetic datasets demonstrated that G-OPTICS outperformed the previously fastest FOPTICS algorithm by up to 118.3 times (67.7 times on the average).	cluster analysis;computer graphics;graphics processing unit;optics algorithm	Wookey Lee;Woong-Kee Loh	2018	IJWGS	10.1504/IJWGS.2018.092583	optics algorithm;graphics processing unit;cluster analysis;graphics;parallel algorithm;computer science;optics	ML	-2.401657989567706	-40.45965952351967	111360
c9df9c747706c53dab53d0976b12d581563e38be	multi-stage filtering for improving confidence level and determining dominant clusters in clustering algorithms of gene expression data	confidence level;dominant cluster;qp physiology;gene expression;fuzzy clustering	"""A drastic improvement in the analysis of gene expression has lead to new discoveries in bioinformatics research. In order to analyse the gene expression data, fuzzy clustering algorithms are widely used. However, the resulting analyses from these specific types of algorithms may lead to confusion in hypotheses with regard to the suggestion of dominant function for genes of interest. Besides that, the current fuzzy clustering algorithms do not conduct a thorough analysis of genes with low membership values. Therefore, we present a novel computational framework called the """"multi-stage filtering-Clustering Functional Annotation"""" (msf-CluFA) for clustering gene expression data. The framework consists of four components: fuzzy c-means clustering (msf-CluFA-0), achieving dominant cluster (msf-CluFA-1), improving confidence level (msf-CluFA-2) and combination of msf-CluFA-0, msf-CluFA-1 and msf-CluFA-2 (msf-CluFA-3). By employing double filtering in msf-CluFA-1 and apriori algorithms in msf-CluFA-2, our new framework is capable of determining the dominant clusters and improving the confidence level of genes with lower membership values by means of which the unknown genes can be predicted."""	apriori algorithm;benchmark (computing);bioinformatics;bioinformatics;cluster analysis;confusion;database;fuzzy clustering;gene expression;genes, dominant;information;numerous;prg4 gene;parkinson disease;sept9 protein, human;sensitivity and specificity;septin-9;statistical cluster;z-score	Shahreen Kasim;Safaai Deris;Razib M. Othman	2013	Computers in biology and medicine	10.1016/j.compbiomed.2013.05.011	correlation clustering;gene expression;confidence interval;fuzzy clustering;computer science;bioinformatics;machine learning;data mining;cluster analysis;genetics	Comp.	6.245850560013711	-49.628993567494696	111479
a7f0a86919784412201e84297c0dbb8e1812d04e	neurohydrodynamics as a heuristic mechanism for cognitive processes in decision-making	turing machines;oracles;neural nets;decision field theory;faculty research lightning talk;turing machines biological fluid dynamics hydrodynamics neural nets neurophysiology;biological fluid dynamics;reaction diffusion equation;discrete fourier transforms decision making equations biological neural networks humans neurons mathematical model;morphogenesis;neurophysiology;turing machines decision field theory quantum hydrodynamics oracles adaptive resonance theory morphogenesis reaction diffusion equation;video;turing machine neurohydrodynamics heuristic mechanism cognitive process decision making optimal decision rules cohen grossberg neural network equation reaction diffusion process decision field theory dft mammalian brain human cognition quantum hydrodynamics quantum mechanical particle schrodinger wave equation adaptive resonance neuromodulation frontal lobes basal ganglia;quantum hydrodynamics;hydrodynamics;adaptive resonance theory;videos	We propose to model learning of optimal decision rules using a mathematically rigorous descriptive model given by a modified set of Cohen-Grossberg neural network equations with reaction-diffusion processes in an environment of uncertainty. Our theory, which we call Neurohydrodynamics, naturally arises within the framework of neural networks while utilizing the foundations of Decision Field Theory (DFT) for describing the cognitive processes of the mammalian brain in the decision-making processes. Human cognition and intelligence requires more than an algorithmic description by a formal set of rules for its operation; it must possess what Alan Turing called an uncomputable human “intuition” (oracle) as a guide for decision-making processes. We draw an analogy with an idea from Quantum Hydrodynamics, namely, that a “pilot wave” guides quantum mechanical particles along a deterministic path by stochastic “forces” naturally arising from Schrodinger's wave equation. This type of equation was also investigated by Turing, and the reaction-diffusion processes of real neurons have been shown to aid in pattern formation while exhibiting self-organization. Because searching over all courses of action is costly, in both resources and time, we seek to include a mechanism that shortcuts the decision-making processes as described by DFT. Some empirical research has determined that diffusion does occur in the cognitive processing of real human brains. We propose a model for high-level decision processes by combining diffusion with other mechanisms (e.g., adaptive resonance and neuromodulation) for the interactions between different brain regions. For dynamic decision-making tasks, the diffusion processes within certain parts of the frontal lobes and basal ganglia are assumed to interact in hierarchical networks that integrate emotion and cognition incorporating both heuristic and deliberative decision rules.	adaptive resonance theory;artificial neural network;basal (phylogenetics);cognition;computation;decision field theory;experiment;ganglia;heuristic;heuristic (computer science);high- and low-level;interaction;jargon file;keyboard shortcut;neuromodulation (medicine);oracle database;pattern formation;pilot wave;quantum hydrodynamics;quantum mechanics;requirement;resonance;self-organization;time complexity;tree network;turing	Leon C. Hardy;Dahai Liu;Daniel S. Levine	2012	The 2012 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2012.6252375	video;morphogenesis;decision field theory;computer science;turing machine;artificial intelligence;adaptive resonance theory;theoretical computer science;machine learning;quantum hydrodynamics;mathematics;neurophysiology;reaction–diffusion system;artificial neural network	ML	-3.013936195247822	-47.2162923254284	111669
f12ba0594e9d8ae7c304968676886f76efd369eb	a new possibilistic clustering method: the possibilistic k-modes	uncertainty;k modes method;clustering;possibility theory;categorical data	This paper investigates the problem of clustering data pervaded by uncertainty. Dealing with uncertainty, in particular, using clustering methods can be of great interest since it helps to make a better decision. In this paper, we combine the k-modes method within the possibility theory in order to obtain a new clustering approach for uncertain categorical data; more precisely we develop the so-called possibilistic kmodes method (PKM) allowing to deal with uncertain attribute values of objects where uncertainty is presented through possibility distributions. Experimental results show good performance on well-known benchmarks.		Asma Ammar;Zied Elouedi	2011		10.1007/978-3-642-23954-0_40	correlation clustering;constrained clustering;fuzzy clustering;artificial intelligence;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis	EDA	1.332598054299337	-39.59219507490715	111687
66e55678338bdb3a66c36a4a9bccd8fea013c730	towards better outliers detection for gene expression datasets	silicon;biology computing;cluster algorithm;gene expression datasets;clustering based outliers detection method gene expression datasets clustering algorithms k means clustering bisecting k means clustering partitioning around medoids algorithms;pattern clustering;k means;genetics;outlier detection;gene expression;distance measurement;statistical analysis biology computing genetics pattern clustering;statistical analysis;bisecting k means clustering;clustering based outliers detection method;clustering algorithms;k means algorithm;entropy;partitioning around medoids algorithms;quality measures;breast cancer;k means clustering;partitioning algorithms;gene expression clustering algorithms partitioning algorithms pattern analysis clustering methods diseases bioinformatics biomedical computing biomedical engineering data engineering	This paper compares the performance of three clustering algorithms on the task of outlier's detection. The goal is to illustrate that better clustering indicates better detection of outliers. k-means (KM), Bisecting k-means (BKM) and the partitioning around medoids (PAM) algorithms are each combined with the clustering-based outliers detection (Find CBLOF) method. Undertaken experimental results over four gene expression datasets where outliers are presented show that the clustering solutions of the PAM algorithm enable the Find CBLOF algorithm to discover more outliers than those of both the k-means and the bisecting k-means algorithms. The main reason for this is that PAM provides better clustering quality than that of the other two clustering algorithms on the tested datasets measured by external and internal quality measures.	bkm algorithm;cluster analysis;gene co-expression network;k-means clustering;k-medoids;medoid	Rasha F. Kashef;Mohamed S. Kamel	2008	2008 International Conference on Biocomputation, Bioinformatics, and Biomedical Technologies	10.1109/BIOTECHNO.2008.29	correlation clustering;anomaly detection;fuzzy clustering;computer science;bioinformatics;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;cluster analysis;k-means clustering;clustering high-dimensional data	ML	5.186894333298884	-48.19917406302833	111900
b38ac03b806a291593c51cb51818ce8e919a1a43	pattern classification - a unified view of statistical and neural approaches				Jürgen Schürmann	1996				ML	7.366372454166411	-45.7914359692551	111936
4fc97f781a7515ef0578bef280a9e62b19969ffd	nabeeco: biological network alignment with bee colony optimization algorithm	bee colony optimization;network alignment;protein protein interaction networks;graph edit distance	Motivation: A growing number of biological networks of ever increasing sizes are becoming available nowadays, making the ability to solve Network Alignment of primer importance. However, computationally the problem is hard for data sets of real-world sizes.  Results: we developed NABEECO, a novel and robust Network Alignment heuristic based on Bee Colony Optimization. We use the so-called Graph Edit Distance (GED) as optimization criterion, which is defined as the minimal amount of edge and node modifications necessary to transform one graph into another. We compare NABEECO on a set of protein-protein interaction networks to the current state of the art tool for biological networks, MI-GRAAL.  Conclusion: We present the first Bee Colony Optimization algorithm for biological Network Alignment. NABEECO, in contrast to many other tools, can be applied to all kinds of networks and allows incorporating prior knowledge about node/edge similarity, though this is not required a priori. NABEECO together with a more detailed description and all data sets used are publicly available at http://nabeeco.mpi-inf.mpg.de.	bees algorithm;biological network;graph edit distance;heuristic;mathematical optimization;primer	Rashid Ibragimov;Jan Martens;Jiong Guo;Jan Baumbach	2013		10.1145/2464576.2464600	computer science;bioinformatics;artificial intelligence;machine learning	Comp.	0.08846855984117817	-49.97647451422935	111949
21f4b9fcec984e7b514b73a094102eabec0088c3	selection of important attributes for medical diagnosis systems	data mining;machine learning;feature selection;classification accuracy;medical diagnosis;decision rule	Success of machine learning algorithms is usually dependent on a quality of a dataset they operate on. For datasets containing noisy, inadequate or irrelevant information these algorithms may produce less accurate results. Therefore a common pre-processing step in data mining domain is a selection of highly predictive attributes. In this case study we select subsets of attributes from medical data using filter feature selection algorithms. To validate the algorithms we induce decision rules from the selected subsets of attributes and compare classification accuracy on both training and test datasets. Additionally medical relevance of the selected attributes is checked with help of domain experts.		Grzegorz Ilczuk;Alicja Wakulicz-Deja	2007	Trans. Rough Sets	10.1007/978-3-540-71663-1_5	computer science;machine learning;medical diagnosis;pattern recognition;data mining;decision rule;feature selection	AI	7.761593241039475	-43.44643671728995	112022
a1d90fa757563457f537849e378aa2c6c41729d8	particle swarm classification for high dimensional data sets	databases;particle swarm;mechanisms controlling confinement particle swarm classification high dimensional data sets pso classification technique classification accuracy;wind dispersal;classification algorithms accuracy mathematical model databases training equations wind speed;pattern classification particle swarm optimisation;high dimensionality;wind dispersion machine learning classification particle swarm optimization confinement;training;pso;particle swarm classification;classification;confinement;accuracy;particle swarm optimizer;machine learning;wind speed;particle swarm optimization;high dimensional data;classification algorithms;classification technique;pattern classification;mathematical model;mechanisms controlling confinement;classification accuracy;high dimensional data sets;particle swarm optimisation;wind dispersion	This work studies the use of Particle Swarm Optimization (PSO) as a classification technique. Beyond assessing classification accuracy, it investigates the following questions: does PSO present limitations for high dimensional application domains? Is it less efficient for multi class problems? To answer the questions, an experimental set up was realized that uses three high dimensional data sets. Our results are that, depending on the mechanisms controlling confinement and dispersion in the PSO algorithm, the classification accuracy varied with the dimensionality of the data and the cardinality of the output space.	algorithm;application domain;particle swarm optimization	Nabila Nouaouria;Mounir Boukadoum	2010	2010 22nd IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2010.21	statistical classification;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;particle swarm optimization	DB	9.763215707971566	-39.43697902550324	112094
dc291b599eaeffc29190409206db79c6b3110fb7	enhancing multiclass classification in farc-hd fuzzy classifier: on the synergy between  $n$-dimensional overlap functions and decomposition strategies	overlaps multi classification one vs one fuzzy rule based classification systems aggregations;articulo artikulua;pragmatics;electronic mail;fuzzy reasoning;statistical analysis fuzzy reasoning fuzzy set theory pattern classification;fuzzy rule based classification systems;training;one vs one;info eu repo semantics article;pragmatics fuzzy reasoning proposals computational modeling vectors training electronic mail;overlaps;computational modeling;vectors;aggregations;state of the art fuzzy classifiers multiclass classification farc hd fuzzy classifier n dimensional overlap functions decomposition strategies real world classification problems bioinformatics computer vision medicine binary counterparts fuzzy association rule based classification model for high dimensional problems fuzzy classifier multiclass classification problems one versus one strategies one versus all strategies fuzzy reasoning method product t norm association degrees weighted voting ova ovo statistical analysis;multi classification;proposals	There are many real-world classification problems involving multiple classes, e.g., in bioinformatics, computer vision, or medicine. These problems are generally more difficult than their binary counterparts. In this scenario, decomposition strategies usually improve the performance of classifiers. Hence, in this paper, we aim to improve the behavior of fuzzy association rule-based classification model for high-dimensional problems (FARC-HD) fuzzy classifier in multiclass classification problems using decomposition strategies, and more specifically One-versus-One (OVO) and One-versus-All (OVA) strategies. However, when these strategies are applied on FARC-HD, a problem emerges due to the low-confidence values provided by the fuzzy reasoning method. This undesirable condition comes from the application of the product t-norm when computing the matching and association degrees, obtaining low values, which are also dependent on the number of antecedents of the fuzzy rules. As a result, robust aggregation strategies in OVO, such as the weighted voting obtain poor results with this fuzzy classifier. In order to solve these problems, we propose to adapt the inference system of FARC-HD replacing the product t-norm with overlap functions. To do so, we define n-dimensional overlap functions. The usage of these new functions allows one to obtain more adequate outputs from the base classifiers for the subsequent aggregation in OVO and OVA schemes. Furthermore, we propose a new aggregation strategy for OVO to deal with the problem of the weighted voting derived from the inappropriate confidences provided by FARC-HD for this aggregation method. The quality of our new approach is analyzed using 20 datasets and the conclusions are supported by a proper statistical analysis. In order to check the usefulness of our proposal, we carry out a comparison against some of the state-of-the-art fuzzy classifiers. Experimental results show the competitiveness of our method.	association rule learning;bioinformatics;competitive analysis (online algorithm);computer vision;fuzzy concept;inference engine;logic programming;multiclass classification;synergy;t-norm	Mikel Elkano;Mikel Galar;José Antonio Sanz;Alberto Fernández;Edurne Barrenechea Tartas;Francisco Herrera;Humberto Bustince	2015	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2014.2370677	fuzzy classification;artificial intelligence;machine learning;pattern recognition;data mining;mathematics;computational model;statistics;pragmatics	ML	9.974758269436267	-39.61611742185348	112100
b747f0410af8a9cad00e25703012747314729842	robnca: robust network component analysis for recovering transcription factor activities	matrix algebra biology genetics iterative methods lab on a chip;biology;matrix algebra;genetics;optimization estimation sparse matrices robustness algorithm design and analysis closed form solutions iterative methods;iterative methods;lab on a chip;gene regulations robust network component analysis transcription factor activity recovery tfa gene expression data tf gene control strength matrix tf activity profiles activity profiles ni nca noniterative nca fastnca connectivity matrix closed form solution robnca algorithm microarray data iterative algorithm	Motivation: Network component analysis (NCA) is an efficient method of reconstructing the transcription factor activity (TFA), which makes use of the gene expression data and prior information available about transcription factor (TF) gene regulations. Most of the contemporary algorithms either exhibit the drawback of inconsistency and poor reliability, or suffer from prohibitive computational complexity. In addition, the existing algorithms do not possess the ability to counteract the presence of outliers in the microarray data. Hence, robust and computationally efficient algorithms are needed to enable practical applications. Results: We propose ROBust Network Component Analysis (ROBNCA), a novel iterative algorithm that explicitly models the possible outliers in the microarray data. An attractive feature of the ROBNCA algorithm is the derivation of a closed form solution for estimating the connectivity matrix, which was not available in prior contributions. The ROBNCA algorithm is compared to FastNCA and the Non-iterative NCA (NI-NCA). ROBNCA estimates the TF activity profiles as well as the TF-gene control strength matrix with a much higher degree of accuracy than FastNCA and NI-NCA, irrespective of varying noise, correlation and/or amount of outliers in case of synthetic data. The ROBNCA algorithm is also tested on Saccharomyces Cerevisiae data and Eschericia coli data and it is observed to outperform the existing algorithms. The run time of the ROBNCA algorithm is comparable to that of FastNCA, and is hundreds of times faster than NI-NCA. Availability: The ROBNCA software is available at http://people.tamu.edu/ ̃amina/ROBNCA. Contact: serpedin@ece.tamu.edu	adjacency matrix;algorithm;algorithmic efficiency;computational complexity theory;computer performance;gene regulatory network;ibm notes;iterative method;mathematical optimization;microarray;next-generation secure computing base;run time (program lifecycle phase);signal-to-noise ratio;sparse matrix;synthetic data;transcription (software)	Amina Noor;Aitzaz Ahmad;Erchin Serpedin;Mohamed N. Nounou;Hazem N. Nounou	2013		10.1109/GENSIPS.2013.6735919	biology;lab-on-a-chip;computer science;bioinformatics;theoretical computer science;data mining;iterative method;genetics	Comp.	3.5199179849938655	-51.44093201632171	112881
375db8971a491fb88ea246419e88321fd7cd2f42	dna-based modelling of parallel algorithms	dna structure;parallel algorithm;dna computing;computational biology;dna sequence	The area of computational biology, is li ving a fast growth, fed with a revolution in DNA sequencing and technology of mapping. With these new technologies for manipulation of sequences, the relevance of finding eff icient solution to the so-called computer intractable problems has also grown, because many problems involved in analyzing DNA belong to this class of problems. One approach to find such solutions is to use DNA itself to perform computations, taking advantage of the massive paralleli sm involved in operations that manipulate DNA sequences. This is what is studied in the area of DNA computing. This work proposes a formal model to represent the DNA structure and the operations performed in laboratory with it. This model can be used to analyze DNA-based algorithms, as well as for simulating such algorithms in a computer. We use graph grammars, a formal specification language, to model the DNA sequences and operations.	computation;computational biology;computer simulation;correctness (computer science);dna computing;experiment;formal language;formal specification;l-system;mathematical model;np-hardness;parallel algorithm;relevance;software bug;specification language	Leonardo Vieira Cervo;Leila Ribeiro	2002			dna;formal specification;massively parallel;computation;dna computing;parallel algorithm;grammar systems theory;theoretical computer science;dna sequencing;computer science	Comp.	-2.5776531285912214	-50.266353550787066	113193
93f2ffecc251d21b4e6a191c1feec66861872341	new evolutionary approaches to high-dimensional data	classification algorithm;evolutionary search;dimensionality reduction;high dimensional data;evolutionary algorithm;dimensional reduction	High-dimensional data often threatens the performance of classification algorithms. We propose a two-step approach for dealing with high-dimensional data. In the first step, features are arranged into bins, where each bin corresponds to a much smaller sub-space of features. In the second step, classifiers are independently applied to the set of features within each sub-space, and their results are then aggregated. We consider slicing a space Rd into smaller subspaces as a multi-objective search problem, which can be solved by evolutionary algorithms. We performed a systematic evaluation using three classification algorithms on high-dimensional data.	aggregate data;evolutionary algorithm;search problem	Luis Matoso;José Felipe Júnior;Adriano M. Pereira;Adriano Veloso;Wagner Meira	2012		10.1145/2330784.2330982	mathematical optimization;computer science;machine learning;evolutionary algorithm;pattern recognition;data mining;mathematics;dimensionality reduction;clustering high-dimensional data	ML	9.611020042968361	-44.77570095974933	113207
d5a8716a94054522a0c24a9d7dd37858030a171c	optimized leaf ordering with class labels for hierarchical clustering	dynamic programming;hierarchical clustering;dendrogram;leaf ordering;biomedical data	Hierarchical clustering is extensively used in the bioinformatics community to analyze biomedical data. These data are often tagged with class labels, as e.g. disease subtypes or gene ontology (GO) terms. Heatmaps in connection with dendrograms are the common standard to visualize results of hierarchical clustering. The heatmap can be enriched by an additional color bar at the side, indicating for each instance in the data set to which class it belongs. In the ideal case, when the clustering matches perfectly with the classes, one would expect that instances from the same class cluster together and the color bar consists of well-separated color blocks without frequent alteration of colors (classes). But even in the case when instances from the same class cluster perfectly together, the dendrogram might not reflect this important aspect due to the fact that its representation is not unique. In this paper, we propose a leaf ordering algorithm for the dendrogram that preserving the hierarchical clustering result tries to group instances from the same class together. It is based on the concept of dynamic programming which can efficiently compute the optimal or nearly optimal order, consistent with the structure of the tree.	algorithm;bioinformatics;class;cluster analysis;color;computation;data element;dendrogram;dynamic programming;emoticon;expect;experiment;fo (complexity);fr 139317;fallopia multiflora plant;gallium;gene ontology;heatmap;heuristic;heuristics;hierarchical clustering;loss function;nl (complexity);numerical aperture;optimization problem;r language;sodium;software release life cycle;subtype (attribute);time complexity;tracer;statistical cluster	Natalia Novoselova;Junxi Wang;Frank Klawonn	2015	Journal of bioinformatics and computational biology	10.1142/S0219720015500122	complete-linkage clustering;constrained clustering;fuzzy clustering;computer science;bioinformatics;canopy clustering algorithm;machine learning;dynamic programming;pattern recognition;data mining;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;dendrogram;statistics;hierarchical clustering of networks	ML	4.092410583774025	-49.25311958871988	113595
8ae69339429b3b50cb44d819d6756eefbdcb54ea	gene expression studies with dgl global optimization for the molecular classification of colon cancer	patient diagnosis;microarray data;optimisation;global solution;tumours cancer genetics molecular biophysics optimisation patient diagnosis;cancer;tumours;gene expression data;gene expression colon cancer neoplasms large scale systems data analysis clustering algorithms biomedical engineering biomedical informatics biomedical computing;null;genetics;colon cancer;gene expression;tumors gene expression dgl global optimization molecular classification colon cancer;dgl global optimization;molecular biophysics;clinical practice;tumors;molecular classification;global optimization;cancer classification	The study attempts to analyze multiple sets of genes simultaneously, for an overall global solution to the gene's joint discriminative ability in assigning tumors to known classes. With the workable concepts and methodologies described here an accurate classification of the type and seriousness of cancer can be made. The colon cancer microarray data can be classified 100% correctly without previous knowledge of their classes. The classification processes are automated after the gene expression data being inputted. Instead of examining a single gene at a time, the DGL method can find the global optimum solutions and construct a multi-subsets pyramidal hierarchy class predictor containing up to 23 gene subsets based on a given microarray gene expression data collection within a period of several hours. An automatically derived class predictor makes the reliable cancer classification and accurate tumor diagnosis in clinical practice possible.	colon classification;global optimization;kerrison predictor;microarray	Dongguang Li	2008	2008 International Conference on BioMedical Engineering and Informatics	10.1109/BMEI.2008.359	biology;microarray analysis techniques;gene expression;pathology;bioinformatics;genetics;global optimization;cancer;molecular biophysics	Comp.	8.489131927770728	-50.77877795022058	113866
0602ce2b21732d94b4736264807890bd5744cfcc	ensemble clustering using semidefinite programming	health research;uk clinical guidelines;biological patents;image segmentation;europe pubmed central;citation search;optimization problem;uk phd theses thesis;polynomial time;life sciences;nonlinear optimization;uk research reports;medical journals;europe pmc;biomedical research;semidefinite program;bioinformatics	We consider the ensemble clustering problem where the task is to 'aggregate' multiple clustering solutions into a single consolidated clustering that maximizes the shared information among given clustering solutions. We obtain several new results for this problem. First, we note that the notion of agreement under such circumstances can be better captured using an agreement measure based on a 2D string encoding rather than voting strategy based methods proposed in literature. Using this generalization, we first derive a nonlinear optimization model to maximize the new agreement measure. We then show that our optimization problem can be transformed into a strict 0-1 Semidefinite Program (SDP) via novel convexification techniques which can subsequently be relaxed to a polynomial time solvable SDP. Our experiments indicate improvements not only in terms of the proposed agreement measure but also the existing agreement measures based on voting strategies. We discuss evaluations on clustering and image segmentation databases.	aggregate data;algorithm;cluster analysis;convex hull;database;decision problem;evaluation;experiment;generalization (psychology);image segmentation;linear programming;mathematical optimization;nonlinear programming;nonlinear system;optimization problem;partition problem;relaxation;semidefinite programming;simulation;solutions;time complexity;biologic segmentation;statistical cluster	Vikas Singh;Lopamudra Mukherjee;Jiming Peng;Jinhui Xu	2007	Advances in neural information processing systems		time complexity;correlation clustering;optimization problem;constrained clustering;mathematical optimization;data stream clustering;fuzzy clustering;nonlinear programming;computer science;data science;machine learning;cure data clustering algorithm;data mining;mathematics;image segmentation;cluster analysis;statistics	ML	4.296730119990404	-40.27445140032311	113959
88aac30d2ee71c4e54baff86f59a173487486f02	volume visualization for exploration of population trends in two-dimensional gel electrophoresis protein data	biology computing;2d gel electrophoresis protein data;statistical measures;standards;biological variable;volume visualization method;transfer functions;sorting;spearman rank correlation;standard deviation;electrophoresis;data analysis volume visualization method population exploration 2d gel electrophoresis protein data proteomics protein content biological sample patients culture cell cultures visual analysis approach biological variable protein pattern dynamic slice sorting spearman rank correlation standard deviation statistical transfer function explorer filter statistical measures;dynamic slice sorting;statistical transfer function;proteins correlation data visualization transfer functions visualization sorting standards;data visualisation;data analysis;visualization;patients culture;proteins;explorer filter;proteomics biology computing cellular biophysics data analysis data visualisation electrophoresis proteins;data visualization;protein content;correlation;protein pattern;cell cultures;proteomics;biological sample;population exploration;cellular biophysics;visual analysis approach	We propose a visualization method for exploring and analyzing data from two-dimensional (2D) gel electrophoresis, a frequently used laboratory technique in proteomics for characterizing and studying protein content in biological samples from patients or cell cultures. We show how 2D gel electrophoresis data from a population can be treated as a volume with no preferred slice ordering, and present a visual analysis approach to investigate the relation between an external biological variable such as subject age and the protein patterns based on dynamic slice sorting. We calculate the Spearman rank correlation as a measure of the relation, and use this together with standard deviation as statistical transfer functions to let the explorer filter the data. We also show a method for embedding the selected external variable distribution and the statistical measures in the volume, providing a statistical context for the pattern exploration. Finally we show the applicability of our method in an example of analysis of data from real populations.	cluster analysis;external variable;population;proteomics;scientific visualization;sorting;supervised learning	Ola Kristoffer Øye;Dag Magne Ulvang;Katarina M. Jørgensen;Sigrun M. Hjelle;Bjørn Tore Gjertsen	2012	2012 25th IEEE International Symposium on Computer-Based Medical Systems (CBMS)	10.1109/CBMS.2012.6266326	electrophoresis;visualization;biological specimen;computer science;bioinformatics;sorting;data science;spearman's rank correlation coefficient;transfer function;data analysis;standard deviation;correlation;cell culture;data visualization;statistics	Visualization	5.981862099331267	-51.450253497572646	113969
7d8cf81487cfb9e9d15058b6b8efae04fdf818e0	semi-supervised hierarchical clustering	constrained optimization;hierarchical clustering;pattern clustering;pattern clustering constraint handling matrix algebra;measurement clustering methods optimization clustering algorithms vectors educational institutions usa councils;triple wise relative constraints;matrix algebra;dissimilarity matrix semisupervised hierarchical clustering partitional clustering methods must link constraints cannot link constraints triple wise relative constraints ultra metric transformation;clustering method;triple wise relative constraints hierarchical clustering semi supervised clustering;constraint handling;semi supervised clustering;knowledge base	Semi-supervised clustering (i.e., clustering with knowledge-based constraints) has emerged as an important variant of the traditional clustering paradigms. However, most existing semi-supervised clustering algorithms are designed for partitional clustering methods and few research efforts have been reported on semi-supervised hierarchical clustering methods. In addition, current semi-supervised clustering methods have been focused on the use of background information in the form of instance level must-link and cannot-link constraints, which are not suitable for hierarchical clustering where data objects are linked over different hierarchy levels. In this paper, we propose a novel semi-supervised hierarchical clustering framework based on ultra-metric dendrogram distance. The proposed framework is able to incorporate triple-wise relative constraints. We establish the connection between hierarchical clustering and ultra-metric transformation of dissimilarity matrix and propose two techniques (the constrained optimization technique and the transitive dissimilarity based technique) for semi-supervised hierarchical clustering. Experimental results demonstrate the effectiveness and the efficiency of our proposed methods.	algorithm;cluster analysis;constrained clustering;constrained optimization;dendrogram;distance matrix;hierarchical clustering;mathematical optimization;semi-supervised learning;semiconductor industry;transitive reduction	Li Zheng;Tao Li	2011	2011 IEEE 11th International Conference on Data Mining	10.1109/ICDM.2011.130	correlation clustering;constrained clustering;knowledge base;constrained optimization;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;dendrogram;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	DB	1.861785220445516	-43.63477978700486	114219
4d84212efb226b2f2e5a27bcb8f1c2141579a6fc	the classification of cancer based on dna microarray data that uses diverse ensemble genetic programming	diversity;genetic program;genetic programming;classification;ensemble;dna microarray data	OBJECT The classification of cancer based on gene expression data is one of the most important procedures in bioinformatics. In order to obtain highly accurate results, ensemble approaches have been applied when classifying DNA microarray data. Diversity is very important in these ensemble approaches, but it is difficult to apply conventional diversity measures when there are only a few training samples available. Key issues that need to be addressed under such circumstances are the development of a new ensemble approach that can enhance the successful classification of these datasets.   MATERIALS AND METHODS An effective ensemble approach that does use diversity in genetic programming is proposed. This diversity is measured by comparing the structure of the classification rules instead of output-based diversity estimating.   RESULTS Experiments performed on common gene expression datasets (such as lymphoma cancer dataset, lung cancer dataset and ovarian cancer dataset) demonstrate the performance of the proposed method in relation to the conventional approaches.   CONCLUSION Diversity measured by comparing the structure of the classification rules obtained by genetic programming is useful to improve the performance of the ensemble classifier.	avg;bioinformatics;carcinoma of lung;classification;dna microarray format;ensemble learning;estimated;gene expression;genetic programming;lymphoma;lymphoma, non-hodgkin;machine learning;malignant neoplasm of ovary;non-small cell lung carcinoma;rule (guideline);stap2 gene;silo (dataset);digital tomosynthesis;ovarian neoplasm	Jin-Hyuk Hong;Sung-Bae Cho	2006	Artificial intelligence in medicine	10.1016/j.artmed.2005.06.002	ensembl;genetic programming;biological classification;computer science;bioinformatics;artificial intelligence;machine learning;data mining	Comp.	8.432623596241324	-48.96173315754511	114673
19a00895ca3c44bbf6fad34cb1284a1080471669	a kernel svm algorithm to detect mislabeled microarrays in human cancer samples	dna;support vector machines kernel colon labeling breast cancer bioinformatics;support vector machines;cancer;information filtering;genetics;medical computing;feature extraction;support vector machines cancer dna feature extraction genetics information filtering lab on a chip medical computing pattern classification;pattern classification;labeling errors kernel svm algorithm mislabeled microarrays detection human cancer samples dna microarrays cancer types identification gene expression profiles filtering algorithm one class svm classification mislabelled samples detection feature space dissimilarities kernel outliers detection cancer microarray datasets;lab on a chip	DNA Microarrays have been successfully applied to the identification of different cancer types considering the gene expression profiles. However, previous studies have shown that labeling errors are not uncommon in microarray studies. In this case, the training set may contain mislabelled examples that may lead the classifier to poor performance. In this paper we propose a new filtering algorithm based on one-class SVM classification to detect mislabelled samples. To this aim, samples and labels are mapped together to feature space using the kernel of dissimilarities. Next, outliers are detected via one-class classification. Mislabeled samples and outliers in input space can be separated comparing the outliers obtained in input and feature spaces. The algorithm proposed has been tested using several complex cancer microarray datasets in which some samples are mislabelled according to the literature. The experimental results suggest that our algorithm is effective detecting labeling errors and compares favorably with a standard technique such as simple SVM.	algorithm;cross-validation (statistics);dna microarray;feature vector;kernel (operating system);one-class classification;sensor;statistical classification;test set	Manuel Martín-Merino	2013	13th IEEE International Conference on BioInformatics and BioEngineering	10.1109/BIBE.2013.6701579	biology;support vector machine;lab-on-a-chip;feature extraction;computer science;bioinformatics;pattern recognition;data mining;genetics;dna;cancer	Visualization	8.419197914899737	-47.995140519330356	114870
0c39aeeecc876703953dc13585b25df97d6f826b	on the impact of post-clustering phase in multi-way spectral partitioning	muti way partitioning;clustering;spectral partitioning	Spectral clustering is one of the most popular modern graph clustering techniques in machine learning. By using the eigenvalue analysis, spectral methods partition the given set of points into number of disjoint groups. Spectral methods are very useful in determining non-convex shaped clusters, identifying such clusters is not trivial for many traditional clustering methods including hierarchical and partitional methods. Spectral clustering may be carried out either as recursive bi-partitioning using fiedler vector second eigenvector or as muti-way partitioning using first k eigenvectors, where k is the number of clusters. Although spectral methods are widely discussed, there has been a little attention on which post-clustering algorithm for eg. K-means should be used in multi-way spectral partitioning. This motivated us to carry out an experimental study on the influence of post-clustering phase in spectral methods. We consider three clustering algorithms namely K-means, average linkage and FCM. Our study shows that the results of multi-way spectral partitioning strongly depends on the post-clustering algorithm.	cluster analysis	R. Jothi;Sraban Kumar Mohanty;Aparajita Ojha	2015		10.1007/978-3-319-26832-3_16	correlation clustering;mathematical optimization;combinatorics;fuzzy clustering;canopy clustering algorithm;machine learning;mathematics;cluster analysis;single-linkage clustering	Theory	0.6457075411540079	-41.927073147638886	115107
ca531a46c4a0d7bfe54ac714ea862ff65470efd2	document clustering algorithm based on tree-structured growing self-organizing feature map *	document clustering;hierarchical clustering;cluster algorithm;text mining;self organized feature map;tree structure;self organized map;spreading factor;neural network	Document clustering is widely studied in text mining. In this paper, document clustering algorithm based on Tree-Structured Growing Self- organizing Feature Map (TGSOM) is presented as an extended version of the clustering algorithm of Self-organizing Map (SOM) in neural network, which has a dynamic tree-structure generated during the training process. TGSOM 's growth speed can be controlled through the function of the Spread Factor (SF), and the precision of clustering results is different because of the difference value of SF. The user can get the hierarchical clustering results through chang- ing the size of SF in different steps during clustering.		Xiaoshen Zheng;Wenling Liu;Pilian He;Weidi Dai	2004		10.1007/978-3-540-28647-9_138	correlation clustering;data stream clustering;text mining;document clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;tree structure;cluster analysis;single-linkage clustering;brown clustering;biclustering;artificial neural network;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	EDA	2.6805658021960093	-42.91517410774254	115282
65416316f2d37f016ed685bfcd0f502c25b8bab9	coercion: a distributed clustering algorithm for categorical data	sqeezer;pattern clustering;data mining;pattern clustering data mining;clustering algorithms servers algorithm design and analysis distributed databases partitioning algorithms rocks presses;data mining categorical data sqeezer clustering distributed;clustering;squeezer coercion distributed clustering algorithm categorical data data mining;distributed;categorical data	Clustering is an important technology in data mining. Squeezer is one such clustering algorithm for categorical data and it is more efficient than most existing algorithms for categorical data. But Squeezer is time consuming for very large datasets which are distributed in different servers. Thus, we employ the distributed thinking to improve Squeezer and a distributed algorithm for categorical data called Coercion is proposed in this paper. In order to present detailed complexity results for Coercion, we also conduct an experimental study with standard as well as synthetic data sets to demonstrate the effectiveness of the new algorithm.	categorical variable;cluster analysis;data mining;distributed algorithm;experiment;human-based computation;synthetic data	Bin Wang;Yang Zhou;Xinhong Hei	2013	2013 Ninth International Conference on Computational Intelligence and Security	10.1109/CIS.2013.149	correlation clustering;constrained clustering;data stream clustering;categorical variable;fuzzy clustering;computer science;data science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;database;cluster analysis;biclustering;affinity propagation;clustering high-dimensional data	DB	-1.5645859528245338	-39.37947365867035	115287
db513c2026e6e1690db5419553b935119f59dc24	self organized swarms for cluster preserving projections of high-dimensional data	high dimensional data;self organization	A new approach for topographic mapping, called Swarm-Organized Projection (SOP) is presented. SOP has been inspired by swarm intelligence methods for clustering and is similar to Curvilinear Component Analysis (CCA) and SOM. In contrast to the latter the choice of critical parameters is substituted by selforganization. On several crucial benchmark data sets it is demonstrated that SOP outperforms many other projection methods. SOP produces coherent clusters even for complex entangled high dimensional cluster structures. For a nontrivial dataset on protein DNA sequence Multi Dimensional Scaling (MDS) and CCA fail to represent the clusters in the data, although the clusters are clearly defined. With SOP the correct clusters in the data could be easily detected.	benchmark (computing);cluster analysis;coherence (physics);multidimensional scaling;swarm intelligence;topography	Alfred Ultsch;Lutz Herrmann	2010	ECEASST	10.14279/tuj.eceasst.27.387	self-organization;computer science;theoretical computer science;machine learning;data mining;clustering high-dimensional data	ML	1.4216091978410756	-42.24000164697413	115338
d22e607e9c8182af41d0382687804cb8ce365c73	evolving two-dimensional cellular automata to perform density classification: a report on work in progress	evolutionary computation;bepress selected works;cellular automata genetic algorithms pattern recognition systems;information processing;genetic algorithm;work in progress;cellular automata;evolutionary computing	We present results from experiments in which a genetic algorithm (GA) is used to evolve 2D cellular automata (CA) to perform a particular computational task (“density classification”) that requires globally coordinated information processing. The results are similar to that of earlier work on evolving 1D CAs. The behavior of the evolved 2D CAs is analyzed, and their performance is compared with that of several hand-designed 2D CAs.	automata theory;cellular automaton	Francisco Jiménez-Morales;James P. Crutchfield;Melanie Mitchell	2001	Parallel Computing	10.1016/S0167-8191(00)00078-8	genetic algorithm;growcut algorithm;computer science;artificial intelligence;machine learning;work in process;algorithm;evolutionary computation	Metrics	-3.705163383835748	-47.99894213874362	115455
9974d2a4b469e1d61e4bcea0e1128232bb55bd57	agregación de medidas de similitud para la detección de ortólogos: validación con medidas basadas en la teoría de conjuntos aproximados	rough set theory	This paper presents a novel algorithm for ortholog detection that involves the aggregation of similarity measures characterizing the relationship between gene pairs of two genomes. The measures are based on the alignment score, the length of the sequences, the membership in the conserved regions as well as on the protein physicochemical profile. The clustering step over the similarity bipartite graph is performed by using the Markov clustering algorithm (MCL). A new ortholog assignment policy is applied over the homology groups obtained in the graph clustering. The classification results are validated with the Saccharomyces Cerevisiae and the Schizosaccharomyces Pombe genomes with the ortholog list of the INPARANOID 7.0 algorithm with the Adjusted Rand Index (ARI) external measure. Other validation measures based on the rough set theory are applied to calculate the quality of the classification dealing with class imbalance.	algorithm;cluster analysis;homology (biology);inparanoid;linear algebra;markov chain;monte carlo localization;naruto shippuden: clash of ninja revolution 3;rand index;rough set;set theory	Reinier Millo Sánchez;Deborah Galpert Cañizares;Gladys Casas Cardoso;Ricardo del Corazón Grau-Ábalo;Leticia Arco;María Matilde García Lorenzo;Miguel Ángel Fernández Marin	2014	Computación y Sistemas		combinatorics;rough set;computer science;bioinformatics;machine learning;mathematics;algorithm	Comp.	6.098234515983583	-48.61810402747332	115624
13dda0f8c3bb3f38a51e62607921451a2ef3d6ec	mining class contrast functions by gene expression programming	data mining;contrast mining;gene expression programming	Finding functions whose accuracies change significantly between two classes is an interesting work. In this paper, this kind of functions is defined as class contrast functions. As Gene Expression Programming (GEP) can discover essential relations from data and express them mathematically, it is desirable to apply GEP to mining such class contrast functions from data. The main contributions of this paper include: (1) proposing a new data mining task – class contrast function mining, (2) designing a GEP based method to find class contrast functions, (3) presenting several strategies for finding multiple class contrast functions in data, (4) giving an extensive performance study on both synthetic and real world datasets. The experimental results show that the proposed methods are effective. Several class contrast functions are discovered from the real world datasets. Some potential works on class contrast function mining are discussed based on the experimental results.	data mining;gene expression programming;synthetic intelligence	Lei Duan;Changjie Tang;Liang Tang;Tianqing Zhang;Jie Zuo	2009		10.1007/978-3-642-03348-3_14	computer science;artificial intelligence;machine learning;data mining;gene expression programming	ML	7.471361238519612	-43.746815917974104	116012
3e2f9fd98f6b79d8c2e5978a9a6493dc235a1109	fuzzy indication of reliability in metagenomics ngs data analysis		NGS data processing in metagenomics studies has to deal with noisy data that can contain a large amount of reading errors which are difficult to detect and account for. This work introduces a fuzzy indicator of reliability technique to facilitate solutions to this problem. It includes modified Hamming and Levenshtein distance functions that are aimed to be used as drop-in replacements in NGS analysis procedures which rely on distances, such as phylogenetic tree construction. The distances utilise fuzzy sets of reliable bases or an equivalent fuzzy logic, potentially aggregating multiple sources of base reliability.	artificial neural network;communications satellite;experiment;fuzzy logic;fuzzy set;levenshtein distance;metagenomics;phylogenetic tree;phylogenetics;signal-to-noise ratio;window function	Milko Krachunov;Dimitar Vassilev;Maria Nisheva-Pavlova;Ognyan Kulev;Valeriya Simeonova;Vladimir T. Dimitrov	2015		10.1016/j.procs.2015.05.448	bioinformatics;machine learning;data mining;mathematics	ML	4.1722057785047415	-49.86131833738746	116114
4b222ec0a8f7d016e5702ae79ec20707fa9a54f9	u*c: self-organized clustering with emergent feature maps	hierarchical clustering;k means;high dimensional data;self organization	A new clustering algorithm based on grid projections is proposed. This algorithm, called U*C, uses distance information together with density structures. The number of clusters is determined automatically. The validity of the clusters found can be judged by the U*-Matrix visualization on top of the grid. A U*-Matrix gives a combined visualization of distance and density structures of a high dimensional data set. For a set of critical clustering problems it is demonstrated that U*C clustering is superior to standard clustering algorithms such as Kmeans and hierarchical clusterings.	algorithm;cluster analysis;correctness (computer science);emergent;k-means clustering;u-matrix	Alfred Ultsch	2005			correlation clustering;complete-linkage clustering;cluster analysis;artificial intelligence;single-linkage clustering;hierarchical clustering;k-medians clustering;brown clustering;cure data clustering algorithm;pattern recognition;mathematics	ML	1.6901124514800363	-41.7796332240749	116117
012f6d17279fadaedf68ccd0f55f735d8beeedcf	approximate protein folding in oz through frequency analysis	mathematical model;protein folding;frequency analysis;amino acid	The protein folding problem consists in determining the spatial shape of a protein once the linear sequence of its amino acids is known. Its solution uses mathematical models of the protein, the HP model being one of the simplest. Despite its simplicity, the problem of protein folding inside the HP model is still NP-complete. In this paper we rephrase the HP model in a way that allows us to devise a new heuristic to burst the performance of automatic protein folding inside the HP model. Our implementation of protein folding in Mozart/Oz shows that our heuristic is very effective in practice and features a high level of precision for the solution it computes.	algorithm;fastest;frequency analysis;heuristic;high-level programming language;mathematical model;mathematical optimization;np-completeness;protein structure prediction	S. Bozzoli;Fausto Spoto;Agostino Dovier	2003			frequency analysis;mathematical optimization;protein folding;mathematical model;heuristic;computer science;bioinformatics	Comp.	-2.1735851625928766	-49.49573273924364	116333
381958a0c2d644f7856a535615082f04296c54a1	learning phenotype structure using sequence model	microarray data;molecular biophysics biology computing computational complexity data mining learning artificial intelligence;biology computing;expression pattern;finder algorithm;data mining;journal;gene expression data sets;computational complexity;biological significance;molecular biophysics;expression signature;learning artificial intelligence;g sequence model;biological significance phenotype structure learning sequence model microarray technologies microarray data analysis expression pattern expression signature g sequence model gene ordered expression values np complete problem finder algorithm phenotype structure discovery trivial g sequences identification pruning strategies gene expression data sets statistical significance;np complete problem;microarray data data mining bioinformatics;bioinformatics	Advanced microarray technologies have enabled to simultaneously monitor the expression levels of all genes. An important problem in microarray data analysis is to discover phenotype structures. The goal is to 1) find groups of samples corresponding to different phenotypes (such as disease or normal), and 2) for each group of samples, find the representative expression pattern or signature that distinguishes this group from others. Some methods have been proposed for this issue, however, a common drawback is that the identified signatures often include a large number of genes but with low discriminative power. In this paper, we propose a g*-sequence model to address this limitation, where the ordered expression values among genes are profitably utilized. Compared with the existing methods, the proposed sequence model is more robust to noise and allows to discover the signatures with more discriminative power using fewer genes. This is important for the subsequent analysis by the biologists. We prove that the problem of phenotype structure discovery is NP-complete. An efficient algorithm, FINDER, is developed, which includes three steps: 1) trivial g*-sequences identifying, 2) phenotype structure discovery, and 3) refinement. Effective pruning strategies are developed to further improve the efficiency. We evaluate the performance of FINDER and the existing methods using both synthetic and real gene expression data sets. Extensive experimental results show that FINDER dramatically improves the accuracy of the phenotype structures discovered (in terms of both statistical and biological significance) and detects signatures with high discriminative power. Moreover, it is orders of magnitude faster than other alternatives.	algorithm;antivirus software;digital signature;electronic signature;microarray;np-completeness;refinement (computing);synthetic data;synthetic intelligence;type signature	Yuhai Zhao;Guoren Wang;Xiang Zhang;Jeffrey Xu Yu;Zhanghui Wang	2014	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2013.31	microarray analysis techniques;np-complete;computer science;bioinformatics;machine learning;data mining;computational complexity theory;molecular biophysics	DB	4.788848568236073	-49.495855238555336	116340
41d4bd62a9ee2a931583fd23d1a49adb658867cf	estimation of errors introduced by confocal imaging into the data on segmentation gene expression in drosophila	animals;blastoderm;invertebrata;dato;microscopy confocal;arthropoda;data;gen;segmentation;image processing computer assisted;drosophilidae;gene expression;expression genique;drosophila;estimation erreur;donnee;error estimation;erreur estimation;insecta;diptera;drosophila melanogaster;imaging;estimacion error;gene;error estimacion;formation image;regression analysis;formacion imagen;estimation error;expresion genetica;segmentacion;gene expression profiling;genes insect	MOTIVATION Currently the confocal scanning microscopy of fluorescently tagged molecules is extensively employed to acquire quantitative data on gene expression at cellular resolution. Following this approach, we generated a large dataset on the expression of segmentation genes in the Drosophila blastoderm, that is widely used in systems biology studies. As data accuracy is of critical importance for the success of studies in this field, we took a shot to evaluate possible errors introduced in the data by acquisition and processing methods. This article deals with errors introduced by confocal microscope.   RESULTS In confocal imaging, the inevitable photon noise is commonly reduced by the averaging of multiple frames. The averaging may introduce errors into the data, if single frames are clipped by microscope hardware. A method based on censoring technique is used to estimate and correct this type of errors. Additional source of errors is the quantification of blurred images. To estimate and correct these errors, the Richardson-Lucy deconvolution method was modified to provide the higher accuracy of data read off from blurred images of the Drosophila blastoderm. We have found that the sizes of errors introduced by confocal imaging make up approximately 5-7% of the mean intensity values and do not disguise the dynamic behavior and characteristic features of gene expression patterns. We also defined a range of microscope parameters for the acquisition of sufficiently accurate data.   AVAILABILITY http://urchin.spbcas.ru/downloads/step/step.htm	blastoderm;censor;frame (physical object);gene expression;gene co-expression network;microscope device component;photons;quantitation;richardson number;richardson–lucy deconvolution;shot noise;silo (dataset);systems biology;tracer;biologic segmentation	Ekaterina M. Myasnikova;Svetlana Surkova;Lena Panok;Maria Samsonova;John Reinitz	2009	Bioinformatics	10.1093/bioinformatics/btn620	medical imaging;blastoderm;biology;computer vision;gene expression;bioinformatics;gene;gene expression profiling;segmentation;genetics;regression analysis;data	Comp.	4.112245393880391	-51.66375823167597	116575
1cf2056fc80d8cc19966e176c963d1958f70fa83	amino acid interaction network prediction using multi-objective optimization		Protein can be represented by amino acid interactio n network. This network is a graph whose vertices are the proteins amino acids and whose edges are th e interactions between them. This interaction network is the first step of proteins three-dimensi o al structure prediction. In this paper we present a multi-objective evolutionary algorithm for interact ion prediction and ant colony probabilistic optimization algorithm is used to confirm the inter action.	ant colony optimization algorithms;data structure;evolutionary algorithm;genetic algorithm;graph (discrete mathematics);graph theory;interaction network;mathematical optimization;multi-objective optimization;parallel algorithm	Md. Shiplu Hawlader;Saifuddin Md. Tareeq	2013	CoRR	10.5121/csit.2014.4113	bioinformatics;machine learning;pattern recognition	ML	-1.2011429202349686	-49.291295858192434	116805
f3a9d831476a379fed24cde6589245822c15f620	oligonucleotide microarray probe correction by fixedpoint ica algorithm	oligonucleotide microarray;independent component analysis;genetics;gene expression;signal processing;differential expression;hybridization;microarray;point of view	Oligonucleotide Microarrays have become powerful tools in genetics, as they serve as parallel scanning mechanisms to detect the presence of genes using test probes. The detection of each gene depends on the multichannel differential expression of perfectly matched segments against mismatched ones. This methodology posse some interesting problems under the point of view of Genomic Signal Processing, as test probes express themselves in rather different patterns, not showing proportional expression levels for most of the segment pairs, as it would be expected. The method proposed in this paper consists in isolating gene expressions showing unexpected behavior using independent component analysis.	algorithm;dna microarray;independent computing architecture	Raul Malutan;Pedro Gómez Vilda;Monica Borda	2009		10.1007/978-3-642-02481-8_150	independent component analysis;orbital hybridisation;gene expression;computer science;bioinformatics;signal processing;microarray	ML	4.2071875131684395	-50.82360844579716	116872
77cf2d8a174c5a9a6b41c44203695c1d7f83f391	machine learning for medical diagnosis: history, state of the art and perspective	complementary medicine;decision tree;intelligent data analysis;machine learning;kirlian camera;point of view;reliability of prediction;medical diagnosis;neural network	The paper provides an overview of the development of intelligent data analysis in medicine from a machine learning perspective: a historical view, a state-of-the-art view, and a view on some future trends in this subfield of applied artificial intelligence. The paper is not intended to provide a comprehensive overview but rather describes some subareas and directions which from my personal point of view seem to be important for applying machine learning in medical diagnosis. In the historical overview, I emphasize the naive Bayesian classifier, neural networks and decision trees. I present a comparison of some state-of-the-art systems, representatives from each branch of machine learning, when applied to several medical diagnostic tasks. The future trends are illustrated by two case studies. The first describes a recently developed method for dealing with reliability of decisions of classifiers, which seems to be promising for intelligent data analysis in medicine. The second describes an approach to using machine learning in order to verify some unexplained phenomena from complementary medicine, which is not (yet) approved by the orthodox medical community but could in the future play an important role in overall medical diagnosis and treatment.	applied artificial intelligence;artificial neural network;bayesian network;decision trees;decision tree;machine learning;naive bayes classifier;neural network simulation;trees (plant)	Igor Kononenko	2001	Artificial intelligence in medicine	10.1016/S0933-3657(01)00077-X	computer science;artificial intelligence;data science;machine learning;decision tree;medical diagnosis;data mining;artificial neural network	AI	4.380269760337584	-45.549433606240605	117026
17817aca90a5e6390de67b0e4d8591751c67f15d	self-adaptive exploration in evolutionary search	information gain	We address a primary question of computational as well as biological research on evolution: How can an exploration strategy adapt in such a way as to exploit the information gained about the problem at hand? We first introduce an integrated formalism of evolutionary search that provides a unified view on different specific approaches. On this basis we discuss the implications of indirect modeling (via a “genotype-phenotype mapping”) on the exploration strategy. Notions such as modularity, pleiotropy and functional phenotypic complex are discussed as implications. Then, rigorously reflecting the notion of self-adaptability, we introduce a new definition that captures self-adaptability of exploration: different genotypes that map to the same phenotype may represent different exploration strategies (also topologically different); self-adaptability requires a variation of exploration strategies along such a “neutral space”. By this definition, the concept of neutrality becomes a central concern of this paper. Finally, we present examples of these concepts: For a specific grammar-type encoding, we observe a large variability of exploration strategies for a fixed phenotype, and a self-adaptive drift towards short representations with highly structured exploration strategy that matches the “problem’s structure”.	exploit (computer security);heart rate variability;semantics (computer science)	Marc Toussaint	2001	CoRR		biology;simulation;bioinformatics;artificial intelligence;kullback–leibler divergence;physics;statistics	ML	-4.321463145585329	-47.575550873892865	117158
2c56920416bae3be328000d68e4fe01145e284ab	feature extraction of clusters based on flexdice	information technology;data engineering;data mining;feature extraction;clustering method;high dimensional data;cluster system;clustering algorithms;scalability;clustering methods;feature extraction clustering methods data mining information technology large scale systems costs clustering algorithms data engineering scalability;large scale systems	We have developed a fast clustering method FlexDice for large high-dimensional data sets[10]. General clustering methods including FlexDice may be able to find data groups consisting of similar data objects, but they have difficult problems of setting some input parameters to suitable values and showing features of clustering results intelligibly. Then, in order to construct a clustering system with user-friendly interface, we propose a feature extraction method for clustering results. We find a feature of clustering results by using FlexDice again and extracting clusters which differ widely from the distribution of data objects in each attribute with ordinary clusters.	benchmark (computing);cluster analysis;experiment;feature extraction;usability	Tomotake Nakamura;Yoko Kamidoi;Shin'ichi Wakabayashi;Noriyoshi Yoshida	2005	21st International Conference on Data Engineering Workshops (ICDEW'05)	10.1109/ICDE.2005.221	correlation clustering;determining the number of clusters in a data set;data stream clustering;scalability;fuzzy clustering;feature extraction;flame clustering;computer science;data science;canopy clustering algorithm;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;information technology;dbscan;biclustering;affinity propagation;clustering high-dimensional data;conceptual clustering	DB	0.8042045339205163	-39.30403608182491	117241
2499967ac636e110de8559e778efcff9b075e311	automatic exploration of machine learning experiments on openml		Understanding the influence of hyperparameters on the performance of a machine learning algorithm is an important scientific topic in itself and can help to improve automatic hyperparameter tuning procedures. Unfortunately, experimental meta data for this purpose is still rare. This paper presents a large, free and open dataset addressing this problem, containing results on 38 OpenML data sets, six different machine learning algorithms and many different hyperparameter configurations. Results where generated by an automated random sampling strategy, termed the OpenML Random Bot. Each algorithm was crossvalidated up to 20.000 times per dataset with different hyperparameters settings, resulting in a meta dataset of around 2.5 million experiments overall.	algorithm;experiment;machine learning;openml;performance tuning;sampling (signal processing)	Daniel Kühn;Philipp Probst;Janek Thomas;Bernd Bischl	2018	CoRR		artificial intelligence;machine learning;mathematics;metadata;hyperparameter;sampling (statistics);data set	ML	-2.1382528040554183	-46.36899504046422	117437
f2cb09c934265c04a517d8b1ef526378bd13814d	a new approach for accurate distributed cluster analysis for big data: competitive k-means	k means;sk means;mapreduce;streaming k means	The tremendous growth in data volumes has created a need for new tools and algorithms to quickly analyse large datasets. Cluster analysis techniques, such as K-Means can be distributed across several machines. The accuracy of K-Means depends on the selection of seed centroids during initialisation. K-Means++ improves on the K-Means seeder, but suffers from problems when it is applied to large datasets. In this paper, we describe a new algorithm and a MapReduce implementation we developed that addresses these problems. We compared the performance with three existing algorithms and found that our algorithm improves cluster analysis accuracy and decreases variance. Our results show that our new algorithm produced a speedup of 76 ± 9 times compared with the serial K-Means++ and is as fast as the streaming K-Means. Our work provides a method to select a good initial seeding in less time, facilitating fast accurate cluster analysis over large datasets.	big data;cluster analysis;computer cluster;k-means clustering	Rui Máximo Esteves;Thomas J. Hacker;Chunming Rong	2014	IJBDI	10.1504/IJBDI.2014.063844	computer science;data science;machine learning;data mining	ML	-2.278611839152586	-39.78185559129641	117466
00fc7098142a5775214b46e9ba22896383f604ef	research on the application of improved k-means in intrusion detection	anomaly;cluster;intrusion detection;k-means	To solve the shortages of traditional k-means algorithm that it needs to input the clustering number and it is sensitive to initial clustering center, the improved k-means algorithm is put forward. In the improved algorithm, each data object will be represented by the number of points around it in a certain region. Data objects will be clustered on the basis of that the distances between data objects belonging to different kinds are farther than the ones between the same. Both k-means and improved k-means are used in intrusion detection, which shows that the improved can overcome inherent disadvantages of k-means and has good clustering results. © 2011 Springer-Verlag Berlin Heidelberg.	intrusion detection system;k-means clustering	Mingjun Wei;Lichun Xia;Jingjing Su	2011		10.1007/978-3-642-27503-6_92	economic shortage;k-means clustering;intrusion detection system;artificial intelligence;pattern recognition;cluster analysis;computer science	ML	4.388567955632205	-38.44603141921239	117578
bc9a689a8a2281a003c21f4528555413401cd9bf	q-divergence-based relational fuzzy c-means clustering				Yuchi Kanzawa	2018	JACIII	10.20965/jaciii.2018.p0034	pattern recognition;divergence;machine learning;computer science;artificial intelligence;fuzzy logic;fuzzy clustering;cluster analysis;relational database	Vision	2.5078509375910616	-40.612866506102606	117614
cc8f282f88b010d3e75f5d0db70aaeff4018ee31	a framework of gene subset selection using multiobjective evolutionary algorithm	sample selection;multiobjective evolutionary algorithm;non negative matrix factorization;gene selection	Microarray gene expression technique can provide snap shots of gene expression levels of samples. This technique is promising to be used in clinical diagnosis and genomic pathology. However, the curse of dimensionality and other problems have been challenging researchers for a decade. Selecting a few discriminative genes is an important choice. But gene subset selection is a NP hard problem. This paper proposes an effective gene selection framework. This framework integrates gene filtering, sample selection, and multiobjective evolutionary algorithm (MOEA). We use MOEA to optimize four objective functions taking into account of class relevance, feature redundancy, classification performance, and the number of selected genes. Experimental comparison shows that the proposed approach is better than a well-known recursive feature elimination method in terms of classification performance and time complexity.	computational complexity theory;curse of dimensionality;discriminative model;evolutionary algorithm;feature selection;moea framework;microarray;np-hardness;non-negative matrix factorization;recursion;relevance;time complexity	Yifeng Li;Alioune Ngom;Luis Rueda	2012		10.1007/978-3-642-34123-6_4	gene-centered view of evolution;biology;mathematical optimization;computer science;bioinformatics;machine learning;non-negative matrix factorization	Comp.	9.876053656915653	-44.092430750128315	117999
5986fa318e9f35f12a803736556c6d3790296e69	a framework for regional association rule mining in spatial datasets	cluster algorithm;pattern clustering;regional knowledge discovery;spatial data;geographically referenced data;pattern clustering data mining geography;grid based supervised clustering;water supply;efficient discovery;data mining;reward based regional discovery;association rule mining;spatial data mining;association rule;association rules data mining clustering algorithms computer science explosions phase measurement itemsets data engineering knowledge engineering statistical distributions;grid based supervised clustering regional association rule mining spatial dataset geographically referenced data efficient discovery spatial knowledge spatial data mining regional knowledge discovery reward based regional discovery;regional association rule mining;arsenic;spatial dataset;spatial knowledge;geography	The immense explosion of geographically referenced data calls for efficient discovery of spatial knowledge. One of the special challenges for spatial data mining is that information is usually not uniformly distributed in spatial datasets. Consequently, the discovery of regional knowledge is of fundamental importance for spatial data mining. This paper centers on discovering regional association rules in spatial datasets. In particular, we introduce a novel framework to mine regional association rules relying on a given class structure. A reward-based regional discovery methodology is introduced, and a divisive, grid-based supervised clustering algorithm is presented that identifies interesting subregions in spatial datasets. Then, an integrated approach is discussed to systematically mine regional rules. The proposed framework is evaluated in a real-world case study that identifies spatial risk patterns from arsenic in the Texas water supply.	algorithm;association rule learning;cluster analysis;data mining;fitness function;hotspot (wi-fi);scientific literature	Wei Ding;Christoph F. Eick;Jing Wang;Xiaojing Yuan	2006	Sixth International Conference on Data Mining (ICDM'06)	10.1109/ICDM.2006.5	association rule learning;computer science;data science;pattern recognition;data mining	DB	1.9417272399927639	-43.0274361652968	118401
0a3863a0915256082aee613ba6dab6ede962cdcd	early and reliable event detection using proximity space representation		Let us consider a specific action or situation (called event) that takes place within a time series. The objective in early detection is to build a decision function that is able to go off as soon as possible from the onset of an occurrence of this event. This implies making a decision with an incomplete information. This paper proposes a novel framework that i) guarantees that a detection made with a partial observation will also occur at full observation of the time-series; ii) incorporates in a consistent manner the lack of knowledge about the minimal amount of information needed to make a decision. The proposed detector is based on mapping the temporal sequences to a landmarking space thanks to appropriately designed similarity functions. As a by-product, the framework benefits from a scalable training algorithm and a theoretical guarantee concerning its generalization ability. We also discuss an important improvement of our framework in which decision function can still be made reliable while being more expressive. Our experimental studies provide compelling results on toy data, presenting the trade-off that occurs when aiming at accuracy, earliness and reliability. Results on real physiological and video datasets show that our proposed approach is as accurate and early as state-of-the-art algorithm, while ensuring reliability and being far more efficient to learn. Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).	algorithm;international conference on machine learning;journal of machine learning research;lagrangian relaxation;norm (social);onset (audio);performance tuning;scalability;time series	Maxime Sangnier;Jérôme Gauthier;Alain Rakotomamonjy	2016			real-time computing;simulation;machine learning;data mining;mathematics;statistics	ML	0.18012187190478024	-45.60061613168654	118661
415faadd58b11ff0cac912a7e11ccf64e2e48e15	multiple sequence alignment using an exhaustive and greedy algorithm	greedy algorithm;sequence analysis;multiple sequence alignment;exhaustive algorithm;iterative alignment	We describe an exhaustive and greedy algorithm for improving the accuracy of multiple sequence alignment. A simple progressive alignment approach is employed to provide initial alignments. The initial alignment is then iteratively optimized against an objective function. For any working alignment, the optimization involves three operations: insertions, deletions and shuffles of gaps. The optimization is exhaustive since the algorithm applies the above operations to all eligible positions of an alignment. It is also greedy since only the operation that gives the best improving objective score will be accepted. The algorithms have been implemented in the EGMA (Exhaustive and Greedy Multiple Alignment) package using Java programming language, and have been evaluated using the BAliBASE benchmark alignment database. Although EGMA is not guaranteed to produce globally optimized alignment, the tests indicate that EGMA is able to build alignments with high quality consistently, compared with other commonly used iterative and non-iterative alignment programs. It is also useful for refining multiple alignments obtained by other methods.		Yi Wang;Kuo-Bin Li	2005	Journal of bioinformatics and computational biology	10.1142/S021972000500103X	biology;mathematical optimization;greedy algorithm;multiple sequence alignment;computer science;bioinformatics;machine learning;sequence analysis	Comp.	-1.0465249346532206	-50.87486610025167	118816
c95394bb9e1b318cd6d7af59cf250cbd4e985c56	database search algorithm based on track predicting in fingerprinting localization		Fingerprint positioning is a common technique in the field of indoor positioning. As a result of avoiding and utilizing the complex indoor structure to block and reflect the signal effectively, it has the most necessary room-level positioning accuracy. Due to the large amount of data in the radio-map, the clustering algorithm has become a commonly used method to reduce the workload of the search. But the artificial clustering and automatic clustering have their own limitations. This paper proposes a predictive radio-map search strategy to accelerate the database search, which means taking the positioning results, predicted by the filtering algorithm, to be the priori information to accelerate the next positioning. The simulation results show that the algorithm is superior to the traditional clustering - localization strategy in positioning accuracy.	fingerprint (computing);search algorithm	Deyue Zou;Yuqun Guo;Xin Liu	2017		10.1007/978-3-319-67777-4_24	workload;filter (signal processing);algorithm;machine learning;cluster analysis;database search engine;computer science;artificial intelligence	Vision	-3.7139019070224824	-43.314911329897576	119153
81a6f025dea5f58c5874601887d836158cf1e18e	a memetic kernel clustering algorithm for change detection in sar images		For SAR images change detection problem, a memetic kernel clustering algorithm (MKCA) is proposed. SAR image change detection problem as an optimization problem can solved by kernel clustering method. In this paper, the kernel function can transform nonlinear data into a high dimensional feature space, and increases the probability of the linear separability of the patterns within the transformed space and simplifies the associated data structure. Meanwhile local learning operators are designed to further enhance the ability of global exploration and promote performance of classification. Finally, to evaluate this method, the performance of the proposed method has been compared with some classical clustering algorithms for change detection in SAR (synthetic aperture radar) images. The experiments showed that the proposed algorithm can reduce the number of missed alarms and false alarms. So the proposed algorithm can effectively separate the unchanged area and the changed area.		Yangyang Li;Gao Lu;Licheng Jiao	2016		10.1007/978-981-10-3614-9_47	mathematical optimization;kernel (statistics);machine learning;change detection;canopy clustering algorithm;artificial intelligence;synthetic aperture radar;cluster analysis;feature vector;memetic algorithm;computer science;pattern recognition;optimization problem	ML	4.863827764306861	-41.25373521948197	119155
727270e90fef543b18767a5f8a2fa524f95167d1	clustering data stream under a belief function framework		Clustering is a crucial task for massive data that continuously arrive and evolve over time, generated as stream. However, data may be pervaded by uncertainty and imprecision, and techniques that achieve the unsupervised learning with imperfect data sets are unable to deal with such evolving environment. On the other hand, standard methods for clustering data streams are not adapted to an uncertain framework. Hence, in this paper, we propose a method for clustering data stream in an imperfect context, particularly using belief function theory in order to handle the belonging of objects to singletons and disjunctions of clusters.	cluster analysis;computer cluster;experiment;partition problem;scalability;unsupervised learning	Maroua Bahri;Zied Elouedi	2016	2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2016.7945618	computer science;flame clustering;correlation clustering;machine learning;cluster analysis;data stream clustering;constrained clustering;canopy clustering algorithm;artificial intelligence;brown clustering;pattern recognition;cure data clustering algorithm	DB	-0.8017927868053405	-38.9365645029175	119163
81ea9fe182440d96b8987826a343c41f465b25fe	on the definition and the construction of pockets in macromolecules	delaunay method;alpha complexes;space filling and solvent accessible models;biology computing;molecular modeling;diagramme voronoi;geometrie algorithmique;geometria combinatoria;computational geometry;biologia molecular;voronio cells;three dimensional;methode delaunay;algorithme;algorithm;docking;combinatorial geometry;molecular biology;combinatorial geometry and topology;algorithms;macromolecule;solvent accessibility;geometria computacional;geometrie combinatoire;delaunay simplices;macromolecula;diagrama voronoi;biologie informatique;voronoi diagram;algoritmo;biologie moleculaire	The shape of a protein is important for its functions. This includes the location and size of identifiable regions in its complement space. We formally define pockets as regions in the complement with limited accessibility from the outside. Pockets can be efficiently constructed by an algorithm based on alpha complexes. The algorithm is implemented and applied to proteins with known three-dimensional conformations.	accessibility;complement system proteins;dec alpha;staphylococcal protein a;algorithm;macromolecule	Herbert Edelsbrunner;Michael A. Facello;Jie Liang	1996	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing	10.1016/S0166-218X(98)00067-5	docking;macromolecule;three-dimensional space;combinatorics;voronoi diagram;computational geometry;molecular model;mathematics;geometry;algorithm	Logic	-0.13241921027059164	-49.79095013410817	119259
09dfb01c69d513e93bd0a439d585cffb9c9aa612	two-stage variable selection for molecular prediction of disease	statistical analysis blood diseases genetics molecular biophysics;genetics;statistical analysis;sampling budget constraints two stage variable selection molecular prediction disease high dimensional data symptomatic infection molecular expression levels blood lasso method gene subset;blood;molecular biophysics;diseases;correlation covariance matrices gene expression vectors resource management heating conferences	A two-stage predictor strategy is introduced in the context of high dimensional data (large p, small n). Here the focus application is a medical one: prediction of symptomatic infection given molecular expression levels in blood. The first stage of the two-stage predictor uses the previously introduced method of Predictive Correlation Screening (PCS) to select a subset of genes that are important in the prediction of symptom scores. Selected genes are used in the second stage to learn a predictor for the prediction of symptom scores. Under sampling budget constraints we derive the optimal sample allocation rules to the first and second stages of the two-stage predictor. Superiority of the proposed predictor relative to the well known method of LASSO is shown via experiment.	emoticon;experiment;feature selection;heat map;kerrison predictor;lasso;multinomial logistic regression;randomness;sampling (signal processing);whole earth 'lectronic link	Hamed Firouzi;Bala Rajaratnam;Alfred O. Hero	2013	2013 5th IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)	10.1109/CAMSAP.2013.6714034	biology;pathology;bioinformatics;statistics	ML	8.160751354296446	-50.07902066087068	119350
29fec0fd127871c90c7139e84fddf66555a134ba	comparing accuracies of rule evaluation models to determine human criteria on evaluated rule sets	learning algorithm;learning artificial intelligence data mining;nickel;training;humans data mining classification algorithms learning systems conferences labeling database systems filters displays iterative methods;classification learning algorithm;data mining;index value rule evaluation model evaluated rule sets rule selection objective rule evaluation indices classification learning algorithm balanced randomized class distribution data mining;rule evaluation index;objective rule evaluation indices;accuracy;indexes;balanced randomized class distribution;indexation;classification algorithms;rule selection;human criteria determination data mining post processing rule evaluation index;humans;learning artificial intelligence;evaluated rule sets;index value;rule evaluation model;human criteria determination;labeling;evaluation model;post processing	In data mining post-processing, rule selection using objective rule evaluation indices is one of a useful method to find out valuable knowledge from mined patterns. However, the relationship between an index value and experts' criteria has never been clarified. In this study, we have compared the accuracies of classification learning algorithms for datasets with randomized class distributions and real human evaluations. As a method to determine the relationship, we used rule evaluation models, which are learned from a dataset consisting of objective rule evaluation indices and evaluation labels for each rule. Then, the results show that accuracies of classification learning algorithms with/without criteria of human experts are different on a balanced randomized class distribution. With regarding to the results, we can consider about a way to distinguish randomly evaluated rules using the accuracies of multiple learning algorithms.	data mining;machine learning;mined;randomized algorithm;randomness;rule 184;statistical classification;video post-processing	Hidenao Abe;Shusaku Tsumoto	2008	2008 IEEE International Conference on Data Mining Workshops	10.1109/ICDMW.2008.49	statistical classification;nickel;database index;labeling theory;computer science;machine learning;pattern recognition;data mining;accuracy and precision;video post-processing;statistics	ML	5.636069017934114	-39.49788521012345	119499
611184b770417d7833625bfbc5a34c29766a1e88	real coded genetic algorithm for development of optimal g-k clustering algorithm		Clustering has been used as a popular technique for identifying a natural grouping or meaningful partition of a given data set by using a distance or similarity function. This paper proposes a novel real coded Genetic algorithm (GA) for the development of optimal Gustafson Kessel (GK) clustering algorithm. In this work, the objective function of the GK algorithm is optimized using real coded genetic algorithm. The cluster centers are represented as real numbers and real-parameter genetic operators are applied to obtain the optimal cluster centers that minimize the intra-cluster distance. The performance of the proposed approach is demonstrated through three gene expression data sets. Xie-Beni index is used to arrive at the best possible number of clusters. The proposed method has produced the objective function value which is less than the value obtained using K-Means, Fuzzy C-Means and GK algorithms. Statistical analysis of the test results shows the superiority of the proposed algorithm over the existing methods.	cluster analysis;genetic algorithm	C. Devi Arockia Vanitha;D. Devaraj;M. Venkatesulu	2014		10.1007/978-3-319-20294-5_23	correlation clustering;mathematical optimization;meta-optimization;cultural algorithm;canopy clustering algorithm;cure data clustering algorithm;fsa-red algorithm;cluster analysis;linde–buzo–gray algorithm;k-medoids;population-based incremental learning	ML	3.614887657267229	-41.833168908274146	119646
5dc10c1590509e8436916ed9d74f8d3ff6f95784	techniques for clustering gene expression data	microarray data;bi clustering;clustering techniques;gene expression data;gene expression;microarray analysis;microarray data analysis;clustering;clustering method;bioinformatics	Many clustering techniques have been proposed for the analysis of gene expression data obtained from microarray experiments. However, choice of suitable method(s) for a given experimental dataset is not straightforward. Common approaches do not translate well and fail to take account of the data profile. This review paper surveys state of the art applications which recognise these limitations and addresses them. As such, it provides a framework for the evaluation of clustering in gene expression analyses. The nature of microarray data is discussed briefly. Selected examples are presented for clustering methods considered.	addresses (publication format);algorithm;analysis of algorithms;cluster analysis;computational complexity theory;experiment;gene expression profiling;gene expression programming;microarray;numerical analysis;silo (dataset);virtual community;statistical cluster	Grainne Kerr;Heather J. Ruskin;Martin Crane;P. Doolan	2008	Computers in biology and medicine	10.1016/j.compbiomed.2007.11.001	microarray analysis techniques;gene chip analysis;computer science;bioinformatics;data science;data mining;microarray databases;clustering high-dimensional data	Comp.	6.049856773119084	-50.19717663565554	120167
48aac6dc5b12f158feb731e8592c2b2deceb7acc	associations between floral asymmetry and individual genetic variability differ among three prickly pear (opuntia echios) populations	developmental instability;flower;genetic diversity;radial asymmetry;opuntia echios;fluctuating asymmetry;flower morphology;prickly pear	While stress is expected to increase developmental instability (DI), not all studies confirm this. This heterogeneity could in part be due to the use of subtle differences between the left and right side of bilateral symmetrical organisms to quantify DI, leading to large sampling error obscuring associations with DI. Traits that develop simultaneously more than twice (such as flower petals or bird feathers) reflect individual DI more reliably, such that stronger associations are expected to emerge. Furthermore, some studies have shown differences in strengths of associations among populations. We studied the association between individual genetic diversity and DI in flower petals within three Opuntia echios populations inhabiting Galápagos. Quantifying individual DI through variation in length and width of a high number of petals within individual cacti, lead to a strong association between DI and genetic diversity in one population. We conclude that associations between individual DI and genetic diversity can be more easily revealed by measuring traits that develop repeatedly.	bilateral filter;developmental robotics;heart rate variability;instability;pear;population;sampling (signal processing)	Philippe Helsen;Stefan Van Dongen	2016	Symmetry	10.3390/sym8110116	fluctuating asymmetry;genetic diversity	AI	-4.466046911616958	-46.310255538977636	120203
0043718f8864eae73d232382bb8e25cd1f73635c	gene expression programming algorithm for transient security classification	selected works;bepress	In this paper, a gene expression programming (GEP) based algorithm is implemented for power system transient security classification. The GEP algorithms as evolutionary algorithms for pattern classification have recently received attention for classification problems because they can perform global searches. The proposed methodology applies the GEP for the first time in transient security assessment and classification problems of power systems. The proposed algorithm is examined using different IEEE standard test systems. Power system three phase short circuit contingency has been used to test the proposed algorithm. The algorithm checks the static security status of the power system then classifies the transient security of the power system as secure or not secure. Performance of the algorithm is compared with other neural network based classification algorithms to show its superiority for transient security classification.	algorithm;gene expression programming	Almoataz Y. Abdelaziz;S. F. Mekhamer;H. M. Khattab;M. L. A. Badr;Bijaya Ketan Panigrahi	2012		10.1007/978-3-642-35380-2_48	computer science;theoretical computer science;data mining;computer security	PL	7.21151342642278	-38.93591292067318	120404
7956915a2536361070ce66c1ef931057ab25d14b	new proximity estimate for incremental update of non-uniformly distributed clusters		The conventional clustering algorithms mine static databases and generate a set of patterns in the form of clusters. Many real life databases keep growing incrementally. For such dynamic databases, the patterns extracted from the original database become obsolete. Thus the conventional clustering algorithms are not suitable for incremental databases due to lack of capability to modify the clustering results in accordance with recent updates. In this paper, the author proposes a new incremental clustering algorithm called CFICA(Cluster Feature-Based Incremental Clustering Approach for numerical data) to handle numerical data and suggests a new proximity metric called Inverse Proximity Estimate (IPE) which considers the proximity of a data point to a cluster representative as well as its proximity to a farthest point in its vicinity. CFICA makes use of the proposed proximity metric to determine the membership of a data point into a cluster.	algorithm;birch;cluster analysis;data point;database;entity;level of measurement;numerical analysis;pure function;real life	A. M. Sowjanya;M. Shashi	2013	CoRR		correlation clustering;constrained clustering;k-medians clustering;fuzzy clustering;computer science;machine learning;cure data clustering algorithm;data mining;database;cluster analysis	DB	0.32670301909647903	-40.1175412266693	120876
63d87539f15aa44dfad3c90e842d31bb144f22e8	mining good sliding window for positive pathogens prediction in pathogenic spectrum analysis	pathogens prediction;time series;data mining;sliding window	Positive pathogens prediction is the basis of pathogenic spectrum analysis, which is a meaningful work in public health. Gene Expression Programming (GEP) can develop the model without predetermined assumptions, so applying GEP to positive pathogens prediction is desirable. However, traditional time-adjacent sliding window may not be suitable for GEP evolving accurate prediction model. The main contributions of this work include: (1) applying GEP-based prediction method to diarrhea syndrome related pathogens prediction, (2) analyzing the disadvantages of traditional time-adjacent sliding window in GEP prediction, (3) proposing a heuristic method to mine good sliding window for generating training set that is used for GEP evolution, (4) proving the problem of training set selection is NP-hard, (5) giving an experimental study on both real-world and simulated data to demonstrate the effectiveness of the proposed method, and discussing some future studies.	experiment;futures studies;gene expression programming;heuristic;np-hardness;simulation;test set	Lei Duan;Changjie Tang;Chi Gou;Min Jiang;Jie Zuo	2011		10.1007/978-3-642-25856-5_12	sliding window protocol;simulation;computer science;artificial intelligence;time series;data mining;statistics	AI	4.939475656598549	-47.36697502120756	120975
7fb23442a66bb36a5ed3a45130012773bee4b94f	fuzzy clustering-based formal concept analysis for association rules mining	fuzzy k means;functional dependency;association rule mining;fuzzy clustering;association rule;formal concept analysis	Formal Concept Analysis (FCA), in which data is represented as a formal context, offers a framework for Association Rules Mining (ARM) by handling functional dependencies in the data. However, with the size of the formal context, the number of rules grows exponentially. In this article, we apply Fuzzy K-Means clustering on the data set to reduce the formal context and FCA on the reduced data set for mining association rules. With experiments on two real-world healthcare data sets, we offer the evidence for performance of FKM-based FCA in mining association rules.	cluster analysis;formal concept analysis;fuzzy clustering	Cherukuri Aswani Kumar	2012	Applied Artificial Intelligence	10.1080/08839514.2012.648457	association rule learning;computer science;machine learning;pattern recognition;data mining	AI	-0.12185508992314517	-38.23784362077629	121480
3d298cdbf0f2a27fea273b1745d16ccae934f13b	discovery of genes implied in cancer by genetic algorithms and association rules		This work proposes a methodology to identify genes highly related with cancer. In particular, a multi-objective evolutionary algorithm named CANGAR is applied to obtain quantitative association rules. This kind of rules are used to identify dependencies between genes and their expression levels. Hierarchical cluster analysis, fold-change and review of the literature have been considered to validate the relevance of the results obtained. The results show that the reported genes are consistent with prior knowledge and able to characterize cancer colon patients.	genetic algorithm	Alejandro Sánchez Medina;Alberto Gil Pichardo;José Manuel García-Heredia;María Martínez-Ballesteros	2016		10.1007/978-3-319-32034-2_58	bioinformatics;machine learning;data mining	NLP	7.541879116564617	-47.118538685560175	121696
fa65315419c8f31e5116c14296ed4aa6968ea27d	hybridization of genetic and quantum algorithm for gene selection and classification of microarray data	microarray data;support vector machines;genetic quantum algorithm;classification;feature selection;quantum computing;gene selection	In this work, we hybridize the Genetic Quantum Algorithm with the Support Vector Machines classifier for gene selection and classification of high dimensional Microarray Data. We named our algorithm GQASVM. Its purpose is to identify a small subset of genes that could be used to separate two classes of samples with high accuracy.	artificial neural network;dna microarray;decision tree;feature selection;genetic algorithm;quantum algorithm;selection algorithm;support vector machine	Allani Abderrahim;El-Ghazali Talbi;Khaled Mellouli	2009	2009 IEEE International Symposium on Parallel & Distributed Processing	10.1142/S0129054112400217	gene-centered view of evolution;support vector machine;microarray analysis techniques;biological classification;computer science;bioinformatics;pattern recognition;data mining;quantum computer;feature selection	Arch	8.177617986003872	-47.9366585815482	121705
305bde415af329e6768d110445a77a209fb3dab6	spectral aggregation for clustering ensemble	eigenvalues and eigenfunctions;pattern clustering;spectral clustering spectral aggregation clustering ensemble clustering algorithm clustered partitions;symmetric matrices;spectral clustering;error analysis;number of clusters;clustering algorithms;clustering algorithms partitioning algorithms symmetric matrices automation robustness unsupervised learning sensor fusion eigenvalues and eigenfunctions pairwise error probability;algorithm design and analysis;partitioning algorithms;automation	Since a large number of clustering algorithms exist, aggregating different clustered partitions into a single consolidated one to obtain better results has become an important problem. We propose a new algorithm for clustering ensemble based on spectral clustering. We also propose a criteria along with this algorithm, for the detection of cluster numbers. Our algorithm can determine the number of clusters more accurately with less volatility, and therefore can deduce a better combined clustering result. Experimental results on both synthesis and real data-sets show the capability and robustness of our approach.	algorithm;cluster analysis;computer cluster;processor affinity;spectral clustering;volatility	Xi Wang;Chunyu Yang;Jie Zhou	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761779	correlation clustering;constrained clustering;algorithm design;mathematical optimization;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;automation;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;mathematics;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;spectral clustering;affinity propagation;symmetric matrix;clustering high-dimensional data	Robotics	1.6409596123425467	-41.00136661254019	121791
a47cf57ec815923070237ba271b1afddd8429fc2	on dynamic clustering and two options	cluster algorithm;object recognition;pattern clustering;fuzzy c mean;sampling time;interaction power model;image classification;fuzzy set theory;dynamic clustering;hard c means;pattern clustering fuzzy set theory image classification object recognition;sampling time dynamic clustering hard c means fuzzy c means agglomerative hierarchical method clustering algorithms data global information data local information cluster merging interaction power model unit weight model data classification universal gravitation;fuzzy c means;clustering algorithms;cluster merging;clustering algorithms systems engineering and theory fuzzy systems heuristic algorithms merging sampling methods testing fuzzy sets humans;universal gravitation;data classification;agglomerative hierarchical method;unit weight model;power modeling;data local information;data global information	"""Hard/fuzzy c-means and agglomerative hierarchical method are representative clustering algorithms. The one is an algorithm using """"global information of data"""", the other is using """"local information of data"""". In this paper, a new clustering algorithm (dynamic clustering; DC) is proposed, which has the advantages over the conventional clustering algorithms. On DC, clusters are updated by some model introduced in advance. That is, the clusters are moved according to the introduced model and merged. Here, merging two clusters means that two clusters contact each other. The model is called option of DC. In this paper, two options of DC are proposed, i.e., interaction power model (IP) and unit weight model (UW). Moreover, through numerical examples, it is shown that DC gives good classification for the data which is difficult to classify by the former clustering algorithms"""	algorithm;cluster analysis;numerical analysis	Yasunori Endo;Hayato Iwata	2005	The 14th IEEE International Conference on Fuzzy Systems, 2005. FUZZ '05.	10.1109/FUZZY.2005.1452530	correlation clustering;constrained clustering;determining the number of clusters in a data set;contextual image classification;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;newton's law of universal gravitation;computer science;canopy clustering algorithm;cognitive neuroscience of visual object recognition;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;fuzzy set;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	DB	3.1202648045486896	-39.77522204511698	121841
78f98c695b232054cec9bc4f321348b58d639edb	constraint acquisition methods for data clustering		Constrained data clustering algorithms allow the incorporation of a priori knowledge for specific problems into the clustering task in the form of constraints. The quality of the constraints have great impact in the performance of the constrained clustering algorithms. Therefore, special care must be taken while building the sets of constraints. In order to take the maximum advantage of the constrained clustering algorithms, these constraints must be highly informative and non-redundant. We propose two constraint acquisition methods based on user-feedback. The first method searches for non-redundant intra-cluster and inter-cluster query-candidates supported by information contained in an initial partition of the data set, ranks the candidates by decreasing order of interest and, finally, prompts the user the most relevant query-candidates. The constraints may optionally be used for learning a new data representation, which may enhance the performance of clustering. The second method iterates between using the previous method for expanding the set of constraints, and producing an updated partition of the data. The motivation is to iteratively increment the set of constraints by including new informative and non-redundant constraints at each iteration. Experimental results advocate that the proposed constraint acquisition methods increase the performance of data clustering.	cluster analysis	João M. M. Duarte;Ana L. N. Fred;F. Jorge F. Duarte	2014	Intell. Data Anal.	10.3233/IDA-140708	correlation clustering;constrained clustering;mathematical optimization;data stream clustering;fuzzy clustering;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;mathematics;cluster analysis;constraint;biclustering;clustering high-dimensional data	ML	3.530767220308399	-43.685463623620834	121941
1b2761295654d6f400e2ee3b329507e8bd1f7ddd	a graph based framework for clustering and characterization of som	statistical test;graph coloring;graphs;local features;clustering method;high dimensional data;characterization;self organized map;som;spanning tree	In this paper, a new graph based framework for clustering characterization is proposed. In this context, Self Organizing Map (SOM) is one popular method for clustering and visualizing high dimensional data, which is generally succeeded by another clustering methods (partitional or hierarchical) for optimizing the final partition. Recently, we have developed a new SOM clustering method based on graph coloring called McSOM. In the current study, we propose to automatically characterize the classes obtained by this method. To this end, we propose a new approach combining a statistical test with a maximum spanning tree for local features selection in each class. Experiments will be given over several databases for validating our approach.		Rakia Jaziri;Khalid Benabdeslem;Haytham Elghazel	2010		10.1007/978-3-642-15825-4_51	correlation clustering;statistical hypothesis testing;data stream clustering;fuzzy clustering;spanning tree;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;graph coloring;data mining;mathematics;cluster analysis;graph;single-linkage clustering;clustering high-dimensional data;conceptual clustering	Vision	1.8178272342154183	-41.71977297856523	122200
7b7a5bf1d5952d6cda69e35fbab3334624c60677	alignment-free phylogeny of whole genomes using underlying subwords	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;physiological cellular and medical topics;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	With the progress of modern sequencing technologies a large number of complete genomes are now available. Traditionally the comparison of two related genomes is carried out by sequence alignment. There are cases where these techniques cannot be applied, for example if two genomes do not share the same set of genes, or if they are not alignable to each other due to low sequence similarity, rearrangements and inversions, or more specifically to their lengths when the organisms belong to different species. For these cases the comparison of complete genomes can be carried out only with ad hoc methods that are usually called alignment-free methods. In this paper we propose a distance function based on subword compositions called Underlying Approach (UA). We prove that the matching statistics, a popular concept in the field of string algorithms able to capture the statistics of common words between two sequences, can be derived from a small set of “independent” subwords, namely the irredundant common subwords. We define a distance-like measure based on these subwords, such that each region of genomes contributes only once, thus avoiding to count shared subwords a multiple number of times. In a nutshell, this filter discards subwords occurring in regions covered by other more significant subwords. The Underlying Approach (UA) builds a scoring function based on this set of patterns, called underlying. We prove that this set is by construction linear in the size of input, without overlaps, and can be efficiently constructed. Results show the validity of our method in the reconstruction of phylogenetic trees, where the Underlying Approach outperforms the current state of the art methods. Moreover, we show that the accuracy of UA is achieved with a very small number of subwords, which in some cases carry meaningful biological information. http://www.dei.unipd.it/∼ciompin/main/underlying.html	biopolymer sequencing;chromosome inversion;composition;dna sequence rearrangement;genome;hoc (programming language);inversion (discrete mathematics);matching;phylogenetic tree;phylogenetics;score;scoring functions for docking;sequence alignment;string (computer science);substring;trees (plant);user agent;algorithm	Matteo Comin;Davide Verzotto	2012		10.1186/1748-7188-7-34	biology;medical research;computer science;bioinformatics	Comp.	0.03454504310111532	-51.591488218275614	122285
6b9a985de6a9c6742c5200c99ca986ca8f7d8400	a novel adaptive density-based aco algorithm with minimal encoding redundancy for clustering problems	unsupervised learning;redundancy;estimation;clustering algorithms;encoding;algorithm design and analysis;data models	In the so-called Big Data paradigm descriptive analytics are widely conceived as techniques and models aimed at discovering knowledge within unlabeled datasets (e.g. patterns, similarities, etc) of utmost help for subsequent predictive and prescriptive methods. One of these techniques is clustering, which hinges on different multi-dimensional measures of similarity between unsupervised data instances so as to blindly collect them in groups of clusters. Among the myriad of clustering approaches reported in the literature this manuscript focuses on those relying on bio-inspired meta-heuristics, which have been lately shown to outperform traditional clustering schemes in terms of convergence, adaptability and parallelization. Specifically this work presents a new clustering approach based on the processing fundamentals of the Ant Colony Optimization (ACO) algorithm, i.e. stigmergy via pheromone trails and progressive construction of solutions through a graph. The novelty of the proposed scheme beyond previous research on ACO-based clustering lies on a significantly pruned graph that not only minimizes the representation redundancy of the problem at hand, but also allows for an embedded estimation of the number of clusters within the data. However, this approach imposes a modified ant behavior so as to account for the optimality of entire paths rather than that of single steps within the graph. Simulation results over conventional datasets will evince the promising performance of our approach and motivate further research aimed at its applicability to real scenarios.	algorithm;ant colony optimization algorithms;big data;british informatics olympiad;cluster analysis;embedded system;heuristic (computer science);parallel computing;programming paradigm;simulation;stigmergy;unsupervised learning	Esther Villar-Rodriguez;Antonio González-Pardo;Javier Del Ser;Miren Nekane Bilbao;Sancho Salcedo-Sanz	2016	2016 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2016.7744186	unsupervised learning;correlation clustering;data modeling;constrained clustering;algorithm design;mathematical optimization;estimation;data stream clustering;fuzzy clustering;computer science;artificial intelligence;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;cluster analysis;redundancy;encoding;statistics;clustering high-dimensional data;conceptual clustering	DB	9.427122974557637	-44.830229808081306	122355
1c109e582c824f7d5bb8eee2d401bef1a5e6d6bd	bayesian learning with local support vector machines for cancer classification with gene expression data	bayes estimation;genetic engineering;cancerology;analisis estadistico;modele agrege;tumor maligno;naive bayes;modelo agregado;bioinformatique;hombre;leucemie;gene expression data;classification;support vector;colon cancer;leukemia;gene expression;expression genique;estimacion bayes;statistical analysis;bayesian learning;robustesse;cancerologie;machine exemple support;analyse statistique;ingenieria genetica;human;genie genetique;leucemia;aggregate model;algorithme evolutionniste;ovarian cancer;robustness;feature selection;algoritmo evolucionista;tumeur maligne;cancerologia;bioinformatica;support vector machine;maquina ejemplo soporte;evolutionary algorithm;vector support machine;cancer classification;article in monograph or in proceedings;expresion genetica;clasificacion;malignant tumor;estimation bayes;homme;robustez;bioinformatics	This paper describes a novel method for improving classification of support vector machines (SVM) with recursive feature selection (SVM-RFE) when applied to cancer classification with gene expression data. The method employs pairs of support vectors of a linear SVMRFE classifier for generating a sequence of new SVM classifiers, called local support classifiers. This sequence is used in two Bayesian learning techniques: as ensemble of classifiers in Optimal Bayes, and as attributes in Naive Bayes. The resulting classifiers are applied to four publically available gene expression datasets from leukemia, ovarian, lymphoma, and colon cancer data, respectively. The results indicate that the proposed approach improves significantly the predictive performance of the baseline SVM classifier, its stability and robustness, with satisfactory results on all datasets. In particular, perfect classification is achieved on the leukemia and ovarian cancer datasets.	baseline (configuration management);colon classification;feature selection;heuristic;naive bayes classifier;recursion;semi-continuity;support vector machine	Elena Marchiori;Michèle Sebag	2005		10.1007/978-3-540-32003-6_8	random subspace method;support vector machine;naive bayes classifier;computer science;machine learning;evolutionary algorithm;pattern recognition;data mining;feature selection	ML	6.490183464739742	-46.564511688908205	122650
50346ee2aee0202984c10c90e685cf14a4e66f04	density biased sampling with locality sensitive hashing for outlier detection		Outlier or anomaly detection is one of the major challenges in big data analytics since unusual but insightful patterns are often hidden in massive data sets such as sensing data and social networks. Sampling techniques have been a focus for outlier detection to address scalability on big data. The recent study has shown uniform random sampling with ensemble can boost outlier detection performance. However, uniform sampling assumes that all points are of equal importance, which usually fails to hold for outlier detection because some points are more sensitive to sampling than others. Thus, it is necessary and promising to utilise the density information of points to reflect their importance for sampling based detection. In this paper, we formally investigate density biased sampling for outlier detection, and propose a novel density biased sampling approach. To attain scalable density estimation, we use Locality Sensitive Hashing (LSH) for counting the nearest neighbours of a point. Extensive experiments on both synthetic and real-world data sets show that our approach significantly outperforms existing outlier detection methods based on uniform sampling.	anomaly detection;locality of reference;locality-sensitive hashing	Xuyun Zhang;Mahsa Salehi;Christopher Leckie;Yun Luo;Qiang He;Rui Zhou;Kotagiri Ramamohanarao	2018		10.1007/978-3-030-02925-8_19	locality-sensitive hashing;data mining;anomaly detection;computer science;unsupervised learning;big data;sampling (statistics);density estimation;outlier;pattern recognition;artificial intelligence;data set	DB	-1.3448353745392254	-41.84357486378964	122690
1f6f70abd2f5afd5fc0c94a7ff1d999001558223	a new imputation method for small software project data sets	k nn imputation;data imputation;missing completely at random;software project management;class mean imputation;missing at random;software effort prediction;missing data;missing values	Effort prediction is a very important issue for software project management. Historical project data sets are frequently used to support such prediction. But missing data are often contained in these data sets and this makes prediction more difficult. One common practice is to ignore the cases with missing data, but this makes the originally small software project database even smaller and can further decrease the accuracy of prediction. The alternative is missing data imputation and there are many imputation methods. But small size (in terms of the number of cases) is usually an important characteristic of such software data sets, and complicated imputation methods prefer larger data sets, so we explore using simple methods to impute missing data in small project effort data sets. For this reason we propose a class mean imputation (CMI) method based k-NN hot deck imputation method (MINI) to impute both continuous and categorical missing data in small data sets. We use an incremental approach to increase the variance of population. To evaluate MINI (and k-NN and CMI methods as benchmarks) we use data sets with 50 cases and 100 cases sampled from a larger industrial data set with 10%, 15%, 20% and 30% missing data percentages respectively. We also simulated MCAR (Missing Completely At Random) and MAR (Missing At Random) missingness mechanisms. The result suggests that the MINI method outperforms both CMI and the k-NN methods. We conclude that this new imputation technique can be used to impute missing values in small data sets.	computer memories inc.;endeavour (supercomputer);experiment;geo-imputation;interaction;k-nearest neighbors algorithm;missing data;population;self-replicating machine;software engineering;software project management	Qinbao Song;Martin J. Shepperd	2007	Journal of Systems and Software	10.1016/j.jss.2006.05.003	missing data;engineering;data mining;imputation;statistics	ML	5.694867776831139	-43.22994351107388	122879
8e2d561b4497fe6bffdcef388c6c10341616ed45	clustering categorical data based on the relational analysis approach and mapreduce	categorical data;clustering;mapreduce;relational analysis approach	The traditional methods of clustering are unable to cope with the exploding volume of data that the world is currently facing. As a solution to this problem, the research is intensified in the direction of parallel clustering methods. Although there is a variety of parallel programming models, the MapReduce paradigm is considered as the most prominent model for problems of large scale data processing of which the clustering. This paper introduces a new parallel design of a recently appeared heuristic for hard clustering using the MapReduce programming model. In this heuristic, clustering is performed by efficiently partitioning categorical large data sets according to the relational analysis approach. The proposed design, called PMR-Transitive, is a single-scan and parameter-free heuristic which determines the number of clusters automatically. The experimental results on real-life and synthetic data sets demonstrate that PMR-Transitive produces good quality results.	categorical variable;cluster analysis;heuristic;mapreduce;pmr446;parallel computing;programming model;real life;synthetic data	Yasmine Lamari;Said Chah Slaoui	2017	Journal of Big Data	10.1186/s40537-017-0090-7	data mining;data science;computer science;data stream clustering;consensus clustering;conceptual clustering;machine learning;clustering high-dimensional data;programming paradigm;cluster analysis;brown clustering;canopy clustering algorithm;artificial intelligence	DB	-2.111975922559029	-39.558537128571984	123200
0b40af1ad2b9781fa14e999db2d7d3270b6d2862	data clustering: 50 years beyond k-means	category information distinguishes cluster;discriminant analysis;organizing data;data clustering;unsupervised learning;class label;category label;future data;supervised learning;classifying object;cluster analysis	Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering. 2009 Elsevier B.V. All rights reserved.	algorithm;algorithmic efficiency;ana (programming language);application domain;benchmark (computing);cluster analysis;combinatorial optimization;computation;data (computing);data mining;dimensionality reduction;dynamic data;emergence;emoticon;eye of the beholder;feature extraction;feature selection;fred (chatterbot);ground truth;holographic principle;identifier;k-means clustering;knowledge management;linear discriminant analysis;machine learning;mathematical optimization;np-hardness;os-tan;optimization problem;pattern recognition;requirement;semi-supervised learning;semiconductor industry;social network;statistical classification;supervised learning;taxonomy (general);text corpus;time series;unsupervised learning;well-posed problem;eric	Anil K. Jain	2008		10.1007/978-3-540-87479-9_3	artificial intelligence;machine learning;pattern recognition;data mining;mathematics	ML	2.520367586152191	-41.645301537571584	123397
f631bf1f9a2516c014410dea42f0d879d63c27c6	processing mutual nearest neighbor queries for moving object trajectories	moving object;nearest neighbor searches;nearest neighbor queries;set theory batch processing computers query processing;neural networks;query processing;cpu cost reduction mutual nearest neighbor queries moving object trajectories query interval query object batch processing reusing technology;conference management;set theory;data mining;multi layer neural network;artificial neural networks;three dimensional displays;nearest neighbor;mobile communication;batch process;batch processing computers;nearest neighbor searches multi layer neural network neural networks mobile computing conference management pattern recognition computer science decision making data mining costs;pattern recognition;cpu cost reduction;query object;query interval;computer science;mutual nearest neighbor queries;mobile computing;batch processing;algorithm design and analysis;moving object trajectories;reusing technology	Given a set of trajectories D, a query object (point or trajectory) q, and a query interval T, a mutual (i.e., symmetric) nearest neighbor (MNN) query over trajectories finds from D within T, the set of trajectories that are among the k1 nearest neighbors (NNs) of q, and meanwhile, have q as one of their k2 NNs. This type of queries considers proximity of q to the trajectories and the proximity of the trajectories to q, which is useful in many applications (e.g., decision making, data mining, pattern recognition, etc.). In this paper, we first formalize MNN query and identify some problem characteristics, and then develop two algorithms to process MNN queries efficiently. In particular, we thoroughly investigate two classes of queries, viz. MNNP and MNNT queries, which are defined w.r.t. stationary query points and moving query trajectories, respectively. Our techniques utilize the advantages of batch processing and reusing technology to reduce the I/O (i.e., number of node/page accesses) and CPU costs significantly. Extensive experiments demonstrate the efficiency and scalability of our proposed algorithms using both real and synthetic datasets.	batch processing;central processing unit;data mining;experiment;input/output;k-nearest neighbors algorithm;pattern recognition;scalability;stationary process;synthetic intelligence;viz: the computer game	Yunjun Gao;Gencai Chen;Qing Li;Baihua Zheng;Chun Li	2008	The Ninth International Conference on Mobile Data Management (mdm 2008)	10.1109/MDM.2008.17	computer science;operating system;machine learning;data mining;database;artificial neural network;batch processing	DB	-3.493634627109421	-42.10372913491487	123604
adbf490b5d792f67074452739ecb65d4a4aabcdd	identifying structure across pre-partitioned data	synthetic data;information theoretic	We propose an information-theoretic clustering approach that incorporates a pre-known partition of the data, aiming to identify common clusters that cut across the given partition. In the standard clustering setting the formation of clusters is guided by a single source of feature information. The newly utilized pre-partition factor introduces an additional bias that counterbalances the impact of the features whenever they become correlated with this known partition. The resulting algorithmic framework was applied successfully to synthetic data, as well as to identifying text-based cross-religion correspondences.	cluster analysis;information theory;synthetic data;text-based (computing)	Zvika Marx;Ido Dagan;Eli Shamir	2003			computer science;machine learning;pattern recognition;data mining;mathematics;statistics;synthetic data	ML	0.9566435391504127	-43.26961531553792	123760
0c611cce67b7ff59db41471d0e305f63d069c529	efficient genotype elimination via adaptive allele consolidation	dynamic change;vectors heuristic algorithms genetics bioinformatics computational biology polynomials couplings;celer tool;linkage analysis;pedigree genotype elimination allele consolidation;polynomials;genetics;genotype elimination;algorithms alleles computational biology computer simulation databases genetic female founder effect genotype haplotypes humans hypophosphatasia inheritance patterns male models genetic pedigree;celer tool genotype elimination adaptive allele consolidation lange goradia algorithm pedigrees mendelian inheritance law genetic data error linkage analysis haplotype imputation combinatorial problem;combinatorial problem;haplotype imputation;vectors;heuristic algorithms;molecular biophysics;mendelian inheritance law;couplings;pedigree;computational biology;lange goradia algorithm;molecular biophysics bioinformatics cellular biophysics combinatorial mathematics genetics;high performance;combinatorial mathematics;allele consolidation;pedigrees;cellular biophysics;genetic data error;adaptive allele consolidation;bioinformatics	"""We propose the technique of Adaptive Allele Consolidation, that greatly improves the performance of the Lange-Goradia algorithm for genotype elimination in pedigrees, while still producing equivalent output. Genotype elimination consists in removing from a pedigree those genotypes that are impossible according to the Mendelian law of inheritance. This is used to find errors in genetic data and is useful as a preprocessing step in other analyses (such as linkage analysis or haplotype imputation). The problem of genotype elimination is intrinsically combinatorial, and Allele Consolidation is an existing technique where several alleles are replaced by a single """"lumped” allele in order to reduce the number of combinations of genotypes that have to be considered, possibly at the expense of precision. In existing Allele Consolidation techniques, alleles are lumped once and for all before performing genotype elimination. The idea of Adaptive Allele Consolidation is to dynamically change the set of alleles that are lumped together during the execution of the Lange-Goradia algorithm, so that both high performance and precision are achieved. We have implemented the technique in a tool called Celer and evaluated it on a large set of scenarios, with good results."""	alleles;danny lange;excretory function;genotype;geo-imputation;linkage (software);lung consolidation;preprocessor;semiconductor consolidation;statistical imputation;algorithm;genetic pedigree	Nicoletta De Francesco;Giuseppe Lettieri;Luca Martini	2012	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2012.46	biology;genetic linkage;biotechnology;bioinformatics;coupling;pedigree chart;genetics;polynomial;molecular biophysics	PL	2.4198536772682524	-51.18508449479674	124235
c9e15de64b416b7bdb210f96559cfd5e0f657749	a novel mutual information-based feature selection algorithm		From a machine learning point of view to identify a subset of relevant features from a real data set can be useful to improve the results achieved by classification methods and to reduce their time and space complexity. To achieve this goal, feature selection methods are usually employed. These approaches assume that the data contains redundant or irrelevant attributes that can be eliminated. In this work we propose a novel algorithm to manage the optimization problem that is the foundation of the Mutual Information feature selection methods thus to formalize a novel approach that is also able to automatically estimates the number of dimensions to retain. The main advantages of this new approach are: the ability to automatically estimate the number of features to retain, and the possibility to rank the features to select from the most probable to the less probable. Experiments on standard real data sets and the comparison with state-of-the-art feature selection techniques confirms the high quality of our approach.	dspace;display resolution;experiment;feature selection;machine learning;mathematical optimization;mutual information;optimization problem;relevance;selection algorithm	Pietro Cassarà;Alessandro Rozza	2016	CoRR		machine learning;pattern recognition;data mining;mathematics;feature selection;feature	ML	7.911459652247653	-42.254149357883556	124477
7d035f484b0f0521070dcd6d767e6f76d93df1d2	combining gene expression profiles and protein-protein interactions for identifying functional modules	biology computing;bayesian model gene expression profile protein protein interaction functional module identification ppibm data analysis data integration data clustering;pattern clustering;proteins bayes methods biology computing data analysis data integration genetics pattern clustering;bayes methods;genetics;data analysis;accuracy usa councils proteins gene expression dvd bayesian methods machine learning;proteins;data integration	Identifying functional modules from protein-protein interaction networks is an important and challenging task. This paper presents a new approach called PPIBM which is designed to integrate gene expression data analysis and clustering of protein-protein interactions. The proposed approach relies on a Bayesian model which uses as its base protein-protein interactions given as part of input. The proposed method is evaluated with standard measures and its performance is compared with the state-of-the-art network analysis methods. Experimental results on both real-world data and synthetic data demonstrate the effectiveness of the proposed approach.	cluster analysis;interaction;synthetic data	Dingding Wang;Mitsunori Ogihara;Erliang Zeng;Tao Li	2012	2012 11th International Conference on Machine Learning and Applications	10.1109/ICMLA.2012.28	computer science;bioinformatics;data science;data integration;machine learning;data mining;cluster analysis;data analysis	SE	5.441322765983489	-49.81876763797194	124536
d1bab7eef1ba6bed0ab627e9e4030d1b4f442ae4	cross-domain data fusion	data fusion;computing;data analysis;big data;spotlight on transactions computing big data data fusion data analysis;spotlight on transactions	In big data, there are datasets with di erent representations, distributions, and scales in di erent domains. How can we unlock the power of knowledge from multiple disparate, but potentially connected, datasets? Addressing this challenge is of paramount importance in big data research. Integrating heterogeneous datasets is essentially what distinguishes big data from traditional data manipulation and analytic tasks. In “Methodologies for CrossDomain Data Fusion: An Overview,” Yu Zheng summarizes the crossdomain data-fusion methodologies for big data analytic tasks (IEEE Trans. Big Data, vol. 1, no. 1, 2015, pp. 16–34). His survey paper categorizes these tasks as stage-based, feature level–based, and semantic meaning–based data-fusion methods. The third category is further divided into four groups: multiview learning–based, similarity-based, probabilistic dependency–based, and transfer learning–based methods. Di erent from traditional data fusion studied in the database community, these cross-domain data-fusion methods focus on knowledge fusion rather than schema mapping and data merging (see Figure 1).	big data;sim lock	Qiang Yang	2016	IEEE Computer	10.1109/MC.2016.113	computing;big data;computer science;data science;operating system;data mining;data analysis;information retrieval	DB	-1.461930924479702	-42.19362564951189	124645
debb295d1b262b78a9d31d8ab169658c4c821b44	discretization of continuous attributes through low frequency numerical values and attribute interdependency	corrupt data detection;data mining;data pre processing;missing value imputation;data discretization;data cleansing	A new discretization technique called LFD.Does not require any user input.Interval width, number and frequency are automatically determined; all data driven.Minimizes information loss due to discretization by choosing low frequency cut points.Categorical attributes are taken as reference point for discretization. Discretization is the process of converting numerical values into categorical values. There are many existing techniques for discretization. However, the existing techniques have various limitations such as the requirement of a user input on the number of categories and number of records in each category. Therefore, we propose a new discretization technique called low frequency discretizer (LFD) that does not require any user input. There are some existing techniques that do not require user input, but they rely on various assumptions such as the number of records in each interval is same, and the number of intervals is equal to the number of records in each interval. These assumptions are often difficult to justify. LFD does not require any assumptions. In LFD the number of categories and frequency of each category are not pre-defined, rather data driven. Other contributions of LFD are as follows. LFD uses low frequency values as cut points and thus reduces the information loss due to discretization. It uses all other categorical attributes and any numerical attribute that has already been categorized. It considers that the influence of an attribute in discretization of another attribute depends on the strength of their relationship. We evaluate LFD by comparing it with six (6) existing techniques on eight (8) datasets for three different types of evaluation, namely the classification accuracy, imputation accuracy and noise detection accuracy. Our experimental results indicate a significant improvement based on the sign test analysis.	discretization;interdependence;numerical analysis	Md. Geaur Rahman;Md Zahidul Islam	2016	Expert Syst. Appl.	10.1016/j.eswa.2015.10.005	discretization error;computer science;machine learning;data mining;mathematics;data pre-processing;data cleansing;discretization of continuous features;statistics	DB	6.576551850896052	-42.679920399837584	124996
c8d51005bd53ee3cdb7a20a429adc63482f08f31	probabilistic estimation of overlap graphs for large sequence datasets	overlap graphs	Sequence overlap graphs, constructed based on suffix-prefix relationships between pairs of sequences, are an important data structure in computational biology. High throughput sequencers can read several million to a few billion DNA fragments in a single experiment, making the construction of overlap graphs for such datasets compute-intensive. In this paper, we present a Locality-Sensitive Hashing based parallel heuristic algorithm to construct overlap graphs for large genomic datasets. With reasonable assumptions on the characteristics of input sequences, we establish probabilistic bounds on the quality of the overlap graphs so produced. We demonstrate the validity and efficiency of our approach by comparing against true overlap graphs using datasets derived from small (E. coli) and large (H. sapiens) genomes.	algorithm;antivirus software;computation;computational biology;data structure;experiment;heuristic (computer science);k-mer;locality of reference;locality-sensitive hashing;longest common substring problem;maximal set;minhash;overhead (computing);parallel computing;throughput	Rahul Nihalani;Sriram P. Chockalingam;Shaowei Zhu;Vijay Vazirani;Srinivas Aluru	2017	2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2017.8217657	computer science;throughput;machine learning;artificial intelligence;genomics;heuristic (computer science);hash function;probabilistic logic;data structure;graph	DB	-0.7028545875497392	-51.73627507297115	125368
c806018cbcd5b58d76472b00c3924f82f7b11cc4	a hyper-solution framework for classification problems via metaheuristic approaches		This is a summary of the author’s PhD thesis, supervised by Prof. Domenico Conforti and defended on 26-02-2010 at the Universita della Calabria, Cosenza. The thesis is written in Italian and a copy is available from the author upon request. This work deals with the development of a high-level classification framework which combines parameters optimization of a single classifier with classifiers ensemble optimization, through meta-heuristics. Support Vector Machines (SVM) is used for learning while the meta-heuristics adopted and compared are Genetic-Algorithms (GA), Tabu-Search (TS) and Ant Colony Optimization (ACO). Single SVM optimization usually concerns two approaches: searching for optimal set up of a SVM with fixed kernel (Model Selection) or with linear combination of basic kernels (Multiple Kernel Learning), both issues were considered. Meta-heuristics were used in order to avoid time consuming grid-approach for testing several classifiers configurations and some ad-hoc variations to GA were proposed. Finally, different frameworks were developed and then tested on 8 datasets providing reliable solutions.	metaheuristic	Antonio Candelieri	2011	4OR	10.1007/s10288-011-0166-8	mathematical optimization;artificial intelligence;machine learning;data mining	Theory	9.7092141890492	-43.47941353339001	125591
d137283e897d28e8d9b6666d14dc46c0cd4abee8	a set correlation model for partitional clustering	distance measure;k means;shared neighbor;rsc;euclidean distance;data representation;data clustering;clustering;correlation;cluster model;sash;similarity measure;k means clustering	3 microarrays data sets with known class labels. B1: 384 genes x 17 conditions B2: 237 genes x 17 conditions B3: 205 genes x 20 conditions GlobalRSC The new model, GlobalRSC, resembles the famous Kmeans clustering heuristic, but with a sharedneighbor similarity measure instead of the Euclidean distance. Unlike K-means and most other clustering heuristics that can only work with real-valued data and distance measures taken from specific families, GlobalRSC has the advantage that it can work with any distance measure, and any data representation. Nguyen Xuan VINH National ICT Australia, University of New South Wales Michael E. HOULE NII	cluster analysis;dna microarray;data (computing);euclidean distance;heuristic (computer science);k-means clustering;national information infrastructure;similarity measure	Xuan Vinh Nguyen;Michael E. Houle	2010		10.1007/978-3-642-13657-3_4	complete-linkage clustering;correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;dendrogram;k-means clustering;clustering high-dimensional data	ML	2.7132915041561536	-42.45691257716214	125709
d4344a2e0b860a81105e0e231c981fef2dfbfa34	comparison of metaheuristics to measure gene effects on phylogenetic supports and topologies	binary particle swarm optimization;chloroplasts;genetic algorithms;lasso test;metaheuristics;phylogeny;simulated annealing	A huge and continuous increase in the number of completely sequenced chloroplast genomes, available for evolutionary and functional studies in plants, has been observed during the past years. Consequently, it appears possible to build large-scale phylogenetic trees of plant species. However, building such a tree that is well-supported can be a difficult task, even when a subset of close plant species is considered. Usually, the difficulty raises from a few core genes disturbing the phylogenetic information, due for example from problems of homoplasy. Fortunately, a reliable phylogenetic tree can be obtained once these problematic genes are identified and removed from the analysis.Therefore, in this paper we address the problem of finding the largest subset of core genomes which allows to build the best supported tree. As an exhaustive study of all core genes combination is untractable in practice, since the combinatorics of the situation made it computationally infeasible, we investigate three well-known metaheuristics to solve this optimization problem. More precisely, we design and compare distributed approaches using genetic algorithm, particle swarm optimization, and simulated annealing. The latter approach is a new contribution and therefore is described in details, whereas the two former ones have been already studied in previous works. They have been designed de novo in a new platform, and new experiments have been achieved on a larger set of chloroplasts, to compare together these three metaheuristics. The ways genes affect both tree topology and supports are assessed using statistical tools like Lasso or dummy logistic regression, in an hybrid approach of the genetic algorithm. By doing so, we are able to provide the most supported trees based on the largest subsets of core genes.	anatomy, regional;chloroplast proton-translocating atpases;chloroplasts;computational complexity theory;de novo transcriptome assembly;dummy variable (statistics);experiment;genetic algorithm;genome;large;largest;lasso;logistic regression;mathematical optimization;metaheuristic;optimization problem;particle swarm optimization;phylogenetic tree;phylogenetics;simulated annealing;subgroup;tree network;trees (plant)	Régis Garnier;Christophe Guyeux;Jean-François Couchot;Michel Salomon;Bashar Al-Nuaimi;Bassam AlKindy	2018		10.1186/s12859-018-2172-8	biology;bioinformatics;lasso (statistics);simulated annealing;genetic algorithm;metaheuristic;phylogenetics;artificial intelligence;phylogenetic tree;particle swarm optimization;pattern recognition;optimization problem	Comp.	2.097821163053722	-51.102153740044145	125774
7996342571103fd313142adaf2e33d4afb88c4d4	two-stage clustering using one-pass k-medoids and medoid-based agglomerative hierarchical algorithms	pattern clustering;two stage method k medoids weighted network ward type linkage;two stage method;medoid based agglomerative hierarchical procedure two stage clustering one pass k medoid medoid based agglomerative hierarchical algorithm weighted network clustering medoid based clustering agglomerative hierarchical clustering cluster generation one pass k medoids;clustering algorithms couplings euclidean distance indexes kernel linear programming educational institutions;weighted network;k medoids;ward type linkage	Clustering on weighted networks needs consideration of particular techniques. Medoid-based clustering seems promising in this sense. Although K-medoids have already been studied, agglomerative hierarchical clustering using medoids should also be studied. A drawback of agglomerative hierarchical clustering is that large number of objects cannot be handled, while it has the advantage of having much information on the process of cluster generation. We propose a two-stage method of clustering that has the advantage of agglomerative hierarchical clustering and handles a large number of objects. In the first stage a one-pass K-medoids++ is used to have a medium number of small clusters, and then medoid-based agglomerative hierarchical procedure is applied. Numerical examples show effectiveness of the proposed method.	algorithm;cluster analysis;hierarchical clustering;k-medoids;medoid;numerical method;weighted network	Yusuke Tamura;Sadaaki Miyamoto	2014	2014 Joint 7th International Conference on Soft Computing and Intelligent Systems (SCIS) and 15th International Symposium on Advanced Intelligent Systems (ISIS)	10.1109/SCIS-ISIS.2014.7044641	complete-linkage clustering;correlation clustering;data stream clustering;weighted network;k-medians clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;ward's method;cluster analysis;single-linkage clustering;brown clustering;k-medoids;dendrogram;hierarchical clustering of networks	EDA	2.750017792271944	-39.8993246808994	125856
23507312de08a08843de60dff5dedd6d7b87e7bf	a decision-theoretic rough set approach to spam filtering	filtering unsolicited electronic mail bayes methods support vector machines training probabilistic logic;filtering;unsolicited electronic mail;support vector machines;bayes methods;training;bayesian decision theory spam filtering decision theoreic rough sets α positive region;unsolicited e mail bayes methods decision theory information filtering rough set theory security of data;minimum risk bayesian decision theory decision theoretic rough set dtrs spam filtering α positive region attribute reduction theorem spam email attribute reduction;probabilistic logic	Spam filtering is a research hotspot of information security. For the weak fault-tolerant ability of traditional filtering methods, an approach to spam filtering based on α - positive-region of decision-theoretic rough set (DTRS) is developed. Firstly, α -positive-region attribute reduction theorem is adopted to reduce email attributes. Then, according to the minimum risk Bayesian decision theory, a three-way decision, named spam, doubt and non-spam, is realized by depicting the undecided emails using boundary region of DTRS. The simulation results show that this approach is effective and helpful to improve the performance of spam filtering.	anti-spam techniques;binary classification;computation;dvd region code;decision theory;email filtering;fault tolerance;information security;java hotspot virtual machine;loss function;precision and recall;rough set;simulation;spamming	Chunsheng Zhao;Wei Zeng;Mai Jiang;Zhiyong He	2013	2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2013.6816180	filter;support vector machine;computer science;bag-of-words model;machine learning;pattern recognition;data mining;probabilistic logic	DB	6.991203265122169	-38.23901572733754	125882
a2f21e787bb32ef149826affceb9f3f3fe8e1be3	automatic fuzzy clustering based on mistake analysis	pattern clustering;pattern clustering fuzzy set theory iterative methods merging;clustering algorithms indexes algorithm design and analysis robustness merging partitioning algorithms pattern recognition;fuzzy set theory;iterative methods;mistake analysis automatic fuzzy clustering method iterative splitting operation iterative merging operation mistake measurement validity index fcm;merging	This paper presents a robust fuzzy clustering algorithm which can perform clustering without pre-assigning the number of clusters and is not sensitive to the initialization of cluster centers. This is achieved by iteratively splitting and merging operations under the guidance of mistake measurements. In every step of the iteration, we first split the cluster containing data points belonging to different classes, and then merge the wrongly divided cluster pair. A validity index is proposed based on the two mistake measurements to determine the termination of the clustering process. Experimental results confirm the effectiveness and robustness of the proposed clustering algorithm.	algorithm;cluster analysis;data point;data structure;experiment;fuzzy clustering;iteration	Shenglan Ben;Zhong Jin;Jingyu Yang	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;iterative method;fuzzy set;cluster analysis;single-linkage clustering;brown clustering;clustering high-dimensional data	DB	2.812766644487433	-39.712351197588106	125924
e5e6c0971287b8e031d628c65c074501e9b4e047	clustering by growing incremental self-organizing neural network	unsupervised learning;reduction;k means;parameter;incremental learning;clustering;data visualization;self organizing neural networks;algorithms;distance	A topological graph visualizes both data partitions and details of each cluster.The presented approach aims to learn data distribution of each cluster.The number of clusters is not a prerequisite for clustering tasks.The proposed method is able to detect arbitrary-shaped clusters. This paper presents a new clustering algorithm that detects clusters by learning data distribution of each cluster. Different from most existing clustering techniques, the proposed method is able to generate a dynamic two-dimensional topological graph which is used to explore both partitional information and detailed data relationship in each cluster. In addition, the proposed method is also able to work incrementally and detect arbitrary-shaped clusters without requiring the number of clusters as a prerequisite. The experimental data sets including five artificial data sets with various data distributions and an original hand-gesture data set are used to evaluate the proposed method. The comparable experimental results demonstrate the superior performance of the proposed algorithm in learning robustness, efficiency, working with outliers, and visualizing data relationships.	artificial neural network;organizing (structure);self-organization	Hao Liu;Xiao-juan Ban	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.02.006	unsupervised learning;correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;reduction;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;competitive learning;distance;parameter;data visualization;affinity propagation;k-means clustering;clustering high-dimensional data;conceptual clustering	ML	1.8349487031892293	-41.908894527549364	125999
9998f3e84092eaa588086ac9fbd662cc206550ee	a parallel ant colonies approach to de novo prediction of protein backbone in casp8/9	ant colony optimization;protein backbone;protein backbone prediction;heuristic algorithms;protein backbone prediction parallel ant colony optimization;parallel;protein folding;de novo prediction;parallel algorithms	Predicting the three-dimensional structure of proteins from amino acid sequences with only a few remote homologs, or de novo prediction, remains a major challenge in computational biology. The modeling of the protein backbone represents the initial phase of a protein structure prediction process. Using a parallel ant colony optimization based on sharing one pheromone matrix, this report proposes a parallel approach to predict the structure of a protein backbone. The parallel approach combines various sources of energy functions and generates protein backbones with the lowest energies jointly determined by the various energy functions. All free modeling targets in CASP8/9 are used to evaluate the performance of the method. For 13 targets in CASP8, two out of the predicted model1s selected by our approach are the best of the published CASP8 results, and seven out of the model1s are ranked in the top 10. For 29 targets in CASP9, 20 out of the best models from our predictions are ranked in the top 10, and 11 out of the model1s are ranked in the top 10.	ant colony optimization algorithms;casp;computational biology;de novo protein structure prediction;de novo transcriptome assembly;dennis shasha;entity–relationship model;executable;experiment;fm broadcasting;internet backbone;mathematical optimization;nondeterministic algorithm;refinement (computing)	Qiang Lv;Hongjie Wu;Jinzhen Wu;Xu Huang;Xiaohu Luo;Peide Qian	2011	Science China Information Sciences	10.1007/s11432-011-4444-z	protein folding;peptide bond;mathematical optimization;ant colony optimization algorithms;computer science;bioinformatics;machine learning;parallel;mathematics;parallel algorithm	Comp.	-1.5183627326834146	-49.37128431307866	126046
4a8da906a949c1eee2aea780ad73b12d85489d3b	topological rearrangements and local search method for tandem duplication trees	phylogeny;proteins;sequences;indexing terms;zinc finger;tandem duplication;genetics;molecular biophysics;parsimony;local search	The problem of reconstructing the duplication history of a set of tandemly repeated sequences was first introduced by Fitch (1977). Many recent studies deal with this problem, showing the validity of the unequal recombination model proposed by Fitch, describing numerous inference algorithms, and exploring the combinatorial properties of these new mathematical objects, which are duplication trees. In this paper, we deal with the topological rearrangement of these trees. Classical rearrangements used in phylogeny (NNI, SPR, TBR, ...) cannot be applied directly on duplication trees. We show that restricting the neighborhood defined by the SPR (Subtree Pruning and Regrafting) rearrangement to valid duplication trees, allows exploring the whole duplication tree space. We use these restricted rearrangements in a local search method which improves an initial tree via successive rearrangements. This method is applied to the optimization of parsimony and minimum evolution criteria. We show through simulations that this method improves all existing programs for both reconstructing the topology of the true tree and recovering its duplication events. We apply this approach to tandemly repeated human Zinc finger genes and observe that a much better duplication tree is obtained by our method than using any other program.	algorithm;anatomy, regional;attack tree;computation;dna sequence rearrangement;finger tree;fitch notation;gene duplication abnormality;genome;homology (biology);inference;local search (optimization);mathematical optimization;mathematics;maxima and minima;maximum parsimony (phylogenetics);network topology;nut hypersensitivity;occam's razor;phylogenetic tree;simulation;stochastic process;tandem computers;tandem repeat sequences;tree rearrangement;trees (plant)	Denis Bertrand;Olivier Gascuel	2005	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1145/1057651.1057654	biology;zinc finger;repeated sequence;computer science;bioinformatics;artificial intelligence;local search;zinc;genetics;algorithm;phylogenetics;search algorithm	Comp.	1.086969586694034	-51.95857117531232	126182
fcd335cf94ad3d1a1d0784cc81ada65ddceb169c	a novel faster approximate fuzzy clustering approach with highly accurate results		Clustering has been used extensively for exploratory data analysis. GK clustering algorithm can provide a data partition that is more meaningful than the standard fuzzy c-means and its variants. In this paper we propose a novel approach towards fuzzy clustering which reduces the processing time significantly while keeping the results highly accurate. It is a matrix based approach using the concept of equivalent samples and the weighting samples. Equivalence is measured in terms of proximity of the samples and then weighted samples are used as an input to the modified GK clustering algorithm. Objective function and validation index estimates are used to assess the goodness of partition. Experimental results are shown to emphasize the benefits of the proposed technique in domains like Telecom where we have massive data sets to be processed for real time clustering and recommendation engines.	fuzzy clustering	Gargi Aggarwal;Mahinder Pal Singh Bhatia	2012		10.1007/978-3-642-32129-0_25	correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;brown clustering;biclustering;statistics;clustering high-dimensional data	NLP	0.7353237884788475	-40.58983652877992	126263
479673eaeb0b4bbeffb253aceb95ea11795767f2	analysis of gene expression data based on density and biological knowledge	distance based clustering method;cluster algorithm;functional annotation;pattern clustering bioinformatics genetics;pattern clustering;kernel;clustering algorithms kernel gene expression algorithm design and analysis noise proposals;biological interpretability;gene expression data;genetics;gene expression;cluster analysis;clustering method;clustering algorithms;density;gene function;proposals;distance based clustering method gene expression cluster analysis biological knowledge biological interpretability;biological knowledge gene expression data density;algorithm design and analysis;measurement noise;biological knowledge;noise;bioinformatics	Cluster analysis of gene expression data is one of the most useful tools for identifying biologically relevant groups of genes, however, gene expression data suffer severely from the problems of measurement noise, dimension curse, high redundancy between genes, and the functional annotation of genes is incomplete and imprecise. These properties lead to most of the traditional clustering algorithms are very sensitive to the initialization, and are likely to get the local result, and also made the analysis results lacking of stability, reliability and biological interpretability. In the present article, we propose incorporating the data density and gene functions into distance-based clustering method, which can get more stable and reliable results, especially in discovering gene set with completely unknown function.	algorithm;areal density (computer storage);cluster analysis;gene regulatory network	Xu Zhou;Hang Sun;De-Ping Wang;Yu Zhang;You Zhou	2010	2010 Fifth International Conference on Frontier of Computer Science and Technology	10.1109/FCST.2010.97	computer science;bioinformatics;machine learning;data mining;cluster analysis	HPC	4.989437511403635	-48.911193498250334	126515
a25c595caa63bbc231fe7d341404c6523faddd56	comparison of intelligent classification techniques applied to marble classification	classification algorithm;soft computing;simulated annealing;genetics;genetic algorithm;subjective evaluation;fuzzy classifier	  Automatic marbles classification based on their visual appearance is an important industrial issue. However, there is no definitive  solution to the problem, mainly due to the presence of randomly distributed high number of different colors and due to the  subjective evaluation made by human experts. In this paper, we present a study of soft computing classification algorithms,  which proved to be a valuable tool to be applied in this type of problems. Fuzzy, neural, simulated annealing, genetic and  combinations of these approaches are compared. Color and vein classification of marbles are compared. The combination of fuzzy  classifiers optimized by genetic algorithms revealed to be the best classifier for this application.    		João Miguel da Costa Sousa;João Rogério Caldas Pinto	2004		10.1007/978-3-540-30126-4_97	genetic algorithm;simulated annealing;computer science;artificial intelligence;machine learning;pattern recognition;soft computing	NLP	9.146870389474161	-40.73132298339814	126524
92430dd8a9127917c9e811abd811b40238eb9a08	enhanced situation space mining for data streams		Data streams can capture the situation which an actor is experiencing. Knowledge of the present situation is highly beneficial for a wide range of applications. An algorithm called pcStream can be used to extract situations from a numerical data stream in an unsupervised manner. Although pcStream outperforms other stream clustering algorithms at this task, pcStream has two major flaws. The first is its complexity due to continuously performing principal component analysis (PCA). The second is its difficulty in detecting emerging situations whose distributions overlap in the same feature space.  In this paper we introduce pcStream2, a variant of pcStream which employs windowing and persistence in order to distinguish between emerging overlapping concepts. We also propose the use of incremental PCA (IPCA) to reduce the overall complexity and memory requirements of the algorithm. Although any IPCA algorithm can be used, we use a novel IPCA algorithm called Just-In-Time PCA which is better suited for processing streams. JIT-PCA makes intelligent 'short cuts' in order to reduce computations. We provide experimental results on real-world datasets that demonstrates how the proposed improvements make pcStream2 a more accurate and practical tool for situation space mining.	algorithm;cluster analysis;computation;feature vector;just-in-time compilation;level of measurement;numerical analysis;persistence (computer science);principal component analysis;requirement;sensor;stream (computing);stream processing;unsupervised learning	Yisroel Mirsky;Tal Halpern;Rishabh Upadhyay;Sivan Toledo;Yuval Elovici	2017		10.1145/3019612.3019671	streams;data stream;cluster analysis;principal component analysis;data stream mining;feature vector;persistence (computer science);asteroid mining;artificial intelligence;machine learning;computer science	ML	8.1067705948476	-42.13495368750476	126785
4b1faddaa8a65104fdc79ac00679530a0aabb474	a l1-regularized feature selection method for local dimension reduction on microarray data	microarray data;classification;l1 regularized logistic regression;local dimension reduction;partial least squares pls	Dimension reduction is a crucial technique in machine learning and data mining, which is widely used in areas of medicine, bioinformatics and genetics. In this paper, we propose a two-stage local dimension reduction approach for classification on microarray data. In first stage, a new L1-regularized feature selection method is defined to remove irrelevant and redundant features and to select the important features (biomarkers). In the next stage, PLS-based feature extraction is implemented on the selected features to extract synthesis features that best reflect discriminating characteristics for classification. The suitability of the proposal is demonstrated in an empirical study done with ten widely used microarray datasets, and the results show its effectiveness and competitiveness compared with four state-of-the-art methods. The experimental results on St Jude dataset shows that our method can be effectively applied to microarray data analysis for subtype prediction and the discovery of gene coexpression.	bioinformatics;biological markers;classification;computer vision;data mining;dimensionality reduction;excretory function;experiment;feature extraction;feature selection;image noise;machine learning;microarray;papillon-lefevre disease;relevance;silo (dataset);stage level 1	Shun Guo;Donghui Guo;Lifei Chen;Qingshan Jiang	2017	Computational biology and chemistry	10.1016/j.compbiolchem.2016.12.010	biology;microarray analysis techniques;biological classification;computer science;bioinformatics;pattern recognition;data mining;mathematics	ML	8.820295133057229	-48.49681133122494	126969
8e92809372b9ccad732769a66448adc2ae631441	density-ratio based clustering for discovering clusters with varying densities	density based clustering;varying densities;scaling;density ratio	Density-based clustering algorithms are able to identify clusters of arbitrary shapes and sizes in a dataset which contains noise. It is well-known that most of these algorithms, which use a global density threshold, have difficulty identifying all clusters in a dataset having clusters of greatly varying densities. This paper identifies and analyses the condition under which density-based clustering algorithms fail in this scenario. It proposes a density-ratio based method to overcome this weakness, and reveals that it can be implemented in two approaches. One approach is to modify a density-based clustering algorithm to do density-ratio based clustering by using its density estimator to compute density-ratio. The other approach involves rescaling the given dataset only. An existing density-based clustering algorithm, which is applied to the rescaled dataset, can find all clusters with varying densities that would otherwise impossible had the same algorithm been applied to the unscaled dataset. We provide an empirical evaluation using DBSCAN, OPTICS and SNN to show the effectiveness of these two approaches. HighlightsAnalyse a key weakness of density-based clustering algorithms.Introduce two approaches based on density-ratio to overcome this weakness.ReCon converts an existing density estimator to a density-ratio estimator.ReScale transforms a dataset by an adaptive scaling based on density-ratio.ReCon and ReScale approaches improve three density-based clustering algorithms.	cluster analysis	Ye Zhu;Kai Ming Ting;Mark James Carman	2016	Pattern Recognition	10.1016/j.patcog.2016.07.007	correlation clustering;constrained clustering;econometrics;determining the number of clusters in a data set;data stream clustering;subclu;k-medians clustering;fuzzy clustering;flame clustering;scaling;canopy clustering algorithm;cure data clustering algorithm;data mining;mathematics;cluster analysis;brown clustering;dbscan;affinity propagation;statistics;clustering high-dimensional data	Vision	0.3962938885980936	-40.92472913520008	127005
bc873b15689288f6dfa954b11d17e6834a368365	machine learning models in error and variant detection in high-variation high-throughput sequencing datasets		In high-variation genomics datasets, such as found in metagenomics or complex polyploid genome analysis, error detection and variant calling are impeded by the difficulty in discerning sequencing errors from actual biological variation. Confirming base candidates with high frequency of occurrence is no longer a reliable measure, because of the natural variation and the presence of rare bases. This work employs machine learning models to classify bases into erroneous and rare variations, after preselecting potential error candidates with a weighted frequency measure, which aims to focus on unexpected variations by using the inter-sequence pairwise similarity. Different similarity measures are used to account for different types of datasets. Four machine learning models are tested.	error detection and correction;high-throughput computing;machine learning;metagenomics;similarity learning;throughput	Milko Krachunov;Maria Nisheva;Dimitar Vassilev	2017		10.1016/j.procs.2017.05.242	error detection and correction;machine learning;genomics;artificial intelligence;computer science;genome;pairwise comparison;dna sequencing;metagenomics;bioinformatics	ML	4.156080205984993	-50.00324429437089	127010
29a18c4fb12161fec641ed8159e5482c4de2c750	exploiting the essential assumptions of analogy-based effort estimation	albrecht data set;software;pattern clustering;software cost estimation;project management;supertree variance;trees mathematics pattern clustering program testing project management software cost estimation;estimation training software training data linear regression euclidean distance humans;analogy based effort estimation;project data;software effort estimation;training;estimation variance;linear regression;cluster subtrees;turkish companies;euclidean distance;isbsg data set;trees mathematics;nearest neighbor method;dynamic selection;desharnais data set;training data;subtree variance;nearest neighbor selection;program testing;effort estimation;estimation;k nn;nearest neighbor;analogy;essential assumption;humans;coc81 data set;software effort estimator design;binary cluster tree;project data analogy based effort estimation software effort estimator design essential assumption supertree variance subtree variance coc81 data set nasa93 data set desharnais data set albrecht data set isbsg data set turkish companies estimation variance binary cluster tree cluster subtrees dynamic selection nearest neighbor selection;k nn software cost estimation analogy;nasa93 data set;binary tree	Background: There are too many design options for software effort estimators. How can we best explore them all? Aim: We seek aspects on general principles of effort estimation that can guide the design of effort estimators. Method: We identified the essential assumption of analogy-based effort estimation, i.e., the immediate neighbors of a project offer stable conclusions about that project. We test that assumption by generating a binary tree of clusters of effort data and comparing the variance of supertrees versus smaller subtrees. Results: For 10 data sets (from Coc81, Nasa93, Desharnais, Albrecht, ISBSG, and data from Turkish companies), we found: 1) The estimation variance of cluster subtrees is usually larger than that of cluster supertrees; 2) if analogy is restricted to the cluster trees with lower variance, then effort estimates have a significantly lower error (measured using MRE, AR, and Pred(25) with a Wilcoxon test, 95 percent confidence, compared to nearest neighbor methods that use neighborhoods of a fixed size). Conclusion: Estimation by analogy can be significantly improved by a dynamic selection of nearest neighbors, using only the project data from regions with small variance.	binary tree;cost estimation in software engineering;k-nearest neighbors algorithm;tree (data structure)	Ekrem Kocaguneli;Tim Menzies;Ayse Basar Bener;Jacky W. Keung	2012	IEEE Transactions on Software Engineering	10.1109/TSE.2011.27	project management;nearest neighbour algorithm;training set;estimation;analogy;binary tree;computer science;linear regression;pattern recognition;data mining;euclidean distance;k-nearest neighbors algorithm;statistics	SE	6.721244596754059	-40.50985800122097	127173
dd926d543e2a86ceae9ca0b761230596b8fdea03	phd: an efficient data clustering scheme using partition space technique for knowledge discovery in large databases	cluster algorithm;k means;data mining;density based clustering;data clustering;number of clusters;partitioning clustering	Rapid technological advances imply that the amount of data stored in databases is rising very fast. However, data mining can discover helpful implicit information in large databases. How to detect the implicit and useful information with lower time cost, high correctness, high noise filtering rate and fit for large databases is of priority concern in data mining, specifying why considerable clustering schemes have been proposed in recent decades. This investigation presents a new data clustering approach called PHD, which is an enhanced version of KIDBSCAN. PHD is a hybrid density-based algorithm, which partitions the data set by K-means, and then clusters the resulting partitions with IDBSCAN. Finally, the closest pairs of clusters are merged until the natural number of clusters of data set is reached. Experimental results reveal that the proposed algorithm can perform the entire clustering, and efficiently reduce the run-time cost. They also indicate that the proposed new clustering algorithm conducts better than several existing well-known schemes such as the K-means, DBSCAN, IDBSCAN and KIDBSCAN algorithms. Consequently, the proposed PHD algorithm is efficient and effective for data clustering in large databases.	approximation algorithm;closest pair of points problem;cluster analysis;correctness (computer science);dbscan;data mining;database;k-means clustering;local search (optimization);national supercomputer centre in sweden;run time (program lifecycle phase);scalability;simulation	Cheng-Fa Tsai;Heng-Fu Yeh;Jui-Fang Chang;Ning-Han Liu	2010	Applied Intelligence	10.1007/s10489-010-0239-y	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;subclu;k-medians clustering;fuzzy clustering;flame clustering;computer science;data science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;database;cluster analysis;single-linkage clustering;brown clustering;dbscan;optics algorithm;biclustering;affinity propagation;k-means clustering;clustering high-dimensional data	ML	-3.7701740399962165	-39.07768054682656	127337
0ac0b777d7162fbf09a0137689a696774fe3d1bd	risk aversion as an evolutionary adaptation	health research;uk clinical guidelines;biological patents;animals;europe pubmed central;models theoretical;citation search;biological evolution;uk phd theses thesis;risk;adaptation physiological;life sciences;humans;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	Risk aversion is a common behavior universal to humans and animals alike. Economists have traditionally defined risk preferences by the curvature of the utility function. Psychologists and behavioral economists also make use of concepts such as loss aversion and probability weighting to model risk aversion. Neurophysiological evidence suggests that loss aversion has its origins in relatively ancient neural circuitries (e.g., ventral striatum). Could there thus be an evolutionary origin to risk aversion? We study this question by evolving strategies that adapt to play the equivalent mean payoff gamble. We hypothesize that risk aversion in this gamble is beneficial as an adaptation to living in small groups, and find that a preference for risk averse strategies only evolves in small populations of less than 1,000 individuals, or in populations segmented into groups of 150 individuals or fewer - numbers thought to be comparable to what humans encountered in the past. We observe that risk aversion only evolves when the gamble is a rare event that has a large impact on the individual's fitness. As such, we suggest that rare, high-risk, high-payoff events such as mating and mate competition could have driven the evolution of risk averse behavior in humans living in small groups.	acclimatization;extreme value theory;gambling;national origin;neostriatum;population;risk aversion;utility;ventral striatum;hearing impairment	Arend Hintze;Randal S. Olson;Christoph Adami;Ralph Hertwig	2015	Scientific reports	10.1038/srep08242	risk aversion;biology;medicine;bioinformatics;ambiguity aversion;artificial intelligence;risk;operations research	ML	-4.188557556464369	-46.15640641385295	127350
5bbb28ac6a8124154e660698188db02960b2b116	penalized feature selection and classification in bioinformatics	software;aplicacion;penalization;bioinformatique;models biological;classification;cluster analysis;artificial intelligence;algorithms;feature selection;pattern recognition automated;bioinformatica;extraction caracteristique;computational biology;application;computer simulation;clasificacion;bioinformatics application;bioinformatics	In bioinformatics studies, supervised classification with high-dimensional input variables is frequently encountered. Examples routinely arise in genomic, epigenetic and proteomic studies. Feature selection can be employed along with classifier construction to avoid over-fitting, to generate more reliable classifier and to provide more insights into the underlying causal relationships. In this article, we provide a review of several recently developed penalized feature selection and classification techniques--which belong to the family of embedded feature selection methods--for bioinformatics studies with high-dimensional input. Classification objective functions, penalty functions and computational algorithms are discussed. Our goal is to make interested researchers aware of these feature selection and classification methods that are applicable to high-dimensional bioinformatics data.	algorithm;bioinformatics;causality;embedded system;embedding;feature selection;machine learning;overfitting;proteomics;supervised learning;study of epigenetics	Shuangge Ma;Jian Huang	2008	Briefings in bioinformatics	10.1093/bib/bbn027	computer simulation;biological classification;computer science;bioinformatics;machine learning;linear classifier;pattern recognition;data mining;cluster analysis;feature selection;statistics	Comp.	7.408159567826689	-49.73492165291284	127357
8e98ace2fdaf1647e63660837df061e5769c2952	meta-analysis of protein structural alignment	optimisation;meta program;protein alignment;molecular configurations;benchmark test;meta analysis;structure comparison;proteins molecular biophysics molecular configurations optimisation;proteins;molecular biophysics;tm align optimization method protein structural alignment three dimensional structure protein molecule biological function protein structural analysis heuristics meta analysis approach benchmark pairwise alignments;meta program meta analysis protein alignment structure comparison benchmark test;proteins synthetic aperture sonar principal component analysis optimization protein engineering software clustering algorithms	The three-dimensional structure of a protein molecule provides significant insight into its biological function. Structural alignment of proteins is an important and widely performed task in the analysis of protein structures, whereby functionally and evolutionarily important segments are identified. However, structural alignment is a computationally difficult problem and a large number of heuristics introduced to solve it do not agree on their results. Consequently, there is no widely accepted solution to the structure alignment problem. In this study, we present a meta-analysis approach to generate a re-optimized, best-of-all result using the alignments generated from several popular methods. Evaluations of the methods on a large set of benchmark pairwise alignments indicate that TM-align provides superior alignments (except for RMSD), compared to other methods we have surveyed. Smolign provides smaller cores than other methods with best RMSD values. The re-optimization of the alignments using TM-align's optimization method does not alter the relative performance of the methods. Additionally, visualization approaches to delineate the relationships of the alignment methods have been performed and their results provided.	align (company);benchmark (computing);cluster analysis;compiler;converge;function (biology);heuristic (computer science);input/output;mathematical optimization;parsing;proteomics;sas	Jim Havrilla;Ahmet Sacan	2012	2012 IEEE International Conference on Bioinformatics and Biomedicine Workshops	10.1109/BIBMW.2012.6470218	biochemistry;structural alignment;meta-analysis;computer science;bioinformatics;machine learning;data mining;molecular biophysics	Comp.	-0.36387351542579854	-50.715240373887916	127448
b21b1f9fc14d14f73754217d4744c2ac14513696	a complexity-invariant measure based on fractal dimension for time series classification		Classification is an important task in time series mining. It is often reported in the literature that nearest neighbor classifiers perform quite well in time series classification, especially if the distance measure properly deals with invariances required by the domain. Complexity invariance was recently introduced, aiming to compensate from a bias towards classes with simple time series representatives in nearest neighbor classification. To this end, a complexity correcting factor based on the ratio of the more complex to the simpler series was proposed. The original formulation uses the length of the rectified time series to estimate its complexity. In this paper we investigate an alternative complexity estimate, based on fractal dimension. Results show that this alternative is very competitive with the original proposal, and has a broader application as it does neither depend on the number of points in the series nor on a previous normalization. Furthermore, these results also verify, using a different formulation, the validity of complexity invariance in time series classification.	database normalization;fractal dimension;k-nearest neighbors algorithm;time series	Ronaldo C. Prati;Gustavo E. A. P. A. Batista	2012	IJNCR	10.4018/jncr.2012070104	machine learning	ML	7.4730665912836	-42.303195633986135	127450
14bbed0fe7dbec601b75f33d940a9e758dde29af	evolutionary spectral co-clustering	pattern clustering;evolutionary computation;matrix algebra;data mining;pattern clustering evolutionary computation matrix algebra;co clustering;spectral clustering data mining clustering co clustering evolving data;computer science evolutionary spectral co clustering rochester institute of technology manjeet rege green;spectral clustering;clustering;clustering algorithms history partitioning algorithms matrix decomposition equations noise gaussian distribution;clustering quality evolutionary spectral co clustering algorithm larger data matrix sub matrices escc respect to the current approach respect to historical approach;evolving data;nathan s	Co-clustering is the problem of deriving sub-matrices from the larger data matrix by simultaneously clustering rows and columns of the data matrix. Traditional co-clustering techniques are inapplicable to problems where the relationship between the instances (rows) and features (columns) evolve over time. Not only is it important for the clustering algorithm to adapt to the recent changes in the evolving data, but it also needs to take the historical relationship between the instances and features into consideration. We present ESCC, a general framework for evolutionary spectral co-clustering. We are able to efficiently co-cluster evolving data by incorporation of historical clustering results. Under the proposed framework, we present two approaches, Respect To the Current (RTC), and Respect To Historical (RTH). The two approaches differ in the way the historical cost is computed. In RTC, the present clustering quality is of most importance and historical cost is calculated with only one previous time-step. RTH, on the other hand, attempts to keep instances and features tied to the same clusters between time-steps. Extensive experiments performed on synthetic and real world data, demonstrate the effectiveness of the approach.	algorithm;biclustering;blog;cluster analysis;co-np;column (database);evolutionary data mining;experiment;mashup (web application hybrid);rss;server (computing);singular value decomposition;synthetic data;well-formed element;wiki	Nathan Green;Manjeet Rege;Xumin Liu;Reynold Bailey	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033342	correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;data science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;spectral clustering;affinity propagation;evolutionary computation;clustering high-dimensional data;conceptual clustering	ML	-0.6027476310520017	-39.95106016517684	127464
5c20e4bd662bc7079d81c0a78aa741998ecc5567	real-valued negative selection algorithm with a quasi-monte carlo genetic detector generation	negative selection;automatic generation;genetics;analog circuits;fault detection;quasi monte carlo;detector generation;genetic algorithm;genetic algorithms	A new scheme for detector generation for the Real-Valued Negative Selection Algorithm (RNSA) is presented. The proposed method makes use of genetic algorithms and Quasi-Monte Carlo Integration to automatically generate a small number of very efficient detectors. Results have demonstrated that a fault detection system with detectors generated by the proposed scheme is able to detect faults in analog circuits and in a ball bearing dataset.	analogue electronics;fault detection and isolation;genetic algorithm;monte carlo integration;monte carlo method;quasi-monte carlo method;selection algorithm;sensor	Jorge Luís Machado do Amaral;José Franco Machado do Amaral;Ricardo Tanscheit	2007		10.1007/978-3-540-73922-7_14	genetic algorithm;computer science;theoretical computer science;machine learning	EDA	9.515614958774604	-40.442144110070714	127555
3b66923bbf66e263e7a5c9351cb655af7de5c1ac	a class of evolution-based kernels for protein homology analysis: a generalization of the pam model	kernel derived clusters;protein family;computer science and information systems;amino acid;evolution modeling;evolutionary kernel function;k means;amino acid sequence;amino acid sequence alignment;kernel function;protein homology analysis;linear space;similarity measure;k means clustering;local alignment	There are two desirable properties that a pair-wise similarity measure between amino acid sequences should possess in order to produce good performance in protein homology analysis. First, it is the presence of kernel properties that allow using popular and well-performing computational tools designed for linear spaces, like SVM and k-means. Second, it is very important to take into account common evolutionary descent of homologous proteins. However, none of the existing similarity measures possesses both of these properties at once. In this paper, we propose a simple probabilistic evolution model of amino acid sequences that is built as a straightforward generalization of the PAM evolution model of single amino acids. This model produces a class of kernel functions each of which is computed as the likelihood of the hypothesis that both sequences are results of two independent evolutionary transformations of a hidden common ancestor under some specific assumptions on the evolution mechanism. The proposed class of kernels is rather wide and contains as particular subclasses not only the family of J.-P Vert’s local alignment kernels, whose algebraic structure was introduced without any evolutionary motivation, but also some other families of local and global kernels. We demonstrate, via kmeans clustering of a set of amino acid sequences from the VIDA database, that the global kernel can be useful in bringing together otherwise very different protein families.	cluster analysis;computation;evolution;homology (biology);homology modeling;k-means clustering;kernel (operating system);peptide sequence;protein family;similarity measure;smith–waterman algorithm;statistical model;support vector machine;whole earth 'lectronic link	Valentina Sulimova;Vadim Mottl;Boris G. Mirkin;Ilya B. Muchnik;Casimir A. Kulikowski	2009		10.1007/978-3-642-01551-9_28	biology;combinatorics;string kernel;kernel embedding of distributions;computer science;bioinformatics;machine learning;mathematics;k-means clustering	ML	6.82633234516183	-49.91983386306232	128023
1c3b27afaaf5f81ba4ee2a79a6608df040c1ab40	a fuzzy intelligent approach to the classification problem in gene expression data analysis	discriminant analysis da;classification;fuzzy logic;gene expression;artificial neural networks anns;pattern recognition;support vector machines svms;k nearest neighbor knn	Classification is an important data mining task that widely used in several different real world applications. In microarray analysis, classification techniques are applied in order to discriminate diseases or to predict outcomes based on gene expression patterns, and perhaps even to identify the best treatment for given genetic signature. The most important challenge in gene expression data analysis lies in how to deal with its unique ''high dimension small sample'' characteristic, which makes many traditional classification techniques non-applicable or inefficient; and hence, more dedicated techniques are nowadays needed in order to approach this problem. Fuzzy logic is recently shown that is a powerful and suitable soft computing tool for handling the complex problems under incomplete data conditions. In this paper, a new hybrid model is proposed that combines artificial intelligence with fuzzy in order to benefit from unique advantages of both fuzzy logic and the classification power of the artificial neural networks (ANNs), to construct an efficient and accurate hybrid classifier in less available data situations. The proposed model, because of using the fuzzy parameters instead of the crisp parameters, will need less data set in comparing with traditional nonfuzzy neural networks in its training process or with same training sample can better learn and hence can yield more accurate results than traditional neural networks. In addition of theoretical evidence of using fuzzy logic, empirical results of gene expression classification indicate that the proposed model exhibits effectively improved classification accuracy in comparison with traditional artificial neural networks (ANNs) and also some other well-known statistical and intelligent classification models such as the linear discriminant analysis (LDA), the quadratic discriminant analysis (QDA), the K-nearest neighbor (KNN), and the support vector machines (SVMs). Therefore, the proposed model can be applied as an appropriate alternate approach for solving problems with scant data such as gene expression data classification, specifically when higher classification accuracy is needed.		Mehdi Khashei;Ali Zeinal Hamadani;Mehdi Bijari	2012	Knowl.-Based Syst.	10.1016/j.knosys.2011.10.012	fuzzy logic;gene expression;biological classification;fuzzy classification;computer science;artificial intelligence;neuro-fuzzy;machine learning;pattern recognition;data mining	ML	9.8064105999662	-38.85677828486428	128045
a0014baaf933b6ea9fad076b156ab17b7e58c77f	hierarchical generative biclustering for microrna expression analysis	expression analysis;information retrieval;non hodgkin lymphoma;melanoma;gene expression;biclustering;trees;machine learning;stochastic processes;clustering method;mirna;graphical model;nested chinese restaurant process;microrna	Clustering methods are a useful and common first step in gene expression studies, but the results may be hard to interpret. We bring in explicitly an indicator of which genes tie each cluster, changing the setup to biclustering. Furthermore, we make the indicators hierarchical, resulting in a hierarchy of progressively more specific biclusters. A non-parametric Bayesian formulation makes the model rigorous and yet flexible, and computations feasible. The formulation additionally offers a natural information retrieval relevance measure that allows relating samples in a principled manner. We show that the model outperforms other four biclustering procedures in a large miRNA data set. We also demonstrate the model’s added interpretability and information retrieval capability in a case study that highlights the potential and novel role of miR-224 in the association between melanoma and non-Hodgkin lymphoma. Software is publicly available.	biclustering;computation;cross-validation (statistics);gene ontology term enrichment;gene regulatory network;global serializability;graphical model;information retrieval;microarray;relevance;tree structure	José Caldas;Samuel Kaski	2010		10.1007/978-3-642-12683-3_5	biology;computer science;bioinformatics;machine learning;data mining;genetics;biclustering;microrna	ML	6.5173959681703515	-50.28079039058177	128101
02697c2a7be1a2080f0b1b8171d5f1e1cce425aa	a comparison of three heuristic methods for solving the parsing problem for tandem repeats		In many applications of tandem repeats the outcome de- pends critically on the choice of boundaries (beginning and end) of the repeated motif: for example, different choices of pattern boundaries can lead to different duplication history trees. However, the best choice of boundaries or parsing of the tandem repeat is often ambiguous, as the flanking regions before and after the tandem repeat often contain partial approximate copies of the motif, making it difficult to determine where the tandem repeat (and hence the motif) begins and ends. We define the parsing problem for tandem repeats to be the problem of discriminating among the possible choices of parsing. In this paper we propose and compare three heuristic methods for solv- ing the parsing problem, under the assumption that the parsing is fixed throughout the duplication history of the tandem repeat. The three meth- ods are PAIR, which minimises the number of pairs of common adjacent mutations which span a boundary; VAR, which minimises the total num- ber of variants of the motif; and MST, which minimises the length of the minimum spanning tree connecting the variants, where the weight of each edge is the Hamming distance of the pair of variants. We test the meth- ods on simulated data over a range of motif lengths and relative rates of substitutions to duplications, and show that all three perform better than choosing the parsing arbitrarily. Of the three MST typically performs the best, followed by VAR then PAIR.	heuristic;parsing	Atheer A. Matroud;Christopher P. Tuffley;D. Bryant;Michael D. Hendy	2012		10.1007/978-3-642-31927-3_4	theoretical computer science;machine learning;algorithm	NLP	0.9213447610061819	-51.186989739665414	128334
39e149c6fdcfc2e6e54a2005578587db5912cd1c	an efficient local-recoding k-anonymization algorithm based on clusterin	k anonymity;c modes algorithm;information loss;kaca algorithm	KACA is a typical local-recoding k-anonymization algorithm. It can generate k-anonymizing data with high quality. The main drawback of KACA algorithm is its high computational cost in dealing with large dataset. To remedy this problem, we propose an new efficient k-anonymization algorithm. The main idea of the proposed algorithm is that we first adopt the c-modes algorithm to partition the whole dataset into some large clusters, and then take KACA algorithm to k-anonymize each cluster separately. Finally, comprehensive experiments demonstrate the effectiveness of our algorithm.		Lifeng Yu;Qiong Yang	2015	Trans. Edutainment	10.1007/978-3-662-48247-6_24	computer science;theoretical computer science;machine learning;data mining;fsa-red algorithm;dinic's algorithm;freivalds' algorithm	EDA	-3.9552778774014983	-39.04508759812537	128508
2acb2f37d7dba6e1d5e4d66d8a1cc163a5fee156	evolutionary and iterative crisp and rough clustering i: theory	cluster algorithm;crisp clustering;ga rough k medoid;k means;rough clustering;genetic algorithm;ga rough k means;theoretical foundation	Researchers have proposed several Genetic Algorithms (GA) based clustering algorithms for crisp and rough clustering. In this two part series of papers, we compare the effect of GA optimization on resulting cluster quality of K-means, GA K-means, rough K-means, GA rough K-means and GA rough K-medoid algorithms. In this first part, we present the theoretical foundation of the transformation of the crisp clustering K-means and K-medoid algorithms into rough and evolutionary clustering algorithms. The second part of the paper will present experiments with a real world data set, and a standard data set.	iterative and incremental development	Manish Joshi;Pawan Lingras	2009		10.1007/978-3-642-11164-8_100	correlation clustering;genetic algorithm;computer science;machine learning;data mining;mathematics;cluster analysis;algorithm;k-means clustering	Theory	3.459645611394909	-41.63336963205955	128673
91f9cd25392eef585d973ddfbb3ce8c4e987c97e	a prudent based approach for customer churn prediction		This study contributes to formalize a three phase customer churn prediction technique. In the first phase, a supervised feature selection procedure is adopted to select the most relevant subset of features by laying-off the redundancy and increasing the relevance that leads to reduced and highly correlated features set. In the second phase, a knowledge based system (KBS) is built through Ripple Down Rule (RDR) learner which acquires knowledge about seen customer churn behavior and handles the problem of brittle in churn KBS through prudence analysis that will issue a prompt to the decision maker whenever a case is beyond the maintained knowledge in the knowledge database. In the final phase, a technique for Simulated Expert (SE) is proposed to evaluate the Knowledge Acquisition (KA) in KB system. Moreover, by applying the proposed approach on publicly available dataset, the results show that the proposed approach can be a worthy alternate for churn prediction in telecommunication industry.		Adnan Amin;Faisal Rahim;Muhammad Ramzan;Sajid Anwar	2015		10.1007/978-3-319-18422-7_29	knowledge acquisition;redundancy (engineering);data mining;knowledge base;knowledge-based systems;ripple-down rules;feature selection;ripple;computer science	AI	8.097934705988862	-42.89148343880322	128695
0d0c570a693d0ccbb5b8cdc9a3f63c8be06a8544	data mining using mlc a machine learning library in c++	benchmark;data mining;classification;machine learning	Data mining algorithms including machine learning, statistical analysis, and pattern recognition techniques can greatly improve our understanding of data warehouses that are now becoming more widespread. In this paper, we focus on classification algorithms and review the need for multiple classification algorithms. We describe a system called MLC++, which was designed to help choose the appropriate classification algorithm for a given dataset by making it easy to compare the utility of different algorithms on a specific dataset of interest. MLC++ not only provides a workbench for such comparisons, but also provides a library of C++ classes to aid in the development of new algorithms, especially hybrid algorithms and multi-strategy algorithms. Such algorithms are generally hard to code from scratch. We discuss design issues, interfaces to other programs, and visualization of the resulting classifiers.	algorithm;c++ classes;data mining;machine learning;multi-level cell;pattern recognition;workbench	Ron Kohavi;Dan Sommerfield;James Dougherty	1997	International Journal on Artificial Intelligence Tools	10.1142/S021821309700027X	benchmark;biological classification;computer science;artificial intelligence;data science;machine learning;data mining	ML	8.952840368971033	-45.608371148179074	128803
1f5a3d094a2811daeb8b646a7919def7710cc41e	a hierarchical projection pursuit clustering algorithm	decision trees;pattern clustering;statistical analysis;1d projections;decision tree;hierarchical projection pursuit clustering algorithm;high dimensional space;lower dimensional subspaces;optimal threshold technique;projection pursuit index function;projection search procedure	We define a cluster to be characterized by regions of high density separated by regions that are sparse. By observing the downward closure property of density, the search for interesting structure in a high dimensional space can be reduced to a search for structure in lower dimensional subspaces. We present a hierarchical projection pursuit clustering (HPPC) algorithm that repeatedly bi-partitions the dataset based on the discovered properties of interesting 1-dimensional projections. We describe a projection search procedure and a projection pursuit index function based on Cho, Haralick and Yi's improvement of the Kittler and Illingworth optimal threshold technique. The output of the algorithm is a decision tree whose nodes store a projection and threshold and whose leaves represent the clusters (classes). Experiments with various real and synthetic datasets show the effectiveness of the approach.	algorithm;cluster analysis;decision tree;experiment;friedrich kittler;robert haralick;sparse matrix;synthetic intelligence	Alexei D. Miasnikov;Jayson E. Rome;Robert M. Haralick	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334104	projection pursuit;mathematical optimization;computer science;machine learning;decision tree;pattern recognition;mathematics;matching pursuit	ML	1.050733173632469	-40.82095176462282	128880
d44fa9520de9e83e31f0bd5ae3e2e124d0fb91cc	estimating intrinsic dimensionality using the multi-criteria decision weighted model and the average standard estimator	performance measure;business process optimization;average standard estimator;information retrieval;singular value;singular value decomposition;random noise;complex data;efficient implementation;document retrieval;market analytics;dimensional reduction;business process;dimensionality estimation	Information retrieval today is much more challenging than traditional small document retrieval. The main difference is the importance of correlations between related concepts in complex data structures. As collections of data grow and contain more entries, they require more complex relationships, links, and groupings between individual entries. This paper introduces two novel methods for estimating data intrinsic dimensionality based on the singular value decomposition (SVD). The average standard estimator (ASE) and the multi-criteria decision weighted model are used to estimate matrix intrinsic dimensionality for large document collections. The multi-criteria weighted model calculates the sum of weighted values of matrix dimensions which demonstrated best performance using all possible dimensions [1]. ASE estimates the level of significance for singular values that resulted from the singular value decomposition. ASE assumes that those variables with deep relations have sufficient correlation and that only those relationships with high singular values are significant and should be maintained [1]. Experimental results indicate that ASE improves precision and relative relevance for MEDLINE document collection by 10.2% and 12.9% respectively compared to the percentage of variance dimensionality estimation. Results based on testing three document collections over all possible dimensions using selected performance measures indicate that ASE improved matrix intrinsic dimensionality estimation by including the effect of both singular values magnitude of decrease and random noise distracters. The multi-criteria weighted model with dimensionality reduction provides a more efficient implementation for information retrieval than using a full rank model. 2010 Elsevier Inc. All rights reserved.	adaptive server enterprise;archive;data structure;dimensionality reduction;document retrieval;information retrieval;medline;noise (electronics);relevance;singular value decomposition;technical standard	Tareq Z. Ahram;Pamela McCauley;Waldemar Karwowski	2010	Inf. Sci.	10.1016/j.ins.2010.04.006	document retrieval;mathematical optimization;computer science;business process management;machine learning;data mining;mathematics;business process;singular value decomposition;singular value;statistics;complex data type	Web+IR	-4.0913475602608	-44.37693653126707	128968
48a1f0d4557b7e442037ebb0ce801c04ef7553ed	a probabilistic approach to diachronic phonology	probabilistic approach	We present a probabilistic model of diachronic phonology in which individual word forms undergo stochastic edits along the branches of a phylogenetic tree. Our approach allows us to achieve three goals with a single unified model: (1) reconstruction of both ancient and modern word forms, (2) discovery of general phonological changes, and (3) selection among different phylogenies. We learn our model using a Monte Carlo EM algorithm and present quantitative results validating the model.	expectation–maximization algorithm;monte carlo method;phylogenetic tree;phylogenetics;statistical model;unified model	Alexandre Bouchard-Côté;Percy Liang;Thomas L. Griffiths;Dan Klein	2007			probabilistic relevance model;computer science;linguistics	NLP	2.0695686157882363	-50.297777158719796	128999
0cbc5719797b15821bc7d9be7d13b582312feaf1	ovarian cancer classification based on dimensionality reduction for seldi-tof data	female;statistical moment;ovarian neoplasms;high resolution;high dimensionality;statistical machine learning;mass spectrometry;mean variance;tumor markers biological;computational biology bioinformatics;spectrometry mass matrix assisted laser desorption ionization;algorithms;ovarian cancer;humans;cross validation;combinatorial libraries;high throughput;proteomics;correlation coefficient;computer appl in life sciences;dimensional reduction;leave one out cross validation;gene expression profiling;microarrays;bioinformatics	Recent advances in proteomics technologies such as SELDI-TOF mass spectrometry has shown promise in the detection of early stage cancers. However, dimensionality reduction and classification are considerable challenges in statistical machine learning. We therefore propose a novel approach for dimensionality reduction and tested it using published high-resolution SELDI-TOF data for ovarian cancer. We propose a method based on statistical moments to reduce feature dimensions. After refining and t-testing, SELDI-TOF data are divided into several intervals. Four statistical moments (mean, variance, skewness and kurtosis) are calculated for each interval and are used as representative variables. The high dimensionality of the data can thus be rapidly reduced. To improve efficiency and classification performance, the data are further used in kernel PLS models. The method achieved average sensitivity of 0.9950, specificity of 0.9916, accuracy of 0.9935 and a correlation coefficient of 0.9869 for 100 five-fold cross validations. Furthermore, only one control was misclassified in leave-one-out cross validation. The proposed method is suitable for analyzing high-throughput proteomics data.	coefficient;cross reactions;cross-validation (statistics);dimensionality reduction;high-throughput computing;image resolution;limited stage (cancer stage);machine learning;malignant neoplasms;papillon-lefevre disease;proteomics;sample variance;scientific publication;sensitivity and specificity;spectrometry;throughput;time-of-flight camera;ovarian neoplasm;surface enhanced laser desorption ionization	Kai-Lin Tang;Tong-Hua Li;Wen-Wei Xiong;Kai Chen	2009		10.1186/1471-2105-11-109	biology;mass spectrometry;computer science;bioinformatics;data science;proteomics;cross-validation;dimensionality reduction	ML	8.372944717952288	-49.761677546029226	129107
c436ac4dd0686b3fec70596bb0a4f9f68f137db2	generalized fuzzy c-means clustering algorithm with improved fuzzy partitions	unsupervised learning;fuzzy c means algorithm;unsupervised learning fuzzy set theory pattern clustering;cluster algorithm;pattern clustering;convergence;membership constraint function generalized fuzzy c means clustering algorithm improved fuzzy partitions;clustering algorithm;generic algorithm;distance measure;information technology;fuzzy set theory;competitive learning;image texture;objective function;fuzzy clustering;computer science education;generalized fuzzy c means clustering algorithm;indexation;pattern recognition;clustering algorithms;membership constraint function clustering algorithm competitive learning fuzzy partitions;robustness;membership constraint function;clustering algorithms partitioning algorithms laboratories computer science education information technology robustness pattern recognition convergence algorithm design and analysis image texture;fuzzy partitions;fuzzy c means clustering;improved fuzzy partitions;algorithm design and analysis;partitioning algorithms	The fuzziness index m has important influence on the clustering result of fuzzy clustering algorithms, and it should not be forced to fix at the usual value m = 2. In view of its distinctive features in applications and its limitation in having m = 2 only, a recent advance of fuzzy clustering called fuzzy c-means clustering with improved fuzzy partitions (IFP-FCM) is extended in this paper, and a generalized algorithm called GIFP-FCM for more effective clustering is proposed. By introducing a novel membership constraint function, a new objective function is constructed, and furthermore, GIFP-FCM clustering is derived. Meanwhile, from the viewpoints of L p norm distance measure and competitive learning, the robustness and convergence of the proposed algorithm are analyzed. Furthermore, the classical fuzzy c-means algorithm (FCM) and IFP-FCM can be taken as two special cases of the proposed algorithm. Several experimental results including its application to noisy image texture segmentation are presented to demonstrate its average advantage over FCM and IFP-FCM in both clustering and robustness capabilities.	algorithm;cluster analysis;competitive learning;constraint (mathematics);convergence (action);fuzzy clustering;fuzzy cognitive map;image texture;intermediate filament proteins;loss function;optimization problem;published comment;biologic segmentation;statistical cluster	Lin Zhu;Korris Fu-Lai Chung;Shitong Wang	2009	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2008.2004818	unsupervised learning;correlation clustering;constrained clustering;data stream clustering;defuzzification;k-medians clustering;fuzzy clustering;type-2 fuzzy sets and systems;flame clustering;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;fuzzy set;cluster analysis;information technology;dbscan;fuzzy set operations;clustering high-dimensional data	ML	3.1556500673017007	-39.79276921795128	129208
2302c05a1dff6b786949f12b8c872ce7482c84ae	statistical strategies for pruning all the uninteresting association rules	association rule	We propose a general framework to describe formally the #R##N#problem of capturing the intensity of implication for #R##N#association rules through statistical metrics.#R##N#In this framework we present properties that influence the#R##N#interestingness of a rule, analyze the conditions that #R##N#lead a measure to perform a perfect prune at a time, #R##N#and define a final proper order to sort the surviving#R##N#rules. We will discuss why none of the currently employed #R##N#measures can capture objective interestingness, and#R##N#just the combination of some of them, in a multi-step fashion, #R##N#can be reliable. In contrast, we propose a new simple modification #R##N#of the Pearson coefficient that will meet all the necessary #R##N#requirements. We statistically infer the convenient cut-off #R##N#threshold for this new metric by empirically describing its #R##N#distribution function through simulation. Final experiments #R##N#serve to show the ability of our proposal.		Gemma C. Garriga	2004			association rule learning;computer science;artificial intelligence;machine learning;data mining	DB	6.8456829552508704	-42.69843372350171	129275
b156455f5de1536f66f17fb29074d0d9e66d5e5a	an improved svm classifier for medical image classification	fault tolerant;rough set theory;digital mammography;medical image;support vector machine	Support Vector Machine (SVM) has high classifying accuracy and good capabilities of fault-tolerance and generalization. The Rough Set Theory (RST) approach has the advantages on dealing with a large amount of data and eliminating redundant information. In this paper, we join SVM classifier with RST which we call the Improved Support Vector Machine (ISVM) to classify digital mammography. The experimental results show that this ISVM classifier can get 96.56% accuracy which is higher about 3.42% than 92.94% using SVM, and the error recognition rates are close to 100% averagely.	computer vision;fault tolerance;intel matrix raid;preprocessor;redundancy (engineering);rough set;set theory;support vector machine;vagueness	Ying Jiang;Zhanhuai Li;Longbo Zhang;Peter P Sun	2007		10.1007/978-3-540-73451-2_80	margin classifier;computer science;machine learning;pattern recognition;data mining;structured support vector machine	ML	8.77439533634335	-39.99574933562587	129330
9b6ac74c5e113cd7dd388c5fe3efbd76ef6a48c2	condist: a context-driven categorical distance measure		A distance measure between objects is a key requirement for many data mining tasks like clustering, classification or outlier detection. However, for objects characterized by categorical attributes, defining meaningful distance measures is a challenging task since the values within such attributes have no inherent order, especially without additional domain knowledge. In this paper, we propose an unsupervised distance measure for objects with categorical attributes based on the idea, that categorical attribute values are similar if they appear with similar value distributions on correlated context attributes. Thus, the distance measure is automatically derived from the given data set. We compare our distance measure to existing categorical distance measures and evaluate on different data sets from the UCI machine-learning repository. The experiments show that our distance measure is recommendable, since it achieves similar or better results in a more robust way than previous approaches.	anomaly detection;cluster analysis;data mining;experiment;machine learning;statistical classification	Markus Ring;Florian Otto;Martin Becker;Thomas Niebler;Dieter Landes;Andreas Hotho	2015		10.1007/978-3-319-23528-8_16	anomaly detection;domain knowledge;distance measures;unsupervised learning;categorical variable;artificial intelligence;machine learning;data set;cluster analysis;pattern recognition;mathematics	ML	0.08976140081141071	-41.80539614150196	129770
ad1136937be16be36a6958616de1ff8cfbccba25	incorporating prior knowledge in a fuzzy least squares support vector machines model	fls svm;least squares approximations;support vector machines fault diagnosis fuzzy set theory least squares approximations;support vector machines;fuzzy least squares support vector machine;noise support vector machines training petroleum refining mathematical model equations;training;training process;prior knowledge;fuzzy set theory;automatic generation;noise distribution;petroleum;fuzzy membership;mathematical model;least squares support vector machines;noise least squares support vector machines fuzzy membership prior knowledge;fuzzy membership fuzzy least squares support vector machine noise distribution training process fls svm fault diagnosis;refining;fault diagnosis;noise;least squares support vector machine	The least squares support vector machines (LS-SVM) is sensitive to noises or outliers. To address the drawback, a least squares support vector machines model incorporated with a prior knowledge on data is presented. Information of noise distribution for samples is introduced in the training process. A strategy based on the sample affinity is presented to discriminate data and noises. A fuzzy membership is automatically generated and assigned to each corresponding data point in the sample set by using the strategy and the noise model. The performance of FLS-SVM is improved to resist against noises. The flexibility increase to treat data points with noises or outliers. The proposed method is applied to fault diagnosis for the lubricating oil refining process. The experiment result shows better robust of the proposed method.	affinity analysis;algorithm;artificial neural network;cluster analysis;computation;data mining;data point;free library of springfield township;function model;fuzzy control system;heuristic (computer science);john d. wiley;least squares support vector machine;local interconnect network;natural computing;neural networks;pattern recognition letters;processor affinity;signal-to-noise ratio;statistical learning theory;vector processor	Liang Xu;XiaoBo Zhang	2010	2010 Sixth International Conference on Natural Computation	10.1109/ICNC.2010.5583847	least squares support vector machine;machine learning;pattern recognition;data mining;mathematics	Robotics	9.968852823844596	-40.07455714054972	129911
0e7e4d1d3f5f5af6806c3fd4d206b380f044de41	an improved bayesian algorithm for filtering spam e-mail	bayesian algorithm;spam;unsolicited electronic mail;electronic mail;unsolicited e mail bayes methods;bayes methods;training;bayesian methods unsolicited electronic mail postal services filtering theory training;false negative;bayesian methods;bayesian method;postal services;bayesian algorithm emai spam spam filtering minimum risk;spam filtering;error rate;emai;unsolicited e mail;false negative error rate improved bayesian algorithm spam e mail filtering unsolicited bulk email e mail users loss factor;filtering theory;minimum risk	With the wide application of E-mail, unsolicited bulk email has become a major problem for E-mail users. In order to reduce the influence of spam false negative result, an improvement solution based on the traditional Bayesian algorithm is proposed, in which the loss factor is introduced to evaluate the risk of spam false negative rate. At last, the experimental result indicates that the improved Bayesian algorithm can reduce the false negative error rate when filtering spam E-mail, and get more desirable recall ratios and precision ratios.	algorithm	Yin Hu;Chaoyang Zhang	2011		10.1109/IPTC.2011.29	bayesian probability	Theory	6.300566843343292	-38.81203045550364	130459
27385aa9de01c278e9cc0f8464c3db290b1a5100	extracting information from support vector machines for pattern-based classification	data mining;anti discrimination;frequent pattern discovery	Statistical machine learning algorithms building on patterns found by pattern mining algorithms have to cope with large solution sets and thus the high dimensionality of the feature space. Vice versa, pattern mining algorithms are frequently applied to irrelevant instances, thus causing noise in the output. Solution sets of pattern mining algorithms also typically grow with increasing input datasets. The paper proposes an approach to overcome these limitations. The approach extracts information from trained support vector machines, in particular their support vectors and their relevance according to their coefficients. It uses the support vectors along with their coefficients as input to pattern mining algorithms able to handle weighted instances. Our experiments in the domain of graph mining and molecular graphs show that the resulting models are not significantly less accurate than models trained on the full datasets, yet require only a fraction of the time using much smaller sets of patterns.	algorithm;coefficient;data mining;experiment;feature vector;machine learning;molecular graph;relevance;structure mining;support vector machine	Madeleine Seeland;Andreas Maunz;Andreas Karwath;Stefan Kramer	2014		10.1145/2554850.2555065	computer science;machine learning;pattern recognition;data mining	ML	8.5944480132882	-47.90319251455738	130612
bb0f6e5b067d5f615ee1074037057357b5f25039	3d models		Crisp clustering is a technique to cluster objects into group without having overlapping partitions. Each data point is either belongs to or not to a group. Most of the clustering algorithms are categorized as crisp clustering. There are several categories of crisp clustering algorithm such as partitional algorithm, hierarchical algorithm, density-based algorithm, and grid-based algorithm. The general definition of each group could be defined as follows (Kovács et al. 2005):	algorithm;categorization;cluster analysis;data point;word lists by frequency	Suhaibah Azri;A Rahman;Uznir Ujang;François Anton;Darka Mioc	2017		10.1007/978-3-319-17885-1_100007		ML	1.409203701309677	-42.27029762187133	130948
2614d5499108e3489b815f7456baaa922f962cb3	variable width rough-fuzzy c-means		The richness of soft clustering algorithms in the scientific literature reflects from one side the complexity of the underlying problem and from the other the many attempts that have been made to preserve interpretability while modeling vagueness through different theories. In this paper a hybrid rough-fuzzy unsupervised learning algorithm called Variable Width Rough-Fuzzy c-Means (VWRFCM) is derived from a unifying view of the most popular crisp, fuzzy, rough and fuzzy-rough partitive clustering algorithms. VWRFCM provides a user-defined parameter that sets the width of the core regions of all clusters in a probabilistic sense, allowing the domain experts to have both an intuitive interpretation and a powerful control possibility on the maximum allowed degree of vagueness in the clustering solution. Tests on several real datasets show a good effectiveness together with a speed-up in efficiency of VWRFCM compared to its baseline competitors.	algorithm;baseline (configuration management);cluster analysis;scientific literature;theory;unsupervised learning;vagueness	Alessio Ferone;Ardelio Galletti;Antonio Maratea	2017	2017 13th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)	10.1109/SITIS.2017.81	artificial intelligence;pattern recognition;fuzzy clustering;fuzzy logic;computer science;fuzzy set;cluster analysis;probabilistic logic;approximation algorithm;unsupervised learning;interpretability	DB	-0.20164669235965546	-39.36014625244252	130995
e86a48238d1fd013b580b3ebc87e33632048be58	a comparison between neural and fuzzy cluster analysis techniques for functional mri	hypothesis generation;unsupervised clustering;performance evaluation;fmri;functional mri;minimal free energy vector quantization;fuzzy n means algorithm;gath geva algorithm;independent component analysis;neural gas;fuzzy clustering;neural gas network;functional magnetic resonance images;roc analysis;self organizing map;self organized map;vector quantizer;cluster validity;free energy;quantitative evaluation	Exploratory data-driven methods such as unsupervised clustering and independent component analysis (ICA) are considered to be hypothesisgenerating procedures, and are complementary to the hypothesis-led statistical inferential methods in functional magnetic resonance imaging (fMRI). In this paper, we present a comparison between neural and fuzzy clustering techniques in a systematic fMRI study. For the fMRI data, a comparative quantitative evaluation based on ROC analysis between the Gath–Geva algorithm, the fuzzy n-means algorithm, Kohonen’s selforganizing map, fuzzy n-means algorithm with unsupervised initialization, minimal free energy vector quantizer and the ‘‘neural-gas’’ network was performed. The most important findings in this paper are: (1) SOM is outperformed by all other neural and fuzzy techniques regardless of the chosen number of codebook vectors in terms of detecting small activation areas, (2) the variations among the other techniques are minimal, and (3) a small number of codebook vectors is in general required to obtain consistent task-related activation maps, as determined by the performance evaluation based on cluster validity indices. # 2006 Elsevier Ltd. All rights reserved.	algorithm;cluster analysis;codebook;dijkstra's algorithm;experiment;exploratory testing;fuzzy clustering;independent computing architecture;independent component analysis;inferential theory of learning;map;neural gas;performance evaluation;quantization (signal processing);receiver operating characteristic;resonance;sensor;teuvo kohonen;vector quantization	Oliver Lange;Anke Meyer-Bäse;Monica K. Hurdal;Simon Y. Foo	2006	Biomed. Signal Proc. and Control	10.1016/j.bspc.2006.11.002	neural gas;independent component analysis;self-organizing map;fuzzy clustering;computer science;machine learning;pattern recognition;data mining;mathematics;receiver operating characteristic	ML	6.194571956386789	-45.267808269880994	131205
a62ae12e31a6d3b37a6bc57e23c980efdffa262b	similarity search in moving object trajectories	similarity search	The continuous and rapid advent in mobile and communications technology opens the way for new research areas and new applications. Moving Object Databases(MODs) are among the emerging research topics that are attracting many work due to their vital need in many applications. Generally, MODs deal with geometries changing over time. In this paper we study an interesting point in moving object databases; namely, similarity search between moving object trajectories. We propose a novel similarity search technique that is based on trajectory alignment using gaps. Our approach binds the idea of biological sequence alignment using the Needleman-Wunsch algorithm [3] together with different similarity search distances. Finally, we present an empirical study for the efficiency and accuracy of our proposed approaches.	database;needleman–wunsch algorithm;sequence alignment;similarity search	Omnia Ossama Hoda;M. O. Mokhtar	2009			machine learning;data mining;empirical research;information and communications technology;sequence alignment;artificial intelligence;trajectory;computer science;nearest neighbor search	DB	-1.754181276186885	-47.05119947661587	131421
b74f3493b62146353132148cc29ec7e438960041	performance evaluation of a community structure finding algorithm using modularity and c-rand measures	erbium;microarray data;hierarchical clustering;pattern clustering;performance evaluation;csf algorithm;bayes methods;naive bayes;k means;tree data structures bayes methods internet pattern clustering performance evaluation social networking online;tree data structures;microarray data sets;hierarchical clustering algorithm;c rand community structure modularity clustering;social network;internet;clustering;community structure finding algorithm;community structure;social networks;social networking online;biological networks community structure finding algorithm modularity measures c rand measures csf algorithm microarray data sets three data sets clustering hierarchical clustering algorithm k means algorithm dynamic tree cut algorithm naive bayes clustering social networks world wide web;world wide web;naive bayes clustering;k means algorithm;modularity;biological networks;dynamic tree cut algorithm;modularity measures;biological network;three data sets clustering;c rand measures;c rand	Biological networks, social networks, and the World Wide Web are some examples of real world networks exhibiting community structure. We present a concise review of community structure finding (CSF) algorithms and applications. We apply a CSF algorithm and various other algorithms on three different microarray data sets. We calculate modularity and C-rand indices as an indication of the quality of each clustering of the three data sets. We compare the performance of the CSF algorithm with the performance of three other algorithms: hierarchical clustering (HC) algorithm, K-means, dynamic tree cut (DTC) algorithm and Naive Bayes Clustering (NBC) using both C-rand and modularity values. We report that the CSF algorithm detects clusters resulting in high modularity; however the CSF does not result in clusters with high C-rand values compared to the other methods.	algorithm;cluster analysis;grid north;hierarchical clustering;k-means clustering;link/cut tree;microarray;naive bayes classifier;performance evaluation;pseudocode;pseudorandom number generator;rand index;sensor;social network;world wide web	Harun Pirim;Dilip Gautam;Tanmay Bhowmik;Andy D. Perkins;Burak Eksioglu	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596962	biological network;computer science;canopy clustering algorithm;machine learning;pattern recognition;data mining;clique percolation method;hierarchical clustering of networks;social network;k-means clustering	ML	-0.27346444290522515	-40.63890993445774	131455
b92ee1c0bd3083cb6de609d827f47210af33aaea	cooperative dynde for temporal data clustering	dynamic differential evolution;pattern clustering;evolutionary computation;data clustering dynamic de;standards;solutions cooperative dynde temporal data clustering dynamic differential evolution data clustering dynamic de dcdynde algorithm dataset dimensions migration types statistical analysis;dcdynde algorithm;solutions;cooperative dynde;statistical analysis evolutionary computation pattern clustering;indexes;statistical analysis;vectors;heuristic algorithms;statistics;clustering algorithms;temporal data clustering;migration types;dataset dimensions;clustering algorithms sociology statistics heuristic algorithms indexes vectors standards;sociology	Temporal data is common in real-world datasets. Clustering of such data allows for relationships between data patterns over time to be discovered. Differential evolution (DE) algorithms have previously been used to cluster temporal data. This paper proposes the cooperative data clustering dynamic DE algorithm (CDCDynDE), which is an adaptation to the data clustering dynamic DE (DCDynDE) algorithm where each population searches for a single cluster centroid. The paper applies the proposed algorithm to a variety of temporal datasets with different frequencies of change, severities of change, dataset dimensions and data migration types. The clustering results of the cooperative data clustering DynDE are compared against the original data clustering DynDE, the re-initialising data clustering DE and the standard data clustering DE. A statistical analysis of these results shows that the cooperative data clustering DynDE algorithm obtains better data clustering solutions to the other three algorithms despite changes in frequency, severity, dimension and data migration types.	algorithm;cluster analysis;differential evolution;iteration;kepler engelbrecht;particle swarm optimization;turi	Kristina Georgieva;Andries Petrus Engelbrecht	2014	2014 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2014.6900344	database index;computer science;data science;machine learning;data mining;cluster analysis;statistics;evolutionary computation	ML	0.37519618816291744	-39.47942245817852	131583
0b4a412caa74c93bd62a108f7b70c4e698cb7bd1	why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: comment on friston (2012)	medical and health sciences;medicin och halsovetenskap	"""It is sometimes argued that small studies provide better evidence for reported effects because they are less likely to report findings with small and trivial effect sizes (Friston, 2012). But larger studies are actually better at protecting against inferences from trivial effect sizes, if researchers just make use of effect sizes and confidence intervals. Poor statistical power also comes at a cost of inflated proportion of false positive findings, less power to """"confirm"""" true effects and bias in reported (inflated) effect sizes. Small studies (n=16) lack the precision to reliably distinguish small and medium to large effect sizes (r<.50) from random noise (α=.05) that larger studies (n=100) does with high level of confidence (r=.50, p=.00000012). The present paper presents the arguments needed for researchers to refute the claim that small low-powered studies have a higher degree of scientific evidence than large high-powered studies."""	confidence intervals;doppler effect;high-level programming language;large;noise (electronics);power (psychology);statistical power	Michael Ingre	2013	NeuroImage	10.1016/j.neuroimage.2013.03.030	psychology;econometrics;mathematics;social psychology;statistics	HCI	5.390362134560142	-51.679294573216104	131601
1f0805fdcffa0242318b6d1d516684214f42bd7b	semantic similarity based feature extraction from microarray expression data	semantic similarity;sample classifiers;microarray expression data;dato;data;semantics;bioinformatique;semantica;semantique;similitude;virtual genes;microreseau;gene expression profiles;donnee;feature extraction;microarreglo;similarity;microarray;extraction de caracteristiques;bioinformatica;similitud;colon cancer data;classification accuracy;article;bioinformatics;gene ontology	Previous studies have proven that it is feasible to build sample classifiers using gene expression profiles. To build an effective sample classifier, dimension reduction process is necessary since classic pattern recognition algorithms do not work well in high dimensional space. In this paper, we present a novel feature extraction algorithm by integrating microarray expression data with Gene Ontology (GO). Applying semantic similarity measures, we identify the groups of genes, called virtual genes, which potentially interact with each other for a biological function. The correlation in expressions of virtual genes is used to classify samples. For colon cancer data, this approach significantly improved the classification accuracy by more than 10%.	algorithm;colon carcinoma;colon classification;dimensionality reduction;embedded system;embedding;feature extraction;function (biology);gene expression profiling;gene ontology;greater than;ibm notes;mental association;microarray;neoplasms;nephrogenic systemic fibrosis;pattern recognition;sample rate conversion;semantic similarity;similarity measure;united states national institutes of health	Young-Rae Cho;Aidong Zhang;Xian Xu	2009	International journal of data mining and bioinformatics	10.1504/IJDMB.2009.026705	semantic similarity;similarity;feature extraction;computer science;bioinformatics;similitude;pattern recognition;microarray;data mining;semantics;data	Comp.	7.25375298647368	-49.6141852931228	131607
ee55fbd2e77a351ee709b0908661e93d168e67cc	a new gene selection method for microarray data based on pso and informativeness metric	informativeness metric;extreme learning machine;particle swarm optimization;gene selection	In this paper, a new method encoding a priori information of informativeness metric of microarray data into particle swarm optimization (PSO) is proposed to select informative genes. The informativeness metric is an analysis of variance statistic that represents the regulation hide in the microarray data. In the new method, the informativeness metric is combined with the global searching algorithms PSO to perform gene selection. The genes selected by the new method reveal the data structure highly hided in the microarray data and therefore improve the classification accuracy rate. Experiment results on two microarray datasets achieved by the proposed method verify its effectiveness and efficiency.	microarray;particle swarm optimization	Jian Guan;Fei Han;Shanxiu Yang	2013		10.1007/978-3-642-39482-9_17	gene-centered view of evolution;mathematical optimization;computer science;bioinformatics;pattern recognition;data mining;mathematics;particle swarm optimization	Comp.	7.729547384425465	-47.390872838245194	131837
ba83b280ad33d31bf03b424a440cefc186025895	a toolbox for fuzzy clustering using the r programming language	fuzzy approach;fclust;clustering;r	Fuzzy clustering is used extensively in several domains of research. In the literature, starting from the well-known fuzzy  k -means (f k m) clustering algorithm, an increasing number of papers devoted to f k m and its extensions can be found. Nevertheless, a lack of the related software for implementing these algorithms can be observed preventing their use in practice. Even the standard f k m is not necessarily available in the most common software. For this purpose, a new toolbox for fuzzy clustering using the  R  programming language is presented by examples. The toolbox, called  fclust , contains a suit of fuzzy clustering algorithms, fuzzy cluster validity indices and visualization tools for fuzzy clustering results.	algorithm;cluster analysis;fuzzy clustering;polynomial;programming language;r language	Maria Brigida Ferraro;Paolo Giordani	2015	Fuzzy Sets and Systems	10.1016/j.fss.2015.05.001	correlation clustering;defuzzification;fuzzy clustering;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;rhodopsin;artificial intelligence;fuzzy number;theoretical computer science;canopy clustering algorithm;neuro-fuzzy;machine learning;cure data clustering algorithm;data mining;fuzzy set;cluster analysis;fuzzy set operations;clustering high-dimensional data	AI	1.2355347376029466	-38.74461376061827	131987
3f4a0d2cb6ea7992607695e472ef4b335c0094fb	biomarker discovery and visualization in gene expression data with efficient generalized matrix approximations	gene expression data;dynamical system;biomarker discovery;data visualization;matrix approximation;gene selection	In most real-world gene expression data sets, there are often multiple sample classes with ordinals, which are categorized into the normal or diseased type. The traditional feature or attribute selection methods consider multiple classes equally without paying attention to the up/down regulation across the normal and diseased types of classes, while the specific gene selection methods particularly consider the differential expressions across the normal and diseased, but ignore the existence of multiple classes. In this paper, to improve the biomarker discovery, we propose to make the best use of these two aspects: the differential expressions (that can be viewed as the domain knowledge of gene expression data) and the multiple classes (that can be viewed as a kind of data set characteristic). Therefore, we simultaneously take into account these two aspects by employing the 1-rank generalized matrix approximations (GMA). Our results show that GMA cannot only improve the accuracy of classifying the samples, but also provide a visualization method to effectively analyze the gene expression data on both genes and samples. Based on the mechanism of matrix approximation, we further propose an algorithm, CBiomarker, to discover compact biomarker by reducing the redundancy.		Wenyuan Li;Yanxiong Peng;Hung-Chung Huang;Ying Liu	2007	Journal of bioinformatics and computational biology	10.1142/S0219720007002746	gene-centered view of evolution;biology;computer science;bioinformatics;dynamical system;machine learning;data mining;mathematics;biomarker discovery;data visualization	Comp.	7.439581922315302	-48.488180066881434	132637
7f6f3b39e2c1cfbe3375666fcf217edd95b84aed	a generalization of distance functions for fuzzy $c$ -means clustering with centroids of arithmetic means	point to centroid distance p2c d convex function fuzzy c means fcm l_p norm;cluster algorithm;distance function;pattern clustering;fuzzy c mean;convergence;electronic mail;point to centroid distance p2c d;membership grade matrix distance function generalization fuzzy c means clustering centroids arithmetic means fuzzy clustering clustering community differentiable convex function generalized point to centroid distance;membership grade matrix;distance function generalization;global convergence;euclidean distance;convergence clustering algorithms educational institutions convex functions optimization electronic mail euclidean distance;fuzzy set theory;l_p norm;clustering community;convex functions;differentiable convex function;fuzzy clustering;generic point;convex function;arithmetic mean;pattern clustering fuzzy set theory generalisation artificial intelligence;generalized point to centroid distance;clustering algorithms;centroids;optimization;generalisation artificial intelligence;fuzzy c means fcm;fuzzy c means clustering;arithmetic means	Fuzzy c-means (FCM) is a widely used fuzzy clustering method, which allows an object to belong to two or more clusters with a membership grade between zero and one. Despite the considerable efforts made by the clustering community, the common characteristics of distance functions suitable for FCM remain unclear. To fill this crucial void, in this paper, we first provide a generalized definition of distance functions that fit FCM directly. The goal is to provide more flexibility to FCM in the choice of distance functions while preserving the simplicity of FCM by using the centroids of arithmetic means. Indeed, we show that any distance function that fits FCM directly can be derived by a continuously differentiable convex function and, thus, is an instance of the generalized point-to-centroid distance (P2C-D) by definition. In addition, we prove that if the membership grade matrix is nondegenerate, any instance of the P2C-D fits FCM directly. Finally, extensive experiments have been conducted to demonstrate that the P2C-D leads to the global convergence of FCM and that the clustering performances are significantly affected by the choices of distance functions.	cluster analysis;convex function;experiment;fits;fuzzy clustering;fuzzy cognitive map;local convergence;performance	Junjie Wu;Hui Xiong;Chen Liu;Jian Chen	2012	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2011.2179659	convex function;mathematical optimization;discrete mathematics;arithmetic mean;machine learning;mathematics	ML	3.1691645194595197	-39.54881757872877	132894
42bb966b918242a5822b09572c1b242fd50adb9b	density-based clustering with side information and active learning		Data clustering is one of the most important tasks in machine learning and data mining, which aims to discover structure and the relational between observations inside data sets. In many situations, side information about the clusters is available in addition to the values of the features. For example, the cluster labels of some observations may be known (called seeds), or certain observations may be known to belong (or not) to the same cluster (pairwise constraints). Many semi-supervised clustering algorithms are presented in literature to improve the clustering accuracy by effectively exploring these available side information. However, each algorithm usually uses one kind of side information. In this paper, we propose a new semi-supervised density based clustering which integrates both kinds of side information, and embeds an active learning strategy in the process of finding clusters, named MCSSDBS. Experiments conducted on real data sets from UCI show the effectiveness of our algorithm compared with the semi-supervised density-based clustering (SSDBSCAN).	active learning (machine learning);algorithm;cluster analysis;computation;data mining;experiment;iteration;machine learning;seeds (cellular automaton);semi-supervised learning;semiconductor industry	Viet-Vu Vu;Hong-Quan Do	2017	2017 9th International Conference on Knowledge and Systems Engineering (KSE)	10.1109/KSE.2017.8119453	computer science;machine learning;pattern recognition;artificial intelligence;active learning;cluster analysis;pairwise comparison;data set	ML	2.0613953275249726	-42.096076346467434	132942
f66f39d232928244bbac8aef1067efe171fe0682	towards improving the efficiency of the fuzzy cognitive map classifier	data mining;fuzzy cognitive maps	Fuzzy Cognitive Map (FCM) is a model that combines selected features of fuzzy sets and neural networks. FCM is usually applied as a decision support tool or as a predictive model for time series forecasting. It is less well known as a classifier. To perform the classification, numeric data produced by the FCM must be assigned to class labels. To accomplish this task, we propose a new algorithm for generating thresholds for the discrimination of FCM outcomes. The thresholds resulting from the proposed algorithm are determined after the learning of the FCM, and, then they are applied when classifying new data. The results of the experiments conducted with publicly available data provide evidence that the application of the proposed algorithm leads to improved efficiency of the FCM classifier. Comparative experiments showed that the proposed approach makes the FCM a very competitive alternative to other state-of-the-art classifiers.	fuzzy cognitive map	Wojciech Froelich	2017	Neurocomputing	10.1016/j.neucom.2016.11.059	fuzzy cognitive map;computer science;machine learning;pattern recognition;data mining	HCI	9.470545899107323	-41.55400499339374	132989
27bf3db7fb50fd58e21b7d99025615ac33bd4e19	two-phase support vector clustering for multi-relational data mining	pattern clustering;multirelational data mining;support vector machines;information extraction;designed kernel two phase support vector clustering multirelational data mining cluster contours support vector machine classification information extraction cluster central zones adjacent matrix computation;data mining kernel support vector machines support vector machine classification clustering algorithms static var compensators logic programming computer science probabilistic logic educational institutions;adjacent matrix computation;data mining;support vector;cluster central zones;support vector clustering;designed kernel;support vector machine classification;support vector machines data mining pattern clustering;support vector machine;cluster contours;multi relational data mining;adjacency matrix;two phase support vector clustering	A novel two-phase support vector clustering (TPSVC) algorithm is proposed in this paper, which is implemented in multi-relational data mining (MRDM). Based on the designed kernel which is incorporated with MRDM environment, TPSVC provides an appreciate description of cluster contours using support vectors at the first step and then a support vector machine (SVM) classification procedure is employed to further extract the information of cluster central zones. The algorithm does the cluster assignment according to desired definition of affinity without suffering the expensive operations of adjacent matrix computation used in traditional support vector clustering (SVC). Experimental results indicate that the designed kernel can capture the features of relational schema and TPSVC is of fine clustering performance	algorithm;cluster analysis;computation;database schema;feature vector;iteration;kernel (operating system);kernel method;knowledge engineering;linux;nonlinear system;numerical linear algebra;processor affinity;relational data mining;support vector machine;two-phase locking;virtual world;xml	Ping Ling;Yan Wang;Nan Lu;Jianyu Wang;Chunguang Zhou;Shuang Liang	2005	2005 International Conference on Cyberworlds (CW'05)	10.1109/CW.2005.92	correlation clustering;support vector machine;k-medians clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;data mining;cluster analysis;information extraction;clustering high-dimensional data	ML	-2.636939136128728	-38.70121497631492	133079
9b602154ca91594fa4a56fe40d360c4c0f547655	a parallel approach for decision trees learning from big data streams		In this paper we introduce PdsCART, a parallel decision tree learning algorithm. There are three characteristics that are important to emphasize and make this algorithm particularly interesting. Firstly, the algorithm we present here can work with streaming data, i.e. one pass over data is sufficient to construct the tree. Secondly, the algorithm is able to process in parallel a larger amount of data stream records and can therefor handle efficiently very large data sets. And thirdly, the algorithm can be implemented in the MapReduce framework. Details about the algorithm and some basic performance results are presented.	big data;decision tree	Ionel Tudor Calistru;Paul Cotofrei;Kilian Stoffel	2015		10.1007/978-3-319-19027-3_1	weighted majority algorithm;computer science;theoretical computer science;machine learning;incremental decision tree;data mining;database;fsa-red algorithm;parallel algorithm;id3 algorithm	ML	-3.6454045448201695	-38.12695876352068	133143
65d13af6859c0d949700cdd15ac7c7f37845da49	fuzzy clustering of image pixels with a fitness-based adaptive differential evolution		Fuzzy c-means is currently the most widely used pixel clustering technique in the field of automatic image segmentation. However performance of fuzzy c-means largely relies upon the selection of initial cluster centers. For improper choice of initial cluster centers, FCM sometimes gets stuck at local optima. In this work a modified Fitness-Based Adaptive Differential Evolution algorithm is presented for clustering the pixels of an image. The control parameters, which are crucial for convergence of Differential Evolution, are chosen adaptively based on fitness statistics of population. For performance measurements, the Berkley Segmentation Dataset and Benchmarks BSD 300 is used. The outcomes of the proposed algorithm are compared with the famous fuzzy c-means algorithm both qualitatively and quantitatively.	differential evolution;fuzzy clustering;pixel	Soham Sarkar;Gyana Ranjan Patra;Swagatam Das;Sheli Sinha Chaudhuri	2013		10.1007/978-3-319-03753-0_17	fuzzy clustering;cluster analysis	ML	3.9764810304005604	-41.8280414662499	133163
833aa831c722f59d90bf229849dafd1549aeb691	towards automatic generation of metafeatures		The selection of metafeatures for metalearning (MtL) is often an ad hoc process. The lack of a proper motivation for the choice of a metafeature rather than others is questionable and may originate a loss of valuable information for a given problem (e.g., use of class entropy and not attribute entropy). We present a framework to systematically generate metafeatures in the context of MtL. This framework decomposes a metafeature into three components: meta-function, object and postprocessing. The automatic generation of metafeatures is triggered by the selection of a meta-function used to systematically generate metafeatures from all possible combinations of object and post-processing alternatives. We executed experiments by addressing the problem of algorithm selection in classification datasets. Results show that the sets of systematic metafeatures generated from our framework are more informative than the non-systematic ones and the set regarded as state-of-the-art.	algorithm selection;entropy (information theory);experiment;function object;hoc (programming language);information;metaprogramming;pmr446;video post-processing	Fábio Pinto;Carlos Soares;João Mendes-Moreira	2016		10.1007/978-3-319-31753-3_18	artificial intelligence;computer science;machine learning;metalearning;algorithm selection	NLP	6.440845057544807	-43.473776101511035	133347
03c0741ddec770aa320716e94e2f6eaa1fa65a96	spectral clustering strategies for heterogeneous disease data		Clustering of gene expression data simplifies subsequent data analyses and forms the basis of numerous approaches for biomarker identification, prediction of clinical outcome, and personalized therapeutic strategies. The most popular clustering methods such as K-means and hierarchical clustering are intuitive and easy to use, but they require arbitrary choices on their various parameters (number of clusters for K-means, and a threshold to cut the tree for hierarchical clustering). Human disease gene expression data are in general more difficult to cluster efficiently due to background (genotype) heterogeneity, disease stage and progression differences and disease subtyping; all of which cause gene expression datasets to be more heterogeneous. Spectral clustering has been recently introduced in many fields as a promising alternative to standard clustering methods. The idea is that pairwise comparisons can help reveal global features through the eigen techniques. In this paper, we developed a new recursive K-means spectral clustering method (ReKS) for disease gene expression data. We benchmarked ReKS on three large-scale cancer datasets and we compared it to different clustering methods with respect to execution time, background models and external biological knowledge. We found ReKS to be superior to the hierarchical methods and equally good to K-means, but much faster than them and without the requirement for a priori knowledge of K. Overall, ReKS offers an attractive alternative for efficient clustering of human disease data.	biological markers;choice behavior;cluster analysis;color gradient;eigen (c++ library);gene expression;genetic heterogeneity;hierarchical clustering;k-means clustering;personalization;recursion;run time (program lifecycle phase);spectral clustering;statistical cluster	Grace T. Huang;Kathryn I. Cunningham;Panayiotis V. Benos;Chakra Chennubhotla	2013	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		spectral clustering;bioinformatics;biology	Comp.	6.536465430704516	-49.95319421031672	133566
ef1a51b73774986c058eca7e3d87f501b53b2b98	unfold high-dimensional clouds for exhaustive gating of flow cytometry data	manuals;mice;multidimensional scaling unfold high dimensional clouds exhaustive gating flow cytometry data multiple protein expressions single cell level biological sample protein marker measurements data analysis cell subpopulations distinct phenotypes analytical approach nested biaxial plots sequence gating analysis clustering algorithms manual gating single cell data high dimensional point cloud 2d visualizations principal component analysis;visualization;proteins biology computing cellular biophysics cloud computing data analysis flow measurement molecular biophysics molecular configurations pattern clustering principal component analysis;bones;proteins;logic gates;principal component analysis;data visualization;exhaustive gating;flow cytometry;data visualization logic gates computational biology proteins genomics biomedical signal processing principal component analysis cytometry cells biology	Flow cytometry is able to measure the expressions of multiple proteins simultaneously at the single-cell level. A flow cytometry experiment on one biological sample provides measurements of several protein markers on or inside a large number of individual cells in that sample. Analysis of such data often aims to identify subpopulations of cells with distinct phenotypes. Currently, the most widely used analytical approach in the flow cytometry community is manual gating on a sequence of nested biaxial plots, which is highly subjective, labor intensive, and not exhaustive. To address those issues, a number of methods have been developed to automate the gating analysis by clustering algorithms. However, completely removing the subjectivity can be quite challenging. This paper describes an alternative approach. Instead of automating the analysis, we develop novel visualizations to facilitate manual gating. The proposed method views single-cell data of one biological sample as a high-dimensional point cloud of cells, derives the skeleton of the cloud, and unfolds the skeleton to generate 2D visualizations. We demonstrate the utility of the proposed visualization using real data, and provide quantitative comparison to visualizations generated from principal component analysis and multidimensional scaling.	cluster analysis;flow cytometry;image scaling;multidimensional scaling;phenotype;point cloud;principal component analysis;test scaling;algorithm;statistical cluster	Peng Qiu	2014	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2014.2321403	biology;visualization;logic gate;computer science;bioinformatics;theoretical computer science;data mining;flow cytometry;data visualization;statistics;principal component analysis	Visualization	4.6114370799200275	-50.46341775439757	133655
0a03d8744c1a48702b388538d56a2e4cfbf0cb9e	using weighted clustering and symbolic data to evaluate institutes’s scientific production	interval data;symbolic data analysis;clustering method;researcher scientific production	It is increasingly common to use tools of Symbolic Data Analysis to reduce the data set without losing much information. Moreover, symbolic variables can be used to preserving the privacy of individuals when their information are present in the data set. In this work, we use information about researchers of institutions from Brazil through the tools of Symbolic Data Analysis and a weighted clustering method for interval data. The main goal is to analyze the scientific production of Brazilian institutions. Results of the cluster analysis and concluding remarks are given.		Bruno A. Pimentel;Jarley Palmeira Nóbrega;Renata M. C. R. de Souza	2012		10.1007/978-3-642-33266-1_54	computer science;data science;data mining;database;symbolic data analysis	HPC	0.7078892269202203	-38.49847707647063	133688
b5f754e6249d287c8977ea1dc0a45a24760263d4	robustness in evolved grid structures	biology computing;evolved grid structures;genomics;evolutionary computation;noise robustness;evolved polyomino genomes evolved grid structures dynamic polyominos cellular encoding;genetics;evolution biology;shape;cellular encoding;biological systems;robustness;genetic algorithms;dynamic polyominos;evolved polyomino genomes;evolutionary computation biology computing;genetic mutations;noise robustness evolutionary computation evolution biology genetic mutations protein engineering genomics bioinformatics biological systems genetic algorithms shape;protein engineering;competitive advantage;bioinformatics	This study explores the ability of dynamic polyominos to acquire different types of robustness in a variety of environments. A polyomino is a collection of identical squares joined along their sides to form a connected shape. This study introduces a cellular encoding for polyominos that grow in a manner that adapts to environmental obstructions. Fitness evaluation places polyominos in competition to occupy space with each square of a grid occupiable by only a single individual. Evolved polyomino genomes are studied for their robustness to choice of opponent and environment. This study is part of a series studying the evolution of robustness, enlarging the scope of the series to include robustness against choice of opponent and environment. Polyomino fitness is evaluated in monoculture, multiculture, and obstructed environments. It is found that in all cases added time evolving grants a greater degree of robustness than the other possible sources of robustness. When polyomino genomes have been evolved for comparable amounts of time it is found those with competitive fitness evaluation are superior. When the impact of environmental obstructions are considered it is found that being in your home environment grants a competitive advantage, though not as strong of an advantage as added evolution, with a single exception.	environmental resource management;monoculture (computer science);robustness (computer science)	Daniel A. Ashlock;Justin Schonfeld;James Humphrey	2009	2009 IEEE Congress on Evolutionary Computation	10.1109/CEC.2009.4983100	genomics;genetic algorithm;shape;computer science;bioinformatics;machine learning;protein engineering;competitive advantage;robustness;evolutionary computation	Visualization	-3.9652393150547933	-46.4836147751755	133699
114c92c24a423cf76490b751da792d42b10b4097	an evaluation of data stream clustering algorithms			algorithm;cluster analysis;data stream clustering	Stratos Mansalis;Eirini Ntoutsi;Nikos Pelekis;Yannis Theodoridis	2018	Statistical Analysis and Data Mining	10.1002/sam.11380	machine learning;artificial intelligence;data mining;data stream clustering;data stream mining;computer science	ML	-0.3184168993380816	-38.57933189234644	133861
ca07b339cec0385c4c6924eb7e21e3c08d6c566f	a protein secondary structure prediction framework based on the support vector machine	prediction method;information structure;context information;validacion cruzada;structure information;analisis datos;structure secondaire;protein sequence;estructura informacion;fenetre coulissante;classification;residu;structure proteine;protein secondary structure;protein structure;data analysis;estructura secundaria;protein secondary structure prediction;secondary structure;machine exemple support;validation croisee;analyse donnee;ventana deslizante;cross validation;support vector machine;vector support machine;residuo;residue;clasificacion;sliding window	Our framework for predicting protein secondary structures differs from existing prediction methods since we consider physiochemical information and context information of secondary structure segments. We have employed Support Vector Machine (SVM) for training the CB513 and RS126 data sets, which are collections of protein secondary structure sequences, through sevenfold cross validation to uncover the structural differences of protein secondary structures. We apply the sliding window technique to test a set of protein sequences based on the group classification learned from the training data set. Our prediction approach achieves 77.8% segment overlap accuracy (SOV) and 75. 2% three-state overall per-residue accuracy (Q 3 ) on CB513 set, which outperform existing protein secondary structure prediction methods.	support vector machine	Xiaochun Yang;Bin Wang;Yiu-Kai Ng;Ge Yu;Guoren Wang	2003		10.1007/978-3-540-45160-0_26	computer science;bioinformatics;machine learning;data mining;protein secondary structure	ML	6.31458989553507	-46.65663914682537	133978
c31f7e301ea20f0771ae5cff2e14c9d40f62340e	research of marker gene selection for tumor classfication based on decision forests	gene selection	Feature selection techniques have been widely applied to bioinformatics, where decision forests (DF) is an important one. To prove the advantage of DF, Significance Analysis of Microarray (SAM), PCA and ReliefF were employed to compare with it. Support Vectors Machine (SVM) was used to test the feature genes selected by the four methods. The comparison results show that feature genes selected by DF contain more classification information and can get higher accuracy rate when were applied to classification. As a reliable method, DF should be applied in bioinformatics broadly.		Jian-Geng Li;Xin Li	2010		10.1007/978-3-642-14831-6_28	gene-centered view of evolution;computer science;bioinformatics	Logic	8.252120423326703	-49.10765567580759	134189
9e2e3f41c0e9a8c90503c7a4b4feaef382ba4f1e	biomarker identification and cancer classification based on microarray data using laplace naive bayes model with mean shrinkage	electrical reasons;microarray data;regularization parameter;cancer data sets;gene ontology terms;support vector machines;cancer;gene selection method;biomarker identification;normal distribution;highly correlated genes;biological system modeling;closely related problems;piecewise linear function;gene expression data analysis biomarker identification cancer classification laplace distribution l_1 penalty;genetics;biological analysis;medical computing;penalty;gene expression;laplace equations;l_1 penalty;biological pathway;mean shrinkage;computational modeling;gene expression data sets;lab on a chip;physiological models biochemistry bioinformatics cancer genetics lab on a chip laplace equations medical computing normal distribution;robustness;highly correlated genes biomarker identification cancer classification microarray data mean shrinkage closely related problems gene expression data sets biological pathway electrical reasons chemical reasons gene selection method laplace naive bayes model laplace distribution normal distribution automatic feature selection piecewise linear function regularization parameter cancer data sets biochemical research biomedical research biological analysis functional correlation gene ontology terms;cancer classification;chemical reasons;gene expression data analysis;biochemical research;physiological models;cancer gene expression support vector machines biological system modeling computational modeling;biochemistry;functional correlation;automatic feature selection;biomedical research;laplace distribution;laplace naive bayes model;bioinformatics	Biomarker identification and cancer classification are two closely related problems. In gene expression data sets, the correlation between genes can be high when they share the same biological pathway. Moreover, the gene expression data sets may contain outliers due to either chemical or electrical reasons. A good gene selection method should take group effects into account and be robust to outliers. In this paper, we propose a Laplace naive Bayes model with mean shrinkage (LNB-MS). The Laplace distribution instead of the normal distribution is used as the conditional distribution of the samples for the reasons that it is less sensitive to outliers and has been applied in many fields. The key technique is the L_1 penalty imposed on the mean of each class to achieve automatic feature selection. The objective function of the proposed model is a piecewise linear function with respect to the mean of each class, of which the optimal value can be evaluated at the breakpoints simply. An efficient algorithm is designed to estimate the parameters in the model. A new strategy that uses the number of selected features to control the regularization parameter is introduced. Experimental results on simulated data sets and 17 publicly available cancer data sets attest to the accuracy, sparsity, efficiency, and robustness of the proposed algorithm. Many biomarkers identified with our method have been verified in biochemical or biomedical research. The analysis of biological and functional correlation of the genes based on Gene Ontology (GO) terms shows that the proposed method guarantees the selection of highly correlated genes simultaneously.	algorithm;biological markers;breakpoint;feature selection;gene expression profiling;gene ontology;gene regulatory network;linear function;loss function;low-noise block downconverter;matrix regularization;microarray;naive bayes classifier;optimization problem;piecewise linear continuation;population parameter;sparse matrix	Meng-Yun Wu;Dao-Qing Dai;Yu Shi;Hong Yan;Xiao-Fei Zhang	2012	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2012.105	normal distribution;biology;support vector machine;microarray analysis techniques;gene expression;piecewise linear function;lab-on-a-chip;computer science;bioinformatics;machine learning;biological pathway;data mining;laplace distribution;mathematics;computational model;genetics;statistics;robustness;cancer	Comp.	8.449446082733736	-51.468608699570325	134242
ecf3917470c08bebefa112728f6c2ddaaa94aacc	schism: a new approach to interesting subspace mining	high dimensionality;chernoff hoeffding bounds;interestingness measure;data mining;feature space;interestingness measures;clustering;high dimensional data;maximal subspaces;depth first search;subspace mining	High dimensional data pose challenges to traditional clustering algorithms due to their inherent sparseness and data tend to cluster in different and possibly overlapping subspaces of the entire feature space. Finding such subspaces is called subspace mining. We present SCHISM, a new algorithm for mining interesting subspaces, using the notions of support and Chernoff-Hoeffding bounds. We use a vertical representation of the dataset, and use a depth first search with backtracking to find maximal interesting subspaces. We test our algorithm on a number of high dimensional synthetic and real datasets to test its effectiveness.		Karlton Sequeira;Mohammed J. Zaki	2005	IJBIDM	10.1504/IJBIDM.2005.008360	feature vector;breadth-first search;computer science;machine learning;pattern recognition;data mining;cluster analysis;clustering high-dimensional data	ML	-0.6452064329595818	-41.41690888625964	134477
896d4da1877967ccef78807249237d27914a7d6b	oracle clustering: dynamic partitioning based on random observations	cluster algorithm;pattern clustering;approximation algorithms;perforation;random sampling oracle clustering dynamic partitioning dynamic clustering algorithm;random sampling;prediction algorithms;clustering algorithms sampling methods stability large scale systems heuristic algorithms partitioning algorithms artificial intelligence computer science niobium algorithm design and analysis;average distance;dynamic stability;dynamic clustering algorithm;oracle clustering;stability;dynamic clustering;arrays;large scale;stability clustering dynamic clustering oracle clustering;mixture model;clustering;heuristic algorithms;random processes;dynamic partitioning;number of clusters;clustering algorithms;random processes pattern clustering;approximation methods;high performance;clustered data;algorithm design and analysis	In this paper, a new dynamic clustering algorithm based on random sampling is proposed. The algorithm addresses well known challenges in clustering such as dynamism, stability, and scaling. The core of the proposed method isbased on the definition of a function, named the Oracle,which can predict whether two random data points belongto the same cluster or not. Furthermore, this algorithm isalso equipped with a novel technique for determination ofthe optimal number of clusters in datasets. These properties add the capabilities of high performance and reducing the effect of scale in datasets to this algorithm. Finally, the algorithm is tuned and evaluated by means of various experiments and in-depth analysis. High accuracy and performance results obtained, demonstrate the competitiveness of our algorithm.	algorithm;cluster analysis;competitive analysis (online algorithm);data point;experiment;image scaling;monte carlo method;randomness;sampling (signal processing)	Reza Zafarani;Ali A. Ghorbani	2008	2008 20th IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2008.128	correlation clustering;stochastic process;data stream clustering;k-medians clustering;fuzzy clustering;computer science;theoretical computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;fsa-red algorithm;cluster analysis;k-medoids;approximation algorithm;statistics	Robotics	0.21644372097791206	-40.65525446971648	134546
596ebeefb6ed91a81f96addfa51ce51efeac86c7	comparative analysis for k-means algorithms in network community detection	community detection;comparative analysis;k means;dissimilarity index;ad hoc network;complex system;computational complexity;community structure;indexation;k means algorithm;optimal prediction;diffusion distance	Detecting the community structure exhibited by real networks is a crucial step toward an understanding of complex systems beyond the local organization of their constituents. Many algorithms proposed so far, especially the group of methods in the k-means formulation, can lead to a high degree of efficiency and accuracy. Here we test three k-means methods, based on optimal prediction, diffusion distance and dissimilarity index, respectively, on two artificial networks, including the widely known ad hoc network with same community size and a recently introduced LFR benchmark graphs with heterogeneous distributions of degree and community size. All of them display an excellent performance, with the additional advantage of low computational complexity, which enables the analysis of large systems. Moreover, successful applications to several real world networks confirm the capability of the methods.		Jian Liu	2010		10.1007/978-3-642-16493-4_17	computer science;artificial intelligence;machine learning;data mining	ML	1.1386293461842474	-42.510826654777446	134654
8299e3599e210d6ca68839a5a5e9eab61a08b67e	a genetic approach for efficient outlier detection in projected space	projected dimension;detection erreur;deteccion error;partition method;evolutionary computation;computational techniques;projective dimension;grid count tree;outlier;calcul evolutionniste;gene expression data;algoritmo genetico;genetics;outlier detection;gene expression;observacion aberrante;methode partition;estructura datos;deviation detection;algorithme genetique;pattern recognition;projective space;observation aberrante;genetic algorithm;structure donnee;metodo particion;reconnaissance forme;error detection;reconocimiento patron;data structure	In this paper we present a genetic solution to the outlier detection problem. The essential idea behind this technique is to define outliers by examining those projections of the data, along which the data points have abnormal or inconsistent behavior (defined in terms of their sparsity values). We use a partitioning method to divide the data set into groups such that all the objects in a group can be considered to behave similarly. We then identify those groups that contain outliers. The algorithm assigns an ‘outlier-ness’ value that gives a relative measure of how strong an outlier group is. An evolutionary search computation technique is employed for determining those projections of the data over which the outliers can be identified. A new data structure, called the grid count tree (GCT), is used for efficient computation of the sparsity factor. GCT helps in quickly determining the number of points within any grid defined over the projected space and hence facilitates faster computation of the sparsity factor. A new crossover is also defined for this purpose. The proposed method is applicable for both numeric and categorical attributes. The search complexity of the GCT traversal algorithm is provided. Results are demonstrated for both artificial and real life data sets including four gene expression data sets. 2007 Elsevier Ltd. All rights reserved.	algorithm;anomaly detection;computation;data point;data structure;geometric complexity theory;real life;sparse matrix;tree traversal	Sanghamitra Bandyopadhyay;Santanu Santra	2008	Pattern Recognition	10.1016/j.patcog.2007.10.003	econometrics;projective space;outlier;gene expression;error detection and correction;genetic algorithm;data structure;computer science;machine learning;mathematics;algorithm;statistics;evolutionary computation	AI	1.1513067321520956	-41.323081488946436	134736
4c52a7179dfc5ab29677bcc2e2123272c5eeb23e	inducing better rule sets by adding missing attribute values	incomplete data;error rate	Our main objective was to verify the following hypothesis: for some complete (i.e., without missing attribute vales) data sets it is possible to induce better rule sets (in terms of an error rate) by increasing incompleteness (i.e., removing some existing attribute values) of the original data sets. In this paper we present detailed results of experiments on one data set, showing that some rule sets induced from incomplete data sets are significantly better than the rule set induced from the original data set, with the significance level of 5%, two-tailed test. Additionally, we discuss criteria for inducing better rules by increasing incompleteness and present graphs for some well-known data sets.		Jerzy W. Grzymala-Busse;Witold J. Grzymala-Busse	2008		10.1007/978-3-540-88425-5_17	pattern recognition;data mining;mathematics;statistics	DB	7.005758078418412	-42.55494467155572	134785
466d9c7c3f5e4dc9253fce74f5700eb9184ba002	3n-q: natural nearest neighbor with quality		In this paper, a novel algorithm for enhancing the performance of classification is proposed. This new method provides rich information for clustering and outlier detection. We call it Natural Nearest Neighbor with Quality (3N-Q). Comparing to K-nearest neighbor and E-nearest neighbor, 3N-Q employs a completely different concept to find the nearest neighbors passively, which can adaptively and automatically get the K value. This value as well as distribution of neighbors and frequency of being neighbors of others offer precious foundation not only in classification but also in clustering and outlier detection. Subsequently, we propose a fitness function that reflects the quality of each training sample, retaining the good ones while eliminating the bad ones according to the quality threshold. From the experiment results we report in this paper, it is observed that 3N-Q is efficient and accurate for solving data mining problems.	anomaly detection;cluster analysis;constraint algorithm;data mining;fitness function;genetic algorithm;k-nearest neighbors algorithm;similarity measure;statistical classification	Shu Zhang;Malek Mouhoub;Samira Sadaoui	2014	Computer and Information Science	10.5539/cis.v7n1p94	nearest-neighbor chain algorithm;large margin nearest neighbor;r-tree;best bin first;computer science;machine learning;pattern recognition;data mining;nearest neighbor search;fixed-radius near neighbors;k-nearest neighbors algorithm	ML	7.538921384683715	-41.654439311693515	134810
1b29bc69dc8a55ea0498adbabd376505da49b245	interval-valued data clustering based on range metrics (p)			cluster analysis	Sérgio Galdino;W. R. N. Santos;Ricardo Paranhos	2018		10.18293/SEKE2018-113	data mining;computer science;cluster analysis	Theory	2.0926752278923773	-40.664061977696036	135462
ebf0e1736ad047932937e7c53c33d5eb37da399a	a new classification method to overcome over-branching	frequent pattern;classification algorithm;decision tree;data mining;classification;over branching	Classification is an important technique in data mining. The decision trees built by most of the existing classification algorithms commonly feature over-branching, which will lead to poor efficiency in the subsequent classification period. In this paper, we present a new value-oriented classification method, which aims at building accurately proper-sized decision trees while reducing over-branching as much as possible, based on the concepts of frequent-pattern-node and exceptive-child-node. The experiments show that while using relevant analysis as pre-processing, our classification method, without loss of accuracy, can eliminate the over-branching greatly in decision trees more effectively and efficiently than other algorithms do.	algorithm;data mining;decision tree;experiment;preprocessor;tree (data structure)	Aoying Zhou;Weining Qian;Hailei Qian;Wen Jin	2002	Journal of Computer Science and Technology	10.1007/BF02949821	decision tree learning;biological classification;computer science;machine learning;decision tree;linear classifier;multiclass classification;pattern recognition;data mining;one-class classification	ML	9.144573468007962	-39.09441747726966	135511
7e62d6a33c5ca4fe43123e03e407025b2588d5b4	a study of the influence of rule measures in classifiers induced by evolutionary algorithms		The Pittsburgh representation is a well-known encoding for symbolic classifiers in evolutionary algorithms, where each individual represents one symbolic classifier, and eac h symbolic classifier is composed by a rule set. These rule sets can be interpreted asordered or unordered sets. The major difference between these two approaches is whether rule ordering define s a rule precedence relationship or not. Although ordered rulesets are simple to implement in a computer system, the rule set is difficult to be interpreted by human domain experts, since rules are not independent from each other. In contrast, unorderedrule sets are more flexible regarding their interpretation. Rules are independent from each other and can be individually present ed to a human domain expert. However, the algorithm to decide a classification of a given example is more complex. As rules have no precedence, an example should be presented to all rul es at once and some criteria should be established to decide the final classification based on all fired rules. A simple approac h to decide which rule should provide the final classification i s to select the rule that has the best rating according to a chosen quality measure. Dozens of measures were proposed in literature; however, it is not clear whether any of them would provide a better classification performance. This work perf orms a comparative study of rule performance measures for unorde red symbolic classifiers induced by evolutionary algorithms. We compare 9 rule quality measures in 10 data sets. Our experime nts point out that confidence (also known as precision) presente d the best mean results, although most of the rule quality measure s presented approximated classification performance assess ed with the area under the ROC curve (AUC).	approximation algorithm;computer;evolutionary algorithm;perf (linux);receiver operating characteristic;rule 184;subject-matter expert;symbolic computation	Claudia Regina Milaré;Gustavo E. A. P. A. Batista;André Carlos Ponce de Leon Ferreira de Carvalho	2010	IEEE Intelligent Informatics Bulletin		encoding (memory);subject-matter expert;evolutionary algorithm;pattern recognition;artificial intelligence;data set;mathematics;classifier (linguistics)	ML	6.9662622772286955	-43.90558472012468	135879
b509f3039346fe213f13bfa91434f0b2b8284bdb	partitioning based n-gram feature selection for malware classification		Byte level N-Gram is one of the most used feature extraction algorithms for malware classification because of its good performance and robustness. However, the N-Gram feature selection for a large dataset consumes huge time and space resources due to the large amount of different N-Grams. This paper proposes a partitioning based algorithm for large scale feature selection which efficiently resolves the original problem into in-memory solutions without heavy IO load. The partitioning process adopts an efficient implementation to convert the original interactional dataset to unrelated data partitions. Such data independence enables the effectiveness of the in-memory solutions and the parallelism on different partitions. The proposed algorithm was implemented on Apache Spark, and experimental results show that it is able to select features in a very short period of time which is nearly three times faster than the comparison MapReduce approach.	feature selection;malware;n-gram	Weiwei Hu;Ying Tan	2016		10.1007/978-3-319-40973-3_18	machine learning;pattern recognition	ML	-4.327770486823638	-39.024957976438074	135978
4aa78afba0e38d8aec0b2a524155d06e5dae9308	prompredictor: a hybrid machine learning system for recognition and location of transcription start sites in human genome	genetique;modelizacion;sensitivity and specificity;cpg island;learning algorithm;red www;sistema hibrido;analisis datos;genetica;dimension reduction;localization;gene start;reseau web;hombre;intelligence artificielle;localizacion;algorithme apprentissage;data mining;genetics;reduction dimension;modelisation;data analysis;localisation;internet;machine learning;fouille donnee;coding theory;human genome;genome;human;hybrid system;pattern recognition;reduccion dimension;artificial intelligence;world wide web;analyse donnee;feature selection;inteligencia artificial;reconnaissance forme;genoma;transcription start site;reconocimiento patron;algoritmo aprendizaje;modeling;dimensional reduction;busca dato;homme;systeme hybride	In this paper we present a novel hybrid machine learning system for recognition of gene starts in human genome. The system makes predictions of gene start by extracting compositional features and CpG islands information from promoter regions. It combines a new promoter recognition model, coding theory, feature selection and dimensionality reduction with machine learning algorithm. Evaluation on Human chromosome 4, 21, 22 was 64.47% in sensitivity and 82.20% in specificity. Comparison with the three other systems revealed that our system had superior sensitivity and specificity in predicting gene starts. PromPredictor is written in MATLAB and requires Matlab to run. PromPredictor is freely available at www.whtelecom.com/Prompredictor.htm.	algorithm;central pattern generator;coding theory;dimensionality reduction;feature selection;matlab;machine learning;medical transcription;sensitivity and specificity	Tao Li;Chuanbo Chen	2005		10.1007/11527503_66	human genome;the internet;systems modeling;internationalization and localization;computer science;bioinformatics;artificial intelligence;machine learning;cpg site;data analysis;transcription;feature selection;coding theory;dimensionality reduction;hybrid system;genome	AI	3.427376736149354	-46.97284205654902	136121
6adda424936d88b8a3d9e10959903bcda7ec6053	mining flipping correlations from large datasets with taxonomies	itemset mining;flipping correlation	In this paper we introduce a new type of pattern – a flipping correlation pattern. The flipping patterns are obtained from contrasting the correlations between items at different levels of abstraction. They represent surprising correlations, both positive and negative, which are specific for a given abstraction level, and which “flip” from positive to negative and vice versa when items are generalized to a higher level of abstraction. We design an efficient algorithm for finding flipping correlations, the Flipper algorithm, which outperforms näıve pattern mining methods by several orders of magnitude. We apply Flipper to real-life datasets and show that the discovered patterns are non-redundant, surprising and actionable. Flipper finds strong contrasting correlations in itemsets with low-to-medium support, while existing techniques cannot handle the pattern discovery in this frequency range.	abstraction layer;algorithm;anti-aliasing filter;apriori algorithm;data mining;data structure;experiment;frequency band;pattern language;portable document format;principle of abstraction;real life;robot combat;taxonomy (general);value (computer science)	Marina Barsky;Sangkyum Kim;Tim Weninger;Jiawei Han	2011	PVLDB	10.14778/2095686.2095695	computer science;artificial intelligence;machine learning;data mining	DB	-2.448773643769844	-41.78400634521018	136149
292d7b83651b92b45d83e65f011c1992c08099f3	the impact of obstruction on a model of competitive exclusion in plants	genomics;biological system modeling genomics bioinformatics games production adaptation models computational modeling;biological system modeling;obstruction evolutionary pressure exponential growth seed barrier seed mortality seed production agent based model grid plant competitive exclusion;multi agent systems botany modelling;computational modeling;games;production;adaptation models;bioinformatics	This study extends an earlier work on an agent based model of competitive exclusion in plants by adding obstructions to a toroidal agent world. The agents are called grid plants, whose genome specifies their pattern of growth and when they make seeds. Seed production is the figure of merit used to assess the success of grid plants. Barriers are found to substantially inhibit seed production, out of proportion to the amount of space they occupy. Two types of barriers are used, ones that occupy productive space in the simulation and ones that block growth between grids of the simulation but occupy no space. Both sorts of barriers are found to inhibit seed production well in excess of the physical space obstructed, nor is fraction of obstruction a strong determinant of the level of inhibition. There is a cooperative effect from both seed mortality and barriers: past some threshold dependent on both, the plants take much longer to achieve exponential growth, if at all. A very strong effect of nonlocal adaptation is apparent in the results, where plants evolved under increasing hardship are initially better adapted, even to other boards, but the effect reverses when evolutionary pressure becomes too high.	agent-based model;aharonov–bohm effect;experiment;nonlinear system;offset binary;random seed;robertson–seymour theorem;seeds (cellular automaton);simulation;time complexity;toroidal graph	Jeffrey Tsang;Daniel A. Ashlock	2015	2015 IEEE Symposium Series on Computational Intelligence	10.1109/SSCI.2015.159	simulation;engineering;operations management;ecology	AI	-4.0594864838646965	-46.56578477342896	136398
560f535a411d59cd12f06c6533a4e39f2a716ebe	multiview clustering on ppi network for gene selection and enrichment from microarray data	databases;expression profiles;interstitial lung disease;diseases accuracy biological processes proteins databases gene expression;engineering biomedical;cancer;statistical analysis biochemistry bioinformatics data structures feature selection genetics lab on a chip learning artificial intelligence molecular biophysics pattern clustering proteins;molecular interaction database;benchmark microarray dataset multiview clustering ppi network gene enrichment statistical algorithm machine learning based algorithm informative gene subset selection gene selection trend functional knowledge clustering algorithm multiple view generation cluster generation microarray expression profile data facet representation multiple cluster connected region representation protein protein interaction network biological process microarray data clustering integration ppi knowledge;multiview clustering;gene expression;accuracy;relevant;proteins;gene enrichment;waldenstroms macroglobulinemia;diseases;mechanisms;gene enrichment microarray multiview clustering ppi network gene selection;microarray;patterns;update;gene selection;biological processes;chronic lymphocytic leukemia;ppi network	Various statistical and machine learning based algorithms have been proposed in literature for selecting an informative subset of genes from micro array data sets. The recent trend is to use functional knowledge to aid the gene selection process. In this paper we propose a clustering algorithm which generates multiple views (clusters) from the micro array expression profiles, each representing a particular facet of the data. Such multiple clusters are found to represent strongly connected regions of the known protein -- protein interaction (PPI) networks, perhaps corresponding to those responsible for certain biological processes. Thus we integrate micro array data clustering with PPI knowledge to obtain enriched gene sets. Results on benchmark micro array data sets demonstrate the competitiveness of our method compared to gene selection techniques.	algorithm;benchmark (computing);bioinformatics;cluster analysis;faceted classification;gene ontology term enrichment;information;machine learning;microarray;pixel density;relevance;selection algorithm;strongly connected component	Tripti Swarnkar;Sérgio Nery Simões;David Correa Martins;Anji Anurak;Helena Brentani;Ronaldo Fumio Hashimoto;Pabitra Mitra	2014	2014 IEEE International Conference on Bioinformatics and Bioengineering	10.1109/BIBE.2014.33	gene-centered view of evolution;biology;gene expression;bioinformatics;data science;microarray;data mining;accuracy and precision;pattern;biological process;genetics;cancer	HPC	5.159952835778517	-48.646498825500316	136519
89c71ec63c441ead4979a215d11ec65abcc58588	pfc: an efficient soft graph clustering method for ppi networks based on purifying and filtering the coupling matrix		One of the most pressing problems of the post genomic era is identifying protein functions. Clustering Protein-Protein-Interaction networks is a systems biological approach to this problem. Traditional Graph Clustering Methods are crisp, and allow only membership of each node in at most one cluster. However, most real world networks contain overlapping clusters. Recently the need for scalable, accurate and efficient overlapping graph clustering methods has been recognized and various soft (overlapping) graph clustering methods have been proposed. In this paper, an efficient, novel, and fast overlapping clustering method is proposed based on purifying and filtering the coupling matrix (PFC). PFC is tested on PPI networks. The experimental results show that PFC method outperforms many existing methods by a few orders of magnitude in terms of average statistical (hypergeometrical) confidence regarding biological enrichment of the identified clusters.	cluster analysis;gene ontology term enrichment;pixel density;powerbuilder foundation classes;protein data bank;purification of quantum state;scalability	Ying Liu;Amir Foroushani	2016		10.1007/978-3-319-42291-6_24	computer science;theoretical computer science;machine learning;pattern recognition	ML	4.045166515481641	-49.12605544182157	136634
fd8f3e9d5a816728af8dd69b08811048f5697080	training support vector machines with privacy-protected data		In this paper, we address a machine learning task using encrypted training data. Our basic scenario has three parties: Data Owners, who own private data; an Application, which wants to train and use an arbitrary machine learning model on the Users’ data; and an Authorization Server, which provides Data Owners with public and secret keys of a partial homomorphic cryptosystem (that protects the privacy of their data), authorizes the Application to get access to the encrypted data, and assists it in those computations not supported by the partial homomorphism. As machine learning model, we have selected the Support Vector Machine (SVM) due to its excellent performance in supervised classification tasks. We evaluate two well known SVM algorithms, and we also propose a new semiparametric SVM scheme better suited for the privacy-protected scenario. At the end of the paper, a performance analysis regarding the accuracy and the complexity of the developed algorithms and protocols is presented. © 2017 Elsevier Ltd. All rights reserved.	algorithm;authorization;computation;cryptosystem;encryption;information privacy;machine learning;rsa (cryptosystem);semiparametric model;support vector machine	Francisco Javier González-Serrano;Ángel Navia-Vázquez;Adrian Amor-Martin	2017	Pattern Recognition	10.1016/j.patcog.2017.06.016	data mining;encryption;structured support vector machine;support vector machine;online machine learning;machine learning;cryptosystem;artificial intelligence;pattern recognition;active learning (machine learning);computational learning theory;relevance vector machine;computer science	ML	7.585759223352444	-39.44340358834061	136820
384e242b5c023153043a6e136bf2452c75233f78	background correction for cdna microarray images using the tv+l1 model	use;morphologie;pulga de dna;analisis datos;puce a dna;error sistematico;methode;microreseau;variations;utilisation;data analysis;modelo;morphology;bias;microarreglo;uso;dna chip;cdna microarray;analyse donnee;total variation;modele;microarray;variacion;variation;synthetic data;morfologia;metodo;method;models;erreur systematique	MOTIVATION Background correction is an important preprocess in cDNA microarray data analysis. A variety of methods have been used for this purpose. However, many kinds of backgrounds, especially inhomogeneous ones, cannot be estimated correctly using any of the existing methods. In this paper, we propose the use of the TV+L1 model, which minimizes the total variation (TV) of the image subject to an L1-fidelity term, to correct background bias. We demonstrate its advantages over the existing methods by both analytically discussing its properties and numerically comparing it with morphological opening.   RESULTS Experimental results on both synthetic data and real microarray images demonstrate that the TV+L1 model gives the restored intensity that is closer to the true data than morphological opening. As a result, this method can serve an important role in the preprocessing of cDNA microarray data.	dna microarray;numerical analysis;opening (morphology);preprocessor;synthetic data;total disc replacement	Wotao Yin;Terrence Chen;Xiang Sean Zhou;Amit Chakraborty	2005	Bioinformatics	10.1093/bioinformatics/bti341	method;dna microarray;morphology;computer science;bioinformatics;bias;microarray;data analysis;theme and variations;total variation;algorithm;synthetic data	Comp.	4.5890933259019775	-51.48077731035341	136879
484d90ceef265a9ad57029394aa89fbd7079b2d0	an effective evolutionary algorithm for discrete-valued data clustering	cluster algorithm;distance function;pattern clustering;real life databases;evolutionary computation;performance evaluation;spatial data;discrete valued data clustering;geometry;euclidean distance measure;evolutionary clustering algorithm;euclidean distance;data mining;data clustering;visual databases data mining evolutionary computation geometry pattern clustering;interest groups;euclidean space;evolutionary algorithm;evolutionary clustering algorithm discrete valued data clustering spatial data euclidean distance measure real life databases data mining;visual databases	Clustering is concerned with the discovery of interesting groupings of records in a database. Of the many algorithms have been developed to tackle clustering problems in a variety of application domains, a lot of effort has been put into the development of effective algorithms for handling spatial data. These algorithms were originally developed to handle continuous-valued attributes, and the distance functions such as the Euclidean distance measure are often used to measure the pair-wise similarity/distance between records so as to determine the cluster memberships of records. Since such distance functions cannot be validly defined in non-Euclidean space, these algorithms therefore cannot be used to handle databases that contain discrete-valued data. Owing to the fact that data in the real-life databases are always described by a set of descriptive attributes, many of which are not numerical or inherently ordered in any way, it is important that a clustering algorithm should be developed to handle data mining tasks involving them. In this paper, we propose an effective evolutionary clustering algorithm for this problem. For performance evaluation, we have tested the proposed algorithm using several real data sets. Experimental results show that it outperforms the existing algorithms commonly used for discrete-valued data clustering, and also, when dealing with mixed continuous- and discrete-valued data, its performance is also promising.	application domain;cluster analysis;data mining;database;euclidean distance;evolutionary algorithm;numerical analysis;performance evaluation;real life	Patrick C. H. Ma;Keith C. C. Chan;Xin Yao	2008	2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)	10.1109/CEC.2008.4630801	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;metric;fuzzy clustering;flame clustering;computer science;euclidean space;canopy clustering algorithm;machine learning;evolutionary algorithm;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;euclidean distance;mathematics;spatial analysis;cluster analysis;dbscan;biclustering;affinity propagation;evolutionary computation;clustering high-dimensional data	DB	1.1969144194914223	-40.545300769804136	137014
c42129257f2c18197ba80857968eac9574392ca0	automatic generation of biclusters from gene expression data using multi-objective simulated annealing approach	electronic mail;simulated annealing;gene expression;linear programming;clustering algorithms;computer science	The invention of microarray technology aids in the successful monitoring of the gene expression patterns. Biclustering is a method in which a number of co-regulated genes are identified over subset of conditions. Our aim is to detect all the non trivial biclusters having low mean squared residue(MSR) and high row variance. In this paper, we have proposed a multi-objective simulated annealing based solution framework to solve the biclustering problem from gene expression data sets. Two objective functions MSR and row-variance capturing two important properties of biclusters are optimized in parallel using the search capability of multi-objective simulated annealing based optimization technique, AMOSA. A new encoding strategy and several different search operators are defined for fast convergence of the algorithm. We have done experiment on two real-life data sets and obtained results are quantified by using several cluster validity indices. We have compared our obtained results with some state-of-the-art biclustering techniques.	algorithm;biclustering;characterization test;euclidean distance;mathematical optimization;mean squared error;microarray;real life;simulated annealing	Pracheta Sahoo;Sudipta Acharya;Sriparna Saha	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899958	gene expression;simulated annealing;computer science;bioinformatics;linear programming;machine learning;data mining;cluster analysis;adaptive simulated annealing	Vision	7.062647641195198	-47.00846896359227	137163
87ed6e6b31fe87b3598ccd74681f3fbbef383bf7	integer programming approaches to haplotype inference by pure parsimony	biology computing;genomics;sequences;haplotype inference by pure parsimony;instruments;genotype sequences integer programming haplotype inference pure parsimony polynomial sized ip;molecular configurations;biological system modeling;indexing terms;polynomials;genetics;molecular configurations biology computing genetics integer programming molecular biophysics;algorithms computational biology computer simulation gene frequency genetics population genotype haplotypes humans mathematical computing models genetic phylogeny polymorphism single nucleotide;pure parsimony;integer programming;biology and genetics;molecular biophysics;linear programming;linear programming polynomials sequences humans biological system modeling genomics bioinformatics genetic mutations instruments diseases;diseases;haplotype inference;humans;genetic mutations;computations on discrete structures;integer program;haplotype inference computations on discrete structures integer programming biology and genetics;polynomial sized ip;biomedical computing;bioinformatics;genotype sequences	In 2003, Gusfield introduced the Haplotype Inference by Pure Parsimony (HIPP) problem and presented an integer program (IP) that quickly solved many simulated instances of the problem [1]. Although it solved well on small instances, Gusfield's IP can be of exponential size in the worst case. Several authors [2], [3] have presented polynomial-sized IPs for the problem. In this paper, we further the work on IP approaches to HIPP. We extend the existing polynomial-sized IPs by introducing several classes of valid cuts for the IP. We also present a new polynomial-sized IP formulation that is a hybrid between two existing IP formulations and inherits many of the strengths of both. Many problems that are too complex for the exponential-sized formulations can still be solved in our new formulation in a reasonable amount of time. We provide a detailed empirical comparison of these IP formulations on both simulated and real genotype sequences. Our formulation can also be extended in a variety of ways to allow errors in the input or model the structure of the population under consideration.	best, worst and average case;chromosome (genetic algorithm);class;haplotypes;ips panel;incised wound;inference;integer (number);integer programming;maximum parsimony (phylogenetics);occam's razor;polynomial;exponential	Daniel G. Brown;Ian M. Harrower	2006	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2006.24	biology;genomics;integer programming;index term;computer science;bioinformatics;linear programming;theoretical computer science;sequence;mathematics;genetics;algorithm;polynomial;molecular biophysics	Comp.	1.555306002232427	-51.412840833048115	137215
c129f84fb3b1d6a1e33bcbf92253ed516776bbac	unsupervised binning of metagenomic assembled contigs using improved fuzzy c-means method	genomics dna linear programming microorganisms clustering methods clustering algorithms indexes;fcm metagenomic binning k mer clustering method	Metagenomic contigs binning is a necessary step of metagenome analysis. After assembly, the number of contigs belonging to different genomes is usually unequal. So a metagenomic contigs dataset is a kind of imbalanced dataset and traditional fuzzy c-means method FCM fails to handle it very well. In this paper, we will introduce an improved version of fuzzy c-means method IFCM into metagenomic contigs binning. First, tetranucleotide frequencies are calculated for every contig. Second, the number of bins is roughly estimated by the distribution of genome lengths of a complete set of non-draft sequenced microbial genomes from NCBI. Then, IFCM is used to cluster DNA contigs with the estimated result. Finally, a clustering validity function is utilized to determine the binning result. We tested this method on a synthetic and two real datasets and experimental results have showed the effectiveness of this method compared with other tools.	cluster analysis;ferric carboxymaltose solution;fuzzy cognitive map;genome;genome, microbial;metagenome;metagenomics;ncbi taxonomy;product binning;silo (dataset);synthetic intelligence;unsupervised learning;statistical cluster	Yun Liu;Tao Hou;Bing Kang;Fu Liu	2017	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2016.2576452	machine learning;genomics;computer science;fuzzy logic;microbial genomes;cluster analysis;genome;metagenomics;artificial intelligence;bioinformatics;pattern recognition;contig	Comp.	4.182527561231627	-49.522412118524365	137263
b1c6c9677e9c78989b7d8f3b41dc99bc2711a5de	clustering analysis of blood cells using multidimensional parametric signatures	cluster analysis		cluster analysis;electronic signature	Alison Valdivieso;Malek Adjouadi;John Riley	2001			cluster analysis;artificial intelligence;parametric statistics;mathematics;pattern recognition	NLP	7.249328784731964	-46.19291280352346	137360
7aebd1109f57f64ef13fa454724d73f38652860d	two local dissimilarity measures for weighted graphs with application to protein interaction networks	dissimilarity measure;protein network;graph partitioning;weighted graph;biological network;protein interaction network	We extend the Czekanowski-Dice dissimilarity measure, classically used to cluster the vertices of unweighted graphs, to weighted ones. The first proposed formula corresponds to edges weighted by a probability of existence. The second one is adapted to edges weighted by intensity or strength. We show on simulated graphs that the class identification process is improved by computing weighted compared to unweighted edges. Finally, an application to a drosophila protein network illustrates the fact that using these new formulas improves the ’biological accuracy’ of partitioning.		Jean-Baptiste Angelelli;Anaïs Baudot;Marie-Christine Brun;Alain Guénoche	2008	Adv. Data Analysis and Classification	10.1007/s11634-008-0018-3	biological network;combinatorics;discrete mathematics;multiple edges;computer science;graph partition;machine learning;mathematics	ML	2.1216190593393187	-48.46282618974445	137564
f432b4b834ae61ca34644d654eed6aa3763a96c8	uncovering the structure of heterogenous biological data: fuzzy graph partitioning in the k-partite setting	biological data	With the increasing availability of large-scale interaction networks derived either from experimental data or from text mining, we face the challenge of interpreting and analyzing these data sets in a comprehensive fashion. A particularity of these networks, which sets it apart from other examples in various scientific fields lies in their k-partiteness. Whereas graph partitioning has received considerable attention, only few researchers have focused on this generalized situation. Recently, Long et al. have proposed a method for jointly clustering such a network and at the same time estimating a weighted graph connecting the clusters thereby allowing simple interpretation of the resulting clustering structure. In this contribution, we extend this work by allowing fuzzy clusters for each node type. We propose an extended cost function for partitioning that allows for overlapping clusters. Our main contribution lies in the novel efficient minimization procedure, mimicking the multiplicative update rules employed in algorithms for non-negative matrix factorization. Results on clustering a manually annotated bipartite gene-complex graph show significantly higher homogeneity between gene and corresponding complex clusters than expected by chance. The algorithm is freely available at http://cmb.helmholtz-muenchen.de/ fuzzyclustering.	algorithm;algorithmic efficiency;cluster analysis;computer cluster;graph partition;interaction;loss function;non-negative matrix factorization;randomness;scalability;sensor;systems biology;text mining	Florian Blöchl;Mara L. Hartsperger;Volker Stümpflen;Fabian J. Theis	2010			fuzzy clustering;graph (abstract data type);experimental data;bioinformatics;correlation clustering;biological data;cluster analysis;machine learning;clustering coefficient;graph partition;artificial intelligence;mathematics	ML	5.093700317357067	-50.53775801072506	137661
f2c7e6fbabebaaf854738d55bf54bb911405281c	management and analysis of dna microarray data by using weighted trees	combinatorics on words;dna microarrays;classification;weighted automata;dna microarray data;dna microarray;weighted trees;candidate gene	We investigate discrete structures and combinatoric modeling of weighted prefix trees for managing and analyzing DNA microarray data. We describe the algorithms to construct the weighted trees for these data. Using these weighted trees with our algorithms, we propose methods to compute the appearance probability of a DNA microarray, to compare the informational distances in the expression of genes between the DNA microarrays, to search the characteristic microarrays and the group of candidate genes suggestive of a pathology.	dna microarray	Tran Trang;Nguyen Cam Chi;Hoang Ngoc Minh	2007	J. Global Optimization	10.1007/s10898-007-9158-9	gene chip analysis;dna microarray;bioinformatics;theoretical computer science;data mining;mathematics	Comp.	2.81584996104363	-48.786607683069064	137687
10e674c20494b763f3db9100aa75ae4d0c824525	redundancy reduction: does it help associative classifiers?	association rules;redundancy;associative classifiers	The number of classification rules discovered in associative classification is typically quite large. In addition, these rules contain redundant information since classification rules are obtained from mined frequent itemsets and the latter are known to be repetitive. In this paper we investigate through an empirical study the performance of associative classifiers when the classification rules are generated from frequent, closed and maximal itemsets. We show that maximal itemsets substantially reduce the number of classification rules without jeopardizing the accuracy of the classifier. Our extensive analysis demonstrates that the performance remains stable and even improves in some cases. Our analysis using cost curves also provides recommendations on when it is appropriate to remove redundancy in frequent itemsets.	foremost;maximal set;mined	Maria-Luiza Antonie;Osmar R. Zaïane;Robert C. Holte	2016		10.1145/2851613.2851649	association rule learning;computer science;machine learning;pattern recognition;data mining;redundancy	ML	9.091378213000585	-39.4166667650123	137891
a80130e62a0c76149e3b07d082bba0f99be62f9e	topevm: using co-occurrence and topology patterns of enzymes in metabolic networks to construct phylogenetic trees	document clustering;topevm;evolutionary distance;metabolic network;co occurrence pattern;topology pattern;enzyme;phylogenetic tree;pattern recognition;degree centrality;phylogenetic analysis	Network-based phylogenetic analysis typically involves representing metabolic networks as graphs and analyzing the characteristics of vertex sets using set theoretic measures. Such approaches, however, fail to take into account the structural characteristics of graphs. In this paper we propose a new pattern recognition technique,  TopEVM , to help representing metabolic networks as weighted vectors. We assign weights according to co-occurrence patterns and topology patterns of enzymes, where the former are determined in a manner similar to the  Tf-Idf approach used in document clustering, and the latter are determined using the degree centrality of enzymes. By comparing the weighted vectors of organisms, we determine the evolutionary distances and construct the phylogenetic trees. The resulting  TopEVM trees are compared to the previous  NCE trees with the NCBI Taxonomy trees as reference. It shows that  TopEVM can construct trees much closer to the NCBI Taxonomy trees than the previous  NCE methods.	phylogenesis	Tingting Zhou;Keith C. C. Chan;Zhenghua Wang	2008		10.1007/978-3-540-88436-1_20	biology;biochemistry;enzyme;combinatorics;phylogenetic tree;document clustering;computer science;bioinformatics;machine learning;mathematics;centrality;phylogenetic network;metabolic network	Comp.	2.3836198272024594	-48.82461895501441	137903
1c91c64534700e053ab0ba1fba499d74bc3007d7	cloud-scale genomic signals processing classification analysis for gene expression microarray data	bioinformatics wavelet analysis wavelet transforms gene expression genomics tumors signal processing;wavelet transforms cloud computing data analysis dna genetics genomics inference mechanisms medical signal processing rna signal classification signal denoising;functional genomics cloud scale genomic signals processing classification analysis gene expression microarray data dna sequence data analysis mrna sequence data analysis biological inference wavelet preprocessing wavelet classification wavelet thresholding wavelet based denoising cloud based distributed processing environment cloud computing global cancer map gcm gene pattern analysis biological processes	As microarray data available to scientists continues to increase in size and complexity, it has become overwhelmingly important to find multiple ways to bring inference though analysis of DNA/mRNA sequence data that is useful to scientists. Though there have been many attempts to elucidate the issue of bringing forth biological inference by means of wavelet preprocessing and classification, there has not been a research effort that focuses on a cloud-scale classification analysis of microarray data using Wavelet thresholding in a Cloud environment to identify significantly expressed features. This paper proposes a novel methodology that uses Wavelet based Denoising to initialize a threshold for determination of significantly expressed genes for classification. Additionally, this research was implemented and encompassed within cloud-based distributed processing environment. The utilization of Cloud computing and Wavelet thresholding was used for the classification 14 tumor classes from the Global Cancer Map (GCM). The results proved to be more accurate than using a predefined p-value for differential expression classification. This novel methodology analyzed Wavelet based threshold features of gene expression in a Cloud environment, furthermore classifying the expression of samples by analyzing gene patterns, which inform us of biological processes. Moreover, enabling researchers to face the present and forthcoming challenges that may arise in the analysis of data in functional genomics of large microarray datasets.	analysis of algorithms;class;classification;cloud computing;computation (action);disseminated malignant neoplasm;distributed computing;feature selection;functional genomics;gnas wt allele;gene expression;google cloud messaging;guardian service processor;inference;microarray;neoplasms;noise reduction;p-value;preprocessor;robustness (computer science);signal processing;thresholding (image processing);wavelet	Benjamin Simeon Harvey;Soo-Yeon Ji	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6943968	biology;bioinformatics;data science;data mining	Comp.	6.0543010904664305	-49.99059787744714	137912
043754242b481d296ddb74c3cab1e5413e3f64eb	fuzzy weighted c-harmonic regressions clustering algorithm		As a well-known regression clustering algorithm, fuzzy c-regressions (FCR) has been widely studied and applied in various areas. However, FCR appears to be rather sensitive to the undesirable initialization and the presence of noise or outliers in data sets. As a modified alternative, possibilistic c-regressions (PCR) can ameliorate the problem of noise and outliers, but it depends more heavily on initial values. Besides, the number of models should be determined a priori in both algorithms. To overcome these issues, this paper proposes a generalized alternative, called fuzzy weighted c-harmonic regressions (FWCHR), in which, a dynamic-like weight term based on the distinguished feature of the harmonic average is first introduced to enhance robustness. Furthermore, FWCHR can encompass FCR and PCR if some conditions are satisfied. And then a generalized mountain method (GMM) is proposed to automatically determine the number of models and estimate the initial values, which makes the proposed FWCHR algorithm totally unsupervised. Some numerical simulations and real applications are conducted to validate the performance of our algorithms.	algorithm;cluster analysis	Yang Zhao;Pei-hong Wang;Yi-Guo Li;Meng-yang Li	2018	Soft Comput.	10.1007/s00500-017-2642-3	correlation clustering;k-medians clustering;machine learning;pattern recognition;cure data clustering algorithm;cluster analysis	ECom	2.9545785461825727	-38.899229926954085	137947
35fee5e6fccbdad9582a305475b26c08f2e7c6b7	automated classification of software issue reports using machine learning techniques: an empirical study	bug classification;machine learning;f-measure;accuracy;random forest	Software developers, testers and customers routinely submit issue reports to software issue trackers to record the problems they face in using a software. The issues are then directed to appropriate experts for analysis and fixing. However, submitters often misclassify an improvement request as a bug and vice versa. This costs valuable developer time. Hence automated classification of the submitted reports would be of great practical utility. In this paper, we analyze how machine learning techniques may be used to perform this task. We apply different classification algorithms, namely naive Bayes, linear discriminant analysis, k-nearest neighbors, support vector machine (SVM) with various kernels, decision tree and random forest separately to classify the reports from three open-source projects. We evaluate their performance in terms of F-measure, average accuracy and weighted average F-measure. Our experiments show that random forests perform best, while SVM with certain kernels also achieve high performance.	decision tree;dictionary;experiment;f1 score;issue tracking system;k-nearest neighbors algorithm;linear discriminant analysis;local-density approximation;machine learning;naive bayes classifier;open-source software;preprocessor;r language;random forest;software developer;support vector machine	Nitish Pandey;Debarshi Kumar Sanyal;Abir Hudait;Amitava Sen	2017	Innovations in Systems and Software Engineering	10.1007/s11334-017-0294-1	support vector machine;computer science;naive bayes classifier;random forest;decision tree;data mining;software;versa;statistical classification;machine learning;linear discriminant analysis;artificial intelligence	SE	7.040887353760578	-40.44127909217578	138208
16fd25b235ec5e931291d56f845c13fd67a5f3c3	time series motifs statistical significance	motif discovery;statistical significance tests;haslab haslab uminho;significant patterns;time series	Time series motif discovery is the task of extracting previously unknown recurrent patterns from time series data. It is an important problem within applications that range from finance to health. Many algorithms have been proposed for the task of efficiently finding motifs. Surprisingly, most of these proposals do not focus on how to evaluate the discovered motifs. They are typically evaluated by human experts. This is unfeasible even for moderately sized datasets, since the number of discovered motifs tends to be prohibitively large. Statistical significance tests are widely used in bioinformatics and association rules mining communities to evaluate the extracted patterns. In this work we present an approach to calculate time series motifs statistical significance. Our proposal leverages work from the bioinformatics community by using a symbolic definition of time series motifs to derive each motif's p-value. We estimate the expected frequency of a motif by using Markov Chain models. The p-value is then assessed by comparing the actual frequency to the estimated one using statistical hypothesis tests. Our contribution gives means to the application of a powerful technique statistical tests to a time series setting. This provides researchers and practitioners with an important tool to evaluate automatically the degree of relevance of each extracted motif.	algorithm;association rule learning;bioinformatics;characterization test;markov chain;relevance;sequence motif;time series	Nuno Castro;Paulo J. Azevedo	2011		10.1137/1.9781611972818.59	bioinformatics;time series;pattern recognition;statistics	ML	3.5070630367573754	-50.215212158440856	138348
fdc2f73850994229e0b315a35eece968be3050e2	linmap: visualizing complexity gradients in evolutionary landscapes	development;complexity;visualization;phase transition;artificial cell lineages complexity development evolution visualization;cell lineage;artificial cell lineages;parameter space;evolution	This article describes an interactive visualization tool, LinMap, for exploring the structure of complexity gradients in evolutionary landscapes. LinMap is a computationally efficient and intuitive tool for visualizing and exploring multidimensional parameter spaces. An artificial cell lineage model is presented that allows complexity to be quantified according to several different developmental and phenotypic metrics. LinMap is applied to the evolutionary landscapes generated by this model to demonstrate that different definitions of complexity produce different gradients across the same landscape; that landscapes are characterized by a phase transition between proliferating and quiescent cell lineages where both complexity and diversity are maximized; and that landscapes defined by adaptive fitness and complexity can display different topographical features.	artificial cell;computational complexity theory;image gradient;interactive visualization;lineage (evolution);phase transition;population parameter;topography	Nicholas Geard;Janet Wiles	2008	Artificial Life	10.1162/artl.2008.14.3.14304	phase transition;biology;complexity;visualization;computer science;bioinformatics;evolution;parameter space;ecology;genetics	Graphics	-4.074265477562451	-47.73166006860748	138353
4bc7227cb297905698701786390469509afb576b	survey and taxonomy of feature selection algorithms in intrusion detection system	intruder detector;empaqueteur;redundancia;securite informatique;intrusion detection;systematique;envolvero;computer security;hybrid;research and development;redundancy;sistematica;filter;criptografia;cryptography;seguridad informatica;taxonomy;intrusion detection systems;detection rate;cryptographie;feature selection;detecteur intrus;detector intruso;systeme detection intrusion;intrusion detection system;redondance;wrapper	The Intrusion detection system deals with huge amount of data which contains irrelevant and redundant features causing slow training and testing process, higher resource consumption as well as poor detection rate. Feature selection, therefore, is an important issue in intrusion detection. In this paper we introduce concepts and algorithms of feature selection, survey existing feature selection algorithms in intrusion detection systems, group and compare different algorithms in three broad categories: filter, wrapper, and hybrid. We conclude the survey by identifying trends and challenges of feature selection research and development in intrusion detection system.	algorithm;feature selection;intrusion detection system;relevance	You Chen;Yang Li;Xueqi Cheng;Li Guo	2006		10.1007/11937807_13	anomaly-based intrusion detection system;intrusion detection system;anomaly detection;computer science;pattern recognition;data mining;feature selection;computer security;feature;taxonomy	Security	7.480581285233453	-38.01810843260099	138394
a5a1d6b4a45eaae41313306267e0a15d5aab4044	an efficient k-means clustering algorithm using simple partitioning	extraction information;analyse amas;partition method;analisis datos;centroid;information extraction;algorithme k moyenne;k d tree;data mining;data analysis;cluster analysis;methode partition;fouille donnee;clustering;algoritmo k media;analyse donnee;k means algorithm;analisis cluster;metodo particion;busca dato;extraccion informacion;k means clustering	The k-means algorithm is one of the most widely used methods to partition a dataset into groups of patterns. However, most k-means methods require expensive distance calculations of centroids to achieve convergence. In this paper, we present an efficient algorithm to implement a k-means clustering that produces clusters comparable to slower methods. In our algorithm, we partition the original dataset into blocks; each block unit, called a unit block (UB), contains at least one pattern. We can locate the centroid of a unit block (CUB) by using a simple calculation. All the computed CUBs form a reduced dataset that represents the original dataset. The reduced dataset is then used to compute the final centroid of the original dataset. We only need to examine each UB on the boundary of candidate clusters to find the closest final centroid for every pattern in the UB. In this way, we can dramatically reduce the time for calculating final converged centroids. In our experiments, this algorithm produces comparable clustering results as other k-means algorithms, but with much better performance.	algorithm;binary splitting;cluster analysis;computer performance;converged storage;disk partitioning;experiment;iteration;k-means clustering;ming library;response time (technology);run time (program lifecycle phase);stochastic process;time complexity;undefined behavior;yang	Ming-Chuan Hung;Jungpin Wu;Jih-Hua Chang;Don-Lin Yang	2005	J. Inf. Sci. Eng.		computer science;machine learning;data mining;mathematics;cluster analysis;information extraction;algorithm;k-means clustering	AI	-2.0966212326093676	-39.88390993979601	138398
bb4ff1990065b56c6e5371d8fa6af9000e6f0abd	classification based on associations (cba) - a performance analysis		Classification Based on Associations (CBA) has for two decades been the algorithm of choice for researchers as well as practitioners owing to simplicity of the produced rules, accuracy of models, and also fast model building. Two versions of CBA differing in speed – M1 and M2 – were originally proposed by Liu et al in 1998. While the more complex M2 version was originally designated as on average 50% faster, in this article we present benchmarks performed with multiple CBA implementations on the UCI lymph dataset contesting the M2 supremacy: the results show that M1 had faster processing speeds in most evaluated setups. M2 was recorded to be faster only when the number of input rules was very small and the number of input instances was large. We hypothesize that the better performance of the M1 version can be attributed to recent advances in optimization of vectorized operations and memory structures in SciKit learn and R, which the M1 can better utilize due to better predispositions for vectorization. This paper is accompanied by a Python implementation of CBA available at https://pypi.org/project/pyARC/.	algorithm;array programming;automatic vectorization;hl7publishingsubsection <operations>;mathematical optimization;mental association;mice, inbred cba;profiling (computer programming);python package index;rule (guideline);silo (dataset);supremacy: your will be done;version	Filip Jǐŕı;Tomás Kliegr	2018			data mining;computer science	AI	9.283189660654283	-46.21023674602902	138478
686b9b1bb4e5b44cf74cd143d0b4aea0f49ac62b	condorcet's jury theorem for consensus clustering		The goal of consensus clustering is to improve the quality of clustering by combining a sample of partitions of a dataset to a single consensus partition. This contribution extends Condorcet's Jury Theorem to the mean partition approach of consensus clustering. As a consequence of the proposed result, we challenge and reappraise the role of diversity in consensus clustering.	cluster analysis;consensus clustering	Brijnesh Jain	2018		10.1007/978-3-030-00111-7_14	machine learning;data mining;mathematics;statistics	AI	2.178322879216052	-42.041254482622605	138507
37ee38613a57ee5d91c6140d53bf3528ac9303d2	cofre: a fuzzy rule coevolutionary approach for multiclass classification problems	evolutionary computation;coevolution genetic fuzzy rule classifiers;fuzzy rules;fuzzy set theory;multiclass classification problem genetic fuzzy rule classifiers fuzzy rule coevolutionary approach;fuzzy set theory evolutionary computation;multiclass classification;gamma ray bursts genetics evolutionary computation character generation machine learning fuzzy logic neural networks knowledge based systems iterative methods round robin	This paper presents a technique for solving multiclass classification problems using a revolutionary approach. There are m populations, where m is the number of classes. Individuals from different classes work together to define a classifier, while individuals from the same population compete among them to define the best rule for a class. Finally, the best classifier is selected according to its performance. Experiments are conducted on different publicly available data sets.	best practice;experiment;fuzzy rule;multiclass classification;population;statistical classification	Jonatan Gómez;Arturo Garcia;Camilo Silva	2005	2005 IEEE Congress on Evolutionary Computation	10.1109/CEC.2005.1554885	defuzzification;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;multiclass classification;pattern recognition;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;evolutionary computation	Vision	9.191328329426955	-40.55695517829072	138767
f4d3ef8195010f69df109dfcc8d0e112631e889d	variable lag variography using k-means clustering	experimental variogram;variogram modelling;k means clustering	Experimental variography in?three dimensions based on drillhole data and current modelling software requires the selection of particular directions (azimuth and plunge) and a basic lag distance. Variogram points are then calculated on distances which are multiples of that basic lag. As samples rarely follow a regular grid, directional and distance tolerances are applied in order to have sufficient pairs to calculate reliable variogram points. This process is adequate when drillholes follow a drilling pattern (even if not an exactly regular grid) but can be time consuming and hard when the drilling pattern is irregular or when drillhole orientations vary considerably. Having all variogram points being calculated on multiples of a fixed lag, and the same tolerance being applied throughout the range of distances used, can be very restrictive and a reason for considerable time wasting or even failure to calculate an interpretable experimental variogram. The method discussed in this paper is using k-means clustering of sample pairs based on pair separation distance leading to a number of clusters each representing a different variogram point. This way, lag parameters are adjusted automatically to match the spatial distribution of sample locations and the resulting variogram is improved. Case studies are provided showing the benefits of this method over current fixed-lag experimental variogram calculation techniques.	cluster analysis;k-means clustering	Ioannis Konstantinou Kapageridis	2015	Computers & Geosciences	10.1016/j.cageo.2015.04.004	econometrics;computer science;variogram;data mining;mathematics;statistics;k-means clustering	AI	-0.4896995740855534	-42.352272606029025	139081
18afe5cd26ce5bc05e268705be604e7d422eb1e6	remotely sensed image classification by complex network eigenvalue and connected degree	image processing computer assisted;algorithms;computer simulation	It is a well-known problem of remotely sensed images classification due to its complexity. This paper proposes a remotely sensed image classification method based on weighted complex network clustering using the traditional K-means clustering algorithm. First, the degree of complex network and clustering coefficient of weighted feature are used to extract the features of the remote sensing image. Then, the integrated features of remote sensing image are combined to be used as the basis of classification. Finally, K-means algorithm is used to classify the remotely sensed images. The advantage of the proposed classification method lies in obtaining better clustering centers. The experimental results show that the proposed method gives an increase of 8% in accuracy compared with the traditional K-means algorithm and the Iterative Self-Organizing Data Analysis Technique (ISODATA) algorithm.	algorithm;bands;classification;cluster analysis;clustering coefficient;complex network;computer vision;eigenvalue;experiment;iterative method;k-means clustering;network synthesis filters;normal statistical distribution;statistical cluster	Mengxi Xu;Chenglin Wei	2012		10.1155/2012/632703	computer simulation;computer vision;computer science;machine learning;pattern recognition;algorithm	AI	2.4010031388582536	-41.43776355129685	139093
4db53f6862035096fbf0aa08c4a6e128bfae3952	an extended fuzzy clustering algorithm and its application		For data analyses, it is very important to combine data with similar attribute values into a categorically homogeneous subset, called a cluster, and this technique is called clustering. Generally crisp clustering algorithms are weak in noise, because each datum should be assigned to exactly one cluster. In order to solve the problem, a fuzzy c-means, a fuzzy maximum likelihood estimation, and an optimal fuzzy clustering algorithms in the fuzzy set theory have been proposed. They, however, require a lot of processing time because of exhaustive iteration with an amount of data and their memberships. Especially large memory space results in the degradation of performance in real-time processing applications, because it takes too much time to swap between the main memory and the secondary memory. To overcome these limitations, an extended fuzzy clustering algorithm based on an unsupervised optimal fuzzy clustering algorithm is proposed in this paper. This algorithm assigns a weight factor to each distinct datum considering its occurrence rate. Also, the proposed extended fuzzy clustering algorithm considers the degree of importances of each attribute, which determines the characteristics of the data. The worst case is that the whole data has an uniformly normal distribution, which means the importance of all attributes are the same. The proposed extended fuzzy clustering algorithm has better performance than the unsupervised optimal fuzzy clustering algorithm in terms of memory space and execution time in most cases. For simulation the proposed algorithm is applied to color image segmentation. Also automatic target detection and multipeak detection are considered as applications. These schemes can be applied to any other fuzzy clustering algorithms.	algorithm;cluster analysis;fuzzy clustering	Su Hwan Kim;Seon Wook Kim;Tae Won Rhee	1995	Journal of Circuits, Systems, and Computers	10.1142/S0218126695000175	correlation clustering;constrained clustering;data stream clustering;defuzzification;k-medians clustering;fuzzy clustering;type-2 fuzzy sets and systems;flame clustering;fuzzy classification;fuzzy number;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;dbscan;fuzzy set operations;affinity propagation;clustering high-dimensional data	EDA	2.345719083662658	-39.79766049564157	139195
764c316047296557678dd6f4c62f040cf4013833	pbirch: a scalable parallel clustering algorithm for incremental data	pbirch scalable parallel clustering algorithm;cluster algorithm;pattern clustering;message passing share nothing model;resource allocation;large data sets;very large databases message passing parallel algorithms pattern clustering resource allocation;massive dataset clustering pbirch scalable parallel clustering algorithm incremental data load balance message passing share nothing model;clustering algorithms message passing computer science scalability delay broadcasting algorithm design and analysis partitioning algorithms memory management time factors;message passing;load balance;very large databases;incremental data;massive dataset clustering;parallel algorithms	We present a parallel version of BIRCH with the objective of enhancing the scalability without compromising on the quality of clustering. The incoming data is distributed in a cyclic manner (or block cyclic manner if the data is bursty) to balance the load among processors. The algorithm is implemented on a message passing share-nothing model. Experiments show that for very large data sets the algorithm scales nearly linearly with the increasing number of processors. Experiments also show that clusters obtained by PBIRCH are comparable to those obtained using BIRCH	algorithm;birch;central processing unit;cluster analysis;experiment;message passing;scalability	Ashwani Garg;Ashish Mangla;Neelima Gupta;Vasudha Bhatnagar	2006	2006 10th International Database Engineering and Applications Symposium (IDEAS'06)	10.1109/IDEAS.2006.36	correlation clustering;constrained clustering;data stream clustering;parallel computing;message passing;resource allocation;computer science;load balancing;theoretical computer science;canopy clustering algorithm;cure data clustering algorithm;distributed computing;parallel algorithm;cluster analysis;programming language;affinity propagation;clustering high-dimensional data	DB	-3.0391244103038138	-39.203910587201754	139212
2f31864dde77b6ecd7bdc346e49ebcc97d2b74ee	subspace clustering	pattern clustering;image motion analysis;image segmentation;computer vision;face recognition	Subspace clustering refers to the task of identifying clusters of similar objects or data records (vectors) where the similarity is defined with respect to a subset of the attributes (i.e., a subspace of the data space). The subspace is not necessarily (and actually is usually not) the same for different clusters within one clustering solution. In this article, the problems motivating subspace clustering are sketched, different definitions and usages of subspaces for clustering are described, and exemplary algorithmic solutions are discussed. Finally, we sketch current research directions. © 2012 Wiley Periodicals, Inc. © 2012 Wiley Periodicals, Inc.	cluster analysis;clustering high-dimensional data	Hans-Peter Kriegel;Peer Kröger;Arthur Zimek	2012	Wiley Interdiscip. Rev. Data Min. Knowl. Discov.	10.1002/widm.1057	facial recognition system;correlation clustering;computer vision;flame clustering;computer science;canopy clustering algorithm;machine learning;segmentation-based object categorization;consensus clustering;pattern recognition;cure data clustering algorithm;image segmentation;cluster analysis;dbscan;biclustering;clustering high-dimensional data	ML	0.7684245245148243	-41.88648591528291	139349
6c58631dc40c8ccdf7043216dbc87429057c0f8c	machine learning and knowledge discovery in databases	inductive database;constraint based clustering;inductive query language	s of Invited Talks Using Machine Learning Powers for Good	data mining;machine learning	Hendrik Blockeel;Kristian Kersting;Siegfried Nijssen;Filip Zelezný	2013		10.1007/978-3-642-40994-3	correlation clustering;constrained clustering;data stream clustering;cluster analysis;brown clustering	ML	-0.22286101410959283	-38.28439831432522	139439
03f8d42e7d9db00dbcaa452c908f8295af2a515f	using machine learning techniques and genomic/proteomic information from known databases for defining relevant features for ppi classification	support vector machines;ppi classification;feature extraction and selection;proteomic in protein interaction	In modern proteomics, prediction of protein-protein interactions (PPIs) is a key research line, as these interactions take part in most essential biological processes. In this paper, a new approach is proposed to PPI data classification based on the extraction of genomic and proteomic information from well-known databases and the incorporation of semantic measures. This approach is carried out through the application of data mining techniques and provides very accurate models with high levels of sensitivity and specificity in the classification of PPIs. The well-known support vector machine paradigm is used to learn the models, which will also return a new confidence score which may help expert researchers to filter out and validate new external PPIs. One of the most-widely analyzed organisms, yeast, will be studied. We processed a very high-confidence dataset by extracting up to 26 specific features obtained from the chosen databases, half of them calculated using two new similarity measures proposed in this paper. Then, by applying a filter-wrapper algorithm for feature selection, we obtained a final set composed of the eight most relevant features for predicting PPIs, which was validated by a ROC analysis. The prediction capability of the support vector machine model using these eight features was tested through the evaluation of the predictions obtained in a set of external experimental, computational, and literature-collected datasets.		José M. Urquiza;Ignacio Rojas;Héctor Pomares;Luis Javier Herrera;J. P. Florido;Olga Valenzuela;Maria del Mar Cepero	2012	Computers in biology and medicine	10.1016/j.compbiomed.2012.01.010	support vector machine;computer science;bioinformatics;machine learning;pattern recognition;data mining	ML	8.11331823345569	-48.88177960730089	139500
b0c22c48b43a2d7fb4f1106b41a2888188194b05	a support vector machine based method for credit risk assessment	credit scoring tool support vector machine credit risk assessment credit card industry financial company svm based ensemble model principle component analysis credit feature selection genetic algorithm optimal ensemble strategy;kernel;credit scoring;support vector machines credit transactions genetic algorithms principal component analysis risk management;ensemble method;support vector machines;support vector machines kernel risk management artificial neural networks genetic algorithms classification algorithms gallium;risk management;financial company;artificial neural networks;svm based ensemble model;credit risk assessment;credit feature selection;principal component analysis;credit card industry;credit transactions;classification algorithms;credit scoring tool;genetic algorithm;genetic algorithms;feature selection;principle component analysis;support vector machine;credit scoring support vector machine ensemble method credit risk assessment;optimal ensemble strategy;credit risk;credit cards;gallium	The credit card industry has been growing rapidly in recent years, and credit risk assessment becomes critically important for financial companies. In this paper, a novel support vector machine (SVM) based ensemble model is proposed for credit risk assessment. In the proposed method, principles component analysis (PCA) is firstly employed for credit feature selection. Secondly, SVMs with different kernels are trained by using genetic algorithm (GA) to optimize the parameters, and the corresponding assessment results are obtained. Thirdly, all results produced by different SVMs are combined by several ensemble strategies. Finally, an optimal ensemble strategy is selected for credit scoring. For validation, two real world credit datasets are used to test the effectiveness and efficiency of our proposed method. The experiment results find that our proposed ensemble model outperforms commonly used credit scoring tools. The findings of the study reveal the support vector machine based ensemble method to be a promising alternative for credit scoring.	decision support system;ensemble kalman filter;ensemble learning;feature selection;futures studies;genetic algorithm;information engineering;information security;intrusion detection system;knowledge engineering;mathematical optimization;risk assessment;support vector machine	Wei Xu;Shenghu Zhou;Dongmei Duan;Yanhui Chen	2010	2010 IEEE 7th International Conference on E-Business Engineering	10.1109/ICEBE.2010.44	support vector machine;genetic algorithm;risk management;computer science;machine learning;pattern recognition;data mining;feature selection;principal component analysis	AI	8.41224689840873	-38.74612532877504	139818
a53338fe3bfd0a1677f2bc23b772d41bbc8511e8	whole genome phylogenetic tree reconstruction using colored de bruijn graphs		We present kleuren, a novel assembly-free method to reconstruct phylogenetic trees using the Colored de Bruijn Graph. kleuren works by constructing the Colored de Bruijn Graph and then traversing it, finding bubble structures in the graph that provide phylogenetic signal. The bubbles are then aligned and concatenated to form a supermatrix, from which a phylogenetic tree is inferred. We introduce the algorithms that kleuren uses to accomplish this task, and show its performance on reconstructing the phylogenetic tree of 12 Drosophila species. kleuren reconstructed the established phylogenetic tree accurately, and is a viable tool for phylogenetic tree reconstruction using whole genome sequences. Software package available at: https://github.com/Colelyman/kleuren	algorithm;concatenation;de bruijn graph;phylogenetic tree;phylogenetics	Cole A. Lyman;M. Stanley Fujimoto;Anton Suvorov;Paul Bodily;Quinn Snell;Keith A. Crandall;Seth M. Bybee;Mark J. Clement	2017	2017 IEEE 17th International Conference on Bioinformatics and Bioengineering (BIBE)	10.1109/BIBE.2017.00-44	bioinformatics;phylogenetics;phylogenetic tree;computer science;phylogenetic network;genome;de bruijn graph;de bruijn sequence;graph	Comp.	0.7358529632049293	-51.03789335582792	139958
5bc10450dc0a67aa0c3c0201783a9bdc5eda4e2f	"""a comment on """"correlation as a heuristic for accurate and comprehensible ant colony optimization-based classifiers"""""""	ripper algorithm ant colony optimization based classifiers classification rule mining algorithm correlation based function aco component antminer algorithm;statistical analysis ant colony optimisation data mining pattern classification	The paper provides a comment from the authors regarding the correlation as a heuristic for accurate and comprehensible ant colony optimization-based classifiers. Baig et al. proposed a new classification rule mining algorithm in IEEE TEC. The technique introduces a correlation-based function for the ant colony optimization (ACO) component. The authors compare variants of the technique with several existing ACO-based algorithms and the state-of-the art RIPPER algorithm. The AntMiner+ algorithm, proposed by Martens et al. (2007) in IEEE TEC, was also included in the comparison. However, the reported performance is far below what is expected and is more akin to making random predictions.	ant colony optimization algorithms;heuristic	Bart Minnaert;David Martens	2014	IEEE Trans. Evolutionary Computation	10.1109/TEVC.2014.2358376	ant colony optimization algorithms;computer science;machine learning;pattern recognition;data mining	EDA	9.515402670420393	-43.53402441359367	140075
491158f9b9df38747af7e910dc23b7f205a729f5	a hybrid metaheuristic for biclustering based on scatter search and genetic algorithms	scatter search;evolutionary computation;gene expression data;biclustering;b cell;cell cycle;genetic algorithm;hybrid algorithm;evolutionary computing	In this paper a hybrid metaheuristic for biclustering based on Scatter Search and Genetic Algorithms is presented. A general scheme of Scatter Search has been used to obtain high---quality biclusters, but a way of generating the initial population and a method of combination based on Genetic Algorithms have been chosen. Experimental results from yeast cell cycle and human B-cell lymphoma are reported. Finally, the performance of the proposed hybrid algorithm is compared with a genetic algorithm recently published.		Juan A. Nepomuceno;Alicia Troncoso Lora;Jesús S. Aguilar-Ruiz	2009		10.1007/978-3-642-04031-3_18	biology;mathematical optimization;genetic algorithm;hybrid algorithm;computer science;bioinformatics;cell cycle;machine learning;genetic representation;evolutionary computation	AI	4.22628387469497	-46.822508636781286	140197
7f568497d60b7ce4ddded755754efdd82db1cdbf	pattern cores and connectedness in cancer gene expression	tumours cancer genetics pattern clustering;cancer gene expression neoplasms clustering algorithms pattern analysis data mining shape data engineering algorithm design and analysis data analysis;gene expressions;cluster algorithm;hidden information;pattern clustering;expression pattern;pattern cores;cancer;tumours;genetics;connected based clustering gene expressions breast cancer tumor pattern cores;gene expression;clustering;tumor;connected patterns;density based cores;gene expression pattern;breast cancer;connected patterns gene expression clustering density based cores;connected based clustering	The huge number of gene expressions resulting from a single microarray experiment, together with the large number of tumor samples, needs efficient methods that can extract hidden information and structure in such data sets. Clustering is a common analysis tool used to find groups of gene expression patterns. However, analysis of large clusters can be an infeasible task in large sets. In this work, a method is proposed to capture the main structure of the data by identifying core gene expressions. This reduces the data to only a subset of representatives used to grasp the main behavior of gene expression. When integrated with clustering, it becomes feasible to analyze clusters of large sizes, and to identify main expression patterns and relations between them. The importance of using a connected-based clustering is emphasized in order to reveal the gradual change between core gene expressions, something which cannot be achieved using traditional clustering algorithms. Analysis is done on breast cancer data to illustrate the significance of the proposed methodology.	algorithm;cluster analysis;gene co-expression network;gene expression profiling;gene expression programming;microarray	Noha A. Yousri;Mohamed S. Kamel;Mohamed A. Ismail	2007	2007 IEEE 7th International Symposium on BioInformatics and BioEngineering	10.1109/BIBE.2007.4375551	correlation clustering;biology;constrained clustering;gene expression;fuzzy clustering;bioinformatics;breast cancer;machine learning;data mining;cluster analysis;genetics;cancer;clustering high-dimensional data	Comp.	5.083564987035755	-48.34978439396255	140349
515b3dbe5c879d7abc500dc9eba1d946040cf07c	fuzzy clustering with principal component analysis	cluster algorithm;pattern clustering;oblique hyperellipsoidal shapes principal component analysis similarity based fuzzy clustering hyperspherical shapes;similarity based fuzzy clustering;hyperspherical shapes;incremental clustering;fuzzy set theory;fuzzy clustering;oblique hyperellipsoidal shapes;principal component analysis fuzzy set theory pattern clustering;principal component analysis;number of clusters;web mining;oblique hyper ellipsoidal cluster;web mining incremental clustering fuzzy clustering principal component analysis oblique hyper ellipsoidal cluster	We propose a clustering algorithm which incorporates a similarity-based fuzzy clustering and principal component analysis. The proposed algorithm is capable of discovering clusters with hyper-spherical, hyper-ellipsoidal, or oblique hyper-ellipsoidal shapes. Besides, the number of the clusters need not be specified in advance by the user. For a given dataset, the orientation, locations, and the number of clusters obtained can truthfully reflect the characteristics of the dataset. Experimental results, obtained by running on datasets generated synthetically, show that our method performs better than other methods.	algorithm;cluster analysis;fuzzy clustering;oblique projection;principal component analysis	Min-Zong Rau;Chi-Yuan Yeh;Shie-Jue Lee	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580756	complete-linkage clustering;correlation clustering;web mining;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;fuzzy set;cluster analysis;single-linkage clustering;brown clustering;affinity propagation;principal component analysis;clustering high-dimensional data	ML	1.7859655553061955	-40.13914183985931	140698
cc7b5f20904927b668d2a939f2b60a610f9aac11	differential prioritization between relevance and redundancy in correlation-based feature selection techniques for multiclass gene expression data	computers;software;animals;data interpretation statistical;databases genetic;sequence analysis dna;gene expression data;computational biology bioinformatics;gene expression;tissue classification;reproducibility of results;models statistical;algorithms;feature selection;pattern recognition automated;humans;combinatorial libraries;classification accuracy;computational biology;computer appl in life sciences;gene expression profiling;oligonucleotide array sequence analysis;microarrays;bioinformatics	Due to the large number of genes in a typical microarray dataset, feature selection looks set to play an important role in reducing noise and computational cost in gene expression-based tissue classification while improving accuracy at the same time. Surprisingly, this does not appear to be the case for all multiclass microarray datasets. The reason is that many feature selection techniques applied on microarray datasets are either rank-based and hence do not take into account correlations between genes, or are wrapper-based, which require high computational cost, and often yield difficult-to-reproduce results. In studies where correlations between genes are considered, attempts to establish the merit of the proposed techniques are hampered by evaluation procedures which are less than meticulous, resulting in overly optimistic estimates of accuracy. We present two realistically evaluated correlation-based feature selection techniques which incorporate, in addition to the two existing criteria involved in forming a predictor set (relevance and redundancy), a third criterion called the degree of differential prioritization (DDP). DDP functions as a parameter to strike the balance between relevance and redundancy, providing our techniques with the novel ability to differentially prioritize the optimization of relevance against redundancy (and vice versa). This ability proves useful in producing optimal classification accuracy while using reasonably small predictor set sizes for nine well-known multiclass microarray datasets. For multiclass microarray datasets, especially the GCM and NCI60 datasets, DDP enables our filter-based techniques to produce accuracies better than those reported in previous studies which employed similarly realistic evaluation procedures.	algorithm;algorithmic efficiency;automated essay scoring;awards;benchmark (computing);computation (action);data-directed programming;differential dynamic programming;education, medical, graduate;estimated;evaluation function;feature selection;gene expression;gene co-expression network;google cloud messaging;kerrison predictor;lazy evaluation;lung diseases;manuscripts;mathematical optimization;mechwarrior: living legends;microarray;population parameter;redundancy (engineering);relevance;score;silo (dataset);standard widget toolkit;structure of parenchyma of lung;supervised learning;trusted computer system evaluation criteria	Chia Huey Ooi;Madhu Chetty;Shyh Wei Teng	2005	BMC Bioinformatics	10.1186/1471-2105-7-320	biology;gene expression;dna microarray;computer science;bioinformatics;data science;data mining;gene expression profiling;feature selection	Comp.	7.0141584447767595	-52.02154130513587	141308
654e9d15095254d9c52e09a6767cb276316cb7f7	analysis of proteomic spectral data by multi resolution analysis and self-organizing maps	wavelet analysis;high dimensionality;multi resolution analysis;spectra preprocessing;clinical proteomics;biomarker;self organized map;fuzzy visualization;mass spectrometric	Analysis and visualization of high-dimensional clinical proteomic spectra obtained from mass spectrometric measurements is a complicated issue. We present a wavelet based preprocessing combined with an unsupervised and supervised analysis by Self-Organizing Maps and a fuzzy variant thereof. This leads to an optimal encoding and a robust classifier incorporating the possibility of fuzzy labels.	fuzzy concept;map;organizing (structure);preprocessor;proteomics;wavelet	Frank-Michael Schleif;Thomas Villmann;Barbara Hammer	2007		10.1007/978-3-540-73400-0_72	wavelet;computer science;bioinformatics;data mining;biomarker;statistics	ML	7.608074403498337	-48.095766006096184	141943
5167000c9b21731e2e7eb6cc1496e960e8e80f73	trigen: a genetic algorithm to mine triclusters in temporal gene expression data	microarray data;tricluster;time series;genetic algorithms	Analyzing microarray data represents a computational challenge due to the characteristics of these data. Clustering techniques are widely applied to create groups of genes that exhibit a similar behavior under the conditions tested. Biclustering emerges as an improvement of classical clustering since it relaxes the constraints for grouping genes to be evaluated only under a subset of the conditions and not under all of them. However, this technique is not appropriate for the analysis of longitudinal experiments in which the genes are evaluated under certain conditions at several time points. We present the TriGen algorithm, a genetic algorithm that finds triclusters of gene expression that take into account the experimental conditions and the time points simultaneously. We have used TriGen to mine datasets related to synthetic data, yeast (Saccharomyces cerevisiae) cell cycle and human inflammation and host response to injury experiments. TriGen has proved to be capable of extracting groups of genes with similar patterns in subsets of conditions and times, and these groups have shown to be related in terms of their functional annotations extracted from the Gene Ontology. & 2013 Elsevier B.V. All rights reserved.	biclustering;chip-on-chip;cluster analysis;evaluation function;evolutionary algorithm;experiment;fitness function;gene ontology;genetic algorithm;genetic operator;geographic coordinate system;heuristic;interaction;microarray;synthetic data;synthetic intelligence;transcription (software)	David Gutiérrez-Avilés;Cristina Rubio-Escudero;Francisco Martínez-Álvarez;José Cristóbal Riquelme Santos	2014	Neurocomputing	10.1016/j.neucom.2013.03.061	microarray analysis techniques;genetic algorithm;computer science;bioinformatics;machine learning;time series;data mining	Comp.	4.50360070087546	-47.3112148049004	141981
b97047c4dc75cbe8d6fc5cb3dd5a81d36458892d	applied federated learning: improving google keyboard query suggestions		Federated learning is a distributed form of machine learning where both the training data and model training are decentralized. In this paper, we use federated learning in a commercial, global-scale setting to train, evaluate and deploy a model to improve virtual keyboard search suggestion quality without direct access to the underlying user data. We describe our observations in federated training, compare metrics to live deployments, and present resulting quality increases. In whole, we demonstrate how federated learning can be applied endto-end to both improve user experiences and enhance user privacy.		T. C. C. Yang;Galen Andrew;H. Eichner;Haicheng Sun;Wei Li;Nicholas Kong;Daniel Ramage;Franccoise Beaufays	2018	CoRR			HCI	-3.051560732033981	-45.12887156014811	141984
7c2dd937d71324db58c17028e6d666b2b6cb3841	adaptive multiobjective memetic fuzzy clustering algorithm for remote sensing imagery	remote sensing clustering algorithms memetics linear programming sociology statistics optimization;afcmoma algorithm adaptive multiobjective memetic fuzzy clustering algorithm remote sensing imagery global search capabilities;memetics;remote sensing fuzzy clustering memetic algorithm multiobjective;remote sensing fuzzy systems optimisation;remote sensing;statistics;linear programming;clustering algorithms;optimization;sociology	Due to the intrinsic complexity of remote sensing images and the lack of prior knowledge, clustering for remote sensing images has always been one of the most challenging tasks in remote sensing image processing. Recently, clustering methods for remote sensing images have often been transformed into multiobjective optimization problems, making them more suitable for complex remote sensing image clustering. However, the performance of the multiobjective clustering methods is often influenced by their optimization capability. To resolve this problem, this paper proposes an adaptive multiobjective memetic fuzzy clustering algorithm (AFCMOMA) for remote sensing imagery. In AFCMOMA, a multiobjective memetic clustering framework is devised to optimize the two objective functions, i.e., Jm and the Xie-Beni (XB) index. One challenging task for memetic algorithms is how to balance the local and global search capabilities. In AFCMOMA, an adaptive strategy is used, which can adaptively achieve a balance between them, based on the statistical characteristic of the objective function values. In addition, in the multiobjective memetic framework, in order to acquire more individuals with high quality, a new population update strategy is devised, in which the updated population is composed of individuals generated in both the local and global searches. Finally, to evaluate the proposed AFCMOMA algorithm, experiments using three remote sensing images were conducted, which confirmed the effectiveness of the proposed algorithm.	cluster analysis;data structure;display resolution;experiment;fuzzy clustering;fuzzy cognitive map;ground truth;image processing;local search (optimization);mathematical optimization;memetic algorithm;memetics;multi-objective optimization;optimization problem;pareto efficiency;single-index model;x image extension	Ailong Ma;Yanfei Zhong;Liangpei Zhang	2015	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2015.2393357	memetics;mathematical optimization;linear programming;canopy clustering algorithm;machine learning;data mining;mathematics;cluster analysis	Mobile	4.701394913271373	-41.3643960371696	142043
8fc34bc2f6a89aaefdbabc1a67cef8678f2c94dd	distributed nonnegative matrix factorization with hals algorithm on mapreduce		Nonnegative Matrix Factorization (NMF) is a commonly used method in machine learning and data analysis for feature extraction and dimensionality reduction of nonnegative data. Recently, we observe its increasing popularity in processing massive data, and advances in developing various distributed algorithms for NMF. In the paper, we propose a computational strategy for implementation of the Hierarchical Alternating Least Squares (HALS) algorithm using the MapReduce programming paradigm. Due to this approach, the scalable HALS NMF, which can be implemented on parallel and distributed computer architectures, is obtained. The scalability and efficiency of the proposed algorithm is confirmed in the numerical experiments, performed on large-scale synthetic and recommendation system datasets.	algorithm;mapreduce;non-negative matrix factorization	Rafal Zdunek;Krzysztof Fonal	2017		10.1007/978-3-319-65482-9_14	computer science;theoretical computer science;recommender system;distributed algorithm;scalability;feature extraction;programming paradigm;dimensionality reduction;least squares;algorithm;artificial intelligence;machine learning;non-negative matrix factorization	ML	-3.0452654220806217	-39.824092727465185	142191
8a35ac6f18ce1f1f21e99e978164a8d75efc8b32	ik-bkm: an incremental clustering approach based on intra-cluster distance	pattern clustering;intracluster distance;belief function theory;clustering algorithms uncertainty training context data mining heuristic algorithms clustering methods;dissimilarity measure;uncertainty;clusters number;k modes method;cluster partition update;intra cluster dissimilarity measure incremental clustering k modes method belief function theory clusters number;training;incremental clustering;incremental k belief k modes method;data mining;cluster partition update incremental clustering approach intracluster distance incremental k belief k modes method;data analysis;heuristic algorithms;pattern clustering belief maintenance data analysis;clustering algorithms;belief maintenance;intra cluster dissimilarity measure;clustering methods;categorical data;incremental clustering approach;context	This paper introduces a novel incremental approach to clustering uncertain categorical data. This so-called Incremental K Belief K-modes Method (IK-BKM) extends the Belief K-modes one to update the cluster partition when new information is available namely the increase of final desired clusters' number. The main objective is to update clusters' partition without complete reclustring. Our method will be illustrated by an example showing the comparative results of the incremental process and the non incremental one.	bkm algorithm;categorical variable;cluster analysis;programming paradigm;stationary process	Sarra Ben Hariz;Zied Elouedi	2010	ACS/IEEE International Conference on Computer Systems and Applications - AICCSA 2010	10.1109/AICCSA.2010.5587008	uncertainty;categorical variable;computer science;machine learning;pattern recognition;data mining;cluster analysis;data analysis;statistics	DB	2.409295865591578	-40.08489884310552	142343
5aa3b1f1459aeb567b35c9959e14f0552d04070f	a memetic co-clustering algorithm for gene expression profiles and biological annotation	dna;gene expression profile;unsupervised learning;biology computing;cluster algorithm;pattern clustering;evolutionary computation;expression pattern;expression profile;time measurement;annotation information;gene ontology information;data mining;memetic coclustering algorithm;ontologies artificial intelligence;genetics;gene expression;gene expression data analysis clustering algorithms ontologies biology computing dna unsupervised learning bioinformatics time measurement biological processes;data analysis;a priori knowledge;gene expression profiles;general regulatory mechanisms;biological data analysis;clustering algorithms;gene clustering;ontologies;biological data analysis memetic coclustering algorithm gene expression profiles biological annotation gene expression levels biological process general regulatory mechanisms gene clustering expression patterns gene ontology information annotation information;biological data;expression patterns;biological processes;biological annotation;biological process;bioinformatics;gene ontology;data analysis ontologies artificial intelligence pattern clustering genetics biology computing evolutionary computation data mining;gene expression levels	With the invention of microarrays, researchers are capable of measuring thousands of gene expression levels in parallel at various time points of the biological process. To investigate general regulatory mechanisms, biologists cluster genes based on their expression patterns. In this paper, we propose a new memetic co-clustering algorithm for expression profiles, which incorporates a priori knowledge in the form of gene ontology information. Ontologies offer a mechanism to capture knowledge in a shareable form that is also processable by computers. The use of this additional annotation information promises to improve biological data analysis and simplifies the identification of processes that are relevant under the measured conditions.	algorithm;biclustering;cluster analysis;computer cluster;gene ontology;memetics;microarray;ontology (information science)	Nora Speer;Christian Spieth;Andreas Zell	2004	Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)	10.1109/CEC.2004.1331091	unsupervised learning;computer science;bioinformatics;data science;data mining;biological process;evolutionary computation	Comp.	5.299448557135844	-49.68279783536719	142738
0caf08ab5da002218c197e783b6d44170cfa0f58	cluc: a natural clustering algorithm for categorical datasets based on cohesion	cluster algorithm;clustering;similarity;scalability;synthetic data;similarity measure;categorical data	We propose a clustering algorithm for categorical datasets, called CLUC (CLUstering with Cohesion), which uses a novel similarity measure, called cohesion, to determine the degree with which items/objects stick to clusters. We have implemented CLUC and carried out extensive experiments on real-life and synthetic datasets. The results of experiments and their analyses indicate that CLUC generates high quality clusters in that they conform to expert's opinion. Our experiments on large synthetic data confirm that CLUC is scalable when the dataset grows in the number of objects and/or dimensions. We also repeated the experiments with different orders of the items in the datasets. The results show that the proposed algorithm is order insensitive	algorithm;cluster analysis;cohesion (computer science);display resolution;experiment;real life;scalability;similarity measure;synthetic data;synthetic intelligence	Aida Nemalhabib;Nematollaah Shiri	2006		10.1145/1141277.1141422	scalability;similarity;categorical variable;computer science;data science;machine learning;data mining;cluster analysis;synthetic data	DB	0.07162395324897107	-41.36431861254006	142780
c33d90a47f4ae60983f7e09b720b23492068a531	maximum-scoring segment sets	simple recursive relationship;protein segmentation;disjoint segment;maximum-scoring segment set;sequence alignment;maximum-scoring set;segmentation;key result;noncoding rna gene;segment set;fast algorithm;noncoding rna;thermophiles.;maximum-scoring segment sets;change point estimation;thermophiles;molecular biophysics;dna;proteins;genetics	Abstract— We examine,the problem,of finding maximumscoring sets of disjoint segments in a sequence of scores. The problem arises in DNA and protein segmentation, and in postprocessing of sequence alignments. Our key result states a simple recursive relationship between maximum-scoring segment sets. The statement leads to fast algorithms for finding such segment sets. We apply our methods to the identification of non-coding RNA genes in thermophiles. Key words: segmentation, change point estimation, noncoding RNA, thermophiles I. I NTRODUCTION		Miklós Csürös	2004	IEEE/ACM Trans. Comput. Biology Bioinform.	10.1145/1042198.1042358	biology;thermophile;bioinformatics;sequence alignment;non-coding rna;segmentation;genetics;dna;molecular biophysics	Arch	-0.6911648271749113	-51.45818825988579	143525
59ccd6c38c2cd40254029a72c66c0607b0e9741f	a systematic mapping study of process mining		ABSTRACTThis study systematically assesses the process mining scenario from 2005 to 2014. The analysis of 705 papers evidenced ‘discovery’ (71%) as the main type of process mining addressed and ‘categorical prediction’ (25%) as the main mining task solved. The most applied traditional technique is the ‘graph structure-based’ ones (38%). Specifically concerning computational intelligence and machine learning techniques, we concluded that little relevance has been given to them. The most applied are ‘evolutionary computation’ (9%) and ‘decision tree’ (6%), respectively. Process mining challenges, such as balancing among robustness, simplicity, accuracy and generalization, could benefit from a larger use of such techniques.		Ana Rocío Cárdenas Maita;Lucas Corrêa Martins;Carlos Ramón López Paz;Laura Rafferty;Patrick C. K. Hung;Sarajane Marques Peres;Marcelo Fantinato	2018	Enterprise IS	10.1080/17517575.2017.1402371	robustness (computer science);data mining;evolutionary computation;computer science;categorical variable;decision tree;business process management;process mining;computational intelligence;graph	DB	8.83556781044426	-44.90600470821446	143536
f5c2631b5b0165d9213ecd944ad62fb4ffd22abe	a fast hardware software platform for computing irreducible testors	testor theory;fpga;hardware architecture;feature selection	Among the systems involved with data and knowledge that give answers, solutions, or diagnoses, based on available information; those based on feature selection are very important since they allow us to solve important tasks into pattern recognition and decision making areas. Feature selection consists in finding a minimum subset of attributes that preserves the ability to discern between objects from different classes. Testor theory is a convenient way to solve this problem since a testor is defined as a subset of attributes that can discern between objects from different classes; and an irreducible testor is a minimal subset with this property. However, the computation of these minimal subsets is a problem whose space complexity grows exponentially regarding the number of attributes. Therefore, in the literature, several hardware implementations of algorithms for computing testors, which take advantage of the inherent parallelism in the evaluation of testor candidates, have been proposed. In this paper, a new fast hardware software platform for computing irreducible testors is introduced. Our proposal follows a pruning strategy that, in most cases, reduces the search space more than any other alternative reported in the literature. The experimental results show the runtime reduction achieved by the proposed platform in contrast to other state-of-the-art hardware and software implementations. © 2015 Elsevier Ltd. All rights reserved. o g ( a a e t t a T s t l	algorithm;computation;dspace;feature selection;irreducibility;irreducible complexity;parallel computing;pattern recognition	Vladimir Rodriguez;José Francisco Martínez Trinidad;Jesús Ariel Carrasco-Ochoa;Manuel Lazo-Cortés;Claudia Feregrino Uribe;René Cumplido	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.07.037	computer science;artificial intelligence;theoretical computer science;machine learning;hardware architecture;mathematics;feature selection;algorithm;field-programmable gate array	AI	7.761391801176695	-42.05565812132871	143627
9ec6a56e0d3811ef4e965c3e773f10e8ed703aae	estimating the indexability of multimedia descriptors for similarity searching	communication conference;data analysis;visual content;audio content;similarity search	A study on properties of data sets representing public domain audio and visual content and their relation to their indexability is presented. Data analysis considers the pairwise distance distributions and various techniques to estimate the true intrinsic dimensionality of the studied data. One own alternative to dimensionality estimation is also presented. These results are contrasted with the indexability results gathered using indexing techniques M-Tree, LSH and hierarchical k-means tree.	curse of dimensionality;k-means clustering;m-tree;lsh	Stanislav Barton;Valérie Gouet-Brunet;Marta Rukoz;Christophe Charbuillet;Geoffroy Peeters	2010			computer science;data mining;world wide web;information retrieval	ML	-4.5064962585307065	-42.17025212322466	143846
19b58bc13c1626b5c5dc376a2e16f4d4fead9ac9	betamax: towards optimal sampling strategies for high-throughput screens	algorithms	Sample size is a critical component in the design of any high-throughput genetic screening approach. Sample size determination from assumptions or limited data at the planning stages, though standard practice, may at times be unreliable because of the difficulty of a priori modeling of effect sizes and variance. Methods to update the sample size estimate during the course of the study could improve statistical power. In this article, we introduce an approach to estimate the power and update it continuously during the screen. We use this estimate to decide where to sample next to achieve maximum overall statistical power. Finally, in simulations, we demonstrate significant gains in study recall over the naive strategy of equal sample sizes while maintaining the same total number of samples.	genetic screening (procedure);high-throughput computing;sample size;sample variance;simulation;throughput;total number	Dhruv Grover;Juan Nunez-Iglesias	2012	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2012.0036	sample size determination;biology;econometrics;computer science;data mining;statistics	Comp.	5.960477163075516	-51.95622288279973	143918
2c698cc745d8ff6c5aadd5829f94db94d8f6bdad	analyse the academic performance of students using ann classification with modified pillar k-means and iwfa	modified pillar k means;inertia weight firefly algorithm;minkowski distance;pre-processing;ann classification	The main objective of our work is to analyse the educational performance of students using Artificial Neural Network (ANN) classification with Modified Pillar K-Means and Inertia Weight Firefly Algorithm. The two theoretical measure of an ANN is training and testing. Those are pre-processed based on Minkowski distance as modified pillar k-means algorithm. It is an algorithm to optimize the initial centroids for k-means clustering with Minkowski distance. Modified pillar k means make use of cluster analysis to segment students into groups according to their grade value. An efficient optimization method called inertia weight firefly optimization algorithm for optimizing weight in ANN Classification. In order to progress the performance of the Firefly Algorithm (FA), the time changeable inertia weight is imported into position resumption of FA. The performance of the proposed work is evaluated in terms of Sensitivity, Precision, Specificity, Negative Predictive Value, Fall-out, False Negative Rate, False Discovery Rate, Accuracy and Mathews Correlation Coefficient. The proposed work will be implemented in the MATLAB tool.	k-means clustering	Smita Pallavi;Kanhaiya Lal;S. P. Lal	2017	Wireless Personal Communications	10.1007/s11277-017-4489-4	minkowski distance;false discovery rate;computer science;firefly algorithm;artificial neural network;correlation coefficient;cluster analysis;machine learning;k-means clustering;centroid;artificial intelligence	Mobile	6.052555815933176	-41.8883646748899	143924
74a13385008d98fb7d3c50c91e58f5eb7b31abc5	vector quantized spectral clustering applied to soybean whole genome sequences		We develop a Vector Quantized Spectral Clustering (VQSC) algorithm that is a combination of Spectral Clustering (SC) and Vector Quantization (VQ) sampling for grouping Soybean genomes. The inspiration here is to use SC for its accuracy and VQ to make the algorithm computationally cheap (the complexity of SC is cubic in-terms of the input size). Although the combination of SC and VQ is not new, the novelty of our work is in developing the crucial similarity matrix in SC as well as use of k-medoids in VQ, both adapted for the Soybean genome data. We compare our approach with commonly used techniques like UPGMA (Un-weighted Pair Graph Method with Arithmetic Mean) and NJ (Neighbour Joining). Experimental results show that our approach outperforms both these techniques significantly in terms of cluster quality (up to 25% better cluster quality) and time complexity (order of magnitude faster).	algorithm;cluster analysis;computational complexity theory;cubic function;information;k-medoids;medoid;neighbor joining;sampling (signal processing);scalability;similarity measure;spectral clustering;time complexity;upgma;vector quantization	Aditya A. Shastri;Kapil Ahuja;Milind B. Ratnaparkhe;Aditya Shah;Aishwary Gagrani;Anant Lal	2018	CoRR		order of magnitude;machine learning;time complexity;mathematics;artificial intelligence;vector quantization;sampling (statistics);spectral clustering;quantization (physics);upgma;arithmetic mean	HPC	-1.0004200347715186	-46.472570616227955	144054
fc1e1ca8828463993bdf9a2d99119a934140a95e	intclust: a software package for clustering replicated microarray data	microarray data;clustering quality evaluation;biology computing;pattern clustering;pattern clustering biology computing data analysis data mining data structures genetics;transformation model;distance measure;intclust;repeated measures;gene expression data;data mining;data representation;genetics;interval data;gene expression;data analysis;gene expression microarray data mining;replicated microarray data clustering;quality evaluation;data structures;software package;software packages data analysis software measurement clustering algorithms stability reproducibility of results computer science bayesian methods data mining performance analysis;modified k means clustering;k means clustering;clustering quality evaluation intclust software package replicated microarray data clustering gene expression microarray data mining interval data analysis data representation modified k means clustering;interval data analysis;fiu	IntClust is a software package for clustering gene-expression data with repeated measurements based on interval data analysis. By utilizing interval data for representing replicated microarray data, IntClust is able to take into account the scopes where replicate microarray data are distributed instead of simple data points. The soft ware package offers several transformation models for interval data representations, supports different extended distance similarity/distance measures for interval data analysis, provides some variations of modified K-means clustering, and presents three popular clustering quality evaluation measures. Our experiments show that IntClust improves the clustering performance of gene-expression microarray data over traditional approaches. The software package is available at http://cadse24.cs.fiu.edu/IntClust	cluster analysis;data point;experiment;k-means clustering;microarray;self-replicating machine;warez	Wei Peng;Tao Li	2006	Sixth IEEE Symposium on BioInformatics and BioEngineering (BIBE'06)	10.1109/BIBE.2006.253322	microarray analysis techniques;gene chip analysis;repeated measures design;gene expression;data structure;computer science;bioinformatics;data science;data mining;external data representation;cluster analysis;data analysis;genetics;k-means clustering;clustering high-dimensional data	Comp.	4.82127271340048	-47.960749401382344	144380
756d2a99f11ea0bf5015b4ec19a990ac6d6b5232	multicore and gpu algorithms for nussinov rna folding	computers;rna folding;computational biology bioinformatics;rna;nucleic acid conformation;algorithms;combinatorial libraries;computational biology;computer appl in life sciences;microcomputers;microarrays;bioinformatics	One segment of a RNA sequence might be paired with another segment of the same RNA sequence due to the force of hydrogen bonds. This two-dimensional structure is called the RNA sequence's secondary structure. Several algorithms have been proposed to predict an RNA sequence's secondary structure. These algorithms are referred to as RNA folding algorithms. We develop cache efficient, multicore, and GPU algorithms for RNA folding using Nussinov's algorithm. Our cache efficient algorithm provides a speedup between 1.6 and 3.0 relative to a naive straightforward single core code. The multicore version of the cache efficient single core algorithm provides a speedup, relative to the naive single core algorithm, between 7.5 and 14.0 on a 6 core hyperthreaded CPU. Our GPU algorithm for the NVIDIA C2050 is up to 1582 times as fast as the naive single core algorithm and between 5.1 and 11.2 times as fast as the fastest previously known GPU algorithm for Nussinov RNA folding.	algorithm;cpu (central processing unit of computer system);cache;central processing unit;fastest;graphics processing unit;hydrogen;hyper-threading;multi-core processor;rna folding;speedup	Junjie Li;Sanjay Ranka;Sartaj Sahni	2014		10.1186/1471-2105-15-S8-S1	computational biology;biology;molecular biology;rna;dna microarray;computer science;bioinformatics;microcomputer;genetics	Theory	-1.9333681339626063	-51.981531429731106	144389
0f6f0adda413c049d6a55561b2a50dd3f9e27336	multiple sequence alignment using a glocsa guided genetic algorithm	score function;glocsa;genetic algorithm;genetic algorithms;autoadaptation;sequence alignment;multiple sequence alignment;biological applications	This paper introduces GLOCSA as a new scoring function to rate multiple sequence alignments. It is intended to be simple, considering the whole alignment at once and reflecting the parsimony of an alignment. Then, a GLOCSA Guided Genetic Algorithm is proposed in order to refine alignments previously generated by MUSCLE. The results so far are depicted in this paper.	genetic algorithm;maximum parsimony (phylogenetics);multiple sequence alignment;occam's razor;scoring functions for docking	Edgar David Arenas-Díaz;Helga Ochoterena-Booth;Katya Rodríguez-Vázquez	2008		10.1145/1388969.1388973	genetic algorithm;multiple sequence alignment;computer science;bioinformatics;artificial intelligence;machine learning;sequence alignment;alignment-free sequence analysis	Comp.	0.05899118644218098	-50.84114033922738	144508
6dcb04b3ad2a11b10f2b4199f5dffd0c2a67bc1f	dp-poirs: a diversified and personalized point-of-interest recommendation system		Diversity point-of-interest recommendation system benefits users to broaden their interests, access and discover new interest points. This paper describes a Diversified and Personalized Point-Of-Interest Recommendation System (DP-POIRS) by leveraging the geo-social relationships between POIs. The system consists of three components. The first component – geo-social distance measuring component is used to construct a correlation matrix to describe the geo-social distance between points of interests. The second component –point-of-interest partition component, divides the interest points into diverse clusters by using the spectral clustering algorithm over the correlation matrix. The third component –personalized sorting component, finds out the user's favorite interest points from each cluster, and then sorts them into a list of recommendation by the use of matrix factorization algorithms.	algorithm;cluster analysis;interest point detection;non-standard raid levels;point of interest;recommender system;sorting;spectral clustering	Xiangfu Meng;Yanhuan Tang;Xiaoyan Zhang	2017	2017 IEEE International Conference on Data Science and Advanced Analytics (DSAA)	10.1109/DSAA.2017.24	point of interest;spectral clustering;recommender system;covariance matrix;data modeling;matrix decomposition;cluster analysis;computer science;pattern recognition;artificial intelligence;sorting	DB	-2.176756037616869	-42.38084406711908	144518
1f695241be16150c3c32e738a7d0c916a9fd4a0d	gene expression studies with dgl global optimization for the molecular classification of cancer	microarray data;computer program;global solution;microarray gene expression;cancer;search space;orthogonal arrays;gene expression data;data mining;classification;colon cancer;gene expression;microarray analysis;cancer diagnosis;clinical practice;orthogonal array;global optimization;cancer classification;dna microarray;gene selection;bioinformatics	This paper combines a powerful algorithm, called Dongguang Li (DGL) global optimization, with the methods of cancer diagnosis through gene selection and microarray analysis. A generic approach to cancer classification based on gene expression monitoring by DNA microarrays is proposed and applied to two test cancer cases, colon and leukemia. The study attempts to analyze multiple sets of genes simultaneously, for an overall global solution to the gene’s joint discriminative ability in assigning tumors to known classes. With the workable concepts and methodologies described here an accurate classification of the type and seriousness of cancer can be made. Using the orthogonal arrays for sampling and a search space reduction process, a computer program has been written that can operate on a personal laptop computer. Both the colon cancer and the leukemia microarray data can be classified 100% correctly without previous knowledge of their classes. The classification processes are automated after the gene expression data being inputted. Instead of examining a single gene at a time, the DGL method can find the global optimum solutions and construct a multi-subsets pyramidal hierarchy class predictor containing up to 23 gene subsets based on a given microarray gene expression data collection within a period of several hours. An automatically derived class predictor makes the reliable cancer classification and accurate tumor diagnosis in clinical practice possible.	algorithm;colon classification;computable function;computer program;dna microarray;design of experiments;desktop computer;digital data;experiment;gene expression profiling;global optimization;image scanner;interdependence;kerrison predictor;laptop;li-chen wang;loss function;mathematical optimization;optimization problem;resultant;sampling (signal processing);signal-to-noise ratio;unbiased rendering	Dongguang Li	2011	Soft Comput.	10.1007/s00500-010-0542-x	microarray analysis techniques;computer science;bioinformatics;data science;data mining;orthogonal array;global optimization	Comp.	8.575739867943716	-50.659461284243285	144579
72f735ee1bf8a7ba5438125d3b1b5fd2895ea8be	generalization of c-means for identifying non-disjoint clusters with overlap regulation	overlap regulation;multi label data;overlapping clustering;clustering evaluation	Clustering is an unsupervised learning method that enables to fit structures in unlabeled data sets. Detecting overlapping structures is a specific challenge involving its own theoretical issues but offering relevant solutions for many application domains. This paper presents generalizations of the c-means algorithm allowing the parametrization of the overlap sizes. Two regulation principles are introduced, that aim to control the overlap shapes and sizes as regard to the number and the dispersal of the cluster concerned. The experiments performed on real world datasets show the efficiency of the proposed principles and especially the ability of the second one to build reliable overlaps with an easy tuning and whatever the requirement on the number of clusters.		Chiheb-Eddine Ben N'cir;Guillaume Cleuziou;Nadia Essoussi	2014	Pattern Recognition Letters	10.1016/j.patrec.2014.03.007	bioinformatics;machine learning;data mining;mathematics	Vision	5.538359382361516	-43.729378343611494	144866
671169da636b1cd18ec6082daa27b52a98ce4d45	a hybrid immune-estimation distribution of algorithm for mining thyroid gland data	type ii error;immune algorithms;genetic operator;immune algorithm;search space;type i error;computational intelligence;thyroid gland;data mining;journal article;estimation of distribution algorithm;classification rules;probability distribution;it adoption;genetic algorithm;genetic algorithms;article;estimation of distribution algorithms;hybrid algorithm	In this paper we combine the main concepts of estimation of distribution algorithms (EDAs) and immune algorithms (IAs) to be a hybrid algorithm called immune-estimation of distribution algorithms (IEDA). Both EDAs and IAs are extended from genetic algorithms (GAs). EDAs eliminate the genetic operation including crossover and mutation from the GAs and places more emphasis on the relation between gene loci. It adopts the distribution of selected individuals in search space and models the probability distributions to generate the next population. However, the primary gap of EDAs is lock of diversity between individuals. Hence, we introduce the IAs that is a new branch in computational intelligence. The main concepts of IAs are suppression and hypermutation that make the individuals be more diversity. Moreover, the primary gap of IAs is to pay no attention to the relation between individuals. Therefore, we combine the main concepts of two algorithms to improve the gaps each other. The classification risk of data mining is applied by this paper and compares the results between IEDA and general GAs in the experiments. We adopt the thyroid gland data set from UCI databases. Based on the obtained results, our research absolute is better than general GAs including accuracy, type I error and type II error. The results show not only the excellence of accuracy but also the robustness of the proposed algorithm. In this paper we have got high quality results which can be used as reference for hospital decision making and research workers. 2009 Elsevier Ltd. All rights reserved.	computational intelligence;crossover (genetic algorithm);data mining;database;display resolution;estimation of distribution algorithm;experiment;genetic algorithm;hybrid algorithm;zero suppression	Wei-Wen Chang;Wei-Chang Yeh;Pei-Chiao Huang	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.06.100	genetic algorithm;type i and type ii errors;estimation of distribution algorithm;computer science;bioinformatics;artificial intelligence;machine learning;computational intelligence;data mining;statistics	AI	5.157539656065601	-44.54094401738088	144881
abf0d2be4fc85ea1661074e2295abb9ad0d9fbf3	avoiding data overfitting in scientific discovery: experiments in functional genomics	functional genomics	Functional genomics is a typical scientific discovery domain characterized by a very large number of attributes (gen es) relative to the number of examples (observations). The danger of data overfitting is crucial in such domains. This work presents an approach which can help in avoiding data overfitting in supervi sed inductive learning of short rules that are appropriate for h uman interpretation. The approach is based on the subgroup discove ry rul learning framework, enhanced by methods of restricting the hypothesis search space by exploiting the relevancy of features th at enter the rule construction process as well as their combinations tha t form the rules. A multi-class functional genomics problem of classi fying fourteen cancer types based on more than 16000 gene expression va lues is used to illustrate the methodology.	feature vector;functional genomics;inductive reasoning;marginal model;overfitting;relevance;sed;test set	Dragan Gamberger;Nada Lavrac	2004				ML	8.501461511776311	-46.536467122980184	144979
beabe1e7ae29443974daaf9b2ca5f77339ba9541	a quality metric for multi-objective optimization based on hierarchical clustering techniques	unary diversity metric quality metric multiobjective optimization hierarchical clustering counting technique nondominated set iterative method hierarchical agglomerative clustering;hierarchical clustering;evolutionary computation sampling methods clustering methods algorithm design and analysis density measurement concurrent computing clustering algorithms terminology optimization methods design optimization;quality metric;pattern clustering;optimisation;multi objective optimization;set theory;hierarchical agglomerative clustering;set theory iterative methods optimisation pattern clustering;iterative methods;multiobjective optimization	This paper presents the Hierarchical Cluster Counting (HCC), a new quality metric for nondominated sets generated by multi-objective optimizers that is based on hierarchical clustering techniques. In the computation of the HCC, the samples in the estimate set are sequentially grouped into clusters. The nearest clusters in a given iteration are joined together until all the data is grouped in only one class. The distances of fusion used at each iteration of the hierarchical agglomerative clustering process are integrated into one value, which is the value of the HCC for that estimate set. The examples show that the HCC metric is able to evaluate both the extension and uniformity of the samples in the estimate set, making it suitable as a unary diversity metric for multiobjective optimization.	circuit complexity;cluster analysis;computation;hierarchical clustering;human-centered computing;iteration;mathematical optimization;multi-objective optimization;unary operation	Frederico G. Guimarães;Elizabeth F. Wanner;Ricardo H. C. Takahashi	2009	2009 IEEE Congress on Evolutionary Computation	10.1109/CEC.2009.4983362	correlation clustering;constrained clustering;mathematical optimization;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;canopy clustering algorithm;multi-objective optimization;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dendrogram;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	EDA	3.4523242532913656	-40.54596577495333	145033
93dd778e15e53df74f9e677ab7b6fabe126a5f23	ecological data analysis based on machine learning algorithms		Classification is an important supervised machine learning method, which is necessary and challenging issue for ecological research. It offers a way to classify a dataset into subsets that share common patterns. Notably, there are many classification algorithms to choose from, each making certain assumptions about the data and about how classification should be formed. In this paper, we applied eight machine learning classification algorithms such as Decision Trees, Random Forest, Artificial Neural Network, Support Vector Machine, Linear Discriminant Analysis, k-nearest neighbors, Logistic Regression and Naive Bayes on ecological data. The goal of this study is to compare different machine learning classification algorithms in ecological dataset. In this analysis we have checked the accuracy test among the algorithms. In our study we conclude that Linear Discriminant Analysis and k-nearest neighbors are the best methods among all other methods		Md.Siraj-Ud-Doula;Md. Ashad Alam	2018	CoRR			ML	10.006349955935718	-39.00479284157655	145147
cd557af759ba5131146acf781e1110d412e97fb1	the fitness-rough: a new attribute reduction method based on statistical and rough set theory	information loss;heuristic rule;rough set theory;attribute reduction;fitness degree;rough reducts	Attribute reduction has become an important pre-processing task to reduce the complexity of the data mining task. Rough reducts, statistical methods and correlation-based methods have gradually contributed towards improving attribute reduction techniques to a certain extent. Statistical methods are generally lower in computational complexity compared to the rough reducts and the correlation-based methods, but many have proven that the rough reducts method is significant in reducing important attributes without causing too much information loss. Correlation-based methods on the other hand evaluate features as a subset instead of individual attribute. In this paper, we propose a combination of statistical and rough set methods to reduce important attributes in a simpler way while maintaining a lesser degree of information loss from the raw data. The fitness-rough method (FsR) indicates important attributes from raw data and it is further simplified to a more compact information table. Besides that, we have also looked into the problem of information loss in this method. Ten UCI machine learning datasets were used as testing sets on the proposed method as compared to the classical rough reducts (RR) method, the statistical entropy (ENT) method and the correlation-based feature selection (CFS) method. Experimental results show that our method has performed comparatively well with higher reduction strength and smaller rules set against the benchmarking methods, especially in medium size datasets. However, the FsR method is basically less efficient when used on mix-mode and nominal datasets as the non-quantitative attributes involved in these datasets are normally pre-categorised.	rough set;set theory	Yun-Huoy Choo;Azuraliza Abu Bakar;Abdul Razak Hamdan	2008	Intell. Data Anal.		rough set;computer science;machine learning;pattern recognition;data mining;mathematics	EDA	7.599951072202646	-41.95857193374504	145185
807d6525208531e61b26f9bd4c4b19df4e7cf080	post-supervised fuzzy c-means classifier with hard clustering	cluster algorithm;pattern clustering;fuzzy c mean;search method;clustering algorithms error analysis iterative algorithms least squares methods benchmark testing clustering methods computational intelligence data mining data engineering search methods;fuzzy set theory;objective function;fuzzy clustering;number of clusters;pattern clustering fuzzy set theory pattern classification;pattern classification;classification error;error rate;fuzzy c means clustering;local minima;iterative reweighted least squares;generalized hard c means clustering algorithm post supervised fuzzy c means classifier hard clustering fuzzy c means clustering iteratively reweighted least square technique;fuzzy classifier	A fuzzy c-means classifier (FCMC) based on a generalized fuzzy c-means clustering with iteratively reweighted least square technique (IRLS-FCM) has been proposed. In this paper, we derive a generalized hard c-means (HCM-g) clustering algorithm by defuzzifying IRLS-FCM. Many hard clustering results are obtained from local minima of the HCM-g objective function. Although HCM-g is not a fuzzy clustering algorithm, it is applied to a fuzzy classifier and the best values of the parameters such as the fuzzifiers were chosen by using golden section search method. Whereas the goal of FCMC is to minimize classification error rate on unseen new test data, the proposed classifier aims at minimizing resubstitution error rate by using only a small number of clusters. The proposed classifier with two clusters for each class achieves low resubstitution error rate on several benchmark data sets	algorithm;benchmark (computing);cluster analysis;fuzzy clustering;fuzzy cognitive map;iteratively reweighted least squares;loss function;maxima and minima;optimization problem;test data	Hidetomo Ichihashi;Katsuhiro Honda;Naho Kuwamoto;Takao Hattori	2007	2007 IEEE Symposium on Computational Intelligence and Data Mining	10.1109/CIDM.2007.368928	correlation clustering;fuzzy clustering;word error rate;computer science;machine learning;maxima and minima;pattern recognition;data mining;mathematics;fuzzy set	AI	3.7393197840543992	-39.96508107204478	145436
1c76b1b302188eefdecebc7c75454a546e0c9f7d	cache-oblivious parallel simd viterbi decoding for sequence search in hmmer	databases genetic;computational biology bioinformatics;algorithms;sequence analysis;humans;sequence alignment;combinatorial libraries;computational biology;computer appl in life sciences;markov chains;databases protein;microarrays;bioinformatics	HMMER is a commonly used bioinformatics tool based on Hidden Markov Models (HMMs) to analyze and process biological sequences. One of its main homology engines is based on the Viterbi decoding algorithm, which was already highly parallelized and optimized using Farrar’s striped processing pattern with Intel SSE2 instruction set extension. A new SIMD vectorization of the Viterbi decoding algorithm is proposed, based on an SSE2 inter-task parallelization approach similar to the DNA alignment algorithm proposed by Rognes. Besides this alternative vectorization scheme, the proposed implementation also introduces a new partitioning of the Markov model that allows a significantly more efficient exploitation of the cache locality. Such optimization, together with an improved loading of the emission scores, allows the achievement of a constant processing throughput, regardless of the innermost-cache size and of the dimension of the considered model. The proposed optimized vectorization of the Viterbi decoding algorithm was extensively evaluated and compared with the HMMER3 decoder to process DNA and protein datasets, proving to be a rather competitive alternative implementation. Being always faster than the already highly optimized ViterbiFilter implementation of HMMER3, the proposed Cache-Oblivious Parallel SIMD Viterbi (COPS) implementation provides a constant throughput and offers a processing speedup as high as two times faster, depending on the model’s size.	automatic vectorization;bioinformatics;cache valley virus ab:titr:pt:ser:qn:neut;cache-oblivious algorithm;decoder device component;hmmer;hidden markov model;homologous gene;homology (biology);locality of reference;markov chain;mathematical optimization;parallel computing;simd;sse2;sequence alignment;speedup;task parallelism;throughput;twice (numerical qualifier);viterbi algorithm;viterbi decoder	Miguel Ferreira;Nuno Roma;Luís M. S. Russo	2013		10.1186/1471-2105-15-165	biology;markov chain;dna microarray;computer science;bioinformatics;theoretical computer science;machine learning;sequence analysis;sequence alignment	HPC	-2.1272307858653963	-51.73867760108053	145679
4b33646e783f2a400337c092550a1fe622ad9ee3	ica-based clustering of genes from microarray expression data	expression pattern;statistical significance;independent component analysis;mixture model;dna microarray data;biological process	We propose an unsupervised methodology using independent component analysis (ICA) to cluster genes from DNA microarray data. Based on an ICA mixture model of genomic expression patterns, linear and nonlinear ICA finds components that are specific to certain biological processes. Genes that exhibit significant up-regulation or down-regulation within each component are grouped into clusters. We test the statistical significance of enrichment of gene annotations within each cluster. ICA-based clustering outperformed other leading methods in constructing functionally coherent clusters on various datasets. This result supports our model of genomic expression data as composite effect of independent biological processes. Comparison of clustering performance among various ICA algorithms including a kernel-based nonlinear ICA algorithm shows that nonlinear ICA performed the best for small datasets and natural-gradient maximization-likelihood worked well for all the datasets.	cluster analysis;coherence (physics);dna microarray;expectation–maximization algorithm;gene ontology term enrichment;gradient;independent computing architecture;independent component analysis;information geometry;mixture model;nonlinear system	Su-In Lee;Serafim Batzoglou	2003			independent component analysis;computer science;bioinformatics;machine learning;pattern recognition;mixture model;data mining;statistical significance;biological process;statistics	ML	6.619597197008667	-50.080840237977284	146030
a45eef28ff0bf117440661524fac4d79b55326f9	computational problems in perfect phylogeny haplotyping: xor-genotypes and tag snps	genetique;methode diviser pour regner;grado libertad;theorie type;phylogeny;genetica;genotype;phylogenese;degree of freedom;temps lineaire;metodo dividir para vencer;tipo dato;data type;tiempo lineal;algoritmo genetico;genetics;combinatorial problem;probleme combinatoire;tipificacion;problema combinatorio;typing;pattern matching;polymorphism;perfect phylogeny haplotyping;type theory;filogenesis;linear time;divide and conquer method;typage;algorithme genetique;genetic algorithm;polymorphisme;polimorfismo;concordance forme;type donnee;freedom degree;divide and conquer;genotipo;degre liberte	The perfect phylogeny model for haplotype evolution has been successfully applied to haplotype resolution from genotype data. In this study we explore the application of the perfect phylogeny model to other problems in the design and analysis of genetic studies. We consider a novel type of data, xorgenotypes, which distinguish heterozygote from homozygote sites but do not identify the homozygote alleles. We show how to resolve xor-genotypes under perfect phylogeny model, and study the degrees of freedom in such resolutions. Interestingly, given xor-genotypes that produce a single possible resolution, we show that the full genotype of at most three individuals suffice in order to determine all haplotypes across the phylogeny. Our experiments with xorgenotyping data indicate that the approach requires a number of individuals only slightly larger than full genotyping, at a potentially reduced typing cost. We also consider selection of minimum-cost sets of tag SNPs, i.e., polymorphisms whose alleles suffice to recover the haplotype diversity. We show that this problem lends itself to divide-and-conquer linear-time solution. Finally, we study genotype tags, i.e., genotype calls that suffice to recover the alleles of all other SNPs. Since most genetic studies are genotype-based, such tags are more relevant in such studies than the haplotype tags. We show that under the perfect phylogeny model a SNP subset of haplotype tags, as it is usually defined, tags the haplotypes by genotype calls as well.	computation;exclusive or;experiment;phylogenetics;time complexity	Tamar Barzuza;Jacques S. Beckmann;Ron Shamir;Itsik Pe'er	2004		10.1007/978-3-540-27801-6_2	time complexity;polymorphism;divide and conquer algorithms;genetic algorithm;haplotype estimation;data type;computer science;bioinformatics;pattern matching;genotype;degrees of freedom;programming language;type theory;phylogenetics	Comp.	1.0687660855314174	-51.41591974530638	146201
addc23b8f93c2b657f77f7061c304a63de8b288d	analysis of consensus partition in cluster ensemble	pattern clustering;generic model;data mining;pattern clustering data mining;clustering algorithms partitioning algorithms voting algorithm design and analysis computer science stochastic processes robustness mutual information labeling data mining;mean partition consensus partition cluster ensemble artificial data set real world data set supervised classifier systems convergence properties unsupervised clustering ensembles stochastic partition generation model relabeling function consensus function plurality voting	"""In combination of multiple partitions, one is usually interested in deriving a consensus solution with a quality better than that of given partitions. Several recent studies have empirically demonstrated improved accuracy of clustering ensembles on a number of artificial and real-world data sets. Unlike certain multiple supervised classifier systems, convergence properties of unsupervised clustering ensembles remain unknown for conventional combination schemes. In this paper, we present formal arguments on the effectiveness of cluster ensemble from two perspectives. The first is based on a stochastic partition generation model related to re-labeling and consensus function with plurality voting. The second is to study the property of the """"mean"""" partition of an ensemble with respect to a metric on the space of all possible partitions. In both the cases, the consensus solution can be shown to converge to a true underlying clustering solution as the number of partitions in the ensemble increases. This paper provides a rigorous justification for the use of cluster ensemble."""	cluster analysis;consensus (computer science);converge;machine learning;rate of convergence;software metric;solaris cluster	Alexander P. Topchy;Martin H. C. Law;Anil K. Jain;Ana L. N. Fred	2004	Fourth IEEE International Conference on Data Mining (ICDM'04)	10.1109/ICDM.2004.10100	correlation clustering;constrained clustering;k-medians clustering;computer science;machine learning;consensus clustering;pattern recognition;data mining;mathematics;cluster analysis;k-means clustering	DB	3.0706898142145738	-40.42404463974061	146450
42ddead8411650ab6632b222cdb4aa823ab0ef13	automatic elastic net clustering algorithm	clustering algorithms classification algorithms accuracy indexes algorithm design and analysis traveling salesman problems approximation algorithms;nonlinearly separable data partitioning automatic elastic net clustering algorithm;cluster validity index elastic net algorithm clustering algorithm;pattern clustering optimisation	Clustering has always been playing a vital role in many different disciplines because it is an important tool for analyzing a set of unknown input patterns. However, some important issues related to clustering, such as automatically determining the number of clusters and partitioning non-linearly separable data, are never fully solved even though many researchers work on this subject for a long time. As such, a novel method based on the so called elastic net clustering algorithm is presented in this paper to deal with exactly the two issues: partitioning non-linearly separable data and automatically determining the number of clusters. To evaluate the performance of the proposed algorithm, several well-known datasets are used. The experimental results show that not only can the proposed algorithm find the appropriate number of clusters, but it can also provide a higher accuracy rate than all the other methods compared in this study for most datasets.	algorithm;cluster analysis;elastic map;elastic net regularization;evaluation function;linear separability;netpbm format	Chun-Wei Tsai;Tsung-Hsien Lin;Ming-Chao Chiang	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974347	correlation clustering;mathematical optimization;determining the number of clusters in a data set;data stream clustering;subclu;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;mathematics;cluster analysis;linde–buzo–gray algorithm;k-medoids;dbscan;biclustering;affinity propagation;clustering high-dimensional data	EDA	3.2632703987222573	-41.42895359212228	146481
2f1bec4220bd4d1a63a3afccfe0bf4769ecba92f	recognizing 100 speakers using homologous naive bayes	conjunto independiente;processus gauss;independent set;reconocimiento palabra;naive bayes;prior information;naive bayesian classifier;classification;speaker recognition;feature vector;gaussian mixture model;ensemble independant;informacion a priori;reconnaissance caractere;reconnaissance locuteur;speech recognition;code size;teoria mezcla;gaussian process;reconnaissance parole;classification accuracy;proceso gauss;mixture theory;theorie melange;character recognition;information a priori;clasificacion;reconocimiento caracter	"""This paper presents an extension of the naive Bayesian classifier, called """"homologous naive Bayes (HNB),"""" which is applied to the problem of text-independent, close-set speaker recognition. Unlike the standard naive Bayes, HNB can take advantage of the prior information that a sequence of input feature vectors belongs to the same unknown class. We refer to such a sequence a homologous set, which is naturally available in speaker recognition. We empirically compare HNB with the Gaussian mixture model (GMM), the most widely used approach to speaker recognition. Results show that, in spite of its simplisity, HNB can achieve comparable classification accuracies for up to a hundred speakers while taking much less resources in terms of time and code size for both training and classification."""	homology (biology);naive bayes classifier	Hung-Ju Huang;Chun-Nan Hsu	2002		10.1007/3-540-45683-X_43	speaker recognition;naive bayes classifier;speech recognition;independent set;feature vector;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;mixture model;gaussian process;mathematics;statistics	ML	6.983675256738856	-46.27179407943276	146557
561434ee61ffac7513529f8d935af6516e3ceb48	original synteny	original synteny	Abst rac t . The inference of genome rearrangement requires detailed gene maps of related species. For most multichromosomal species, however, knowledge of chromosomal assignment of genes outstrips mapping data. Comparison of these species is thus a question of comparing sets of syntenic genes, without any gene order or gene orientation information. Given synteny data from present-day species, can we infer the synteny sets of ancestor species? How many chromosomes did these ancestors possess, and what genes were on each one? We first study the problem of calculating a syntenic edit distance between two genomes, based o n reciprocal translocation, chromosome fusion and fission. This distance is then used in the analysis of the median problem for synteny, and hence for a prelimary approach to phylogenetic inference of synteny.	computational phylogenetics;graph edit distance;map;oracle rac;singlet fission;synteny	Vincent Ferretti;Joseph H. Nadeau;David Sankoff	1996		10.1007/3-540-61258-0_13		Comp.	0.839667779952563	-51.18087268694074	146956
d344c6c3c51bb71f53eae49acbf2d74e9baf1ff2	feature subset selection using adaptive differential evolution: an application to banking		In this paper, we developed a feature subset selection method by employing Adaptive Differential Evolution as a wrapper. The proposed wrapper utilizes four independent classifiers namely Logistic Regression, Probabilistic Neural Network, Naive Bayes and Support Vector Machine. We employed the Matthews Correlation Coefficient (MCC) as the fitness function or evaluation measure. In order to demonstrate the efficacy of the proposed method, we tested on three datasets, of which two are related to credit scoring and one is related to financial statement fraud. Our proposed method yielded better results than other standard methods in the literature as well as Differential Evolution. We also performed a statistical significance test i.e. t-test at 1% level of significance, which infers that some of the proposed wrappers are statistically one and the same.		Gutha Jaya Krishna;Vadlamani Ravi	2019		10.1145/3297001.3297021	differential evolution;support vector machine;naive bayes classifier;machine learning;logistic regression;probabilistic neural network;artificial intelligence;fitness function;computer science;matthews correlation coefficient	ML	9.52853164039743	-41.38816625086575	147047
7cfc90f228d0ac1ae54cdd5a6306a67918809804	fuzzy–rough sets for information measures and selection of relevant genes from microarray data	microarray data;histograms;biology computing;gene expression entropy mutual information density functional theory density measurement fuzzy set theory rough sets histograms uncertainty fuzzy reasoning;fuzzy reasoning;continuous gene expression values;high dimensionality;information measures;support vector machines;density measurement;uncertainty;algorithms artificial intelligence computer simulation decision support techniques fuzzy logic gene expression profiling models theoretical oligonucleotide array sequence analysis pattern recognition automated;rough set theory;fuzzy equivalence partition matrix;matrix algebra;classification;fuzzy set theory;genetics;gene expression;density functional theory;microarray analysis;prediction accuracy;rough sets;mutual information;entropy;fuzzy rough sets;support vector machine;rough set;relevant genes;gene selection;support vector machines biology computing fuzzy set theory genetics matrix algebra rough set theory;density functional;nonredundant genes;rough sets classification gene selection information measures microarray analysis;support vector machine fuzzy rough sets information measures relevant genes microarray data nonredundant genes continuous gene expression values fuzzy equivalence partition matrix	Several information measures such as entropy, mutual information, and f-information have been shown to be successful for selecting a set of relevant and nonredundant genes from a high-dimensional microarray data set. However, for continuous gene expression values, it is very difficult to find the true density functions and to perform the integrations required to compute different information measures. In this regard, the concept of the fuzzy equivalence partition matrix is presented to approximate the true marginal and joint distributions of continuous gene expression values. The fuzzy equivalence partition matrix is based on the theory of fuzzy-rough sets, where each row of the matrix represents a fuzzy equivalence partition that can automatically be derived from the given expression values. The performance of the proposed approach is compared with that of existing approaches using the class separability index and the predictive accuracy of the support vector machine. An important finding, however, is that the proposed approach is shown to be effective for selecting relevant and nonredundant continuous-valued genes from microarray data.	approximation algorithm;equivalence partitioning;fuzzy logic;gene expression;genetic selection;linear separability;marginal model;microarray;mutual information;rough set;support vector machine;the matrix;turing completeness	Pradipta Maji;Sankar K. Pal	2010	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2009.2028433	rough set;computer science;machine learning;pattern recognition;data mining;mathematics	ML	7.416489960200124	-48.223936520929755	147174
f162eb62edf021cb0d4ebaa03c9df4740d8b2fc9	v3coca: an effective clustering algorithm for complicated objects and its application in breast cancer research and diagnosis	cluster algorithm;clustering algorithm;computer aided diagnosis;complicated objects;breast cancer	Abstract In breast cancer studies, researchers often use clustering algorithms to investigate similarity/dissimilarity among different cancer cases. The clustering algorithm design becomes a key factor to provide intrinsic disease information. However, the traditional algorithms do not meet the latest multiple requirements simultaneously for breast cancer objects. The Variable parameters, Variable densities, Variable weights, and Complicated Objects Clustering Algorithm (V3COCA) presented in this paper can handle these problems very well. The V3COCA (1) enables alternative inputs of none or a series of objects for disease research and computer aided diagnosis; (2) proposes an automatic parameter calculation strategy to create clusters with different densities; (3) enables noises recognition, and generates arbitrary shaped clusters; and (4) defines a flexibly weighted distance for measuring the dissimilarity between two complicated medical objects, which emphasizes certain medically concerned issues in the objects. The experimental results with 10,000 patient cases from SEER database show that V3COCA can not only meet the various requirements of complicated objects clustering, but also be as efficient as the traditional clustering algorithms.	categorization;cluster analysis;dbscan;experiment;k-means clustering;numerical analysis;optics algorithm;requirement;run time (program lifecycle phase);time complexity;weight function	Kun Wang;Zhihui Du;Yinong Chen;Sanli Li	2009	Simulation Modelling Practice and Theory	10.1016/j.simpat.2008.10.005	correlation clustering;constrained clustering;simulation;fuzzy clustering;flame clustering;computer science;artificial intelligence;theoretical computer science;canopy clustering algorithm;breast cancer;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis	AI	3.140684917519954	-41.270528672296706	147182
dfa7d83d6ee5bcd4c9c598c48d9cbcf3941cf5db	a novel combined ica and clustering technique for the classification of gene expression data	ica pre processing tool combined ica clustering technique gene expression data blind classification biologically meaningful groups independent component analysis gene expression profile;pattern clustering;expression profile;gene expression data;independent component analysis;genetics;conference contribution;domain knowledge;genetics medical signal processing independent component analysis pattern classification pattern clustering;pattern classification;medical signal processing;independent component analysis gene expression	This study presents an effective method of blindly classifying large amounts of gene expression data into biologically meaningful groups using a combination of independent component analysis (ICA) and clustering techniques. Specifically, we show that the genes can be classified blindly into several groups based solely on their expression profiles. These groups have a very close correspondence with benchmarks obtained by studies using domain knowledge. These results suggest that ICA can be a very useful pre-processing tool in blind gene classification, rather than using the resulting sources as the final model profiles.		Amrish Kapoor;Thomas Bowles;Jonathon A. Chambers	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1416380	independent component analysis;computer science;bioinformatics;pattern recognition;data mining;domain knowledge	Visualization	6.981715155857255	-49.292532936263946	147688
4ea5db4094e82a7a9f66d99c370d5baa95a26192	a data-driven functional projection approach for the selection of feature ranges in spectra with ica or cluster analysis	institutional repositories;fedora;features extraction;functional projection;infrared spectra;independent component analysis;vital;variable selection;cluster analysis;clustering;feature extraction;vtls;ils	Prediction problems from spectra are largely encountered in chemometry. In addition to accurate predictions, it is often needed to extract information about which wavelengths in the spectra contribute in an effective way to the quality of the prediction. This implies to select wavelengths (or wavelength intervals), a problem associated to variable selection. In this paper, it is shown how this problem may be tackled in the specific case of smooth (for example infrared) spectra. The functional character of the spectra (their smoothness) is taken into account through a functional variable projection procedure. Contrarily to standard approaches, the projection is performed on a basis that is driven by the spectra themselves, in order to best fit their characteristics. The methodology is illustrated by two examples of functional projection, using Independent Component Analysis and functional variable clustering, respectively. The performances on two standard infrared spectra benchmarks are	b-spline;benchmark (computing);cluster analysis;computation;curse of dimensionality;curve fitting;database;experiment;feature selection;functional derivative;granular computing;independent computing architecture;independent component analysis;mutual information;nonlinear system;overfitting;performance;point of view (computer hardware company);preprocessor;spectral method	Catherine Krier;Fabrice Rossi;Damien François;Michel Verleysen	2007	CoRR	10.1016/j.chemolab.2007.09.004	infrared spectroscopy;computer science;machine learning;pattern recognition;mathematics;cluster analysis;feature selection;statistics	Vision	9.677096877507724	-48.0840821574186	148105
475f1b7d3cd1ecc56eb178ad92444738d1648354	selection of clusters number and features subset during a two-levels clustering task	feature selection;mod el selection;self-organizing maps;clustering;data mining;indexation;k means	Simultaneous selection of the number of clusters and of a relevant subset of features is part of data mining challenges. A new approach is proposed to address this difficult issue. It takes benefits of both two-levels clustering approaches and wrapper features selection algorithms. On the one hands, the former enhances the robustness to outliers and to reduce the running time of the algorithm. On the other hands, wrapper features selection (FS) approache s are known to given better results than filter FS methods because the algorithm that uses the data is taken into account. First, a Self-Organizing Maps (SOM), trained using the original data sets, is clustered using k-means and the Davies-Bouldin index to determinate the best number of a clusters. Then, an individual pertinence measure guides the backward elimination procedure and the feature mutual pertinence is measure using a collective pertinence based on the quality of the clustering.	cluster analysis;data mining;davies–bouldin index;k-means clustering;organizing (structure);property (philosophy);relevance;selection algorithm;stepwise regression;time complexity;unsupervised learning	Sébastien Guérif;Younès Bennani	2006			robustness (computer science);machine learning;artificial intelligence;outlier;feature selection;correlation clustering;computer science;cluster analysis;data set;indexation;k-means clustering;pattern recognition	ML	1.6571267063944042	-41.44903969780081	148426
e4d78440b2e19bda2dcc1931115ef283c0c8ebfc	efficient algorithms for local density based anomaly detection		Anomaly detection is a crucial problem in the field of data mining. However, prevailing anomaly detection algorithms are serial in nature which fail to handle huge volume of data. In this paper, we propose two parallel local density based algorithms namely, MapReduce based Local Outlier Factor (MRLOF) and Spark based Local Outlier Factor (SLOF). The proposed algorithms have time complexity of O(N) for each. This is an improvement over the Simplified LOF (Local Outlier Factor) which has time complexity of ( O(textit{N}^{2}) ), where N is the data size. We conducted extensive experiments with MRLOF and SLOF on various real life and synthetic datasets. The proposed algorithms are shown to outperform the serial Simplified LOF.	algorithm;anomaly detection	Ankita Sinha;Prasanta K. Jana	2018		10.1007/978-3-319-72344-0_30	time complexity;local outlier factor;anomaly detection;spark (mathematics);big data;computer science;algorithm	ML	-3.6659283911216876	-39.552178846542205	148433
166f5b460afdd5a4a9b25ede2996d2e5ae30f7cb	set covering submodular maximization: an optimal algorithm for data mining in bioinformatics and medical informatics	set covering causal model;protein functional annotation;medical decision making;data mining;submodular function;medical informatic;optimal algorithm;set cover	In this paper we show how several problems in different areas of data mining and knowledge discovery can be viewed as finding the optimal covering of a finite set. Many such problems arise in biomedical and bioinformatics research. For example, protein functional annotation based on sequence information is an ubiquitous bioinformatics problem. It consists of finding a set of homolog (high similarity) sequences of known function to a given amino acid sequence of unknown function from the various annotated sequence data bases. These can then be used as clues in suggesting further experimental analysis of the new protein. In the present paper we show that these optimization problems can be stated as maximizations of submodular functions on the set of candidate subsets -- a generalization that may be especially useful when conclusions from data mining need to be interpreted by human experts. This is common to a number of examples we consider below: diagnostic hypothesis generation, logical methods of data analysis, conceptual clustering, and proteins functional annotations.	bioinformatics;data mining;expectation–maximization algorithm;informatics;submodular set function	Alexander Genkin;Casimir A. Kulikowski;Ilya B. Muchnik	2002	Journal of Intelligent and Fuzzy Systems		mathematical optimization;computer science;bioinformatics;submodular set function;machine learning;data mining;mathematics;set cover problem	ML	5.26407809026717	-49.02998919663094	148482
4a581b3c357b38f3b04f9980568f4651c4550590	a supervised learning approach to the ensemble clustering of genes	supervised learning;ensemble;clustering;microarray;random subspaces	High-throughput techniques have become a primary approach to gathering biological data. These data can be used to explore relationships between genes and guide development of drugs and other research. However, the deluge of data contains an overwhelming amount of unknown information about the organism under study. Therefore, clustering is a common first step in the exploratory analysis of high-throughput biological data. We present a supervised learning approach to clustering that utilises known gene-gene interaction data to improve results for already commonly used clustering techniques. The approach creates an ensemble similarity measure that can be used as input to any clustering technique and provides results with increased biological significance while not altering the clustering method.	cluster analysis;high-throughput computing;similarity measure;supervised learning;throughput;statistical cluster	Andrew K. Rider;Geoffrey Siwo;Scott J. Emrich;Michael T. Ferdig;Nitesh V. Chawla	2014	International journal of data mining and bioinformatics	10.1504/IJDMB.2014.059062	ensembl;correlation clustering;fuzzy clustering;flame clustering;computer science;bioinformatics;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;microarray;data mining;ensemble learning;cluster analysis;supervised learning;brown clustering;biclustering;clustering high-dimensional data;conceptual clustering	Comp.	5.986527703237037	-49.43349098367064	148762
f866aac87c8dc7a973a41da0415048c937a39e27	a probabilistic approach for constrained clustering with topological map	gtm;soft constraints;probabilistic approach;constrained clustering;clustering method	"""This paper describes a new topological map dedicated to clustering under probabilistic constraints. In general, traditional clustering is used in an unsupervised manner. However, in some cases, background information about the problem domain is available or imposed in the form of constraints in addition to data instances. In this context, we modify the popular GTM algorithm to take these """"soft"""" constraints into account during the construction of the topology. We present experiments on synthetic known databases with artificial generated constraints for comparison with both GTM and another constrained clustering methods."""	constrained clustering	Khalid Benabdeslem;Jihène Snoussi	2009		10.1007/978-3-642-03070-3_31	correlation clustering;constrained clustering;mathematical optimization;data stream clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;mathematics;cluster analysis;clustering high-dimensional data;conceptual clustering	ML	2.287678231096104	-41.582172602978936	148861
00000505c94412fa356e2a715d7af13317baf212	a semi-supervised incremental clustering algorithm for streaming data	constraint based clustering;semi supervised learning;stream clustering	Nowadays many applications need to deal with  evolving data streams  . In this work, we propose an incremental clustering approach for the exploitation of user constraints on data streams. Conventional constraints do not make sense on streaming data, so we extend the classic notion of constraint set into a  constraint stream  . We propose methods for using the constraint stream as data items are forgotten or new items arrive. Also we present an on-line clustering approach for the cost-based enforcement of the constraints during cluster adaptation on evolving data streams. Our method introduces the concept of multi-clusters (m-clusters) to capture arbitrarily shaped clusters. An m-cluster consists of multiple dense overlapping regions, named s-clusters, each of which can be efficiently represented by a single point. Also it proposes the definition of outliers clusters in order to handle outliers while it provides methods to observe changes in structure of clusters as data evolves.	algorithm;cluster analysis;stream (computing)	Maria Halkidi;Myra Spiliopoulou;Aikaterini Pavlou	2012		10.1007/978-3-642-30217-6_48	semi-supervised learning;correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;computer science;machine learning;cure data clustering algorithm;data mining;database;cluster analysis	ML	-0.88552895749742	-39.04010399080412	148891
020b3070366548c5afa7443fb2e59b86a8492697	the l 1/2 regularization network cox model for analysis of genomic data	cox proportional hazards model;l(1/2) penalty;network	Abstract Methods based on a L 1/2 penalty have been utilized to solve the variable selection problem associated with the Cox proportional hazards model. One limitation of the existing methods for survival analysis is that these ignore the regulatory networks and pathways information. To merge prior pathway information into the analysis of genomic data, we proposed a network-based regularization method for the L 1/2 penalty and applied it to high-dimensional survival analysis data. This method used a L 1/2 regularized solver and network that penalizes a Cox proportional hazards model with respect to the sparsity of the regression and the smoothness between the coefficients in a given network. Based on the limited simulation studies and real breast cancer gene expression datasets, the experimental results showed that our method achieves a higher predictive accuracy than previous methods. Even though fewer genes were selected compared to those using previous methods, results showed stronger associations with cancer. The results of the analysis were also validated using GeneCards.		Hong-kun Jiang;Yong Liang	2018	Computers in biology and medicine	10.1016/j.compbiomed.2018.07.009	survival analysis;pattern recognition;artificial intelligence;proportional hazards model;merge (version control);computer science;genecards;feature selection;regularization (mathematics);solver	Comp.	7.755701476435726	-52.00255505677446	148919
cda53c669d3700b54f7011c2e502f87f542b4a30	reconstruction of clonal trees and tumor composition from multi-sample sequencing data	high throughput nucleotide sequencing;sequence analysis dna;gene frequency;models statistical;algorithms;humans;clonal evolution;neoplasms;mutation	MOTIVATION DNA sequencing of multiple samples from the same tumor provides data to analyze the process of clonal evolution in the population of cells that give rise to a tumor.   RESULTS We formalize the problem of reconstructing the clonal evolution of a tumor using single-nucleotide mutations as the variant allele frequency (VAF) factorization problem. We derive a combinatorial characterization of the solutions to this problem and show that the problem is NP-complete. We derive an integer linear programming solution to the VAF factorization problem in the case of error-free data and extend this solution to real data with a probabilistic model for errors. The resulting AncesTree algorithm is better able to identify ancestral relationships between individual mutations than existing approaches, particularly in ultra-deep sequencing data when high read counts for mutations yield high confidence VAFs.   AVAILABILITY AND IMPLEMENTATION An implementation of AncesTree is available at: http://compbio.cs.brown.edu/software.	angina pectoris, variant;clonal evolution;clone;deep sequencing;gene frequency;integer (number);integer programming;karp's 21 np-complete problems;linear programming;mutation;neoplasms;nucleotides;numerous;solutions;statistical model;trees (plant);algorithm	Mohammed El-Kebir;Layla Oesper;Hannah Acheson-Field;Benjamin J. Raphael	2015		10.1093/bioinformatics/btv261	somatic evolution in cancer;mutation;biology;bioinformatics;allele frequency;genetics	Comp.	1.4384715784915638	-51.51521569604451	149100
b3ce02f281977099e4696b8f4fbca9b1b178fd52	yinyang k-means: a drop-in replacement of the classic k-means with consistent speedup		This paper presents Yinyang K-means, a new algorithm for Kmeans clustering. By clustering the centers in the initial stage, it leverages efficiently maintained lower and upper bounds between a point and centers, it more effectively avoids unnecessary distance calculations than prior algorithms do. It significantly outperforms the classic K-means and the prior known alternatives (Elkan’s, Hamery’s and Drake’s accelerated algorithms) consistently across all experimented data sets, cluster numbers, and machine configurations. The consistent superior performance—plus its simiplicity, elastic control of space cost, and guarantee in producing the same clustering results as the standard K-means does—makes Yinyang K-means a drop-in replacement of the classic K-means with an order of magnitude higher performance.	algorithm;cluster analysis;computer cluster;drop-in replacement;k-means clustering;speedup	Yufei Ding;Yue Zhao;Xipeng Shen;Madan Musuvathi;Todd Mytkowicz	2015			mathematical optimization;computer science;theoretical computer science;algorithm	ML	-2.7527674959100508	-40.22672866018059	149216
613e53176ce969e4f871c3519ab81dae0beea72c	a rapid exact solution for the guided genome halving problem		\subsubsection*Background Genome rearrangements are large-scale evolutionary events that shuffle genomic architectures. Since genome rearrangements are rare, the number of events between two genomes is used in phylogenomic studies to measure the evolutionary distance between them. Such measurement is often based on the maximum parsimony assumption, implying that the evolutionary distance can be estimated as the minimum number of rearrangements between genomes. The maximum parsimony assumption enables addressing the ancestral genome reconstruction problem, which asks for reconstructing of ancestral genomes from the given extant genomes, by minimizing the total distance between genomes along the branches of the phylogenetic tree. The basic case of this problem with just three given genomes is known as the genome median problem (GMP), which asks for a single ancestral genome (\emphmedian genome ) at the minimum total distance from the given genomes. \emphWhole genome duplication (WGD) represents yet another type of dramatic evolutionary events, which simultaneously duplicate each chromosome of a genome. WGDs are known to have happened in the evolution of plants~\citeguyot2004ancestral. An analog of the GMP in presence of a WGD is known as the guided genome halving problem (GGHP). This problem is posed for input genomes A and B, where all genes in B are present in a single copy (\emphordinary genome ), while all genes in A are present in two copies (\emphduplicated genome ). The GGHP asks for an ordinary ancestral genome R that minimizes the total evolutionary distance between genomes A and $2R$ (genome resulted from the WGD of R) and between B and R. \vspace-0.5em \subsubsection*Methods A major tool for analysis of genome rearrangements is the breakpoint graph, which encodes gene adjacencies in different genomes by edges of different colors. A median genome corresponds to a certain optimal perfect matching in the breakpoint graph of the given genomes. While the GMP is NP-hard \citetannier2009multichromosomal, one of the prominent exact and practical solutions to the GMP is based on decomposition of the breakpoint graph intoadequate subgraphs ~\citexu2009fast, i.e., induced subgraphs where any optimal matching can be extended to an optimal matching in the whole graph. To handle genomes with duplicated genes, one has to generalize the notion of the breakpoint graph to the contracted breakpoint graph of the given genomes A and B. In the present study, we extend the adequate subgraph approach to the GGHP. \vspace-0.5em \subsubsection*Results We extended the notion of adequate subgraphs to contracted breakpoint graphs and identified all simple adequate subgraphs of order $2$ and $4$ (shown in Fig. \reffig:adequate_gghp ). This enables us to design an efficient divide-and-conquer algorithm for the GGHP. Our algorithm searches for adequate subgraphs in the given contracted breakpoint graph and combines optimal matchings in these subgraphs into an optimal matching (representing a solution to the GGHP) in the whole graph. \vspace-0.5em \subsubsection*Conclusion Our present study provides an exact fast algorithm for the GGHP. In future research, we plan to extend the notion of adequate subgraphs to other ancestral reconstruction problems with duplicated genomes, such as the guided genome aliquoting problem. \vspace-1em	algorithm;ancestral reconstruction;color;division by two;gnu multiple precision arithmetic library;genome-based peptide fingerprint scanning;induced subgraph;matching (graph theory);maximum parsimony (phylogenetics);optimal matching;phylogenetic tree;phylogenetics;reconstruction conjecture;yet another	Anton Nekhai;Maria Atamanova;Pavel Avdeyev;Max A. Alekseyev	2018		10.1145/3233547.3233659	matching (graph theory);phylogenetic tree;genome;bioinformatics;gene;optimal matching;maximum parsimony;comparative genomics;ancestral reconstruction;computer science	Theory	0.5317532688502643	-50.71424954548135	149536
1b1b08858952749fc084c6bc8bd8a5f7d4474ffd	multiclass molecular cancer classification by kernel subspace methods with effective kernel parameter selection	kernel methods;parameter selection;subspace method;microarray;cancer classification	Microarray techniques provide new insights into molecular classification of cancer types, which is critical for cancer treatments and diagnosis. Recently, an increasing number of supervised machine learning methods have been applied to cancer classification problems using gene expression data. Support vector machines (SVMs), in particular, have become one of the most effective and leading methods. However, there exist few studies on the application of other kernel methods in the literature. We apply a kernel subspace (KS) method to multiclass cancer classification problems, and assess its validity by comparing it with multiclass SVMs. Our comparative study using seven multiclass cancer datasets demonstrates that the KS method has high performance that is comparable to multiclass SVMs. Furthermore, we propose an effective criterion for kernel parameter selection, which is shown to be useful for the computation of the KS method.	acomys herpesvirus svms 226,222;apricot kernel oil;computation;dipodillus herpesvirus svms 065,081;effective method;elfacos ow 100;existential quantification;fo (complexity);fr 139317;fallopia multiflora plant;gene expression;genetic selection;k-nearest neighbors algorithm;kernel (operating system);kernel method;large;linux;machine learning;mathematical optimization;microarray;nl-complete;neoplasms;numerical aperture;optimization problem;population parameter;radial basis function kernel;sodium;status epilepticus;supervised learning;support vector machine;the matrix;unit per liter;windows legacy audio components	Satoshi Niijima;Satoru Kuhara	2005	Journal of bioinformatics and computational biology	10.1142/S0219720005001491	kernel method;radial basis function kernel;computer science;bioinformatics;machine learning;multiclass classification;pattern recognition;microarray;data mining;mathematics	ML	9.049964878321434	-48.494926439296606	149685
9c41a75630e9c2c5f1110d7c775867acaa2832aa	analyzing the similarity of samples and genes by mg-pcc algorithm, t-sne-ss and t-sne-sg maps	a-value;mg-pcc;pcc;t-sne-sgi;t-sne-ssp	For analyzing these gene expression data sets under different samples, clustering and visualizing samples and genes are important methods. However, it is difficult to integrate clustering and visualizing techniques when the similarities of samples and genes are defined by PCC(Person correlation coefficient) measure. Here, for rare samples of gene expression data sets, we use MG-PCC (mini-groups that are defined by PCC) algorithm to divide them into mini-groups, and use t-SNE-SSP maps to display these mini-groups, where the idea of MG-PCC algorithm is that the nearest neighbors should be in the same mini-groups, t-SNE-SSP map is selected from a series of t-SNE(t-statistic Stochastic Neighbor Embedding) maps of standardized samples, and these t-SNE maps have different perplexity parameter. Moreover, for PCC clusters of mass genes, they are displayed by t-SNE-SGI map, where t-SNE-SGI map is selected from a series of t-SNE maps of standardized genes, and these t-SNE maps have different initialization dimensions. Here, t-SNE-SSP and t-SNE-SGI maps are selected by A-value, where A-value is modeled from areas of clustering projections, and t-SNE-SSP and t-SNE-SGI maps are such t-SNE map that has the smallest A-value. From the analysis of cancer gene expression data sets, we demonstrate that MG-PCC algorithm is able to put tumor and normal samples into their respective mini-groups, and t-SNE-SSP(or t-SNE-SGI) maps are able to display the relationships between mini-groups(or PCC clusters) clearly. Furthermore, t-SNE-SS(m)(or t-SNE-SG(n)) maps are able to construct independent tree diagrams of the nearest sample(or gene) neighbors, where each tree diagram is corresponding to a mini-group of samples(or genes).		Xingang Jia;Qiuhong Han;Zuhong Lu	2018		10.1186/s12859-018-2495-5		Comp.	2.7577419890168864	-48.311953415579	149775
820155e3f63179e947dbf493831a9bff668387cb	homogeneity of cluster ensembles		The expectation and the mean of partitions generated by a cluster ensemble are not unique in general. This issue poses challenges in statistical inference and cluster stability. In this contribution, we state sufficient conditions for uniqueness of expectation and mean. The proposed conditions show that a unique mean is neither exceptional nor generic. To cope with this issue, we introduce homogeneity as a measure of how likely is a unique mean for a sample of partitions. We show that homogeneity is related to cluster stability. This result points to a possible conflict between cluster stability and diversity in consensus clustering. To assess homogeneity in a practical setting, we propose an efficient way to compute a lower bound of homogeneity. Empirical results using the k-means algorithm suggest that uniqueness of the mean partition is not exceptional for real-world data. Moreover, for samples of high homogeneity, uniqueness can be enforced by increasing the number of data points or by removing outlier partitions. In a broader context, this contribution can be placed as a further step towards a statistical theory of partitions.	algorithm;cluster analysis;computer cluster;consensus clustering;data point;k-means clustering	Brijnesh J. Jain	2016	CoRR		mathematical optimization;combinatorics;mathematics;statistics	ML	-0.35493546877176296	-42.52972068760201	149858
e72ed4c2ee115875126f2c54e645945f1b7571bc	an hybrid validity index for dynamic cut-off in hierarchical agglomerative clustering	trees mathematics learning artificial intelligence pattern clustering;indexes eigenvalues and eigenfunctions vegetation iris ionosphere vectors amplitude modulation;uci machine learning repository hybrid validity index dynamic cut off hierarchical agglomerative clustering partitioning unlabelled data sample cluster analysis true cluster hierarchical clustering technique clustering process global proximity local proximity hierarchical tree;hierarchical agglomerative clustering clustering validity index dynamic cut off true clusters	Clustering aims at partitioning unlabelled data samples into clusters so that the samples within a cluster are close to each other. One of the most challenging issues in cluster analysis is the determination of true clusters. This challenge becomes more intriguing in case of hierarchical clustering techniques that requires dynamic termination of the clustering process at true conception of clusters. In this paper, a dynamic cut-off for hierarchical agglomerative clustering has been proposed which automatically terminates the clustering process once optimal number of clusters are obtained. The cut-off is based on the computation of validity index to assess the global proximity and local proximity of the clusters at every level of the hierarchy. The decision of optimality is rendered during the construction of the hierarchical tree. This eliminates the need for the computation of the complete tree. Experimental analyses on real datasets from UCI machine learning repository demonstrate the efficiency of the proposed method in detecting true clusters.	cluster analysis;computation;hierarchical clustering;machine learning;sensor	S. M. BhargaviM.;Sahana D. Gowda	2014	2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2014.6968593	complete-linkage clustering;correlation clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;biclustering;dendrogram;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	DB	1.5223270148595167	-40.17082697823832	149940
0d26a0a65cc5b6dd96d6856af24d5eab7ec4c9db	classification error as a measure of gene relevance in cancer diagnosis	patient diagnosis;expression profile;cancer;genetics;colon cancer;cancer pathology diseases gene expression dna accuracy neoplasms colon genomics bioinformatics;gene relevance;dna microarray data classification error gene relevance cancer diagnosis;cancer diagnosis;prediction accuracy;pattern classification cancer genetics medical diagnostic computing patient diagnosis;pattern classification;classification error;dna microarray data;medical diagnostic computing;selection bias	One of the main problems in cancer diagnosis by using DNA microarray data is selecting genes relevant for the pathology by analyzing their expression profiles in tissues in two different phenotypical conditions. The question we pose is the following: how do we measure the relevance of a single gene in a given pathology? A gene is relevant for a particular disease if it is possible to correctly predict the occurrence of the pathology in new patients on the basis of expression level of this gene only. In other words, a gene is informative for the disease if its expression levels are useful for training a classifier able to generalize, that is, able to correctly predict the status of new patients. In this paper we present a selection bias free, statistically well founded method for finding relevant genes on the basis of their classification ability. We applied the method on a colon cancer data set and produced a list of relevant genes, ranked on the basis of their prediction accuracy. We found, out of more than 6500 available genes, 54 overexpressed in normal tissue and 77 overexpressed in tumor tissue having prediction accuracy greater than 70% with p-value p les 0.05.	colon classification;dna microarray;information;let expression;pose (computer vision);relevance;selection bias;statistical classification	Rosalia Maglietta;Annarita D'Addabbo;Ada Piepoli;Francesco Perri;Sabino Liuni;Graziano Pesole;Nicola Ancona	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.247364	selection bias;bioinformatics;data mining;cancer	Comp.	8.436147783176647	-50.640536079024194	150112
2e1a88bf49930c0f45ad892b21ca46b2ba677c4e	svd based feature selection and sample classification of proteomic data	proteomic data;high dimensionality;singular value decomposition;gene expression;svd;membranous nephropathy;feature selection;type 2 diabetes	Feature selection becomes a central task when ’signature’ profiles specific to a pathological status have to be extracted from high dimensional gene expression or proteomic data. In the present paper, we propose a feature selection method based on Singular Value Decomposition (SVD) and apply it to SELDI-TOF/MS proteomic data from a cohort of Type 2 Diabetics affected by Glomerulosclerosis and Membranous Nephropathy. We have selected a profile composed of 24 proteins that seems to be an effective signature for the pathology at hand, allowing to efficiently discriminate between the considered subtype of diabetes.	feature selection;proteomics;singular value decomposition	Annarita D'Addabbo;Massimo Papale;Salvatore Di Paolo;Simona Magaldi;Roberto Colella;Valentina d'Onofrio;Annamaria Di Palma;Elena Ranieri;Loreto Gesualdo;Nicola Ancona	2008		10.1007/978-3-540-85567-5_69	bioinformatics;pattern recognition;data mining;mathematics	ML	8.049461204828042	-49.976286587372506	150310
3da387a23507e7ad13243eaf64544576ef9deeac	improving the dynamic hierarchical compact clustering algorithm by using feature selection	hierarchical clustering;cluster algorithm;local features;text clustering;feature selection;quality measures	Feature selection has improved the performance of text clustering. In this paper, a local feature selection technique is incorporated in the dynamic hierarchical compact clustering algorithm to speed up the computation of similarities. We also present a quality measure to evaluate hierarchical clustering that considers the cost of finding the optimal cluster from the root. The experimental results on several benchmark text collections show that the proposed method is faster than the original algorithm while achieving approximately the same clustering quality.	algorithm;benchmark (computing);cluster analysis;computation;feature selection;hierarchical clustering;relevance;speedup	Reynaldo Gil-García;Aurora Pons-Porrata	2010		10.1007/978-3-642-16687-7_19	correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;feature selection;dbscan;biclustering;dendrogram;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	AI	0.9512211140928687	-41.42266576747683	150884
15bf3468d3e3445df701feacf2a344ed7e466810	incomplete lineage sorting: consistent phylogeny estimation from multiple loci	stochastic processes;topology;data structure;sorting;convergence;incomplete lineage sorting;population genetics;probability and statistics;history;voting;bioinformatics;phylogeny;glass;genetics;estimation theory;phylogenetics;bayesian methods	We introduce a simple computationally efficient algorithm for reconstructing phylogenies from multiple gene trees in the presence of incomplete lineage sorting, that is, when the topology of the gene trees may differ from that of the species tree. We show that our technique is statistically consistent under standard stochastic assumptions, that is, it returns the correct tree given sufficiently many unlinked loci. We also show that it can tolerate moderate estimation errors.	lineage (evolution);phylogenetics;sorting	Elchanan Mossel;Sébastien Roch	2010	IEEE/ACM Trans. Comput. Biology Bioinform.	10.1145/1719272.1719288	probability and statistics;biology;convergence;voting;bayesian probability;computer science;bioinformatics;sorting;mathematics;coalescent theory;glass;estimation theory;population genetics;genetics;statistics;phylogenetics	Networks	2.329745741407047	-51.79441567013374	151131
cd75402d3645571e90f36f82d3cdf81e3a6eb926	analysis of the influence of diversity in collaborative and multi-view clustering		Multi-source clustering is common data mining task the aim of which is to use several clustering algorithms to analyze different aspects of the same data. Well known applications of multi-source clustering include horizontal collaborative clustering and multi-view clustering, where several algorithms combine their strengths by exchanging information about their finding on local structures with a goal of mutual improvement. However, many of these proposed algorithms and statistical models lack the capability to detect weak collaborations that may prove detrimental to the global clustering process. In this article, we propose a weighing optimization method that will help detecting which algorithms should exchange their information based on the diversity between the different algorithms' solutions.	algorithm;cluster analysis;data mining;global variable;mathematical optimization;multi-source;sensor;statistical model	Jérémie Sublime;Basarab Matei;Pierre-Alexandre Murena	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966377	fuzzy clustering;artificial intelligence;correlation clustering;data stream clustering;flame clustering;machine learning;cluster analysis;computer science;data mining;canopy clustering algorithm;brown clustering;cure data clustering algorithm	ML	1.89144391676226	-42.705993512529474	151185
61e33fbf3a8fd53e55e1f83951e33b467cca70b9	clustering of interval data based on city-block distances	l 1 distance;interval data;dynamic clustering;adaptive distances;symbolic data analysis;clustering method;dynamic cluster algorithm	The recording of interval data has become a common practice with the recent advances in database technologies. This paper introduces clustering methods for interval data based on the dynamic cluster algorithm. Two methods are considered: one with adaptive distances and the other without.		Renata M. C. R. de Souza;Francisco de A. T. de Carvalho	2004	Pattern Recognition Letters	10.1016/j.patrec.2003.10.016	complete-linkage clustering;correlation clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;symbolic data analysis;cluster analysis;single-linkage clustering;k-medoids;clustering high-dimensional data	Vision	2.1438552288946204	-41.14218216133636	151359
7fd1f9b0bfbbc26529fe61d2060dd7dfef7012e4	an ilp solution for the gene duplication problem	optimal solution;genomics;performance guarantee;plants;phylogeny;genome plant;gene trees;computational biology bioinformatics;gene duplication;large scale;phylogenetic tree;programming linear;gene family;algorithms;combinatorial libraries;computer appl in life sciences;computer simulation;integer linear program;microarrays;bioinformatics	The gene duplication (GD) problem seeks a species tree that implies the fewest gene duplication events across a given collection of gene trees. Solving this problem makes it possible to use large gene families with complex histories of duplication and loss to infer phylogenetic trees. However, the GD problem is NP-hard, and therefore, most analyses use heuristics that lack any performance guarantee. We describe the first integer linear programming (ILP) formulation to solve instances of the gene duplication problem exactly. With simulations, we demonstrate that the ILP solution can solve problem instances with up to 14 taxa. Furthermore, we apply the new ILP solution to solve the gene duplication problem for the seed plant phylogeny using a 12-taxon, 6, 084-gene data set. The unique, optimal solution, which places Gnetales sister to the conifers, represents a new, large-scale genomic perspective on one of the most puzzling questions in plant systematics. Although the GD problem is NP-hard, our novel ILP solution for it can solve instances with data sets consisting of as many as 14 taxa and 1, 000 genes in a few hours. These are the largest instances that have been solved to optimally to date. Thus, this work can provide large-scale genomic perspectives on phylogenetic questions that previously could only be addressed by heuristic estimates.	classification;coniferophyta;estimated;gene duplication abnormality;gene family;gnetales;heuristics;inference;integer (number);integer programming;linear iga bullous dermatosis;linear programming;np-hardness;phylogenetic tree;phylogenetics;simulation;trees (plant);xiap gene	Wen-Chieh Chang;John Gordon Burleigh;David Fernández-Baca;Oliver Eulenstein	2011		10.1186/1471-2105-12-S1-S14	computer simulation;biology;genomics;phylogenetic tree;dna microarray;bioinformatics;gene family;genetics;gene duplication	Comp.	1.4598621086158061	-51.455249198569234	151753
cdd7d40a1f895c3265d75d91121c143b11a66126	credit risk evaluation modeling using evolutionary linear svm classifiers and sliding window approach	evaluation;particle swarm optimization;credit risk;analysis;support vector machines;bankruptcy;genetic algorithms	Abstract This paper presents a study on credit risk evaluation modeling using linear Support Vector Machines (SVM) classifiers, combined with evolutionary parameter selection using Genetic Algorithms and Particle Swarm Optimization, and sliding window approach. Discriminant analysis was applied for evaluation of financial instances and dynamic formation of bankruptcy classes. The possibilities of feature selection application were also researched by applying correlation-based feature subset evaluator. The research demonstrates a possibility to develop and apply an intelligent classifier based on original discriminant analysis method evaluation and shows that it might perform bankruptcy identification better than original model.		Paulius Danenas;Gintautas Garsva	2012		10.1016/j.procs.2012.04.145	machine learning;pattern recognition;data mining	Vision	9.409849226257842	-41.136102861521444	151762
e8ca53210e71ee30984f014f2fc2018a9f41486a	the multi-dimensional ensemble empirical mode decomposition method	empirical mode decomposition emd;minimal scale principle;multi dimensional;pseudo multi dimensional ensemble empirical mode decomposition;multi dimensional ensemble empirical mode decomposition;ensemble empirical mode decomposition eemd;empirical mode decomposition	A multi-dimensional ensemble empirical mode decomposition (MEEMD) for multidimensional data (such as images or solid with variable density) is proposed here. The decomposition is based on the applications of ensemble empirical mode decomposition (EEMD) to slices of data in each and every dimension involved. The final reconstruction of the corresponding intrinsic mode function (IMF) is based on a comparable minimal scale combination principle. For two-dimensional spatial data or images, f(x, y), we consider the data (or image) as a collection of one-dimensional series in both x-direction and y-direction. Each of the one-dimensional slices is decomposed through EEMD with the slice of the similar scale reconstructed in resulting two-dimensional pseudo-IMF-like components. This new two-dimensional data is further decomposed, but the data is considered as a collection of one-dimensional series in y-direction along locations in x-direction. In this way, we obtain a collection of two-dimensional components. These directly resulted components are further combined into a reduced set of final components based on a minimal-scale combination strategy. The approach for two-dimensional spatial data can be extended to multi-dimensional data. EEMD is applied in the first dimension, then in the second direction, and then in the third direction, etc., using the almost identical procedure as for the two-dimensional spatial data. A similar comparable minimal-scale combination strategy can be applied to combine all the directly resulted components into a small set of multi-dimensional final components. For multi-dimensional temporal-spatial data, EEMD is applied to time series of each spatial location to obtain IMF-like components of different time scales. All the ith IMF-like components of all the time series of all spatial locations are arranged to obtain ith temporal-spatial multi-dimensional IMF-like component. The same approach to the one used in temporal-spatial data decomposition is used to obtain the resulting	hilbert–huang transform;time series	Zhaohua Wu;Norden E. Huang;Xianyao Chen	2009	Advances in Adaptive Data Analysis	10.1142/S1793536909000187	hilbert–huang transform;machine learning;pattern recognition;mathematics;statistics	ML	0.17973310312717758	-43.75642669395104	151828
c15d9ea60d3ff472809d6a19277153ab604b665c	to optimize graph based power iteration for big data based on mapreduce paradigm	convergence;modified constraint;performance;big data;scalability;mapreduce;speedup	The next big thing in the IT world is Big Data. The values generated from storing and processing of Big Data cannot be analyzed using traditional computing techniques. The main aim of this paper is to design a scalable machine learning algorithm to scaleup and speedup clustering algorithm without losing its accuracy. Clustering using power iteration is fast and scalable. However, it requires matrix computation which makes the algorithm infeasible for Big Data. Moreover, power method converges slowly based on eigen vector. Hence, in this paper an investigation is done on convergence factor by applying a modified constraint that minimizes the computational cost by making the algorithm converge quickly. MapReduce parallel environment for Big Data is verified for the proposed algorithm using different sizes of datasets with different nodes in the cluster selecting speedup, scalability, and efficiency as the indicators. The performance of the proposed algorithm has been shown with respect to the execution time and the number of nodes. The results show that the proposed method is feasible and valid. It improves the overall performance and efficiency of the algorithm that can meet the needs of large scale processing.	big data;mapreduce;power iteration	Dhanapal Jayalatchumy;Perumal Thambidurai	2015		10.1007/978-3-319-26832-3_35	big m method;computer science;theoretical computer science;data mining;distributed computing	DB	-3.470905995096398	-39.55829283397436	151929
9ff0bd4da823b68ce86e8f664e673a7960089910	double indices-induced fcm clustering and its integration with fuzzy subspace clustering	convergence;fuzzy clustering;fuzzy subspace clustering algorithm;fcm	As one of the most popular algorithms for cluster analysis, fuzzy c-means (FCM) and its variants have been widely studied. In this paper, a novel generalized version called double indices-induced FCM (DI-FCM) is developed from another perspective. DI-FCM introduces a power exponent r into the constraints of the objective function such that the fuzziness index m is generalized and a new criterion of selecting an appropriate fuzziness index m is defined. Furthermore, it can be explained from the viewpoint of entropy concept that the power exponent r facilitates the introduction of entropy-based constraints into fuzzy clustering algorithms. As an attractive and judicious application, DI-FCM is integrated with a fuzzy subspace clustering (FSC) algorithm so that a new fuzzy subspace clustering algorithm called double indices-induced fuzzy subspace clustering (DI-FSC) algorithm is proposed for high-dimensional data. DI-FSC replaces the commonly used Euclidean distance with the feature-weighted distance, resulting in having two fuzzy matrices in the objective function. A convergence proof of DI-FSC is also established by applying Zangwill’s convergence theorem. Several experiments on both artificial data and real data were conducted and the experimental results show the effectiveness of the proposed algorithm.	algorithm;cluster analysis;clustering high-dimensional data;euclidean distance;experiment;fuzzy clustering;fuzzy cognitive map;loss function;optimization problem	Jun Wang;Korris Fu-Lai Chung;Shitong Wang;Zhaohong Deng	2012	2012 IEEE International Conference on Fuzzy Systems	10.1007/s10044-013-0341-y	correlation clustering;constrained clustering;mathematical optimization;data stream clustering;convergence;defuzzification;k-medians clustering;fuzzy clustering;flame clustering;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;fuzzy set operations;clustering high-dimensional data	DB	2.9720713088590363	-39.692726616373825	152360
f9bf59f37f78a421418237284026dcaacded240b	multiple manifold clustering using curvature constrained path	manifolds;tangents;models theoretical;imaging techniques;sequence databases;cluster analysis;decision support techniques;principal component analysis;clustering algorithms;artificial intelligence;algorithms;curvature;pattern recognition automated;humans;computer simulation	The problem of multiple surface clustering is a challenging task, particularly when the surfaces intersect. Available methods such as Isomap fail to capture the true shape of the surface near by the intersection and result in incorrect clustering. The Isomap algorithm uses shortest path between points. The main draw back of the shortest path algorithm is due to the lack of curvature constrained where causes to have a path between points on different surfaces. In this paper we tackle this problem by imposing a curvature constraint to the shortest path algorithm used in Isomap. The algorithm chooses several landmark nodes at random and then checks whether there is a curvature constrained path between each landmark node and every other node in the neighborhood graph. We build a binary feature vector for each point where each entry represents the connectivity of that point to a particular landmark. Then the binary feature vectors could be used as a input of conventional clustering algorithm such as hierarchical clustering. We apply our method to simulated and some real datasets and show, it performs comparably to the best methods such as K-manifold and spectral multi-manifold clustering.	graph - visual representation;hierarchical clustering;intersection of set of elements;node - plant part;numerous;short;algorithm;manifold;statistical cluster	Amir Babaeian;Alireza Bayestehtashk;Mojtaba Bandarabadi;Xuhui Huang	2015		10.1371/journal.pone.0137986	computer simulation;correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;cure data clustering algorithm;cluster analysis	Vision	2.765323406141818	-45.55286350790016	152368
6a755143d7e9fcde7a7a1f26684aab7b1d503b94	single cluster visualization to optimize air traffic management	institut fur flugfuhrung;feature vector;fuzzy clustering;high dimensional data;air traffic management	In this paper we present an application of single cluster visualization (SCV) a technique to visualize single clusters of high-dimensional data. This method maps a single cluster to the plane trying to preserve the relative distances of feature vectors to the corresponding prototype vector. Thus, fuzzy clustering results representing relative distances in the form of a partition matrix as well as hard clustering partitions can be visualized with this technique. The resulting two-dimensional scatter plot illustrates the compactness of a certain cluster and the need of additional prototypes as well. In this work, we will demonstrate the visualization method on a practical application.	cluster analysis;feature vector;fuzzy clustering;map;prototype;single customer view	Frank Rehm;Frank Klawonn;Rudolf Kruse	2006		10.1007/978-3-540-70981-7_36	k-medians clustering;fuzzy clustering;computer science;data science;machine learning;data mining	Visualization	0.9048183036534415	-39.67661150610116	152734
818cc85bd371741579ecf105dfe889eccba84eb6	minimizing the variance of cluster mixture models for clustering uncertain objects	clustering;uncertain cluster prototype;uncertain data	The increasing demand for dealing with uncertainty in data has led to the development of effective and efficient approaches in the data management and mining contexts. Clustering uncertain data objects has particularly attracted great attention in the data mining community. Most existing clustering methods however have urgently to come up with a number of issues, some of which are related to a poor efficiency mainly due to an expensive computation of the distance between uncertain objects. In this work, we propose a novel formulation to the problem of clustering uncertain objects, which allows for reaching accurate solutions by minimizing the variance of the mixture models that represent the clusters to be identified. We define a heuristic, MMVar, which exploits some analytical properties about the computation of variance for mixture models to compute local minima of the objective function at the basis of the proposed formulation. This characteristic allows MMVar to discard any distance measure between uncertain objects and, therefore, to achieve high efficiency. Experiments have shown that MMVar outperforms state-of-the-art algorithms from an efficiency viewpoint, while achieving better average performance in terms of accuracy.	algorithm;benchmark (computing);best, worst and average case;cluster analysis;computation;computer cluster;data mining;design rationale;experiment;heuristic;local optimum;loss function;maxima and minima;mixture model;optimization problem;uncertain data	Francesco Gullo;Giovanni Ponti;Andrea Tagarelli	2010	2010 IEEE International Conference on Data Mining	10.1002/sam.11170	correlation clustering;k-medians clustering;flame clustering;computer science;machine learning;consensus clustering;data mining;mathematics;cluster analysis;statistics	DB	-0.10201221244258647	-40.48521706242786	153052
0580e7d5c3410ab589fa5f989cca75203ddd8220	genetic-guided semi-supervised clustering algorithm with instance-level constraints	cluster algorithm;search space;data mining;genetics;machine learning;background knowledge;pattern recognition;genetic algorithm;genetic algorithms;semi supervised clustering;hybrid genetic algorithm	Semi-supervised clustering with instance-level constraints is one of the most active research topics in the areas of pattern recognition, machine learning and data mining. Several recent studies have shown that instance-level constraints can significantly increase accuracies of a variety of clustering algorithms. However, instance-level constraints may split the search space of the optimal clustering solution into pieces, thus significantly compound the difficulty of the search task. This paper explores a genetic approach to solve the problem of semi-supervised clustering with instance-level constraints. In particular, a novel semi-supervised clustering algorithm with instance-level constraints, termed as the hybrid genetic-guided semi-supervised clustering algorithm with instance-level constraints (Cop-HGA), is proposed. Cop-HGA uses a hybrid genetic algorithm to perform the search task of a high quality clustering solution that is able to draw a good balance between predefined clustering criterion and available instance-level background knowledge. The effectiveness of Cop-HGA is confirmed by experimental results on several real data sets with artificial instance-level constraints.	c object processor;cluster analysis;data mining;display resolution;genetic algorithm;hercules graphics card;machine learning;memetic algorithm;pattern recognition;semi-supervised learning;semiconductor industry	Yi Hong;Sam Kwong;Hui Xiong;Qingsheng Ren	2008		10.1145/1389095.1389363	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;genetic algorithm;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;brown clustering;dbscan;biclustering;affinity propagation;hierarchical clustering of networks;k-means clustering;clustering high-dimensional data;conceptual clustering	ML	3.7985587286954132	-42.0628732044949	153163
5926d9c72774a930175371bbd0fbf04afeb9df38	cellular automata pattern recognition and rule evolution through a neuro-genetic approach	pattern;automata;recognition;approach;rule;neuro;evolution;cellular;genetic		cellular automaton;pattern recognition	Stefania Bandini;Leonardo Vanneschi;Andrew Wuensche;Alessandro Bahgat Shehata	2009	J. Cellular Automata		growcut algorithm;computer science;evolution;mathematics;pattern	Vision	-3.5778642392494886	-48.07391302114284	153348
1723059f81983cebb8e1a0f942403be23ce9784b	local ordinal classification	classification accuracy	Given ordered classes, one is not only concerned to maximize the classification accuracy, but also to minimize the distances between the actual and the predicted classes. This paper offers an organized study on the various methodologies that have tried to handle this problem and presents an experimental study of these methodologies with the proposed local ordinal technique, which locally converts the original ordinal class problem into a set of binary class problems that encode the ordering of the original classes. The paper concludes that the proposed technique can be a more robust solution to the problem because it minimizes the distances between the actual and the predicted classes as well as improves the classification accuracy.	complexity class;encode;experiment;level of measurement;ordinal data;statistical classification	Sotiris B. Kotsiantis	2006		10.1007/0-387-34224-9_1	statistical classification;computer science;multiclass classification;classification rule	Vision	9.969926972409402	-42.057296921116546	153464
8067aa15ce0c22a191e559f27461da867808edaa	genetic algorithm based clustering: a survey	ga based clustering;unsupervised learning;data clustering algorithm;cluster algorithm;pattern clustering;unsupervised learning data mining genetic algorithms pattern classification pattern clustering;data mining;genetics;data analysis;biological cells;ga based clustering genetic algorithm clustering algorithms pattern recognition;unsupervised pattern classification;data mining genetic algorithm data clustering algorithm evolutionary technique unsupervised pattern classification;number of clusters;pattern classification;pattern recognition;clustering algorithms;genetic algorithm;genetic algorithms;image analysis;clustering algorithms genetic algorithms gallium algorithm design and analysis genetics partitioning algorithms biological cells;algorithm design and analysis;gallium;evolutionary technique;partitioning algorithms	This survey gives state-of-the-art of genetic algorithm (GA) based clustering techniques. Clustering is a fundamental and widely applied method in understanding and exploring a data set. Interest in clustering has increased recently due to the emergence of several new areas of applications including data mining, bioinformatics, web use data analysis, image analysis etc. To enhance the performance of clustering algorithms, Genetic Algorithms (GAs) is applied to the clustering algorithm. GAs are the best-known evolutionary techniques. The capability of GAs is applied to evolve the proper number of clusters and to provide appropriate clustering. This paper present some existing GA based clustering algorithms and their application to different problems and domains.	bioinformatics;cluster analysis;data mining;emergence;genetic algorithm;image analysis;software release life cycle	Rahila H. Sheikh;Mukesh M. Raghuwanshi;Anil N. Jaiswal	2008	2008 First International Conference on Emerging Trends in Engineering and Technology	10.1109/ICETET.2008.48	correlation clustering;determining the number of clusters in a data set;data stream clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;brown clustering;dbscan;biclustering;affinity propagation;clustering high-dimensional data;conceptual clustering	DB	3.625054885547385	-42.20374627840386	153484
0900dda0e4492c1b26d0496acce2ac8ec824c448	a new approach for phylogenetic tree construction based on minimal absent words	trees mathematics automata theory bioinformatics pattern clustering;phylogeny automata rabbits mice bovine genomics bioinformatics;forbidden words;suffix automaton;minimal absent words;hierarchical clustering phylogenetic tree construction minimal absent word linear time algorithm dna sequence suffix automaton neighbor joining method;phylogenetic tree construction;phylogenetic tree construction suffix automaton forbidden words minimal absent words	An absent word (or a forbidden word) is a word that does not appear in a given sequence. It is a minimal absent word if all its proper factors occur in the given sequence. In this paper, we propose a linear-time algorithm to compute the minimal absent words for DNA sequence using a suffix automaton. This method outputs the whole set of minimal absent words. We apply a Neighbor-Joining method to construct phylogenetic tree based on the minimal absent words.	algorithm;neighbor joining;phylogenetic tree;phylogenetics;suffix automaton;time complexity	Supaporn Chairungsee	2014	2014 25th International Workshop on Database and Expert Systems Applications	10.1109/DEXA.2014.21	generalized suffix tree;algorithm	Logic	-0.608956674177411	-51.41622522679287	153506
d49bf3168b912cd310648742e7b47db1440cf8ea	extension and robustness of transitivity clustering for protein-protein interaction network analysis	protein protein interaction network;protein complex;gold standard;graphic user interface;biological data;computational biology;transitive graph;biomedical application	Partitioning biological data objects into groups such that the objects within the groups share common traits is a longstanding challenge in computational biology. Recently, we developed and established transitivity clustering, a partitioning approach based on weighted transitive graph projection that utilizes a single similarity threshold as density parameter. In previous publications, we concentrated on the graphical user interface and on concrete biomedical application protocols. Here, we contribute the following theoretical considerations: (1) We provide proofs that the average similarity between objects from the same cluster is above the user-given threshold and that the average similarity between objects from different clusters is below the threshold. (2) We extend transitivity clustering to an overlapping clustering tool by integrating two new approaches. (3) We demonstrate the power of transitivity clustering for protein-complex detection. We evaluate our approaches against others by utilizing gold-standard data that was previously used by Brohée et al. for reviewing existing bioinformatics clustering tools. The extended version of this article is available online at http://transclust.mpi-inf. mpg.de. C © Taylor & Francis Group, LLC ISSN: 1542-7951 print 255 D ow nl oa de d by [ U ni ve rs ity o f A ri zo na ] at 0 7: 05 2 6 D ec em be r 20 12 256 Internet Mathematics	bioinformatics;cluster analysis;computational biology;francis;graphical user interface;interaction network;international standard serial number;numerical aperture;vertex-transitive graph	Tobias Wittkop;Sven Rahmann;Richard Röttger;Sebastian Böcker;Jan Baumbach	2011	Internet Mathematics	10.1080/15427951.2011.604559	correlation clustering;combinatorics;fuzzy clustering;biological data;gold standard;computer science;bioinformatics;theoretical computer science;data mining;graphical user interface;mathematics;multiprotein complex;cluster analysis;world wide web;algorithm	Comp.	1.2910933074163964	-48.4429973958029	153719
062e1f7a841e5eab3748b9de4eff61ead2957c08	denpehc: density peak based efficient hierarchical clustering		Existing hierarchical clustering algorithms involve a flat clustering component and an additional agglomerative or divisive procedure. This paper presents a density peak based hierarchical clustering method (DenPEHC), which directly generates clusters on each possible clustering layer, and introduces a grid granulation framework to enable DenPEHC to cluster large-scale and high-dimensional (LSHD) datasets. This study consists of three parts: (1) utilizing the distribution of the parameter γ, which is defined as the product of the local density ρ and the minimal distance to data points with higher density δ in “clustering by fast search and find of density peaks” (DPClust), and a linear fitting approach to select clustering centers with the clustering hierarchy decided by finding the “stairs” in the γ curve; (2) analyzing the leading tree (in which each node except the root is led by its parent to join the same cluster) as an intermediate result of DPClust, and constructing the clustering hierarchy efficiently based on the tree; and (3) designing a framework to enable DenPEHC to cluster LSHD datasets when a large number of attributes can be grouped by their semantics. The proposed method builds the clustering hierarchy by simply disconnecting the center points from their parents with a linear computational complexity O(m), where m is the number of clusters. Experiments on synthetic and real datasets show that the proposed method has promising efficiency, accuracy and robustness compared to state-of-the-art methods. ∗Corresponding author at: Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, No.266 Fangzheng Avenue, Shuitu Hi-tech Industrial Park, Shuitu Town, Beibei District, Chongqing, 400714, China. wanggy@ieee.org(Guoyin Wang) Email address: alanxuch@hotmail.com(Ji Xu), dengweihui@cigit.ac.cn(Weihui Deng) Preprint submitted to Journal of LTEX Templates August 31, 2016	academy;algorithm;cluster analysis;computational complexity theory;data point;email;experiment;hierarchical clustering;little computer people;robustness (computer science);sync;synthetic intelligence;tree structure;wang tile	Ji Xu;Guoyin Wang;Weihui Deng	2016	Inf. Sci.	10.1016/j.ins.2016.08.086	complete-linkage clustering;correlation clustering;constrained clustering;combinatorics;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;dendrogram;hierarchical clustering of networks;clustering high-dimensional data	ML	2.4652704420521636	-43.34423558348884	153766
ddbdb73ba0374fa918f774b36380f94c24a812e4	cda: a novel clustering delegate algorithm based on minority protection	pattern clustering;convergence;evolutionary computation;complexity theory;farthest neighborhood clustering concept;data mining clustering gene expression programming evolutionary computation;farthest neighborhood clustering concept clustering delegate algorithm minority protection gene expression programming individual similarity concept s cluster concept;clustering algorithms protection genetic programming convergence computer science gene expression fuzzy systems educational institutions evolutionary computation genetic mutations;s cluster concept;data mining;clustering delegate algorithm;gene expression;minority protection;pattern clustering genetic algorithms;clustering;clustering algorithms;genetic algorithms;gene expression programming;programming;algorithm design and analysis;individual similarity concept;evolutionary computing	In traditional Gene Expression Programming (GEP), individuals’ survival too much depends on fitness while their relationships are ignored. Borrowing the idea from the minority protection in real life, this study introduces a novel Cluster Delegate algorithm (CDA) and makes the following contributions: (1) propose several new concepts including individual similarity, β- cluster, and the farthest neighborhood clustering, (2) implement CDA algorithm which clustering population by fitness and selects delegate from each cluster, (3)Conduct extensive experiments to show that newly proposed method can accurately discover functions in complex problems.	.cda file;algorithm;cluster analysis;data mining;distribution (mathematics);experiment;gene expression programming;real life;software release life cycle	Jianping Xiang;Changjie Tang;Yu Chen;Lei Duan;Yue Wang;Ning Yang	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.384	computer science;bioinformatics;machine learning;data mining;mathematics;cluster analysis;evolutionary computation	DB	4.028728203017972	-41.96469215965295	154063
efe8a5983571a310f744d8817ce9918780cc2454	an approach to feature selection for keystroke dynamics systems based on pso and feature weighting	evolutionary computation;probability;personal recognition personal identification systems biometrics technique feature selection keystroke dynamics systems feature weighting support vector machines stochastic optimization technique genetic algorithms particle swarm optimization probability feature classification error feature reduction rate;support vector machines;biometrics access control;optimization technique;authorisation;dynamic system;support vector machines authorisation biometrics access control feature extraction genetic algorithms particle swarm optimisation pattern classification probability stochastic processes;feature reduction;stochastic optimization;particle swarm optimizer;stochastic processes;feature extraction;feature weighting;pattern classification;hybrid system;classification error;genetic algorithm;genetic algorithms;feature selection;support vector machine;optimal algorithm;particle swarm optimisation	Techniques based on biometrics have been successfully applied to personal identification systems. One rather promising technique uses the keystroke dynamics of each user in order to recognize him/her. In the present study, we present the development of a hybrid system based on support vector machines and stochastic optimization techniques. The main objective is the analysis of these optimization algorithms for feature selection. We evaluate two optimization techniques for this task: genetic algorithms (GA) and particle swarm optimization (PSO). We use the standard GA and we created a PSO variation, where each particle is represented by a vector of probabilities that indicate the possibility of selecting a particular feature and directly affects the original values of the features. In the present study, PSO outperformed GA with regard to classification error, processing time and feature reduction rate.	biometrics;event (computing);feature selection;genetic algorithm;hybrid system;keystroke dynamics;mathematical optimization;particle swarm optimization;software release life cycle;stochastic optimization;support vector machine	Gabriel L. F. B. C. Azevedo;George D. C. Cavalcanti;Edson C. B. Carvalho Filho	2007	2007 IEEE Congress on Evolutionary Computation	10.1109/CEC.2007.4424936	support vector machine;genetic algorithm;computer science;stochastic optimization;machine learning;pattern recognition;data mining;feature selection;evolutionary computation	AI	9.460637906445278	-42.74082383931564	154178
b8e2696e29c8c480de581757258291e2b6945ffc	active learning for high throughput screening	active learning;hpc kul;high throughput screening;chemical compounds;structure activity relationship;machine learning;optimization;gaussian process;qsar	An important task in many scientific and engineering disciplines is to set up experiments with the goal of finding the best instances (substances, compositions, designs) as evaluated on an unknown target function using limited resources. We study this problem using machine learning principles, and introduce the novel task of active k-optimization. The problem consists of approximating the k best instances with regard to an unknown function and the learner is active, that is, it can present a limited number of instances to an oracle for obtaining the target value. We also develop an algorithm based on Gaussian processes for tackling active k-optimization, and evaluate it on a challenging set of tasks related to structure-activity relationship prediction.	approximation algorithm;customer relationship management;experiment;gaussian process;machine learning;mathematical optimization;oracle database;throughput	Kurt De Grave;Jan Ramon;Luc De Raedt	2008		10.1007/978-3-540-88411-8_19	high-throughput screening;structure–activity relationship;computer science;artificial intelligence;machine learning;data mining;gaussian process;active learning;quantitative structure–activity relationship;active learning	ML	9.900858718336186	-51.16645194281061	154211
39ebac62009e5f3b2aeb4dc6295f7ce4530e7174	distribution sensitive product quantization		Product quantization (PQ) seems to have become the most efficient framework of performing approximate nearest neighbor (ANN) search for high-dimensional data. However, almost all existing PQ-based ANN techniques uniformly allocate precious bit budget to each subspace. This is not optimal, because data are often not evenly distributed among different subspaces. A better strategy is to achieve an improved balance between data distribution and bit budget within each subspace. Motivated by this observation, we propose to develop an optimized PQ (OPQ) technique, named distribution sensitive PQ (DSPQ) in this paper. The DSPQ dynamically analyzes and compares the data distribution based on a newly defined aggregate degree for high-dimensional data; whenever further optimization is feasible, resources such as memory and bits can be dynamically rearranged from one subspace to another. Our experimental results have shown that the strategy of bit rearrangement based on aggregate degree achieves modest improvements on most datasets. Moreover, our approach is orthogonal to the existing optimization strategy for PQ; therefore, it has been found that distribution sensitive OPQ can even outperform previous OPQ in the literature.		Linhao Li;Qinghua Hu;Yahong Han;Xin Li	2018	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2017.2759277	computer science;linear subspace;quantization (signal processing);encoding (memory);improved balance;mathematical optimization;k-nearest neighbors algorithm;distortion;algorithm;subspace topology	DB	7.358576891373836	-41.61665273660539	154416
272789a8e2710f61ab2cd6ce565e3793db814a62	attribute selection based on information gain ratio in fuzzy rough set theory with application to tumor classification	gain ratio;attribute selection;mutual information;tumor classification;fuzzy rough sets	Tumor classification based on gene expression levels is important for tumor diagnosis. Since tumor data in gene expression contain thousands of attributes, attribute selection for tumor data in gene expression becomes a key point for tumor classification. Inspired by the concept of gain ratio in decision tree theory, an attribute selection method based on fuzzy gain ratio under the framework of fuzzy rough set theory is proposed. The approach is compared to several other approaches on three real world tumor data sets in eywords: ttribute selection utual information uzzy rough sets ain ratio gene expression. Results show that the proposed method is effective. This work may supply an optional strategy for dealing with tumor data in gene expression or other applications. © 2012 Elsevier B.V. All rights reserved.	decision tree;information gain in decision trees;information gain ratio;rough set;set theory	Jianhua Dai;Qing Xu	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.07.029	information gain ratio;machine learning;pattern recognition;data mining;mathematics;mutual information;feature selection;statistics	AI	9.478528582500102	-40.28289828994327	154584
ab1bb68ca871e69d81c1323d9850fb99b0c2b81a	a novel hierarchical clustering framework for complex symbolic data exploration	hierarchical clustering;symbolic data exploration;upper bound;indexes;weight measurement;interval;synthetic data sets hierarchical clustering framework complex symbolic data exploration stochastic measurements stochastic pattern based symbolic data;stochastic processes;pattern matching;pattern clustering data handling;mathematical model;hierarchical clustering symbolic data exploration stochastic pattern interval;stochastic processes indexes pattern matching mathematical model upper bound data models weight measurement;data models;stochastic pattern	It has been recognized that the variable values of symbolic data may take the form of either an interval or a set of stochastic measurements of some underlying patterns. However, most existing work in symbolic data exploration are still concentrated on interval values only. Although some work in stochastic pattern-based symbolic data has been explored, it is not adaptable enough for various types of symbolic data yet. As a result, we bring forward a novel hierarchical clustering framework for complex symbolic data exploration. Our framework is able to be applied on either qualitative multi-valued or interval-valued or stochastic pattern-based symbolic data. Extensive experiments on a series of synthetic data sets have validated its efficiency and effectiveness.	cluster analysis;computation;experiment;hierarchical clustering;missing data;synthetic data	Xin Xu	2016	2016 IEEE 32nd International Conference on Data Engineering Workshops (ICDEW)	10.1109/ICDEW.2016.7495644	interval;database index;stochastic process;data modeling;computer science;theoretical computer science;machine learning;pattern matching;mathematical model;data mining;hierarchical clustering;symbolic data analysis;upper and lower bounds;statistics	DB	-2.386083942353343	-42.09857439793376	155281
bc84ad21f84d1b8592fb4840cc094b4729f635fa	ranked tiling		Tiling is a well-known pattern mining technique. Traditionally, it discovers large areas of ones in binary databases or matrices, where an area is defined by a set of rows and a set of columns. In this paper, we introduce the novel problem of ranked tiling, which is concerned with finding interesting areas in ranked data. In this data, each transaction defines a complete ranking of the columns. Ranked data occurs naturally in applications like sports or other competitions. It is also a useful abstraction when dealing with numeric data in which the rows are incomparable. We introduce a scoring function for ranked tiling, as well as an algorithm using constraint programming and optimization principles. We empirically evaluate the approach on both synthetic and real-life datasets, and demonstrate the applicability of the framework in several case studies. One case study involves a heterogeneous dataset concerning the discovery of biomarkers for different subtypes of breast cancer patients. An analysis of the tiles by a domain expert shows that our approach can lead to the discovery of novel insights.	algorithm;column (database);constraint programming;data mining;database;experiment;g.m. nijssen;level of measurement;mathematical optimization;real life;scoring functions for docking;subject-matter expert;synthetic intelligence;tiling window manager	Thanh Le Van;Matthijs van Leeuwen;Siegfried Nijssen;Ana Carolina Fierro;Kathleen Marchal;Luc De Raedt	2014		10.1007/978-3-662-44851-9_7		ML	6.98805467045597	-46.719202967988075	155649
31564323c0106037ab7db204c00e4645568173f6	a distribution adaptive framework for prediction interval estimation using nominal variables		Proposed methods for prediction interval estimation so far focus on cases where input variables are numerical. In datasets with solely nominal input variables, we observe records with the exact same input x, but different real valued outputs due to the inherent noise in the system. Existing prediction interval estimation methods do not use representations that can accurately model such inherent noise in the case of nominal inputs. We propose a new prediction interval estimation method tailored for this type of data, which is prevalent in biology and medicine. We call this method Distribution Adaptive Prediction Interval Estimation given Nominal inputs (DAPIEN) and has four main phases. First, we select a distribution function that can best represent the inherent noise of the system for all unique inputs. Then we infer the parameters θi (e.g. θi = [meani, variancei]) of the selected distribution function for all unique input vectors xi and generate a new corresponding training set using pairs of xi , θi. III). Then, we train a model to predict θ given a new xu. Finally, we calculate the prediction interval for a new sample using the inverse of the cumulative distribution function once the parameters θ is predicted by the trained model. We compared DAPIEN to the commonly used Bootstrap method on three synthetic datasets. Our results show that DAPIEN provides tighter prediction intervals while preserving the requested coverage when compared to Bootstrap. This work can facilitate broader usage of regression methods in medicine and biology where it is necessary to provide tight prediction intervals while preserving coverage when input variables are nominal.	bootstrapping (statistics);gene expression programming;gene prediction;interval arithmetic;level of measurement;numerical analysis;synthetic data;synthetic intelligence;test set	Ameen Eetemadi;Ilias Tagkopoulos	2015	CoRR		econometrics;machine learning;data mining;mathematics;statistics	ML	6.846449989046398	-51.947662889115385	155666
1c369241ef2f76d98d3f720189a91b4c9b4c9de8	association analysis techniques for bioinformatics problems	frequent pattern mining;data mining;pattern mining;protein function prediction;frequent;association analysis;computational biology;protein interaction network;bioinformatics	Association analysis is one of the most popular analysis paradigms in data mining. Despite the solid foundation of association analysis and its potential applications, this group of techniques is not as widely used as classification and clustering, especially in the domain of bioinformatics and computational biology. In this paper, we present different types of association patterns and discuss some of their applications in bioinformatics. We present a case study showing the usefulness of association analysis-based techniques for pre-processing protein interaction networks for the task of protein function prediction. Finally, we discuss some of the challenges that need to be addressed to make association analysis-based techniques more applicable for a number of interesting problems in bioinformatics.	bioinformatics;cluster analysis;computational biology;data mining;preprocessor;protein function prediction;statistical classification	Gowtham Atluri;Rohit Gupta;Gang Fang;Gaurav Pandey;Michael Steinbach;Vipin Kumar	2009		10.1007/978-3-642-00727-9_1	biology;computer science;bioinformatics;data science;genetic association;data mining;protein function prediction;k-optimal pattern discovery	Comp.	5.165464484376499	-48.37512617779679	155714
7ee604dc5ea123e494eabc4b4fc560f56c1136bc	a novel scale-invariant, dynamic method for hierarchical clustering of data affected by measurement uncertainty		Abstract An enhanced technique for hierarchical agglomerative clustering is presented. Classical clusterings suffer from non-uniqueness, resulting from the adopted scaling of data and from the arbitrary choice of the function to measure the proximity between elements. Moreover, most classical methods cannot account for the effect of measurement uncertainty on initial data, when present. To overcome these limitations, the definition of a weighted, asymmetric function is introduced to quantify the proximity between any two elements. The data weighting depends dynamically on the degree of advancement of the clustering procedure. The novel proximity measure is derived from a geometric approach to the clustering, and it allows to both disengage the result from the data scaling, and to indicate the robustness of a clustering against the measurement uncertainty of initial data. The method applies to both flat and hierarchical clustering, maintaining the computational cost of the classical methods.	cluster analysis;hierarchical clustering	Federica Vignati;Damiano Fustinoni;Alfonso Niro	2018	J. Computational Applied Mathematics	10.1016/j.cam.2018.05.062	robustness (computer science);mathematical optimization;dynamic method;scaling;cluster analysis;measurement uncertainty;hierarchical clustering;scale invariance;mathematics;weighting	ML	1.1301876803677038	-40.705426008759275	155801
9239ecf1f9a7afa0ed006ba672f42e40ba260504	parallel clustering of high-dimensional social media data streams	directed graphs;directed acyclic graph;pattern clustering;activemq;clustering algorithms synchronization twitter media storms heuristic algorithms algorithm design and analysis;cloud dikw;parallel clustering workers;parallel algorithm;dag;parallel batch processing;high dimensional social media data streams parallel algorithm activemq pub sub messaging system synchronization channel dag parallel clustering workers directed acyclic graph stream processing engines parallel batch processing social media data stream clustering data information knowledge wisdom cloud dikw;pub sub messaging system;synchronization strategies;social media data stream clustering;stream processing engines;high dimensional social media data streams;synchronisation;media;data information knowledge wisdom;storms;synchronization;heuristic algorithms;synchronisation cloud computing directed graphs media streaming message passing middleware parallel algorithms pattern clustering social networking online;high dimensional data;social networking online;message passing;media streaming;clustering algorithms;middleware;synchronization strategies social media data stream clustering parallel algorithms stream processing engines high dimensional data;twitter;synchronization channel;algorithm design and analysis;cloud computing;parallel algorithms	"""We introduce Cloud DIKW (Data, Information, Knowledge, Wisdom) as an analysis environment supporting scientific discovery through integrated parallel batch and streaming processing, and apply it to one representative domain application: social media data stream clustering. In this context, recent work demonstrated that high-quality clusters can be generated by representing the data points using high-dimensional vectors that reflect textual content and social network information. However, due to the high cost of similarity computation, sequential implementations of even single-pass algorithms cannot keep up with the speed of real-world streams. This paper presents our efforts in meeting the constraints of realtimesocial media stream clustering through parallelization in Cloud DIKW. Specifically, we focus on two system-level issues. Firstly, most stream processing engines such as Apache Storm organize distributed workers in the form of a directed acyclic graph (DAG), which makes it difficult to dynamically synchronize the state of parallel clustering workers. We tackle this challenge by creating a separate synchronization channel using a pub-sub messaging system (ActiveMQ in our case). Secondly, due to the sparsity of the high-dimensional vectors, the size of cancroids grows quickly as new data points are assigned tithe clusters. As a result, traditional synchronization that directly broadcasts cluster cancroids becomes too expensive and limits the scalability of the parallel algorithm. We address this problem by communicating only dynamic changes of the clusters rather than the whole centred vectors. Our algorithm under Cloud DIKWcan process the Twitter 10% data stream (""""gardenhose"""") in realtimewith 96-way parallelism. By natural improvements to CloudDIKW, including advanced collective communication techniques developed in our Harp project, we will be able to process the full Twitter data stream in real-time with 1000-way parallelism. Our use of powerful general software subsystems will enable many other applications that need integration of streaming and batch data analytics."""	apache activemq;apache hadoop;apache storm;cluster analysis;computation;dikw pyramid;data (computing);data point;data stream clustering;directed acyclic graph;inter-process communication;iterative method;mapreduce;parallel algorithm;parallel computing;queueing theory;real-time clock;real-time data;real-time transcription;scalability;social media;social network;sparse matrix;stream processing;streaming algorithm;streaming media;twister	Xiaoming Gao;Emilio Ferrara;Judy Qiu	2015	2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing	10.1109/CCGrid.2015.19	synchronization;data stream clustering;parallel computing;computer science;theoretical computer science;operating system;database;distributed computing;parallel algorithm;directed acyclic graph	HPC	-3.565046639048664	-39.61973073638999	156109
46a3606ccc56b16738199603552ab6a85060a61a	a novel clustering algorithm based on a modified model of random walk	cluster algorithm;transition probability;data clustering;random walk;particle system;self organization;generating function	We introduce a modified model of random walk, and then develop two novel clustering algorithms based on it. In the algorithms, each data point in a dataset is considered as a particle which can move at random in space according to the preset rules in the modified model. Further, this data point may be also viewed as a local control subsystem, in which the controller adjusts its transition probability vector in terms of the feedbacks of all data points, and then its transition direction is identified by an event-generating function. Finally, the positions of all data points are updated. As they move in space, data points collect gradually and some separating parts emerge among them automatically. As a consequence, data points that belong to the same class are located at a same position, whereas those that belong to different classes are away from one another. Moreover, the experimental results have demonstrated that data points in the test datasets are clustered reasonably and efficiently, and the comparison with other algorithms also provides an indication of the effectiveness of the proposed algorithms.	cluster analysis;data point;dijkstra's algorithm;emoticon;feedback;heuristic;markov chain;randomness;simplex algorithm	Qiang Li;Yan He;Jingping Jiang	2008	CoRR		markov chain;generating function;self-organization;computer science;machine learning;particle system;data mining;mathematics;cluster analysis;random walk;statistics	ML	2.23867669961379	-42.406138476919935	156328
ca07f6bcfd0065b3c761ae487c6649837efad3d8	efficient layered density-based clustering of categorical data	cluster algorithm;large dataset;index;scalable;clustering;categorical;indexation;layered;categorical data;network;bioinformatics	"""A challenge involved in applying density-based clustering to categorical biomedical data is that the """"cube"""" of attribute values has no ordering defined, making the search for dense subspaces slow. We propose the HIERDENC algorithm for hierarchical density-based clustering of categorical data, and a complementary index for searching for dense subspaces efficiently. The HIERDENC index is updated when new objects are introduced, such that clustering does not need to be repeated on all objects. The updating and cluster retrieval are efficient. Comparisons with several other clustering algorithms showed that on large datasets HIERDENC achieved better runtime scalability on the number of objects, as well as cluster quality. By fast collapsing the bicliques in large networks we achieved an edge reduction of as much as 86.5%. HIERDENC is suitable for large and quickly growing datasets, since it is independent of object ordering, does not require re-clustering when new data emerges, and requires no user-specified input parameters."""	algorithm;categorical variable;cluster analysis;education, medical, graduate;internet go server;inverted index;natural science disciplines;physical object;pubmed;published database;python;runtime system;scalability;sequence database;source code;anatomical layer;benefit;statistical cluster	Bill Andreopoulos;Aijun An;Xiaogang Wang;Dirk Labudde	2009	Journal of biomedical informatics	10.1016/j.jbi.2008.11.004	correlation clustering;constrained clustering;data stream clustering;categorical variable;k-medians clustering;fuzzy clustering;flame clustering;computer science;data science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;database;cluster analysis;single-linkage clustering;clustering high-dimensional data	DB	-3.4239079592434853	-40.07863264589866	156369
4f8be781a32044f31c2fa4448a2e165d41989345	reducing hierarchical clustering instability using clustering based on indiscernibility and indiscernibility level	hierarchical clustering;pattern clustering;cluster;information systems;rough set theory;classical rough set theory;tree data structures;information systems couplings clustering methods clustering algorithms classification algorithms educational institutions;instability;agglomerative hierarchical clustering;tree data structure;tree data structure indiscernibility level classical rough set theory data clustering method information systems simulation data set single linkage agglomerative hierarchical clustering method average agglomerative hierarchical clustering method hierarchical clustering yield dendrogram instability method;clustering method;indiscernibility level;classification algorithms;tree data structures pattern clustering rough set theory;clustering algorithms;instability rough set cluster indiscernibility level;information system;couplings;rough set;clustering methods;simulation data set;clustered data;average agglomerative hierarchical clustering method;single linkage agglomerative hierarchical clustering method;data clustering method;hierarchical clustering yield dendrogram instability method	The notions of indiscernibility and discernibility are the core concept of classical rough sets to cluster similarities and differences of data objects. In this paper, we use a new method of clustering data based on the combination of indiscernibility (quantitative indiscernibility relations) and its indiscernibility level. The indiscernibility level quantify the indiscernibility of pair of objects among other objects in information systems and this level represent the granularity of pair of objects in information system. For comparison to the new method, the following four clustering methods were selected and evaluated on a simulation data set : average-, complete- and single-linkage agglomerative hierarchical clustering and Ward’s method. The result of this paper shows that the four methods of hierarchical clustering yield dendrogram instability that give different solution under permutation of input order of data object while the new method reduce dendrogram instability.	cluster analysis;dendrogram;hierarchical clustering;information system;instability;linkage (software);rough set;simulation;tree structure;ward's method	R. B. Fajriya Hakim;Subanar;Edi Winarko	2010	2010 IEEE International Conference on Granular Computing	10.1109/GrC.2010.136	statistical classification;rough set;computer science;machine learning;pattern recognition;data mining;mathematics;tree;single-linkage clustering;information system	DB	1.296319994629272	-39.132872646826925	156395
db143f6812c5da04e50ebc7b6c713973f7188142	discretization using clustering and rough set theory	pattern clustering;class attribute interdependence redundancy value data clustering rough set theory data mining algorithm;class attribute interdependence redundancy value;information loss;rough set theory;set theory clustering algorithms data mining data preprocessing statistics entropy decision trees heuristic algorithms visual databases data visualization;set theory;data mining;data clustering;heuristic algorithms;data mining algorithm;data visualization;statistics;clustering algorithms;entropy;rough set theory data mining pattern clustering;decision trees;data preprocessing;visual databases	The majority of the data mining algorithms are applied to data described by discrete or nominal attributes. In order to apply these algorithms effectively to any dataset the continuous attribute need to be transformed to discretized ones. This paper presents an approach using clustering and rough set theory (RST). The experiments are performed on four datasets from UCI ML repository. The performance of the proposed approach is compared with some common discretization methods based on the two parameters - the number of intervals and the class-attribute interdependence redundancy (CAIR) value. The results of the proposed method show a satisfactory trade off between the number of intervals and the information loss due to discretization	algorithm;centre for artificial intelligence and robotics;cluster analysis;data mining;discretization;experiment;html attribute;intel matrix raid;interdependence;rough set;set theory	Girish Kumar Singh;Sonajharia Minz	2007	2007 International Conference on Computing: Theory and Applications (ICCTA'07)	10.1109/ICCTA.2007.51	discretization error;computer science;machine learning;pattern recognition;data mining;mathematics;cluster analysis;discretization of continuous features;data visualization	DB	1.26292245899673	-39.08323521839005	156522
9c6545b65ee9fb99206182c1473267531dcbb3ed	comparing sequence scaffolds	k median problem;whole genome shotgun;efficient algorithm;shotgun sequencing;disambiguating repeats;human genome;computational biology;dna sequence	The DNA sequence assembler we built for the whole genome shotgun assembly of the human genome, utilizes end-reads of inserts to order and orient assembled contigs into scaffolds for which the distances between consecutive contigs are statistically characterized. We consider the problem of comparing two such scaffolds. Applications include comparison of two distinct assemblies for mutual confirmation, and comparison of scaffold assemblies of BACs to determine a whole genome tiling of the BACs. We formalize the problem and develop efficient algorithms for a number of variations of the problem, the essential result being a sparse algorithm that refines gap estimates based on the overlap evidence.	algorithm;assembly language;sequence assembly;sparse matrix;tiling window manager	Eugene W. Myers	2001		10.1145/369133.369218	biology;bioinformatics;genome project;genetics;hybrid genome assembly;shotgun sequencing	Comp.	-0.3066176370243499	-51.02338207113047	156646
61819d865be831bdda172b62316a855e2fb631dd	optimistic bias in the assessment of high dimensional classifiers with a limited dataset	sample size;high dimensionality;pattern classification bioinformatics genetics;development process;feature space;genetics;gene expression;incomplete data;training algorithm optimistic bias high dimensional classifier bioinformatics data classifier development process gene expression;pattern classification;training testing classification algorithms signal to noise ratio measurement breast cancer covariance matrix;training algorithm;selection bias;bioinformatics	It is commonly recognized that using the same dataset for training and testing the classifier introduces optimistic bias in estimating classifier performance. However, bias of the same kind may still exist even when independent datasets are used for training and testing a classifier. This problem is especially important in the setting of high dimensional feature space and limited data. Bioinformatics data is typically characterized by a tremendous amount of data per patient but from a limited number of patients. Often the entire data set is utilized in a “pre-training” stage during which the feature set is winnowed to a manageable number, and the parameters of the training algorithm are established. Subsequently the data is bifurcated into training and test sets; however, bias has already been introduced into the classifier development process. We investigate the significance of this bias by performing simulated gene expression experiments. We find that, for data with moderate intrinsic separability and modest sample size, any observed separation is due to selection bias introduced in the aforementioned pre-training process. For greater intrinsic separability, correct data hygiene, i.e., complete separation of development and validation data yields a positive result, but one far less impressive than that mistakenly obtained using incomplete data separation.	algorithm;bias–variance tradeoff;bioinformatics;experiment;feature vector;linear separability;selection bias;simulation;statistical classification	Weijie Chen;David G. Brown	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033572	sample size determination;gene expression;selection bias;feature vector;computer science;machine learning;pattern recognition;data mining;software development process	ML	9.209886953654063	-49.7141088441901	156705
bd15329050a8c583130d07393521822735c444c0	bayesian classification for data from the same unknown class	multiple instance;bayesian classification;bayesian methods speaker recognition pattern recognition machine learning speech processing information science voting testing laboratories;information science;query vectors;helium;bayes methods;speech processing;naive bayes;bayesian methods;testing;set theory;indexing terms;classification;speaker recognition;naive bayes classifier;gaussian mixture model;machine learning;voting;single unit;pattern recognition;machine learning query vectors speaker recognition homologous sets homologous naive bayes algorithm naive bayes classifier classification accuracy gaussian mixture model;set theory speaker recognition bayes methods;classification accuracy;article;homologous sets;homologous naive bayes algorithm	In this paper, we address the problem of how to classify a set of query vectors that belong to the same unknown class. Sets of data known to be sampled from the same class are naturally available in many application domains, such as speaker recognition. We refer to these sets as homologous sets. We show how to take advantage of homologous sets in classification to obtain improved accuracy over classifying each query vector individually. Our method, called homologous naive Bayes (HNB), is based on the naive Bayes classifier, a simple algorithm shown to be effective in many application domains. RNB uses a modified classification procedure that classifies multiple instances as a single unit. Compared with a voting method and several other variants of naive Bayes classification, HNB significantly outperforms these methods in a variety of test data sets, even when the number of query vectors in the homologous sets is small. We also report a successful application of HNB to speaker recognition. Experimental results show that HNB can achieve classification accuracy comparable to the Gaussian mixture model (GMM), the most widely used speaker recognition approach, while using less time for both training and classification.	algorithm;classification;google map maker;homology (biology);mixture model;naive bayes classifier;normal statistical distribution;question (inquiry);sampling - surgical action;speaker recognition;test data	Hung-Ju Huang;Chun-Nan Hsu	2002	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.990870	speaker recognition;naive bayes classifier;information science;computer science;machine learning;pattern recognition;data mining;speech processing	ML	7.814770860816929	-46.18417222648637	156762
726d2d44a739769e90bb7388ed120efe84ea8147	clustering and projected clustering with adaptive neighbors	clustering with dimensionality reduction;block diagonal similarity matrix;adaptive neighbors;clustering	Many clustering methods partition the data groups based on the input data similarity matrix. Thus, the clustering results highly depend on the data similarity learning. Because the similarity measurement and data clustering are often conducted in two separated steps, the learned data similarity may not be the optimal one for data clustering and lead to the suboptimal results. In this paper, we propose a novel clustering model to learn the data similarity matrix and clustering structure simultaneously. Our new model learns the data similarity matrix by assigning the adaptive and optimal neighbors for each data point based on the local distances. Meanwhile, the new rank constraint is imposed to the Laplacian matrix of the data similarity matrix, such that the connected components in the resulted similarity matrix are exactly equal to the cluster number. We derive an efficient algorithm to optimize the proposed challenging problem, and show the theoretical analysis on the connections between our method and the K-means clustering, and spectral clustering. We also further extend the new clustering model for the projected clustering to handle the high-dimensional data. Extensive empirical results on both synthetic data and real-world benchmark data sets show that our new clustering methods consistently outperforms the related clustering approaches.	algorithm;benchmark (computing);cluster analysis;connected component (graph theory);data point;k-means clustering;laplacian matrix;similarity learning;similarity measure;spectral clustering;synthetic data	Feiping Nie;Xiaoqian Wang;Heng Huang	2014		10.1145/2623330.2623726	complete-linkage clustering;correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;spectral clustering;affinity propagation;clustering high-dimensional data	ML	0.9974096767532306	-41.74385092525779	156971
2c6b8bf80d2f13f02883fd119ba681e9a853f9e0	psm-flow: probabilistic subgraph mining for discovering reusable fragments in workflows		Scientific workflows define computational processes needed for carrying out scientific experiments. Existing workflow repositories contain hundreds of scientific workflows, where scientists can find materials and knowledge to facilitate workflow design for running related experiments. Identifying reusable fragments in growing workflow repositories has become increasingly important. In this paper, we present PSM-Flow, a probabilistic subgraph mining algorithm designed to discover commonly occurring fragments in a workflow corpus using a modified version of the Latent Dirichlet Allocation algorithm. The proposed model encodes the geodesic distance between workflow steps into the model for implicitly modeling fragments. PSM-Flow captures variations of frequent fragments while maintaining its space complexity bounded polynomially, as it requires no candidate generation. We applied PSM-Flow to three real-world scientific workflow datasets containing more than 750 workflows for neuroimaging analysis. Our results show that PSM-Flow outperforms three state of the art frequent subgraph mining techniques. We also discuss other potential future improvements of the proposed method.		Chin Wang Cheong;Daniel Garijo;William Kwok-Wai Cheung;Yolanda Gil	2018	2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI)	10.1109/WI.2018.00-93	resource management;latent dirichlet allocation;data mining;computer science;probabilistic logic;geodesic;bounded function;stochastic process;workflow	DB	-2.278535657293764	-46.53766603777955	157023
0ed82107e636de4bb2131b72df1052b4311fcee2	significance analysis of time-course gene expression profiles	gene expression profile;gaussian noise;time dependent;f testing;time course;standard deviation;statistical method;fold change;autoregressive model;gene expression;time dependence;time course gene expression;regression analysis;gene selection	This paper proposes a statistical method for significance analysis of time-course gene expression profiles, called SATgene. The SATgene models time-dependent gene expression profiles by autoregressive equations plus Gaussian noises, and time-independent gene expression profiles by constant numbers plus Gaussian noises. The statistical F-testing for regression analysis is used to calculate the confidence probability (significance level) that a time-course gene expression profile is not time-independent. The user can use this confidence probability to select significantly expressed genes from a time-course gene expression dataset. Both one synthetic dataset and one biological dataset were employed to evaluate the performance of the SATgene, compared to traditional gene selection methods: the pairwise R-fold change method and the standard deviation method. The results show that the SATgene outperforms the traditional methods.		Fang-Xiang Wu	2007		10.1007/978-3-540-72031-7_2	gene-centered view of evolution;gaussian noise;biology;gene expression;bioinformatics;data mining;mathematics;autoregressive model;standard deviation;regression analysis;statistics	Theory	4.956141176025835	-52.04161536845088	157395
f6406acabdc7d5b8d99ecb10bf8e6723ec1488cc	visualization of tradeoff in evaluation: from precision-recall & pn to lift, roc & bird		– Evaluation often aims to reduce the correctness or error characteristics of a system down to a single number, but that always involves trade-offs. Another way of dealing with this is to quote two numbers, such as Recall and Precision, or Sensitivity and Specificity. But it can also be useful to see more than this, and a graphical approach can explore sensitivity to cost, prevalence, bias, noise, parameters and hyper-parameters. Moreover, most techniques are implicitly based on two balanced classes, and our ability to visualize graphically is intrinsically two dimensional, but we often want to visualize in a multiclass context. We review the dichotomous approaches relating to Precision, Recall, and ROC as well as the related LIFT chart, exploring how they handle unbalanced and multiclass data, and deriving new probabilistic and information theoretic variants of LIFT that help deal with the issues associated with the handling of multiple and unbalanced classes.	correctness (computer science);gantt chart;graphical user interface;precision and recall;quantum decoherence;receiver operating characteristic;sensitivity and specificity;significant figures;theory;unbalanced circuit	David M. W. Powers	2015	CoRR		computer science;machine learning;data mining;statistics	SE	7.023945212154599	-43.70099376749383	157678
d27ae2c6c9a9257c6dbbf55cb3f5e411e30c296d	three-dimensional qsar using the k-nearest neighbor method and its interpretation	three dimensional;k nearest neighbor	In this paper we report a novel three-dimensional QSAR approach, kNN-MFA, developed based on principles of the k-nearest neighbor method combined with various variable selection procedures. The kNN-MFA approach was used to generate models for three different data sets and predict the activity of test molecules through each of these models. The three data sets used were the standard steroid benchmark, an antiinflammatory and an anticancerous data set. The study resulted in kNN-MFA models having better statistical parameters than the reported CoMFA models for all the three data sets. It was also found that stochastic methods generate better models resulting in more accurate predictions as compared to stepwise forward selection procedures. Thus, kNN-MFA method represents a good alternative to CoMFA-like methods.	(1+ε)-approximate nearest neighbor search;anti-inflammatory agents;benchmark (computing);feature selection;k-nearest neighbors algorithm;mfa message structure;quantitative structure-activity relationship;quantitative structure–activity relationship;single linkage cluster analysis;stepwise regression;steroids	Subhash Ajmani;Kamalakar Jadhav;Sudhir A. Kulkarni	2006	Journal of chemical information and modeling	10.1021/ci0501286	three-dimensional space;econometrics;chemistry;machine learning;data mining;mathematics;k-nearest neighbors algorithm	ML	9.6124170165272	-50.32167816704933	157798
711ed216d236abed4b4de2cf5ac3e44565043c65	consensus clustering algorithms: comparison and refinement	cluster algorithm;performance guarantee;optimization problem	Consensus clustering is the problem of reconciling clustering information about the same data set coming from different sources or from different runs of the same algorithm. Cast as an optimization problem, consensus clustering is known as median partition, and has been shown to be NP-complete. A number of heuristics have been proposed as approximate solutions, some with performance guarantees. In practice, the problem is apparently easy to approximate, but guidance is necessary as to which heuristic to use depending on the number of elements and clusterings given. We have implemented a number of heuristics for the consensus clustering problem, and here we compare their performance, independent of data size, in terms of efficacy and efficiency, on both simulated and real data sets. We find that based on the underlying algorithms and their behavior in practice the heuristics can be categorized into two distinct groups, with ramification as to which one to use in a given situation, and that a hybrid solution is the best bet in general. We have also developed a refined consensus clustering heuristic for the occasions when the given clusterings may be too disparate, and their consensus may not be representative of any one of them, and we show that in practice the refined consensus clusterings can be much superior to the general consensus clustering.	approximation algorithm;authorization;categorization;cluster analysis;consensus clustering;greedy algorithm;heat map;heuristic (computer science);hybrid algorithm;library (computing);local search (optimization);mathematical optimization;np-completeness;optimization problem;partition problem;ramification problem;theory	Andrey Goder;Vladimir Filkov	2008		10.1137/1.9781611972887.11	correlation clustering;optimization problem;constrained clustering;mathematical optimization;combinatorics;data stream clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;uniform consensus;mathematics;cluster analysis;clustering high-dimensional data	ML	4.479883105316151	-40.46359406477973	158266
3f13c802a1131ade62077273da68ed5356cb4d86	integration of particle swarm optimization and immune genetic algorithm-based dynamic clustering for customer clustering	dynamic clustering;cluster analysis;particle swarm optimization algorithm;immune genetic algorithm	This study intends to present a dynamic clustering (DC) approach based on particle swarm optimization (PSO) and immune genetic (IG) (DCPIG) algorithm, which is able to cluster the data into adequate clusters through data characteristics with pre-specified numbers of clusters. The proposed DCPIG algorithm is compared with three DC algorithms in the literature using Iris, Wine, Glass and Vowel benchmark data sets. The experiment results show that the DCPIG algorithm can achieve higher stability and accuracy than the other algorithms. In addition, the DCPIG algorithm is also applied to a real-world problem considering the customer clustering for a cyber flower shop. Lastly, we recommend different products and services to customers based on the clustering results.	cluster analysis;genetic algorithm;particle swarm optimization	R. J. Kuo;S. H. Lin;Zhen-Yao Chen	2015	International Journal on Artificial Intelligence Tools	10.1142/S0218213015500190	correlation clustering;mathematical optimization;multi-swarm optimization;determining the number of clusters in a data set;data stream clustering;meta-optimization;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;cluster analysis;k-medoids;affinity propagation;k-means clustering;population-based incremental learning;clustering high-dimensional data	AI	4.085749998231563	-41.88496454897295	158538
18a1322ae83fd73c4613d61c695c37e62e66ae72	clustering using an improved hybrid genetic algorithm	clustering;local iteration algorithm;genetic algorithms;hybrid genetic algorithm	In this article, a new genetic clustering algorithm called the Improved Hybrid Genetic Clustering Algorithm (IHGCA) is proposed to deal with the clustering problem under the criterion of minimum sum of squares clustering. In IHGCA, the improvement operation including five local iteration methods is developed to tune the individual and accelerate the convergence speed of the clustering algorithm, and the partition-absorption mutation operation is designed to reassign objects among different clusters. By experimental simulations, its superiority over some known genetic clustering methods is demonstrated.	genetic algorithm	Yongguo Liu;Xiaorong Pu;Yi-Dong Shen;Zhang Yi;Xiaofeng Liao	2007	International Journal on Artificial Intelligence Tools	10.1142/S021821300700362X	correlation clustering;mathematical optimization;determining the number of clusters in a data set;genetic algorithm;k-medians clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;cluster analysis;dbscan;biclustering;algorithm;affinity propagation;hierarchical clustering of networks;population-based incremental learning;clustering high-dimensional data	Robotics	3.7915087657325457	-41.79306820132251	158602
9dbf3e9fc9b194779518f67a51a09e0a32530a71	bayesian inference with historical data-based informative priors improves detection of differentially expressed genes.		MOTIVATION Modern high-throughput biotechnologies such as microarray are capable of producing a massive amount of information for each sample. However, in a typical high-throughput experiment, only limited number of samples were assayed, thus the classical 'large p, small n' problem. On the other hand, rapid propagation of these high-throughput technologies has resulted in a substantial collection of data, often carried out on the same platform and using the same protocol. It is highly desirable to utilize the existing data when performing analysis and inference on a new dataset.   RESULTS Utilizing existing data can be carried out in a straightforward fashion under the Bayesian framework in which the repository of historical data can be exploited to build informative priors and used in new data analysis. In this work, using microarray data, we investigate the feasibility and effectiveness of deriving informative priors from historical data and using them in the problem of detecting differentially expressed genes. Through simulation and real data analysis, we show that the proposed strategy significantly outperforms existing methods including the popular and state-of-the-art Bayesian hierarchical model-based approaches. Our work illustrates the feasibility and benefits of exploiting the increasingly available genomics big data in statistical inference and presents a promising practical strategy for dealing with the 'large p, small n' problem.   AVAILABILITY AND IMPLEMENTATION Our method is implemented in R package IPBT, which is freely available from https://github.com/benliemory/IPBT CONTACT: yuzhu@purdue.edu; zhaohui.qin@emory.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.		Pak Ching Li;Zhaonan Sun;Qing He;Yu Zhu;Zhaohui S. Qin	2016	Bioinformatics	10.1093/bioinformatics/btv631	pattern recognition;data mining;statistics	Comp.	3.6681799987855435	-51.30083235435522	158738
16e600a7563b46c000726adc48abf31eaa19266b	computation of similarity measures for sequential data using generalized suffix trees	generic algorithm;text processing;embedded language;network intrusion detection;suffix tree;dna analysis;similarity measure	We propose a generic algorithm for computation of similarit y measures for sequential data. The algorithm uses generalized suffix trees f or efficient calculation of various kernel, distance and non-metric similarity func tions. Its worst-case run-time is linear in the length of sequences and independen t of the underlying embedding language, which can cover words, k-grams or all contained subsequences. Experiments with network intrusion detection, DN A analysis and text processing applications demonstrate the utility of distan ces and similarity coefficients for sequences as alternatives to classical kernel fu ctions.	algorithm;best, worst and average case;bioinformatics;coefficient;computation;data breach;execution unit;experiment;generalized suffix tree;generic programming;grams;intrusion detection system;kernel (operating system);machine learning;natural language processing;suffix tree;tree traversal;unsupervised learning	Konrad Rieck;Pavel Laskov;Sören Sonnenburg	2006			generalized suffix tree;dna profiling;genetic algorithm;computer science;theoretical computer science;machine learning;pattern recognition;mathematics;compressed suffix array	ML	-0.9641281110672423	-47.76746191023853	158760
109b422ec3ec3bb24a804c8104dae55154d07459	a faster algorithm for detecting network motifs	distributed system;systeme reparti;algorithm analysis;analisis estructural;echantillonnage;complex network;error sistematico;bioinformatique;network motif;probabilistic approach;random networks;sampling;sistema repartido;bias;enfoque probabilista;approche probabiliste;analyse algorithme;bioinformatica;analyse structurale;muestreo;structural analysis;structural design;analisis algoritmo;biological network;erreur systematique;bioinformatics	Motifs in a network are small connected subnetworks that occur in significantly higher frequencies than in random networks. They have recently gathered much attention as a useful concept to uncover structural design principles of complex networks. Kashtan et al. [Bioinformatics, 2004] proposed a sampling algorithm for efficiently performing the computationally challenging task of detecting network motifs. However, among other drawbacks, this algorithm suffers from sampling bias and is only efficient when the motifs are small (3 or 4 nodes). Based on a detailed analysis of the previous algorithm, we present a new algorithm for network motif detection which overcomes these drawbacks. Experiments on a testbed of biological networks show our algorithm to be orders of magnitude faster than previous approaches. This allows for the detection of larger motifs in bigger networks than was previously possible, facilitating deeper insight into the field.	algorithm;bioinformatics;biological network;complex network;flow network;sampling (signal processing);sensor;sequence motif;testbed	Sebastian Wernicke	2005		10.1007/11557067_14	sampling;biological network;computer science;bioinformatics;artificial intelligence;network motif;machine learning;bias;mathematics;structural analysis;complex network;algorithm	Comp.	0.13524681870239488	-46.79122807752389	158775
6a15dfb43f2b7658f943537b2cb25bb64b6975cc	a primer on gene expression and microarrays for machine learning researchers	microarray data;supervised learning;gene expression data;gene expression;microarray data analysis;machine learning;microarrays;bioinformatics	Data originating from biomedical experiments has provided machine learning researchers with an important source of motivation for developing and evaluating new algorithms. A new wave of algorithmic development has been initiated with the publication of gene expression data derived from microarrays. Microarray data analysis is particularly challenging given the large number of measurements (typically in the order of thousands) that are reported for relatively few samples (typically in the order of dozens). Many data sets are now available on the web. It is important that machine learning researchers understand how data are obtained and which assumptions are necessary in the analysis. Microarray data have the potential to cause significant impact in machine learning research, not just as a rich and realistic source of cases for testing new algorithms, as has been the UCI machine learning repository in the past decades, but also as a main motivation for their development. In this article, we briefly review the biology underlying microarrays, the process of obtaining gene expression measurements, and the rationale behind the common types of analyses involved in a microarray experiment. We outline the main challenges and reiterate critical considerations regarding the construction of supervised learning models that use this type of data. The goal of this article is to familiarize machine learning researchers with data originated from gene expression microarrays.	algorithm;design rationale;experiment;gene expression profiling;gene expression programming;generalization (psychology);machine learning;microarray;primer;supervised learning	Winston Patrick Kuo;Eun-Young Kim;Jeff Trimarchi;Tor-Kristian Jenssen;Staal A. Vinterbo;Lucila Ohno-Machado	2004	Journal of biomedical informatics	10.1016/j.jbi.2004.07.002	microarray analysis techniques;computer science;bioinformatics;data science;machine learning;data mining;supervised learning;microarray databases	ML	6.3062700721931115	-47.39680760494779	159196
0ab1520db6e138d7ee67284154ec052fa97da6a7	robust twin boosting for feature selection from high-dimensional omics data with label noise	ensemble learning;boosting;期刊论文;feature selection	Omics data such as microarray transcriptomic and mass spectrometry proteomic data are typically characterized by high dimensionality and relatively small sample sizes. In order to discover biomarkers for diagnosis and prognosis from omics data, feature selection has become an indispensable step to find a parsimonious set of informative features. However, many previous studies report considerable label noise in omics data, which will lead to unreliable inferences to select uninformative features. Yet, to the best of our knowledge, very few feature selection methods are proposed to address this problem. This paper proposes a novel ensemble feature selection algorithm, robust twin boosting feature selection (RTBFS), which is robust to label noise in omics data. The algorithm has been validated on an omics feature selection test bed and seven real-world heterogeneous omics datasets, of which some are known to have label noise. Compared with several state-of-the-art ensemble feature selection methods, RTBFS can select more informative features despite label noise and obtain better classification results. RTBFS is a general feature selection method and can be applied to other data with label noise. MATLAB implementation of RTBFS and sample datasets are available at: http://www.cs.bham.ac.uk/∼szh/TReBFSMatlab.zip	feature selection;information;matlab;microarray;occam's razor;omics;proteomics;selection algorithm;testbed	Shan He;Bingbing Jiang;Zexuan Zhu;Douglas G. Ward;Helen J. Cooper;Mark R. Viant;John K. Heath;Xin Yao	2015	Inf. Sci.	10.1016/j.ins.2014.08.048	computer science;bioinformatics;machine learning;pattern recognition;data mining;ensemble learning;feature selection;boosting	ML	9.958927451624115	-47.136296166345346	159208
3f93a27fbb32ca26a6b7c0a0899609902e178e1e	earth science deep learning: applications and lessons learned		Deep learning has revolutionized computer vision and natural language processing with various algorithms scaled using high-performance computing. The Data Science and Informatics Group (DSIG) at the NASA Marshall Space Flight Center (MSFC), has been using deep learning for a variety of Earth science applications. This paper provides examples of the applications and also addresses some of the challenges that have been encountered.	algorithm;computer vision;data science;deep learning;informatics;natural language processing;supercomputer	Manil Maskey;Rahul Ramachandran;Jeffrey J. Miller;Jia Zhang;Iksha Gurung	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8517346	supervised learning;deep learning;artificial neural network;informatics;earth science;training set;satellite;computer science;transfer of training;artificial intelligence	Embedded	-2.0167538968784884	-45.424035964974045	159419
71dc9258915e8ee062664df6650a36183fb256df	a novel model of working set selection for smo decomposition methods	contrast sets ranking;exploratory quantitative contrast set mining;categorical attributes;modified equal width binning interval approach;association rules;data mining;width approximation;approximation theory;data analysis;association rule;association rules data mining diabetes error correction artificial intelligence computer science statistical analysis testing data analysis computational complexity;objective measure;discretization approach;data analysis exploratory quantitative contrast set mining discretization approach association rules categorical attributes continuous valued attributes modified equal width binning interval approach width approximation objective measure contrast sets ranking;continuous valued attributes;data mining approximation theory data analysis	In the process of training support vector machines (SVMs) by decomposition methods, working set selection is an important technique, and some exciting schemes were employed into this field. To improve working set selection, we propose a new model for working set selection in sequential minimal optimization (SMO) decomposition methods. In this model, it selects B as working set without reselection. Some properties are given by simple proof, and experiments demonstrate that the proposed method is in general faster than existing methods.	a library for support vector machines;experiment;full scale;mathematical optimization;sequential minimal optimization;support vector machine;working set	Zhendong Zhao;Lei Yuan;Yuxuan Wang;Forrest Sheng Bao;Shunyi Zhang Yanfei Sun	2007	19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)	10.1109/ICTAI.2007.99	association rule learning;computer science;machine learning;pattern recognition;data mining;mathematics;statistics	Robotics	3.566990531414329	-38.509987967700766	159493
01421f5c01b2d410b94862e10361ed2fe9765123	fuzzy cluster validation based on fuzzy pca-guided procedure	systems analysis methods;data mining;fuzzy clustering;fuzzy pattern recognition;fuzzy systems;knowledge discovery	Cluster validation is an important issue in fuzzy clustering research and many validity measures, most of which are motivated by intuitive justification considering geometrical features, have been developed. This paper proposes a new validation approach, which evaluates the validity degree of cluster partitions from the view point of the optimality of objective functions in FCM-type clustering. This approach makes it possible to evaluate the validity degree of robust cluster partitions, in which geometrical features are not available because of their possibilistic natures.	cluster analysis	Katsuhiro Honda;Akira Notsu;Tomohiro Matsui;Hidetomo Ichihashi	2011	IJFSA	10.4018/ijfsa.2011010104	defuzzification;fuzzy clustering;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;fuzzy measure theory;data mining;mathematics;fuzzy set operations	Robotics	1.7058020681697617	-39.5198952480416	159755
5c968456c5676d34875091ce5d82f8c518c247cc	optimality test for generalized fcm and its application to parameter selection	cluster algorithm;pattern clustering;optimisation;fuzzy c mean;indexing terms;fixed point;testing clustering algorithms fuzzy sets algorithm design and analysis fuzzy neural networks image analysis pattern analysis pattern recognition image processing biomedical imaging;fuzzy clustering;clustering analysis optimality test generalized fuzzy c means clustering parameter selection;cluster analysis;parameter selection;theoretical analysis;parameter selection fixed point fuzzy clustering fuzzy c means fcm hessian matrix local minimum optimality test;pattern clustering optimisation;saddle point	In cluster analysis, the fuzzy c-means (FCM) clustering algorithm is the best known and most widely used method. It was proven to converge to either a local minimum or saddle points by Bezdek et al. Wei and Mendel produced efficient optimality tests for FCM fixed points. Recently, a weighting exponent selection for FCM was proposed by Yu et al. Inspired by these results, we unify several alternative FCM algorithms into one model, called the generalized fuzzy c-means (GFCM). This GFCM model presents a wide variation of FCM algorithms and can easily lead to new and interesting clustering algorithms. Moreover, we construct a general optimality test for GFCM fixed points. This is applied to theoretically choose the parameters in the GFCM model. The experimental results demonstrate the precision of the theoretical analysis.	algorithm;cluster analysis;converge;fixed point (mathematics);fuzzy clustering;fuzzy cognitive map;maxima and minima;pattern recognition;serial digital video out;unsupervised learning	Jian Yu;Miin-Shen Yang	2005	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2004.836065	mathematical optimization;index term;k-medians clustering;fuzzy clustering;flame clustering;computer science;machine learning;pattern recognition;mathematics;fixed point;saddle point;cluster analysis	ML	3.177349481350389	-39.755448736173456	159788
1f74e077463f978c5ab382939278d743373588c4	clustering large data sets described with discrete distributions and its application on timss data set	ward s method;discrete distribution;hierarchical clustering;agglomerative hierarchical clustering method;symbolic object;large data sets;timss;k means method;clustering;symbolic data analysis;clustering method;leaders method;number of clusters;mixed units	Symbolic Data Analysis is based on a special descriptions of data – symbolic objects. Such descriptions preserve more detailed in formation about data than the usual representations with mean values. A spec ial kind of symbolic object is also representation with distributions . In the clustering process this representation enables us to consider the vari ables of all types at the same time. We present two clustering methods based on the data descript ions with discrete distributions: the adapted leaders method and the adapted agglomerative hierarchical clustering Ward’s method. Both metho ds are compatible – they can be viewed as two approaches for solving the same clu stering optimization problem. In the obtained clustering to each clus ter is assigned its leader. The descriptions of the leaders offer simple int rpretation of the clusters’ characteristics. The leaders method enables us to efficiently solve clusterin g problems with large number of units; while the agglomerative method i s applied on the obtained leaders and enables us to decide upon the right numb er of clusters on the basis of the corresponding dendrogram. University of Ljubljana, Faculty of Economics, Department of Statistics, simona.cerne@ef.uni-lj.si (corresponding author) University of Ljubljana, Faculty of Mathematics and Physic s, Department of Mathematics, vladimir.batagelj@fmf.uni-lj.si The Educational Research Institute, Slovenia, b rbara.japelj@pei.si	cluster analysis;dendrogram;hierarchical clustering;mathematical optimization;optimization problem;spec#;symbolic data analysis;ward's method	Simona Korenjak-Cerne;Vladimir Batagelj;Barbara Japelj Pavesic	2011	Statistical Analysis and Data Mining	10.1002/sam.10105	probability distribution;correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;theoretical computer science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;ward's method;symbolic data analysis;cluster analysis;single-linkage clustering;brown clustering;dendrogram;affinity propagation;statistics;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	ML	2.3365914743864553	-42.80037181787695	159867
f05747ea1bc41cc99c18a73e2706b2ab7befb47d	similarity and cluster analysis algorithms for microarrays using r* trees	microarray data;hierarchical clustering;biology computing;cluster algorithm;pattern clustering;k means;algorithm design and analysis clustering algorithms indexing principal component analysis data analysis fungi information analysis time series analysis information science clustering methods;genetics;arrays;cluster analysis;yeast cell cycle dataset cluster analysis algorithm microarray data r tree similarity analysis time series data principle components analysis pca distance conservation clustering algorithm kmeans r hierarchy r k means hierarchical clustering;principal component analysis;indexation;comparative study;time series data;cell cycle;principle component analysis;false positive;cellular biophysics;arrays pattern clustering genetics principal component analysis cellular biophysics biology computing;similarity search	Similarity and cluster analysis are important aspects for analyzing microarray data. Based on our perspective of viewing microarrays as time series data, both similarity analysis and cluster analysis are carried out through indexing on time series data using R*-Trees. We have developed algorithms for similarity and cluster analysis on microarray data, and conducted experimental studies and comparative studies. First, our study shows that principle components analysis (PCA) has superiority over several other methods (such as DFT and PAA) as far as distance conservation is concerned. A similarity analysis tool based on PCA has been developed, which is able to explore less R*-Tree nodes before finding its similar counterparts and returns less false positives than other methods. In addition, we also extend R*-Tree's application to cluster analysis. With the aid of R*-Tree indexing, two clustering algorithms. KMeans-R and Hierarchy-R, are proposed as an improved version of K-Means and hierarchical clustering, respectively. Experiments for similarity search and cluster analysis based on proposed algorithms have been carried out and have shown favorable results. Experiments related to yeast cell cycle dataset are reported in this paper.	algorithm;amplifier;cell (microprocessor);cluster analysis;discrete fourier transform;experiment;hierarchical clustering;k-means clustering;microarray;principal component analysis;r* tree;similarity search;time series	Jiaxiong Pi;Yong Shi;Zhengxin Chen	2005	2005 IEEE Computational Systems Bioinformatics Conference - Workshops (CSBW'05)	10.1109/CSBW.2005.125	computer science;bioinformatics;machine learning;data mining;principal component analysis	Visualization	5.890158989519005	-48.216076538677285	159966
080c536dd5d7ece4bddd8d2e7f6419d96b15043e	a real-time pe-malware detection system based on chi-square test and pe-file features		Constructing an efficient malware detection system requires taking into consideration two important aspects, which are the accuracy and the detection time. However, finding an appropriate balance between these two characteristics remains at this time a very challenging problem. In this paper, we present a real-time PE (Portable Executable) malware detection system, which is based on the analysis of the information stored in the PE-Optional Header fields (PEF). Our system used a combination of the Chi-square (KHI2) score and the Phi (ϕ) coefficient as feature selection method. We have evaluated our system using Rotation Forest classifier implemented in WEKA and we reached more than 97% of accuracy. Our system is able to categorize a file in 0.077 seconds, which makes it adequate for real-time detection of malware.	chi;malware;real-time transcription	Mohamed Belaoued;Smaine Mazouzi	2015		10.1007/978-3-319-19578-0_34	computer vision;computer science;malware;categorization;header;feature selection;artificial intelligence;chi-square test;malware analysis;pattern recognition;portable executable	ML	6.3836410568504185	-39.39504915068258	159986
03f22c4328d10546ce910904bc331b395ed8500d	dimensionality reduction particle swarm algorithm for high dimensional clustering	pattern clustering data analysis data reduction particle swarm optimisation;particle swarm;high dimensional dataset;cluster algorithm;pattern clustering;general and miscellaneous mathematics computing and information science;high dimensionality;pso clustering;runtime;high dimensional clustering;accuracy;distance measurement;data analysis;pso clustering dimensionality reduction high dimensional clustering particle swarm optimization k means clustering data analysis;dimensions;dimensionality reduction;particle swarm optimizer;large scale integration;particle swarm optimization;computer calculations;high dimensional data;calculation methods;clustering algorithms;algorithms;data reduction;particle swarm optimization clustering algorithms frequency approximation algorithms data analysis laboratories usa councils data preprocessing computational efficiency runtime;particle swarm optimisation;dimensional reduction;sparse matrices;k means clustering;algorithm design and analysis	The Particle Swarm Optimization (PSO) clustering algorithm can generate more compact clustering results than the traditional K-means clustering algorithm. However, when clustering high dimensional datasets, the PSO clustering algorithm is notoriously slow because its computation cost increases exponentially with the size of the dataset dimension. Dimensionality reduction techniques offer solutions that both significantly improve the computation time, and yield reasonably accurate clustering results in high dimensional data analysis. In this paper, we introduce research that combines different dimensionality reduction techniques with the PSO clustering algorithm in order to reduce the complexity of high dimensional datasets and speed up the PSO clustering process. We report significant improvements in total runtime. Moreover, the clustering accuracy of the dimensionality reduction PSO clustering algorithm is comparable to the one that uses full dimension space.	algorithm;cluster analysis;clustering high-dimensional data;computation;dimensionality reduction;experiment;integrated circuit;k-means clustering;particle swarm optimization;phase-shift oscillator;rp (complexity);time complexity	Xiaohui Cui;Justin M. Beaver;Jesse St. Charles;Thomas E. Potok	2008	2008 IEEE Swarm Intelligence Symposium	10.1109/SIS.2008.4668309	correlation clustering;mathematical optimization;data stream clustering;subclu;k-medians clustering;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;mathematics;cluster analysis;dbscan;biclustering;spectral clustering;affinity propagation;clustering high-dimensional data	DB	-0.6310910259199877	-40.595967269019305	160016
2993beb35a34246fbc80ae43cfb570003aebf779	dissimilarity-based representation for radiomics applications		Radiomics is a term which refers to the analysis of the large amount of quantitative tumor features extracted from medical images to find useful predictive, diagnostic or prognostic information. Many recent studies have proved that radiomics can offer a lot of useful information that physicians cannot extract from the medical images and can be associated with other information like gene or protein data. However, most of the classification studies in radiomics report the use of feature selection methods without identifying the machine learning challenges behind radiomics. In this paper, we first show that the radiomics problem should be viewed as an high dimensional, low sample size, multi view learning problem, then we compare different solutions proposed in multi view learning for classifying radiomics data. Our experiments, conducted on several real world multi view datasets, show that the intermediate integration methods work significantly better than filter and embedded feature selection methods commonly used in radiomics.	concatenation;distance matrix;embedded system;experiment;feature selection;machine learning;missing data;radiomics;random forest	Hongliu Cao;Simon Bernard;Laurent Heutte;Robert Sabourin	2018	CoRR		pattern recognition;artificial intelligence;feature selection;computer science;radiomics	AI	9.269387212113022	-48.985644991221	160027
a7fb80af6f95989eb04aa3cc43de5537bb322727	support vector data description model to map specific land cover with optimal parameters determined from a window-based validation set	support vector data description;simulated annealing;window based validation set;optimal parameters;land cover	This paper developed an approach, the window-based validation set for support vector data description (WVS-SVDD), to determine optimal parameters for support vector data description (SVDD) model to map specific land cover by integrating training and window-based validation sets. Compared to the conventional approach where the validation set included target and outlier pixels selected visually and randomly, the validation set derived from WVS-SVDD constructed a tightened hypersphere because of the compact constraint by the outlier pixels which were located neighboring to the target class in the spectral feature space. The overall accuracies for wheat and bare land achieved were as high as 89.25% and 83.65%, respectively. However, target class was underestimated because the validation set covers only a small fraction of the heterogeneous spectra of the target class. The different window sizes were then tested to acquire more wheat pixels for validation set. The results showed that classification accuracy increased with the increasing window size and the overall accuracies were higher than 88% at all window size scales. Moreover, WVS-SVDD showed much less sensitivity to the untrained classes than the multi-class support vector machine (SVM) method. Therefore, the developed method showed its merits using the optimal parameters, tradeoff coefficient (C) and kernel width (s), in mapping homogeneous specific land cover.	class;coefficient;coefficient;entity name part qualifier - adopted;exclusion;feature vector;genetic heterogeneity;increment;information;large;multiclass classification;numerous;one-class classification;pixel;randomness;sensor;support vector machine;support vector machine;test set;trees (plant);weaver syndrome;window function;algorithm;benefit;width	Jinshui Zhang;Zhoumiqi Yuan;Guanyuan Shuai;Yaozhong Pan;Xiufang Zhu	2017		10.3390/s17050960	simulated annealing;computer science;machine learning;pattern recognition;data mining	ML	9.55485881631885	-48.74473603308038	160321
6614a0f2b4038caea18a0679d70e3ed9a4ebf1eb	application of factor analysis on mycobacterium tuberculosis transcriptional responses for drug clustering, drug target, and pathway detections	dimensionalidad;genetic engineering;hierarchical clustering;correlacion;analyse amas;ontologie;medicament;test statistique;deteccion blanco;pulga de dna;analisis factorial;high dimensionality;grouping;analisis datos;latent variable;classification non supervisee;puce a dna;drug targeting;test estadistico;statistical test;dimensionality;bioinformatique;data mining;similitude;detection cible;conference paper;gene expression;expression genique;data analysis;hierarchical classification;analyse factorielle;cluster analysis;fouille donnee;factor analysis;dimensionnalite;clustering method;clasificacion no supervisada;ingenieria genetica;similarity;genie genetique;test methods;dna chip;classification hierarchique;unsupervised classification;analyse donnee;ontologia;analisis cluster;medicamento;agrupamiento;bioinformatica;similitud;drug;correlation;target detection;clasificacion jerarquizada;busca dato;ontology;expresion genetica;groupage;mycobacterium tuberculosis;bioinformatics;gene ontology	Recently, the differential transcriptional responses of Mycobacterium tuberculosis to drug and growth-inhibitory conditions were monitored to generate a data set of 436 microarray profiles. These profiles were valuably used for grouping drugs, identifying drug targets and detecting related pathways, based on various conventional methods; such as Pearson correlation, hierarchical clustering, and statistical tests. These conventional clustering methods used the high dimensionality of gene space to reveal drug groups basing on the similarity of expression levels of all genes. In this study, we applied the factor analysis with these conventional methods for drug clustering, drug target detection and pathway detection. The latent variables or factors of gene expression levels in loading space from factor analysis allowed the hierarchical clustering to discover true drug groups. The t-test method was applied to identify drug targets which most significantly associated with each drug cluster. Then, gene ontology was used to detect pathway associations for each group of drug targets.	factor analysis;gene regulatory network;sensor	Jeerayut Chaijaruwanich;Jamlong Khamphachua;Sukon Prasitwattanaseree;Saradee Warit;Prasit Palittapongarnpim	2006		10.1007/11811305_91	latent variable;genetic engineering;statistical hypothesis testing;targeted drug delivery;gene expression;curse of dimensionality;dna microarray;similarity;computer science;bioinformatics;similitude;machine learning;ontology;data mining;hierarchical clustering;cluster analysis;test method;factor analysis;data analysis;correlation;statistics	ML	5.884097678960081	-50.5309228873818	160652
4139f0db7dfa1697a134e2a600427155f4cf37e9	a novel clustering algorithm using hypergraph-based granular computing	granular computing	Clustering is an important technique in data mining. In this paper, we introduce a new clustering algorithm. This algorithm, based on granular computing, constructs a hypergraph (simplicial complex) by the hypergraph bisection algorithm. It will discover the similarities and associations among documents. In some experiments on Web data, the proposed algorithm is used; the results are quite satisfactory. C © 2009 Wiley Periodicals, Inc.	algorithm;bisection method;cluster analysis;computation;data mining;experiment;granular computing;granule (oracle dbms);graph partition;john d. wiley;simplicial complex;windows update	Qun Liu;Xiaofeng Liao;Yu Wu	2010	Int. J. Intell. Syst.	10.1002/int.20393	combinatorics;data stream clustering;granular computing;computer science;theoretical computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;fsa-red algorithm;cluster analysis	ML	1.588283199527044	-40.59384927776367	160688
0008b185d542590f20ebd0b9fd409a2fd345922b	on the complexity of finding emerging patterns	treatment planning;gene expression profile;biology computing;polynomial time approximation computational complexity emerging pattern diseases gene expression profile;data mining;genetics;computational complexity;diseases gene expression computer science frequency pattern analysis polynomials mathematics circuits transaction databases computer applications;polynomial time;diseases;polynomial time approximation scheme;biology computing computational complexity diseases genetics data mining	Emerging patterns have been studied as a useful type of pattern for the diagnosis and understanding of diseases based on the analysis of gene expression profiles. They are useful for capturing interactions among genes (or other biological entities), for capturing signature patterns for disease subtypes, and deriving potential disease treatment plans, etc. We study the complexity of finding emerging patterns (with the highest frequency). We first show that the problem is MAX SNP-hard. This implies that polynomial time approximation schemes do not exist for the problem unless P = NP. We then prove that for any constant /spl delta/ < 1, the emerging pattern problem cannot be approximated within ratio 2/sup log/spl delta// in polynomial time unless NP /spl sube/ DTIME[2/sup polylogn/], where n is the number of positions in a pattern.	approximation algorithm;entity;interaction;max;p versus np problem;polynomial;snp (complexity);time complexity	Lusheng Wang;Hao Zhao;Guozhu Dong;Jianping Li	2004	Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004.	10.1109/CMPSAC.2004.1342691	time complexity;polynomial-time approximation scheme;radiation treatment planning;computer science;bioinformatics;theoretical computer science;computational complexity theory	Theory	2.496583891906177	-49.57515722301488	160794
332b6dc5a54070a03ac0f7947b14082d4cfd894e	identification of signatures in biomedical spectra using domain knowledge	classification of biomedical spectra;spectral signature;l 1 norm svm;domain knowledge;dimensionality reduction;genetic algorithm;feature selection;dimensional reduction;consensus feature sets;l1 norm svm	OBJECTIVE Demonstrate that incorporating domain knowledge into feature selection methods helps identify interpretable features with predictive capability comparable to a state-of-the-art classifier.   METHODS Two feature selection methods, one using a genetic algorithm (GA) the other a L(1)-norm support vector machine (SVM), were investigated on three real-world biomedical magnetic resonance (MR) spectral datasets of increasing difficulty. Consensus sets of the feature sets obtained by the two methods were also assessed.   RESULTS AND CONCLUSIONS Features identified independently by the two methods and by their consensus, determine class-discriminatory groups or individual features, whose predictive power compares favorably with that of a state-of-the-art classifier. Furthermore, the identified feature signatures form stable groupings at definite spectral positions, hence are readily interpretable. This is a useful and important practical result for generating hypothesis for the domain expert.		Erinija Pranckeviciene;Ray L. Somorjai;Richard Baumgartner;Moongu Jeon	2005	Artificial intelligence in medicine	10.1016/j.artmed.2004.12.002	genetic algorithm;computer science;machine learning;pattern recognition;data mining;feature selection;domain knowledge;dimensionality reduction;spectral signature	ML	8.805181994918449	-49.0045719795301	160919
86e5522c8d7e9434f9f42723c9f2e901f8fa54fc	identifying structures with informative dimensions in streams	entropy data streaming structure discovering informative dimension data structure data clustering;pattern clustering;informative dimension;memory management;uncertainty;entropy data streams clustering;data mining;data streams;data clustering;clustering;data structures;merging;clustering algorithms;entropy;data streaming structure discovering;pattern clustering data mining data structures entropy;clustering algorithms entropy monitoring statistical distributions information science computer science data mining algorithm design and analysis application software telephony;data structure;clustered data;algorithm design and analysis	Discovering structures in streaming data is an important data mining task and has motivated design of several well known algorithms. However, in some applications, a higher level of analysis is desirable to reveal the set of dimensions which contribute heavily to the structures. In this paper, we propose an algorithm ISID (Identifying Structures with Informative Dimensions), which operates in the streaming environment and delivers clusteres along with dimensions that contribute significantly to these clusters. The algorithm uses a three stage approach and utilizes entropy in an innovative way to achieve the goal in four different ways, depending on the desired guarantees on structural richness or minimal dimension set for a cluster.The experimental results on synthetic and real data sets demonstrate the efficiency and effectiveness of the proposed algorithm.	algorithm;data mining;information;streaming media;synthetic intelligence	Vasudha Bhatnagar;Sharanjit Kaur;Neelima Gupta	2009	2009 Eighth IEEE/ACIS International Conference on Computer and Information Science	10.1109/ICIS.2009.95	computer science;data science;pattern recognition;data mining	Visualization	-0.5786143293197832	-41.40633906528713	160931
a3d7066a50612f787fc2749aded4dcba7d22bc24	efficient hierarchical clustering for single-dimensional data using cuda		Hierarchical clustering is a widely-used and well-researched clustering technique. The classical algorithm for agglomerative hierarchical clustering is prohibitively expensive for use with large datasets. Numerous algorithms exist to improve the efficiency of hierarchical clustering for various linkage metrics, and for large datasets. Recent research has proposed approaches for improving the efficiency of hierarchical clustering through parallelism. The newest approaches utilise GPGPU technologies, which facilitate massive parallelism on commodity consumer hardware. Existing GPGPU implementations fail to maximise the number of merges that can be performed in parallel, and feature high use of memory. These limitations prevent existing implementations from achieving the full performance offered by GPGPU. In this paper, we propose a novel GPGPU algorithm for hierarchical clustering of single-dimensional data. Our proposed algorithm exploits the unique characteristics of one-dimensional data to maximise merge parallelism and significantly reduce memory requirements. Validation demonstrates that our proposed algorithm produces equivalent results to the classical algorithm for both the single-linkage and complete-linkage metrics. Benchmarking results show that our algorithm scales well to large datasets, and offers a substantial speed-up over the classical algorithm. Future work will look to extend our proposed approach to larger datasets with higher dimensions.	algorithm;cuda;cluster analysis;dspace;data structure;fowlkes–mallows index;general-purpose computing on graphics processing units;hierarchical clustering;image scaling;linkage (software);parallel computing;requirement;single-linkage clustering	Adam Rehn;Aidan Possemiers;Jason Holdsworth	2018		10.1145/3167918.3167929	parallel computing;implementation;massively parallel;correlation clustering;theoretical computer science;benchmarking;cluster analysis;hierarchical clustering;computer science;cuda;general-purpose computing on graphics processing units	HPC	-2.542897650728837	-40.18977811144196	161023
2160909026a2151a506b539ed3d50cd7d62365dc	evaluation and improvements of clustering algorithms for detecting remote homologous protein families	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;clustering sequence algorithms;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;sequence analysis;combinatorial libraries;uk research reports;medical journals;computer appl in life sciences;remote homology detection;europe pmc;biomedical research;microarrays;bioinformatics	An important problem in computational biology is the automatic detection of protein families (groups of homologous sequences). Clustering sequences into families is at the heart of most comparative studies dealing with protein evolution, structure, and function. Many methods have been developed for this task, and they perform reasonably well (over 0.88 of F-measure) when grouping proteins with high sequence identity. However, for highly diverged proteins the performance of these methods can be much lower, mainly because a common evolutionary origin is not deduced directly from sequence similarity. To the best of our knowledge, a systematic evaluation of clustering methods over distant homologous proteins is still lacking. We performed a comparative assessment of four clustering algorithms: Markov Clustering (MCL), Transitive Clustering (TransClust), Spectral Clustering of Protein Sequences (SCPS), and High-Fidelity clustering of protein sequences (HiFix), considering several datasets with different levels of sequence similarity. Two types of similarity measures, required by the clustering sequence methods, were used to evaluate the performance of the algorithms: the standard measure obtained from sequence–sequence comparisons, and a novel measure based on profile-profile comparisons, used here for the first time. The results reveal low clustering performance for the highly divergent datasets when the standard measure was used. However, the novel measure based on profile-profile comparisons substantially improved the performance of the four methods, especially when very low sequence identity datasets were evaluated. We also performed a parameter optimization step to determine the best configuration for each clustering method. We found that TransClust clearly outperformed the other methods for most datasets. This work also provides guidelines for the practical application of clustering sequence methods aimed at detecting accurately groups of related protein sequences.	algorithm;amino acid sequence;cluster analysis;computational biology;computer cluster;homology (biology);markov chain monte carlo;mathematical optimization;monte carlo localization;peptide sequence;pierre robin syndrome;population parameter;protein family;sensor;sequence alignment;spectral clustering;statistical cluster	Juliana S. Bernardes;Fabio Vieira;Lygia Costa;Gerson Zaverucha	2014		10.1186/s12859-014-0445-4	correlation clustering;biology;dna microarray;fuzzy clustering;computer science;bioinformatics;data science;sequence analysis;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;genetics;biclustering;clustering high-dimensional data	Comp.	5.251345177128263	-51.02642407222147	161248
b00d356eebb97d96a3f8929bff09e27e03480f35	an hybrid multi-core/gpu-based mimetic algorithm for big association rule mining	association rule mining;big data;gpu algorithm;mimetic algorithm;multi-core algorithm	This paper addresses the problem of big association rule mining using an evolutionary approach. The mimetic method has been successfully applied to small and medium size databases. However, when applied on larger databases, the performance of this method becomes an important issue and current algorithms have very long execution times. Modern CPU/GPU architectures are composed of many cores, which are massively threaded and provide a large amount of computing power, suitable for improving the performance of optimization techniques. The parallelization of such method on GPU architecture is thus promising to deal with very large datasets in real time. In this paper, an approach is proposed where the rule evaluation process is parallelized on GPU, while the generation of rules is performed on a multi-core CPU. Furthermore, an intelligent strategy is proposed to partition the search space of rules in several independent sub-spaces to allow multiple CPU cores to explore the search space efficiently and without performing redundant work. Experimental results reveal that the suggested approach outperforms the sequential version by upto at 600 times for large datasets. Moreover, it outperforms the-state-of-the-art high performance computing based approaches when dealing with the big WebDocs dataset. © Springer Nature Singapore Pte Ltd. 2018.	algorithm;association rule learning;graphics processing unit;multi-core processor	Youcef Djenouri;Asma Belhadi;Philippe Fournier-Viger;Chun-Wei Lin	2017		10.1007/978-981-10-6487-6_8	architecture;machine learning;big data;multi-core processor;artificial intelligence;association rule learning;algorithm;supercomputer;computer science	ML	-3.217438699749903	-39.65603413663604	161257
2615aec253b99be03e413348712a7ebe3453b05b	computational intelligence techniques: a study of scleroderma skin disease	genomics;similarity structure preservation;differential evolution;genetic program;skin disease;computational intelligence;virtual reality;genetic programming;information content;particle swarm optimizer;visual data mining;clustering;virtual reality spaces;scleroderma disease;particle swarm optimization;data mining algorithm;characteristic function;structure preservation;rough sets;cross validation;rough set;optimal algorithm;grid computing;hybrid evolutionary classical optimization	This paper presents an analysis of microarray gene expression data from patients with and without scleroderma skin disease using computational intelligence and visual data mining techniques. Virtual reality spaces are used for providing unsupervised insight about the information content of the original set of genes describing the objects. These spaces are constructed by hybrid optimization algorithms based on a combination of Differential Evolution (DE) and Particle Swarm Optimization respectively, with deterministic Fletcher-Reeves optimization. A distributed-pipelined data mining algorithm composed of clustering and cross-validated rough sets analysis is applied in order to find subsets of relevant attributes with high classification capabilities. Finally, genetic programming (GP) is applied in order to find explicit analytic expressions for the characteristic functions of the scleroderma and the normal classes. The virtual reality spaces associated with the set of function arguments (genes) are also computed. Several small subsets of genes are discovered which are capable of classifying the data with complete accuracy. They represent genes potentially relevant to the understanding of the scleroderma disease.	broyden–fletcher–goldfarb–shanno algorithm;cluster analysis;computation;computational intelligence;data mining;differential evolution;fletcher's checksum;genetic programming;mathematical optimization;microarray;particle swarm optimization;rough set;self-information;unsupervised learning;virtual reality	Julio J. Valdés;Alan J. Barton	2007		10.1145/1274000.1274028	mathematical optimization;genomics;rough set;computer science;bioinformatics;artificial intelligence;machine learning;computational intelligence;data mining;mathematics;virtual reality	Comp.	7.2872641036881705	-47.65906877278545	161280
a67967bb32a2aaa4be09e252c53ec00f3ef0a6a0	probing the effect of conformational constraints on binding	graphics conformational constraint effect binding strength effective drug design binding entropy overall binding affinity receptor molecule complexes bond constraint constrained molecules entropy paradox molecular dynamics simulations single molecules genome high performance computing high performance theory computer modeling computer simulations complex biomolecular systems disease human health animations;molecular dynamics method bonds chemical chemistry computing macromolecules molecular biophysics molecular configurations;molecular configurations;chemistry computing;molecular dynamics method;biomedical engineering;data visualization biomedical engineering;molecular biophysics;data visualization;macromolecules;bonds chemical	"""Increasing the strength of binding between a molecule and a receptor is an important technique in the design of effective drugs. One experimental technique to increase the strength of binding (called """"binding affinity"""") is to synthesize molecules that are already in the shape that it will take when bound to a receptor. This technique works because it decreases the binding entropy which increases the overall binding affinity. A recent experimental study of a series of receptor-molecule complexes (the Grb2 SH2 domain with peptide analogues) aimed to increase the binding affinity by introducing a bond constraint. However, the constrained molecules had less favorable binding entropies than their flexible counterparts. Yue Shi of the Ren lab at UT Austin aimed to probe the origin of this entropy paradox with molecular dynamics simulations which were run on Lonestar and Ranger at TACC. Their group used approximately 2 million CPU hours on Ranger and almost 1 million on Lonestar this past year. Their research addresses biological and medical challenges from single molecules to the genome with high performance computing and theory. In collaboration with other experimental groups, they utilize computer modeling and simulations to understand these complex biomolecular systems and to discover molecules for treating disease and improving human health. Effectively communicating the results of their computational studies to experimentalists is essential to the success of their collaborative efforts. Anne Bowen of the Texas Advanced Computing Center collaborated with Yue Shi to prepare animations and graphics to better explain the origins of the """"entropy paradox"""" to experimentalists and the general public."""	central processing unit;computation;computer simulation;epr paradox;experiment;graphics;molecular dynamics;name binding;processor affinity;supercomputer;ranger	Anne Dara Bowen;Yue Shi	2012	2012 SC Companion: High Performance Computing, Networking Storage and Analysis	10.1109/SC.Companion.2012.334	macromolecule;computer science;bioinformatics;data visualization;molecular biophysics	HPC	-4.481091682165072	-51.124283228624215	161350
397f1c3f3c198a3b01d485e5ec7181449f5f536f	parameterized algorithms for clustering ppi networks.		With the advent of high-throughput wet lab technlogies the amount of protein interaction data available publicly has increased substantially, in turn spurring a plethora of computational methods for in silico knowledge discovery from this data. In this paper, we focus on parameterized methods for modeling and solving complex computational problems encountered in such knowledge discovery from protein data. Specifically, we concentrate on three relevant problems today in proteomics, namely detection of lethal proteins, functional modules and alignments from protein interaction networks. We propose novel graph theoretic models for these problems and devise practical parameterized algorithms. At a broader level, we demonstrate how these methods can be viable alternatives for the several heurestic, randomized, approximation and sub-optimal methods by arriving at parameterized yet optimal solutions for these problems. We substantiate these theoretical results by experimenting on real protein interaction data of S.cerevisiae(budding yeast) and verifying the results using gene ontology.	approximation algorithm;computational problem;database;emoticon;experiment;gene ontology;graph partition;graph theory;high-throughput computing;pixel density;proteomics;randomized algorithm;scalability;sequence alignment;throughput;usb hub;verification and validation	Sriganesh Srihari;Hon Wai Leong	2015	CoRR		computational problem;bioinformatics;artificial intelligence;machine learning;knowledge extraction;ontology;protein interaction networks;parameterized complexity;cluster analysis;algorithm;computer science;graph	ML	-0.7488474984081246	-49.35368652684769	161494
d73f64a54df966e9cd4796d41110b76384a7c78b	protein folding on the hexagonal lattice in the hp model	approximation algorithms;hp model;protein folding;heuristics	"""In this paper, we introduce the 2D hexagonal lattice as a biologically meaningful alternative to the standard square lattice for the study of protein folding in the HP model. We show that the hexagonal lattice alleviates the """"sharp turn"""" problem and models certain aspects of the protein secondary structure more realistically. We present a 1/6-approximation and a clustering heuristic for protein folding on the hexagonal lattice. In addition to these two algorithms, we also implement a Monte Carlo Metropolis algorithm and a branch-and-bound partial enumeration algorithm, and conduct experiments to compare their effectiveness."""	branch and bound;cluster analysis;experiment;heuristic;metropolis;metropolis–hastings algorithm;monte carlo method;protein folding;statistical cluster	Minghui Jiang;Binhai Zhu	2005	Journal of bioinformatics and computational biology	10.1142/S0219720005000850	crystallography;protein folding;biology;mathematical optimization;combinatorics;computer science;heuristics;mathematics;approximation algorithm	Comp.	0.1491791755128625	-49.73054705123569	161592
35bef1d7006d2b2d4a3b0df69bd74447e2433c77	the research of id strategy based on antigen presenting principle and its adaptability	cluster algorithm;pattern clustering;gaussian processes;pattern clustering data mining gaussian processes;k means;efficient k means clustering algorithm;satisfiability;data mining;gaussian dataset;initial points;clustering algorithms acceleration distributed computing merging pattern recognition partitioning algorithms sampling methods software engineering artificial intelligence software algorithms;clustering;influence factor clustering k means initial points;number of clusters;pattern recognition;selection effect;influence factor;similarity measure;k means clustering;gaussian dataset efficient k means clustering algorithm influence factor data mining pattern recognition similarity measure	Because of huge size of rule set, intrusion detection systems(IDSs) usually take dynamic update strategy which realizes all of rules to be replaced once in a period of time, which is costing times for spaces. Under such strategy, IDSs may not realize the flexible detection according to different states of network security. Based on the immune principle of antigen presenting, this paper proposes a mechanism of Abnormity Presenting and Abnormity Triggering, which means detecting only when need. Once any abnormal event captured, ID system generates corresponding detectors, which discards the traditional manner to generate detectors blindly and randomly. The experimental results demonstrate the proposed schemes can realize the flexible detection.	algorithm;intrusion detection system;network security;port triggering;randomness;sensor	Fuxiong Sun;Ping Xiong;Tao Sun	2007	Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)	10.1109/SNPD.2007.148	correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;dbscan;affinity propagation;k-means clustering;clustering high-dimensional data	SE	4.598578179405207	-38.122690183468585	161747
d1f35d95dcb487ee5672eec7880d5a730740b350	parallel area navigation enhanced information extraction algorithm based on massive historical gnss data and mapreduce	parallel computing;information extraction;data mining;satellites algorithm design and analysis satellite navigation systems receivers signal processing algorithms data mining heuristic algorithms;receivers;parallel computing auxiliary gnss mapreduce information extraction;satellite navigation data handling geographic information systems parallel programming;heuristic algorithms;satellites;mapreduce;auxiliary gnss;signal processing algorithms;multisatellite system pseudorange and navigation message parallel area navigation enhanced information extraction algorithm massive historical gnss data mapreduce navigation enhanced information gnss receivers static station observation data;algorithm design and analysis;satellite navigation systems	Navigation enhanced information (NEI) is very important to improve the positioning and navigation performance of GNSS receivers, and the current available NEI mainly come from static station observation data, which don't has enough credibility and area attribute. In this paper, an parallel area navigation enhanced information (ANEI) extraction algorithm based on massive historical GNSS data and MapReduce is proposed, to get the ANEI which is much more effective than traditional NEI, and the high efficiency of the algorithm is guaranteed by using the Hadoop parallel programming model MapReduce. The principle of the algorithm is as follows: a large number of historical GNSS Intermediate Frequency data is divided into blocks to be acquired and tracked parallel to get massive multi satellite system pseudorange and navigation message (MPD), then, the massive MPD are weighted and fused parallel by using variance component estimation to get the NEI which will be corresponded to the corresponding location coordinate, completing the parallel extraction of ANEI. Experimental results show that the positioning error of GNSS receivers is reduced about 18.24% by using ANEI instead of traditional NEI, and the execution time of the algorithm is reduced about 46.72% by using MapReduce, so the algorithm proposed in this paper is more reliable and effective.	algorithm;apache hadoop;fault tolerance;information extraction;intermediate frequency;mapreduce;parallel computing;parallel programming model;pseudorange;random effects model;run time (program lifecycle phase);satellite navigation;signal processing	Deng-ao Li;Gang Wu;Ju-Min Zhao;Wen-Hui Niu;Shuai Li	2015	2015 Third International Conference on Advanced Cloud and Big Data	10.1109/CBD.2015.18	embedded system;gnss augmentation;computer science;theoretical computer science;database	HPC	-3.034366940769676	-38.049712751444275	161915
3ec4aa5ce4527d01f99879c51d1377c9623897e1	classification by rough set reducts, adaboost and svm	document handling;adaboost classification rough set reducts svm;information systems;support vector machines;support vector machines support vector machine classification information systems software engineering artificial intelligence distributed computing greedy algorithms rough sets data analysis machine learning;support vector machines document handling greedy algorithms learning artificial intelligence pattern classification rough set theory;rough set theory;distributed computing;greedy algorithms;classification;software engineering;training data;accuracy;indexes;data analysis;machine learning;classification algorithms;adaboost;pattern classification;greedy algorithm;rough sets;artificial intelligence;support vector machine classification;k nearest neighbor;svm;support vector machine;learning artificial intelligence;rough set;rough set reducts;classification rough set reducts adaboost svm greedy algorithm k nearest neighbor support vector machine	Most classification studies are done by using all the objects data. It is expected to classify objects by using some subsets data in the total data. A rough set based reduct is a minimal subset of features, which has almost the same discernible power as the entire conditional features. Here, we propose a greedy algorithm to compute a set of rough set reducts which is followed by the k-nearest neighbor to classify documents. To improve the classification performance, reducts-kNN with confidence was developed. These proposed rough set reduct based methods are compared with the classification by AdaBoost and SVM(Support Vector Machine) methods. Experiments have been conducted on some benchmark datasets from the Reuters 21578 data set.	adaboost;benchmark (computing);greedy algorithm;k-nearest neighbors algorithm;rough set;support vector machine	Naohiro Ishii;Yuichi Morioka;Shinichi Suyama;Yongguang Bao	2010	2010 11th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2010.19	statistical classification;support vector machine;rough set;computer science;machine learning;pattern recognition;data mining	DB	8.295868598056598	-39.28093444671951	161985
6a54106b7feb419987ac50f00ff50afb77de19d3	feature selection of intrusion detection data using a hybrid genetic algorithm/knn approach	intrusion detection;information processing;system development;feature selection;k nearest neighbour;classification accuracy;hybrid genetic algorithm	Feature selection is an important part of the information processing and system development process. The selection of an appropriate set of features can provide an insight into the underlying processes present in the data set and greatly improve the accuracy of the overall classification model. In this paper we investigate the use of a hybrid genetic algorithm/k-nearest neighbour approach to features selection and apply this approach to an intrusion detection data set. We have found that this feature selection process is able to identify features that are important for identifying different types of attacks present in the, data set leading to improved classification accuracy.	feature selection;genetic algorithm;intrusion detection system;k-nearest neighbors algorithm	Melanie Middlemiss;Grant Dick	2003			engineering;machine learning;pattern recognition;data mining;feature	Security	8.053570883001905	-38.37965499333653	161996
0c75368d0f46c42c340ad023d9ae07511f8ce505	large-scale parallel simulations of 3d cell colony dynamics	microprocessors;biology computing;high performance computing;computational modeling biological system modeling mathematical model computer architecture microprocessors density functional theory parallel processing;tumours bioinformatics cellular biophysics digital simulation parallel processing;biological system modeling;tumours;tumor growth large scale parallel simulations 3d cell colony dynamics biological processes heuristic experimental approach mathematical modeling computer simulations clinically detectable tumor sizes high performance computational approach cell colony simulation;parallel scalability;density functional theory;computer architecture;computational modeling;hpc;scientific computing bioinformatics parallel scalability high performance computing hpc;tumors;scientific computing;mathematical model;cellular biophysics;parallel processing;digital simulation;bioinformatics	Biological processes are inherently complex and involve many unknown relationships and mechanisms at different scales. Despite many efforts, researchers still can't explain all of the observed phenomena and, if necessary, make any desirable changes in the dynamics. Recently, it has become apparent that a research opportunity lies in complementing the traditional, heuristic experimental approach with mathematical modeling and computer simulations. Achieving a simulation scale that corresponds, for instance, to clinically detectable tumor sizes is still a huge challenge, however, this scale is necessary to understand and control complex biological processes. This article presents a novel, high-performance computational approach allowing simulations of 3D cell colony dynamics at a previously unavailable tissue scale. Due to its high parallel scalability, the method achieves simulation of cell colonies composed of 109 cells, which allows for describing tumor growth in its early clinical stage.	computation;computer simulation;de novo transcriptome assembly;emoticon;grand challenges;heuristic;kinetics internet protocol;mathematical model;scalability	Maciej Cytowski;Zuzanna Szymanska	2014	Computing in Science & Engineering	10.1109/MCSE.2014.2	computational science;parallel processing;supercomputer;parallel computing;simulation;computer science;bioinformatics;theoretical computer science;mathematical model;density functional theory;computational model	HPC	-4.4055368333038025	-50.95496765991355	162262
8ba148282813c266aab4917fb14c42a3ba676921	ascertaining spam web pages based on ant colony optimization algorithm		Web spam is troubling both internet users and search engine companies, because it seriously damages the reliability of search engine and the benefit of Web users, degrades the Web information quality. This paper discusses a Web spam detection method inspired by Ant Colony Optimization (ACO) algorithm. The approach consists of two stages: preprocessing and Web spam detection. On preprocessing stage, the class-imbalance problem is solved by using a clustering technique and an optimal feature subset is culled by Chi-square statistics. The dataset is also discretized based on the information entropy method. These works make the spam detection at the second stage more efficient and easier. On next stage, spam detection model is built based on the ant colony optimization algorithm. Experimental results on the WEBSPAM-UK2006 reveal that our approach can achieve the same or even better results with less number of features.	algorithm;ant colony optimization algorithms	Shou-Hong Tang;Yan Zhu;Fan Yang;Qing Xu	2014		10.1007/978-3-319-10085-2_21	artificial intelligence;data mining;world wide web	Theory	5.747453110978643	-38.62746875285354	163069
1cef882b465d55b41b6e179506ee96af94ce7969	a closed patterns-based approach to the consensus clustering problem. (une approche basée sur les motifs fermés pour résoudre le problème de clustering par consensus)		Clustering is the process of partitioning a dataset into groups, so that the instances in the same group are more similar to each other than to instances in any other group. Many clustering algorithms were proposed, but none of them proved to provide good quality partition in all situations. Consensus clustering aims to enhance the clustering process by combining different partitions obtained from different algorithms to yield a better quality consensus solution. In this work, a new consensus clustering method, called MultiCons, is proposed. It uses the frequent closed itemset mining technique in order to discover the similarities between the different base clustering solutions. The identified similarities are presented in a form of clustering patterns, that each defines the agreement between a set of base clusters in grouping a set of instances. By dividing these patterns into groups based on the number of base clusters that define the pattern, MultiCons generates a consensus solution from each group, resulting in having multiple consensus candidates. These different solutions are presented in a tree-like structure, called ConsTree, that facilitates understanding the process of building the multiple consensuses, and also the relationships between the data instances and their structuring in the data space. Five consensus functions are proposed in this work in order to build a consensus solution from the clustering patterns. Approach 1 is to just merge any intersecting clustering patterns. Approach 2 can either merge or split intersecting patterns based on a proposed measure, called intersection ratio. Approach 3 differs from the previous approaches by searching for the best similar pattern before making a merge/split decision, and, in addition, it uses the average intersection ratio. While approach 3 works sequentially on the clustering patterns, approach 4 uses a similarity matrix of intersection ratios to search for the best merge/split. Approach 5 is a simple graph partitioning process to build clusters of clustering patterns. These five approaches are tested with many benchmark datasets to compare their performance on different clustering problems.		Atheer Al-Najdi	2016				ML	1.4042810355914308	-42.06916912418515	163261
e792231c0f172754007951558388ed49393044f2	a tractable variant of the single cut or join distance with duplicated genes		In this work, we introduce a variant of the Single Cut or Join distance that accounts for duplicated genes, in the context of directed evolution from an ancestral genome to a descendant genome where orthology relations between ancestral genes and their descendant are known. Our model includes two duplication mechanisms: single-gene tandem duplication and creation of single-gene circular chromosomes. We prove that in this model, computing the distance and a parsimonious evolutionary scenario in terms of SCJ and single-gene duplication events can be done in linear time. Simulations show that the inferred number of cuts and joins scales linearly with the true number of such events even at high rates of genome rearrangements and segmental duplications. We also show that the median problem is tractable for this distance.	cobham's thesis;computer simulation;occam's razor;tandem computers;time complexity	Pedro Feijão;Aniket C. Mane;Cédric Chauve	2017		10.1007/978-3-319-67979-2_2	gene;segmental duplication;bioinformatics;genetics;genome;chromosome;descendant;biology;gene duplication;tandem exon duplication;directed evolution	ML	0.9518591794946402	-51.34376615202362	163283
983333d699242dfc221ff332081f31d8a18ae179	"""performance characterization of state-of-the-art deep learning workloads on an ibm """"minsky"""" platform"""		Deep learning algorithms are known to demand significant computing horsepower, in particular when it comes to training these models. The capability of developing new algorithms and improving the existing ones is in part determined by the speed at which these models can be trained and tested. One alternative to attain significant performance gains is through hardware acceleration. However, deep learning has evolved into a large variety of models, including but not limited to fully-connected, convolutional, recurrent and memory networks. Therefore, it appears difficult that a single solution can provide effective acceleration for this entire deep learning ecosystem. This work presents detailed characterization results of a set of archetypal state-of-the-art deep learning workloads on a last-generation IBM POWER8 system with NVIDIA Tesla P100 GPUs and NVLink interconnects. The goal is to identify the performance bottlenecks (i.e. the accelerable portions) to provide a thorough study that can guide the design of prospective acceleration platforms in a more effective manner. In addition, we analyze the role of the GPU (as one particular type of acceleration engine) and its effectiveness as a function of the size of the problem. This research was developed, in part, with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.	algorithm;deep learning;ecosystem;electrical connection;graphics processing unit;hardware acceleration;machine learning;nvlink;nvidia tesla;prospective search	Mauricio Guignard;Marcelo Schild;Carlos S. Bederián;Nicolás Wolovick;Augusto J. Vega	2018			software engineering;knowledge management;computer science;deep learning;ibm;artificial intelligence	ML	-1.7117717410011666	-45.399615424945765	163458
a63de96270dbcce58e02cb34e5532699a520c888	locality in continuous fitness-valued cases and genetic programming difficulty		It is commonly accepted that a mapping is local if it preserves neighbourhood. In Evolutionary Computation, locality is generally described as the property that neighbouring genotypes correspond to neighbouring phenotypes. Locality has been classified in one of two categories: high and low locality. It is said that a representation has high locality if most genotypic neighbours correspond to phenotypic neighbours. The opposite is true for a representation that has low locality. It is argued that a representation with high locality performs better in evolutionary search compared to a representation that has low locality. In this work, we explore, for the first time, a study on Genetic Programming (GP) locality in continuous fitnessvalued cases. For this, we extended the original definition of locality (first defined and used in Genetic Algorithms using bitstrings) from genotype-phenotype mapping to the genotype-fitness mapping. Then, we defined three possible variants of locality in GP regarding neighbourhood. The experimental tests presented here use a set of symbolic regression problems, two different encoding and two different mutation operators. We show how locality can be studied in this type of scenarios (continuous fitness-valued cases) and that locality can successfully been used as a performance prediction tool. Edgar Galvan Distributed Systems Group, School of Computer Science and Statistics, Trinity College Dublin, Ireland e-mail: edgar.galvan@scss.tcd.ie Leonardo Trujillo Departamento de Ingenierı́a Eléctrica y Electrónica, Instituto Tecnológico de Tijuana, Tijuana, BC, México e-mail: leonardo.trujillo.ttl@gmail.com		Edgar Galván López;Leonardo Trujillo;James McDermott;Ahmed Kattan	2012		10.1007/978-3-642-31519-0_3	machine learning	AI	8.735930792179671	-41.875760756781524	163664
520fc047b6193f86118cdf5dc1df97eabea6ddee	a novel clustering and verification based microarray data bi-clustering method	microarray data;cluster algorithm;k means;gene expression data;data clustering;bicluster detection;clustering method	Microarray data biclustering is very important for the research on gene regulatory mechanisms. Genes which exhibit similar patterns are often functionally related. In this paper a novel bicluster detection method is proposed. It makes use of one of the existing traditional clustering algorithms such as K-means as an intermediate tool to do data clustering with the submatrices created from the original data matrix. Especially, in order to save the memory storage requirement, reduce the useless clustering processing and accelerate the bicluster detection speed, a clustering and verification combined algorithm is applied. The former helps to find out the row numbers where possible biclusters lie in, while the latter efficiently speed up the detection processing. Based on a characteristic of bicluster, the biclusters are detected one by one. At the end of the paper experiment with the simulated data are presented.	cluster analysis;microarray	Yanjie Zhang;Hong Wang;Zhanyi Hu	2010		10.1007/978-3-642-13498-2_80	correlation clustering;microarray analysis techniques;fuzzy clustering;computer science;bioinformatics;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;cluster analysis;biclustering;k-means clustering;clustering high-dimensional data	EDA	4.759883003520502	-48.00937928085364	164091
a2d2b7bb3496c51acecdf3e3574278dfbf17174b	transaction aggregation as a strategy for credit card fraud detection	performance measure;high dimensionality;computer science and information systems;supervised classification;logistic regression;transaction data;random forest;preprocessing;fraud detection;credit cards;transaction level	The problem of preprocessing transaction data for supervised fraud classification is considered. It is impractical to present an entire series of transactions to a fraud detection system, partly because of the very high dimensionality of such data but also because of the heterogeneity of the transactions. Hence, a framework for transaction aggregation is considered and its effectiveness is evaluated against transaction-level detection, using a variety of classification methods and a realistic cost-based performance measure. These methods are applied in two case studies using real data. Transaction aggregation is found to be advantageous in many but not all circumstances. Also, the length of the aggregation period has a large impact upon performance. Aggregation seems particularly effective when a random forest is used for classification. Moreover, random forests were found to perform better than other classification methods, including SVMs, logistic regression and KNN. Aggregation also has the advantage of not requiring precisely labeled data and may be more robust to the effects of population drift.	credit card fraud;k-nearest neighbors algorithm;logistic regression;online transaction processing;preprocessor;random forest;transaction data	Christopher Whitrow;David J. Hand;Piotr Juszczak;David John Weston;Niall M. Adams	2008	Data Mining and Knowledge Discovery	10.1007/s10618-008-0116-z	random forest;computer science;data science;machine learning;transaction data;data mining;logistic regression;computer security;preprocessor	ML	9.122043728323076	-38.88441899016975	164114
dba8bcacf31b111a9bf36b1833dada16ef2c80d7	student behavior clustering method based on campus big data		Nowadays, a large amount of valuable data have been accumulated. According to the big data from the management system of university, we attempt to subdivide students' behavior into different groups from various aspects, so as to identifying the different groups of students. Given this, this paper can get the characteristics of students from different groups. In this way, universities can know students well and manage them reasonably. First, in order to solve the segmentation of student behavior, this paper presents a set of description index system of student behavior and the segmentation model of student behavior based on clustering analysis. Meanwhile, in order to obtain more accurate clustering results, the traditional K-Means clustering algorithm is improved from the selection of the initial clustering center and the number of clusters. In addition, the improved method is parallelized on the Spark platform and applied to subdivide student behavior into different groups. Finally, experiments are conducted to verify the reliability of the results.	algorithm;apache spark;big data;cluster analysis;experiment;k-means clustering;parallel computing	Dong Ding;Junhuai Li;Huaijun Wang;Zhu Liang	2017	2017 13th International Conference on Computational Intelligence and Security (CIS)	10.1109/CIS.2017.00116	computer science;artificial intelligence;machine learning;cluster analysis;big data;management system;spark (mathematics);algorithm design;data modeling	Robotics	-2.7491117431370466	-38.7332087977139	164125
35766bdbda1e9935d0b8b39c59419fd5c159ecf3	strategies for network motifs discovery	network motifs discovery;software;complex networks;computer networks complex networks biology computing sociology sequences proteins taxonomy neuroscience network address translation runtime;helium;probability density function;data mining algorithm theory;complex network;network motif;workflow bottleneck;data mining;graph mining;graph isomorphism;accuracy;proteins;network motifs;algorithm theory;e science data sets;motif detection algorithm;motif detection algorithm network motifs discovery e science data sets workflow bottleneck subgraph mining graph isomorphism;subgraph mining;detection algorithm;complex networks network motifs graph mining algorithms;algorithms	"""Complex networks from domains like Biology or Sociology are present in many e-Science data sets. Dealing with networks can often form a workflow bottleneck as several related algorithms are computationally hard. One example is detecting characteristic patterns or """"network motifs"""" - a problem involving subgraph mining and graph isomorphism. This paper provides a review and runtime comparison of current motif detection algorithms in the field. We present the strategies and the corresponding algorithms in pseudo-code yielding a framework for comparison. We categorize the algorithms outlining the main differences and advantages of each strategy. We finally implement all strategies in a common platform to allow a fair and objective efficiency comparison using a set of benchmark networks. We hope to inform the choice of strategy and critically discuss future improvements in motif detection."""	algorithm;benchmark (computing);categorization;common platform;control flow;e-science;enumerated type;flow network;graph (discrete mathematics);graph isomorphism;motif;open research;pseudocode;sensor	Pedro Manuel Pinto Ribeiro;Fernando M. A. Silva;Marcus Kaiser	2009	2009 Fifth IEEE International Conference on e-Science	10.1109/e-Science.2009.20	computer science;bioinformatics;machine learning;data mining	DB	-0.9763334185388766	-48.830161601372055	164161
6720e59a053d073985e2eb7c5746721fa907e0d6	capped robust k-means algorithm		K-means algorithm is a classical algorithm and has been widely used in many applications. However, the traditional K-means algorithm is easily influenced by outliers and it usually obtains an unstable clustering result and poor clustering accuracy. In this paper, aiming at K-means algorithm resistant to outliers, we proposed a Capped Robust K-means Algorithm (CRK-means) by adding a capped norm and an efficient iteration re-weighted algorithm to solve the problem. Experiment results on the simulation data set and five real data sets demonstrate that the proposed algorithm can improve the accuracy of clustering and reduce the influence of outliers.	algorithm;cluster analysis;control theory;iteration;k-means clustering;signal-to-noise ratio;simulation	Shaobo Zhang;Fang Yuan;Liu Yang	2017	2017 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2017.8107757	artificial intelligence;robustness (computer science);machine learning;outlier;k-means clustering;pattern recognition;algorithm design;cluster analysis;computer science;linear programming;data modeling;data set	ML	4.563802759666254	-41.32114268423896	164394
9013ce1281a76f49c8bb6249bd4949fd1f3bdc11	semi-supervised clustering algorithm for haplotype assembly problem based on mec model	minimum error correction;k means;machine learning;mec;haplotype assembly;semi supervised clustering;bioinformatics	Haplotype assembly is to infer a pair of haplotypes from localized polymorphism data. In this paper, a semi-supervised clustering algorithm-SSK (semi-supervised K-means) is proposed for it, which, to our knowledge, is the first semi-supervised clustering method for it. In SSK, some positive information is firstly extracted. The information is then used to help k-means to cluster all SNP fragments into two sets from which two haplotypes can be reconstructed. The performance of SSK is tested on both real data and simulated data. The results show that it outperforms several state-of-the-art algorithms on minimum error correction (MEC) model.	algorithm;cluster analysis;code;computation;constrained clustering;error detection and correction;experiment;extraction;haplotypes;inference;k-means clustering;microsoft fingerprint reader;natural science disciplines;nitroprusside;preprocessor;snp array;semi-supervised learning;semiconductor industry;serial digital video out;simulation;software release life cycle;x image extension;error correction;statistical cluster	Xin-Shun Xu;Ying-Xin Li	2012	International journal of data mining and bioinformatics	10.1504/IJDMB.2012.049279	computer science;bioinformatics;machine learning;data mining;cluster analysis;single-linkage clustering;k-means clustering	ML	4.520011726089265	-49.10050637642526	164505
0f2a46be6830943b58a656493764e9c588d01b03	approximation algorithms for bregman co-clustering and tensor clustering	approximate algorithm;k means;science learning;power generation;data structure	In the past few years powerful generalizations to the Euclidean k-means problem have been made. Some examples include Bregman clustering [7], co-clustering (i.e., simultaneous clustering of rows and columns of an input matrix) [9, 17], and tensor clustering [8, 30]. Like k-means, these more general problems also suffer from the NP-hardness of the associated optimization. Researchers have developed approximation algorithms of varying degrees of sophistication for k-means, k-medians, and more recently also for Bregman clustering [2]. However, there seem to be no approximation algorithms for Bregman coand tensor clustering. In this paper we derive the first (to our knowledge) guaranteed methods for these increasingly important clustering settings. Our experiments indicate that our results also have some practical impact.	approximation algorithm;biclustering;bregman divergence;cluster analysis;column (database);experiment;k-means clustering;k-medians clustering;mathematical optimization;np-hardness;order of approximation;subroutine	Stefanie Jegelka;Suvrit Sra;Arindam Banerjee	2008	CoRR		correlation clustering;electricity generation;constrained clustering;mathematical optimization;combinatorics;data structure;computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;mathematics;cluster analysis;programming language;biclustering;statistics;bregman divergence;k-means clustering;clustering high-dimensional data	ML	-0.07826854486746897	-40.5976006624722	164538
6fb1f86623502b417f37ff5bfe7f46ec722b7594	semi interactive method for data mining	interactive method;data mining	Usual visualization techniques for multidimensional data sets, such as parallel coordinates and scatter-plot matrices, do not scale well to high numbers of dimensions. A common approach to solve this problem is dimensionality selection. We present new semi-interactive method for dimensionality selection to select pertinent dimension subsets without losing information. Our cooperative approach uses automatic algorithms, interactive algorithms and visualization methods: an evolutionary algorithm is used to obtain optimal dimension subsets which represent the original data set without losing information for unsupervised tasks (clustering or outlier detection) using a new validity criterion. A visualization method is used to present the user interactive evolutionary algorithm results and let him actively participate in evolutionary algorithm search with more efficiency resulting in a faster evolutionary algorithm convergence. We have implemented our approach and applied it to real data set to confirm it is effective for supporting the user in the exploration of high dimensional data sets and evaluate the visual data representation.	algorithmic trading;anomaly detection;cluster analysis;clustering high-dimensional data;data (computing);data mining;data visualization;evolutionary algorithm;experiment;fitness function;genetic algorithm;interactive evolutionary computation;interactivity;k-means clustering;local optimum;numerical analysis;parallel coordinates;relevance;semiconductor industry;unsupervised learning	Lydia Boudjeloud;François Poulet	2006			computer science;data mining;data stream mining	ML	2.958884552456359	-43.742275133132544	164768
421998cc4274931db930359772b4d10ed4b0ce35	a new line symmetry distance and its application to data clustering	distance measure;genetics;data clustering;face recognition;clustering;kd tree;line symmetry based distance;unsupervised classification;genetic algorithm;nearest neighbor search;sriparna saha sanghamitra bandyopadhyay 距离度量 数据聚类 线对称 应用 集群技术 技术开发 自适应变异 聚类技术 a new line symmetry distance and its application to data clustering;symmetry property;k means clustering	In this paper, at first a new line-symmetry-based distance is proposed. The properties of the proposed distance are then elaborately described. Kd-tree-based nearest neighbor search is used to reduce the complexity of computing the proposed line-symmetry-based distance. Thereafter an evolutionary clustering technique is developed that uses the new line-symmetry-based distance measure for assigning points to different clusters. Adaptive mutation and crossover probabilities are used to accelerate the proposed clustering technique. The proposed GA with line-symmetry-distance-based (GALSD) clustering technique is able to detect any type of clusters, irrespective of their geometrical shape and overlapping nature, as long as they possess the characteristics of line symmetry. GALSD is compared with the existing well-known K-means clustering algorithm and a newly developed genetic point-symmetry-distance-based clustering technique (GAPS) for three artificial and two real-life data sets. The efficacy of the proposed line-symmetry-based distance is then shown in recognizing human face from a given image.	algorithm;cluster analysis;k-means clustering;nearest neighbor search;real life;software release life cycle	Sriparna Saha;Sanghamitra Bandyopadhyay	2009	Journal of Computer Science and Technology	10.1007/s11390-009-9244-1	facial recognition system;complete-linkage clustering;correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;dendrogram;clustering high-dimensional data	AI	3.4848104702635383	-42.39284522996458	164782
183a4e3c1fcd261a58e81d06f0ecf50de4cb899e	exploring the evolution of a trade-off between vigilance and foraging in group-living organisms	biological patents;biomedical journals;text mining;europe pubmed central;citation search;many eyes hypothesis;citation networks;tragedy of the commons;research articles;abstracts;genetic relatedness;open access;life sciences;clinical guidelines;reproductive strategy;full text;anti predator vigilance;rest apis;orcids;europe pmc;biomedical research;group foraging;bioinformatics;literature search	Even though grouping behaviour has been actively studied for over a century, the relative importance of the numerous proposed fitness benefits of grouping remain unclear. We use a digital model of evolving prey under simulated predation to directly explore the evolution of gregarious foraging behaviour according to one such benefit, the 'many eyes' hypothesis. According to this hypothesis, collective vigilance allows prey in large groups to detect predators more efficiently by making alarm signals or behavioural cues to each other, thereby allowing individuals within the group to spend more time foraging. Here, we find that collective vigilance is sufficient to select for gregarious foraging behaviour as long there is not a direct cost for grouping (e.g. competition for limited food resources), even when controlling for confounding factors such as the dilution effect. Furthermore, we explore the role of the genetic relatedness and reproductive strategy of the prey and find that highly related groups of prey with a semelparous reproductive strategy are the most likely to evolve gregarious foraging behaviour mediated by the benefit of vigilance. These findings, combined with earlier studies with evolving digital organisms, further sharpen our understanding of the factors favouring grouping behaviour.	collective intelligence;data visualization;digital organism;direct costs;eye;mass effect trilogy;prey;reproduction;xslt/muenchian grouping;benefit	Randal S. Olson;Patrick B. Haley;Fred C. Dyer;Christoph Adami	2015		10.1098/rsos.150135	biology;text mining;bioinformatics;ecology;tragedy of the commons	HCI	-4.386639987338446	-46.237026490629276	164797
1f9c1ecdafc8f7560801fc5c6e92d5bf84a704f5	data validation algorithm for wireless sensor networks		This paper presents a novel data validation algorithm for wireless sensor network. We applied qualitative methods such as heuristic rule, temporal correlation, spatial correlation, Chauvenet’s criterion, and modified z-score as algorithms for validating sensor data samples for faults. Performance of the algorithms is evaluated using real data samples of WSNs prototype for environment monitoring injected with different types of data faults such as out-of-range faults, struck-at faults, and outliers and spike faults. Results show heuristic rule, temporal correlation, spatial correlation, chauvenet’s criterion, and modified z-score method sit at different point on accuracy, no single method is perfect in detecting different types of data faults and reports false positives when sensor data samples contain different types of data faults. Selected effective methods such as heuristic rule, temporal correlation, and modified z-score are applied successively to data set for detecting different types of data faults but report false positives due to masking effects and increased fault rate. Finally we propose a novel data validation algorithm that uses novel approach in applying heuristic rule, temporal correlation, and modified z-score to data set for detecting different types of data faults. Compared to other methods, the proposed novel data validation algorithm is effective in detecting different types of data faults and reports high fault detection rate by eliminating false positives.	algorithm;data validation;fault detection and isolation;heuristic;prototype;rule 184;sensor	Jaichandran Ravichandran;Anthony Irudhayaraj Arulappan	2013	IJDSN	10.1155/2013/634278	computer science;machine learning;data mining	SE	6.755219929121029	-39.62394929554537	164844
7a5471b3889515a1426c6481af82f5ee1cf84861	a practical parameterised algorithm for the individual haplotyping problem mlf	computacion informatica;ciencias basicas y experimentales;matematicas;grupo a	Haplotypes are more useful in complex disease gene mapping than single-nucleotide polymorphisms (SNPs). However, haplotypes are difficult to obtain directly using biological experiments, which has prompted research into efficient computational methods for determining haplotypes. The individual haplotyping problem called Minimum Letter Flip (MLF) is a computational problem that, given a set of aligned DNA sequence fragment data of an individual, induces the corresponding haplotypes by flipping minimum SNPs. There has been no practical exact algorithm for solving the problem. Due to technical limits in DNA sequencing experiments, the maximum length of a fragment sequenced directly is about 1kb. In consequence, with a genome-average SNP density of 1.84 SNPs per 1 kb of DNA sequence, the maximum number k1 of SNP sites that a fragment covers is usually small. Moreover, in order to save time and money, the maximum number k2 of fragments that cover an SNP site is usually no more than 19. Building on these fragment data properties, the current paper introduces a new parameterised algorithm with running time O(nk22k2 + mlogm + mk1), where m is the number of fragments and n is the number of SNP sites. In practical biological applications, the algorithm solves the MLF problem efficiently even if m and n are large.	algorithm	Minzhu Xie;Jianxing Wang;Jianer Chen	2010	Mathematical Structures in Computer Science	10.1017/S096012951000023X	computer science;bioinformatics;algorithm	Theory	0.39536583278978094	-51.567730938710426	164939
ad44bf376ba4bd87b4b579cb6668678d732f3975	biclustering as strategy for improving feature selection in consensus qsar modeling		Abstract Feature selection applied to QSAR (Quantitative Structure-Activity Relationship) modeling is a challenging combinatorial optimization problem due to the high dimensionality of the chemical space associated with molecules and the complexity of the physicochemical properties usually studied in Cheminformatics. This derives commonly in classification models with a large number of variables, decreasing the generalization and interpretability of these classifiers. In this paper, a novel strategy based on biclustering analysis is proposed for addressing this problem. The new method is applied as a post-processing step for feature selection outputs generated by consensus feature selection methods. The approach was evaluated using datasets oriented to ready biodegradation prediction of chemical compounds. These preliminary results show that biclustering can help to identify features with low class-discrimination power, which it is useful for reducing the complexity of QSAR models without losing prediction accuracy.	biclustering;feature selection;quantitative structure–activity relationship	María Jimena Martínez;Julieta Sol Dussaut;Ignacio Ponzoni	2018	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2018.07.016	discrete mathematics;cheminformatics;feature selection;chemical space;machine learning;combinatorial optimization;curse of dimensionality;biclustering;interpretability;mathematics;quantitative structure–activity relationship;artificial intelligence	ML	8.295460912815974	-48.62903653056957	164987
253a2b1a145341af7dfd9dee4021fccb1afb3435	a new evolutionary gene selection technique	support vector machines biology computing evolutionary computation pattern classification stochastic processes;kernel;support vector machines;cancer;differential evolution microarray gene selection gene ontology;lungs;gene ontology identification evolutionary gene selection technique microarray technology gene expression levels high dimensional dataset analysis discriminative selection biological processes student t stochastic neighbor embedding t sne differential evolution de support vector machine svm classification task optimization problem fixed length partitions microarray datasets homo sapien cancerous tissues statistical test;accuracy;tumors;support vector machines accuracy cancer tumors lungs kernel signal to noise ratio;signal to noise ratio	Microarray technology allows to investigate gene expression levels by analyzing high dimensional datasets of few samples. Selection of discriminative, differentially expressed genes from such datasets is important to differentiate, prognose and understand the underlying biological processes. In this regard, the paper presents a new evolutionary gene selection method based on Student-t Stochastic Neighbor Embedding (t-SNE), Differential Evolution (DE) and Support Vector Machine (SVM). Here the underlying classification task of SVM is used as an optimization problem of DE, while t-SNE provides better ordering of genes for selection purpose. Generally, t-SNE is used to reorder the genes in such a way so that similar genes are grouped together and dissimilar genes are kept further apart. These reordered genes are then fragmented into fixed-length partitions. Thereafter, from each partition, a gene is selected randomly to encode the initial population of DE along with the combination of its weight and threshold values in order to participate in fitness computation. In the final generation of DE, a subset of genes is selected based on higher classification accuracy. The proposed technique is tested on six publicly available microarray datasets concerning various cancerous tissues of Homo sapiens and yields a potential set of genes by providing prefect or nearly perfect classification accuracy. Moreover, the superiority of the proposed technique has been demonstrated in comparison with other widely used techniques. Finally, the achieved results have also been justified by a statistical test and allowed us to draw biological conclusions through the identification of Gene Ontologies.	computation;cross-validation (statistics);differential evolution;encode;experiment;feature selection;gene ontology term enrichment;gene expression programming;interaction;local search (optimization);mathematical optimization;memetic algorithm;memetics;microarray;ontology (information science);optimization problem;overfitting;particle swarm optimization;pixel density;program optimization;randomness;support vector machine;t-distributed stochastic neighbor embedding	Adrian Lancucki;Indrajit Saha;Piotr Lipinski	2015	2015 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2015.7257080	support vector machine;kernel;computer science;bioinformatics;machine learning;pattern recognition;accuracy and precision;signal-to-noise ratio;statistics;cancer	Comp.	8.344848872378742	-47.16579600221479	165226
37b414cfd0f0c5a9f7ff7d4c3201179ae26d60ef	comparisons and validation of statistical clustering techniques for microarray gene expression data	budding yeast;microarray data;hierarchical clustering;cluster algorithm;expression profile;perforation;partial least square;large data sets;gene expression data;chip;clustering method;temporal pattern;number of clusters;model based clustering;biological process	MOTIVATION With the advent of microarray chip technology, large data sets are emerging containing the simultaneous expression levels of thousands of genes at various time points during a biological process. Biologists are attempting to group genes based on the temporal pattern of their expression levels. While the use of hierarchical clustering (UPGMA) with correlation 'distance' has been the most common in the microarray studies, there are many more choices of clustering algorithms in pattern recognition and statistics literature. At the moment there do not seem to be any clear-cut guidelines regarding the choice of a clustering algorithm to be used for grouping genes based on their expression profiles.   RESULTS In this paper, we consider six clustering algorithms (of various flavors!) and evaluate their performances on a well-known publicly available microarray data set on sporulation of budding yeast and on two simulated data sets. Among other things, we formulate three reasonable validation strategies that can be used with any clustering algorithm when temporal observations or replications are present. We evaluate each of these six clustering methods with these validation measures. While the 'best' method is dependent on the exact validation strategy and the number of clusters to be used, overall Diana appears to be a solid performer. Interestingly, the performance of correlation-based hierarchical clustering and model-based clustering (another method that has been advocated by a number of researchers) appear to be on opposite extremes, depending on what validation measure one employs. Next it is shown that the group means produced by Diana are the closest and those produced by UPGMA are the farthest from a model profile based on a set of hand-picked genes.   AVAILABILITY S+ codes for the partial least squares based clustering are available from the authors upon request. All other clustering methods considered have S+ implementation in the library MASS. S+ codes for calculating the validation measures are available from the authors upon request. The sporulation data set is publicly available at http://cmgm.stanford.edu/pbrown/sporulation	algorithm;biological processes;choose (action);code;diana (intermediate language);gene expression;hierarchical clustering;integrated circuit;microarray;partial least squares regression;pattern recognition;performance;saccharomycetales;upgma;sporulation;statistical cluster	Susmita Datta;Somnath Datta	2003	Bioinformatics	10.1093/bioinformatics/btg025	chip;correlation clustering;biology;microarray analysis techniques;determining the number of clusters in a data set;gene chip analysis;k-medians clustering;fuzzy clustering;flame clustering;computer science;bioinformatics;data science;consensus clustering;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;biological process;biclustering;dendrogram;affinity propagation;statistics;clustering high-dimensional data	Comp.	4.921266996576954	-51.317360440873145	165567
bdf09a80f387441285cc49e10606bf0362e2f980	a hierarchical clustering algorithm using strong components	hierarchical clustering		algorithm;hierarchical clustering	Robert E. Tarjan	1982	Inf. Process. Lett.	10.1016/0020-0190(82)90136-3	correlation clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;hierarchical network model;consensus clustering;cure data clustering algorithm;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;biclustering;dendrogram;hierarchical clustering of networks;clustering high-dimensional data	DB	2.367479428804547	-41.27078334715172	165605
2327d0a6c573f28ea4226291b8623ef49ac16764	statistical inference for cluster trees		A cluster tree provides an interpretable summary of a density function by representing the hierarchy of its high-density clusters. It is estimated using the empirical tree, which is the cluster tree constructed from a density estimator. This paper assesses the statistical significance of features of an empirical cluster tree. We first study a variety of metrics that can be used to compare different trees, analyze their properties, and assess their suitability for inference. We then propose methods to construct and summarize confidence sets for the unknown true cluster tree.	computer cluster	Jisu Kim;Yen-Chi Chen;Sivaraman Balakrishnan;Alessandro Rinaldo;Larry A. Wasserman	2016			data mining;mathematics;statistics	ML	6.663157109938339	-44.76287952183559	165775
d39e062b2a2aab9b46b1e1211a1a2cb34ad005fc	ric: parameter-free noise-robust clustering	cluster algorithm;k means;data mining;noise robustness;spectral clustering;data summarization;clustering;clustering method;minimum description length;number of clusters;information theoretic;parameter free data mining;clustered data	How do we find a natural clustering of a real-world point set which contains an unknown number of clusters with different shapes, and which may be contaminated by noise? As most clustering algorithms were designed with certain assumptions (Gaussianity), they often require the user to give input parameters, and are sensitive to noise. In this article, we propose a robust framework for determining a natural clustering of a given dataset, based on the minimum description length (MDL) principle. The proposed framework, robust information-theoretic clustering (RIC), is orthogonal to any known clustering algorithm: Given a preliminary clustering, RIC purifies these clusters from noise, and adjusts the clusterings such that it simultaneously determines the most natural amount and shape (subspace) of the clusters. Our RIC method can be combined with any clustering technique ranging from K-means and K-medoids to advanced methods such as spectral clustering. In fact, RIC is even able to purify and improve an initial coarse clustering, even if we start with very simple methods. In an extension, we propose a fully automatic stand-alone clustering method and efficiency improvements. RIC scales well with the dataset size. Extensive experiments on synthetic and real-world datasets validate the proposed RIC framework.	algorithm;capacitor plague;cluster analysis;computer cluster;euclidean distance;experiment;greedy algorithm;image stitching;information theory;k-means clustering;k-medoids;medoid;minimum description length;new general catalogue;purify;radio frequency;resource description framework;search algorithm;spectral clustering;synthetic intelligence;vacuum cleaner	Christian Böhm;Christos Faloutsos;Jia-Yu Pan;Claudia Plant	2007	TKDD	10.1145/1297332.1297334	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;minimum description length;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;spectral clustering;affinity propagation;k-means clustering;clustering high-dimensional data	ML	0.020825016801097426	-40.368552736388956	165967
5d03ca6574a8d4b96b9a22e2e5d466df44272185	consistency of sequence-based gene clusters	reconstructed gene cluster;restricted gene multiplicity;common interval;sequence-based gene cluster;np completeness;consistency problem;gene order;comparative genomics;different gene cluster model;putative ancestral gene cluster;gene cluster	In comparative genomics, differences or similarities of gene orders are determined to predict functional relations of genes or phylogenetic relations of genomes. For this purpose, various combinatorial models can be used to specify gene clusters--groups of genes that are co-located in a set of genomes. Several approaches have been proposed to reconstruct putative ancestral gene clusters based on the gene order of contemporary species. One prevalent and natural reconstruction criterion is consistency: For a set of reconstructed gene clusters, there should exist a gene order that comprises all given clusters. For permutation-based gene cluster models, efficient methods exist to verify this condition. In this article, we discuss the consistency problem for different gene cluster models on sequences with restricted gene multiplicities. Our results range from linear-time algorithms for the simple model of adjacencies to NP-completeness proofs for more complex models like common intervals.	algorithm;computer cluster;genome;genomics;np-completeness;phylogenetics;time complexity	Roland Wittler;Ján Manuch;Murray Patterson;Jens Stoye	2011	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2011.0083	biology;gene cluster;bioinformatics;comparative genomics;genetics	Comp.	0.9109423910465898	-51.2074313905251	166053
f51d38ab07b96cf47fb3807cc8e2dc6bda51e5a0	an effective pattern pruning and summarization method retaining high quality patterns with high area coverage in relational datasets	itemsets;approximation algorithms;frequency estimation;data mining;statistical analysis;pattern recognition;statistical induced pattern pattern pruning pattern summarization;algorithm design and analysis;itemsets data mining algorithm design and analysis frequency estimation approximation algorithms statistical analysis pattern recognition	Pattern mining has been widely used to uncover interesting patterns from data. However, one of its main problems is that it produces too many patterns and many of them are redundant. To reduce the number of redundant patterns and retain overlapping ones, delta-closed pattern pruning was introduced, yet it can only prune subpatterns if they are covered by superpatterns. Such unduly superpatterns need to be pruned. Furthermore, in order to improve the management and interpretation of patterns, pattern summarization is proposed. It renders a small number of patterns that retain the most crucial information. RuleCover algorithm was one of such algorithms. However, it tends to produce over trivial patterns, whereas more interesting and revealing ones may be pruned. To overcome these problems, this paper presents a new algorithm which integrates delta-closed, and RuleCover methods with our other two new algorithms: 1) statistically induced pattern pruning for pruning statistically induced superpatterns by strong subpatterns and 2) AreaCover algorithm for pruning overlapping patterns but retain higher order and high quality patterns with large coverage of the data “area.” Experimental results show that the proposed algorithms produce very compact yet comprehensive knowledge from patterns discovered from relational data sets.	algorithm;data mining;display resolution;prune and search;rendering (computer graphics);superpattern	Pei-Yuan Zhou;Gary C. L. Li;Andrew K. C. Wong	2016	IEEE Access	10.1109/ACCESS.2016.2624418	algorithm design;computer science;machine learning;pattern recognition;data mining;approximation algorithm;algorithm;statistics	DB	-1.4515982533951477	-40.709839638196335	166150
98ca3cbcbe26d68fe5eeee56b92e5bbcd7858baf	a meta-review of feature selection techniques in the context of microarray data		Microarray technologies produce very large amounts of data that need to be classified for interpretation. Large data coupled with small sample sizes make it challenging for researchers to get useful information and therefore a lot of effort goes into the design and testing of feature selection tools; literature abounds with description of numerous methods. In this paper we select five representative review papers in the field of feature selection for microarray data in order to understand their underlying classification of methods. Finally, on this base, we propose an extended taxonomy for categorizing feature selection techniques and use it to classify the main methods presented in the selected reviews.	feature selection;microarray	Zahra Mungloo-Dilmohamud;Yasmina Jaufeerally-Fakim;Carlos Andrés Peña-Reyes	2017		10.1007/978-3-319-56148-6_3	gene chip analysis;pattern recognition;microarray databases	ML	7.200058892506532	-47.60679016421264	166187
c8383cf90d84cce5a4c98165f6db966ac867640f	continuous learning graphical knowledge unit for cluster identification in high density data sets	mining methods and algorithms;knowledge retrieval;data and knowledge visualization;big data;contour lines;clustering;missing data;real time systems	Big data are visually cluttered by overlapping data points. Rather than removing, reducing or reformulating overlap, we propose a simple, effective and powerful technique for density cluster generation and visualization, where point marker (graphical symbol of a data point) overlap is exploited in an additive fashion in order to obtain bitmap data summaries in which clusters can be identified visually, aided by automatically generated contour lines. In the proposed method, the plotting area is a bitmap and the marker is a shape of more than one pixel. As the markers overlap, the red, green and blue (RGB) colour values of pixels in the shared region are added. Thus, a pixel of a 24-bit RGB bitmap can code up to 224 (over 1.6 million) overlaps. A higher number of overlaps at the same location makes the colour of this area identical, which can be identified by the naked eye. A bitmap is a matrix of colour values that can be represented as integers. The proposed method updates this matrix while adding new points. Thus, this matrix can be considered as an up-to-time knowledge unit of processed data. Results show cluster generation, cluster identification, missing and out-of-range data visualization, and outlier detection capability of the newly proposed method.	24-bit;algorithm;anomaly detection;big data;bitmap;contour line;data point;data visualization;effective method;graphical user interface;mathematical optimization;nonlinear system;online and offline;pixel;sampling (signal processing);swarm intelligence;utility functions on indivisible goods	K. K. Lasantha Britto Adikaram;Mohamed A. Hussein;Mathias Effenberger;Thomas Becker	2016	Symmetry	10.3390/sym8120152	computer vision;big data;missing data;computer science;data science;data mining;cluster analysis;contour line	ML	-0.5628839386702216	-39.884035183268104	166455
c294d4f005226af3b90a598f61a56da44ee9aa35	automated selection and configuration of multi-label classification algorithms with grammar-based genetic programming		This paper proposes (text {Auto-MEKA}_{text {GGP}}), an Automated Machine Learning (Auto-ML) method for Multi-Label Classification (MLC) based on the MEKA tool, which offers a number of MLC algorithms. In MLC, each example can be associated with one or more class labels, making MLC problems harder than conventional (single-label) classification problems. Hence, it is essential to select an MLC algorithm and its configuration tailored (optimized) for the input dataset. (text {Auto-MEKA}_{text {GGP}}) addresses this problem with two key ideas. First, a large number of choices of MLC algorithms and configurations from MEKA are represented into a grammar. Second, our proposed Grammar-based Genetic Programming (GGP) method uses that grammar to search for the best MLC algorithm and configuration for the input dataset. (text {Auto-MEKA}_{text {GGP}}) was tested in 10 datasets and compared to two well-known MLC methods, namely Binary Relevance and Classifier Chain, and also compared to GA-Auto-MLC, a genetic algorithm we recently proposed for the same task. Two versions of (text {Auto-MEKA}_{text {GGP}}) were tested: a full version with the proposed grammar, and a simplified version where the grammar includes only the algorithmic components used by GA-Auto-MLC. Overall, the full version of (text {Auto-MEKA}_{text {GGP}}) achieved the best predictive accuracy among all five evaluated methods, being the winner in six out of the 10 datasets.	algorithm;genetic programming;multi-label classification	Alex Guimarães Cardoso de Sá;Alex Alves Freitas;Gisele Lobo Pappa	2018		10.1007/978-3-319-99259-4_25	machine learning;artificial intelligence;genetic algorithm;multi-label classification;computer science;genetic programming;binary number;algorithm;grammar	ECom	9.504012802268555	-43.69432223603578	166516
9ab869fbd6ff59b94010240c886404cf516d6530	analysis on hybrid dominance-based rough set parameterization using private financial initiative unitary charges data		This paper evaluates the capability of the hybrid parameter reduction approach in handling private financial initiative (PFI) unitary charges data to increase the classification performance. The objective of this study is to analyse the performance of the proposed hybrid parameter reduction approach in assisting the neural network classifier to classify complex data sets that might contain uncertain and inconsistent problems. The proposed hybrid parameter reduction approach consists of several methods that will be executed during the data analysis process. Slicing technique and dominance-based rough set approach (DRSA) are the two techniques that play important roles in the proposed parameter reduction process. In order, to analyse the performance of the proposed work, the PFI data that covers all regions in Malaysia is applied in the experimental works. Besides, several standard data sets have also been used to validate the obtained results. The results reveal that the hybrid approach has successfully assisted the classifier in the classification process.	rough set	Masurah Mohamad;Ali Selamat	2018		10.1007/978-3-319-75417-8_30	machine learning;parametrization;artificial intelligence;unitary state;artificial neural network;data set;computer science;finance;slicing;complex data type;rough set	Robotics	9.369907099903903	-40.3629396529104	166608
4e6f55fcb95c4fd0faf1661a61fad3533ce6d2cc	tissue-level segmentation and tracking of cells in growing plant roots	confocal images;network snakes;markov chain monte carlo;tracking	With the spread of systems approaches to biological research, there is increasing demand for methods and tools capable of extracting quantitative measurements of biological samples from individual and time-based sequences of microscope images. To this end, we have developed a software tool for tissue level segmentation and automatic tracking of a network of cells in confocal images of the roots of the model plant Arabidopsis thaliana. The tool implements a novel hybrid technique, which is a combination of the recently developed Network Snakes technique and MCMC-based particle filters and incorporates automatic initialisation of the network snakes. A novel method of evaluation of network-structured multi-target tracking is also presented, and is used to evaluate the developed tracking framework for accuracy and robustness against several timelapse sequences of Arabidopsis roots. Evaluation results are presented, along with a comparison between the results of the component techniques and the hybrid approach. The results show that the hybrid approach performed consistently well at all levels of complexity and better than the component methods alone.	algorithm;bittorrent tracker;c date and time functions;data structure;emoticon;high-level programming language;internet slang;markov chain monte carlo;mathematical optimization;network topology;norm (social);particle filter;programming tool;real life;reversible-jump markov chain monte carlo;robustness (computer science);synthetic intelligence;time series;watershed (image processing)	Vijaya Sethuraman;Andrew P. French;Darren M. Wells;Kim Kenobi;Tony P. Pridmore	2011	Machine Vision and Applications	10.1007/s00138-011-0329-9	computer vision;simulation;markov chain monte carlo;bioinformatics;tracking	Vision	2.591289620030384	-51.61607757427188	166792
ef79551e85d8821c0ec05e427cd9975aaf7b26c6	similarity search in trajectory databases	databases;similarity metric;generic similarity metrics;trajectory clustering;mobile device;data mining;technology management;data analysis;trajectory database management;classification mining tasks trajectory database management mobile devices explosion trajectory data analysis spatiotemporal knowledge discovery generic similarity metrics temporal dimension trajectory clustering;trajectory data analysis;spatiotemporal phenomena;clustering algorithms;classification mining tasks;explosions;informatics;temporal databases;humans;educational technology;extraterrestrial measurements;spatiotemporal knowledge discovery;temporal dimension;similarity search;databases data analysis informatics technology management explosions educational technology spatiotemporal phenomena extraterrestrial measurements clustering algorithms humans;visual databases data mining temporal databases;mobile devices explosion;visual databases	Trajectory database (TD) management is a relatively new topic of database research, which has emerged due to the explosion of mobile devices and positioning technologies. Trajectory similarity search forms an important class of queries in TD with applications in trajectory data analysis and spatiotemporal knowledge discovery. In contrast to related works which make use of generic similarity metrics that virtually ignore the temporal dimension, in this paper we introduce a framework consisting of a set of distance operators based on primitive (space and time) as well as derived parameters of trajectories (speed and direction). The novelty of the approach is not only to provide qualitatively different means to query for similar trajectories, but also to support trajectory clustering and classification mining tasks, which definitely imply a way to quantify the distance between two trajectories. For each of the proposed distance operators we devise highly parametric algorithms, the efficiency of which is evaluated through an extensive experimental study using synthetic and real trajectory datasets.	algorithm;cluster analysis;experiment;mobile device;semantic similarity;similarity search;synthetic intelligence	Nikos Pelekis;Ioannis Kopanakis;Gerasimos Marketos;Eirini Ntoutsi;Gennady L. Andrienko;Yannis Theodoridis	2007	14th International Symposium on Temporal Representation and Reasoning (TIME'07)	10.1109/TIME.2007.59	computer science;data science;data mining;information retrieval	DB	-3.6526103579475624	-42.09426688978605	166842
101a87902a08c3cbed8b85d8ef34671ff528c067	an insensitivity fuzzy c-means clustering algorithm based on penalty factor	constraint;ifcm;membership;noisy data	This paper analyzes sensitivity of Fuzzy C-means to noisy which generates unreasonable clustering results. We also find that Fuzzy C-means possess monotonicity, which may generate meaningless clustering results. Aiming at these weak points, we present an improved Fuzzy C-means named IFCM (Improved Fuzzy C-means). Firstly, we research the reason of sensitivity and find that constraint leads to sensitivity of algorithm, we propose abolish constraint; secondly, we replace membership with typicality for acquiring more reasonable clustering results; finally, we add penalty factor to objective function to avoid monotonicity and coincident clustering results. On the basis of these, we improve objective function and provide step of algorithm. Experiments on various datasets show that new algorithm recognizes noisy data effectively and makes cluster effect improve furthermore.	algorithm;cluster analysis;experiment;fuzzy cognitive map;lan manager;loss function;optimization problem;signal-to-noise ratio	Jiashun Chen;Dechang Pi;Zhipeng Liu	2013	JSW		constrained clustering;mathematical optimization;fuzzy clustering;flame clustering;fuzzy classification;artificial intelligence;canopy clustering algorithm;machine learning;data mining;constraint;algorithm;statistics	AI	2.9658064827329866	-38.83481023068799	166845
4c598cc55b0a40d4706cf78d255566143953f344	refining decision tree classifiers using rough set tools	candidate algorithm;proposed framework;rough set tool;refining decision tree;related base algorithm;rdt algorithm;induction algorithm;generalized classifier;proposed hybridized rough set;dt algorithm result;induced classifier;hybridized rough set framework;decision tree;decision tree classifier;rough set;data mining;classification;supervised learning	The proposed hybridized rough set framework is composed of traditional Rough Set (RS) approach and classical Decision Tree (DT) induction algorithm. RS helps to identify dominant attributes and DT algorithm results in simpler and generalized classifier. The implementation of the Hybridized Rough Set Framework is presented as the RDT algorithm. GA heuristics are used to generalize the RDT algorithm further. Experimental results obtained on applying the hybridized rough set framework and the related base algorithms on datasets belonging to the three categories are presented in this paper. Accuracy, complexity, number of rules and number of attributes in the induced classifier assess the performance of the candidate algorithms. The results indicate that the proposed framework is an effective model for classification.	decision tree;rough set	Sonajharia Minz;Rajni Jain	2005	Int. J. Hybrid Intell. Syst.		rough set;decision tree learning;biological classification;computer science;machine learning;decision tree;pattern recognition;incremental decision tree;data mining;supervised learning;dominance-based rough set approach	AI	8.63106369592574	-41.072592192443494	166853
f781f66620105cf2408c3d18f688604b7e0a5f02	performance evaluation for distributed join based on mapreduce		Inner-Join is a fundamental and frequent operation in large-scale data analysis. MapReduce is the most widely available framework in large-scale data analysis. A variety of inner-join algorithms are put forward to run on the MapReduce environment. Usually, those algorithms are designed for specific scenarios, but inner-join could present very different performance when data volume, reference ratio, data skew rate, and running environments et al are varied. This paper summarized and implemented those well-known join algorithms in a uniform MapReduce environment. Considering the number of tables, broadcast cost, data skew, join rate and related factors, we designed and conducted a large number of experiments to compare the time cost of those join algorithms. According to the experimental results, we analyzed and summarized the performance and applicability of those algorithms in different scenarios, which would be a reference of performance improvement for large-scale data analysis under different circumstances.	algorithm;decision theory;experiment;join (sql);mapreduce;performance evaluation;requirement	Jingwei Zhang;Qing Yang;Hongjia Shang;Huibing Zhang;Yuming Lin;Rui Zhou	2016	2016 7th International Conference on Cloud Computing and Big Data (CCBD)	10.1109/CCBD.2016.065	computer science;performance improvement;skew;big data;data mining;distributed computing;algorithm design;cloud computing;statistical classification	DB	-4.538699023430454	-39.6601093491165	166933
32a57d2042dcabed8cc45ed3a0125afc8ecc3a9c	a relevant subspace based contextual outlier mining algorithm		For high-dimensional and massive data sets, a relevant subspace based contextual outlier detection algorithm is proposed. Firstly, the relevant subspace, which can effectively describe the local distribution of the various data sets, is redefined by using local sparseness of attribute dimensions. Secondly, a local outlier factor calculation formula in the relevant subspace is defined with probability density of local data sets, and the formula can effectively reflect the outlier degree of data object that does not obey the distribution of the local data set in the relevant subspace. Thirdly, attribute dimensions of constituting the relevant subspace and local outlier factor are defined as the contextual information, which can improve the interpretability and comprehensibility of outlier. Fourthly, the selection of N data objects with the greatest local outlier factor value is defined as contextual outliers. In the end, experimental results validate the effectiveness of the algorithm by using UCI data sets.	algorithm	Jifu Zhang;Xiaolong Yu;Yonghong Li;Sulan Zhang;Yaling Xun;Xiao Qin	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.01.013	machine learning;pattern recognition;data mining	ML	0.4012554418554231	-41.68599998037107	166977
5df9b63a4af1eba78345b94ee3a9de376590ba25	resampling-based selective clustering ensembles	clustering analysis;analyse amas;ensemble method;cluster analysis;signal classification;classification signal;analisis cluster;methode reechantillonnage;resampling method;classification automatique;resampling technique;automatic classification;clustering ensembles;clasificacion automatica	Traditional clustering ensembles methods combine all obtained clustering results at hand. However, we observe that it can often achieve a better clustering solution if only part of all available clustering results are combined. This paper proposes a novel clustering ensembles method, termed as resampling-based selective clustering ensembles method. The proposed selective clustering ensembles method works by evaluating the qualities of all obtained clustering results through resampling technique and selectively choosing part of promising clustering results to build the ensemble committee. The final solution is obtained through combining the clustering results of the ensemble committee. Experimental results on several real data sets demonstrate that resampling-based selective clustering ensembles method is often able to achieve a better solution when compared with traditional clustering ensembles methods. 2008 Elsevier B.V. All rights reserved.	cluster analysis;computer cluster;resampling (statistics);unsupervised learning	Yi Hong;Sam Kwong;Hanli Wang;Qingsheng Ren	2009	Pattern Recognition Letters	10.1016/j.patrec.2008.10.007	correlation clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;clustering high-dimensional data	AI	5.333546324590289	-40.64140038442656	167368
a60fd4a7830f17a5420bb2b0a242b2533390935b	linear coherent bi-cluster discovery via line detection and sample majority voting	cluster algorithm;perforation;line detection;arabidopsis thaliana;gene expression;cluster model;dna microarray;majority voting	Discovering groups of genes that share common expression profiles is an important problem in DNA microarray analysis. Unfortunately, standard bi-clustering algorithms often fail to retrieve common expression groups because (1) genes only exhibit similar behaviors over a subset of conditions, and (2) genes may participate in more than one functional process and therefore belong to multiple groups. Many algorithms have been proposed to address these problems in the past decade; however, in addition to the above challenges most such algorithms are unable to discover linear coherent bi-clusters—a strict generalization of additive and multiplicative bi-clustering models. In this paper, we propose a novel bi-clustering algorithm that discovers linear coherent biclusters, based on first detecting linear correlations between pairs of gene expression profiles, then identifying groups by sample majority voting. Our experimental results on both synthetic and two real datasets, Saccharomyces cerevisiae and Arabidopsis thaliana, show significant performance improvements over previous methods. One intriguing aspect of our approach is that it can easily be extended to identify bi-clusters of more complex gene-gene correlations.	algorithm;cluster analysis;coherent;dna microarray;edge detection;sensor;synthetic intelligence;utility functions on indivisible goods	Yi Shi;Zhipeng Cai;Guohui Lin;Dale Schuurmans	2009		10.1007/978-3-642-02026-1_7	majority rule;gene expression;dna microarray;bioinformatics;machine learning;data mining;mathematics	ML	4.51700835903834	-49.759775860059115	167408
0c988462d070622730ee55dcf20d5e7c5c0f1dfa	a knn-scoring based core-growing approach to cluster analysis	hierarchical clustering;cluster algorithm;fuzzy c mean;national academy of sciences;cluster analysis;core growing;clustering method;k nearest neighbor;self organized map;neural network	This paper proposes a novel core-growing (CG) clustering method based on scoring k-nearest neighbors (CG-KNN). First, an initial core for each cluster is obtained, and then a tree-like structure is constructed by sequentially absorbing data points into the existing cores according to the KNN linkage score. The CG-KNN can deal with arbitrary cluster shapes via the KNN linkage strategy. On the other hand, it allows the membership of a previously assigned training pattern to be changed to a more suitable cluster. This is supposed to enhance the robustness. Experimental results on four UCI real data benchmarks and Leukemia data sets indicate that the proposed CG-KNN algorithm outperforms several popular clustering algorithms, such as Fuzzy C-means (FCM) (Xu and Wunsch IEEE Transactions on Neural Networks 16:645---678, 2005), Hierarchical Clustering (HC) (Xu and Wunsch IEEE Transactions on Neural Networks 16:645---678, 2005), Self-Organizing Maps (SOM) (Golub et al. Science 286:531---537, 1999; Tamayo et al. Proceedings of the National Academy of Science USA 96:2907, 1999), and Non-Euclidean Norm FCM (NEFCM) (Karayiannis and Randolph-Gips IEEE Transactions On Neural Networks 16, 2005).	cluster analysis;k-nearest neighbors algorithm	Tzuen Wuu Hsieh;Jin-Shiuh Taur;Sun-Yuan Kung	2010	Signal Processing Systems	10.1007/s11265-009-0406-8	k-medians clustering;computer science;artificial intelligence;machine learning;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;k-nearest neighbors algorithm;artificial neural network	ML	2.8320059149731747	-42.72754271084033	167619
2703d4e451af1a2456b341bc55e26167f2f317fb	pplsa: parallel probabilistic latent semantic analysis based on mapreduce	document collection;modeling technique;large datasets;会议论文;parallel;map reduce;em;probabilistic latent semantic analysis	PLSA(Probabilistic Latent Semantic Analysis) is a popular topic modeling technique for exploring document collections. Due to the increasing prevalence of large datasets, there is a need to improve the scalability of computation in PLSA. In this paper, we propose a parallel PLSA algorithm called PPLSA to accommodate large corpus collections in the MapReduce framework. Our solution efficiently distributes computation and is relatively simple to implement.	algorithm;computation;mapreduce;probabilistic latent semantic analysis;scalability;speedup;topic model	Ning Li;Fuzhen Zhuang;Qing He;Zhongzhi Shi	2012		10.1007/978-3-642-32891-6_8	semantic computing;computer science;pattern recognition;data mining;database;probabilistic latent semantic analysis	Web+IR	-3.529467730774652	-40.23033668339494	167843
edbb4b04bb70673c56f3dcffb35f1b0873dbe34c	fuzzy clustering ensemble algorithm for partitioning categorical data	computers;clustering ensemble;pattern clustering data handling fuzzy set theory;pattern clustering;fellow small dataset;probability density function;satisfiability;data mining;fuzzy set theory;objective function;error analysis;categorical data partitioning;fuzzy clustering;fellow small dataset fuzzy clustering ensemble algorithm categorical data partitioning objective function zoo dataset;number of clusters;clustering algorithms;clustering algorithms partitioning algorithms text recognition data engineering road transportation conference management financial management engineering management algorithm design and analysis merging;data handling;relationship degree;categorical data;fuzzy clustering ensemble algorithm;algorithm design and analysis;clustering ensemble fuzzy clustering categorical data relationship degree;zoo dataset;partitioning algorithms	Existing clustering ensemble algorithms for partitioning categorical data only apply to know the generating process of clustering members very well. In order to broaden the application of clustering ensemble, a fuzzy clustering ensemble algorithm for partitioning categorical data is proposed in this paper. The proposed algorithm makes use of relationship degree between different attributes for pruning a part of attributes (features). According to the distribution of clustering members, Descartes subset and relationship degree between objects are used for establishing the relationships between objects under unsupervised circumstances and get the minimum value of objective function of clustering and corresponding partitions. Then, numbers of clusters satisfying the difference and differential rate of objective function local maximum are the optimal numbers of clusters and its corresponding partitions are optimal clustering. Finally, the proposed algorithm is applied in Fellow-small dataset and Zoo dataset and results show the algorithm is effective and feasible.	algorithm;categorical variable;cluster analysis;ensemble learning;fuzzy clustering;loss function;maxima and minima;optimization problem;unsupervised learning	Taoying Li;Yan Chen	2009	2009 International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2009.48	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	AI	2.998511087062214	-39.88000143793568	168052
aab1fcf2820b0f92232cd4b456479449ae67458b	weighted pooling high-throughput gene expression data sets to maximize the functional coherence of the top rank genes	top rank gene identification weighted pooling high throughput gene expression functional coherence high throughput technique p values gene functional relationship gene functional network biological pathway gene selection enhanced simulated annealing literature semantic indexing cohesive analysis;semantic indexing;gene expression data;semantic networks;simulated annealing;genetics;simulated annealing bioinformatics biological techniques genetics indexing semantic networks;gene expression;indexing;gene order;biological techniques;coherence educational institutions conferences gene expression usa councils simulated annealing;high throughput;gene function;bioinformatics	In a typical gene expression study with high throughput technique, such as microarray, a biologist usually focuses on the top genes ranked by the P-values to establish gene functional relationship / network, biological pathway, and microbiologically ramifications of the gene's selection. With more datasets publically available, researchers pool data from independent experiments, typically by pooling P-values with equal weight assigned to each dataset, aiming to fetch more biological information from the pooled data. However, the qualities of datasets may vary substantially. Assigning equal weights may not guarantee the optimal result. Applying the equal weights approach to six independent datasets, we observe the top rank genes of data pooled with this approach have less functional coherence than the single dataset that has highest functional coherence. We propose a procedure based on enhanced simulated annealing (ESA) and literature semantic indexing cohesive (LSI-c) analysis to assign optimal weights to datasets so as to maximize the functional coherence of the top rank genes ordered by their pooled P-values. We observe significantly more functional coherence in optimally pooled data than any single dataset or data pooled with equal weights. Identification of top rank genes through our optimal procedure should improve the downstream analysis.	downstream (software development);esa;experiment;gene regulatory network;high-throughput computing;microarray;simulated annealing;throughput	Xiaodong Zhou;E. Olusegun George	2011	2011 IEEE International Conference on Bioinformatics and Biomedicine Workshops (BIBMW)	10.1109/BIBMW.2011.6112550	high-throughput screening;biology;search engine indexing;gene expression;simulated annealing;computer science;bioinformatics;theoretical computer science;machine learning;data mining;semantic network;genetics	Comp.	3.8240438674246127	-48.68900562619852	168129
59f72fec6b505512ba0dfd6d9f6d6259f9fea7c1	the application of sparse estimation of covariance matrix to quadratic discriminant analysis	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;combinatorial libraries;uk research reports;medical journals;computer appl in life sciences;europe pmc;biomedical research;microarrays;bioinformatics	Although Linear Discriminant Analysis (LDA) is commonly used for classification, it may not be directly applied in genomics studies due to the large p, small n problem in these studies. Different versions of sparse LDA have been proposed to address this significant challenge. One implicit assumption of various LDA-based methods is that the covariance matrices are the same across different classes. However, rewiring of genetic networks (therefore different covariance matrices) across different diseases has been observed in many genomics studies, which suggests that LDA and its variations may be suboptimal for disease classifications. However, it is not clear whether considering differing genetic networks across diseases can improve classification in genomics studies. We propose a sparse version of Quadratic Discriminant Analysis (SQDA) to explicitly consider the differences of the genetic networks across diseases. Both simulation and real data analysis are performed to compare the performance of SQDA with six commonly used classification methods. SQDA provides more accurate classification results than other methods for both simulated and real data. Our method should prove useful for classification in genomics studies and other research settings, where covariances differ among classes.	anterior descending branch of left coronary artery;class;gene regulatory network;genomics;linear discriminant analysis;quadratic classifier;simulation;sparse matrix;statistical classification;version	Jiehuan Sun;Hongyu Zhao	2014		10.1186/s12859-014-0443-6	biology;dna microarray;computer science;bioinformatics;data science;data mining	ML	7.364340273486884	-51.1785330282134	168130
c594cfd94d74e5f53923e324960fae186b26d8ae	evaluation of web session cluster quality based on access-time dissimilarity and evolutionary algorithms		Web session cluster refinement is one of the major research issues for the improvement of cluster quality in recent days. The motive of refinement using Evolutionary Algorithms is quite obvious because in any clustering algorithm the obtained clusters shall have some data items that are inappropriately clustered, hence, never giving us well separated and cohesive clusters. Hence the quality of clusters is improved using refinement techniques. Initial clusters are formed using K-Means clustering algorithm which suffers from local minima problem. The refinement on clusters is performed on the basis of access and time features (Modified Knockout Refinement Algorithm) which is a distance based dissimilarity, Genetic Algorithm (GA), Particle Swarm Optimization (PSO) and a combination of MKRA with GA and MKRA with PSO. Results are evaluated on five synthetic datasets and three real datasets. Further, it is shown experimentally that effectiveness of combining MKRA with evolutionary techniques produces better quality clusters.	evolutionary algorithm	Veer Sain Dixit;Shveta Kundra Bhatia;V. B. Singh	2014		10.1007/978-3-319-09156-3_22	computer science;data mining;database;world wide web	Web+IR	4.01739641164503	-42.04121721994399	168270
6fbe00668306a817e9e724acce54407d4f105ed5	on the maximal cliques in c-max-tolerance graphs and their application in clustering molecular sequences	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;physiological cellular and medical topics;computational biology bioinformatics;phylogenetic tree;uk phd theses thesis;polynomial time;life sciences;algorithms;biological data;uk research reports;medical journals;local alignment;europe pmc;biomedical research;bioinformatics	Given a set S of n locally aligned sequences, it is a needed prerequisite to partition it into groups of very similar sequences to facilitate subsequent computations, such as the generation of a phylogenetic tree. This article introduces a new method of clustering which partitions S into subsets such that the overlap of each pair of sequences within a subset is at least a given percentage c of the lengths of the two sequences. We show that this problem can be reduced to finding all maximal cliques in a special kind of max-tolerance graph which we call a c-max-tolerance graph. Previously we have shown that finding all maximal cliques in general max-tolerance graphs can be done efficiently in O(n3 + out). Here, using a new kind of sweep-line algorithm, we show that the restriction to c-max-tolerance graphs yields a better runtime of O(n2 log n + out). Furthermore, we present another algorithm which is much easier to implement, and though theoretically slower than the first one, is still running in polynomial time. We then experimentally analyze the number and structure of all maximal cliques in a c-max-tolerance graph, depending on the chosen c-value. We apply our simple algorithm to artificial and biological data and we show that this implementation is much faster than the well-known application Cliquer. By introducing a new heuristic that uses the set of all maximal cliques to partition S, we finally show that the computed partition gives a reasonable clustering for biological data sets.	alignment;clique (graph theory);cluster analysis;computation;experiment;graph - visual representation;heuristic;maximal set;phylogenetic tree;phylogenetics;subgroup;sweep line algorithm;time complexity;whole earth 'lectronic link;statistical cluster	Katharina Anna Zweig;Michael Kaufmann;Stephan Steigele;Kay Nieselt	2006	Algorithms for Molecular Biology	10.1186/1748-7188-1-9	time complexity;biology;phylogenetic tree;biological data;k-tree;computer science;bioinformatics;graph partition;theoretical computer science;smith–waterman algorithm;data mining;clique-sum;mathematics;intersection number;algorithm	Comp.	-0.1455633662719078	-50.71872490988458	168481
115f15637908ea63a6571ed5bdd3b5937f6eddbf	agglomerative and divisive approaches to unsupervised learning in gestalt clusters		Hierarchical clustering algorithms can be agglomerative or divisive, depending on how partitions are formed. Such algorithms have advantages mainly related to the desired level of granularity the partition should have. The work described in this paper approaches two hierarchical algorithms, one agglomerative (and three of its variants) and the other divisive, focusing on their performance in unsupervised learning tasks related to gestalt clusters. Taking into account that the point sets considered are representative of gestalt clusters, the experiments show that the best results have been obtained when the agglomerative approach was used.	algorithm;cluster analysis;computer cluster;experiment;gestalt psychology;hierarchical clustering;k-means clustering;linkage (software);rand index;single-linkage clustering;unsupervised learning	Rodrigo C. Camargos;Paulo Rogerio Nietto;Maria do Carmo Nicoletti	2016		10.1007/978-3-319-53480-0_4	artificial intelligence;machine learning;granularity;unsupervised learning;cluster analysis;computer science;pattern recognition;cluster (physics);gestalt psychology;hierarchical clustering	ML	1.5891739824399091	-41.76627180515502	168590
86068216d21cb43403f8fb1fbfae4ca49d06f8dc	k-maxoids clustering		We explore the idea of clustering according to extremal rather than to central data points. To this end, we introduce the notion of the maxoid of a data set and present an algorithm for k-maxoids clustering which can be understood as a variant of classical k-means clustering. Exemplary results demonstrate that extremal cluster prototypes are more distinctive and hence more interpretable than central ones.	algorithm;archetypal analysis;cluster analysis;data point;game mechanics;k-means clustering;system archetype;video game developer	Christian Bauckhage;Rafet Sifa	2015			complete-linkage clustering;correlation clustering;constrained clustering;combinatorics;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;affinity propagation;clustering high-dimensional data;conceptual clustering	ML	1.9730808474842871	-41.67849101498169	168703
e35b07f9c684a4dacb08d65fc56ad38b9968bc74	a probabilistic model for mining labeled ordered trees: capturing patterns in carbohydrate sugar chains	mining methods and algorithms;genetique;extraction information;biology computing;stochastic context free grammar;learning algorithm;algorithm complexity;analisis datos;mining methods and algorithms index terms biology and genetics machine learning data mining;information extraction;genetica;complejidad algoritmo;context free grammars;statistical significance;microorganisms data mining tree data structures biology computing pattern recognition learning artificial intelligence context free grammars computational complexity molecular biophysics genetics;algorithme apprentissage;biology;biologia;tree data structures;indexing terms;data mining;limit set;genetics;machine learning probabilistic model labeled ordered tree mining carbohydrate sugar chains glycans multicellular organisms time complexity space complexity stochastic context free grammars biologically significant common subtrees glycobiology genetics;probabilistic model;data analysis;glycan;complexite algorithme;machine learning;fouille donnee;computational complexity;biology and genetics;glicano;molecular biophysics;modele probabiliste;space complexity;pattern recognition;algorithme em;azucar;hidden markov models context modeling data mining machine learning xml organisms stochastic processes bioinformatics sequences documentation;analyse donnee;algoritmo em;index terms biology and genetics;cadena carbohidrato;sugar;learning artificial intelligence;em algorithm;chaine osidique;algoritmo aprendizaje;carbohydrate chain;busca dato;microorganisms;extraccion informacion;biologie;glycane;modelo probabilista;sucre	Glycans, or carbohydrate sugar chains, which play a number of important roles in the development and functioning of multicellular organisms, can be regarded as labeled ordered trees. A recent increase in the documentation of glycan structures, especially in the form of database curation, has made mining glycans important for the understanding of living cells. We propose a probabilistic model for mining labeled ordered trees, and we further present an efficient learning algorithm for this model, based on an EM algorithm. The time and space complexities of this algorithm are rather favorable, falling within the practical limits set by a variety of existing probabilistic models, including stochastic context-free grammars. Experimental results have shown that, in a supervised problem setting, the proposed method outperformed five other competing methods by a statistically significant factor in all cases. We further applied the proposed method to aligning multiple glycan trees, and we detected biologically significant common subtrees in these alignments where the trees are automatically classified into subtypes already known in glycobiology.	context-free language;digital curation;documentation;expectation–maximization algorithm;markov chain;statistical model;stochastic context-free grammar;sugar;supervised learning;tree (data structure)	Nobuhisa Ueda;Kiyoko F. Aoki-Kinoshita;Atsuko Yamaguchi;Tatsuya Akutsu;Hiroshi Mamitsuka	2005	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2005.117	limit set;statistical model;index term;expectation–maximization algorithm;computer science;bioinformatics;artificial intelligence;glycan;machine learning;data mining;database;mathematics;statistical significance;microorganism;tree;dspace;context-free grammar;data analysis;computational complexity theory;world wide web;stochastic context-free grammar;information extraction;algorithm;statistics;molecular biophysics	ML	0.3510935248381662	-51.96122360210237	168887
05832e2fa4667f719d324c55d424957d76adb7ad	noise-resistant bicluster recognition	unsupervised learning;real datasets noise resistant bicluster recognition co expressed genes biclustering algorithms unsupervised feature learning deep neural networks autodecoder automatic feature learning noise suppression nonuniform signal recovery mechanism bicluster patterns ad overlapped bicluster recovery noisy gene expression data synthetic datasets;biology computing;pattern clustering;neural nets;neural network gene expression biclustering;genetics;pattern recognition;unsupervised learning biology computing genetics neural nets pattern clustering pattern recognition;noise gene expression neural networks neurons algorithm design and analysis robustness noise level	Biclustering is crucial in finding co-expressed genes and their associated conditions in gene expression data. While various biclustering algorithms (e.g., combinatorial, probabilistic modelling, and matrix factorization) have been proposed and constantly improved in the past decade, data noise and bicluster overlaps make biclustering a still challenging task. It becomes difficult to further improve biclustering performance, without resorting to a new approach. Inspired by the recent progress in unsupervised feature learning using deep neural networks, in this work, we propose a novel model for biclustering, named Auto Decoder (AD), by relating biclusters to features and leveraging a neural network that is able to automatically learn features from the input data. To suppress severe noise present in gene expression data, we introduce a non-uniform signal recovery mechanism: Instead of reconstructing the whole input data to capture the bicluster patterns, AD weighs the zero and non-zero parts of the input data differently and is more flexible in dealing with different types of noise. AD is also properly regularized to deal with bicluster overlaps. To the best of our knowledge, this is the first biclustering algorithm that leverages neural network techniques to recover overlapped biclusters hidden in noisy gene expression data. We compared our approach with four state-of-the-art biclustering algorithms on both synthetic and real datasets. On three out of the four real datasets, AD significantly outperforms the other approaches. On controlled synthetic datasets, AD performs the best when noise level is beyond 15%.	algorithm;artificial neural network;biclustering;deep learning;detection theory;feature learning;noise (electronics);statistical model;synthetic data;synthetic intelligence	Huan Sun;Gengxin Miao;Xifeng Yan	2013	2013 IEEE 13th International Conference on Data Mining	10.1109/ICDM.2013.34	unsupervised learning;computer science;machine learning;pattern recognition;data mining;biclustering	ML	9.572998109271884	-47.93378466472887	168918
13fe6ff6414500f4d3ba722b3256a92c7ef164f5	a progressive framework for two-way clustering using adaptive subspace iteration for functionally classifying genes	cancer;robustness;proteins;data analysis;image analysis;computer vision;clustering algorithms;colon cancer;ground truth;microarray data	"""This paper presents an adaptive subspace based two-way clustering of microarray data. To analyze the data at various scales a """"Progressive"""" framework is introduced. The goals are to functionally classify genes and also to find differentially expressed genes in microarray expression profiles. Empirical analysis on Colon Cancer dataset shows that ASI performs favorably in grouping genes with similar functions and finding genes that may have been involved in the formation of colon cancer. It was also observed that the proposed algorithm is robust against ordering of samples and yield results consistent with ground truth information."""	algorithm;cluster analysis;colon classification;ground truth;iteration;microarray	Jahangheer S. Shaik;Mohammed Yeasin	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings		microarray analysis techniques;image analysis;ground truth;computer science;bioinformatics;machine learning;data mining;cluster analysis;data analysis;robustness;cancer	Vision	6.089159734194018	-49.301973290179355	169078
1007ca96f18982931b57e8b5ba1b837d0479202f	sparse kernel methods for high-dimensional survival data	gnu public license;high dimensionality;survival data;methode noyau;qa mathematics;sobrevivencia;bioinformatique;inner product;proportional hazard model;support vector;support vector classification;metodo nucleo;survie;kernel method;bri;bioinformatica;support vector machine;survival;partial likelihood;bioinformatics	UNLABELLED Sparse kernel methods like support vector machines (SVM) have been applied with great success to classification and (standard) regression settings. Existing support vector classification and regression techniques however are not suitable for partly censored survival data, which are typically analysed using Cox's proportional hazards model. As the partial likelihood of the proportional hazards model only depends on the covariates through inner products, it can be 'kernelized'. The kernelized proportional hazards model however yields a solution that is dense, i.e. the solution depends on all observations. One of the key features of an SVM is that it yields a sparse solution, depending only on a small fraction of the training data. We propose two methods. One is based on a geometric idea, where-akin to support vector classification-the margin between the failed observation and the observations currently at risk is maximised. The other approach is based on obtaining a sparse model by adding observations one after another akin to the Import Vector Machine (IVM). Data examples studied suggest that both methods can outperform competing approaches.   AVAILABILITY Software is available under the GNU Public License as an R package and can be obtained from the first author's website http://www.maths.bris.ac.uk/~maxle/software.html.	clinical trial censoring;gnu;intravital microscopy;kernel method;proportional hazards model;r language;sparse matrix;support vector machine;web site	Ludger Evers;Claudia-Martina Messow	2008	Bioinformatics	10.1093/bioinformatics/btn253	support vector machine;econometrics;computer science;data mining;statistics	ML	4.081593811100104	-51.273282890016844	169157
9ae0456abfbbc034a9ffb1cbcdc8094ee5b309d5	detecting repeat families in incompletely sequenced genomes	functional genomics;de bruijn graph;genome sequence	Repeats form a major class of sequence in genomes with implications for functional genomics and practical problems. Their detection and analysis pose a number of challenges in genomic sequence analysis, especially if the genome is not completely sequenced. The most abundant and evolutionary active forms of repeats are found in the form of families of long similar sequences. We present a novel method for repeat family detection and characterization in cases where the target genome sequence is not completely known. Therefore we first establish the sequence graph, a compacted version of sparse de Bruijn graphs. Using appropriate analysis of the structure of this graph and its connected components after local modifications, we are able to devise two algorithms for repeat family detection. The applicability of the methods is shown for both simulated and real genomic data sets.	algorithm;complexity;connected component (graph theory);de bruijn graph;experiment;functional genomics;sensor;sequence analysis;sequence graph;sparse matrix	José Augusto Amgarten Quitzau;Jens Stoye	2008		10.1007/978-3-540-87361-7_29	functional genomics;biology;de bruijn graph;whole genome sequencing;computer science;bioinformatics;mathematics;genetics	ML	0.37994165788495754	-50.90173697306482	169249
bf579cfcee9a1c941d1744279eafb061544bf825	spectralcat: categorical spectral clustering of numerical and nominal data	diffusion maps;categorical data clustering;spectral clustering;dimensionality reduction	Data clustering is a common technique for data analysis, which is used in many fields, including machine learning, data mining, customer segmentation, trend analysis, pattern recognition and image analysis. Although many clustering algorithms have been proposed, most of them deal with clustering of one data type (numerical or nominal) or with mix data type (numerical and nominal) and only few of them provide a generic method that clusters all types of data. It is required for most real-world applications data to handle both feature types and their mix. In this paper, we propose an automated technique, called SpectralCAT, for unsupervised clustering of high-dimensional data that contains numerical or nominal or mix of attributes. We suggest to automatically transform the high-dimensional input data into categorical values. This is done by discovering the optimal transformation according to the Calinski–Harabasz index for each feature and attribute in the dataset. Then, a method for spectral clustering via dimensionality reduction of the transformed data is applied. This is achieved by automatic non-linear transformations, which identify geometric patterns in the data, and find the connections among them while projecting them onto low-dimensional spaces. We compare our method to several clustering algorithms using 16 public datasets from different domains and types. The experiments demonstrate that our method outperforms in most cases these algorithms. & 2011 Elsevier Ltd. All rights reserved.	algorithm;cluster analysis;data mining;dimensionality reduction;experiment;image analysis;level of measurement;machine learning;nonlinear system;numerical analysis;pattern recognition;spectral clustering	Gil David;Amir Averbuch	2012	Pattern Recognition	10.1016/j.patcog.2011.07.006	diffusion map;correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;spectral clustering;affinity propagation;dimensionality reduction;clustering high-dimensional data;conceptual clustering	ML	2.3293297347697375	-39.91686175842613	169369
4083ab3f2924251d4c5286201d940e28db1be09c	analysis of big-data based data mining engine		In order to solve the problem of data mining in big data, this paper studies the data mining engine based on big data. Using Spark as the engine core and programming model, some parallel data mining algorithms are designed and implemented, and an efficient data mining engine system is built. Therefore, the traditional data mining algorithms can run in parallel in the cluster environment, in which big data can be made better of use. Through the above work, a complete big data mining system is realized, which provides an efficient and easy-to-use tool for the implementation of data mining algorithms on big data sets.	algorithm;big data;computation;computer science;data mining;parallel computing;programming model	Xinxin Huang;Shu Gong	2017	2017 13th International Conference on Computational Intelligence and Security (CIS)	10.1109/CIS.2017.00043	machine learning;artificial intelligence;computer science;algorithm design;programming paradigm;spark (mathematics);data mining;big data;cluster analysis	ML	-3.253012320813058	-38.76151418428588	169527
3a87c5b57729409d387e97cc6afdc33a6681067a	a data selection framework for k-means algorithm to mine high precision clusters		Traditional clustering algorithms employ all the data items to learn the cluster patterns. However, in real-world applications, some data show clear coherent behaviour and can be summarized well, while some data present weak tendencies to be assigned to any particular pattern. For such situation, this paper presents a data selection framework for K-Means algorithm to get high precision clusters from the data collection. It differs from traditional k-means-type algorithms in three respects. First, in the cluster learning process, we take the changed value of clusteru0027s Bregman Information, which is generated by merging one data item into the potential clusters, as the measure of data itemu0027s clustering tendency. Second, only data items with strong clustering tendencies, that is the changed value of clusteru0027s Bregman Information is less than the predefined radius, are selected to learn the cluster patterns, while the remaining data points are ignored and belong to no cluster. The clustering is non-exhaustive. Third, the radius of the clusters can be changed in the learning process. It is a dynamic learning framework. Experiments on synthetic, document and image data show the effectiveness of the proposed algorithm.	algorithm;bregman divergence;cluster analysis;coherence (physics);computer cluster;data item;data point;dynamic data;experiment;k-means clustering;synthetic intelligence	Zhengzheng Lou;Chaoyang Zhang	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393013	data collection;k-means clustering;machine learning;cluster analysis;cluster (physics);data point;artificial intelligence;merge (version control);linear programming;computer science	ML	0.676140941092323	-40.348157695826174	169531
ecea943f0c1d267dd047a2e3fd0f65cbc83b3911	improving learning outcomes by using clustering validity analysis to reduce label uncertainty	model ensemble cluster validity analysis;classification;machine learning;malware;clustering;dissertation;protein kinase		cluster analysis	U Chon ManChon	2013			computer science;machine learning;consensus clustering;pattern recognition;data mining;cluster analysis;conceptual clustering	ML	6.787281133627412	-45.78767982730385	169542
fe8fd5868dafc3b833eda6875168ea8fbd76a437	prediction of protein function using graph container and message passing	yeast.;protein function prediction;graph container;exemplar protein;interaction network;message passing;scale free;global optimization	We introduce a novel parameter called container flux, which is used to measure the information sharing capacity between two distinct nodes in a graph. Other useful information, both from gene expression or protein interaction test, can be readily incorporated into our container flux. Also, contain flux is a proper candidate for describing the multi-path communication effect caused by large amount of short pathways in real protein networks. In our test, we find there are usually 3∼15 pathways consisted of less than 4 edges, between two different proteins in the network built from Baker’s Yeast. We also formulate a new equation for protein function prediction by integrating the container flux as an information sharing component. Based on the scale-free characteristic of protein interaction network, we propose that these proteins of high degree would most likely to be the exemplars for difference clusters. By further exploration, we reveal an interesting consistency between the global optimization of our prediction equation and the exemplar guided clustering problems. we adopt a log-space version of sumproduct algorithm, a well-established algorithm called affinity propagation, to approximately solve our optimization problem. At the end, we assign every member of the same cluster with the exemplar’s functions. The exemplar-representing assumption is strongly supported by our preliminary experimental results.	affinity propagation;algorithm;boyer–moore string search algorithm;chua's circuit;cluster analysis;global optimization;interaction network;mathematical optimization;message passing;optimization problem;protein function prediction;software propagation;web container	Hongbo Zhou;Qiang Shawn Cheng;Mehdi R. Zargham	2008			flux;protein function prediction;message passing;global optimization;mathematical optimization;cluster analysis;affinity propagation;artificial intelligence;interaction network;machine learning;optimization problem;computer science	ML	1.2152094137390639	-49.742038532970554	169593
20990a380a9aa6b2f7f80bd08c2daade9edc2a4a	a novel hybrid approach of kpca and svm for crop quality classification	tobacco quality data hybrid approach kpca svm crop quality classification quality evaluation crop market price determination artificial neural network cost effective technique small sample effect black box effect kernel principal component analysis support vector machine;kernel principal component analysis;kernel;quality classification kernel principal component analysis support vector machine;black box effect;support vector machines;neural nets;small sample effect;pricing;kpca;cost effective technique;small samples;accuracy;artificial neural networks;hybrid approach;tobacco;quality evaluation;feature extraction;principal component analysis;support vector machines crops neural nets pricing principal component analysis quality management;cost effectiveness;crop market price determination;crops;agriculture;svm;quality classification;tobacco quality data;support vector machine;support vector machines support vector machine classification crops artificial neural networks principal component analysis kernel information science data mining educational institutions feature extraction;crop quality classification;quality management;artificial neural network	Quality evaluation and classification is very important for crop market price determination. A lot of methods have been applied in the field of quality classification including principal component analysis (PCA) and artificial neural network (ANN) etc. The use of ANN has been shown to be a cost-effective technique. But their training is featured with some drawbacks such as small sample effect, black box effect and prone to overfitting. This paper proposes a novel hybrid approach of kernel principal component analysis (KPCA) with support vector machine (SVM) for developing the accuracy of quality classification. The tobacco quality data is evaluated in the experiment. Traditional PCA-SVM, SVM and ANN are investigated as comparison basis. The experimental results show that the proposed approach can achieve better performance in crop quality classification.	artificial neural network;black box;experiment;kernel principal component analysis;overfitting;support vector machine	Jiang Wei;Lv Jiake;Wang Xuan;Sun Rongrong	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.384	support vector machine;computer science;machine learning;pattern recognition;data mining;artificial neural network	SE	8.42014104885707	-38.50780065804745	169902
af7028613ca1700699513edfc94e1687c06a5720	an empirical comparison of dimensionality reduction methods for classifying gene and protein expression datasets	decision tree classifier;lung cancer;locally linear embedding;high dimensionality;statistical significance;linear discriminate analysis;classification;gene expression;dimensionality reduction;gene expression analysis;machine learning;nonlinear dimensionality reduction;principal component analysis;isomap;high dimensional data;multidimensional scaling;ovarian cancer;graph embedding;protein expression;support vector machine;proteomics;classification accuracy;linear discriminant analysis;dimensional reduction;local linear embedding;quantitative evaluation;prostate cancer;bioinformatics	The recent explosion in availability of gene and protein expression data for cancer detection has necessitated the development of sophisticated machine learning tools for high dimensional data analysis. Previous attempts at gene expression analysis have typically used a linear dimensionality reduction method such as Principal Components Analysis (PCA). Linear dimensionality reduction methods do not however account for the inherent nonlinearity within the data. The motivation behind this work is to demonstrate that nonlinear dimensionality reduction methods are more adept at capturing the nonlinearity within the data compared to linear methods, and hence would result in better classification and potentially aid in the visualization and identification of new data classes. Consequently, in this paper, we empirically compare the performance of 3 commonly used linear versus 3 nonlinear dimensionality reduction techniques from the perspective of (a) distinguishing objects belonging to cancer and non-cancer classes and (b) new class discovery in high dimensional gene and protein expression studies for different types of cancer. Quantitative evaluation using a support vector machine and a decision tree classifier revealed statistically significant improvement in classification accuracy by using nonlinear dimensionality reduction methods compared to linear methods.	clustering high-dimensional data;decision tree;emoticon;machine learning;nonlinear dimensionality reduction;nonlinear system;principal component analysis;support vector machine	George Lee;Carlos Rodriguez;Anant Madabhushi	2007		10.1007/978-3-540-72031-7_16	gene expression;computer science;machine learning;pattern recognition;data mining;mathematics;proteomics;linear discriminant analysis;dimensionality reduction	ML	8.728904325432785	-48.997357334354305	170067
91af33f366ea7c48110103c1e3afec699778b8d9	structured sparse canonical correlation analysis for brain imaging genetics: an improved graphnet method		MOTIVATION Structured sparse canonical correlation analysis (SCCA) models have been used to identify imaging genetic associations. These models either use group lasso or graph-guided fused lasso to conduct feature selection and feature grouping simultaneously. The group lasso based methods require prior knowledge to define the groups, which limits the capability when prior knowledge is incomplete or unavailable. The graph-guided methods overcome this drawback by using the sample correlation to define the constraint. However, they are sensitive to the sign of the sample correlation, which could introduce undesirable bias if the sign is wrongly estimated.   RESULTS We introduce a novel SCCA model with a new penalty, and develop an efficient optimization algorithm. Our method has a strong upper bound for the grouping effect for both positively and negatively correlated features. We show that our method performs better than or equally to three competing SCCA models on both synthetic and real data. In particular, our method identifies stronger canonical correlations and better canonical loading patterns, showing its promise for revealing interesting imaging genetic associations.   AVAILABILITY AND IMPLEMENTATION The Matlab code and sample data are freely available at http://www.iu.edu/∼shenlab/tools/angscca/   CONTACT shenli@iu.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.		Lei Du;Heng Huang;Jingwen Yan;Sungeun Kim;Shannon L. Risacher;Mark Inlow;Jason H. Moore;Andrew J. Saykin;Li Shen	2016	Bioinformatics	10.1093/bioinformatics/btw033	computer science;bioinformatics;machine learning;statistics	Comp.	7.235733666020147	-51.02874618788089	170439
03c3f75f5ca05dc49d8902fedff1f1d7cd3c24c8	the impact of clonal mixing on the evolution of social behaviour in aphids	animals;female;social insects;life cycle;aphids;aphid;models biological;behavior animal;selection genetic;animal migration;biological evolution;reproduction;social behavior;population structure;behaviour;population dynamics;social behaviour;computer simulation;ecosystem;evolution	Reports of substantial clonal mixing measured in social aphid colonies seem, on the face of it, to rule out population structure as an explanation of this enigmatic insect's social behaviour. To clarify how selection operates in aphids, and to disentangle direct and indirect fitness components, we present a model of the life cycle of a typical colony-dwelling aphid. The model incorporates ecological factors and includes a trade-off between investing in social behaviour and investing in reproduction. Our focus on inclusive fitness contrasts with previous approaches that optimize colony output. Through deriving a variant of Hamilton's rule, we show that a simple relationship can be established between the patch-carrying capacity and immigration rates into patches. Our results indicate that the levels of clonal mixing reported are not inconsistent with social behaviour. We discuss our model in terms of the evolutionary origins of social behaviour in aphids.	aphids;aphis chenopodii glauci preparation;biological evolution;clone;cryptocurrency tumbler;ecology;national origin	John Bryden;Vincent A A Jansen	2010	Proceedings. Biological sciences	10.1098/rspb.2009.1876	computer simulation;biology;social behavior;population dynamics;ecology;genetics	ML	-4.500069330564785	-46.37163669216147	170508
af84e99be3ae8b14c4b19aea9c43da933282a60e	determining noisy instances relative to attributes of interest	data cleaning;noise detection;attribute of interest	An important issue in the analysis of data is that of data quality. Algorithms for the detection of instances with class noise and, to a lesser degree, attribute noise have been presented in the literature. We propose a novel technique to detect noisy instances relative to an Attribute of Interest. The attribute of interest can be any feature from the dataset as defined by the domain-specific practitioner. The proposed technique determines those instances that contain noise relative to the chosen attribute of interest. This approach can be iterated for any number of user-specified attributes. Our methodology is demonstrated with empirical case studies using real-world datasets and is verified by an expert-based validation of the results. Additional studies show the effectiveness of our technique in detecting noise injected into instances. For detecting noise relative to the class or dependent variable, our technique is compared to the well-known classification and ensemble filters and outperforms both techniques on a real-world dataset with known class noise. Based on the results of a wide variety of case studies presented in this work, we conclude that our methodology for ranking noisy instances relative to an attribute of interest is an effective and useful noise handling procedure.		Taghi M. Khoshgoftaar;Jason Van Hulse	2006	Intell. Data Anal.		computer science;machine learning;pattern recognition;data mining;data cleansing;statistics	ML	7.0680423345183785	-42.67104189128794	170612
869a18e04c76872afd66a49a447729b49dab0903	backward chaining rule induction	microarray data;hypothesis generation;lung cancer;induction machine;rule exploration;rule induction;decision tree;molecular mechanisms;corresponding author;systems biology;class discovery;prior knowledge;rule learning;data mining;semi supervised learning;feature interaction;gene expression;data analysis;microarray data analysis;machine learning;association rule;non small cell lung cancer;brute;system biology;interactive induction;iterative exploration;microarray;molecular mechanics;decision trees	Exploring the vast number of possible feature interactions in domains such as gene expression microarray data is an onerous task. We describe Backward-Chaining Rule Induction (BCRI) as a semi-supervised mechanism for biasing the search for IF-THEN rules that express plausible feature interactions. BCRI adds to a relatively limited tool-chest of hypothesis generation software and is an alternative to purely unsupervised association-rule learning. We illustrate BCRI by using it to search for gene-to-gene causal mechanisms that underlie lung cancer. Mapping hypothesized gene interactions against prior knowledge offers support and explanations for hypothesized interactions, and suggests gaps in current knowledge that induction might help fill. BCRI is implemented as a wrapper around a base supervised-rule-learning algorithm. We summarize our prior work with an adaptation of C4.5 as the base algorithm (C45-BCRI), extending this in the current study to use Brute as the base algorithm (Brute-BCRI). In contrast to C4.5’s greedy strategy, Brute extensively searches the rule space. Moreover, Brute returns many more rules (i.e., hypothesized feature interactions) than does C4.5. To remain an effective hypothesis-generation tool requires that Brute-BCRI more carefully rank and prune hypothesized interactions than does C45-BCRI. Prior knowledge serves to evaluate final BruteBCRI rules just as it does with C45-BCRI, but prior knowledge also serves to evaluate and prune intermediate search states, thus maintaining a manageable number of rules for evaluation by a domain expert.	association rule learning;backward chaining;biasing;c4.5 algorithm;causal filter;greedy algorithm;interaction;microarray;rule induction;semi-supervised learning;semiconductor industry;subject-matter expert	Douglas H. Fisher;Mary E. Edgerton;Zhihua Chen;Lianhong Tang;Lewis J. Frey	2006	Intell. Data Anal.		microarray analysis techniques;computer science;bioinformatics;machine learning;decision tree;data mining;systems biology	ML	8.450527526777229	-46.3002125047352	170718
51376b15910a83eb65ad8a8845157c18acf2341c	model selection with bic and icl criteria for binned data clustering by bin-em-cem algorithms	model selection;pattern clustering;gaussian processes;clustering;bin em cem;bic;parsimonious gaussian mixture models;icl;expectation maximisation algorithm	Several clustering approaches are adapted to binned data in order to accelerate the clustering process or to deal with data of limited precision. Bin-EM-CEM algorithms of fourteen parsimonious Gaussian mixture models are developed. Each model performs differently according to its specific feature. Without knowing any information of the data, a criterion is considered to select the best model in order to obtain a good result. In this article, BIC and ICL criteria are adapted to binned data clustering to choose the bin-EM-CEM algorithm of the right model as well as the number of clusters. By different experiments on simulated data and real data, the performance of BIC and ICL criteria in model selection for binned data clustering are studied and compared on different aspects.	akaike information criterion;algorithm;bayesian information criterion;cluster analysis;computation;data mining;experiment;icl;mixture model;model selection;occam's razor;simulation;time complexity	Hani Hamdan;Jingwen Wu	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.534	correlation clustering;determining the number of clusters in a data set;k-medians clustering;fuzzy clustering;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;gaussian process;mathematics;cluster analysis;model selection;statistics	DB	2.941868444046391	-40.85883598078131	171748
b334764b6db01b6581486bf8a9e1e15e6550c6f0	image segmentation based on multiscale fast spectral clustering		In recent years, spectral clustering has become one of the most popular clustering algorithms for image segmentation. However, it has restricted applicability to large-scale images due to its high computational complexity. In this paper, we first propose a novel algorithm called Fast Spectral Clustering based on quad-tree decomposition. The algorithm focuses on the spectral clustering at superpixel level and its computational complexity is O(n logn)+O(m)+O(m 3 2 ); its memory cost is O(m), where n and m are the numbers of pixels and the superpixels of a image. Then we propose Multiscale Fast Spectral Clustering by improving Fast Spectral Clustering, which is based on the hierarchical structure of the quad-tree. The computational complexity of Multiscale Fast Spectral Clustering is O(n logn) and its memory cost is O(m). Extensive experiments on real large-scale images demonstrate that Multiscale Fast Spectral Clustering outperforms Normalized cut in terms of lower computational complexity and memory cost, with comparable clustering accuracy.		Chongyang Zhang;Guofeng Zhu;Minxin Chen;Hong Chen;Chenjian Wu	2018	CoRR			Vision	-2.527325197191796	-40.76092761181873	172005
209081306f6e9617adaa479110db32b95904c6a9	efficient techniques for genotype‐phenotype correlational analysis	health informatics;genetic association studies;information systems and communication service;multifactor dimensionality reduction;principal component analysis;management of computing and information systems;algorithms;humans;polymorphism single nucleotide	BACKGROUND Single Nucleotide Polymorphisms (SNPs) are sequence variations found in individuals at some specific points in the genomic sequence. As SNPs are highly conserved throughout evolution and within a population, the map of SNPs serves as an excellent genotypic marker. Conventional SNPs analysis mechanisms suffer from large run times, inefficient memory usage, and frequent overestimation. In this paper, we propose efficient, scalable, and reliable algorithms to select a small subset of SNPs from a large set of SNPs which can together be employed to perform phenotypic classification.   METHODS Our algorithms exploit the techniques of gene selection and random projections to identify a meaningful subset of SNPs. To the best of our knowledge, these techniques have not been employed before in the context of genotype-phenotype correlations. Random projections are used to project the input data into a lower dimensional space (closely preserving distances). Gene selection is then applied on the projected data to identify a subset of the most relevant SNPs.   RESULTS We have compared the performance of our algorithms with one of the currently known best algorithms called Multifactor Dimensionality Reduction (MDR), and Principal Component Analysis (PCA) technique. Experimental results demonstrate that our algorithms are superior in terms of accuracy as well as run time.   CONCLUSIONS In our proposed techniques, random projection is used to map data from a high dimensional space to a lower dimensional space, and thus overcomes the curse of dimensionality problem. From this space of reduced dimension, we select the best subset of attributes. It is a unique mechanism in the domain of SNPs analysis, and to the best of our knowledge it is not employed before. As revealed by our experimental results, our proposed techniques offer the potential of high accuracies while keeping the run times low.		Subrata Saha;Sanguthevar Rajasekaran;Jinbo Bi;Sudipta Pathak	2013		10.1186/1472-6947-13-41	health informatics;medicine;multifactor dimensionality reduction;bioinformatics;data mining;principal component analysis	ML	7.507725680832124	-50.363385485374025	172006
30e7b03c1eda046edc42e6f47ce9f24b703517e6	the percolation method for an efficient grouping of data		Abstract   This article presents a method for clustering data. The method is based upon the concept of “percolation” which is a progressive formation of clusters that starts with points having a maximum “density”. Comparisons with principal component analysis, hierarchical classification, and other methods, show why the P.A.P. method (Progressive Assimilation and Percolation) is superior and safer than most known methods.	percolation	Raymond Trémolières	1979	Pattern Recognition	10.1016/0031-3203(79)90035-9	econometrics;continuum percolation theory;machine learning;mathematics;clique percolation method;statistics	Vision	5.288178564779704	-40.4660946039913	172148
c540ebd92f67dd7fdcb87b121a3a69249e16ed64	data clustering using bacterial foraging optimization	optimization based clustering;data mining;data clustering;bacterial foraging optimization	Clustering divides data into meaningful or useful groups (clusters) without any prior knowledge. It is a key technique in data mining and has become an important issue in many fields. This article presents a new clustering algorithm based on the mechanism analysis of Bacterial Foraging (BF). It is an optimization methodology for clustering problem in which a group of bacteria forage to converge to certain positions as final cluster centers by minimizing the fitness function. The quality of this approach is evaluated on several well-known benchmark data sets. Compared with the popular clustering method named k-means algorithm, ACO-based algorithm and the PSO-based clustering technique, experimental results show that the proposed algorithm is an effective clustering technique and can be used to handle data sets with various cluster sizes, densities and multiple dimensions.	algorithm;benchmark (computing);bioinformatics;brainfuck;cluster analysis;converge;data mining;display resolution;fitness function;k-means clustering;loss function;mathematical optimization;optimization problem;particle swarm optimization;real life;sensor;sequence clustering	Miao Wan;Lixiang Li;Jinghua Xiao;Cong Wang;Yixian Yang	2011	Journal of Intelligent Information Systems	10.1007/s10844-011-0158-3	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;bioinformatics;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data	ML	3.58690099918205	-42.030883445102695	172369
676841a1febddf7dbea13528959f56c269db8fed	jnmfma: a joint non-negative matrix factorization meta-analysis of transcriptomics data		MOTIVATION Tremendous amount of omics data being accumulated poses a pressing challenge of meta-analyzing the heterogeneous data for mining new biological knowledge. Most existing methods deal with each gene independently, thus often resulting in high false positive rates in detecting differentially expressed genes (DEG). To our knowledge, no or little effort has been devoted to methods that consider dependence structures underlying transcriptomics data for DEG identification in meta-analysis context.   RESULTS This article proposes a new meta-analysis method for identification of DEGs based on joint non-negative matrix factorization (jNMFMA). We mathematically extend non-negative matrix factorization (NMF) to a joint version (jNMF), which is used to simultaneously decompose multiple transcriptomics data matrices into one common submatrix plus multiple individual submatrices. By the jNMF, the dependence structures underlying transcriptomics data can be interrogated and utilized, while the high-dimensional transcriptomics data are mapped into a low-dimensional space spanned by metagenes that represent hidden biological signals. jNMFMA finally identifies DEGs as genes that are associated with differentially expressed metagenes. The ability of extracting dependence structures makes jNMFMA more efficient and robust to identify DEGs in meta-analysis context. Furthermore, jNMFMA is also flexible to identify DEGs that are consistent among various types of omics data, e.g. gene expression and DNA methylation. Experimental results on both simulation data and real-world cancer data demonstrate the effectiveness of jNMFMA and its superior performance over other popular approaches.   AVAILABILITY AND IMPLEMENTATION R code for jNMFMA is available for non-commercial use via http://micblab.iim.ac.cn/Download/.   CONTACT hqwang@ustc.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	antivirus software;bioinformatics;carcinogenesis;carcinoma of lung;degs1 wt allele;data dependency;degree (graph theory);dimensions;division algorithm;donkey kong country;estimated;experiment;extraction;gene expression;genetic heterogeneity;influenza virus a hong kong ab:acnc:pt:ser:qn;methylation;natural science disciplines;negativity (quantum mechanics);neoplasms;non-negative matrix factorization;omics;population parameter;r language;ravens colored progressive matrices;sensor;simulation;sparse matrix;systems biology;version;xfig;emotional dependency;funding grant;study of epigenetics	Hong-Qiang Wang;Qing Yan;Xing-Ming Zhao	2015	Bioinformatics	10.1093/bioinformatics/btu679	meta-analysis;computer science;bioinformatics;non-negative matrix factorization	Comp.	7.175827641390939	-51.59242061336568	172391
57a7c9ad73debd96d67016140ebaa04cab31686e	using density-based incremental clustering for anomaly detection	security of data pattern clustering;program behavior;pattern clustering;incremental clustering anomaly detection program behavior normal profile;anomaly detection;incremental clustering;false alarm rate;normal profile;incremental detection quality density based incremental clustering anomaly detection algorithm system usage pattern program behavior re clustering updating 1998 darpa bsm audit data noise data objects analogous incremental algorithm adwice;incremental algorithm;security of data;clustering algorithms intrusion detection computer science educational institutions spatial databases shape software engineering information science detection algorithms noise generators	This paper proposed a new anomaly detection algorithm that can update normal profile of system usage pattern dynamically. The feature used to model systempsilas usage pattern was program behavior. When system usage pattern changed, new program behaviors will be inserted into old profiles by density-based incremental clustering. Compared to traditional re-clustering updating, it is much more efficiently. Experiments with 1998 DARPA BSM audit data, shows that normal profiles generated by our algorithm is less sensitive to noise data objects than profile generated by analogous incremental algorithm ADWICE. So our algorithm shows an incremental detection quality and a much lower false alarm rate.	algorithm;anomaly detection;cluster analysis;openbsm	Fei Ren;Liang Hu;Hao Liang;Xiaobo Liu;Weiwu Ren	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.811	correlation clustering;anomaly detection;data stream clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;constant false alarm rate;cluster analysis;population-based incremental learning	SE	-0.8354706063108618	-38.215108729696226	172432
78dc8e797e492f50dbdd52285b160218a2a07527	a benchmark to select data mining based classification algorithms for business intelligence and decision support systems		In today’s business scenario, we percept major changes in how managers use computerized support in making decisions. As more number of decision-makers use computerized support in decision making, decision support systems (DSS) is developing from its starting as a personal support tool and is becoming the common resource in an organization. DSS serve the management, operations, and planning levels of an organization and help to make decisions, which may be rapidly changing and not easily specified in advance. Data mining has a vital role to extract important information to help in decision making of a decision support system. It has been the active field of research in the last two-three decades. Integration of data mining and decision support systems (DSS) can lead to the improved performance and can enable the tackling of new types of problems. Artificial Intelligence methods are improving the quality of decision support, and have become embedded in many applications ranges from ant locking automobile brakes to these days interactive search engines. It provides various machine learning techniques to support data mining. The classification is one of the main and valuable tasks of data mining. Several types of classification algorithms have been suggested, tested and compared to determine the future trends based on unseen data. There has been no single algorithm found to be superior over all others for all data sets. Various issues such as predictive accuracy, training time to build the model, robustness and scalability must be considered and can have tradeoffs, further complex the quest for an overall superior method. The objective of this paper is to compare various classification algorithms that have been frequently used in data mining for decision support systems. Three decision trees based algorithms, one artificial neural network, one statistical, one support vector machines with and without adaboost and one clustering algorithm are tested and compared on four datasets from different domains in terms of predictive accuracy, error rate, classification index, comprehensibility and training time. Experimental results demonstrate that Genetic Algorithm (GA) and support vector machines based algorithms are better in terms of predictive accuracy. Former shows highest comprehensibility but is slower than later. From the decision tree based algorithms, QUEST produces trees with lesser breadth and depth showing more comprehensibility. This research work shows that GA based algorithm is more powerful algorithm and shall be the first choice of organizations for their decision support systems. SVM without adaboost shall be the first choice in context of speed and predictive accuracy. Adaboost improves the accuracy of SVM but on the cost of large training time.	adaboost;artificial intelligence;artificial neural network;automated planning and scheduling;backpropagation;benchmark (computing);bit error rate;boosting (machine learning);cluster analysis;data mining;decision support system;decision tree;embedded system;fastest;genetic algorithm;html attribute;k-means clustering;lock (computer science);logistic regression;machine learning;mike lesser;scalability;software propagation;software release life cycle;statistical classification;support vector machine;web search engine	Pardeep Kumar;Nitin;Vivek Kumar Sehgal;Durg Singh Chauhan	2012	CoRR	10.5121/ijdkp.2012.2503	intelligent decision support system;decision tree learning;decision engineering;computer science;artificial intelligence;machine learning;decision tree;data mining;database	ML	8.531822158738528	-39.00512359537365	172673
e781ccbbc9ef81bace2aa047e3b3e57ce1eccba6	hierarchical divisive clustering with multi view-point based similarity measure		Clustering is task of grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups. In this paper, we introduce hierarchical divisive clustering with multi view point based similarity measure. The hierarchical clustering is produced by the sequence of repeated bisections. The bisecting incremental k-means with multi view point based similarity measure is used in the clustering. We compare our approach with the existing algorithms on various document collections to verify the advantage of our proposed method.	similarity measure	S. Jayaprada;Amarapini Aswani;Gunjiganur Vemanaradhya Gayathri	2013		10.1007/978-3-319-02931-3_55	correlation clustering;fuzzy clustering;consensus clustering;hierarchical clustering;single-linkage clustering;hierarchical clustering of networks	ML	1.509569236368301	-41.85565436389089	173386
79870b4c095d60f1f7b850512e5e1f136da3442d	agglomerative fuzzy k-means clustering algorithm with selection of number of clusters	mining methods and algorithms;clustering algorithms application software clustering methods pattern recognition computer vision algorithm design and analysis statistical analysis minimization methods optimization methods genetic algorithms;cluster algorithm;cluster validation;pattern clustering;analyse amas;fuzzy k means;dato numerico;analisis datos;application software;validacion;algorithme k moyenne;agglomerative fuzzy k means clustering;cluster validation agglomerative fuzzy k means clustering numerical data penalty term objective function consistent clustering results;algoritmo borroso;logique floue;logica difusa;donnee numerique;minimization methods;data mining;classification;fuzzy set theory;fonction objectif;computer vision;fuzzy logic;objective function;fuzzy k means clustering;fuzzy clustering;data analysis;cluster analysis;statistical analysis;numerical data;fouille donnee;clustering;fuzzy algorithm;number of clusters;pattern recognition;pattern clustering fuzzy set theory;algorithme flou;clustering algorithms;funcion objetivo;algoritmo k media;analyse donnee;k means algorithm;validation;genetic algorithms;analisis cluster;difference set;agglomerative;cluster validity;consistent clustering results;synthetic data;penalty term;clustering methods;clustered data;busca dato;clasificacion;k means clustering;algorithm design and analysis;clustering data mining mining methods and algorithms;optimization methods	In this paper, we present an agglomerative fuzzy K-means clustering algorithm for numerical data, an extension to the standard fuzzy K-means algorithm by introducing a penalty term to the objective function to make the clustering process not sensitive to the initial cluster centers. The new algorithm can produce more consistent clustering results from different sets of initial clusters centers. Combined with cluster validation techniques, the new algorithm can determine the number of clusters in a data set, which is a well known problem in $k$-means clustering. Experimental results on synthetic data sets (2 to 5 dimensions, 500 to 5000 objects and 3 to 7 clusters), the BIRCH two-dimensional data set of 20000 objects and 100 clusters, and the WINE data set of 178 objects, 17 dimensions and 3 clusters from UCI, have demonstrated the effectiveness of the new algorithm in producing consistent clustering results and determining the correct number of clusters in different data sets, some with overlapping inherent clusters.	algorithm;birch;cluster analysis;k-means clustering;level of measurement;loss function;numerical analysis;optimization problem;synthetic data	Mark Junjie Li;Michael K. Ng;Yiu-ming Cheung;Joshua Zhexue Huang	2008	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2008.88	nearest-neighbor chain algorithm;complete-linkage clustering;correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;subclu;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data	DB	2.7926482476630685	-39.88390380441319	173401
fa2a3ead6e68710965b40a46f5ab726abf27d6dc	learning with kernels: a local rademacher complexity-based analysis with application to graph kernels		When dealing with kernel methods, one has to decide which kernel and which values for the hyperparameters to use. Resampling techniques can address this issue but these procedures are time-consuming. This problem is particularly challenging when dealing with structured data, in particular with graphs, since several kernels for graph data have been proposed in literature, but no clear relationship among them in terms of learning properties is defined. In these cases, exhaustive search seems to be the only reasonable approach. Recently, the global Rademacher complexity (RC) and local Rademacher complexity (LRC), two powerful measures of the complexity of a hypothesis space, have shown to be suited for studying kernels properties. In particular, the LRC is able to bound the generalization error of an hypothesis chosen in a space by disregarding those ones which will not be taken into account by any learning procedure because of their high error. In this paper, we show a new approach to efficiently bound the RC of the space induced by a kernel, since its exact computation is an NP-Hard problem. Then we show for the first time that RC can be used to estimate the accuracy and expressivity of different graph kernels under different parameter configurations. The authors’ claims are supported by experimental results on several real-world graph data sets.	appendix;brute-force search;computation (action);deuterium;experiment;generalization (psychology);generalization error;glucose^4h dwell specimen:scnc:pt:dial fld prt:qn;graph - visual representation;kernel (operating system);kernel method;local area networks;maxima and minima;np-hardness;population parameter;rademacher complexity;replication competent retrovirus;resampling (statistics);retro city rampage;separators;social inequality;test set;replication compartment	Luca Oneto;Nicol&#x00F2; Navarin;Michele Donini;Sandro Ridella;Alessandro Sperduti;Fabio Aiolli;Davide Anguita	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2017.2771830	resampling;artificial intelligence;kernel (linear algebra);machine learning;computer science;data model;rademacher complexity;hilbert space;brute-force search;kernel method;data set	ML	1.9171660765248941	-50.41540934980202	173438
f3a1f712a3e9b820127bb62b045f0f0d49d10ffb	utility-friendly heterogenous generalization in privacy preserving data publishing		K-anonymity is one of the most important anonymity mod- els that have been widely investigated and various techniques have been proposed to achieve it. Among them generalization is a common tech- nique. In a typical generalization approach, tuples in a table was first divided into many QI(quasi-identifier)-groups such that the size of each QI-group is larger than K. In general, utility of anonymized data can be enhanced if size of each QI-group is reduced. Motivated by this ob- servation, we propose linking-based anonymity model, which achieves K-anonymity with QI-groups having size less than K. To implement linking-based anonymization model, we propose a simple yet efficient heuristic local recoding method. Extensive experiments on real data sets are also conducted to show that the utility has been significantly im- proved by our approach compared to the state-of-the-art methods.		Xianmang He;Dong Li;Yanni Hao;HuaHui Chen	2014		10.1007/978-3-319-12206-9_15	computer science;data mining;database;world wide web	DB	-4.097149109793243	-39.36429325595421	173623
7f99c4dee4ec0440e38bca98f1c40bd6c52c99a1	a maximizing model of bezdek-like spherical fuzzy c-means	spherical clustering;fuzzy c means			Yuchi Kanzawa	2015	JACIII	10.20965/jaciii.2015.p0662	computer science;machine learning;pattern recognition	Vision	2.6347181187788005	-40.773302848382414	174281
9dfe562d70cc0fef47ee7ed4b32bc3e8de6779ea	lb hust: a symmetrical boundary distance for clustering time series	pattern clustering;time series data mining pattern clustering;data dissimilarity;symmetrical boundary distance;time series clustering;asymmetrical distance measure;time series;data mining;time measurement euclidean distance distortion measurement databases educational institutions computer science wavelet analysis discrete wavelet transforms time series analysis aggregates;time series mining;asymmetrical distance measure symmetrical boundary distance time series clustering time series mining data similarity data dissimilarity time series distance measure;data similarity;time series distance measure	Clustering is an important technology in mining time series, and the key is to define the similarity or dissimilarity between data. One of existing time series distance measures LB_Keogh, is tighter lower bounding than Euclidean and dynamic time warping (DTW), however, it is an asymmetrical distance measure, and has its limitation in clustering.To solve the problem, we present a symmetrical boundary distance measure called LB_HUST, and prove that it is tighter lower bounding than LB_Keogh. We apply LB_HUST to cluster time series, and update the boundary of the cluster when a new time series is added into the cluster. The experiments show that the method exceeds the approaches based on Euclidean and DTW in terms of accuracy.	cluster analysis;dynamic time warping;experiment;lattice boltzmann methods;time series	Li Junkui;Wang Yuanzhen;Li Xinping	2006	9th International Conference on Information Technology (ICIT'06)	10.1109/ICIT.2006.63	machine learning;time series;pattern recognition;hierarchical clustering;statistics	DB	0.6015607323454187	-39.639885031562955	174422
9745faa697368253fc8c98603111a2d0909542de	big data analytics for price forecasting in smart grids	forecasting;kernel;support vector machines;redundancy;principal component analysis;correlation;algorithm design and analysis	Demand side management (DSM) is a key mechanism to make smart grids cost efficient using electricity price forecasting issue. Price forecasting method takes the big price data into account, and gives estimates of the future electricity price. However, most of existing price forecasting methods cannot avoid redundancy at feature selection and lack of an integrated framework that coordinates the steps in forecasting. To address this issue, we first propose a new electricity price forecasting framework. It is significant to design a system tool chain based on big data analytics for ensuring that the users can make appropriate decisions. To this end, three algorithms are proposed integratedly. First, feature redundancy elimination is implemented by the fusion of Grey Correlation Analysis (GCA) and ReliefF algorithm. Second, a combination of Kernel function and Principle Component Analysis (KPCA) is designed to achieve dimensionality reduction. Finally, Support Vector Machine (SVM) optimization algorithm based on differential evolution (DE) is proposed to forecast price classification. These three modules jointly power the price forecasting system. Simulation results show the superiority of our proposed framework.	algorithm;big data;cost efficiency;differential evolution;dimensionality reduction;electricity price forecasting;feature selection;kernel principal component analysis;mathematical optimization;simulation;statistical classification;support vector machine;time complexity;time series;toolchain	Kun Wang;Chenhan Xu;Song Guo	2016	2016 IEEE Global Communications Conference (GLOBECOM)	10.1109/GLOCOM.2016.7841630	support vector machine;algorithm design;probabilistic forecasting;kernel;forecasting;computer science;data science;machine learning;data mining;redundancy;correlation;statistics;principal component analysis	EDA	8.284580587719582	-38.740025172261234	174707
adeda37ae8ac73720de10a9acbccdd8f1d859a0a	bidirectional hierarchical clustering for web mining	minimisation;two pass approach bidirectional hierarchical clustering web mining intra cluster similarity maximization cluster merging criterion semantic net bottom up cluster merging phase;hierarchical clustering;hierarchical structure;pattern clustering;bottom up;top down;web mining clustering algorithms partitioning algorithms shape computer science educational institutions web sites explosives computational complexity noise shaping;data mining;pattern clustering data mining web sites internet computational complexity minimisation;semantic net;internet;computational complexity;web sites;web mining	In this paper we propose a new bidirectional hierarchical clustering system for addressing challenges of web mining. The key feature of our approach is that it aims to maximize the intra-cluster similarity in the bottom-up cluster-merging phase and it ensures to minimize the inter-cluster similarity in the top-down refinement phase. This two-pass approach achieves better clustering than existing one-pass approaches. We also propose a new cluster-merging criterion for allowing more than two clusters to be merged in each step and a new measure of similarity for taking into consideration not only the inter-connectivity between clusters but also the internal connectivity within the clusters. These result in reducing the average complexity for creating the final hierarchical structure of clusters from O(n) to O(n). The hierarchical structure represents a semantic structure between concepts of clusters and is directly applicable to the future of semantic net.	cluster analysis;computer cluster;hierarchical clustering;refinement (computing);semantic network;top-down and bottom-up design;web mining	Zhongmei Yao;Ben Choi	2003		10.1109/WI.2003.1241281	correlation clustering;constrained clustering;web mining;fuzzy clustering;computer science;data science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;top-down and bottom-up design;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;biclustering;affinity propagation;hierarchical clustering of networks	ML	0.32185196342137024	-43.12587918981005	174716
3007860b703afd18a63dfd030dcdcf6599defc79	identifying transcription factor binding sites based on a neural network	dna;enumeration;enumeracion;transcription factor binding site;chaine caractere;bioinformatique;cadena caracter;pattern recognition;reconnaissance forme;bioinformatica;biological data;synthetic data;reseau neuronal;reconocimiento patron;dna sequence;red neuronal;character string;neural network;bioinformatics	The identification of regulatory motifs (transcription factor binding sites) in DNA sequences is a difficult pattern recognition problem. Many methods have been developed in the past few years. Although some are better than the others in a sense, yet not a single one is recognized to be the best. Generally, in the case of long and subtle motifs, exhaustive enumeration becomes problematic. In this paper, we present a new method which improves exhaustive enumeration based on a neural network. We test its performance on both synthetic data and realistic biological data. It proved to be successful in identifying very subtle motifs. Experiments also show our method outperforms some popular methods in terms of identifying subtle motifs. We refer to the new method as IMNN (Identifying Motifs based on a Neural Network).	artificial neural network;experiment;gene regulatory network;medical transcription;pattern recognition;sequence motif;synthetic data;transcription (software)	Zhiming Dai;Xianhua Dai;Jiang Wang	2006		10.1007/11760191_106	dna sequencing;string;biological data;computer science;bioinformatics;artificial intelligence;enumeration;dna;artificial neural network;dna binding site;synthetic data	Comp.	3.407368068118376	-47.00841903877694	174819
aff2dd7a85feb3e0445a629a42b7b7175c9fe2e4	applying fuzzy technologies to equivalence learning in protein classification	fuzzy logic;proteins;machine learning;protein classification;similarity learning;protein similarity;artificial intelligence;algorithms;computational biology;protein kinases;databases protein	When sequencing a new genome, its function and structure are important concerns, and inferring methods are based on protein sequence similarity methods. However, sequence groups differ in their parameters such as the number of group members and intra- and inter-class variability. A method that performs well on one group may not perform well on another group. Thus, learning similarity in a supervised manner could provide a general framework to set a similarity function to a specific sequence class. Here we describe a novel method that learns a similarity function between proteins by using a binary classifier and pairs of equivalent sequences (belonging to the same class) as positive samples, and non- equivalent sequences (belonging to different classes) as negative training samples. For sequence pair representation, we propose to use advanced techniques from fuzzy theory, including a sigmoid-type function for normalization and the class of Dombi operators that provide a more robust method. Using some additional constraints, the learned function turns out to be a valid kernel or metric function, and we present a new way of learning it, along with a new parameter-weighting technique. Using a dataset of archeal, bacterial, and eukaryotic 3-phosphoglycerate-kinase sequences (3PGK) and clusters from COG, we evaluate this equivalence learning method from a protein classification point of view. A receiver operator characteristic (ROC) analysis shows that we get a much more robust and accurate methodology for protein classification when these techniques are applied together. (See online Supplementary Material at www.liebertonline.com).	automatic vectorization;binary classification;bioinformatics;biopolymer sequencing;cellular material:mcnt:pt:calculus:qn:estimated;class;cog (project);fuzzy logic;kernel;method (computer programming);non-functional requirement;physical object;pierre robin syndrome;population parameter;protein structure prediction;question (inquiry);receiver operator characteristics;receiver operating characteristic;svk;sequence alignment;sigmoid colon;sigmoid function;silo (dataset);similarity measure;spatial variability;staphylococcal protein a;supervised learning;switch statement;turing completeness;interest	József Dombi;Attila Kertész-Farkas	2009	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2008.0147	fuzzy logic;computer science;bioinformatics;machine learning;pattern recognition;mathematics	Comp.	7.437849919524935	-50.59359762390004	174924
0f96f949132b00d4be14530244940adbc6fa804c	intrusion detection system platform based on light-weighted hybrid artificial immune algorithms	detectors;security of data principal component analysis;pca processed data intrusion detection system platform light weighted hybrid artificial immune algorithms;generic algorithm;ga;immune algorithm;real time;intrusion detection principal component analysis entropy feature extraction software algorithms educational institutions testing monitoring cloning performance evaluation;intrusion detection;data mining;dcsa;hybrid;scsa;pca processed data;heuristic algorithms;feature extraction;light weighted hybrid artificial immune algorithms;principal component analysis;intrusion detection system platform;detection rate;artificial immune algorithm;iris;hybrid ga artificial immune algorithm pca scsa dcsa;pca;security of data;intrusion detection system	Detectors generation algorithm plays an important role in IDS. In this paper, an IDS platform based on light-weighted HIDS is bought up. Firstly raw data is extracted by PCA for effective detection. And then detector generation algorithms based on NSA, SCSA, DCSA are implemented and simulated on PCA processed data. Tests have proved their efficiency. Hybrid intrusion detection is designed for real time detection and high detection rate. Finally, a comprehensive IDS platform is designed combining all above algorithms to compare performances and further study.	algorithm;host-based intrusion detection system;performance;principal component analysis	Jinyin Chen;Dongyong Yang	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.509	engineering;machine learning;pattern recognition;data mining	Robotics	6.832003272674336	-38.109370462108465	175208
6beb0a4ec2823769268ee75636f1f6e5878b032e	exploiting tree-based variable importances to selectively identify relevant variables	false discovery rate;supervised learning;machine learning;bioinformatics;permutation test	This paper proposes a novel statistical procedure based on permutation tests for extracting a subset of truly relevant variables from multivariate importance rankings derived from treebased supervised learning methods. It shows also that the direct extension of the classical approach based on permutation tests for estimating false discovery rates of univariate variable scoring procedures does not extend very well to the case of multivariate tree-based importance measures.	resampling (statistics);supervised learning	Vân Anh Huynh-Thu;Louis Wehenkel;Pierre Geurts	2008			machine learning;pattern recognition;mathematics;statistics	ML	7.7405856403498055	-45.74449007902554	175378
1f06329bf91dba7ee89ebd1561d93e20f6cceae1	optimal algorithms for local vertex quartet cleaning	settore inf 01 informatica;time complexity;quartet based methods;evolutionary trees;error correction;algorithms;optimal algorithm	Reconstructing evolutionary trees is an important problem in biology. A response to the computational intractability of most of the traditional criteria for inferring evolutionary tree has been a focus on new criteria, particularly quartet-based methods that seek to merge trees derived on subsets of four species from a given species-set into a tree for that entire set. Unfortunately, most of these methods are very sensitive to errors in the reconstruction of the trees for individual quartets of species. A recently-developed technique called quartet cleaning can alleviate this difficulty in certain cases by using redundant information in the complete set of quartet topologies for a given species-set to correct such errors. In this paper, we describe two new local vertex quartet cleaning algorithms which have optimal time complexity and error-correction bound, respectively. These are the first known local vertex quartet cleaning algorithms that are optimal with respect to either of these attributes.	algorithm;computational complexity theory;error detection and correction;phylogenetic tree;phylogenetics;plasma cleaning;sputter cleaning;time complexity;trees (plant);vertex;tetrandrine	Gianluca Della Vedova;Todd Wareham	2002	Bioinformatics	10.1145/508791.508827	time complexity;mathematical optimization;combinatorics;phylogenetic tree;error detection and correction;computer science;mathematics;algorithm	Theory	1.1058067172441404	-50.96785970674119	175703
b45ba1955c3671a125c7e3ade2f45e10a67d633d	ant colony based fuzzy c-means clustering for very large data		Fuzzy C-Means (FCM) is a popular technique for clustering of data. It combines the concepts of K-Means algorithm and Fuzzy set theory. However, FCM faces the challenges of running into a local optimal value, and of producing results which are sensitive to initialisation conditions. To solve these problems, there has been prior work which incorporates Ant Colony Optimisation (ACO) into the conventional FCM algorithm. The authors of this paper find that though the FCM-ACO algorithm is a definite improvement over the traditional FCM, there is still scope for improving the scalability and accuracy of the system. The authors propose using a Multi Round Sampling (MRS) technique along with Ant colony Optimisation. The proposed algorithm allows us to cluster the dataset without considering it entirely, hence allowing for a more space and time efficient system. This makes the system highly scalable and hence suitable for large datasets. Moreover, extensive experiments on several publicly available datasets, both large and small, prove that the proposed algorithm of Multi Round Sampling of Ant Colony based Fuzzy C-Means (MRSA-FCM) gives superior clustering results, over the FCM and FCM-ACO systems.	ant colony	Dhruv Mullick;Ayush Garg;Arpit Bajaj;Swati Aggarwal	2017		10.1007/978-3-319-66824-6_51	fuzzy logic;ant colony optimization algorithms;fuzzy clustering;data mining;ant colony;fuzzy set;sampling (statistics);scalability;cluster analysis;computer science	ML	0.9447548338154973	-40.214382075215646	176134
f1cdcbfa5ea364b3a7b4bbe7ae73b26a36f9374c	clustering pairwise distances with missing data: maximum cuts versus normalized cuts	cluster algorithm;spectral clustering;kolmogorov complexity;natural language;missing data;distance matrix;normalized cut	Clustering algorithms based on a matrix of pairwise similarities (kernel matrix) for the data are widely known and used, a particularly popular class being spectral clustering algorithms. In contrast, algorithms working with the pairwise distance matrix have been studied rarely for clustering. This is surprising, as in many applications, distances are directly given, and computing similarities involves another step that is error-prone, since the kernel has to be chosen appropriately, albeit computationally cheap. This paper proposes a clustering algorithm based on the SDP relaxation of the max-k-cut of the graph of pairwise distances, based on the work of Frieze and Jerrum. We compare the algorithm with Yu and Shi’s algorithm based on spectral relaxation of a norm-k-cut. Moreover, we propose a simple heuristic for dealing with missing data, i.e., the case where some of the pairwise distances or similarities are not known. We evaluate the algorithms on the task of clustering natural language terms with the Google distance, a semantic distance recently introduced by Cilibrasi and Vitányi, using relative frequency counts from WWW queries and based on the theory of Kolmogorov complexity.	algorithm;cluster analysis;cognitive dimensions of notations;distance matrix;heuristic;kolmogorov complexity;linear programming relaxation;minimum k-cut;missing data;natural language;norm (social);segmentation-based object categorization;semantic similarity;spectral clustering;www	Jan Poland;Thomas Zeugmann	2006		10.1007/11893318_21	complete-linkage clustering;correlation clustering;constrained clustering;data stream clustering;distance matrix;k-medians clustering;fuzzy clustering;missing data;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;natural language;single-linkage clustering;k-medoids;biclustering;spectral clustering;statistics;clustering high-dimensional data	ML	-0.15583644331377486	-41.000850338588265	176282
91a70dd4c76c70c2ae09d25cc5286f5279609e87	clustering likelihood curves: finding deviations from single clusters	lc ms ms;soft computing;mass spectrometry;probabilistic approach;likelihood curve;fuzzy clustering;hybrid approach;clustering;itraqtm	For systematic analyses of quantitative mass spectrometry data a method was developed in order to reveal peptides within a protein, that show differences in comparison with the remaining peptides of the protein concerning their regulatory characteristics. Regulatory information is calculated and visualised by a probabilistic approach resulting in likelihood curves. On the other hand the algorithm for the detection of one or more clusters is based on fuzzy clustering, so that our hybrid approach combines probabilistic concepts as well as principles from soft computing. The test is able to decide whether peptides belonging to the same protein, cluster into one or more group. In this way obtained information is very valuable for the detection of single peptides or peptide groups which can be regarded as regulatory outliers.	algorithm;cluster analysis;computer cluster;data point;fuzzy clustering;soft computing;volume rendering	Claudia Hundertmark;Frank Klawonn	2008		10.1007/978-3-540-87656-4_48	correlation clustering;mass spectrometry;fuzzy clustering;computer science;bioinformatics;machine learning;data mining;soft computing;cluster analysis;single-linkage clustering	Comp.	6.230181784486701	-49.28204740358746	176305
2cfcab1a68ea0646674e66a17cb154f21d273e96	clustering algorithm for network partitioning with a hypergraph model.	cluster algorithm;network partitioning;clustering algorithm;cost function;complexity analysis;seed generator			Jing Lee;Jung-Hua Chou;Shen-Li Fu	1993	J. Inf. Sci. Eng.		correlation clustering;combinatorics;data stream clustering;k-medians clustering;computer science;theoretical computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;cluster analysis;single-linkage clustering;linde–buzo–gray algorithm;k-medoids;hierarchical clustering of networks;k-means clustering;clustering high-dimensional data	DB	2.4937396903133	-41.26045375735028	176397
2ecc286c3cc6f9b77044555620d9b96ee14655ae	differential gene expression graphs: a data structure for classification in dna microarrays	dna;phenotype sample classifiers;graph theory;clustering differential gene expression graph data structure dna microarrays classification phenotype sample classifiers feature extraction;training;construction industry;differential gene expression;genetics;gene expression data structures dna diseases feature extraction spine tree data structures classification tree analysis decision trees data analysis;dna microarrays classification;gene expression;graph theory bioinformatics dna genetics;clustering;data structures;feature extraction;classification algorithms;diseases;dna microarray;data structure;pathology;differential gene expression graph;bioinformatics	This paper proposes an innovative data structure to be used as a backbone in designing microarray phenotype sample classifiers. The data structure is based on graphs and it is built from a differential analysis of the expression levels of healthy and diseased tissue samples in a microarray dataset. The proposed data structure is built in such a way that, by construction, it shows a number of properties that are perfectly suited to address several problems like feature extraction, clustering, and classification.	algorithm;clique (graph theory);cluster analysis;dna microarray;data model;data structure;experiment;feature extraction;gene expression profiling;heuristic (computer science);internet backbone;noise reduction;statistical classification	Alfredo Benso;Stefano Di Carlo;Gianfranco Politano;Luca Sterpone	2008	2008 8th IEEE International Conference on BioInformatics and BioEngineering	10.1109/BIBE.2008.4696689	biology;gene chip analysis;gene expression;dna microarray;data structure;feature extraction;computer science;bioinformatics;pattern recognition;data mining;cluster analysis;genetics;dna	Robotics	5.541708731170202	-48.311743223538315	176614
9126a3e663408c49c4f0205890326f06d473cf0d	algorithms and architectures for parallel processing		In recent years, Data Stream Mining (DSM) has received a lot of attention due to the increasing number of applicative contexts which generate temporally ordered, fast changing, and potentially infinite data. To deal with such data, learning techniques require to satisfy several computational and storage constraints so that new and specific methods have to be developed. In this paper we introduce a new strategy for dealing with the problem of streaming time series clustering. The method allows to detect a partition of the streams over a user chosen time period and to discover evolutions in proximity relations. We show that it is possible to reach these aims, performing the clustering of temporally non overlapping data batches arriving on-line and then running a suitable clustering algorithm on a dissimilarity matrix updated using the outputs of the local clustering. Through an application on real and simulated data, we will show that this method provides results comparable to algorithms for stored data.	algorithm;applicative programming language;cluster analysis;clustering coefficient;data stream mining;distance matrix;online and offline;temporal logic;time series	J. Kołodziej;Beniamino Di Martino;Domenico Talia;Kaiqi Xiong	2013		10.1007/978-3-319-03859-9		ML	-1.5881903646410007	-38.04594263865686	176709
cb51606f9d6345f187a2440c6bd8ebebb15825d6	fast algorithm for integrating clustering with ranking on heterogeneous graphs		The demands for graph data analysis methods, e.g., clustering and ranking, are increasing. RankClus is a framework to extract good clusters by integrating clustering and ranking on heterogeneous graphs; it enhances the clustering results by alternately updates the results of clustering and ranking for the better understanding of the clusters. However, RankClus is computationally expensive if a graph is large since it needs to iterate both clustering and ranking for all nodes. In this paper, to address this problem, we propose a novel fast RankClus algorithm for heterogeneous graphs. To speed up the entire procedure of RankClus, our proposed algorithm reduces the computational cost of the ranking process in each iteration. Our proposal measures how each node affects the clustering result; if it is not significant, we prune the node. Our extensive evaluations showed that our proposal is much faster than the original method.		Kotaro Yamazaki;Tomoki Sato;Hiroaki Shiokawa;Hiroyuki Kitagawa	2018		10.1145/3282373.3282376	data mining;cluster analysis;speedup;cluster (physics);algorithm;computer science;data analysis;ranking;graph	ML	-2.2770922679136683	-40.66744015714475	176835
5f95b4e6cf52d61fb45210876db6c45601eda1f7	a balanced iterative random forest for gene selection from microarray data	clustering analysis;female;support vector machines;expression data;bayes theorem;journal article;computational biology bioinformatics;naive bayes classifier;models genetic;recognition;reproducibility of results;child;acute lymphoblastic leukemia;algorithms;humans;patterns;cancer classification;neoplasms;combinatorial libraries;computational biology;computer appl in life sciences;genetic markers;gene expression profiling;oligonucleotide array sequence analysis;microarrays;bioinformatics	The wealth of gene expression values being generated by high throughput microarray technologies leads to complex high dimensional datasets. Moreover, many cohorts have the problem of imbalanced classes where the number of patients belonging to each class is not the same. With this kind of dataset, biologists need to identify a small number of informative genes that can be used as biomarkers for a disease. This paper introduces a Balanced Iterative Random Forest (BIRF) algorithm to select the most relevant genes for a disease from imbalanced high-throughput gene expression microarray data. Balanced iterative random forest is applied on four cancer microarray datasets: a childhood leukaemia dataset, which represents the main target of this paper, collected from The Children’s Hospital at Westmead, NCI 60, a Colon dataset and a Lung cancer dataset. The results obtained by BIRF are compared to those of Support Vector Machine-Recursive Feature Elimination (SVM-RFE), Multi-class SVM-RFE (MSVM-RFE), Random Forest (RF) and Naive Bayes (NB) classifiers. The results of the BIRF approach outperform these state-of-the-art methods, especially in the case of imbalanced datasets. Experiments on the childhood leukaemia dataset show that a 7% ∼ 12% better accuracy is achieved by BIRF over MSVM-RFE with the ability to predict patients in the minor class. The informative biomarkers selected by the BIRF algorithm were validated by repeating training experiments three times to see whether they are globally informative, or just selected by chance. The results show that 64% of the top genes consistently appear in the three lists, and the top 20 genes remain near the top in the other three lists. The designed BIRF algorithm is an appropriate choice to select genes from imbalanced high-throughput gene expression microarray data. BIRF outperforms the state-of-the-art methods, especially the ability to handle the class-imbalanced data. Moreover, the analysis of the selected genes also provides a way to distinguish between the predictive genes and those that only appear to be predictive.	algorithm;biological markers;class;colon classification;experiment;gene expression;high-throughput computing;information;iteration;iterative method;microarray;nc (complexity);naive bayes classifier;patients;radio frequency;random forest;randomness;recursion (computer science);set-top box;silo (dataset);support vector machine;throughput;leukemia	Ali Anaissi;Paul J. Kennedy;Madhu Goyal;Daniel R. Catchpoole	2013		10.1186/1471-2105-14-261	biology;support vector machine;naive bayes classifier;dna microarray;computer science;bioinformatics;data science;genetic marker;data mining;gene expression profiling;pattern;cluster analysis;bayes' theorem;genetics	ML	8.19293269039926	-50.179623529742535	176861
b5cf5941bd4c3438bcac3913cb547b8a371376b1	a novel clustering approach: simple swarm clustering		Clustering categorizes data into meaningful groups without any prior knowledge. This paper presents a novel swarm-base clustering algorithm inspired from flock movement. Many algorithms solve the problem by optimizing a cost function but ours clusters data by applying one rule on data agent movements. We demonstrated that not only this simple rule is sufficient but completely effective in accurately dividing the data into natural clusters. It is a good model of how simply nature solves complex problems. Unlike some algorithms, this one does not need number of desired cluster in advance and discovers it by itself correctly. Eight data sets were used to compare the algorithm with five well-known algorithms. K-means and k-harmonic fail to find none-Gaussian clusters and two other swarm-base algorithms suffer severely from performance but our algorithm works successfully in both cases. The result confirms the superiority of our method.	cluster analysis;swarm	Seyed Ghasem RazaviZadegan;Seyed Mohammad RazaviZadegan	2014		10.1007/978-3-319-06932-6_22	correlation clustering;flame clustering;canopy clustering algorithm;consensus clustering;cure data clustering algorithm;cluster analysis;brown clustering;biclustering;hierarchical clustering of networks	Vision	3.149413432816422	-42.111045939914746	176974
6ed86e36e514b07b2ec0ff92716af439f88084cf	an em method based on entropy ld block partition for haplotype inference	medical computing diseases expectation maximisation algorithm genetics inference mechanisms;bioinformatics genomics accuracy biological cells;multilocus linkage disequilibrium em method entropy ld block partition genetic diseases genetic research genotypes haplotype inference problem partition ligation strategy;inference mechanisms;genetics;medical computing;expectation maximization;diseases;haplotype inference;expectation maximization haplotype inference linkage disequilibrium;linkage disequilibrium;genetic disease;expectation maximisation algorithm	Genetic diseases have attracted much attention to the genetic research which depends on the data named haplotypes. Because of most haplotypes are generated from genotypes, haplotype inference (HI) problem becomes very popular. To solve this problem, we propose a new method based on EM and partition-ligation (PL) strategy. Compared to the previous methods, our algorithm uses the PL strategy based on multilocus linkage disequilibrium (LD), which has an advantage over the uniform block partition and pairwise LD. Considering the scale of data and the missing alleles, we also change the ligation strategy to control the complexity of time and space. The experimental results on the real data and the simulated data show that the algorithm in this paper has better performance than previous ones.	block cipher;em (typography);expectation–maximization algorithm;linkage (software);partition type	Yun Xu;Ying Wang;Xiaohui Yao;YuZhong Zhao	2010	2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)	10.1109/BICTA.2010.5645337	biology;bioinformatics;genetics;statistics	Robotics	2.7114796548584064	-50.821442077715645	177065
5d02557b704b6a5cf61fcc936a6cc29a4bbdb8f3	approaches to the automatic discovery of patterns in biosequences	expressive power;research paper;pattern language	This paper surveys approaches to the discovery of patterns in biosequences and places these approaches within a formal framework that systematises the types of patterns and the discovery algorithms. Patterns with expressive power in the class of regular languages are considered, and a classification of pattern languages in this class is developed, covering the patterns that are the most frequently used in molecular bioinformatics. A formulation is given of the problem of the automatic discovery of such patterns from a set of sequences, and an analysis is presented of the ways in which an assessment can be made of the significance of the discovered patterns. It is shown that the problem is related to problems studied in the field of machine learning. The major part of this paper comprises a review of a number of existing methods developed to solve the problem and how these relate to each other, focusing on the algorithms underlying the approaches. A comparison is given of the algorithms, and examples are given of patterns that have been discovered using the different methods.	algorithm;bioinformatics;expressive power (computer science);machine learning;pattern language;programming languages;regular language	Alvis Brazma;Inge Jonassen;Ingvar Eidhammer;David R. Gilbert	1998	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.1998.5.279	software design pattern;computer science;bioinformatics;artificial intelligence;machine learning;data mining;pattern language;expressive power;algorithm	Comp.	1.1938600506877353	-47.18254214158376	177705
b630364dc55ccb49b14616ae33440bdafa5720df	clustering in tweets using a fuzzy neighborhood model	pattern clustering;kernel;oceans;typhoons;word processing constraint handling fuzzy set theory pattern clustering social networking online;fuzzy set theory;accidents;social networking online;rain;clustering algorithms;constraint handling;cluster interpretability keyword clustering fuzzy neighborhood model tweet word sequence product space agglomerative hierarchical clustering c means clustering pairwise constraint;twitter;kernel twitter clustering algorithms typhoons oceans rain accidents;word processing	Clustering of keywords in tweets is studied. A series of tweets is handled as a sequence of words and an inner product space is introduced to a set of keywords on the basis of positive definite kernels using a fuzzy neighborhood defined on that sequence. Methods of agglomerative hierarchical clustering as well as c-means clustering are applied. Pairwise constraints are moreover introduced to improve interpretability of clusters. Real tweets are analyzed with discussion of the resulting clusters.	cluster analysis;dendrogram;hierarchical clustering;kernel (operating system)	Sadaaki Miyamoto;Shohei Suzuki;Satoshi Takumi	2012	2012 IEEE International Conference on Fuzzy Systems	10.1109/FUZZ-IEEE.2012.6250800	correlation clustering;constrained clustering;data stream clustering;kernel;typhoon;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;fuzzy set;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	Robotics	2.503358646904667	-39.758377898008646	178024
6a76756c23929718f2aeb20d37427530046f97be	pattern discovery for large mixed-mode database	mixed mode data;attribute clustering;discrete data;unsupervised discretization;data type;data mining;pattern discovery;mixed mode;mutual information;clustered data	In business and industry today, large databases with mixed data types (continuous and categorical) are very common. There are great needs to discover patterns from them for knowledge interpretation and understanding. In the past, for classification, this problem is solved as a discrete data problem by first discretizing the continuous data based on the class-attribute interdependence relationship. However, so far no proper solution exists when class information is unavailable. Hence, important pattern post-processing tasks such as pattern clustering and summarization cannot be applied to mixed-mode data. This paper presents a new method for solving the problem. It is based on two essential concepts. (1) Though class information is absent, yet for a correlated dataset, the attribute with the strongest interdependence with others in the group can be used to drive the discretization of the continuous data. (2) For a large database, correlated attribute groups must first be obtained by attribute clustering before (1) can be applied. Based on (1) and (2), pattern discovery methods are developed for mixed-mode data. Extensive experiments using synthetic and real world data were conducted to validate the usefulness and effectiveness of the proposed method.	attribute grammar;cluster analysis;database;discrete mathematics;discretization;experiment;html attribute;interdependence;mixed-signal integrated circuit;synthetic intelligence;video post-processing	Andrew K. C. Wong;Bin Wu;Gene P. K. Wu;Keith C. C. Chan	2010		10.1145/1871437.1871547	data type;computer science;data science;machine learning;pattern recognition;data mining;database;mutual information;programming language;statistics	DB	-2.3141703354265544	-42.058087980190244	178293
7fccf13727b3660ac2356b443b850a63ea7df591	vs-fcm: validity-guided spatial fuzzy c-means clustering for image segmentation	color clustering;fuzzy clustering;fuzzy c means	In this paper a new fuzzy clustering approach to the color clustering problem has been proposed. To deal with the limitations of the traditional FCM algorithm, we propose a spatial homogeneity-based FCM algorithm. Moreover, the cluster validity index is employed to automatically determine the number of clusters for a given image. We refer to this method as VS-FCM algorithm. The effectiveness of the proposed method is demonstrated through various clustering examples.		Bo-Yeong Kang;Dae-Won Kim	2010	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2010.10.1.089	correlation clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	AI	2.1125254537937876	-40.49498470230819	178324
3467dcf87e4c4af891a8a91679c35e3f24ed46e7	agnostic classification of markovian sequences	cross entropy;explicit knowledge;information theoretic	"""Classification of finite sequences without explicit knowledge of their statistical nature is a fundamental problem with many important applications. We propose a new information theoretic approach to this problem which is based on the following ingredients: (i) sequences are similar when they are likely to be generated by the same source; (ii) cross entropies can be estimated via """"universal compression""""; (iii) Markovian sequences can be asymptotically-optimally merged. With these ingredients we design a method for the classification of discrete sequences whenever they can be compressed. We introduce the method and illustrate its application for hierarchical clustering of languages and for estimating similarities of protein sequences."""	cluster analysis;hierarchical clustering;information theory;peptide sequence	Ran El-Yaniv;Shai Fine;Naftali Tishby	1997			computer science;explicit knowledge;theoretical computer science;machine learning;pattern recognition;mathematics;cross entropy	ML	-0.7161024010944317	-47.945696527961516	178340
4553a83efc8924c5d45e8473d5f27ab5fb3adcfb	mapreduce approach to collective classification for networks	classification in networks;label propagation;mapreduce;collective classification	The collective classification problem for big data sets using MapReduce programming model was considered in the paper. We introduced a proposal for implementation of label propagation algorithm in the network. The method was examined on real dataset in telecommunication domain. The results indicated that it can be used to classify nodes in order to propose new offerings or tariffs to customers.	big data;iterative method;label propagation algorithm;mapreduce;node (computer science);programming model;software propagation	Wojciech Indyk;Tomasz Kajdanowicz;Przemyslaw Kazienko;Slawomir Plamowski	2012		10.1007/978-3-642-29347-4_76	computer science;data mining;database;world wide web	Vision	-3.518374029702767	-40.400643433219315	178512
92618325e77d77815503c760197ed30ef3303596	multi-objective rule mining using simulated annealing algorithm		Association rule mining process can be visualized as a multi-objective problem rather than as a single objective one. Measures like support, confidence and other interestingness criteria which are used for evaluating a rule, can be thought of as different objectives of association rule mining problem. Support count is the number of records, which satisfies all the conditions that exist in the rule. This objective represents the accuracy of the rules extracted from the database. Confidence represents the proportion of records for which the prediction of the rule (or model in the case of a complete classification) is correct, and it is one of the most widely quoted measures of quality, especially in the context of complete classification. Interestingness measures how much interesting the rule is. Using these three measures as the objectives of rule mining problem, this article uses a Simulated Annealing algorithm to extract some useful and interesting rules from any type databases. The experimental results show that the algorithm may be suitable for large datasets.	algorithm;association rule learning;database;simulated annealing	Mehdi Nasiri;Leyla Sadat Taghavi;Behrouz Minaee	2010	JCIT		simulated annealing;computer science;machine learning;adaptive simulated annealing	ML	6.9460652884903125	-43.9302225235513	178565
10c847060816ffb9ef9dccd05888038d8c875df3	feature grouping-based outlier detection upon streaming trajectories		Outlier detection acts as one of the most important analysis tasks for trajectory stream. In stream scenarios, such properties as unlimitedness, time-varying evolutionary, sparsity, and skewness distribution of trajectories pose new challenges to outlier detection technique. Trajectory outlier detection techniques mainly focus on finding trajectory that is dissimilar to the majority of the others, which is based on the hypothesis that they are probably generated by a different mechanism. Most distance-based methods tend to utilize a function (e.g., weighted linear sum) to measure the similarity of two arbitrary objects provided that representative features have been extracted in advance. However, this kind of method is not tailored to identify the outlier which is close to its neighbors according to some features, but behaves significantly different from its neighbors in terms of the other features. To address this issue, we propose a feature grouping-based mechanism that divides all the features into two groups, where the first group (Similarity Feature) is used to find close neighbors and the second group (Difference Feature) is used to find outliers within the similar neighborhood. According to the feature differences among local adjacent objects in one or more time intervals, we present two outlier definitions, including local anomaly trajectory fragment (TF-outlier) and evolutionary anomaly moving object (MO-outlier ). We devise a basic solution and then an optimized algorithm to detect both types of outliers. Experimental results show that our proposal is both effective and efficient to detect outliers upon trajectory data streams.	acm transactions on database systems;anomaly detection;big data;distributed computing;experiment;k-nearest neighbors algorithm;level of detail;spark;scalability;sensor;sparse matrix;streaming media	Jiali Mao;Tao Wang;Cheqing Jin;Aoying Zhou	2017	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2017.2744619	anomaly detection;skewness;machine learning;data mining;feature extraction;data stream mining;outlier;artificial intelligence;trajectory;basic solution;mathematics;pattern recognition	DB	-0.7774185638454997	-38.61944364123576	178834
8cfc74220f7e374c45e8bab7ad78a384e32a5445	a new multi-objective technique for differential fuzzy clustering	remote sensing image;cluster algorithm;fuzzy c mean;differential evolution;pareto optimal;pixel classification;statistical significance;cluster validity measures;modified differential evolution;fuzzy clustering;multiobjective optimization;genetic algorithm;cluster validity;pareto optimality	This article presents a new multiobjective differential evolution based fuzzy clustering technique. Recent research has shown that clustering techniques that optimize a single objective may not provide satisfactory result because no single validity measure works well on different kinds of data sets. The fact motivated us to present a new multiobjective Differential Evolution based fuzzy clustering technique that encodes the cluster centres in its vectors and optimizes multiple validity measures simultaneously. In the final generation, it produces a set of non-dominated solutions, from which the user can relatively judge and pick up the most promising one according to the problem requirements. Superiority of the uzzy clustering odified differential evolution ultiobjective optimization areto-optimal luster validity measures proposed method over its single objective versions, multiobjective version of classical differential evolution and genetic algorithm, well-known fuzzy C-means and average linkage clustering algorithms has been demonstrated quantitatively and visually for several synthetic and real life data sets. Statistical significance test has been conducted to establish the statistical superiority of the proposed multiobjective clustering approach. Finally, the proposed algorithm has been applied for segmentation of a remote s effe sensing image to show it	cluster analysis;differential evolution;fuzzy clustering;genetic algorithm;linkage (software);mathematical optimization;real life;requirement;synthetic intelligence;turing test;upgma	Indrajit Saha;Ujjwal Maulik;Dariusz Plewczynski	2011	Appl. Soft Comput.	10.1016/j.asoc.2010.11.007	differential evolution;correlation clustering;constrained clustering;mathematical optimization;data stream clustering;genetic algorithm;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;multi-objective optimization;machine learning;data mining;mathematics;statistical significance;cluster analysis	ML	4.063503706138278	-41.94518062560724	178922
27ecb0b89834d3db82bff26e69771bbb2048d964	a composite gene selection for dna microarray data analysis	non overlap area distribution measurement;unsupervised gene selection;supervised gene selection;tnom score;microarray;neural network	An important aspect in microarray data analysis is the selection of an appropriate number of the most relevant genes among a large population of genes. In this study, we have proposed a composite gene selection using both unsupervised and supervised gene selections. In the unsupervised gene selection, we used the threshold number of misclassification (TNoM) score to select an appropriate number of the top-ranked genes for microarray data analysis. In the supervised gene selection, the minimum number of genes showing the highest accuracy is obtained using the non-overlap area distribution measurement (NADM) method provided by the neural network with weighted fuzzy membership functions (NEWFM) from the top-ranked genes. In this study, from a colon cancer dataset and a leukemia dataset, we selected the top-ranked 93 colon cancer and 143 leukemia genes with ≤14 (colon cancer) and ≤13 (leukemia) TNoM scores from a total of 2000 colon cancer and 7129 leukemia genes. By the NADM method, a minimum of 4 colon cancer and 13 leukemia genes were selected from the top-ranked 93 colon cancer and 143 leukemia genes. When the minimal 4 colon cancer and 13 leukemia genes were used as inputs for the NEWFM, the performance accuracies were 98.39 % and 100 % for colon cancer and leukemia, respectively.	artificial neural network;colon classification;dna microarray;overlap zone;unsupervised learning	Dong-Kyun Park;Eun-Young Jung;Sang-Hong Lee;Joon S. Lim	2013	Multimedia Tools and Applications	10.1007/s11042-013-1583-9	computer science;bioinformatics;machine learning;pattern recognition;microarray;artificial neural network	Comp.	9.222489983589343	-48.67517318715067	178926
fa1ea00cb4845011325089f54faf87109ca91ea5	a family of fuzzy and defuzzified c-means algorithms	cluster algorithm;hard c means algorithm;pattern clustering;fuzzy c mean;sequential algorithm;regression analysis entropy fuzzy set theory pattern clustering;regression model;fuzzy set theory;defuzzified c means algorithms;hard clustering algorithm;c regression model fuzzy c means algorithms defuzzified c means algorithms hard c means algorithm generalized entropy based fuzzy c means hard clustering algorithm sequential algorithm matrix multiplication;clustering algorithms covariance matrix virtual colonoscopy computational intelligence computational modeling automation intelligent agent internet business minimization methods;fuzzy c means algorithms;regression analysis;matrix multiplication;entropy;c regression model;generalized entropy based fuzzy c means	This paper proposes a family of fuzzy and hard c-means algorithms. The hard clustering algorithms are derived from defuzzifying a generalized entropy-based fuzzy c-means whereby cluster volume size variables and covariance variables are introduced into hard clustering algorithms. Sequential algorithms are also derived by using advanced formulas of matrix multiplication. Crisp c-means as well as c-regression models are studied. Moreover effectiveness and efficiency of the proposed algorithms are compared using artificial as well as real data sets	automation;cluster analysis;computation;computational intelligence;defuzzification;e-commerce;fuzzy cognitive map;loss function;matrix multiplication;optimization problem;sequential algorithm	Sadaaki Miyamoto;Takeshi Yasukochi;Ryo Inokuchi	2005	International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)	10.1109/CIMCA.2005.1631463	mathematical optimization;fuzzy clustering;computer science;machine learning;pattern recognition;mathematics;regression analysis	Robotics	3.127116524452146	-39.341764825387116	179180
914e5e7ef8be4f74e904ef200bde1ebd3d9d5d2d	a hybrid evolutionary approach for the protein classification problem	genetic program;classification algorithm;evolutionary computation;protein sequence;enzyme;genetic programming;satisfiability;classification rules;protein classification;classification system;genetic algorithm;decision tree induction;protein classification problem;protein data bank;hybrid algorithm;evolutionary computing;bioinformatics	This paper proposes a hybrid algorithm that combines characteristics of both Genetic Programming (GP) and Genetic Algorithms (GAs), for discovering motifs in proteins and predicting their functional classes, based on the discovered motifs. In this algorithm, individuals are represented as IF-THEN classi cation rules. The rule antecedent consists of a combination of motifs automatically extracted from protein sequences. The rule consequent consists of the functional class predicted for a protein whose sequence satis es the combination of motifs in the rule antecedent. The system can be used in two di erent ways. First, as a stand-alone classi cation system, where the evolved classi cation rules are directly used to predict the functional classes of proteins. Second, the system can be used just as an attribute construction method, discovering motifs that are given, as predictor attributes, to another classi cation algorithm. In this usage of the system, a classical decision tree induction algorithm was used as the classi er. The proposed system was evaluated in these two scenarios and compared with another Genetic Algorithm designed speci cally for the discovery of motifs and therefore used only as an attribute construction algorithm. This comparison was performed by mining an enzyme data set extracted from the Protein Data Bank. The best results were obtained when using the proposed hybrid GP/GA as an attribute construction algorithm and performing the classi cation (using the constructed attributes) with the decision tree induction algorithm.	decision tree;feature vector;genetic algorithm;genetic programming;hybrid algorithm;kerrison predictor;peptide sequence;protein data bank;sequence motif;software release life cycle	Denise Fukumi Tsunoda;Heitor Silvério Lopes;Alex Alves Freitas	2009		10.1007/978-3-642-04441-0_55	genetic programming;enzyme;genetic algorithm;protein data bank;hybrid algorithm;computer science;bioinformatics;artificial intelligence;machine learning;protein sequencing;pattern recognition;evolutionary computation;satisfiability	ML	8.66744600276456	-46.67743440801644	179228
1fe54da0913334c1e79a2e0e9975b8edcb491de7	an architecture for component-based design of representative-based clustering algorithms	generic algorithm;k means;reusable component;representative based clustering algorithms;architecture	We propose an architecture for the design of representative-based clustering algorithms based on reusable components. These components were derived from K-means-like algorithms and their extensions. With the suggested clustering design architecture, it is possible to reconstruct popular algorithms, but also to build new algorithms by exchanging components from original algorithms and their improvements. In this way, the design of a myriad of representative-based clustering algorithms and their fair comparison and evaluation are possible. In addition to the architecture, we show the usefulness of the proposed approach by providing experimental evaluation.	algorithm;cluster analysis;component-based software engineering	Boris Delibasic;Milan Vukicevic;Milos Jovanovic;Kathrin Kirchner;Johannes Ruhland;Milija Suknovic	2012	Data Knowl. Eng.	10.1016/j.datak.2012.03.005	correlation clustering;constrained clustering;probabilistic analysis of algorithms;fuzzy clustering;computer science;theoretical computer science;architecture;machine learning;data mining;cluster analysis;biclustering	ML	1.5547333661533547	-41.099446771021356	179405
c255c5648f143c53ff518333015e1dd2cf7b9bd7	reusable component-based architecture for decision tree algorithm design	decision tree;component;classification;algorithm;rapid miner;architecture	Many decision tree algorithms were proposed over the last few decades. A lack of publishing standards for decision tree algorithm software produced a large time gap between algorithm proposals and their wider application in practice. Non-existence of common repository for storing algorithms and their parts led to a need to re-implement these algorithms from a scratch when they had to be implemented on a different platform. This makes the comparison between algorithms and their partial improvements vague. In addition, combinations and interactions between different algorithm parts haven't been analyzed thoroughly. Reusable component design of decision tree algorithms has been recently suggested as a potential solution to these problems. In this paper we describe an architecture for component-based (white-box) decision tree algorithm design, and we present an open-source framework which enables design and fair testing of decision tree algorithms and their parts. This architecture and developed platform can provide the research community with a common codebase for storing, designing, and evaluating decision tree algorithms (traditional, multivariate and hybrid) and their partial improvements. It is intended for data mining practitioners, algorithm and software developers, and as well for students, as a technology enhanced learning tool.	algorithm design;decision tree model	Milan Vukicevic;Milos Jovanovic;Boris Delibasic;Sonja Isljamovic;Milija Suknovic	2012	International Journal on Artificial Intelligence Tools	10.1142/S0218213012500224	c4.5 algorithm;decision tree model;decision tree learning;computer science;artificial intelligence;theoretical computer science;architecture;machine learning;decision tree;incremental decision tree;data mining;component;id3 algorithm	Robotics	8.968020865172214	-45.31990623759706	179511
00b2ac45640bb6d5075c0ab69f1994537983274c	robust biomarker identification for cancer diagnosis with ensemble feature selection methods	institutional repositories;science general;microarray data;fedora;ensemble method;cancer;tumor maligno;selection;diagnostico;methode;gene expression data;classification;vital;large scale;marqueur biologique;tumor;cancer diagnosis;identification;high dimensional data;identificacion;feature selection;tumeur maligne;support vector machine;biological data;extraction caracteristique;marcador biologico;seleccion;vtls;computational biology;diagnosis;gene selection;metodo;biological marker;biomedical application;method;ils;malignant tumor;diagnostic	MOTIVATION Biomarker discovery is an important topic in biomedical applications of computational biology, including applications such as gene and SNP selection from high-dimensional data. Surprisingly, the stability with respect to sampling variation or robustness of such selection processes has received attention only recently. However, robustness of biomarkers is an important issue, as it may greatly influence subsequent biological validations. In addition, a more robust set of markers may strengthen the confidence of an expert in the results of a selection method.   RESULTS Our first contribution is a general framework for the analysis of the robustness of a biomarker selection algorithm. Secondly, we conducted a large-scale analysis of the recently introduced concept of ensemble feature selection, where multiple feature selections are combined in order to increase the robustness of the final set of selected features. We focus on selection methods that are embedded in the estimation of support vector machines (SVMs). SVMs are powerful classification models that have shown state-of-the-art performance on several diagnosis and prognosis tasks on biological data. Their feature selection extensions also offered good results for gene selection tasks. We show that the robustness of SVMs for biomarker discovery can be substantially increased by using ensemble feature selection techniques, while at the same time improving upon classification performances. The proposed methodology is evaluated on four microarray datasets showing increases of up to almost 30% in robustness of the selected biomarkers, along with an improvement of approximately 15% in classification performance. The stability improvement with ensemble methods is particularly noticeable for small signature sizes (a few tens of genes), which is most relevant for the design of a diagnosis or prognosis model from a gene signature.   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	bioinformatics;biological markers;computational biology;embedded system;embedding;ensemble learning;feature selection;microarray;nitroprusside;performance;sampling (signal processing);selection (genetic algorithm);selection algorithm;support vector machine	Thomas Abeel;Thibault Helleputte;Yves Van de Peer;Pierre Dupont;Yvan Saeys	2010	Bioinformatics	10.1093/bioinformatics/btp630	computer science;bioinformatics;machine learning;data mining;feature selection;statistics;cancer	Comp.	8.944294516340154	-49.357399562268455	179968
cabde7dfa1b66a42bb0f5e6f456c0bf9c0862155	speedup of fuzzy co-clustering algorithm for image segmentation on graphic processing unit	parallel computing;image classification;fuzzy co clustering;graphic processing unit	The fuzzy co-clustering algorithms are used to solve the problem clustering of large data, multi-dimension, multi-feature. When the dimension and size of data increases, the size of the membership function matrix increases, the number of calculations increases, computational problems become a burden for the computer. In fuzzy co-clustering algorithm, multiple rules and sub-algorithms may be executed in parallel by the GPU to speedup for the whole system. This paper studies the structure of fuzzy co-clustering algorithm and proposed a new solution how to implement fuzzy co-clustering algorithms on GPU. GPU-based calculations are solutions to improve computational efficiency and independent operation for the CPU. The study is demonstrated through experiments on color image clustering on CPU and GPU. The experiments were shown that implementing fuzzy co-clustering on GPU is much faster than on CPU.	algorithm;biclustering;central processing unit;cluster analysis;color image;computational problem;experiment;graphics processing unit;image segmentation;speedup	Van Nha Pham;Long Thanh Ngo;Hoc Van Vu	2015		10.1145/2833258.2833306	parallel computing;computer science;theoretical computer science;machine learning	ML	-2.3954816511468606	-40.59620294684483	180062
2daba72e3c5a8c84684e08f0869a6072d5a426e2	a correlation-based approach to attribute selection in chemical graph mining	dopamine antagonist;attribute selection;data mining;cascade model;structure activity data;huge number;chemical graph mining;chemical structure;correlation-based approach;descriptive feature;correlation coefficient;categorical variable	Data mining often encounters a problem with a huge number of descriptive features. Authors have analyzed structure activity data of dopamine antagonists, where we have to select useful features out of numerous fragments extracted from chemical structures. Correlation coefficients among categorical attributes are used to select attributes. Rules obtained by the cascade model were evaluated from chemists’ point of view, and the importance of attribute selection was confirmed.	coefficient;data mining;dopamine;molecular graph	Takashi Okada	2004		10.1007/978-3-540-71009-7_49	machine learning;pattern recognition;data mining;mathematics	ML	8.492030172609079	-48.61240218518103	180172
4222f1b78cb910a7acfa0380fa9227f8061abd72	attribute number reduction process and nearest neighbor methods in machine learning		Several nearest neighbor methods were applied to process of decision making to E522144 and modified bases, which are the collections of cases of melanocytic skin lesions. Modification of the bases consists in reducing the number of base attributes from 14 to 13, 4, 3, 2 and finally 1. The reduction process consists in concatenations of values of particular attributes. The influence of this process on the quality of decision making process is reported in the paper.	concatenation;k-nearest neighbors algorithm;machine learning	Aleksander Sokolowski;Anna Gladysz	2006		10.1007/3-540-33521-8_18	nearest-neighbor chain algorithm;large margin nearest neighbor;best bin first;machine learning;pattern recognition;data mining;nearest neighbor search;k-nearest neighbors algorithm	AI	8.64352954104964	-40.736716624035324	180291
cd3a7bb9294e71167ebe67d7e662110c8885a7f2	missing values imputation for a clustering genetic algorithm	extraction information;cancerology;analyse amas;analisis datos;tumor maligno;information extraction;dato que falta;mammary gland;hombre;algoritmo genetico;data mining;classification;nearest neighbor method;donnee manquante;glandula mamaria;vecino mas cercano;data analysis;cluster analysis;data mining application;fouille donnee;voting;cancerologie;human;algorithme genetique;plus proche voisin;analyse donnee;nearest neighbour;genetic algorithm;analisis cluster;tumeur maligne;glande mammaire;voto;missing data;cancerologia;missing values;vote;busca dato;breast cancer;clasificacion;extraccion informacion;malignant tumor;homme	The substitution of missing values, also called imputation, is an important data preparation task for data mining applications. This paper describes a nearest-neighbor method to impute missing values, showing that it can be useful for a clustering genetic algorithm. The proposed nearest-neighbor method is assessed by means of simulations performed in two datasets that are benchmarks for data mining methods: Wisconsin Breast Cancer and Congressional Voting Records. The efficacy of the proposed approach is evaluated both in prediction and clustering scenarios. Empirical results show that the employed imputation method is a suitable data preparation tool.	cluster analysis;computer cluster;data mining;distortion;genetic algorithm;geo-imputation;k-nearest neighbors algorithm;mean squared error;missing data;simulation	Eduardo R. Hruschka;Estevam R. Hruschka;Nelson F. F. Ebecken	2005		10.1007/11539902_29	econometrics;missing data;computer science;data mining;cluster analysis;imputation;information extraction;statistics	ML	5.900096173824639	-46.592114675576354	180724
05ee6526b5a03017507d763c6c6aeb4391182aba	ifd: iterative feature and data clustering	mutually reinforcing;convergence;ifd;data and feature coefficients;clustering;data clustering	In this paper, we propose a new clustering algorithm, IFD1, based on a cluster model of data coefficients D and feature coefficients F . The coefficients denote the degree (or weights) of the data and features associated with the clusters. Clustering is performed via an iterative optimization procedure to mutually reinforce the relationships between the coefficients. The mutually reinforcing optimization exploits the duality of the data and features and enable a simultaneous clustering of both data and features. We have shown the convergence property of the clustering algorithm and discussed its connections with various existential approaches. Extensive experimental results on both synthetic and real data sets show the effectiveness of IFD algorithm.	algorithm;cluster analysis;coefficient;computer cluster;degree (graph theory);i-frame delay;iterative method;mathematical optimization;synthetic intelligence	Tao Li;Sheng Ma	2004		10.1137/1.9781611972740.49	artificial intelligence;machine learning;correlation clustering;data stream clustering;flame clustering;pattern recognition;computer science;fuzzy clustering;cluster analysis;k-medians clustering;canopy clustering algorithm;cure data clustering algorithm	DB	3.5925751188934796	-39.94361035019806	180821
654164110fdb18ca463faa4541d32f3125123d72	a synchronization based algorithm for discovering ellipsoidal clusters in large datasets	phase locking;ellipsoidal cluster discovery;clustering algorithms oscillators partitioning algorithms data analysis multimedia databases robustness image segmentation color data mining mathematics;data subset loading;oscillations;pattern clustering;self organising feature maps pattern clustering data mining synchronisation oscillators self adjusting systems scaling phenomena stability image segmentation image colour analysis;mathematics;image segmentation;scaling phenomena;synchronized oscillator group summarization;scalable clustering approach;large color image segmentation synchronization based algorithm ellipsoidal cluster discovery large data sets scalable clustering approach pulse coupled oscillator synchronization integrate and fire oscillators data point representation oscillator interactions relative point similarity self organization stable phase locked subgroups data subset loading synchronized oscillator group summarization memory purging robust method cluster number determination;large dataset;color;coupled oscillator;data point representation;oscillators;integrate and fire;integrate and fire oscillators;self adjusting systems;large data sets;data mining;stability;synchronisation;data analysis;pulse coupled oscillator synchronization;self organising feature maps;image colour analysis;large color image segmentation;memory purging;synchronization based algorithm;multimedia databases;number of clusters;robust method;clustering algorithms;oscillator interactions;self organization;robustness;stable phase locked subgroups;synthetic data;empirical evaluation;relative point similarity;color image;partitioning algorithms;cluster number determination	This paper introduces a new scalable approach to clustering based on synchronization of pulse-coupled oscillators. Each data point is represented by an integrate-and-fire oscillator, and the interaction between oscillators is defined according to the relative similarity between the points. The set of oscillators will self-organize into stable phase-locked subgroups. Our approach proceeds by loading only a subset of the data and allowing it to self-organize. Groups of synchronized oscillators are then summarized and purged from memory. We show that our method is robust, scales linearly, and can determine the number of clusters. The proposed approach is empirically evaluated with several synthetic data sets and is used to segment large color images.	algorithm;arnold tongue;biological neuron model;cpu cache;cluster analysis;computer data storage;data mining;data point;data remanence;discrete mathematics;experiment;exploit (computer security);interaction;loss function;optimization problem;oscillator (cellular automaton);robustness (computer science);scalability;self-organization;synthetic data	Hichem Frigui;Mohamed Ben Hadj Rhouma	2001		10.1109/ICDM.2001.989511	computer science;theoretical computer science;machine learning;data mining;mathematics;oscillation;statistics	ML	0.23865364905264969	-43.23409040117963	181088
232a9a9b5c94e905ef7e706d199b75eb7348532f	biclustering of gene expression data based on related genes and conditions extraction	gene expression data;biclustering;missing data estimation;microarray	Biclustering is an important tool to find patterns in a microarray data matrix by simultaneous classification in two dimensions of genes and conditions. Unlike most existed biclustering algorithms where almost all genes and conditions are involved in the clustering process even if they contribute little to a bicluster, we propose to perform the biclustering operation only in related genes and conditions of a given bicluster type. In our algorithm, the gene expression matrix is first partitioned to stable and unstable submatrices in both row and column directions by inspecting the similarity between the row (or column) vector and the full 1s vector, then the related genes and conditions of a given type of biclusters are extracted by inspecting the row or column pairs in the corresponding stable or unstable submatrices, finally the resulted biclusters of any type are obtained by performing clustering analysis in the extracted related genes and conditions. Additionally, a novel strategy for estimating the missing data in the gene expression matrix is also presented based on the James-Stein and kernel estimation principle where the estimation matrix is obtained with the k means algorithm. Experimental results show excellent performance of our algorithm both in missing data estimation and biclustering.	biclustering	Dechun Yan;Jiajun Wang	2013	Pattern Recognition	10.1016/j.patcog.2012.09.028	computer science;bioinformatics;pattern recognition;microarray;data mining;mathematics;biclustering	Vision	5.219183242514724	-46.979201394371124	181202
76fe595ab8e10e79de187266790a28584fab1798	enhancing genetic algorithm-based genome-scale metabolic network curation efficiency	flux balance analysis;metabolic engineering;systems biology;genome scale modeling;metabolic modeling;genetic algorithms;computational biology	"""Genome-scale metabolic modeling using constraint-based analysis is a powerful modeling paradigm for simulating metabolic networks. Models are generated via inference from genome annotations. However, errors in the annotation or the identity of a gene's function could lead to """"metabolic inconsistency"""" rendering simulations infeasible. Uncovering the source of metabolic inconsistency is non-trivial due to network size and complexity. Recently published work uses genetic algorithms for curation by generating pools of models with randomly relaxed mass balance constraints. Models are evolved that allow feasible simulation while minimizing the number of constraints relaxed. Relaxed constraints represent metabolites likely to be the root of metabolic inconsistency. Although effective, the approach can result in numerous false positives. Here we present a strategy, MassChecker, which evaluates all of the relaxed mass balance constraints in each generation prior to the next round of evolution to determine if they had become consistent due to recombination/mutation. If so, these constraints are enforced. This approach was applied to the development of genome-scale metabolic model of B. anthracis. The model consisted of 1,049 reactions and 1,003 metabolites. The result was a 60% reduction in the number of relaxed mass balance constraints, significantly speeding up the curation process."""	complexity;digital curation;genetic algorithm;programming paradigm;randomness;simulation	Eddy J. Bautista;Ranjan Srivastava	2014		10.1145/2576768.2598218	genetic algorithm;computer science;bioinformatics;machine learning;flux balance analysis;data mining;metabolic engineering;systems biology	Comp.	2.0326140495571434	-51.987993513108655	182341
0118739a50db9431e1bdffcda56e43400e1df9e2	a resampling based clustering algorithm for replicated gene expression data	genomics;ieee transactions;clustering algorithms data models algorithm design and analysis computational biology bioinformatics genomics;statistical analysis bootstrapping genetics inference mechanisms;mixed effect model;integrative analysis gene clustering mixed effect model replicated microarray data;replicated microarray data;clustering algorithms;gene clustering;resampling based clustering algorithm statistical inference quasimcmc algorithm bootstrap samples integrative analysis noise reduction coexpressed genes molecular mechanism replicated gene expression data;robustness;integrative analysis;computational biology;algorithm design and analysis;data models;bioinformatics	In gene expression data analysis, clustering is a fruitful exploratory technique to reveal the underlying molecular mechanism by identifying groups of co-expressed genes. To reduce the noise, usually multiple experimental replicates are performed. An integrative analysis of the full replicate data, instead of reducing the data to the mean profile, carries the promise of yielding more precise and robust clusters. In this paper, we propose a novel resampling based clustering algorithm for genes with replicated expression measurements. Assuming those replicates are exchangeable, we formulate the problem in the bootstrap framework, and aim to infer the consensus clustering based on the bootstrap samples of replicates. In our approach, we adopt the mixed effect model to accommodate the heterogeneous variances and implement a quasi-MCMC algorithm to conduct statistical inference. Experiments demonstrate that by taking advantage of the full replicate data, our algorithm produces more reliable clusters and has robust performance in diverse scenarios, especially when the data is subject to multiple sources of variance.	algorithm;booting;bootstrapping (statistics);cluster analysis;consensus clustering;galactose;gene expression;gene regulatory network;genetic heterogeneity;inference;markov chain monte carlo;microarray;mixed model;resampling (statistics);sample variance;self-replicating machine;signal transduction pathways;simulation;spline (mathematics);stationary process;replication compartment;statistical cluster	Han Li;Chun Li;Jie Hu;Xiaodan Fan	2015	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2015.2403320	mixed model;correlation clustering;data modeling;algorithm design;genomics;computer science;bioinformatics;cure data clustering algorithm;data mining;cluster analysis;statistics;robustness	Comp.	5.337876887288857	-51.53151450953178	182532
6f5046c73ecc8e79ca5e962b504c1bc7dddbb3f5	a method of biomedical knowledge discovery by literature mining based on spo predications: a case study of induced pluripotent stem cells		A large amount of valuable knowledge is hidden in the vast biomedical literatures, publications, and online contents. In order to identify the previously unknown biomedical knowledge from these resources, we propose a new method of knowledge discovery based on SPO predications, which constructs a three-level SPO-semantic relation network in the considered area. We carry out the experiments in the area of induced pluripotent stem cells, and the experimental results indicate that our proposed method can significantly discover the potential biomedical knowledge in this area, and the performance analysis of this method sheds lights on the ways to further improvements.	pareto efficiency	Zheng-Yin Hu;Rong-Qiang Zeng;Xiao-Chu Qin;Ling Wei;Zhiqiang Zhang	2018		10.1007/978-3-319-96133-0_29	induced pluripotent stem cell;computer science;knowledge extraction;machine learning;artificial intelligence;bioinformatics	ML	8.071190909609665	-49.107539571408864	182558
89769d5bab2130696856f41ba77bf4bf63efb473	data spread-based entropy clustering method using adaptive learning	clustering analysis;adaptive thresholding;decision maker;cluster analysis;clustering method;adaptive learning;entropy clustering analysis;parameter optimization	Clustering analysis is to identify inherent structures and discover useful information from large amount of data. However, the decision makers may suffer insufficient understanding the nature of the data and do not know how to set the optimal parameters for the clustering method. To overcome the drawback above, this paper proposes a new entropy clustering method using adaptive learning. The proposed method considers the data spreading to determine the adaptive threshold within parameters optimized by adaptive learning. Four datasets in UCI database are used as the experimental data to compare the accuracy of the proposed method with the listing clustering methods. The experimental results indicate that the proposed method is superior to the listing methods.	cluster analysis	Ching-Hsue Cheng;Liang-Ying Wei	2009	Expert Syst. Appl.	10.1016/j.eswa.2009.04.050	correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;brown clustering;dbscan;biclustering;statistics;clustering high-dimensional data;conceptual clustering	Vision	5.416046242634887	-38.68991169411476	182606
5af036ea3f281fac115b80bbf287b852c7daa37e	self-adaptive statistical process control for anomaly detection in time series	self adaptive;anomaly detection;statistical process control;fuzzy set theory;statistical testing	We model anomaly detection as a statistical testing based on fuzzy set theory.Detection rate and false alarm rate almost are not affected by different K.K optimization is necessary for AUC performance improvement.Fuzzification can effectively reduce false alarm rate.This approach results in high AUC performance and reduces the detection time. Anomaly detection in time series has become a widespread problem in the areas such as intrusion detection and industrial process monitoring. Major challenges in anomaly detection systems include unknown data distribution, control limit determination, multiple parameters, training data and fuzziness of 'anomaly'. Motivated by these considerations, a novel model is developed, whose salient feature is a synergistic combination of statistical and fuzzy set-based techniques. We view anomaly detection problem as a certain statistical hypothesis testing. Meanwhile, 'anomaly' itself includes fuzziness, therefore, can be described with fuzzy sets, which bring a facet of robustness to the overall scheme. Intensive fuzzification is engaged and plays an important role in the successive step of hypothesis testing. Because of intensive fuzzification, the proposed algorithm is distribution-free and self-adaptive, which solves the limitation of control limit and multiple parameters. The framework is realized in an unsupervised mode, leading to great portability and scalability. The performance is assessed in terms of ROC curve on university of California Riverside repository. A series of experiments show that the proposed approach can significantly increase the AUC, while the false alarm rate is improved. In particular, it is capable of detecting anomalies at the earliest possible time.	anomaly detection;time series	Dequan Zheng;Fenghuan Li;Tiejun Zhao	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.03.029	statistical hypothesis testing;anomaly detection;computer science;artificial intelligence;machine learning;data mining;mathematics;fuzzy set;statistical process control;statistics	ML	8.871242407885758	-38.51049696308593	182754
96760f50297d150a181094016f0c514e9572d799	kernel-based algorithms and visualization for interval data mining	radial basis function networks;graphical method;radial basis kernel function;kernel-based algorithm;interval data type;kernel-based algorithms;interval data mining;aggregate large datasets;continuous data;radial basis kernel function evaluation;interactive graphical decision tree;algorithmic change;support vector machine;data visualisation;interval data;data mining;data visualization;artificial datasets;decision trees;support vector machines;interactive graphical decision tree algorithm;kernel method;decision tree;kernel function	Our investigation aims at extending kernel methods to interval data mining and using graphical methods to explain the obtained  results. Interval data type can be an interesting way to aggregate large datasets into smaller ones or to represent data with  uncertainty. No algorithmic changes are required from the usual case of continuous data other than the modification of the  Radial Basis Kernel Function evaluation. Thus, kernel-based algorithms can deal easily with interval data. The numerical test  results with real and artificial datasets show that the proposed methods have given promising performance. We also use interactive  graphical decision tree algorithms and visualization techniques to give an insight into support vector machines results. The  user has a better understanding of the models’ behavior.  	algorithm;data mining;kernel (operating system)	Thanh-Nghi Do;François Poulet	2009		10.1007/978-3-540-88067-7_5	support vector machine;kernel method;kernel embedding of distributions;radial basis function kernel;computer science;machine learning;decision tree;pattern recognition;data mining;tree kernel;polynomial kernel;data visualization	ML	6.44576386001044	-44.56678255862787	182935
903421bbae19c414be9b05e8084e5fa14bbe8a1e	haplotype inference using a novel binary particle swarm optimization algorithm	binary particle swarm optimization;pure parsimony;genotypes;haplotype inference	The knowledge of haplotypes allows researchers to identify the genetic variation affecting phenotypic such as health, disease and response to drugs. However, getting haplotype data by experimental methods is both time-consuming and expensive. Haplotype inference (HI) from the genotypes is a challenging problem in the genetics domain. There are several models for inferring haplotypes from genotypes, and one of the models is known as haplotype inference by pure parsimony (HIPP) which aims to minimize the number of distinct haplotypes used. The HIPP was proved to be an NP-hard problem. In this paper, a novel binary particle swarm optimization (BPSO) is proposed to solve the HIPP problem. The algorithm was tested on variety of simulated and real data sets, and compared with some current methods. The results showed that the method proposed in this paper can obtain the optimal solutions in most of the cases, i.e., it is a potentially powerful method for HIPP.	algorithm;mathematical optimization;particle swarm optimization	Bin Wei;Jing Zhao	2014	Appl. Soft Comput.	10.1016/j.asoc.2014.03.034	bioinformatics;genotype;statistics	Vision	2.333718783136125	-51.28612839585162	182980
06ebeb5ee68d86f6650cb5470deb67c520e2c8d6	dimension reduction using clustering algorithm and rough set theory	clustering algorithm;dimension reduction;rough set theory;attribute similarity;indiscernibility relation;undirected weighted graph	In real world, datasets have large number of attributes but few are important to describe them properly. The paper proposes a novel dimension reduction algorithm for real valued dataset using the concept of Rough Set Theory and clustering algorithm to generate the reduct. Here, projection of dataset based on two conditional attributes Ci and Cj is taken and K-means Clustering algorithm is applied on it with K = number of distinct values of decision attribute D of the dataset to obtain K clusters. Also the dataset is clustered into K-groups using Indiscernibility relation applied on the decision attribute D. Then the connecting factor k of combined conditional attributes (CiCj) with respect to D is calculated using two cluster sets and attribute connecting set ACS = {(CiCj$\rightarrow^{\hspace*{-2.5mm}^k} D$) for all Ci,Cj ∈ C, Conditional attribute set, and D (Decision attribute)} is formed. Each element (CiCj$\rightarrow^{\hspace*{-2.5mm}^k} D$) ∈ ACS implies that Ci and Cj connecting together partition the objects that yields (k*100) % similar partitions as made on D. Now an undirected weighted graph with weights as the connecting factor k is constructed using attribute connecting set ACS. Finally based on the weight associated with edges, the important attributes, called reduct are generated. Experimental result shows the efficiency of the proposed method.	algorithm;cluster analysis;dimensionality reduction;rough set;set theory	Shampa Sengupta;Asit Kumar Das	2012		10.1007/978-3-642-35380-2_82	combinatorics;discrete mathematics;rough set;attribute domain;computer science;machine learning;data mining;mathematics;cluster analysis;dimensionality reduction	ML	1.1264281364208286	-41.731707662517756	183213
c127fec76f803fb5f0ec853c5660f20e874211c2	an information retrieval approach to finding dependent subspaces of multiple views		Finding relationships between multiple views of data is essential both in exploratory analysis and as pre-processing for predictive tasks. A prominent approach is to apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor retrieval between multiple views. We optimize mappings for each view such as linear transformations to maximize cross-view similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, measures dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments the proposed method outperforms alternatives in preserving cross-view neighborhood similarities, and yields insights into local dependencies between multiple views.	experiment;information retrieval;nonlinear system;preprocessor;whole earth 'lectronic link	Ziyuan Lin;Jaakko Peltonen	2017		10.1007/978-3-319-62416-7_1	theoretical computer science;machine learning;data mining;mathematics	ML	-1.4858667469720463	-42.49341833121831	183216
e604b6f686ca577d2f0a2eb600ab10ccdcd279f9	discrete methods for association search and status prediction in genotype case-control studies	prediction method;predictive validity;complex disease;search engines;case control study;genome wide association study;search method;greedy heuristic;search engines cellular biophysics diseases genetics medical computing prediction theory;genetics;greedy heuristics;medical computing;risk factors;prediction theory;status prediction methods genotype case control studies optimization cross validation disease associated risk factor greedy heuristics;genotype case control studies;diseases;odd ratio;optimization;diseases search methods reproducibility of results prediction methods genomics bioinformatics statistics lungs cancer arthritis;status prediction methods;cross validation;high throughput;cellular biophysics;leave one out cross validation;disease associated risk factor;exhaustive search	Recent improvements in high-throughput genotyping technology make possible genome-wide association studies and status prediction (classification) for common complex diseases. This paper addresses three challenges commonly facing such studies: (i) searching an enormous amount of possible gene interactions, (ii) validating reproducibility of associations and (iii) reliably predicting disease status. These challenges have been traditionally addressed in statistics while here we apply computational approaches -optimization and cross-validation. A complex risk factor is modeled as a subset of SNP's with specified alleles and the optimization formulation asks for the one with the maximum odds ratio. When searching for disease associated risk factor, we show that greedy heuristics are much faster and lead to significantly better solutions than exhaustive heuristics in a reasonable amount of time. We propose a novel randomized complimentary greedy search method that is advantageous to the previously best search method. To measure and compare ability of search methods to find reproducible risk factors, we propose to apply a cross-validation scheme usually used for prediction validation. The proposed heuristic association search methods promise better reproducibility than exhaustive searches. We then show that k-fold cross-validation is more reliable than leave-one-out cross-validation for disease status prediction methods since it captures overtraining effect. We have applied known search methods with proposed enhancements as well as status prediction methods (based on these search methods) to real case-control studies for several diseases (Chron's disease, autoimmune disorder, tick-born encephalitis, lung cancer, and rheumatoid arthritis). 2-and 3-fold cross-validations show that the new methods find strongly associated risk factors and reliably predict disease status for considered case-control studies.	approximation algorithm;cross-validation (statistics);data validation;gene prediction;greedy algorithm;heuristic (computer science);high-throughput computing;interaction;mathematical optimization;optimization problem;randomized algorithm;referring expression generation;risk assessment;risk factor (computing);runge–kutta methods;throughput	Dumitru Brinza;Alex Zelikovsky	2007	2007 IEEE 7th International Symposium on BioInformatics and BioEngineering	10.1109/BIBE.2007.4375576	computer science;bioinformatics;data science;machine learning;data mining;genetics;cross-validation;statistics	Comp.	8.196397526144104	-51.808574062124265	183297
406e061108b7bdb4c350206ed6834930da5a4017	ouroboros: early identification of at-risk students without models based on legacy data	predictive analytics;imbalanced data;self learning;learning analytics;student retention	"""This paper focuses on the problem of identifying students, who are at risk of failing their course. The presented method proposes a solution in the absence of data from previous courses, which are usually used for training machine learning models. This situation typically occurs in new courses. We present the concept of a """"self-learner"""" that builds the machine learning models from the data generated during the current course. The approach utilises information about already submitted assessments, which introduces the problem of imbalanced data for training and testing the classification models.  There are three main contributions of this paper: (1) the concept of training the models for identifying at-risk students using data from the current course, (2) specifying the problem as a classification task, and (3) tackling the challenge of imbalanced data, which appears both in training and testing data.  The results show the comparison with the traditional approach of learning the models from the legacy course data, validating the proposed concept."""	failure;machine learning	Martin Hlosta;Zdenek Zdráhal;Jaroslav Zendulka	2017		10.1145/3027385.3027449	computer science;data science;machine learning;data mining	AI	8.285279621158983	-43.09422745875797	183571
103820f95c30e5758e858ddf6448282a485964d0	in the rough	fractals;l systems;genetic algorithm;morphogenesis;neuronal network;cellular machines	Independent research is in a difficult spot, but some players still have a shot at success.	bliss	Paul Taylor;Jennifer Miller	2005		10.1145/1086057.1086158	biological neural network;genetic algorithm;fractal;morphogenesis;computer science;bioinformatics;artificial intelligence;machine learning;l-system	Robotics	-3.387809006654762	-47.74811624340393	183599
6ee6e7472baa2b68f1fc32dc2599eb59f1dac65b	protein sequence pattern mining with constraints	databases;adaptability;adaptabilite;base donnee;proteine;analisis datos;protein sequence;procedimiento extraccion;procede extraction;database;base dato;bioinformatique;data mining;sequence mining;adaptabilidad;pattern mining;data analysis;fouille donnee;decouverte connaissance;comportement utilisateur;descubrimiento conocimiento;analyse donnee;proteina;user behavior;experimental evaluation;bioinformatica;protein;busca dato;extraction process;comportamiento usuario;bioinformatics;knowledge discovery	Considering the characteristics of biological sequence databases, which typically have a small alphabet, a very long length and a relative small size (several hundreds of sequences), we propose a new sequence mining algorithm (gIL). gIL was developed for linear sequence pattern mining and results from the combination of some of the most efficient techniques used in sequence and itemset mining. The algorithm exhibits a high adaptability, yielding a smooth and direct introduction of various types of features into the mining process, namely the extraction of rigid and arbitrary gap patterns. Both breadth or a depth first traversal are possible. The experimental evaluation, in synthetic and real life protein databases, has shown that our algorithm has superior performance to state-of-the art algorithms. The use of constraints has also proved to be a very useful tool to specify user interesting patterns.	algorithm;constraint (mathematics);data mining;experiment;pattern search (optimization);peptide sequence;real life;sequence database;sequential pattern mining;synthetic intelligence;tree traversal	Pedro Gabriel Ferreira;Paulo J. Azevedo	2005		10.1007/11564126_14	sequential pattern mining;adaptability;computer science;bioinformatics;protein sequencing;data mining;database;knowledge extraction;data analysis	ML	-0.23708216275938782	-47.53728977649836	183614
deb2418f3f0d46759840a12c7ec503c6751e0533	a new smoothing model for analyzing array cgh data	chromosome;wavelet transforms cancer cellular biophysics genetics medical diagnostic computing molecular biophysics smoothing methods;shrinkage estimation;cancer;array cgh;data smoothing;array cgh data;molecular cytogenetic method;copy number alterations;bivariate shrinkage estimator;dual tree complex wavelet transform;comparative genomic hybridization;complex wavelet transform;genetics;smoothing methods data analysis discrete wavelet transforms wavelet transforms genomics bioinformatics biological cells cancer signal synthesis noise reduction;wavelet transforms;smoothing methods;genome;molecular biophysics;array based comparative genomic hybridization;medical diagnostic computing;cellular biophysics;chromosomal imbalances;bivariate shrinkage estimator data smoothing array cgh data comparative genomic hybridization molecular cytogenetic method chromosomal imbalances cancer genome chromosome dual tree complex wavelet transform	Array based comparative genomic hybridization (CGH) is a molecular cytogenetic method for the detection of chromosomal imbalances and it has been extensively used for studying copy number alterations in various cancer types. Our method captures both the intrinsic spatial change of genome hybridization intensities, and the physical distance between adjacent probes along a chromosome which are not uniform. In this paper, we introduce a dual-tree complex wavelet transform method with the bivariate shrinkage estimator into array CGH data smoothing study. We tested the proposed method on both simulated data and real data, and the results demonstrated superior performance of our method in comparison with extant methods.	bivariate data;complex wavelet transform;computer-generated holography;smoothing;spatial analysis	Nha Nguyen;Heng Huang;Soontorn Oraintara;An P. N. Vo	2007	2007 IEEE 7th International Symposium on BioInformatics and BioEngineering	10.1109/BIBE.2007.4375683	biology;bioinformatics;chromosome;comparative genomic hybridization;genetics;statistics;smoothing;genome;cancer;wavelet transform;molecular biophysics	Visualization	3.1451228899136483	-48.971050756768335	183710
5f69fe38d3e72bbccba03b16e77affe727edf289	bigfcm: fast, precise and scalable fcm on hadoop		Abstract Clustering plays an important role in mining big data both as a modeling technique and a preprocessing step in many data mining process implementations. Fuzzy clustering provides more flexibility than non-fuzzy methods by allowing each data record to belong to more than one cluster to some degree. However, a serious challenge in fuzzy clustering is the lack of scalability. Massive datasets in emerging fields such as geosciences, biology, and networking do require parallel and distributed computations with high performance to solve real-world problems. Although some clustering methods are already improved to execute on big data platforms, their execution time is highly increased for gigantic datasets. In this paper, a scalable Fuzzy C-Means (FCM) clustering method named BigFCM is proposed and designed for the Hadoop distributed data platform. Based on the MapReduce programming model, the proposed algorithm exploits several mechanisms including an efficient caching design to achieve several orders of magnitude reduction in execution time. The BigFCM performance compared with Apache Mahout K-Means and Fuzzy K-Means through an evaluation framework developed in this research. Extensive evaluation using over multi-gigabyte datasets including SUSY and HIGGS shows that BigFCM is scalable while it preserves the quality of clustering.	algorithm;apache hadoop;apache mahout;big data;cluster analysis;computation;data mining;display resolution;domain-specific language;fuzzy clustering;fuzzy cognitive map;gigabyte;k-means clustering;linear algebra;mapreduce;preprocessor;programming model;row (database);run time (program lifecycle phase);scalability;telecommuting;ver (command)	Nasser Ghadiri;Meysam Ghaffari;Mohammad Amin Nikbakht	2017	Future Generation Comp. Syst.	10.1016/j.future.2017.06.010	computer science;clustering high-dimensional data;consensus clustering;machine learning;data stream clustering;distributed computing;data mining;fuzzy clustering;canopy clustering algorithm;artificial intelligence;cluster analysis;brown clustering;cure data clustering algorithm	ML	-2.796946630909021	-39.4043203556306	183897
3a486fcd9073bb8c70042cb518cd8156dfe7eb94	a differential evolution approach for classification of multiple sclerosis lesions	databases;differential evolution multiple sclerosis classification knowledge extraction if then rules;visual databases brain data mining decision support systems diseases evolutionary computation image classification medical image processing;lesion abnormality assessment differential evolution multiple sclerosis lesion classification automatic knowledge extraction pattern extraction automatic comprehensible classification rule discovery discriminant database attributes instance categorization decision support system clinical decision medical expert;lesions;statistics;sociology statistics lesions databases optimization multiple sclerosis;optimization;multiple sclerosis;sociology	The problem of automatically extracting novel and interesting knowledge from large amount of data is often performed heuristically when pattern extraction through classical statistical methods is found hard. In this paper an evolutionary approach, based on Differential Evolution, is proposed, which is able to perform the automatic discovery of comprehensible classification rules as a set of IF...THEN rules over a database of Multiple Sclerosis potential lesions. Moreover, this tool also determines which the most discriminant database attributes are in categorizing instances. Therefore, this evolutionary tool provides an efficient decision support system for clinical decisions, that could be a useful tool for medical experts to help them gain insight into the reasons for assessing the abnormality of a lesion.	algorithm;black box;categorization;decision support system;differential evolution;discriminant;experiment;heuristic;iterative and incremental development;numerical analysis;pattern recognition;softening	Ivanoe De Falco;Umberto Scafuri;Ernesto Tarantino	2016	2016 IEEE Symposium on Computers and Communication (ISCC)	10.1109/ISCC.2016.7543729	computer science;machine learning;pattern recognition;data mining;statistics	SE	8.002710008758593	-44.823396137038706	184033
7292e5220b2b42e005787042fda81a6d4af284a6	historical height samples with shortfall: a computational approach		Research in economic history frequently uses human height as a proxy for net nutrition. This anthropometric method enables historians to measure time trends and differences in nutritional status. However, the most widely used data sources for historical heights, military mustering registers, cannot be regarded as random samples of the underlying population. The lower side of the otherwise normal distribution is eroded by a phenomenon called shortfall, because shorter individuals are under-represented below a certain threshold (truncation point). This paper reviews two widely used methods for analyzing historical height samples with shortfall - the Quantile Bend Estimator (QBE) and the Reduced Sample Maximum Likelihood Estimator (RSMLE). Because of the drawbacks of these procedures, a new computational approach for identifying the truncation point of height samples with shortfall, using density estimation techniques, is proposed and illustrated on an Austrian dataset. Finally, this procedure, combined with a truncated regression model, is compared to the QBE to estimate the mean and the standard deviation. The results demonstrate the deficiencies of the QBE again and cast a good light on the new method.	anthropometry;computation;cytochrome p-450 cyp1a1;data sources;ephrin type-b receptor 1, human;human height;kaplan–meier estimator;microsoft query;protein truncation abnormality;quantile;query by example;review [publication type];silo (dataset);truncated regression model	Markus Heintel	1996	History & computing	10.3366/hac.1996.8.1.24	econometrics;demography;geography;data mining;statistics	AI	3.9440487548450434	-51.00223820636607	184320
cd5c59bcb956fa9e3df332df80b71208940b6453	review on general techniques and packages for data imputation in r on a real world dataset		When we collect data, usually they consist of small samples with missing values. As a consequence of this flaw, the data analysis becomes less effective. Almost all algorithms for statistical data analysis need a complete data set. In data preprocessing, we have to deal with missing values. Some well-known methods for filling missing values are: Mean, K-nearest neighbours (kNN), fuzzy K-means (FKM), etc. There are quite a lot of R packages offering the imputation of missing values, but sometimes its hard to find the appropriate algorithm for a particular dataset. When we have to deal with large datasets sometimes, these known methods cannot work as supposed because they need too much memory to perform their operations. This paper provides an overview of a considerable dataset imputation by applying three different algorithms. A comparison was performed using three different algorithms under a missing completely at random (MCAR) assumption, and based on the evaluation criteria: Root mean squared error (RMSE). The experiment results show that Random Forest algorithm can be quite useful for missing values imputation.	geo-imputation	Fitore Muharemi;Doina Logofatu;Florin Leon	2018		10.1007/978-3-319-98446-9_36	data mining;statistics;imputation (statistics);fuzzy logic;computer science;data pre-processing;missing data;random forest;mean squared error	DB	5.778190107626011	-42.93111867893327	184520
0904505ad9e76a64c8b38e0f429a5834835ff6b7	clustering using chemical and colonial odors of real ants	graph theory;pattern clustering;chemical engineering computing;data mining;ants behavior data analysis clustering swarm intelligence;data mining colonial odors chemical odors real ants automatic data clustering model pheromone mechanisms complete dynamic graphs;classification algorithms;pattern clustering chemical engineering computing data mining digital simulation graph theory;digital simulation	We suggest in this paper a new automatic data clustering model based on the behavior of real ants. Drawing on a simulation of colonial odors and pheromone mechanisms, we set up complete dynamic graphs to solve the problem of data clustering. Using graph we will clarify the relationships between clusters of data.	algorithm;artificial ants;binary space partitioning;biomimetics;cluster analysis;euclidean distance;k-means clustering;simulation	Nesrine Masmoudi;Hanene Azzag;Mustapha Lebbah;Cyrille Bertelle	2013	2013 World Congress on Nature and Biologically Inspired Computing	10.1109/NaBIC.2013.6617863	fuzzy clustering;computer science;graph theory;theoretical computer science;machine learning;consensus clustering;data mining;cluster analysis	AI	3.2087186865588353	-42.51177881396912	184586
0467efde8931ca368388c9d95af0f51ad577f509	minimal-risk scoring matrices for sequence analysis	risk score;sequence analysis	We introduce a minimal-risk method for estimating the frequencies of amino acids at conserved positions in a protein family. Our method, called minimal-risk estimation, finds the optimal weighting between a set of observed amino acid counts and a set of pseudofrequencies, which represent prior information about the frequencies. We compute the optimal weighting by minimizing the expected distance between the estimated frequencies and the true population frequencies, measured by either a squared-error or a relative-entropy metric. Our method accounts for the source of the pseudofrequencies, which arise either from the background distribution of amino acids or from applying a substitution matrix to the observed data. Our frequency estimates therefore depend on the size and composition of the observed data as well as the source of the pseudofrequencies. We convert our frequency estimates into minimal-risk scoring matrices for sequence analysis. A large-scale cross-validation study, involving 48 variants of seven methods, shows that the best performing method is minimal-risk estimation using the squared-error metric. Our method is implemented in the package EMATRIX, which is available on the Internet at http://motif.stanford.edu/ematrix.		Thomas D. Wu;Craig G. Nevill-Manning;Douglas L. Brutlag	1999	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.1999.6.219	framingham risk score;biology;econometrics;bioinformatics;sequence analysis;pattern recognition;mathematics;statistics	Comp.	4.615047493274264	-51.291372164004706	184778
b6f77e2f71dbd1b31b0ec1004ab0b7727aa4b985	seeding genetic programming populations	genetic program;benchmark problem;data mining;general solution;machine learning;image compression;breast cancer	We show genetic programming (GP) populations can evolve under the influence of a Pareto multi-objective fitness and program size selection scheme, from “perfect” programs which match the training material to general solutions. The technique is demonstrated with programmatic image compression, two machine learning benchmark problems (Pima Diabetes and Wisconsin Breast Cancer) and an insurance customer profiling task (Benelearn99 data mining).	benchmark (computing);customer relationship management;data mining;genetic programming;image compression;machine learning;maximum parsimony (phylogenetics);occam's razor;pareto efficiency;population;randomness;soft computing;test set	William B. Langdon;Peter Nordin	2000		10.1007/978-3-540-46239-2_23	simulation;image compression;computer science;artificial intelligence;breast cancer;machine learning;data mining	ML	8.887120832588113	-41.23424974071123	184786
42be1e566093153f1f067fa7431278a0a159e0b8	associated evolution of a support vector machine-based classifier for pedestrian detection	genetic operator;pedestrian safety;poison control;injury prevention;kernel function;safety literature;traffic safety;injury control;pedestrian detection system;home safety;injury research;safety abstracts;human factors;parameter tuning;occupational safety;pedestrian detection;safety;safety research;accident prevention;feature selection;violence prevention;bicycle safety;support vector machine;poisoning prevention;falls;ergonomics;suicide prevention;evolutionary method	Support vector machine (SVM) has become a dominant classification technique used in pedestrian detection systems. In such systems, classifiers are used to detect pedestrians in some input frames. The performance of a SVM classifier is mainly influenced by two factors: the selected features and the parameters of the kernel function. These two factors are highly related and therefore, it is desirable that the two factors can be analyzed simultaneously, which are usually not the case in the previous work. In this paper, we propose an evolutionary method to simultaneously optimize the feature set and the parameters for the SVM classifier. Specifically, adaptive genetic operators were designed to be suitable for the feature selection and parameter tuning. The proposed method is used to train a SVM classifier for pedestrian detection. Experiments in real city traffic scenes show that the proposed approach leads to higher detection accuracy and shorter detection time.	evolutionary algorithm;feature selection;numerical aperture;pedestrian detection;radial basis function kernel;support vector machine	X. B. Cao;Yan Wu Xu;D. Chen;Hong Qiao	2009	Inf. Sci.	10.1016/j.ins.2008.10.020	kernel;margin classifier;support vector machine;simulation;quadratic classifier;computer science;suicide prevention;human factors and ergonomics;injury prevention;genetic operator;machine learning;data mining;computer security;structured support vector machine	AI	8.242348028454307	-43.926539201805724	185057
c900601c07a24376960be3fd5f29e5b4fdbfe8c0	improving fuzzy c-means clustering based on adaptive weighting	databases;clustering algorithms iterative algorithms partitioning algorithms fuzzy systems clustering methods laboratories intelligent systems automation euclidean distance input variables;cluster algorithm;pattern clustering;fuzzy c mean;iterative algorithm;fuzzy set theory;uci;iterative awfcm process;iterative methods;distance measurement;uci fuzzy c means clustering data fcm clustering algorithm adaptive weight iterative awfcm process;clustering;fuzzy c means;fcm clustering algorithm;clustering algorithms;adaptive weight;fuzzy c means clustering;clustering methods;pattern clustering fuzzy set theory iterative methods;clustered data;algorithm design and analysis;fuzzy c means clustering data;partitioning algorithms	In traditional FCM clustering algorithm each feature is supposed to have equal importance. Considering different feature with different importance, this paper presented an improved FCM algorithm with adaptive weight for features of each cluster, named AWFCM. In the iterative AWFCM process, to identify the importance of features of each cluster, the weight for feature is computed dynamically based on the variance of the within cluster distances of the feature, and the new weights are used to calculate the cluster memberships of objects in next iteration effectively. Moreover, for the reason that in traditional FCM the features with wider variation range have greater impact on the clustering result even if they are less important, AWFCM introduce an method to normalize the clustering data between 0 and 1 in order to eliminate the over effect of the features with wider variation range. And then, based on four real data sets from UCI, the experiments demonstrated the AWFCM algorithm outperformed the FCM algorithm.	algorithm;cluster analysis;database normalization;experiment;fuzzy cognitive map;iteration	Wei Wang;Chunheng Wang;Xia Cui;Ai Wang	2008	2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2008.160	k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;data mining;mathematics;iterative method;cluster analysis;single-linkage clustering	ML	3.0987726657975143	-39.703643856765005	185291
d2ce0ab7b59066689522dfe9b84b84881bba1432	community self-organizing map and its application to data extraction	unsupervised learning;pattern clustering;winning frequency;unsupervised learning community self organizing map data extraction clustering problem winning frequency;unsupervised learning pattern clustering self organising feature maps;data mining;artificial neural networks;community self organizing map;self organising feature maps;data extraction;lead;clustering algorithms;data mining clustering algorithms humans neurons laboratories animals data visualization frequency image analysis unsupervised learning;self organized map;humans;neurons;communities;clustering problem	The Self-Organizing Map (SOM) is a famous algorithm for the unsupervised learning and visualization introduced by Teuvo Kohonen. One of the most attractive applications of SOM is clustering and several algorithms for various kinds of clustering problems have been reported and investigated. This study proposes the Community Self-Organizing Map (CSOM) algorithm which reflects the community in the human society. In CSOM algorithm, the neurons create some communities according to their winning frequency. We apply CSOM to various input data for clustering and data extraction, and we investigate its behaviors. We confirm that CSOM creates some communities and obtain efficient results for data extraction.	algorithm;cluster analysis;organizing (structure);self-organization;self-organizing map;teuvo kohonen;unsupervised learning	Taku Haraguchi;Haruna Matsushita;Yoshifumi Nishio	2009	2009 International Joint Conference on Neural Networks	10.1109/IJCNN.2009.5178877	unsupervised learning;lead;computer science;machine learning;pattern recognition;data mining;cluster analysis;artificial neural network	ML	2.8344601103483003	-42.70032082896915	185357
8acff6a01237e2fb42f0dcc6d5e8a1e71b16a796	improved initialisation of model-based clustering using gaussian hierarchical partitions	biological patents;62h30 classification and discrimination;biomedical journals;model based agglomerative hierarchical clustering;text mining;europe pubmed central;citation search;citation networks;cluster analysis;research articles;abstracts;open access;data transformation;life sciences;clinical guidelines;mclust;full text;model based clustering;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Initialisation of the EM algorithm in model-based clustering is often crucial. Various starting points in the parameter space often lead to different local maxima of the likelihood function and, so to different clustering partitions. Among the several approaches available in the literature, model-based agglomerative hierarchical clustering is used to provide initial partitions in the popular mclust R package. This choice is computationally convenient and often yields good clustering partitions. However, in certain circumstances, poor initial partitions may cause the EM algorithm to converge to a local maximum of the likelihood function. We propose several simple and fast refinements based on data transformations and illustrate them through data examples.	cluster analysis;converge;electron microscopy;expectation–maximization algorithm;hierarchical clustering;likelihood functions;maxima and minima;maximum;normal statistical distribution;population parameter;cell transformation;statistical cluster	Luca Scrucca;Adrian E. Raftery	2015	Advances in data analysis and classification	10.1007/s11634-015-0220-z	correlation clustering;data stream clustering;text mining;fuzzy clustering;computer science;data science;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;brown clustering;data transformation;statistics;hierarchical clustering of networks	ML	4.578657833882015	-44.76851111326065	185787
b660b4001130d1fdca2f1e64527e3dc40dd079d8	aggregation pheromone density based data clustering	time dependent;swarm intelligence;ant colony optimization;search space;data clustering;cluster validity;social insect;clustered data;aggregation pheromone;k means clustering	Ants, bees and other social insects deposit pheromone (a type of chemical) in order to communicate between the members of their community. Pheromone, that causes clumping or clustering behavior in a species and brings individuals into a closer proximity, is called aggregation pheromone. This article presents a new algorithm (called, APC) for clustering data sets based on this property of aggregation pheromone found in ants. An ant is placed at each location of a data point, and the ants are allowed to move in the search space to find points with higher pheromone density. The movement of an ant is governed by the amount of pheromone deposited at different points of the search space. More the deposited pheromone, more is the aggregation of ants. This leads to the formation of homogenous groups of data. The proposed algorithm is evaluated on a number of well-known benchmark data sets using different cluster validity measures. Results are compared with those obtained using two popular standard clustering techniques namely average linkage agglomerative and k-means clustering algorithm and with an ant-based method called adaptive time-dependent transporter ants for clustering (ATTAC). Experimental results justify the potentiality of the proposed APC algorithm both in terms of the solution (clustering) quality as well as execution time compared to other algorithms for a large number of data sets. 2008 Elsevier Inc. All rights reserved.	algorithm;benchmark (computing);bus bunching;cluster analysis;data point;eusociality;hierarchical clustering;image processing;image segmentation;k-means clustering;linkage (software);run time (program lifecycle phase);soft computing;upgma	Ashish Ghosh;Anindya Halder;Megha Kothari;Susmita Ghosh	2008	Inf. Sci.	10.1016/j.ins.2008.02.015	correlation clustering;ant colony optimization algorithms;fuzzy clustering;swarm intelligence;computer science;artificial intelligence;machine learning;data mining;mathematics;cluster analysis;single-linkage clustering;k-means clustering	ML	3.7161656894574167	-41.8663341484023	185797
ee88ae92d19e208f15d3613016266bb13da8fc98	the influence of inconsistent data on cost-sensitive classification using prism algorithms: an empirical study	cost sensitive;classification;inconsistent;prism algorithm	Cost-sensitive classification is one of the hottest research topics in data mining and machine learning. It is prevalent in many applications, such as Automatic Target Recognition (ATR), medical diagnosis, etc. However, the data in practice may be inconsistent due to dimensional reduction operation or other pre-processing, yet it is not clear how the inconsistent data affects cost-sensitive learning. This paper presents an empirical comparative study using four Prism rule-generating algorithms with Jmeasure pruning, two of which are proposed in this paper. The most important result of our study is that inconsistent data dose often affects the performance of cost-sensitive Prism classifiers, and in the inconsistent data setting, merely a single Prism classifier’s robustness cannot completely satisfy the requirements of cost-sensitive systems.	algorithm;alpha–beta pruning;automatic target recognition;data mining;machine learning;prism (surveillance program);preprocessor;requirement	Zhiyong Hao;Li Yao;Yanjuan Wang	2014	JCP	10.4304/jcp.9.8.1880-1885	biological classification;computer science;machine learning;data mining;database	ML	7.787032556747616	-38.526172385376995	185848
11a720d3904a12c42499da383b901deeed066858	a parallel framework for multipoint spiral search in ab initio protein structure prediction	biological patents;biomedical journals;text mining;europe pubmed central;institute for integrated and intelligent systems;faculty of science environment engineering and technology;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;artificial intelligence and image processing;rest apis;080199;orcids;europe pmc;biomedical research;bioinformatics;literature search	Protein structure prediction is computationally a very challenging problem. A large number of existing search algorithms attempt to solve the problem by exploring possible structures and finding the one with the minimum free energy. However, these algorithms perform poorly on large sized proteins due to an astronomically wide search space. In this paper, we present a multipoint spiral search framework that uses parallel processing techniques to expedite exploration by starting from different points. In our approach, a set of random initial solutions are generated and distributed to different threads. We allow each thread to run for a predefined period of time. The improved solutions are stored threadwise. When the threads finish, the solutions are merged together and the duplicates are removed. A selected distinct set of solutions are then split to different threads again. In our ab initio protein structure prediction method, we use the three-dimensional face-centred-cubic lattice for structure-backbone mapping. We use both the low resolution hydrophobic-polar energy model and the high-resolution 20 × 20 energy model for search guiding. The experimental results show that our new parallel framework significantly improves the results obtained by the state-of-the-art single-point search approaches for both energy models on three-dimensional face-centred-cubic lattice. We also experimentally show the effectiveness of mixing energy models within parallel threads.	collaborator;cubic function;de novo protein structure prediction;experiment;image resolution;internet backbone;iteration;local search (optimization);merge;multipoint ground;newton;parallel computing;physical symbol system;protein, organized by structure;review [publication type];search algorithm;solutions;subgroup;tabu search;version;vertebral column;walker-warburg congenital muscular dystrophy;working set;free energy;interest	Mahmood A. Rashid;Swakkhar Shatabda;M. A. Hakim Newton;Tamjidul Hoque;Abdul Sattar	2014		10.1155/2014/985968	text mining;medical research;computer science;bioinformatics;data science;data mining	Comp.	-1.5180044719412231	-50.73102253392776	185858
1ca88134e9435e461f1dd4b3432e3c362e7161ec	parallel simulation of apoptotic receptor-clustering on gpgpu many-core architectures	apoptosis;receptor clustering;numerical modeling;tumours;computational modeling mathematical model instruction sets computer architecture biological system modeling microprocessors sorting;biomembranes;gpgpu;tumours bioinformatics biological techniques biomembranes cellular biophysics graphics processing units parallel processing physiological models;graphics processing units;hpc simulation tasks parallel simulation apoptotic receptor clustering gpgpu many core architectures apoptosis programmed cell death physiological process tumor necrosis factor receptors ligands cell membrane algorithmic mapping high performance computing;biological techniques;physiological models;cellular biophysics;parallel processing;receptor clustering gpgpu parallel particle simulation numerical modeling apoptosis;parallel particle simulation;bioinformatics	Apoptosis, the programmed cell death, is a physiological process that handles the removal of unwanted or damaged cells in living organisms. The process itself is initiated by signaling through tumor necrosis factor (TNF) receptors and ligands, which form clusters on the cell membrane. The exact function of this process is not yet fully understood and currently subject of basic research. Different mathematical models have been developed to describe and simulate the apoptotic receptor-clustering. In this interdisciplinary work, a previously introduced model of the apoptotic receptor-clustering has been extended by a new receptor type to allow a more precise description and simulation of the signaling process. Due to the high computational requirements of the model, an efficient algorithmic mapping to a modern many-core GPGPU architecture has been developed. Such architectures enable high-performance computing (HPC) simulation tasks on the desktop at low costs. The developed mapping reduces average simulation times from months to days (peak speedup of 256x), allowing the productive use of the model in research.	cluster analysis;computation;desktop computer;general-purpose computing on graphics processing units;manycore processor;mathematical model;parallel computing;requirement;simulation;speedup;supercomputer	Claus Braun;Markus Daub;Alexander Schöll;Guido Schneider;Hans-Joachim Wunderlich	2012	2012 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2012.6392661	parallel processing;simulation;computer science;bioinformatics;apoptosis;theoretical computer science;general-purpose computing on graphics processing units	HPC	-4.508213170985956	-51.02406785744792	185867
3dbe7660a67d2a68456a914adba97e19f093914c	clustering and change detection in multiple streaming time series	change detection;time series data streams;clustering	In recent years, Data Stream Mining (DSM) has received a lot of attention due to the increasing number of applicative contexts which generate temporally ordered, fast changing, and potentially infinite data. To deal with such data, learning techniques require to satisfy several computational and storage constraints so that new and specific methods have to be developed. In this paper we introduce a new strategy for dealing with the problem of streaming time series clustering. The method allows to detect a partition of the streams over a user chosen time period and to discover evolutions in proximity relations. We show that it is possible to reach these aims, performing the clustering of temporally non overlapping data batches arriving on-line and then running a suitable clustering algorithm on a dissimilarity matrix updated using the outputs of the local clustering. Through an application on real and simulated data, we will show that this method provides results comparable to algorithms for stored data.	time series	Antonio Balzanella;Rosanna Verde	2013		10.1007/978-3-319-03859-9_1	correlation clustering;constrained clustering;data stream clustering;real-time computing;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;brown clustering;change detection;biclustering;affinity propagation;clustering high-dimensional data	ML	-1.5163584019449845	-38.051199292391104	185873
967c593683b1cefc4f608c2f41dd1d8a73144f74	novelty detection using a new group outlier factor	group outliers;novelty detection;outliers;clustering;som	"""We present in this paper a new measure named GOF (Group Outlier Factor) for cluster outliers and novelty detection. The main difference between GOF and existing methods is that being an outlier is not associated to a single pattern but to a cluster. GOF is based on relative density of each group of data and provides a quantitative indicator of outlier-ness which enables to detect automatically """"cluster outliers"""". To learn GOF measure, we integrate it in a clustering process using Self-organizing Map. Experimental results and comparison studies show that the use of GOF sensibly improves the results in term of cluster-outlier detection and novelty detection."""		Amine Chaibi;Mustapha Lebbah;Hanene Azzag	2012		10.1007/978-3-642-34487-9_45	outlier;computer science;machine learning;pattern recognition;data mining;cluster analysis;statistics	Vision	1.7296049336786783	-41.208568422870265	186074
790978c1209973f12d1e2fed495550d77a556390	multiple incomplete views clustering via non-negative matrix factorization with its application in alzheimer's disease analysis		Traditional neuroimaging analysis, such as clustering the data collected for the Alzheimer's disease (AD), usually relies on the data from one single imaging modality. However, recent technology and equipment advancements provide with us opportunities to better analyze diseases, where we could collect and employ the data from different image and genetic modalities that may potentially enhance the predictive performance. To perform better clustering in AD analysis, in this paper we conduct a new study to make use of the data from different modalities/views. To achieve this goal, we propose a simple yet efficient method based on Non-negative Matrix Factorization (NMF) which can not only achieve better prediction performance but also deal with some data missing in some views. Experimental results on the ADNI dataset demonstrate the effectiveness of our proposed method.	autostereogram;cluster analysis;image;modality (human–computer interaction);non-negative matrix factorization	Kai Liu;Hua Wang;Shannon L. Risacher;Andrew J. Saykin;Li Shen	2018	2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)	10.1109/ISBI.2018.8363834	artificial intelligence;modalities;computer science;pattern recognition;cluster analysis;matrix decomposition;non-negative matrix factorization	Vision	7.970165876498821	-50.65152728901035	186099
994ca78751ecfc571cb7ce05c4343c12b9677b71	a bayesian missing value estimation method for gene expression profile data	gene expression profile;traitement;tratamiento;analisis componente principal;metodo analisis;treatment;bayesian principal component analysis;multivariate analysis;estimation method;latent variable;singular value decomposition;biology;biologia;result;gene expression;probabilistic model;expression genique;methode analyse;analysis method;principal component analysis;inferencia;modele probabiliste;analyse composante principale;resultado;nearest neigh bor;resultat;missing values;dna microarray data;expresion genetica;inference;biologie;modelo probabilista	MOTIVATION Gene expression profile analyses have been used in numerous studies covering a broad range of areas in biology. When unreliable measurements are excluded, missing values are introduced in gene expression profiles. Although existing multivariate analysis methods have difficulty with the treatment of missing values, this problem has received little attention. There are many options for dealing with missing values, each of which reaches drastically different results. Ignoring missing values is the simplest method and is frequently applied. This approach, however, has its flaws. In this article, we propose an estimation method for missing values, which is based on Bayesian principal component analysis (BPCA). Although the methodology that a probabilistic model and latent variables are estimated simultaneously within the framework of Bayes inference is not new in principle, actual BPCA implementation that makes it possible to estimate arbitrary missing variables is new in terms of statistical methodology.   RESULTS When applied to DNA microarray data from various experimental conditions, the BPCA method exhibited markedly better estimation ability than other recently proposed methods, such as singular value decomposition and K-nearest neighbors. While the estimation performance of existing methods depends on model parameters whose determination is difficult, our BPCA method is free from this difficulty. Accordingly, the BPCA method provides accurate and convenient estimation for missing values.   AVAILABILITY The software is available at http://hawaii.aist-nara.ac.jp/~shige-o/tools/.	dna microarray format;exclusion;gene expression profiling;gene expression programming;inference;k-nearest neighbors algorithm;latent variable;missing data;principal component analysis;singular value decomposition;statistical model	Shigeyuki Oba;Masa-aki Sato;Ichiro Takemasa;Morito Monden;Ken-ichi Matsubara;Shin Ishii	2003	Bioinformatics	10.1093/bioinformatics/btg287	latent variable;statistical model;econometrics;gene expression;missing data;data mining;multivariate analysis;imputation;singular value decomposition;statistics;principal component analysis	Comp.	4.9776324103322604	-51.729429321584114	186137
a689a93da9b6840a2c52d9c694bbe4d7fdc9f710	density-based projected clustering of data streams	computing	In this paper, we have proposed, developed and experimentally validated our novel subspace data stream clustering, termed PreDeConStream. The technique is based on the two phase mode of mining streaming data, in which the first phase represents the process of the online maintenance of a data structure, that is then passed to an offline phase of generating the final clustering model. The technique works on incrementally updating the output of the online phase stored in a microcluster structure, taking into consideration those micro-clusters that are fading out over time, speeding up the process of assigning new data points to existing clusters. A density based projected clustering model in developing PreDeConStream was used. With many important applications that can benefit from such technique, we have proved experimentally the superiority of the proposed methods over state-of-the-art techniques.	algorithm;cluster analysis;clustering high-dimensional data;computer cluster;data point;data stream clustering;data structure;experiment;online and offline;stream (computing);streaming media;testbed	Marwan Hassani;Pascal Spaus;Mohamed Medhat Gaber;Thomas Seidl	2012		10.1007/978-3-642-33362-0_24	correlation clustering;constrained clustering;data stream clustering;computing;fuzzy clustering;computer science;theoretical computer science;machine learning;cure data clustering algorithm;data mining;database;cluster analysis;statistics;clustering high-dimensional data	DB	-3.058137506136674	-38.98064848891545	186490
663921712fc80a8a7efcf992368808a921cdf024	a novel approach to the clustering of microarray data via nonparametric density estimation	microarray data;software;nonparametric density estimation;computational biology bioinformatics;cluster analysis;principal component analysis;models statistical;algorithms;humans;combinatorial libraries;computational biology;computer appl in life sciences;dimensional reduction;computer simulation;model based clustering;gene expression profiling;statistics nonparametric;oligonucleotide array sequence analysis;microarrays;bioinformatics	Cluster analysis is a crucial tool in several biological and medical studies dealing with microarray data. Such studies pose challenging statistical problems due to dimensionality issues, since the number of variables can be much higher than the number of observations. Here, we present a general framework to deal with the clustering of microarray data, based on a three-step procedure: (i) gene filtering; (ii) dimensionality reduction; (iii) clustering of observations in the reduced space. Via a nonparametric model-based clustering approach we obtain promising results both in simulated and real data. The proposed algorithm is a simple and effective tool for the clustering of microarray data, in an unsupervised setting.	algorithm;cluster analysis;dimensionality reduction;microarray;unsupervised learning	Riccardo De Bin;Davide Risso	2010		10.1186/1471-2105-12-49	computer simulation;biology;microarray analysis techniques;dna microarray;computer science;bioinformatics;data science;data mining;gene expression profiling;cluster analysis;principal component analysis;clustering high-dimensional data	ML	6.004429432669854	-50.150491163096376	186546
55c44b3228fb6c093a7aec97d965158614e1f548	protecting data: a fuzzy approach	k anonymity;t closeness;08a72;28e10;data privacy;68t01;68t35;l diversity;fuzzy partition;68t27	Privacy issues represent a longstanding problem nowadays. Measures such as k-anonymity, l-diversity and t-closeness are among the most used ways to protect released data. This work proposes to extend these three measures when the data are protected using fuzzy sets instead of intervals or representative elements. The proposed approach is then tested using Energy Information Authority data set and different fuzzy partition methods. Results shows an improvement in protecting data when data are encoded using fuzzy sets.	algorithm;centrality;cluster analysis;experiment;fuzzy clustering;fuzzy logic;fuzzy set;identifier;information privacy;information sensitivity;quasi-identifier	Pelayo Quirós;Pedro Alonso;Irene Díaz;Susana Montes	2015	Int. J. Comput. Math.	10.1080/00207160.2014.928700	membership function;defuzzification;information privacy;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;data mining;database;mathematics;computer security;fuzzy set operations	AI	0.4454412542150256	-38.04860843845007	186774
ce9f1d480c8f410177c43f95dbbedbda2ba23406	cellular automata for modeling protein folding using the hp model	biology computing;evolutionary computation;neural nets;molecular configurations;proteins biology computing cellular automata evolutionary computation molecular biophysics molecular configurations neural nets;proteins;molecular biophysics;differential evolution protein folding cellular automata;amino acids proteins artificial neural networks computational modeling lattices sociology;cellular automata;differential evolution cellular automata final folded protein conformation prediction dynamic folding process temporal folding process amino acids tip model protein folding modeling classical ca model artificial neural networks evolutionary computing	We used cellular automata (CA) for the modeling of the temporal folding of proteins. Unlike the focus of the vast research already done on the direct prediction of the final folded conformations, we will model the temporal and dynamic folding process. The CA model defines how the amino acids interact through time to obtain a folded conformation. We employed the TIP model to represent the protein conformations in a lattice, we extended the classical CA models using artificial neural networks for their implementation, and we used evolutionary computing to automatically obtain the models by means of Differential Evolution. Moreover, the modeling of the folding provides the final protein conformation.	apply;artificial neural network;automata theory;cellular automaton;connectionism;differential evolution;emergence;evolutionary computation;greedy algorithm;lattice model (physics);natural computing;neural oscillation;peptide sequence	José Santos Reyes;Pablo Villot;Martín Diéguez	2013	2013 IEEE Congress on Evolutionary Computation	10.1109/CEC.2013.6557751	fox proteins;computer science;bioinformatics;theoretical computer science;machine learning;evolutionary computation;molecular biophysics	Comp.	-3.4623894733399405	-48.18249176332946	186883
28985b68d0d5bdfcae242a45b6d71f5d8cca2f4d	novel approaches for the exploitation of high throughput sequencing data. (nouvelles approches pour l'exploitation des données de sequençage haut débit)		Novel approaches for the exploitation of high throughput sequencing data In this thesis we discuss computational methods to deal with DNA sequences provided by high throughput sequencers. We will mostly focus on the reconstruction of genomes from DNA fragments (genome assembly) and closely related problems. These tasks combine huge amounts of data with combinatorial problems. Various graph structures are used to handle this problem, presenting trade-off between scalability and assembly quality. This thesis introduces several contributions in order to cope with these tasks. First, novel representations of assembly graphs are proposed to allow a better scaling. We also present novel uses of those graphs apart from assembly and we propose tools to use such graphs as references when a fully assembled genome is not available. Finally we show how to use those methods to produce less fragmented assembly while remaining tractable.	cobham's thesis;graph (discrete mathematics);image scaling;scalability;throughput	Antoine Limasset	2017				Comp.	-0.7833162862029813	-49.29240186775541	186977
beb7346c65ab423e9288cc2ff85bbed103f06729	improving search ability of genetic learning process for high-dimensional fuzzy classification problems	fitness sharing;fuzzy rule based classification system;high dimensional problems;genetic based machine learning	In this paper, we improve efficiency of the genetic search process for generating fuzzy classification rules from high-dimensional problems by using fitness sharing method. First, we define the similarity level of different fuzzy rules. It represents the structural difference of search space in the genetic population. Next, we use sharing method to balance the fitness of different rules and prevent the search process falling into local regions. Then, we combine the sharing method into a hybrid learning approach (i.e., the hybridization of Michigan and Pittsburgh) to obtain the appropriate combination of different rules. Finally, we examine the search ability of different genetic machine learning approaches on a suite of test problems and some well-known classification problems. Experimental results show that the fitness sharing method has higher search ability and it is able to obtained accurate fuzzy classification rules set.	data compaction;experiment;fuzzy classification;fuzzy rule;machine learning;performance	Ji-Dong Li;Xue-Jie Zhang;Yun Gao;Hao Zhou;Jian Cui	2011		10.1007/978-3-642-23881-9_82	fuzzy classification;artificial intelligence;machine learning;data mining;mathematics	AI	9.120263527914698	-41.718614184311875	187527
42bf481744bd8bed7b33df0503647afbf042169c	slice_op: selecting initial cluster centers using observation points		This paper proposes a new algorithm, Slice_OP, which selects the initial cluster centers on high-dimensional data. A set of observation points is allocated to transform the high-dimensional data into one-dimensional distance data. Multiple Gamma models are built on distance data, which are fitted with the expectation-maximization algorithm. The best-fitted model is selected with the second-order Akaike information criterion. We estimate the candidate initial centers from the objects in each component of the best-fitted model. A cluster tree is built based on the distance matrix of candidate initial centers and the cluster tree is divided into K branches. Objects in each branch are analyzed with k-nearest neighbor algorithm to select initial cluster centers. The experimental results show that the Slice_OP algorithm outperformed the state-of-the-art Kmeans++ algorithm and random center initialization in the k-means algorithm on synthetic and real-world datasets.		Md Abdul Masud;Joshua Zhexue Huang;Ming Zhong;Xianghua Fu;Mohammad Sultan Mahmud	2018		10.1007/978-3-030-05090-0_2	data mining;initialization;cluster analysis;akaike information criterion;algorithm;computer science;k-means clustering;distance matrix	Vision	2.4337272966247667	-42.56733727461789	187611
3feb28bc5bcd24a949cb496527b1a3b3e63cf6cd	dplk-means: a novel differential privacy k-means mechanism		K-means algorithm is an important type of clustering algorithm and the foundation of some data mining methods. But it has the risk of privacy disclosure in the process of clustering. In order to solve this problem, Blum et al. proposed a differential privacy K-means algorithm, which can prevent privacy disclosure effectively. However, the availability of clustering results is reduced due to the added noise. In this paper, we propose a novel DPLK-means algorithm based on differential privacy, which improves the selection of the initial center points through performing the differential privacy K-means algorithm to each subset divided by the original dataset. Performance evaluation shows that our algorithm improves the availability of clustering results compared to the existing differential privacy K-means algorithm at the same privacy level.	algorithm;blum axioms;cluster analysis;data mining;differential privacy;k-means clustering;performance evaluation	Jun Ren;Jinbo Xiong;Zhiqiang Yao;Rong Ma;Mingwei Lin	2017	2017 IEEE Second International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2017.64	differential privacy;data mining;cluster analysis;k-means clustering;algorithm design;information privacy;privacy level;computer science	DB	0.16663207596363122	-38.09082717892879	187845
719485999c4595fdb7db34692578db23980923ff	a mutation-sensitive approach for locating conserved gene pairs between related species	biology computing;genomics;computational hardness mutation conserved gene pairs genome alignment problem structural optimization mum selection problem reversals transpositions;optimisation;mice;mum selection problem;structure optimization;conference_paper;transpositions;genome alignment problem;genetics;structural optimization;whole genome alignment;optimization problem;evolution biology;biological cells;computational hardness;genomics bioinformatics genetic mutations computer science samarium humans mice biological cells software algorithms evolution biology;optimisation biology computing computational complexity genetics;computational complexity;samarium;software algorithms;humans;genetic mutations;computer science;conserved gene pairs;mutation;reversals;bioinformatics	This paper proposes a new approach for solving the whole genome alignment problem. Our approach is based on a new structural optimization problem (called the MUM selection problem) related to mutations via reversals and transpositions. We have devised a practical algorithm for this optimization problem and have evaluated the algorithm using 15 pairs of human and mouse chromosomes. The results show that our algorithm is both effective and efficient. More specifically, our algorithm can reveal 91% of the conserved gene pairs that have been reported in the literature. When compared to existing software MUMmer and MaxMinCluster , our algorithm uncovers 15% and 7% more genes on average, respectively. The sensitivity of our algorithm is also slightly higher. The paper concludes with a remark on the computational hardness of the MUM selection problem.	computation;conserved sequence;mummer;mathematical optimization;optimization problem;selection algorithm;shape optimization	Ho-Leung Chan;Tak Wah Lam;Wing-Kin Sung;Prudence W. H. Wong;Siu-Ming Yiu	2004	Proceedings. Fourth IEEE Symposium on Bioinformatics and Bioengineering	10.1109/BIBE.2004.1317390	mutation;optimization problem;biology;genomics;computer science;bioinformatics;samarium;computational complexity theory;genetics	Comp.	3.1849471491446724	-48.13774107950961	187908
7c6568217fb72d9169e82e8f1a065a84d6287c68	a novel framework for large scale metabolic network alignments by compression	metabolic network;protein structure determination;large scale;ensembles;space complexity;statistical inference;computational biology	Although the problem of aligning metabolic networks has been considered in the past, the running time and space complexity of these solutions has so far limited their use to moderately sized networks. In this paper, we address the problem of aligning two metabolic networks, particularly when both of them are too large to be dealt with using existing methods. We develop a generic framework that can be used with any existing method to significantly improve the scale of the networks that can be aligned in practical time. Our framework has three major phases, namely the compression phase, the alignment phase and the refinement phase. For the first phase, we develop an algorithm which transforms the given networks to a compressed domain where they are summarized using fewer nodes, termed supernodes, and interactions. In the second phase, we carry out the alignment in the compressed domain using an existing method as our base algorithm. This alignment results in supernode mappings in the compressed domain, each of which are smaller instances of network alignment. In the third phase, we solve each of the instances using the base alignment algorithm to refine the alignment results. Our experiments demonstrate that this method can reduce the sizes of metabolic networks by almost half at each compression level. For the overall framework, we demonstrate how well it increases the performance of an existing alignment method. We observe that we can align twice or more as large networks using the same amount of resources with our framework compared to a recent method for network alignment, namely SubMAP. Our results also suggest that the alignment obtained by only one level of compression captures the original alignment results with very high accuracy.	algorithm;align (company);artificial neural network;computation;dspace;experiment;gene regulatory network;image scaling;interaction;kegg;refinement (computing);scalability;supernode (circuit);time complexity	Michael Dang;Ferhat Ay;Tamer Kahveci	2011		10.1145/2147805.2147835	computer science;bioinformatics;theoretical computer science;data mining	Comp.	-1.4393887141872395	-51.97602711065078	187912
2187cd5d7e0a226f917d751a5c9e7aad4996ba29	infer genetic/transcriptional regulatory networks by recognition of microarray gene expression patterns using adaptive neuro-fuzzy inference systems	microarray data;expression pattern;time course;gene network;gene expression data;genetics;feature extraction;true positive;gene expression pattern;adaptive neuro fuzzy inference system;fuzzy model;transcriptional regulatory network	  In our previous studies, we first noted that dot products of 1  st  - and 2  nd  -order differential derivatives are effective to extract paired expression patterns in time-course microarray gene expression  data. Here, the feature extraction model and an adaptive neuro-fuzzy inference system are encompassed together for fusing  information attained from microarray experiments and known gene-gene interactions confirmed by biological experiments. Having  the capability allows computer to associate gene expression patterns with genetic or transcriptional interactions, we can  use it to identify other interactions that have not yet been confirmed by biologists. We also bring forward the concept of  multilayer adaptive neuro-fuzzy inference systems showing how fuzzy models collaborate with each other on the basis of limited  time-course microarray data and take advantage of the known gene-gene interactions. In this chapter, we examine the fuzzy  model that may lead to higher true-positive results in prediction of gene networks. We also discuss the relative merits and  drawbacks of two methods, and provide recommendations for readers to choose the most suitable method specifically for their  applications in bioinformatics.    	fuzzy logic;microarray;neuro-fuzzy	Cheng-Long Chuang;Chung-Ming Chen;Joe-Air Jiang	2009		10.1007/978-3-540-89968-6_11	bioinformatics;data mining;genetics	Comp.	6.427382800562236	-49.8702444066628	187985
bc4def6197462602cbb04fb57c16aec74d1d2001	switching regression models using ambiguity and distance rejects: application to ionogram analysis	minimisation;error rejection;distance rejection;prototypes;optimal method;ambiguity rejection;regression model;maximum likelihood estimation;fuzzy set theory;objective function;training data;fuzzy clustering;marine vehicles;switching regression models;maximum likelihood detection;membership function;pattern recognition;clustering algorithms;parameter estimation clustering algorithms algorithm design and analysis optimization methods marine vehicles training data maximum likelihood detection maximum likelihood estimation prototypes equations;fuzzy regression model;switching regression;minimisation pattern recognition fuzzy set theory parameter estimation;parameter estimation;ionogram;minimisation switching regression models ambiguity rejection distance rejection ionogram fuzzy regression model fuzzy set theory fuzzy clustering error rejection objective function parameter estimation;algorithm design and analysis;optimization methods	Fuzzy c-regression algorithms such as FcRM (fuzzy cregression models) 131 which use calculus-based optimization methods suffer from several drawbacks : they are very sensitive to the presence of noise. Moreover, the memberships are relative numbers. This can be a serious problem in situations where one wishes to generate membership functions from training data. This paper examines how reject options can be used in pe$orming switching regression models. Two types of reject have been included : (1) the ambiguity reject which concerns the data points which fit several models equally well; (2) the distance or error reject dealing with patterns that are far away from all the clusters. To compute these rejects, we use an extension of the Fc+2M algorithm objective function [4]. This algorithm is called the fuzzy c+2-regression model (Fc+2RM) .	algorithm;data point;ionogram;loss function;mathematical optimization;membership function (mathematics);optimization problem	Michel Ménard;Pierre-André Dardignac;Vincent Courboulay	2000		10.1109/ICPR.2000.906168	algorithm design;minimisation;ionogram;training set;membership function;fuzzy clustering;computer science;machine learning;pattern recognition;mathematics;prototype;maximum likelihood;fuzzy set;cluster analysis;estimation theory;regression analysis;statistics	ML	3.6783172315547175	-39.800948517069386	187997
16464835d1ff976bd5354a1da7f31da9794993f8	development and validation of optics based spatio-temporal clustering technique	st dbscan;st optics;clustering;spatio temporal;c mathematical and quantitative methods;cluster validation indices	Spatio-temporal data mining (STDM) is a process of the extraction of implicit knowledge, spatial and temporal relationships, or other patterns not explicitly stored in spatiotemporal databases. As data are growing not only from static view point, but they also evolve spatially and temporally which is dynamic in nature that is the reason why this field is now becoming very important field of research. In addition, spatio-temporal (ST) data tend to be highly auto-correlated, which leads to failure of assumption of independence, which is there in Gaussian distribution model. Vital issues in spatio temporal clustering technique for earth observation data is to obtain clusters of, good quality, arbitrary shape, problem of nested clustering and their validation. The present paper addresses these issues and proposes their solutions. In this direction, an attempt has been made to develop a clustering algorithm named as “Spatio-Temporal Ordering Points to Identify Clustering Structure (ST-OPTICS)” which is modified version of existing density based technique – “Ordering Points to Identify Clustering Structure (OPTICS)”. Experimental work carried out is analysed and found that quality of clusters obtained and run time efficiency are much better than existing technique i.e. ST-DBSCAN. In order to improve the visualization and the interpretation of obtained micro level clusters, sincere effort has been put in to merge the obtained clusters using agglomerative approach. Performance evaluation is done in both ways i.e. qualitatively and quantitatively for cross validating the results. Results show performance improvement of proposed ST-OPTICS clustering technique compared to ST-DBSCAN algorithm. © 2016 Elsevier Inc. All rights reserved.	cluster analysis;computer cluster;concatenation;dbscan;data mining;data structure;emoticon;mathematical optimization;optics algorithm;open research;parallel computing;performance evaluation;run time (program lifecycle phase);scalability;spatiotemporal database;temporal logic;verification and validation	K. P. Agrawal;Sanjay Garg;Shashikant Sharma;Pinkal Patel	2016	Inf. Sci.	10.1016/j.ins.2016.06.048	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;artificial intelligence;data science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;brown clustering;dbscan;optics algorithm;affinity propagation;statistics;clustering high-dimensional data	DB	-0.5293421220304743	-43.603234424239155	188397
b15ecb7840094a7460285dcde84f9659c859dfe1	electre tri machine learning approach to the record linkage		In this short paper, the Electre Tri-Machine Learning Method, generally used to solve ordinal classification problems, is proposed for solving the Record Linkage problem. Preliminary experimental results show that, using the Electre Tri method, high accuracy can be achieved and more than 99% of the matches and nonmatches were correctly identified by the procedure.	level of measurement;linkage (software);machine learning;triangular function	Renato De Leone;Valentina Minnetti	2018	CoRR	10.1007/978-3-319-55708-3_4	electre;record linkage;machine learning;computer science;artificial intelligence	ML	9.420410996046462	-42.494376089714414	188422
ba020a768f29f83862a81ae56d8591990c5a8e86	mhhusp: an integrated algorithm for mining and hiding high utility sequential patterns	databases;electronic mail;data mining;urban areas;business;computer science;security	Hiding High Utility Sequential Patterns (HUSPs) is the task of finding the ways how to hide high utility sequential patterns appearing in sequence databases so that the adversaries cannot discover them after hiding. It has become an important research topic in recent years and has been applied in various domains such as business, marketing, stock, health and security, etc. However, few methods have been proposed for hiding high utility sequential patterns. The traditional approach is first using mining algorithms to discover all high utility sequential patterns in a specific user threshold and then apply hiding algorithms to conceal them. Generally, these algorithms are usually time-consuming when performing in the large datasets. To address this issue, this paper presents an integrated algorithm named MHHUSP (Mining with Hiding High Utility Sequential Patterns) which combines mining process with hiding process in a common process. An extensive experimental evaluation is conducted on large-scale datasets to evaluate the performance of the proposed algorithm in terms of execution time and memory consumption. Experimental results show that MHHUSP outperforms the state-of-the-art HHUSP algorithm [2].	data mining;experiment;genetic algorithm;parallel algorithm;run time (program lifecycle phase);sequence database	Minh Nguyen Quang;Tai Dinh;Ut Huynh;Hoai Bac Le	2016	2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)	10.1109/KSE.2016.7758022	computer science;artificial intelligence;information security;data science;machine learning;data mining;world wide web	AI	-4.423025808199002	-38.13919014515009	188497
ee11d72e4e26eb5872bbc2bb5ba16af260bd0d26	search space partitioning to enhance outlier rule discovery	search space;enhance outlier rule discovery	This paper presents an enhancement of the multidimensional quantitative rule discovery methodology presented in [9] by clustering of preselected μ-tuples. This unconventional, in KDD cycle, usage of clustering as a preprocessing step allows for significant speed-up without compromising informational value of discovered results. In that sense it is a continuation of research described in [2]. The paper presents the outcome of experiments performed with this method over datasets obtained by random sampling of graphic image files. The outcome clearly shows the advantages and applicability of proposed methodology.	space partitioning	Michal J. Okoniewski;Piotr Gawrysiak;Lukasz Gancarz	2002			searching the conformational space for docking;computer science;k-optimal pattern discovery	DB	-2.1223179557786973	-40.223359221421774	188837
e867937ab9c3a371c43bd528c2a02e2d4e57aad5	nearest hyperplane distance neighbor clustering algorithm applied to gene co-expression analysis in alzheimer's disease	dna;clustering algorithms proteins alzheimer s disease gene expression classification algorithms dna;cluster algorithm;nf kappa b p50 subunit;biological tissues;unsupervised clustering;brain;classification algorithm;expression pattern;expression analysis;neurophysiology biological tissues cellular biophysics diseases genetics lab on a chip medical computing;gene expression data;genetics;algorithms alzheimer disease brain cluster analysis gene expression profiling gene expression regulation humans multigene family nf kappa b p50 subunit nerve tissue proteins oligonucleotide array sequence analysis protein interaction mapping;medical computing;gene expression;microarray analysis;cluster analysis;proteins;dna microarray unsupervised clustering algorithm k local hyperplane distance nearest neighbor classifier nearest hyperplane distance neighbor clustering algorithm gene coexpression analysis alzheimer s disease nfkbl gene nf kb complex neuroinflammatory response ad pathogenesis gene expression data hippocampal tissue;high dimensional data;nearest neighbor;classification algorithms;gene expression regulation;alzheimer s disease;diseases;clustering algorithms;lab on a chip;algorithms;nerve tissue proteins;humans;dna microarray data;neurophysiology;protein interaction mapping;nearest neighbor classifier;alzheimer disease;cellular biophysics;multigene family;gene expression profiling;oligonucleotide array sequence analysis;literature search	Microarray analysis can contribute considerably to the understanding of biologically significant cellular mechanisms that yield novel information regarding co-regulated sets of gene patterns. Clustering is one of the most popular tools for analyzing DNA microarray data. In this paper, we present an unsupervised clustering algorithm based on the K-local hyperplane distance nearest-neighbor classifier (HKNN). We adapted the well-known nearest neighbor clustering algorithm for use with hyperplane distance. The result is a simple and computationally inexpensive unsupervised clustering algorithm that can be applied to high-dimensional data. It has been reported that the NFkB1 gene is progressively over-expressed in moderate-to-severe Alzheimer's disease (AD) cases, and that the NF-kB complex plays a key role in neuroinflammatory responses in AD pathogenesis. In this study, we apply the proposed clustering algorithm to identify co-expression patterns with the NFkB1 in gene expression data from hippocampal tissue samples. Finally, we validate our experiments with biomedical literature search.	alzheimer's disease;dna microarray format;experiment;gene expression;kilobyte;nearest neighbour algorithm;single linkage cluster analysis;single-linkage clustering;whole earth 'lectronic link;lyt-10 protein;statistical cluster	Cristian F. Pasluosta;Prerna Dua;Walter J. Lukiw	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091344	correlation clustering;biology;fuzzy clustering;flame clustering;computer science;bioinformatics;machine learning;cure data clustering algorithm;data mining;cluster analysis;neurophysiology;clustering high-dimensional data	Comp.	5.298932579153025	-48.495397351700724	188918
69d2d0d2c3a6ad9f4f59c0021b48df3ba165f260	coupled matrix factorization with sparse factors to identify potential biomarkers in metabolomics	optimisation;metabolomics;coupled matrix factorization;nuclear magnetic resonance;joints;sparsity;matrix decomposition;metabolomics coupled matrix factorization sparsity gradient based optimization missing data;optimisation biochemistry blood chromatography gradient methods mass spectroscopy matrix decomposition nuclear magnetic resonance;blood;sparse matrices metabolomics optimization data models nuclear magnetic resonance joints;apple intake chemical substance detection biological fluids nuclear magnetic resonance spectroscopy nmr spectroscopy liquid chromatography mass spectroscopy lc ms metabolomic data analysis interpretable underlying patterns biomarker discovery sparsity constraints factor matrices all at once optimization algorithm cmf spopt coupled matrix factorization with sparse optimization gradient based optimization approach numerical experiments simulated data blood samples rats potential biomarker identification;gradient methods;optimization;missing data;mass spectroscopy;chromatography;sparse matrices;biochemistry;data models;gradient based optimization	Metabolomics focuses on the detection of chemical substances in biological fluids such as urine and blood using a number of analytical techniques including Nuclear Magnetic Resonance (NMR) spectroscopy and Liquid Chromatography-Mass Spectroscopy (LC-MS). Among the major challenges in analysis of metabolomics data are (i) joint analysis of data from multiple platforms and (ii) capturing easily interpretable underlying patterns, which could be further utilized for biomarker discovery. In order to address these challenges, we formulate joint analysis of data from multiple platforms as a coupled matrix factorization problem with sparsity constraints on the factor matrices. We develop an all-at-once optimization algorithm, called CMF-SPOPT (Coupled Matrix Factorization with SParse Optimization), which is a gradient-based optimization approach solving for all factor matrices simultaneously. Using numerical experiments on simulated data, we demonstrate that CMF-SPOPT can capture the underlying sparse patterns in data. Furthermore, on a real data set of blood samples collected from a group of rats, we use the proposed approach to jointly analyze metabolomic data sets and identify potential biomarkers for apple intake.	algorithm;data mining;experiment;gradient;image scaling;list of content management frameworks;mathematical optimization;metabolomics;numerical analysis;program optimization;resonance;sparse matrix	Evrim Acar;Gozde Gurdeniz;Morten A. Rasmussen;Daniela Rago;Lars Ove Dragsted;Rasmus Bro	2012	2012 IEEE 12th International Conference on Data Mining Workshops	10.1109/ICDMW.2012.17	data modeling;sparse matrix;missing data;bioinformatics;metabolomics;mathematics;matrix decomposition;sparsity-of-effects principle;statistics	ML	7.882271309500904	-51.297653508971116	188977
4d9ee229c8cf7066a40ee34d98c6e740e5a467c3	the application of improved decision tree algorithm in data mining of employment rate: evidence from china	computers;employment;decision trees data mining employment testing education classification tree analysis application software computer science decision making databases;decision tree;training;information gain ratio;information gain decision tree algorithm data mining attribute measure;data mining;reference value;classification algorithms;decision tree algorithm;decision trees;student employment rate;information gain;employment data mining decision trees;information gain ratio decision tree algorithm data mining student employment rate attribute measure;attribute measure	In allusion to the limitations of ID3 algorithm which is the core algorithm of building decision tree, this paper presented the improved project which added attribute measure and introduced information gain ratio. The project trained the example set in the data mining of the students' employment rate and created the employment data mining model. The model was tested with the data of the students in school. The test proved that the improved decision tree algorithm has high reference value to the reallocation of the teaching resources.	data mining;decision tree model;id3 algorithm;information gain ratio;kullback–leibler divergence;list of algorithms	Yuxiang Shao;Weiming Yin	2009	2009 First International Workshop on Database Technology and Applications	10.1109/DBTA.2009.72	statistical classification;decision tree learning;computer science;data science;information gain ratio;machine learning;decision tree;incremental decision tree;data mining;id3 algorithm	ML	8.681840979601986	-39.064301616037724	189054
4794e43418a870a222b8f83d42ce63c58170f827	optimal row and column ordering to improve table interpretation using estimation of distribution algorithms	ordering of tables;conciseness;selective naive bayes;parametrization;bertin matrices;estimation of distribution algorithms	A common information representation task in research as well as educational and statistical practice is to comprehensively and intuitively express data in two-dimensional tables. Examples include tables in scientific papers, as well as reports and the popular press. Data is often simple enough for users to reorder. In many other cases though, there are complex data patterns that make finding the best re-arrangement of rows and columns for optimum readability a tough problem. We propose that row and column ordering should be regarded as a combinatorial optimization problem and solved using evolutionary computation techniques. The use of genetic algorithms has already been proposed in the literature. This paper proposes for the first time the use of estimation of distribution algorithms for table ordering. We also propose alternative ways of representing the problem in order to reduce its dimensionality. By learning a selective naive Bayes classifier, we can find out how to jointly combine the parameters of these algorithms to get good table orderings. Experimental examples in this paper are on 2D tables. E. Bengoetxea ( ) Department of Computer Architecture and Technology, University of the Basque Country, Paseo Manuel Lardizabal 1, 20018 San Sebastian, Spain e-mail: endika@ehu.es P. Larrañaga · C. Bielza · J.A. Fernández del Pozo Departmento de Inteligencia Artificial, Universidad Politécnica de Madrid, Campus de Montegancedo, Boadilla del Monte, 28660 Madrid, Spain P. Larrañaga e-mail: pedro.larranaga@fi.upm.es C. Bielza e-mail: mcbielza@fi.upm.es J.A. Fernández del Pozo e-mail: jafernandez@fi.upm.es 568 E. Bengoetxea et al.	ant colony;bioinformatics;blue brain project;column (database);combinatorial optimization;computer architecture;email;estimation of distribution algorithm;evolutionary computation;experiment;genetic algorithm;mathematical optimization;metaheuristic;naive bayes classifier;optimization problem;relevance;scientific literature;table (database);tabu search	Endika Bengoetxea;Pedro Larrañaga;Concha Bielza;Juan A. Fernández del Pozo	2011	J. Heuristics	10.1007/s10732-010-9145-z	parametrization;mathematical optimization;estimation of distribution algorithm;computer science;artificial intelligence;theoretical computer science;machine learning;data mining;mathematics	AI	4.690555384911847	-43.03750552847718	189147
8c8a668b57e339c94e935954a550f89e77681ba8	noise rejection in mmms-induced fuzzy co-clustering			biclustering;noise reduction;rejection sampling	Katsuhiro Honda;Nami Yamamoto;Seiki Ubukata;Akira Notsu	2017	JACIII	10.20965/jaciii.2017.p1144	machine learning;computer science;fuzzy logic;pattern recognition;artificial intelligence;fuzzy clustering;biclustering	ML	3.0884542717973593	-40.658192494638975	189275
7023f957fad4b2edce318696377cbbd1fa0c91a0	support vectors based correlation coefficient for gene and sample selection in cancer classification	dna;sample selection;support vector machines bioinformatics cancer pattern classification;support vector machines;cancer;training;filters;size measurement;data mining;support vector;gene expression;accuracy;sensitivity;cancer filters gene expression partitioning algorithms support vector machines benchmark testing dna size measurement costs data mining;svcc rfe algorithm correlation coefficient gene selection sample selection cancer classification support vectors backward elimination;support vectors;pattern classification;correlation;cancer classification;correlation coefficient;svcc rfe algorithm;gene selection;benchmark testing;partitioning algorithms;bioinformatics;backward elimination	Correlation is a very widely used filter criterion for gene selection in cancer classification. However, it uses all the training samples in ranking, which may not be equally important for the classification. Using support vectors, we demonstrate that classical correlation coefficient based gene selection is biased because of the sample points away from classification margin. To remove such bias, we use only the support vectors for computation of correlation coefficient and propose a backward elimination based SVcc-RFE algorithm. The proposed method is tested on several benchmark cancer gene expression datasets and the results show improvement in classification performance compared to other state-of-the-art methods.	algorithm;benchmark (computing);coefficient;computation;gene co-expression network;stepwise regression	Piyushkumar A. Mundra;Jagath C. Rajapakse	2010	2010 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology	10.1109/CIBCB.2010.5510689	support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics;genetics	Comp.	8.374024731028236	-47.247236644855455	189863
1e668bf55c7b161b11b372c12db8b7ddbcb0a317	mcsom: minimal coloring of self-organizing map	unsupervised learning;neighborhood relations;cluster algorithm;k means;hierarchical classification;cluster analysis;minimal coloring;clustering;self organizing map;self organized map;clustered data;artificial neural network	A  Self-Organizing Map  (SOM) is an artificial neural network tool that is trained using unsupervised learning to produce a low-dimensional representation of the input space, called a map. This map is generally the subject of a clustering analysis step which aims to partition the referents vectors (map neurons) in compact and well-separated groups. In this paper, we consider the problem of clustering self-organizing map using a modified graph minimal coloring algorithm. Unlike the traditional clustering SOM techniques, using k-means or hierarchical classification, our approach has the advantage to provide a partition of self-organizing map by simultaneously using dissimilarities and neighborhood relations provided by SOM. Experimental results on benchmark data sets demonstrate that the proposed clustering algorithm is able to cluster data in a better way than classical ones and indicates the effectiveness of SOM to offer real benefits for the original minimal coloring clustering approach.	organizing (structure);self-organizing map	Haytham Elghazel;Khalid Benabdeslem;Hamamache Kheddouci	2009		10.1007/978-3-642-03348-3_15	unsupervised learning;correlation clustering;self-organizing map;k-medians clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;artificial neural network	Robotics	1.9810018438109853	-41.97916662543987	189966
b3e2f96442e7347a02d6417cb3c70a18b3db6e2d	a novel fast and memory efficient parallel mlcs algorithm for long and large-scale sequences alignments	sorting bioinformatics computational complexity parallel algorithms problem solving;multiple longest common subsequences mlcs;parallel algorithm;heuristic algorithms algorithm design and analysis dynamic programming complexity theory biology bioinformatics silicon;icsg pcc model;parallel algorithm multiple longest common subsequences mlcs irredundant common subsequence graph icsg parallel collection chain pcc icsg pcc model;parallel collection chain pcc;irredundant common subsequence graph icsg;problem solving model parallel mlcs algorithm large scale sequences alignments big data multiple longest common subsequences parallel topological sorting strategies space complexities time complexities bioinformatics computational genomics	Information usually can be abstracted as a character sequence over a finite alphabet. With the advent of the era of big data, the increasing length and size of the sequences from various application fields (e.g., biological sequences) result in the classical NP-hard problem, searching for the Multiple Longest Common Subsequences of multiple sequences (i.e., MLCS problem with many applications in the areas of bioinformatics, computational genomics, pattern recognition, etc.), becoming a research hotspot and facing severe challenges. In this paper, we firstly reveal that the leading dominant-point-based MLCS algorithms are very hard to apply to long and large-scale sequences alignments. To overcome their defects, based on the proposed problem-solving model and parallel topological sorting strategies, we present a novel efficient parallel MLCS algorithm. The comprehensive experiments on the benchmark datasets of both random and biological sequences demonstrate that both the time and space complexities of the proposed algorithm are only linearly related to the dominants from aligned sequences, and that the proposed algorithm greatly outperforms the existing state-of-the-art dominant-point-based MLCS algorithms, and hence it is very suitable for long and large-scale sequences alignments.	algorithm;benchmark (computing);big data;bioinformatics;computational genomics;dspace;dhrystone;experiment;java hotspot virtual machine;np-hardness;overhead (computing);parallel algorithm;parallel text;pattern recognition;portable c compiler;problem solving;sequence alignment;topological sorting	Yanni Li;Yuping Wang;Zhensong Zhang;Yaxin Wang;Ding Ma;Jianbin Huang	2016	2016 IEEE 32nd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2016.7498322	computer science;bioinformatics;theoretical computer science;parallel algorithm	DB	-1.8222278685240245	-51.78258528003425	190151
32de2ef5ce802acf2c1a36bb57104ce5daa352b7	improvement of jarvis-patrick clustering based on fuzzy similarity	jarvis patrick clustering;cluster algorithm;distance measure;jarvispatrick clustering;vat;fuzzy similarity measure;visualization technique;clustering method;neighborhood relation;k nearest neighbor;mds;similarity measure	Different clustering algorithms are based on different similarity or distance measures (e.g. Euclidian distance, Minkowsky distance, Jackard coefficient, etc.). Jarvis-Patrick clustering method utilizes the number of the common neighbors of the k-nearest neighbors of objects to disclose the clusters. The main drawback of this algorithm is that its parameters determine a too crisp cutting criterion, hence it is difficult to determine a good parameter set. In this paper we give an extension of the similarity measure of the Jarvis-Patrick algorithm. This extension is carried out in the following two ways: (i) fuzzyfication of one of the parameters, and (ii) spreading of the scope of the other parameter. The suggested fuzzy similarity measure can be applied in various forms, in different clustering and visualization techniques (e.g. hierarchical clustering, MDS, VAT). In this paper we give some application examples to illustrate the efficiency of the use of the proposed fuzzy similarity measure in clustering. These examples show that the proposed fuzzy similarity measure based clustering techniques are able to detect clusters with different sizes, shapes and densities. It is also shown that the outliers are also detectable by the proposed measure.	academy;cluster analysis;coefficient;euclidean distance;hierarchical clustering;jaccard index;k-nearest neighbors algorithm;similarity measure	Ágnes Vathy-Fogarassy;Attila Kiss;János Abonyi	2007		10.1007/978-3-540-73400-0_24	complete-linkage clustering;correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;value-added tax;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;similarity;k-nearest neighbors algorithm;dbscan;spectral clustering;clustering high-dimensional data	ML	1.6747271427043167	-40.28315068059252	190170
7e49046ad5a69d15d75cc9b8136f581f6be4a247	extending consensus clustering to explore multiple clustering views		Consensus clustering has emerged as an important extension of the classical clustering problem. Given a set of input clusterings of a given dataset, consensus clustering aims to find a single final clustering which is a better fit in some sense than the existing clusterings. There is a significant drawback in generating a single consensus clustering since different input clusterings could differ significantly. In this paper, we develop a new framework, called Multiple Consensus Clustering (MCC), to explore multiple clustering views of a given dataset from a set of input clusterings. Instead of generating a single consensus, MCC organizes the different input clusterings into a hierarchical tree structure and allows for interactive exploration of multiple clustering solutions. A dynamic programming algorithm is proposed to obtain a flat partition from the hierarchical tree using the modularity measure. Multiple consensuses are finally obtained by applying consensus clustering algorithms to each cluster of the partition. Extensive experimental results on 11 real world data sets and a case study on a Protein-Protein Interaction (PPI) data set demonstrate the effectiveness of our proposed method.	algorithm;cluster analysis;consensus clustering;dynamic programming;hierarchical clustering;pixel density;protein data bank;tree structure	Yi Zhang;Tao Li	2011		10.1137/1.9781611972818.79	machine learning;consensus clustering;computer science;artificial intelligence;pattern recognition;cluster analysis;partition (number theory);tree structure;dynamic programming;modularity;data set	ML	1.7573071512750458	-42.346734883103096	190216
d470d99b6be67ce3ce8f91bcbba354fdae1feeab	diversity-driven generation of link-based cluster ensemble and application to data classification	feature transformation;ensemble clustering;optimization;data classification	HighlightsNew data-transformation method that makes use of link-based cluster ensemble (LCE).For accurate clustering, LCE is coupled with diversity-driven ensemble generation.Evaluated on published datasets with C4.5, NB, KNN, ANN and Random Forest models.New method usually performs better than benckmark techniques. Over decades, a large number of research studies have concentrated on improving the accuracy of classification model. This is the case as several types of classifiers prove to be useful in real-life problems, including the prediction of system failure risk and microarray-based cancer diagnosis. Despite this, the accuracy of existing classifiers has been constrained by uninformative variables typically observed in modern data. In addition to feature selection, one may transform the original data to another variation, where only key feature components are included. Unlike conventional transformation-based techniques found in the literature, this paper presents a novel method that makes use of cluster ensembles, specifically the summarized information matrix, as the transformed data for the following classification step. Among different state-of-the-art methods, the link-based cluster ensemble approach (LCE) provides a highly accurate clustering, and thus particularly employed here. This is uniquely coupled with a diversity-driven generation of ensemble, which provides informative and diverse sets of clusterings. The performance of this transformation model is evaluated on published synthetic, standard and gene expression datasets; using C4.5, Naive Bayes, KNN, Neural Network and Random Forest classifiers; in comparison with benchmark techniques. The findings suggest that the new model can improve the classification accuracy of original data and performs better than the other transformation methods investigated in the empirical study.		Natthakan Iam-on;Tossapon Boongoen	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.06.051	computer science;machine learning;pattern recognition;data mining;statistics	NLP	9.678196809211972	-47.04397146650677	190305
3cae79cf1fdfc0d7a88467e37d6e6b25d44383d4	sequential superparamagnetic clustering for unbiased classification of high-dimensional chemical data	chemical structure	For the clustering of chemical structures that are described by the Similog, ISIS count, and ISIS binary fingerprints, we propose a sequential superparamagnetic clustering approach. To appropriately handle nonbinary feature keys, we introduce an extension of the binary Tanimoto similarity measure. In our applications, data sets composed of structures from seven chemically distinct compound classes are evaluated and correctly clustered. The comparison, with results from leading methods, indicates the superiority of our sequential superparamagnetic clustering approach.	class;cluster analysis;fingerprint;isis;jaccard index;norm (social);similarity measure;statistical cluster	Thomas Ott;Albert Kern;Ansgar Schuffenhauer;Maxim Popov;Pierre Acklin;Edgar Jacoby;Ruedi Stoop	2004	Journal of chemical information and computer sciences	10.1021/ci049905c	correlation clustering;chemical structure;superparamagnetism;mathematics;fuzzy clustering;similarity measure;cluster analysis;machine learning;data set;artificial intelligence	ML	6.448870848415013	-48.64249365378358	190535
570d703791d9c979387adb3cc948dc72488889c7	graph partitioning advance clustering technique		Clustering is a common technique for statistical data analysis, Clustering is the process of grouping the data into classes or clusters so that objects within a cluster have high similarity in comparison to one another, but are very dissimilar to objects in other clusters. Dissimilarities are assessed based on the attribute values describing the objects. Often, distance measures are used. Clustering is an unsupervised learning technique, where interesting patterns and structures can be found directly from very large data sets with little or none of the background knowledge. This paper also considers the partitioning of mdimensional lattice graphs using Fiedler’s approach, which requires the determination of the eigenvector belonging to the second smallest Eigen value of the Laplacian with K-means partitioning algorithm.	algorithm;cluster analysis;eigen (c++ library);graph partition;k-means clustering;unsupervised learning	T. Soni Madhulatha	2012	CoRR		complete-linkage clustering;correlation clustering;constrained clustering;combinatorics;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;mathematics;cluster analysis;single-linkage clustering;spectral clustering;clustering high-dimensional data	DB	1.032169338686685	-41.96161298280495	190570
ee96b8c7f9a3ef680e776f3d7356cf4136b30d11	norm-induced shell-prototypes (nisp) clustering	shell clustering;cluster analysis;pattern recognition;genetic algorithms		cluster analysis;national industrial security program	James C. Bezdek;Richard J. Hathaway;Nikhil R. Pal	1995	Neural Parallel & Scientific Comp.		correlation clustering;consensus clustering;hierarchical clustering;cluster analysis;single-linkage clustering;biclustering	ML	3.8757422278027707	-42.51041918829621	190681
7be45ce051d1ede9d99eb2dda35904fb666465ce	a sequence-element-based hierarchical clustering algorithm for categorical sequence data	hierarchical clustering;cluster algorithm;sequences;sequence similarity;protein sequence;scientific data;data mining;similarity measure	Recently, there has been enormous growth in the amount of commercial and scientific data, such as protein sequences, retail transactions, and web-logs. Such datasets consist of sequence data that have an inherent sequential nature. However, few existing clustering algorithms consider sequentiality. In this paper, we study how to cluster these sequence datasets. We propose a new similarity measure to compute the similarity between two sequences. In the proposed measure, subsets of a sequence are considered, and the more identical subsets there are, the more similar the two sequences. In addition, we propose a hierarchical clustering algorithm and an efficient method for measuring similarity. Using a splice dataset and synthetic datasets, we show that the quality of clusters generated by our proposed approach is better than that of clusters produced by traditional clustering algorithms.	algorithm;hierarchical clustering	Seung-Joon Oh;Jae-Yearn Kim	2005	International Journal of Information Technology and Decision Making	10.1142/S0219622005001398	complete-linkage clustering;correlation clustering;data stream clustering;k-medians clustering;fuzzy clustering;computer science;bioinformatics;canopy clustering algorithm;protein sequencing;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;sequence;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;statistics;hierarchical clustering of networks;data;clustering high-dimensional data	DB	-1.8732274259165462	-41.00280239603635	190695
63f43ce2fb3001825984e3a0ba2fc04e404ae2ca	representative points clustering algorithm based on density factor and relevant degree		Most of the existing clustering algorithms are affected seriously by noise data and high cost of time. In this paper, on the basis of CURE algorithm, a representative points clustering algorithm based on density factor and relevant degree called RPCDR is proposed. The definition of density factor and relevant degree are presented. The primary representative point whose density factor is less than the prescribed threshold will be deleted directly. New representative points can be reselected from non representative points in corresponding cluster. Moreover, the representative points of each cluster are modeled by using K-nearest neighbor method. Relevant degree is computed by comprehensive considering the correlations of objects within a cluster and between different clusters. And then whether the two clusters need to merge is judged. The theoretic experimental results and analysis prove that RPCDR has better clustering accuracy and execution efficiency.	cluster analysis;k-nearest neighbors algorithm;nearest neighbor search;sparse matrix;synthetic intelligence;theory	Di Wu;Jiadong Ren;Long Sheng	2017	Int. J. Machine Learning & Cybernetics	10.1007/s13042-015-0451-5	complete-linkage clustering;correlation clustering;combinatorics;data stream clustering;k-medians clustering;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;dbscan;statistics	DB	1.05613744789327	-41.1178700893867	191024
dab41a163daf5d18adda8ec9ed87ee84cd084c39	automatic quality assessment of affymetrix genechip data	automated quality assessment;quality metric;gene expression data;gene expression;quality assessment;machine learning;expert opinion;quality control;knowledge based expert system;affymetrix genechip microarray experiments;knowledge base;expert system	Computing reliable gene expression levels from microarray experiments is a sophisticated process with many potential pitfalls. Quality control is one of the most important steps in this process. We present a web based expert system for automatic quality assessment of Affymetrix GeneChip data. Our approach combines multiple quality metrics with supervised machine learning in order to identify data of low quality. Our system approximates expert opinion as represented in a knowledge base consisting of 41 microarray experiments with 352 CEL files annotated by a domain expert. GeneChips of low quality are detected automatically and can be excluded from subsequent analysis. This is especially important for large experiments or can assist the inexperienced users. Our expert system is fully implemented and integrated into a publicly available remote analysis computation engine for gene expression data.	affymetrix genechip operating software;computation;experience;experiment;expert system;knowledge base;machine learning;microarray;subject-matter expert;supervised learning	Steffen Heber;Beate Sick	2006		10.1145/1185448.1185540	affymetrix genechip operating software;computer science;bioinformatics;data science;data mining	Comp.	7.558252021854431	-48.380679291615664	191085
df8e07187a5ed724767018a6b2f030b3caa8bc4e	breast cancer detection with logistic regression improved by features constructed by kaizen programming in a hybrid approach	evolutionary computation;standards;standards continuous improvement breast cancer logistics programming evolutionary computation;automatic feature construction breast cancer detection logistic regression kaizen programming cancer deaths machine learning evolutionary computation ml method ec tools statistical method kp approach;logistics;continuous improvement;regression analysis cancer evolutionary computation learning artificial intelligence medical computing patient diagnosis pattern classification;programming;breast cancer	Breast cancer is known as the second largest cause of cancer deaths among women, but thankfully it can be cured if diagnosed early. There have been many investigations on methods to improve the accuracy of the diagnostic, and Machine Learning (ML) and Evolutionary Computation (EC) tools are among the most successfully employed modern methods. On the other hand, Logistic Regression (LR), a traditional and popular statistical method for classification, is not commonly used by computer scientists as those modern methods usually outperform it. Here we show that LR can achieve results that are similar to those of ML and EC methods and can even outperform them when useful knowledge is discovered in the dataset. In this paper, we employ the recently proposed Kaizen Programming (KP) approach with LR to construct high-quality nonlinear combinations of the original features resulting in new sets of features. Experimental analysis indicates that the new sets provide significantly better predictive accuracy than the original ones. When compared to related work from the literature, it is shown that the proposed approach is competitive and a promising method for automatic feature construction.	computer scientist;cryptanalysis of the lorenz cipher;dimensionality reduction;evolutionary computation;feature vector;futures studies;image noise;lr parser;logistic regression;machine learning;missing data;nonlinear system;ordinal data;pixel	Vinicius Veloso de Melo	2016	2016 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2016.7743773	logistics;programming;mathematical optimization;computer science;artificial intelligence;breast cancer;machine learning;data mining;statistics;evolutionary computation	ML	9.39013015849874	-42.94743125910116	191099
c396a0b1cd43bb2b75c692fe39a646379548336f	shrimp: descriptive patterns in a tree		The appliance of the minimum description length (MDL) principle to the field of theory mining enables a precise description of main characteristics of a dataset in comparison to the numerous and hardly understandable output of the popular frequent pattern mining algorithms. The loss function that determines the quality of a pattern selection with respect to the MDL principle is however difficult to analyze and the selection is computed heuristically for all known algorithms. With SHrimp, the attempt to create a data structure that reflects the influences of the pattern selection to the database and that enables a faster computation of the quality of the selection is initiated.	algorithm;code;computation;data mining;data structure;heuristic;human-readable medium;loss function;mdl (programming language);minimum description length;prototype	Sibylle Hess;Nico Piatkowski;Katharina Morik	2014			computer science;artificial intelligence;machine learning;data mining	ML	-0.3228812249148399	-39.35189540477167	191162
d3595c601031d8338721d220d5f476f89bdc1429	an improved mapreduce design of kmeans with iteration reducing for clustering stock exchange very large datasets	databases;riskm;riskapreduce;clustering algorithms stock markets algorithm design and analysis programming big data databases data mining;bigdata;stock markets data mining iterative methods pattern clustering;data mining;stock markets;stock exchange;big data;clustering;stock exchanges daily transaction mapreduce design kmeans algorithm clustering method stock exchange very large datasets data mining iteration reducing method mapreduce programming paradigm;clustering algorithms;apreduce;programming;algorithm design and analysis;riskm apreduce clustering bigdata stock exchange riskapreduce clustering bigdata stock exchange	This paper targets the problem of clustering very large datasets as one of the most challenging tasks for data mining and processing. We propose an improved MapReduce design of Kmeans algorithm with an iteration reducing method. Experiments show that this method reduces the number of iterations and the execution time of the Kmeans algorithm while keeping 80% of the clustering accuracy. The employment of MapReduce programming paradigm and iterations reducing techniques offers the possibility to process the huge volume of data generated by stock exchanges daily transactions which performs a better decision making by analysts.	algorithm;cluster analysis;data mining;experiment;iteration;k-means clustering;mapreduce;programming paradigm;run time (program lifecycle phase)	Oussama Lachiheb;Mohamed Salah Gouider;Lamjed Ben Said	2015	2015 11th International Conference on Semantics, Knowledge and Grids (SKG)	10.1109/SKG.2015.24	big data;computer science;data science;data mining;database;cluster analysis	DB	-3.334730825664558	-38.583667346962564	191518
ad5b0f554f1d8047f206ccacf6b50724906c5e4b	sequential pattern mining for protein function prediction	frequent pattern classifier;frequent pattern;frequent pattern mining;frequent closed pattern;protein sequence;pattern generation;protein function prediction;sequential pattern mining;function prediction	The prediction of protein sequence function is one of the problems arising in the recent progress in bioinformatics. Traditional methods have its limits. We present a novel method of protein sequence function prediction based on sequential pattern mining. First, we use our designed sequential pattern mining algorithms to mine known function sequence dataset. Then, we build a classifier using the patterns generated to predict function of protein sequences. Experiments confirm the effectiveness of our method.	protein function prediction;sequential pattern mining	Miao Wang;Xuequn Shang;Zhanhuai Li	2008		10.1007/978-3-540-88192-6_68	sequential pattern mining;computer science;bioinformatics;protein sequencing;pattern recognition;data mining;protein function prediction	ML	8.035645281660699	-49.42833093504945	191779
ff9d7e47f96018dd7a016a862a75fcbf53071ad6	estimation of missing values in snp array	snp array;microarray;missing values;bioinformatics	DNA microarray usage in genetics is rapidly proliferating, generating huge amount of data. It is estimated that around 5-20% of measurements do not succeed, leading to missing values in the data destined for further analysis. Missing values in further microarray analysis lead to low reliability, therefore there is a need for effective and efficient methods of missing values estimation.#R##N##R##N#This report presents a method for estimating missing values in SNP Microarrays using k-Nearest Neighbors among similar individuals. Usage of preliminary imputation is proposed and discussed. It is shown that introduction of multiple passes of kNN improves quality of missing value estimation.	missing data;snp array	Przemyslaw Podsiadly	2014		10.1007/978-3-319-07467-2_45	snp array;computer science;bioinformatics;microarray;data mining;imputation	HPC	4.350798338757353	-48.04769428681961	191996
62eb44a30e9001f7281d78a6930de9356977b69a	exploring protein fragment assembly using clp	exploring protein fragment assembly;additional knowledge;assembling fragment;novel approach;complete conformation;known protein structure;side chain centroid protein;fragments assembly;protein representation;constraint logic programming;energy model	The paper investigates a novel approach, based on Constraint Logic Programming (CLP), to predict potential 3D conformations of a protein via fragments assembly. The fragments are extracted and clustered by a preprocessor from a database of known protein structures. Assembling fragments into a complete conformation is modeled as a constraint satisfaction problem solved using CLP. The approach makes use of a simplified Cα-side chain centroid protein model, that offers efficiency and a good approximation for space filling. The approach adapts existing energy models for protein representation and applies a large neighboring search (LNS) strategy. The results show the feasibility and efficiency of the method, and the declarative nature of the approach simplifies the introduction of additional knowledge and variations of the model.	approximation;constraint logic programming;constraint satisfaction problem;data structure;declarative programming;global distance test;hoc (programming language);hydrogen;imperative programming;kerrison predictor;local search (optimization);logarithmic number system;preprocessor;programming tool;propagator;solver	Alessandro Dal Palù;Agostino Dovier;Federico Fogolari;Enrico Pontelli	2011		10.5591/978-1-57735-516-8/IJCAI11-431	bioinformatics;theoretical computer science;mathematics;algorithm	AI	-0.9243228537801397	-49.78419429478993	192000
2d37c82d7b8c338a18f356410ab60ecc9deaef7d	finding alternative clusterings using constraints	cluster algorithm;distance function;pattern clustering;distance function data clustering algorithm data mining;probability density function;data mining;clustering algorithms data mining sampling methods monte carlo methods euclidean distance;pattern clustering data mining;indexes;distance measurement;matrix decomposition;clustering;transforms;clustering algorithms;constraints clustering;constraints	The aim of data mining is to find novel and actionable insights. However, most algorithms typically just find a single explanation of the data even though alternatives could exist. In this work, we explore a general purpose approach to find an alternative clustering of the data with the aid of must-link and cannot-link constraints. This problem has received little attention in the literature and since our approach can be incorporated into the many clustering algorithms that use a distance function, compares favorably with existing work.	algorithm;cluster analysis;constrained clustering;data mining;dendrogram;hierarchical clustering;least squares;loss function;optimization problem;simulation;singular value decomposition;transformation matrix;coala	Ian Davidson;Zijie Qi	2008	2008 Eighth IEEE International Conference on Data Mining	10.1109/ICDM.2008.141	constrained clustering;computer science;machine learning;pattern recognition;data mining;mathematics;cluster analysis	DB	-0.16227897007231287	-41.372051800223076	192190
ec54153e2f39e3c930ef939e86508bcadc4d7a69	self-organizing and error driven (soed) artificial neural network for smarter classifications		Abstract Classification tasks are an integral part of science, industry, business, and health care systems; being such a pervasive technique, its smallest improvement is valuable. Artificial Neural Network (ANN) is one of the strongest techniques used in many disciplines for classification. The ANN technique suffers from drawbacks such as intransparency in spite of its high prediction power. In this paper, motivated by learning styles in human brains, ANN’s shortcomings are assuaged and its prediction power is improved. Self-Organizing Map (SOM), an ANN variation which has strong unsupervised power, and Feedforward ANN, traditionally used for classification tasks, are hybridized to solidify their benefits and help remove their limitations. The proposed method, which we name Self-Organizing Error-Driven (SOED) Artificial Neural Network, shows significant improvements in comparison with usual ANNs. We show SOED is a more accurate, more reliable, and more transparent technique through experimentation with five different datasets.	artificial neural network	Ruholla Jafari-Marandi;Mojtaba Khanzadeh;Brian K. Smith;Linkan Bian	2017	J. Computational Design and Engineering	10.1016/j.jcde.2017.04.003	computer science;artificial intelligence;machine learning;data mining	AI	9.605043633479589	-38.31314119718114	192369
028cc28e748c50c55892c6afae2b687e244e3255	ties in proximity and clustering compounds		"""Hierarchical clustering algorithms such as Wards or complete-link are commonly used in compound selection and diversity analysis. Many such applications utilize binary representations of chemical structures, such as MACCS keys or Daylight fingerprints, and dissimilarity measures, such as the Euclidean or the Soergel measure. However, hierarchical clustering algorithms can generate ambiguous results owing to what is known in the cluster analysis literature as the ties in proximity problem, i.e., compounds or clusters of compounds that are equidistant from a compound or cluster in a given collection. Ambiguous ties can occur when clustering only a few hundred compounds, and the larger the number of compounds to be clustered, the greater the chance for significant ambiguity. Namely, as the number of """"ties in proximity"""" increases relative to the total number of proximities, the possibility of ambiguity also increases. To ensure that there are no ambiguous ties, we show by a probabilistic argument that the number of compounds needs to be less than 2(n 1/4), where n is the total number of proximities, and the measure used to generate the proximities creates a uniform distribution without statistically preferred values. The common measures do not produce uniformly distributed proximities, but rather statistically preferred values that tend to increase the number of ties in proximity. Hence, the number of possible proximities and the distribution of statistically preferred values of a similarity measure, given a bit vector representation of a specific length, are directly related to the number of ties in proximities for a given data set. We explore the ties in proximity problem, using a number of chemical collections with varying degrees of diversity, given several common similarity measures and clustering algorithms. Our results are consistent with our probabilistic argument and show that this problem is significant for relatively small compound sets."""	algorithm;bit array;cluster analysis;collections (publication);daylight;fingerprint;hierarchical clustering;large;proximity problems;rough set;similarity measure;ward (environment)	John MacCuish;Christos A. Nicolaou;Norah E. MacCuish	2001	Journal of chemical information and computer sciences	10.1021/ci000069q	combinatorics;machine learning;data mining;mathematics	ML	3.0747710266976305	-49.423514087013395	192532
1a57b0e5d1a360e1a22da5729d8b02c869d18ea3	in-depth motivic analysis based on multiparametric closed pattern and cyclic sequence mining	motivic analysis;pattern discovery	The paper describes a computational system for exhaustive but compact description of repeated motivic patterns in symbolic representations of music. The approach follows a method based on closed heterogeneous pattern mining in multiparametrical space with control of pattern cyclicity. This paper presents a much simpler description and justification of this general strategy, as well as significant simplifications of the model, in particular concerning the management of pattern cyclicity. A new method for automated bundling of patterns belonging to same motivic or thematic classes is also presented. The good performance of the method is shown through the analysis of a piece from the JKUPDD database. Groundtruth motives are detected, while additional relevant information completes the ground-truth musicological analysis. The system, implemented in Matlab, is made publicly available as part of MiningSuite, a new open-source framework for audio and music analysis.	data mining;matlab;open-source software;pattern language	Olivier Lartillot	2014			computer science;machine learning;mathematics;algorithm	ML	1.056424444730191	-47.21092322662194	192659
2172ed57919981989f7fad24f2918153fb450152	a multi-objective genetic programming biomarker detection approach in mass spectrometry data		Mass spectrometry is currently the most commonly used technology in biochemical research for proteomic analysis. The main goal of proteomic profiling using mass spectrometry is the classification of samples from different clinical states. This requires the identification of proteins or peptides (biomarkers) that are expressed differentially between different clinical states. However, due to the high dimensionality of the data and the small number of samples, classification of mass spectrometry data is a challenging task. Therefore, an effective feature manipulation algorithm either through feature selection or construction is needed to enhance the classification performance and at the same time minimise the number of features. Most of the feature manipulation methods for mass spectrometry data treat this problem as a single objective task which focuses on improving the classification performance. This paper presents two new methods for biomarker detection through multi-objective feature selection and feature construction. The results show that the proposed multi-objective feature selection method can obtain better subsets of features than the single-objective algorithm and two traditional multiobjective approaches for feature selection. Moreover, the multi-objective feature construction algorithm further improves the perfomance over the multi-objective feature selection algorithm. The paper is the first multiobjective genetic programming approach for biomarker detection in mass spectrometry data.	bayesian information criterion;crowding;embedded system;feature selection;feature vector;genetic programming;profiling (computer programming);proteomics;selection algorithm	Soha Ahmed;Mengjie Zhang;Lifeng Peng;Bing Xue	2016		10.1007/978-3-319-31204-0_8	biomarker (medicine);artificial intelligence;curse of dimensionality;feature selection;proteomic profiling;genetic programming;mass spectrometry;pattern recognition;small number;computer science	ML	9.104718405699204	-50.01130539497122	192716
934c7281316666edbe502ee11fe75676f637d1e7	reconstructing chromosomal evolution	nadeau taylor model;evolutionary history;chromosome;genome rearrangements;chaine markov;cadena markov;inversions;phylogeny reconstruction;correction distance;genome rearrangement;inversion;transpositions;92b05;rearrangement genome;transposition;modele nadeau taylor;92b10;cromosoma;05e25;distance correction;60j27;neighbor joining;stochastic model;modelo estocastico;modele stochastique;chromosome evolution;65c50;transposicion;markov chain	Chromosomes evolve through genome rearrangement events, including inversions, transpositions, and inverted transpositions, that change the order and strandedness of genes within chromosomes. In this paper we present a method for estimating evolutionary histories for chromosomes based upon such events. The fundamental mathematical challenge of our approach is to estimate the true evolutionary distance between every pair of chromosomes, where the true evolutionary distance is the number of rearrangement events that took place in the evolutionary history between the chromosomes. We present two techniques, Exactand Approx-IEBP, for estimating true evolutionary distances and prove guarantees about the accuracy of these techniques under a very general stochastic model of chromosomal evolution. We then show how we can use these estimated distances to obtain highly accurate estimates of chromosomal evolutionary history, significantly improving upon the previous best techniques.	approximation algorithm;breakpoint;command & conquer:yuri's revenge;dual ec drbg;emoticon;inversion (discrete mathematics);numerical aperture;phylogenetics;simulation;time complexity;ut-vpn	Li-San Wang;Tandy J. Warnow	2006	SIAM J. Comput.	10.1137/S0097539701397229	inversion;mathematics	Theory	1.563272267050928	-50.28187300646408	192817
21f7d2dbc09bca541ea16eb6cc1451b3110f9398	reconstructing ancestral genomic sequences by co-evolution: formal definitions, computational issues, and biological examples	reconstruction of ancestral genomes;maximum likelihood;co evolution;maximum parsimony	The inference of ancestral genomes is a fundamental problem in molecular evolution. Due to the statistical nature of this problem, the most likely or the most parsimonious ancestral genomes usually include considerable error rates. In general, these errors cannot be abolished by utilizing more exhaustive computational approaches, by using longer genomic sequences, or by analyzing more taxa. In recent studies, we showed that co-evolution is an important force that can be used for significantly improving the inference of ancestral genome content. In this work we formally define a computational problem for the inference of ancestral genome content by co-evolution. We show that this problem is NP-hard and hard to approximate and present both a Fixed Parameter Tractable (FPT) algorithm, and heuristic approximation algorithms for solving it. The running time of these algorithms on simulated inputs with hundreds of protein families and hundreds of co-evolutionary relations was fast (up to four minutes) and it achieved an approximation ratio of <1.3. We use our approach to study the ancestral genome content of the Fungi. To this end, we implement our approach on a dataset of 33, 931 protein families and 20, 317 co-evolutionary relations. Our algorithm added and removed hundreds of proteins from the ancestral genomes inferred by maximum likelihood (ML) or maximum parsimony (MP) while slightly affecting the likelihood/parsimony score of the results. A biological analysis revealed various pieces of evidence that support the biological plausibility of the new solutions. In addition, we showed that our approach reconstructs missing values at the leaves of the Fungi evolutionary tree better than ML or MP.	approximation algorithm;biological evolution;computation;computational problem;denture, partial, fixed, resin-bonded;evolution, molecular;genome;heuristic;inference;maximum parsimony (phylogenetics);missing data;np-hardness;occam's razor;parameterized complexity;phylogenetic tree;plausibility structure;protein family;silo (dataset);solutions;time complexity	Tamir Tuller;Hadas Birin;Martin Kupiec;Eytan Ruppin	2010	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2010.0112	biology;mathematical optimization;coevolution;bioinformatics;mathematics;maximum likelihood;maximum parsimony;genetics;statistics	Comp.	1.3721226642419924	-51.79864969726448	192835
532ff7ee3e1eb81db057b7918c2f3c813a2edba1	is the protein model assignment problem under linked branch lengths np-hard?	model selection;maximum likelihood;phylogenomics;np hard;phylogenetics	a r t i c l e i n f o a b s t r a c t In phylogenetics, computing the likelihood that a given tree generated the observed sequence data requires calculating the probability of the available data for a given tree (topology and branch lengths) under a statistical model of sequence evolution. Here, we focus on selecting an appropriate model for the data, which represents a generally non-trivial task. The data is represented as a so-called multiple sequence alignment. That is, each individual sequence of any one species (taxa) is arranged (aligned) in such a way, that the characters of all species at a given position (site) are assumed to share a common evolutionary history. It is well known, that an inappropriate model, which does not fit the data, can generate misleading tree topologies [3,4,26]. More specifically, we consider the case of partitioned protein sequence alignments. This means that the sites of the alignment may be clustered together into different partitions. Each partition may have an individual model of evolution. Our objective is to maximize the likelihood of the per-partition protein model assignments (e.g., JTT, WAG, etc.) when branches are linked across partitions on a given, fixed tree topology. That is, branch lengths are not estimated individually for each partition. Linked branch lengths across partitions substantially reduce the number of free parameters. For p partitions and |M| possible substitution models, there are |M| p possible model assignments. Since the number of combinations grows exponentially with p, an exhaustive search for the highest scoring assignment is computationally prohibitive for |M| > 1. We show that the problem of finding the optimal protein substitution model assignment under linked branch lengths on a given, tree topology, is NP-hard. Our results imply that one should employ heuristics to approximate the solution, instead of striving for the exact solution. Alternatively, the problem can be simplified by relaxing the assumptions.	2-satisfiability;approximation algorithm;assignment problem;boolean satisfiability problem;brute-force search;computation;converge;decision problem;heuristic (computer science);hill climbing;local optimum;maxima and minima;model selection;models of dna evolution;multiple sequence alignment;np-hardness;p (complexity);p versus np problem;phylogenetics;precomputation;statistical model;substitution model;tree (data structure);tree network;phpmyadmin	Kassian Kobert;Jörg Hauser;Alexandros Stamatakis	2014	Theor. Comput. Sci.	10.1016/j.tcs.2013.12.022	mathematical optimization;combinatorics;bioinformatics;np-hard;mathematics;maximum likelihood;phylogenomics;model selection;statistics;phylogenetics	AI	1.1136088723732895	-51.50384615313057	192998
a62a5e02eb223346040a8d4584092f208f5c44a0	semisupervised profiling of gene expressions and clinical data	clinical data;support vector machines;gene expression;statistical learning;functional genomics;liver cancer;prediction accuracy;feature selection;semisupervised classification;support vector machine;dna microarray;dynamic time warping	We present an application of BioDCV, a computational environment for semisupervised profiling with Support Vector Machines, aimed at detecting outliers and deriving informative subtypes of patients with respect to pathological features. First, a sample-tracking curve is extracted for each sample as a by-product of the profiling process. The curves are then clustered according to a distance derived from Dynamic Time Warping. The procedure allows identification of noisy cases, whose removal is shown to improve predictive accuracy and the stability of derived gene profiles. After removal of outliers, the semisupervised process is repeated and subgroups of patients are specified. The procedure is demonstrated through the analysis of a liver cancer dataset of 213 samples described by 1993 genes and by pathological features.	dynamic time warping;information;semi-supervised learning;sensor;support vector machine	Silvano Paoli;Giuseppe Jurman;Davide Albanese;Stefano Merler;Cesare Furlanello	2005		10.1007/11676935_35	support vector machine;computer science;bioinformatics;machine learning;pattern recognition;feature selection	ML	8.2772364051131	-50.465742347444674	193246
03282d4fe45a6efa384e1531c4fc8721e55aed32	k+ means : an enhancement over k-means clustering algorithm		K-means (MacQueen, 1967) [1] is one of the simplest unsupervised learning algorithms that solve the well-known clustering problem. The procedure follows a simple and easy way to classify a given data set to a predefined, say K number of clusters. Determination of K is a difficult job and it is not known that which value of K can partition the objects as per our intuition. To overcome this problem we proposed K+ Means algorithm. This algorithm is an enhancement over K-Means algorithm.	algorithm;cluster analysis;game theory;k-means clustering;machine learning;unsupervised learning;whole earth 'lectronic link	Srikanta Kolay;Kumar Sankar Ray;Abhoy Chand Mondal	2017	CoRR		partition (number theory);cluster analysis;unsupervised learning;cluster (physics);algorithm;k-means clustering;intuition;computer science	ML	3.186567440449163	-41.18746095735177	193424
2591f20edca18b04cc73f6ffff2372c0218305c2	inverted-repeats-aware finite-context models for dna coding	dna context encoding genomics bioinformatics context modeling radiation detectors;genomics bioinformatics dna encoding;model updating scheme inverted repeats aware finite context models dna coding dna sequence compression fall back mechanisms generalized opinion finite context models	Finite-context models have been used for DNA sequence compression as secondary, fall back mechanisms, the generalized opinion being that models with order larger than two or three are inappropriate. In this paper we show that finite-context models can also be used as the main encoding method, and that they are effective for model orders at least as higher as thirteen. Moreover, we propose a new model updating scheme that takes into account inverted repeats, a common characteristic in DNA sequences.	computation;norm (social);requirement;time complexity	Armando J. Pinho;António J. R. Neves;Paulo Jorge S. G. Ferreira	2008	2008 16th European Signal Processing Conference		biology;bioinformatics;theoretical computer science;genetics	Logic	2.4348161871663483	-50.835809922428055	193594
14571995d8c858160ceedac48d2ea581a4958c5c	a fast algorithm for finding correlation clusters in noise data	generalised projected clustering;svd decomposition;280108 database management;clustering method;fast algorithm	Noise significantly affects cluster quality. Conventional clustering methods hardly detect clusters in a data set containing a large amount of noise. Projected clustering sheds light on identifying correlation clusters in such a data set. In order to exclude noise points which are usually scattered in a subspace, data points are projected to form dense areas in the subspace that are regarded as correlation clusters. However, we found that the existing methods for the projected clustering did not work very well with noise data, since they employ randomly generated seeds (micro clusters) to trade-off the clustering quality. In this paper, we propose a divisive method for the projected clustering that does not rely on random seeds. The proposed algorithm is capable of producing higher quality correlation clusters from noise data in a more efficient way than an agglomeration projected algorithm. We experimentally show that our algorithm captures correlation clusters in noise data better than a well-known projected clustering method.	algorithm;cluster analysis;data point;experiment;procedural generation;random seed;scalability;sensor	Jiuyong Li;Xiaodi Huang;Clinton Selke;Jianming Yong	2007		10.1007/978-3-540-71701-0_68	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;subclu;k-medians clustering;fuzzy clustering;flame clustering;theoretical computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering;singular value decomposition;dbscan;affinity propagation;clustering high-dimensional data	ML	-1.6402337948681545	-40.55374207200183	193851
0130abd82d170a63293b3ce1bdf4bfa6be8ef091	adapting k-medians to generate normalized cluster centers.	cluster algorithm;mass spectra;k means;k means algorithm	Many applications of clustering require the use of normalized data, such as text or mass spectra mining. The spherical K-means algorithm [6], an adaptation of the traditional K-means algorithm, is highly useful for data of this kind because it produces normalized cluster centers. The K-medians clustering algorithm is also an important clustering tool because of its wellknown resistance to outliers. K-medians, however, is not trivially adapted to produce normalized cluster centers. We introduce a new algorithm (called MN), inspired by spherical K-means, that integrates with Kmedians clustering to produce locally optimal normalized cluster centers. We then show theoretically and experimentally that MN produces clusters of significantly higher quality than one would obtain via a simple scaling of the cluster centers produced from traditional K-medians.	algorithm;cluster analysis;computer cluster;experiment;image scaling;k-medians clustering;local optimum	Benjamin J. Anderson;Deborah S. Gross;David R. Musicant;Anna M. Ritz;Thomas G. Smith;Leah E. Steinberg	2006		10.1137/1.9781611972764.15	computer science;bioinformatics;machine learning;data mining;k-means clustering	ML	0.5896769543444165	-40.987802306466605	194043
2eca02c94ea79d6c44a3f79661f214ffdd5b316e	ant colony optimization for feature subset selection	conference;meeting	The Ant Colony Optimization (ACO) is a metaheuristic inspired by the behavior of real ants in their search for the shortest paths to food sources. It has recently attracted a lot of attention and has been successfully applied to a number of different optimization problems. Due to the importance of the feature selection problem and the potential of ACO, this paper presents a novel method that utilizes the ACO algorithm to implement a feature subset search procedure. Initial results obtained using the classification of speech segments are very promising. Keywords—Ant Colony Optimization, ant systems, feature selection, pattern recognition.	ant colony optimization algorithms;feature selection;feature vector;genetic algorithm;mathematical optimization;metaheuristic;pattern recognition;selection algorithm;shortest path problem	Ahmed Al-Ani	2005			ant colony optimization algorithms;artificial intelligence;computer science;pattern recognition	AI	10.021965400679571	-43.96612542118449	194116
e4f6fb7065ae4bbb7513d522e77a657d06e913a3	pgac: a parallel genetic algorithm for data clustering	distributed system;parallel genetic algorithm;cluster algorithm;pattern clustering;solution correctness;neural networks;intranets;clustering techniques;intranets genetic algorithms parallel algorithms pattern clustering data analysis;pgac;integrated clustering clustering techniques data analysis parallel processing;solution correctness pgac parallel genetic algorithm data clustering cluster analysis exploratory pattern analysis distributed systems high speed intranet connections island model paradigm demes chromosomes computation time;data clustering;data analysis;integrated clustering;a priori knowledge;biological cells;cluster analysis;chromosomes;parallel;island model paradigm;clustering algorithms;hypercubes;genetic algorithms;pattern analysis;computation time;distributed systems;electronics packaging;demes;genetic algorithms clustering algorithms algorithm design and analysis biological cells data analysis electronics packaging pattern analysis hypercubes neural networks partitioning algorithms;high speed;algorithm design and analysis;parallel processing;exploratory pattern analysis;partitioning algorithms;parallel algorithms;high speed intranet connections	Cluster analysis is a valuable tool for exploratory pattern analysis, especially when very little a priori knowledge about the data is available. Distributed systems, based on high speed intranet connections, provide new tools in order to design new and faster clustering algorithms. Here, a parallel genetic algorithm for clustering called PGAC is described. The used strategy of parallelization is the island model paradigm where different populations of chromosomes (called demes) evolve locally to each processor and from time to time some individuals are moved from one deme to another. Experiments have been performed for testing the benefits of the parallelisation paradigm in terms of computation time and correctness of the solution.	cluster analysis;computation;correctness (computer science);genetic algorithm;intranet;parallel computing;pattern recognition;population;programming paradigm;time complexity	Giosuè Lo Bosco	2005	Seventh International Workshop on Computer Architecture for Machine Perception (CAMP'05)	10.1109/CAMP.2005.41	computer science;theoretical computer science;machine learning;data mining	HPC	4.475937691449672	-38.63985711807263	194196
e2a6d8d4a92f506d3a9b24ccf930264e53d52583	evolutionary fuzzy clustering: an overview and efficiency issues	cluster algorithm;image processing;fuzzy clustering;data analysis;crude oil;number of clusters;web mining;document categorization;evolutionary algorithm;market segmentation;local search	Clustering algorithms have been successf ully applied to several data analysis problems in a wide range of domains, such as image processing, bioinformatics, crude oil analysis, market segmentation, document categoriza tion, and web mining. The need for organizing data into categories of similar objects has made the task of clustering very impor tant to these domains. In this context, there has been an increasingly interest in the study of evolutionary algorithms for clustering, especially those algorithms capable of finding blurred clusters that are not cl early separated from each other. In particular, a number of evolutionary algorithms for fuzzy clustering have been addressed in the lit erature. This chapter has two main contributions. First, it presents an overview of evolutionary algorithms designed for fuzzy clusteri ng. Second, it describes a fuzzy version of an evol utionary algorithm for clustering, which has shown to be more computationally efficien t than systematic (i.e., repetitive) approaches whe n the number of clusters in a data set is unknown. Illustrative experiments showi ng the influence of local optimization on the effic iency of the evolutionary search are also presented. These experiments reveal intere sting aspects of the effect of an important parametr found in many evolutionary algorithms for clustering, namely, the number of iterations of a given local search procedure to be pe rformed at each generation.		Danilo Horta;Murilo Coelho Naldi;Ricardo J. G. B. Campello;Eduardo R. Hruschka;André Carlos Ponce de Leon Ferreira de Carvalho	2009		10.1007/978-3-642-01088-0_8	correlation clustering;constrained clustering;determining the number of clusters in a data set;web mining;data stream clustering;fuzzy clustering;image processing;flame clustering;computer science;local search;canopy clustering algorithm;machine learning;evolutionary algorithm;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;data analysis;market segmentation;affinity propagation;clustering high-dimensional data	AI	3.7608729584095966	-42.21548160246737	194326
b882ef598555ddc81fe599ca8dd9def4179cd901	incorporating gene ontology in clustering gene expression data	biology computing;ontologies gene expression bioinformatics genomics proteins clustering algorithms performance analysis clustering methods public healthcare mathematics;dissimilarity measure;gene cluster;genomic data fusion;data fusion;gene expression data;correlation methods;ontologies artificial intelligence;genetics;protein protein interaction database gene ontology gene expression data clustering dissimilarity measures review genomic data fusion microarray expression dataset correlation based dissimilarity matrix biological process annotations knowledge based validation measure;protein protein interaction database;correlation based dissimilarity matrix;arrays;cluster analysis;proteins;statistical analysis;microarray expression dataset;dissimilarity measures;molecular biophysics;protein protein interaction;knowledge based validation measure;biological data;biological process annotations;review;gene expression data clustering;biochemistry;statistical analysis arrays biochemistry biology computing correlation methods genetics molecular biophysics ontologies artificial intelligence proteins;biological process;knowledge base;gene ontology	In this paper we consider a general framework for clustering expression data that permits integration of various biological data sources through combination of corresponding dissimilarity measures. In the paper we briefly review currently published attempts to genomic data fusion and discuss a problem of validating results from clustering expression data. We apply our approach to a real microarray expression dataset which induces a correlation-based dissimilarity matrix, and use gene ontology - biological process annotations to derive GO-based dissimilarity matrix. The proposed procedure is verified using a simple knowledge-based validation measure based on protein-protein interaction database. Obtained results reveal that combining experimental data with comprehensive and reliable biological repository may improve performance of cluster analysis and yield biologically meaningful gene clusters	cluster analysis;database;distance matrix;gene ontology;interaction;knowledge-based systems;microarray;pixel density;relevance;utility	Rafal Kustra;Adam Zagdanski	2006	19th IEEE Symposium on Computer-Based Medical Systems (CBMS'06)	10.1109/CBMS.2006.100	protein–protein interaction;knowledge base;gene cluster;biological data;computer science;bioinformatics;data science;data mining;sensor fusion;cluster analysis;biological process;molecular biophysics	Comp.	5.35194886473585	-49.725974709211684	194389
eef39364df06eb9933d2fc41a0f13eea17113c58	credit scoring using support vector machines with direct search for parameters selection	credit scoring;support vector machines;search space;search method;direct search method;parameter selection;genetic algorithm;support vector machine;classification accuracy;direct search;parameter optimization;design of experiment	Support vector machines (SVM) is an effective tool for building good credit scoring models. However, the performance of the model depends on its parameters’ setting. In this study, we use direct search method to optimize the SVM-based credit scoring model and compare it with other three parameters optimization methods, such as grid search, method based on design of experiment (DOE) and genetic algorithm (GA). Two real-world credit datasets are selected to demonstrate the effectiveness and feasibility of the method. The results show that the direct search method can find the effective model with high classification accuracy and good robustness and keep less dependency on the initial search space or point setting.	black box;design of experiments;experiment;feature extraction;genetic algorithm;line search;mathematical optimization;risk assessment;support vector machine	Ligang Zhou;Kin Keung Lai;Lean Yu	2009	Soft Comput.	10.1007/s00500-008-0305-0	support vector machine;computer science;machine learning;pattern recognition;data mining	AI	9.83242542860828	-41.445269898650785	194481
b2256bb9efa25b7d02a512e55b36da3303fd23f7	improving tumor clustering based on gene selection	gene expression data;independent component analysis;power method;clustering;non negative matrix factorization;gene selection;independent component	Tumor clustering is becoming a powerful method in cancer class discovery. In this community, non-negative matrix factorization (NMF) has shown its advantages, such as the accuracy and robustness of the representation, over other conventional clustering techniques. Though NMF has shown its efficiency in tumor clustering, there is a considerable room for improvement in clustering accuracy and robustness. In this paper, gene selection and explicitly enforcing sparseness are introduced into clustering process. The independent component analysis (ICA) is employed to select a subset of genes. The unsupervised methods NMF and its extensions, sparse NMF (SNMF) and NMF with sparseness constraint (NMFSC), are then used for tumor clustering on the subset of genes selected by ICA. The experimental results demonstrate the efficiency of the proposed scheme.		Xiangzhen Kong;Qing Yan;Yuqiang Wu;Yutian Wang	2008		10.1007/978-3-540-87442-3_6	gene-centered view of evolution;independent component analysis;correlation clustering;power iteration;computer science;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;biclustering;non-negative matrix factorization;clustering high-dimensional data	Crypto	7.435739390756631	-47.67008653609676	194485
a73d67cd4483c1ae2664b64a3718cecdf3f77c3d	bounds on the value of fuzzy solution to fuzzy programming problem	automatic control;convergence;image segmentation;neural networks;data compression;heterogeneous cluster;computational intelligence;power capacitors;data clustering;number of clusters;clustering algorithms;councils;clustering algorithms power capacitors convergence computational intelligence data security data compression image segmentation neural networks automatic control councils;cooperative learning;data security	The paper is concerned with finding the refined bounds on the value of fuzzy solution to fuzzy programming prob- lem. In this paper we first present the definitions which are the sum of pairs expected value (SPEV), the expected value of the reference scenario (EVRS)and the expectation of pairs expected value (EPEV), and obtain the value of fuzzy solution (VFS) defined by difference between the re- course problem solution and the expected value of reference solution. In addition, several numerical examples are also given in order to explain the definitions specifically. Finally, the properties concerning the concepts are studied, which result in refined bounds on the value of fuzzy solution.	numerical analysis;rp (complexity)	Ming-Fa Zheng;Yian-Kui Liu	2007	2007 International Conference on Computational Intelligence and Security (CIS 2007)	10.1109/CIS.2007.96	correlation clustering;constrained clustering;cooperative learning;data stream clustering;fuzzy clustering;computer science;artificial intelligence;theoretical computer science;machine learning;computational intelligence;automatic control;cure data clustering algorithm;data mining;cluster analysis;artificial neural network	DB	3.314964066440318	-39.70784113874129	194552
ffa56484edbd62f1d77f79b23286c7acd3b606b1	consensus functions for cluster ensembles	cluster algorithm;unsupervised classification;genetic algorithm;hypergraph partitioning;similarity measure	The major task of clustering is to group an heterogeneous population into unknown groups based on a similarity measure. In order to enhance the robustness and the stability of unsupervised classification solutions, clustering ensembles are used; they are considered to be a powerful tool to deal with this issue. Individual clusterers consolidate the process of decision making through the concept of weighting. The aim is to determine an effective combination method that makes use of the benefits of each clusterer while avoiding its weaknesses. In this paper, we study the problem of combining multiple partitioning without accessing the original features. A genetic algorithm is proposed using three different fitness scores. Following three scenarios: Object Distributed Clustering, Feature Distributed Clustering, and Robust Centralized Clustering, the proposed consensus functions algorithm outperforms three existing ones: Cluster-based Similarity Partitioning Algorithm, HyperGraph Partitioning Algorithm and Meta-Clustering Algorithm.		Ghaith Manita;Riadh Khanchel;Mohamed Limam	2012	Applied Artificial Intelligence	10.1080/08839514.2012.687668	correlation clustering;constrained clustering;data stream clustering;genetic algorithm;k-medians clustering;fuzzy clustering;computer science;artificial intelligence;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;k-medoids	AI	3.495586644802517	-42.04845925790127	194556
84412a5430150fc283dd6722705bff360ec518cc	a new kernel based on high-scored pairs of tri-peptides and its application in prediction of protein subcellular localization	blosum matrix;kernel;gram negative bacteria;analisis estadistico;proteine;validacion cruzada;sequence similarity;bacterie;protein sequence;localization;localizacion;similitude;localisation;statistical analysis;machine exemple support;analyse statistique;validation croisee;similarity;protein subcellular localization;subcellular localization;proteina;bacteria;cross validation;support vector machine;similitud;maquina ejemplo soporte;vector support machine;protein	A new kernel has been developed for vectors derived from a coding scheme of the tri-peptide composition for protein sequences. This kernel defines the sequence similarity through a mapping that transforms a tri-peptide coding vector into a new vector based on a matrix formed by the high BLOSUM scores associated with pairs of tri-peptides. In conjunction with the use of support vector machines, the effectiveness of the new kernel is evaluated against the conventional coding schemes of k-peptide (k ≤ 3) for the prediction of subcellular localizations of proteins in Gram-negative bacteria. It is demonstrated that the new method outperforms all the other methods in a 5-fold cross-validation.	blosum;cross-validation (statistics);experiment;kernel (operating system);psort;peptide sequence;sequence alignment;support vector machine;triangular function	Zhengdeng Lei;Yang Dai	2005		10.1007/11428848_115	support vector machine;kernel method;blosum;kernel;string kernel;similarity;internationalization and localization;radial basis function kernel;bacteria;computer science;bioinformatics;similitude;machine learning;protein sequencing;pattern recognition;cross-validation;statistics	ML	6.3138102297879986	-46.66526967564703	194595
b312e4b91023768d56cb701df3a59a2d19a28fa3	a robust clustering algorithm for interval data	pattern clustering;clustering algorithm;ocean temperature;robustness clustering algorithm interval data;cluster initials;cities temperature interval data;interval data;cluster initials robust clustering algorithm interval data cities temperature interval data cluster number;clustering algorithms ocean temperature cities and towns clustering methods robustness correlation algorithm design and analysis;clustering algorithms;cities and towns;robustness;correlation;clustering methods;cluster number;robust clustering algorithm;algorithm design and analysis	In this paper we propose a robust clustering algorithm for interval data. The proposed method is based on similarity measure that is not necessary to specify a cluster number and initials. Several numerical examples demonstrate the effectiveness of the proposed robust clustering algorithm. We then apply this algorithm to the real data set with cities temperature interval data. The proposed clustering algorithm actually presents its robustness.	algorithm;cluster analysis;numerical analysis;similarity measure	Miin-Shen Yang;Hsien-Chun Kuo;Wen-Liang Hung	2012	2012 IEEE International Conference on Fuzzy Systems	10.1109/FUZZ-IEEE.2012.6251364	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;mathematics;fsa-red algorithm;cluster analysis;linde–buzo–gray algorithm;k-medoids;dbscan;affinity propagation;statistics;clustering high-dimensional data	Robotics	2.52137978066902	-39.774791597533316	194681
44ce2d7a12c469df9226f6cecbf9d54e1ec4b2d2	an effective and efficient hierarchical k-means clustering algorithm	data mining;clustering;k-means;top-n merging;cluster pruning	K-means plays an important role in different fields of data mining. However, k-means often becomes sensitive due to its random seeds selecting. Motivated by this, this article proposes an optimized k-means clustering method, named k*-means, along with three optimization principles. First, we propose a hierarchical optimization principle initialized by k* seeds (k*>k) to reduce the risk of random seeds selecting, and then use the proposed “top-n nearest clusters merging” to merge the nearest clusters in each round until the number of clusters reaches at k. Second, we propose an “optimized update principle” that leverages moved points updating incrementally instead of recalculating mean and SSE of cluster in k-means iteration to minimize computation cost. Third, we propose a strategy named “cluster pruning strategy” to improve efficiency of k-means. This strategy omits the farther clusters to shrink the adjustable space in each iteration. Experiments performed on real UCI and synthetic datasets verify the efficiency and effectiveness of our proposed algorithm.	algorithm;cluster analysis;k-means clustering	Jianpeng Qi;Yanwei Yu;Lihong Wang;Jinglei Liu;Yingjie Wang	2017	IJDSN	10.1177/1550147717728627	flame clustering;correlation clustering;hierarchical clustering of networks;distributed computing;subclu;k-means clustering;computer science;canopy clustering algorithm;artificial intelligence;cure data clustering algorithm;brown clustering;pattern recognition	ML	3.8930779731069234	-41.27846337750167	194777
c9e8a92c2404632561d014a6c7c08ce0d1f318ab	model instability in microarray gene expression class prediction studies	gene expression microarrays;prediction error;class prediction;gene expression;cross validation;expected prediction error	This work is devoted to the problem of building a sample classifier based on data from microarray gene expression experiments. Two specific issues related to this are tackled in this paper: (a) selection of parameters of a classification model to ensure best generalization power, and (b) variability of expected prediction error (EPE) for new data as a function of the model parameters. A method is presented for selection of model parameters minimizing the EPE in studies where the number of samples (n) is much smaller then the number of attributes (d). Due to very unstable behaviour of the EPE in the space of model parameters, it seems essential that microarray studies involve systematic search for the right model parameters, as shown in this work.		Henryk Maciejewski;Piotr Twaróg	2009		10.1007/978-3-642-04772-5_96	gene expression;computer science;bioinformatics;mean squared prediction error;data mining;cross-validation;statistics	Comp.	9.856906954157347	-49.039119426657464	194798
328d1b0f30b3906cabdddff8beccaab235bddd63	efficient algorithms for biological stems search	dna;sequence analysis dna;computational biology bioinformatics;rna;nucleotide motifs;algorithms;combinatorial libraries;computer appl in life sciences;amino acid motifs;sequence analysis protein;microarrays;bioinformatics	Motifs are significant patterns in DNA, RNA, and protein sequences, which play an important role in biological processes and functions, like identification of open reading frames, RNA transcription, protein binding, etc. Several versions of the motif search problem have been studied in the literature. One such version is called the Planted Motif Search (PMS)or (l, d)-motif Search. PMS is known to be NP complete. The time complexities of most of the planted motif search algorithms depend exponentially on the alphabet size. Recently a new version of the motif search problem has been introduced by Kuksa and Pavlovic. We call this version as the Motif Stems Search (MSS) problem. A motif stem is an l-mer (for some relevant value of l)with some wildcard characters and hence corresponds to a set of l-mers (without wildcards), some of which are (l, d)-motifs. Kuksa and Pavlovic have presented an efficient algorithm to find motif stems for inputs from large alphabets. Ideally, the number of stems output should be as small as possible since the stems form a superset of the motifs. In this paper we propose an efficient algorithm for MSS and evaluate it on both synthetic and real data. This evaluation reveals that our algorithm is much faster than Kuksa and Pavlovic’s algorithm. Our MSS algorithm outperforms the algorithm of Kuksa and Pavlovic in terms of the run time as well as the number of stems output. Specifically, the stems output by our algorithm form a proper (and much smaller)subset of the stems output by Kuksa and Pavlovic’s algorithm.	alphabet;amino acid sequence;dhrystone;dijkstra's algorithm;frame (physical object);mer;np-completeness;open reading frames;open reading frame;peptide sequence;personality character;planted motif search;rna;reading frames (nucleotide sequence);run time (program lifecycle phase);search algorithm;search problem;sequence motif;transcription (software);version;wildcard character	Tian Mi;Sanguthevar Rajasekaran	2012		10.1186/1471-2105-14-161	biology;rna;dna microarray;bioinformatics;planted motif search;multiple em for motif elicitation;genetics;dna;eukaryotic linear motif resource;sequence motif	AI	-0.5661502418579232	-51.88149164248199	195034
003ab7864ff76181b0770688ec18c9bbdbf0a5d7	iterative bicluster-based least square framework for estimation of missing values in microarray gene expression data	biclustering;gene expression analysis;missing value imputation;iterative estimation;journal magazine article	DNA microarray experiment inevitably generates gene expression data with missing values. An important and necessary pre-processing step is thus to impute these missing values. Existing imputation methods exploit gene correlation among all experimental conditions for estimating the missing values. However, related genes coexpress in subsets of experimental conditions only. In this paper, we propose to use biclusters, which contain similar genes under subset of conditions for characterizing the gene similarity and then estimating the missing values. To further improve the accuracy in missing value estimation, an iterative framework is developed with a stopping criterion on minimizing uncertainty. Extensive experiments have been conducted on artificial datasets, real microarray datasets as well as one non-microarray dataset. Our proposed biclusters-based approach is able to reduce errors in missing value estimation. & 2011 Elsevier Ltd. All rights reserved.	biclustering;dna microarray;experiment;geo-imputation;iterative method;missing data;preprocessor	Kin-On Cheng;Ngai-Fong Law;Wan-Chi Siu	2012	Pattern Recognition	10.1016/j.patcog.2011.10.012	gene expression;computer science;bioinformatics;data mining;imputation;biclustering;statistics	Vision	5.417593988075374	-47.02337212364	195043
bfbca9da545fc771bedf29baa8a3810b2a034c8f	spectral clustering based on learning similarity matrix		Motivation Single-cell RNA-sequencing (scRNA-seq) technology can generate genome-wide expression data at the single-cell levels. One important objective in scRNA-seq analysis is to cluster cells where each cluster consists of cells belonging to the same cell type based on gene expression patterns.   Results We introduce a novel spectral clustering framework that imposes sparse structures on a target matrix. Specifically, we utilize multiple doubly stochastic similarity matrices to learn a similarity matrix, motivated by the observation that each similarity matrix can be a different informative representation of the data. We impose a sparse structure on the target matrix followed by shrinking pairwise differences of the rows in the target matrix, motivated by the fact that the target matrix should have these structures in the ideal case. We solve the proposed non-convex problem iteratively using the ADMM algorithm and show the convergence of the algorithm. We evaluate the performance of the proposed clustering method on various simulated as well as real scRNA-seq data, and show that it can identify clusters accurately and robustly.   Availability and implementation The algorithm is implemented in MATLAB. The source code can be downloaded at https://github.com/ishspsy/project/tree/master/MPSSC.   Supplementary information Supplementary data are available at Bioinformatics online.		Seyoung Park;Hongyu Zhao	2018	Bioinformatics	10.1093/bioinformatics/bty050	spectral clustering;computer science;bioinformatics;pattern recognition;similarity matrix;artificial intelligence	Comp.	6.393263950717118	-50.68468128257682	195114
c03bf295de37aec2b1110633634a11afa5acae08	clustering massive small data for iot	corporate acquisitions;merger strategy massive small data clustering iot internet of things cloud computing hdfs mapreduce data set analysis cluster strategy k means clustering algorithm data processing efficiency system resource utilization;internet of things;resource allocation cloud computing data analysis internet of things parallel processing pattern clustering;k means iot cloud computing mapreduce clustering;clustering algorithms corporate acquisitions algorithm design and analysis cloud computing internet of things file systems educational institutions;clustering algorithms;algorithm design and analysis;file systems;cloud computing	Data of IOT (Internet of things) have characteristics of heterogeneity, massive, timeliness and other features, which indicate that much of its data is in the form of small files. Cloud computing is used to deal with large data sets, but a large number of small data sets in the system will occupy most of the resources, resulting in a waste of system resources. In this paper, according to the characteristics of the mass of small data sets, and the deficiency of HDFS handle huge amounts of small data sets. This paper uses MapReduce to analysis the numerous small data sets and proposes a cluster strategy for massive small data based on the k-means clustering algorithm. The experimental results show that the proposed strategy can improve the data processing efficiency, and can improve the utilization of system resources. The research fruits will help us to design more practical merger strategy of massive small data to provide research reference.	algorithm;angular defect;apache hadoop;cloud computing;cluster analysis;internet of things;k-means clustering;mapreduce	Xin Tao;Chunlei Ji	2014	The 2014 2nd International Conference on Systems and Informatics (ICSAI 2014)	10.1109/ICSAI.2014.7009427	algorithm design;data stream clustering;cloud computing;computer science;operating system;machine learning;data mining;database;cluster analysis;world wide web;internet of things	HPC	-3.0326691832399755	-38.41754876884799	195178
33efe4a68754051850ac7fc48902a5b238f7673e	codet: an easy-to-use community detection tool		Network data plays an important role in biological research. For example, the interaction between proteins in living cells forms large complex networks. The corporation of cells in a living body also makes up networks. As an important approach to analysing the topology of network data, community detection methods have attracted a great interest of researchers, and different algorithms have been developed during the past decade. However, the diversity of these algorithms also makes users confused to choose a suitable one according to the specific application. In this paper, we present CoDeT, a system which integrates 11 state-of-the-art community detection algorithms and 12 recognised metrics, to address the difficulty. Especially, CoDeT is capable to recommend the most suitable algorithm for users when they consider multiple algorithms for a given data set. Experimental results show that the recommended algorithms by our system are effective on bioinformatic networks. In addition, with our provided C++, P...		Yifei Yue;Changping Wang;Xiang Ying;Jun Qian	2017	IJDMB	10.1504/IJDMB.2017.10009478	machine learning;artificial intelligence;computer science;complex network	Logic	3.5143082045797387	-46.00162452051142	195313
52468aa214b8e2d2fb8b26ca8def6da449e7210c	bayesian-based parallel ant system for missing value estimation in large databases		Nowadays most of the real databases are often big and need preprocessing before making any analysis from them. Data analytics will be valid only when the databases are complete. But, values of attributes in the databases may be missing due to several reasons and most of the methods developed were intended to deal with missing values in homogeneous attributes which may be either discrete or continuous. Only few literatures have dealt with missing values in heterogeneous attributes. Ant colony optimisation (ACO) is an evolutionary algorithm which works based on behaviour of ants commonly used to solve combinatorial problems. For estimating missing heterogeneous (discrete and continuous) attribute values, the paper introduces a new algorithm called Bayesian based Parallel Max-Min Ant System (BPMMAS) by hybridising Bayesian principles with Max-Min Ant System (MMAS), an enhanced version of ACO. The algorithm is executed in parallel with subsets of the original dataset and the imputation results are combined. M...	ant colony optimization algorithms;database;missing data	R. Sivaraj;R. Devi Priya	2017	IJBIC	10.1504/IJBIC.2017.10004017	data science;pattern recognition;data mining	DB	-1.2291227335515822	-39.32150932339452	195332
a8e05c6c5c78b1ba987d0fec446b42a8c9db25e4	spatial data mining of a population-based data warehouse of cancer in mexico	cancer;learning technologies;data mining;spatial data mining;pattern recognition;bioinformatics	In the health sciences area, data mining applications have had a fast growth due to its results concerning the generation of patterns of interest; however, its application to spatial population-based databases has been scant. This paper shows the results obtained by applying a spatial data mining system of our making to a real population-based data warehouse of cancer mortality in Mexico. The system consists of a pattern generator module, which uses a variant of a clustering algorithm proposed by us, and a spatial visualization module. Several interesting and potentially useful patterns of stomach cancer were found in the northwest of Mexico, which show promising results for extending the use of data mining in the area of epidemiology.	algorithm;cluster analysis;data mining;database	Joaquín Pérez Ortega;Fátima Miranda;Gerardo Reyes Salgado;René Santaolaya Salgado;Rodolfo A. Pazos Rangel;Adriana Mexicano	2010	IJCOPI		bioinformatics;engineering;data science;data mining;data stream mining	ML	4.703492658183988	-46.88065289779593	195546
0788f8a7feaa84aac29100e668b29ad19ab57e16	cure: an efficient clustering algorithm for large databases	cluster algorithm;random sampling;conference;data mining;knowledge discovery	Clustering, in data mining, is useful for discovering groups and identifying interesting distributions in the underlying data. Traditional clustering algorithms either favor clusters with spherical shapes and similar sizes, or are very fragile in the presence of outliers. We propose a new clustering algorithm called CURE that is more robust to outliers, and identifies clusters having non-spherical shapes and wide variances in size. CURE achieves this by representing each cluster by a certain fixed number of points that are generated by selecting well scattered points from the cluster and then shrinking them toward the center of the cluster by a specified fraction. Having more than one representative point per cluster allows CURE to adjust well to the geometry of non-spherical shapes and the shrinking helps to dampen the effects of outliers. To handle large databases, CURE employs a combination of random sampling and partitioning. A random sample drawn from the data set is first partitioned and each partition is partially clustered. The partial clusters are then clustered in a second pass to yield the desired clusters. Our experimental results confirm that the quality of clusters produced by CURE is much better than those found by existing algorithms. Furthermore, they demonstrate that random sampling and partitioning enable CURE to not only outperform existing algorithms but also to scale well for large databases without sacrificing clustering quality.	algorithm;cluster analysis;data mining;database;monte carlo method;sampling (signal processing)	Sudipto Guha;Rajeev Rastogi;Kyuseok Shim	1998		10.1145/276304.276312	complete-linkage clustering;correlation clustering;sampling;data stream clustering;k-medians clustering;computer science;bioinformatics;cure data clustering algorithm;data mining;knowledge extraction;cluster analysis;single-linkage clustering	DB	-0.4960416375867503	-41.73483068124984	195619
3c3904086e3fbc5e08abb9ca14d320b81ed40f4c	using prior models as a measure of novelty in knowledge discovery	bioinformatics;biomedical research	Introduction Operational Definition of Novelty One ofthe challenges ofknowledge discovery is Several heuristics were used to determine if a rule trying to identify patterns that are interesting from an was novel with respect to the prior model. Novel abundance ofuninteresting patterns. Valdes-Perez rules either contain only features not included in the provides a working definition ofwhat it means for a prior model or use features from the prior model to pattern to be interesting: make a prediction that would not be expected based ... results are interesting in science Xif on the variableu0027s use in the prior model. Novel practitioners ofX (or users ofthe programs) classification rules could also contain some elements believe them to be interesting. used by the prior model and some elements not used In order to discover pattems that would be by the prior model. In this case, the new rule must thought of as interesting according to this definition, correctly classify test items with greater certainty most knowledge discovery problems require a than the same rule with the unused variables subjective definition of interestingness. In this way removed. A final heuristic was the complex the user of the program, or a domain expert, provides combination of individual features of the prior model. the standard by which interestingness is measured. Methods Lenat, in his program AM and Livingston, in his We randomly selected 600 cases from the PORT discovery program HAMB, measure interestingness cohort study data used in validating the PSI, with using a collection ofheuristics. Their heuristics are 70% ofthese devoted to training and 30% to testing. based on domain-independent intuitions about when A rule-learning program (RL) was modified to use a conjecture in mathematics or science meet this definition ofnovelty, combined with a prior operational definitions ofnovelty, empirical-support, model, to search for novel rules. simplicity, and utility. This maps neatly onto other Results definitions of interestingness provided in the RL produced a set of 16 rules, seven ofwhich literature, such as novel, interesting, plausible, and were classified as novel-by the program. Ofthese intelligible  and validity, novelty, usefulness, and novel rules, four ofthem contained features known to simplicity . In all ofthese definitions, novelty be prodictive ofpneumonia severity, but not included (sometimes called surprise) plays an important in the prior model. Three rules contained a single role. feature that was not included in the prior model and In some domains, prior models have been was not known to be predictive ofpneumonia developed that can provide an abundant source of outcome. background knowledge. Whether prior beliefs are Discussion codified in a well-supported, published model or These results demonstrate the feasibility ofusing a exist as informal, common knowledge, a discovery prior model to select novel rules from a set of rules. program that has access to these beliefs can better Our method selected novel rules using variables judge when new conjectures are novel. obviously equivalent to those in the PSI model, The goal of this paper is to develop a domain variables known to be predictive of mortality but not dependent definition of novelty based on the included in the PSI model, and one variable that background knowledge contained in a prior model, appears to be part of a truly novel relationship with and to test this definition on pneumonia outcome data the target class (i.e., not previously thought to be using a published pneumonia severity index (PSI) as associated with mortality). This is exactly what we the prior model. To achieve this, we will embed this wanted to find interesting pattems that cannot be definition ofnovelty within a knowledge discovery summarily validated or invalidated, but instead are program, search for classifiers using pneumonia data, starting points for future investigation. and analyze the results for novelty.		Jeremy Ludwig;Michael J. Fine;Gary Livingston;Emmanouil Vozalis;Bruce G. Buchanan	2000			novelty;knowledge extraction;subject-matter expert;data mining;heuristic;heuristics;artificial intelligence;common knowledge;operational definition;text mining;computer science;pattern recognition	ML	6.917350978954339	-43.89931825894487	196067
0b351a1a2048086430869c16514582ffe047b01f	high confidence rule mining for microarray analysis	association rule;microarray data;microarray data analysis;association rules;bioinformatics;genomics;microarray analysis;genetics;tree data structures;data analysis;search space;data mining;association rule mining;sparse data;algorithm design and analysis	We present an association rule mining method for mining high confidence rules, which describe interesting gene relationships from microarray datasets. Microarray datasets typically contain an order of magnitude more genes than experiments, rendering many data mining methods impractical as they are optimised for sparse datasets. A new family of row-enumeration rule mining algorithms have emerged to facilitate mining in dense datasets. These algorithms rely on pruning infrequent relationships to reduce the search space by using the support measure. This major shortcoming results in the pruning of many potentially interesting rules with low support but high confidence. We propose a new row-enumeration rule mining method, MAXCONF, to mine high confidence rules from microarray data. MAXCONF is a support-free algorithm which directly uses the confidence measure to effectively prune the search space. Experiments on three microarray datasets show that MAXCONF outperforms support-based rule mining with respect to scalability and rule extraction. Furthermore, detailed biological analyses demonstrate the effectiveness of our approach – the rules discovered by MAXCONF are substantially more interesting and meaningful compared with support-based methods.	algorithm;association rule learning;data mining;experiment;microarray;rule induction;scalability;sparse matrix	Tara McIntosh;Sanjay Chawla	2007	IEEE/ACM Trans. Comput. Biology Bioinform.	10.1145/1322075.1322087	microarray analysis techniques;genomics;association rule learning;computer science;bioinformatics;data science;data mining	ML	4.938854907900729	-49.06929315348372	196325
9d4726336e13de31000f62f9999e767083b07c57	hypertension prediction by multi-objective optimization methods	hypertension;pareto optimization;signal processing conferences hypertension bioinformatics entropy pareto optimization;signal processing;entropy;conferences;bioinformatics	Feature selection is the important part of microarray analysis and it aims finding most representative subset of the bio-markers. But selection process is a challenging task due to the high dimensional nature of gene expression data. This should also be independent of sample variations in the dataset. In this paper we present a novel hybrid method that incorporates a multi-objective optimization method, called Pareto Optimal approach (PO) with Analytical Hierarchy Process (AHP). Firstly, PO was used to selects relevant subsets of the attributes, but it does not give any information about priorities of the selected bio-markers. In order to prevent this problem, AHP is incorporated with PO. AHP prioritize the selected genes by PO. This is further supported with different biomarker selection methods. The proposed method was tested on hypertension prediction.	analytical hierarchy;british informatics olympiad;feature selection;mathematical optimization;microarray;multi-objective optimization;pareto efficiency;protein structure prediction	Zeliha Gormez;Huseyin Seker;Ahmet Sertbas	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830371	entropy;mathematical optimization;computer science;bioinformatics;machine learning;signal processing;data mining	AI	8.527608025856093	-47.35273168665542	196472
d85f66bc080086bf71b1b3d55df0cbaf445c019a	cluster-based knn missing value imputation for dna microarray data	microarray data;biology computing;pattern clustering;cancer;clustering algorithms algorithm design and analysis gene expression partitioning algorithms cancer euclidean distance humans;euclidean distance;matrix algebra;gene expression;pattern clustering biology computing data analysis matrix algebra molecular biophysics;data analysis;clustering;molecular biophysics;clustering technique cluster based knn missing value imputation k nearest neighbor dna microarray data gene expression down stream analysis medical application data analysis method cknn impute algorithm matrix representation;missing value;clustering algorithms;humans;clustering missing value imputation microarray data;algorithm design and analysis;imputation;partitioning algorithms	Gene expressions measured using microarrays usually encounter the problem of missing values. Leaving this unsolved may critically degrade the reliability of any consequent down-stream analysis or medical application. Yet, a further study of microarray data might be impossible with many analysis methods requiring a complete data set. This paper introduces a new methodology to impute missing values in microarray data. The proposed algorithm, CKNN impute, is an extension of k nearest neighbor imputation with local data clustering being incorporated for improved quality and efficiency. Gene expression data is typically represented as a matrix whose rows and columns correspond to genes and experiments, respectively. CKNN kicks off by finding a complete dataset via the removal of rows with missing value(s). Then, k clusters and their corresponding centroids are obtained by applying a clustering technique on the complete dataset. A set of similar genes of the target gene (with missing values) are those belonging to the cluster, whose centroid is the closest the target. Having known this, the target gene is imputed by applying k nearest neighbor method with similar genes previously determined. Empirical evaluation with published gene expression datasets suggest that the proposed technique performs better than the classical k nearest neighbor method and its extension found in the literature.	cluster analysis;column (database);dna microarray;experiment;gene co-expression network;gene expression programming;geo-imputation;k-nearest neighbors algorithm;missing data;nearest neighbor search	Phimmarin Keerin;Werasak Kurutach;Tossapon Boongoen	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6377764	computer science;bioinformatics;machine learning;pattern recognition;data mining;cluster analysis;imputation;statistics;molecular biophysics	Visualization	4.757773112857306	-47.853521133688886	197047
2b8cae702e7de2b429336d1d8cae7b5834ab502e	fast construction of fm-index for long sequence reads		SUMMARY We present a new method to incrementally construct the FM-index for both short and long sequence reads, up to the size of a genome. It is the first algorithm that can build the index while implicitly sorting the sequences in the reverse (complement) lexicographical order without a separate sorting step. The implementation is among the fastest for indexing short reads and the only one that practically works for reads of averaged kilobases in length.   AVAILABILITY AND IMPLEMENTATION https://github.com/lh3/ropebwt2 CONTACT: hengli@broadinstitute.org.	algorithm;complement system proteins;fm-index;fastest;indexes;lexicographical order;lexicography;reading (activity);sorting	Heng Li	2014	Bioinformatics	10.1093/bioinformatics/btu541	computer science;bioinformatics;algorithm	Theory	-1.2713064740264746	-51.981493653983016	197313
afcf344535256abb5d20b37f6403e227ad8c0d6a	data mining for genetics: a genetic algorithm approach	relational data;image segmentation;data mining;geometric feature;genetics;research and development;genetic algorithm;biological data;environmental factor;biological process	MINING biological data is an emerging area of intersection between data mining and bioinformatics. Bio-informaticians have been working on the research and development of computational methodologies and tools for expanding the use of biological, medical, behavioral, or health-related data. Biological data mining aims to extract significant information from DNA, RNA and proteins. Many biological processes are not well-understood Biological knowledge is highly complex. In this paper we discussed in discovering genetic features and environmental factors that are involved in multifactorial diseases such as (obesity, diabetes). To exploit this data, data mining tools are required and we using a specific genetic algorithm.	bioinformatics;data mining;genetic algorithm	G. Madhu;Keshava Reddy	2008	JCIT		computer vision;genetic algorithm;biological data;relational database;computer science;bioinformatics;segmentation-based object categorization;data mining;region growing;image segmentation;scale-space segmentation;biological process	ML	4.92750261287844	-48.133816265571184	197662
229461d288d8fb26ccca54e08977c73d58971634	clustering using annealing evolution: application to pixel classification of satellite images	evolutionary programming;simulated annealing;k means algorithm	In this article an efficient clustering technique, that utilizes an effective integration of simulated annealing and evolutionary programming as the underlying search tool, is developed. During the evolution process, data points are redistributed among the clusters probabilistically so that points that are farther away from the cluster center have higher probabilities of migrating to other clusters than those which are closer to it. The superiority of the new clustering algorithm over the widely used K-means algorithm and those based on simulated annealing and conventional evolutionary programming is demonstrated for some real life data sets. Another real life application of the developed clustering technique in classifying the pixels of a satellite image of a part of the city of Mumbai is also provided.	cluster analysis;data point;evolutionary programming;expectation propagation;genetic algorithm;histogram equalization;k-means clustering;pixel;real life;simulated annealing;soft computing	Ujjwal Maulik;Sanghamitra Bandyopadhyay;Malay Kumar Pakhira	2002			computer science;evolutionary programming;computer vision;pattern recognition;pixel;cluster analysis;simulated annealing;data point;data set;machine learning;artificial intelligence;k-means clustering	ML	3.7692860997318296	-41.77804206460653	197726
ce53771489f0701f2753a2bb49c861c5f73bc461	jackknife and bootstrap tests of the composition vector trees	bootstrap;phylogeny;composition vector;cvtree;self consistent;phylogenetic tree;bootstrap test;guanghong zuo zhao xu hongjie yu bailin hao bootstrap 矢量 组分 检验 树木分类 系统发育树 亲缘关系 全基因 jackknife and bootstrap tests of the composition vector trees;topological distance;jackknife	Composition vector trees (CVTrees) are inferred from whole-genome data by an alignment-free and parameter-free method. The agreement of these trees with the corresponding taxonomy provides an objective justification of the inferred phylogeny In this work, we show the stability and self-consistency of CVTrees by performing bootstrap and jackknife re-sampling tests adapted to this alignment-free approach. Our ultimate goal is to advocate the viewpoint that time-consuming statistical re-sampling tests can be avoided at all in using this alignment-free approach. Agreement with taxonomy should be taken as a major criterion to estimate prokaryotic phylogenetic trees.	alignment;booting;inference;jackknife resampling;phylogenetic tree;population parameter;published comment;sampling (signal processing);sampling - surgical action;taxonomy;trees (plant)	Guanghong Zuo;Zhao Xu;Hongjie Yu;Bailin Hao	2010		10.1016/S1672-0229(10)60028-9	biology;phylogenetic tree;data mining;jackknife resampling;phylogenetics	ML	4.528904971736144	-51.60943610981146	197743
2c5ced639d5921ba8d916d6a75cf99665050a6f0	partition-conditional ica for bayesian classification of microarray data	microarray data;conditional independence;bayesian classification;naive bayes;medical decision making;independent component analysis;microarray data analysis;feature extraction;component analysis;mutual information;bayes classifier	Accurate classification of microarray data is very important for medical decision making. Past studies have shown that class-conditional independent component analysis (CC-ICA) is capable of improving the performance of naive Bayes classifier in microarray data analysis. However, when a microarray dataset has a small number of samples for some classes, the application of CC-ICA may become infeasible. This paper extends CC-ICA and proposes a partition-conditional independent component analysis (PC-ICA) method for naive Bayes classification of microarray data. Compared to ICA and CC-ICA, PC-ICA represents an in-between concept for feature extraction. Our experimental results on two microarray datasets show that PC-ICA is more effective than ICA in improving the performance of naive Bayes classification of microarray data.	independent computing architecture;microarray;naive bayes classifier	Liwei Fan;Kim-Leng Poh;Peng Zhou	2010	Expert Syst. Appl.	10.1016/j.eswa.2010.05.068	microarray analysis techniques;bayes classifier;naive bayes classifier;computer science;machine learning;pattern recognition;data mining	ML	7.9993346868988215	-46.189926477634955	198288
aee64cb05f00c3580bd1c6fd98d64923307049b9	consequences of common topological rearrangements for partition trees in phylogenomic inference	software;tree bisection and reconnection;genomics;phylogeny;phylogenetic terraces;subtree pruning and regrafting;decision trees;partial terraces;nearest neighbor interchange	"""In phylogenomic analysis the collection of trees with identical score (maximum likelihood or parsimony score) may hamper tree search algorithms. Such collections are coined phylogenetic terraces. For sparse supermatrices with a lot of missing data, the number of terraces and the number of trees on the terraces can be very large. If terraces are not taken into account, a lot of computation time might be unnecessarily spent to evaluate many trees that in fact have identical score. To save computation time during the tree search, it is worthwhile to quickly identify such cases. The score of a species tree is the sum of scores for all the so-called induced partition trees. Therefore, if the topological rearrangement applied to a species tree does not change the induced partition trees, the score of these partition trees is unchanged. Here, we provide the conditions under which the three most widely used topological rearrangements (nearest neighbor interchange, subtree pruning and regrafting, and tree bisection and reconnection) change the topologies of induced partition trees. During the tree search, these conditions allow us to quickly identify whether we can save computation time on the evaluation of newly encountered trees. We also introduce the concept of partial terraces and demonstrate that they occur more frequently than the original """"full"""" terrace. Hence, partial terrace is the more important factor of timesaving compared to full terrace. Therefore, taking into account the above conditions and the partial terrace concept will help to speed up the tree search in phylogenomic inference."""	alpha–beta pruning;collections (publication);computation;dna sequence rearrangement;inference;maximum parsimony (phylogenetics);missing data;occam's razor;phylogenetic tree;phylogenetics;search algorithm;single linkage cluster analysis;sparse matrix;time complexity;tree (data structure);tree rearrangement;trees (plant)	Olga Chernomor;Bui Quang Minh;Arndt von Haeseler	2015		10.1089/cmb.2015.0146	biology;genomics;combinatorics;discrete mathematics;bioinformatics;decision tree;tree rearrangement;k-d tree;mathematics;weight-balanced tree;ternary search tree;algorithm	Comp.	1.2614394991787643	-51.02658876230547	198370
f419389ddb3bc2fd14bc2943371959ba8606e854	performance analysis of ga-based iterative and non-iterative learning approaches for medical domain data sets			iterative method;profiling (computer programming);software release life cycle	Amit Kumar;Bikash Kanti Sarkar	2017	Intelligent Decision Technologies	10.3233/IDT-170298	iterative learning control;computer science;machine learning;theoretical computer science;data set;artificial intelligence	ML	6.206454285018582	-41.184459810416215	198455
fbad6440a4fa2b5aa5950eb19d678b3315ca6949	mapreduce based method for big data semantic clustering	pattern clustering;optimisation;k means clustering algorithm mapreduce based method big data semantic clustering big data analysis cloud computing environments data chunks parallel clustering ant colony;ant colony;k means;pattern clustering cloud computing data handling optimisation;big data;k means cloud computing big data mapreduce ant colony;clustering algorithms semantics accuracy information management data handling data storage systems algorithm design and analysis;mapreduce;data handling;cloud computing	Big data analysis is very hot in cloud computing environments. How to automatically map heterogeneous data with the same semantics is one of the key problems in big data analysis. A big data clustering method based on the MapReduce framework is proposed in this paper. Big data are decomposed into many data chunks for parallel clustering, which is implemented by Ant Colony. Data elements are moved and clustered by ants according to the presented criterion. The proposed method is compared with the MapReduce framework based k-means clustering algorithm on a great amount of practical data. Experimental results show that the proposal is much effective for big data clustering.	algorithm;ant colony;big data;cloud computing;cluster analysis;computation;computer cluster;data element;effective method;k-means clustering;mapreduce;parallel computing;regular expression;requirement;scheduling (computing)	Jie Yang;Xiaoping Li	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.480	correlation clustering;data stream clustering;big data;cloud computing;computer science;data science;canopy clustering algorithm;ant colony;machine learning;group method of data handling;consensus clustering;cure data clustering algorithm;data mining;database;cluster analysis;k-means clustering;clustering high-dimensional data	DB	-3.126298743833907	-38.56498431165626	198484
f37d89218fe5506cdbaa537a7036edf8a18edefb	a new feature selection methodology for k-mers representation of dna sequences	settore inf 01 informatica	DNA sequence decomposition into k-mers and their frequency counting, defines a mapping of a sequence into a numerical space by a numerical feature vector of fixed length. This simple process allows to compare sequences in an alignment free way, using common similarities and distance functions on the numerical codomain of the mapping. The most common used decomposition uses all the substrings of a fixed length k making the codomain of exponential dimension. This obviously can affect the time complexity of the similarity computation, and in general of the machine learning algorithm used for the purpose of sequence analysis. Moreover, the presence of possible noisy features can also affect the classification accuracy. In this paper we propose a feature selection method able to select the most informative k-mers associated to a set of DNA sequences. Such selection is based on the Motif Independent Measure (MIM), an unbiased quantitative measure for DNA sequence specificity that we have recently introduced in the literature. Results computed on public datasets show the effectiveness of the proposed feature selection method.	feature selection	Giosuè Lo Bosco;Luca Pinello	2014		10.1007/978-3-319-24462-4_9	computer science;bioinformatics;pattern recognition;mathematics;algorithm	Vision	-0.4526456854742344	-48.20072376623281	198556
d4b8c033ccc0e08581a4b0bfc0be2427ea751eeb	non-myopic feature selection method for continuous attributes and discrete class	information systems collaborative tools costs collaborative software collaborative work proposals unified modeling language technology management online communities technical collaboration information analysis;groupware;information systems;workflow management software groupware information systems;cooperative methodology;comis cooperative methodology information systems workflow management system;comis;case tool;workflow management software;workflow management system;information system	Currently there exist diverse feature selection ranking methods and metrics for databases with pure discrete data (attributes and class), or pure continuous data. However, little work has been done for the case of continuous attributes with discrete class, and at the same time evaluating attribute subsets in a non-myopic fashion, considering its inter-dependencies or interactions. Normally what we can do is perform discretization, and then apply some traditional feature selection method; nevertheless the results vary depending on the discretization method that we utilized. Additionally, if we only evaluate isolated attributes, we probably obtain poor results, because we are not considering attribute inter-dependencies. We propose a metric and method for feature selection on continuous data with discrete class, inspired in the Shannon's entropy and the information gain, which overcomes the above problems. In the experiments that we realized, with synthetic and real databases, the proposed method has shown to be fast and produce near optimum solutions, selecting few attributes.	attribute-value system;database;discrete mathematics;discretization;existential quantification;experiment;feature selection;information gain in decision trees;interaction;kullback–leibler divergence;shannon (unit);synthetic intelligence	Manuel Mejía-Lavalle;Guillermo Rodríguez-Ortiz;Gustavo Arroyo-Figueroa	2007	Eighth Mexican International Conference on Current Trends in Computer Science (ENC 2007)	10.1109/ENC.2007.12	computer science;knowledge management;management science;management;workflow management system;information system;workflow engine;workflow technology	DB	1.69109587130247	-43.52781377716425	198703
5a9ec7ea59883549c97eae24fa26fd670db0a96a	temporal bayesian classifiers for modelling muscular dystrophy expression data	microarray data;bayesian classifier;time series;muscular dystrophy;research paper	The analysis of microarray data from time-series experiments requires specialised algorithms, which take the temporal ordering of the data into account. In this paper we explore a new architecture of Bayesian classifier that can be used to understand how biological mechanisms differ with respect to time. We show that this classifier improves the classification of microarray data and at the same time ensures that the models can easily be analysed by biologists by incorporating time transparently. In this paper we focus on data that has been generated to explore different types of muscular dystrophy.	algorithm;bayesian network;experiment;feature selection;interaction;microarray;naive bayes classifier;time series;wiring	Allan Tucker;Peter A. C. 't Hoen;Veronica Vinciotti;Xiaohui Liu	2006	Intell. Data Anal.		microarray analysis techniques;naive bayes classifier;computer science;bioinformatics;machine learning;time series;data mining;statistics	ML	6.551594413312403	-50.26319193081037	198923
b441f2af805938704dd5e6c0ab931834d554fc28	workflow mining alpha algorithm - a complexity study	internal structure;space complexity	Workflow mining algorithms are used to improve and/or refine design of existing workflows. Workflows are composed of sequential,  parallel, conflict and iterative structures. In this paper we present results of experimental complexity study of the alpha  workflow mining algorithm. We studied time and space complexity as dependent on workflow’s internal structure and on the number  of workflow tasks.  	alpha algorithm	Boleslaw Mikolajczak;Jian-Lun Chen	2005		10.1007/3-540-32392-9_51	computer science;bioinformatics;theoretical computer science;worst-case complexity;data mining;dspace	ML	-1.7530482150081275	-50.14349455915989	199185
bde527dc524e03f8ba39ade5a24b514bea68c7c5	accumulated cost based test-cost-sensitive attribute reduction	accumulated cost;attribute reduction test cost;cost sensitive learning	As a generalization of the classical reduct problem, test-costsensitive attribute reduction aims at finding a minimal test-cost reduct. The performance of an existing algorithm is not satisfactory, partly because that the test-cost of an attribute is not appropriate to adjust the attribute significance. In this paper, we propose to use the test-cost sum of selected attributes instead and obtain a new attribute significance function, with which a new algorithm is designed. Experimental results on the Zoo dataset with various test-cost settings show performance improvement of the new algorithm over the existing one.		Huaping He;Fan Min	2011		10.1007/978-3-642-21881-1_39	machine learning;pattern recognition;data mining;mathematics	EDA	7.783841935056675	-41.80809183077283	199223
2c33639ada78b8d34529c31bc4010c69ea81a77b	fuzzy clustering of spatial binary data	mixture models		binary data;cluster analysis;fuzzy clustering	Mô Dang;Gérard Govaert	1998	Kybernetika		correlation clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;pattern recognition;cure data clustering algorithm;mixture model;mathematics;cluster analysis;statistics	ML	2.873324296166639	-40.90653148520682	199358
ee2c4f59e3d4afc208956afb14d28fc2f1970d76	a wrapper-based feature selection method for admet prediction using evolutionary computing	physicochemical properties;hydrophobicity;genetic algorithm;genetic algorithms;feature selection;prediction model;qsar;fitness function;neural network;evolutionary computing	Wrapper methods look for the selection of a subset of features or variables in a data set, in such a way that these features are the most relevant for predicting a target value. In chemoinformatics context, the determination of the most significant set of descriptors is of great importance due to their contribution for improving ADMET prediction models. In this paper, a comprehensive analysis of descriptor selection aimed to physicochemical property prediction is presented. In addition, we propose an evolutionary approach where different fitness functions are compared. The comparison consists in establishing which method selects the subset of descriptors that best predicts a given property, as well as maintaining the cardinality of the subset to a minimum. The performance of the proposal was assessed for predicting hydrophobicity, using an ensemble of neural networks for the prediction task. The results showed that the evolutionary approach using a non linear fitness function constitutes a novel and a promising technique for this bioinformatic application.		Axel J. Soto;Rocío L. Cecchini;Gustavo E. Vazquez;Ignacio Ponzoni	2008		10.1007/978-3-540-78757-0_17	genetic algorithm;computer science;machine learning;pattern recognition;data mining;feature selection	Vision	8.943402712228613	-46.90119090414475	199414
